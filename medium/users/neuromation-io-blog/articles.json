[
    {
        "url": "https://medium.com/neuromation-io-blog/neuronuggets-what-do-deep-learning-frameworks-do-exactly-6615cd55d618?source=---------0",
        "title": "NeuroNuggets: What Do Deep Learning Frameworks Do, Exactly?",
        "text": "Our sixth installment of the NeuroNuggets series is slightly different from previous ones. Today we touch upon an essential and, at the same time, rapidly developing area \u2014 deep learning frameworks, software libraries that modern AI researchers and practitioners use to train all these models that we have been discussing in our first five installments. In today\u2019s post, we will discuss what a deep learning framework should be able to do and see the most important algorithm that all of them must implement.\n\nWe have quite a few very talented junior researchers in our team. Presenting this post on neural networks\u2019 master algorithm is Oktai Tatanov, our junior researcher in St. Petersburg:\n\nA good AI model begins with an objective function. We also begin this essay with explaining the main purpose of deep learning frameworks. What does it mean to define a model (say, a large-scale convolutional network like the ones we discussed earlier), and what should a software library actually do to convert this definition into code that trains and/or applies the model?\n\nActually, every modern deep learning framework should be able to do the following checklist:\n\nAs you can see, every point is somehow about computational graphs\u2026 but what are those? How does it relate to neural networks? Let us explain.\n\nArtificial neural networks are called neural networks for a reason: they model, in a very abstract and imprecise way, processes that happen in our brains. In particular, neural networks consist of a lot of artificial neurons (perceptrons, units); outputs of some of the neurons serve as inputs for others, and outputs of the last neurons are the outputs of the network as a whole. Mathematically speaking, a neural network is a very large and complicated composition of very simple functions.\n\nComputational graphs reflect the structure of this composition. A computational graph is a directed graph where every node represents a mathematical operation or a variable, and edges connect these operations with their inputs. As usual with graphs, a picture is worth a thousand words \u2014 here is a computational graph for the function\n\nThe whole idea of neural networks is based on connectionism: huge compositions of very simple functions can give rise to very complicated behaviour. This has been proven mathematically many times, and modern deep learning techniques show how to actually implement these ideas in practice.\n\nBut why are the graphs themselves useful? What problem are we trying to solve with them, and what exactly are deep learning frameworks supposed to do?\n\nThe main goal of deep learning is to train a neural network in such a way that it best describes the data we have. Most often, this problem is reduced to the problem of minimizing some kind of loss function or maximizing the likelihood or posterior distribution of a model, i.e., we either want to minimize how much our model gets wrong or want to maximize how much it gets right. The frameworks are supposed to help with these optimization problems.\n\nModern neural networks are very complicated and non-convex, so basically the only optimization method we have for large neural networks is the simplest and most universal optimization approach: gradient descent. In gradient descent, we basically compute the derivatives of the objective function (the gradient is the vector consisting of all partial derivatives) and then go into the direction where the objective function increases or decreases, as needed. Like this:\n\nThere are, of course, many interesting improvements and modifications to this simple idea: Nesterov\u2019s momentum, adaptive gradient descent algorithms that change the learning rate separately for every weight\u2026 Perhaps one day we will return to this discussion in NeuroNuggets. But how do we compute the gradient if we have the neural network as model? That\u2019s where computational graphs help\u2026\n\nTo compute the gradient, deep learning frameworks use an algorithm called backpropagation (bprop); it basically amounts to using the chain rule sequentially across the computational graph. Let us walk through an application of backpropagation to our previous example. We begin by computing partial derivatives of every node of the graph with respect to each of its inputs; we assume that it is easy to do, and neural networks do indeed consist of simple units for which it is easy. Like in our example:\n\nNow we need to combine these derivatives with the chain rule. In backpropagation, we do it sequentially from the graph\u2019s output, where the objective function is computed. There we always have\n\nNext, for example, we can get\n\nsince we already know both factors in this formula. Backpropagation means that we go through the graph from right to left, computing partial derivatives of f with respect to every node, including the weights that we are interested in. Here is the final result for our example:\n\nThis very simple algorithm allows us to set up algorithms to train any deep neural network. This is exactly what any deep learning framework is supposed to do; they are in reality automatic differentiation libraries more than anything else. The main function of any framework is to compute and take derivatives of huge compositions of functions. Note, by the way, that to compute the function you also need to traverse the computational graph, but this time from left to right, from variables to the outputs; this process is called forward propagation (fprop).\n\nOnce you have the basic functionality of fprop and bprop in your library, you want to make it as efficient as possible. Efficiency gains mostly come from parallelization: note that operations in one part of the graph are completely independent from what happens in other parts. This means, for instance, that if you have a layer in your neural network, i.e., a set of nodes that do not feed into each other but all receive inputs from previous layers, you can compute them all in parallel during both forward propagation and backpropagation.\n\nThis is exactly the insight that to a large extent fueled the deep learning revolution: this kind of parallelization can be done across hundreds or even thousands of computational cores. What kind of hardware has thousands of cores? Why, the GPUs, of course! In 2009\u20132010, it turned out that regular off-the-shelf GPUs designed for gamers can provide a 10x-50x speedup in training neural networks. This was the final push for many deep learning models and applications into the territory of what is actually computationally feasible. We will stop here for the moment but hopefully will discuss parallelization in deep learning in much greater detail at some future post.\n\nThere is one more interesting complication. Deep learning frameworks come with two different forms of computational graphs, static and dynamic. Let us find out what this means.\n\nThe main idea of a static computational graph is to separate the process of building the graph and executing backpropagation and forward propagation (i.e., computing the function). Your graph is immutable, i.e., you can\u2019t add or remove nodes at runtime.\n\nIn a dynamic graph, though, you can change the structure of the graph at runtime: you can add or remove nodes, dynamically changing its structure.\n\nBoth approaches have their advantages and disadvantages. For static graphs:\n\nWe will see live examples of code that makes use of dynamic computational graphs in the next installment, where we will consider several deep learning frameworks in detail. And now let us finish with an overview.\n\nOn March 10, Andrej Karpathy (Director of AI at Tesla) published a tweet with very interesting statistics about machine learning trends. Here is the graph of unique mentions of deep learning frameworks over the last four years:\n\nUnique mentions of deep learning frameworks in arXiv papers (full text) over time, based on 43K ML papers over last 6 years. Source: https://twitter.com/karpathy/status/972295865187512320\n\nThe graph shows that the top 4 general-purpose deep learning frameworks right now are TensorFlow, Caffe, Keras, and PyTorch, while, for example, historically the first widely used framework theano has basically lost traction.\n\nThe frameworks have interesting relations between them, and it is worthwhile to consider them all, get a feeling of what the code looks like for each, and discuss their pros and cons. This post, however, is already growing long; we will come back to this discussion in the second part."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromations-neurotoken-ntk-secures-listing-on-the-bcex-exchange-cfa7a917ec42?source=---------1",
        "title": "Neuromation\u2019s NeuroToken (NTK) Secures Listing on the BCEX Exchange",
        "text": "Great news from the Neuromation Team! As the next big step in the development of the NeuroPlatform, NTK will be listed on BCEX exchange beginning on May 3, 2018. An ERC20 token, NTK will first be listed as an NTK/ETH pair, then as an NTK/BTC pair within 2 weeks of the initial listing.\n\n\u201cWith the BCEX listing, NeuroPlatform users will enjoy materially improved liquidity and increasingly seamless conversion into NTK from the major cryptocurrencies, Ethereum and Bitcoin. Our goal is simplicity of each transaction, allowing a wider global audience to participate in the NeuroPlatform marketplace, confidentially.\u201d, noted CEO Maxim Prasolov.\n\nThe BCEX listing is one of many steps in the execution of the Neuromation Road Map and the development of Neuromation as a global leader in the AI democratization.\n\nAccounts can be opened at https://bcex.ca. NTK will be listed at 10 am. GMT +2 timezone.\n\nNeuromation\u2019s Mission and the Next Big Step\n\nNeuromation is a technology company focused on the coming revolution in Artificial Intelligence (AI). Neuromation is dedicated to the democratization of AI for researchers, businesses, and individuals by lowering the high costs of the development and training of applied AI systems. Neuromation\u2019s Synthetic Data and Distributed Computing Power Platform is breaking the two major bottlenecks restricting AI \u2014 the lack of large, accurately-labeled datasets and the lack of affordable computing power.\n\nPowering the NeuroPlatform is the Neurotoken (NTK), an exclusive method of exchange on the Neuromation marketplace. Neuromation provides platform users with the liquidity and confidence necessary to seamlessly transact in NTK on the NeuroPlatform.\n\nNeuromation Team is grateful to our wide AI community, supporters and partners for making this possible, and bringing the AI revolution closer!\n\n*BCEX, a Canada-based digital asset trading platform focused on the Chinese-language market, has a market capitalization of $250 million, currently trading 13 trading crypto-pairs. BCEX is owned by Canada-based Cascadia Blockchain Group who is registered with FINTRAC, Canada\u2019s Financial Transactions and Reports Analysis Centre. The exchange operates under the supervision of the Canadian financial regulators and is audited by Manning Elliot LLP. BCEX is fully legal-compliant, providing a secure and open crypto-currency exchange to its global customers.\n\nDISCLAIMER: This material should not be construed as an offer to sell or buy securities or any other assets in any jurisdiction, a solicitation for investment, or investment advice. NTK tokens are utility tokens issued by Neuromation O\u00dc (incorporated under the laws of Estonia, reg. code 14331930) providing users with current and/or future access to Neuromation O\u00dc\u2019s products or services. Persons wishing to sell or purchase NTK tokens should seek the advice of independent experts before entering into any such transaction and shall enter into such transactions at their own risk. Neuromation O\u00dc, its directors, officers, employees, and associates expressly disclaim any liability related to the accuracy, reliability, or completeness of any material contained herein."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuronuggets-understanding-human-poses-in-real-time-b73cb74b3818?source=---------2",
        "title": "NeuroNuggets: Understanding Human Poses in Real-Time",
        "text": "This week, we continue the NeuroNuggets series with the fifth installment on another important computer vision problem: pose estimation. We have already talked about segmentation; applied to humans, segmentation would mean to draw silhouettes around pictures of people. But what about the skeleton? We need pose estimation, in particular, to understand what a person is doing: running, standing, reading NeuroNuggets?\n\nToday, we present a pose estimation model based on the so-called Part Affinity Fields (PAF), a model from this paper that we have uploaded on the NeuroPlatform as a demo. And presenting this model today is Arseny Poezzhaev, our data scientist and computer vision aficionado who has moved from Kazan to St. Petersburg to join Neuromation! We are excited to see Arseny join and welcome him to the team (actually, he joined from the start, more than a month ago, but the NeuroNuggets duty caught up only now). Welcome:\n\nPose estimation is one of the long-standing problems of computer vision. It has interested researchers over the last several decades because not only is pose estimation an important class of problems itself, but it also serves as a preprocessing step for many even more interesting problems. If we know the pose of a human, we can further train machine learning models to automatically infer relative positions of the limbs and generate a pose model that can be used to perform smart surveillance with abnormal behaviour detection, analyze pathologies in medical practices, control 3D model motion in realistic animations, and a lot more.\n\nMoreover, not only humans can have limbs or a pose! Basically, pose estimation can deal with any composition of rigidly moving parts connected to each other at certain joints, and the problem is to recover a representative layout of body parts from image features. We at Neuromation, for example, have been doing pose estimation for synthetic images of pigs (cf. our Piglet\u2019s Big Brother project):\n\nTraditionally, pose estimation used to be done by retrieving motion patterns from optical markers attached to the limbs. Naturally, pose estimation would work much better if we could afford to put special markers on every human on the picture; alas, our problem is a bit harder. The next point of distinction between different approaches is the hardware one can use: can we use multiple cameras? 3D cameras that estimate depth? infrared? Kinect? is there a video stream available or only still images? Again, each additional source of data can only make the problem easier, but in this post we concentrate on a single standard monocular camera. Basically, we want to be able recognize the poses on any old photo.\n\nPose estimation from a single image is a very under-constrained problem precisely due to the lack of hints from other channels, different viewpoints from multiple cameras, or motion patterns from video. The same pose can produce quite different appearances from different viewpoints and, even worse, human body has many degrees of freedom, which means that the solution space has high dimension (always a bad thing, trust me). Occlusions are another big problem: partially occluded limbs cannot be reliably recognized, and it\u2019s hard to teach a model to realize that a hand is simply nowhere to be seen. Nevertheless, single person pose estimation methods show quite good results nowadays.\n\nWhen you move from a single person to multiple people, pose estimation becomes even harder: humans occlude and interact with other humans. In this case, it is common practice to use a so-called top-down approach: apply a separately trained human detector (based on object detection techniques such as the ones we discussed before), find each person, and then run pose estimation on every detection. It sounds reasonable but actually the difficulties are almost insurmountable: if the detector fails to detect a person, or if limbs from several people appear in a single bounding box (which is almost guaranteed to happen in case of close interactions or crowded scenes), the whole algorithm will fail. Moreover, the computation time needed for this approach grows linearly with the number of people on the image, and that can be a big problem for real-time analysis of groups of people.\n\nIn contrast, bottom-up approaches recognize human poses from pixel-level image evidence directly. They can solve both problems above: when you have information from the entire picture you can distinguish between the people, and you can also decouple the runtime from the number of people on the frame\u2026 at least theoretically. However, you are still supposed to be able to analyze a crowded scene with lots of people, assigning body parts to different people, and even this task by itself could be NP-hard in the worst case.\n\nStill, it can work; let us show which pose estimation model we chose for the Neuromation platform.\n\nIn the demo, we use the method based on the \u201cRealtime Multi-Person 2D Pose Estimation using Part Affinity Fields\u201d paper done by researchers from The Robotics Institute at Carnegie Mellon University (Cao et al., 2017). Here is it in live action:\n\nIt is a bottom-up approach, and it uses the so-called Part Affinity Fields (PAFs) together with estimation of body-part confidence maps. PAFs are the main new idea we introduce today, so let us discuss them in a bit more detail. A PAF is a set of 2D vector fields that encode the location and orientation of the limbs. Vector fields? Sounds mathy\u2026 but wait, it\u2019s not that bad.\n\nSuppose you have already detected all body parts (hands, elbows, feet, ankles etc.); how do you now generate poses from them? First, you must find out how to connect two points to form a limb. For each body part, there are several candidates to form a limb: there are multiple people on the image, and there also can be lots of false positives. We need some confidence measure for the association between each body part detection. Cao et al. propose a novel feature representation called Part Affinity Fields that contains information about location as well as orientation across the region of support of the limb.\n\nIn essence a PAF is a set of vectors that encodes the direction from one part of the limb to the other; each limb is considered as an affinity field between body parts. Here is a forehand:\n\nFigure 1. Affinity field visualization for right forehand. The color encodes limb\u2019s orientation.\n\nIf a point lies on the limb then its value in the PAF is a unit vector pointing from starting joint point to ending joint point of this limb; the value is zero if it is outside the limb. Thus, PAF is a vector field that contains information about one specific limb for all the people on the image, and the entire set of PAFs encodes all the limbs for every person. So how do PAFs help us for pose estimation?\n\nFirst, let us go through the overall pipeline of the algorithm.\n\nFigure 2. Overall pipeline. The method of (Cao et al.) takes an input image (a) and simultaneously infers two maps with body-parts (b) and PAFs predictions \u00a9. Then it parses body part candidates and runs a special bipartite matching algorithm to associate them (d); finally, it assembles the body parts into full body poses (e).\n\nFigure 2 above illustrates all the steps from an input image (Fig. 2a) to anatomical keypoints as an output (Fig. 2e). First, a feedforward neural network predicts a set of body part locations on the image (Fig. 2b) in the form of a confidence map and a set of PAFs that encode the degree of association between these body parts (Fig. 2c). Thus, the algorithm gets all information necessary for further matching of limbs and people (all of this stuff does sound a bit bloodthirsty, doesn\u2019t it?). Next, confidence maps and affinity fields are parsed together (Fig 1d) to output the final positions of limbs for all people on the picture.\n\nAll of this sounds very reasonable: we now have a plan. But so far this is only a plan: we don\u2019t know how to do any of these steps above. So now let us consider every step in detail.\n\nOne of the core ideas of (Cao et al., 2017) is to simultaneously predict detection confidence map and affinity fields. The method uses a special feedforward network as a feature extractor. The network looks like this:\n\nFigure 3. Architecture of the two-branch multistage CNN. Each stage in the top branch (beige) predicts a confidence map S, and each stage in the bottom branch (blue) predicts a PAF L. After every stage, predictions from both branches are concatenated with image features F (which come from a VGG-based architecture) and used as input for the next stage. Each branch performs multiple inferences, one per body part.\n\nAs you can see, it is split into two branches: the top branch predicts the detection confidence maps and the bottom branch is for affinity fields. Both branches are organized as an iterative prediction architecture, which refines predictions over successive stages. The improvement of accuracy of predictions is controlled by intermediate supervision at each stage. Here is how it might look on a real image:\n\nFigure 4. Demonstration of real image inference by the two-branched architecture neural network.\n\nBefore passing input to this two-branch network the method uses auxiliary CNN (first 10 layers of VGG-19) to extract an input feature map F. This prediction is processed by both branches, and their predictions concatenated with initial F are used as input for the next stage (as features).\n\nThis process is repeated on every stage, and you can see the refinement process across stages on Figure 4 above.\n\nTake a look at Figure 5 below, which again illustrates the above-mentioned refinement process:\n\nFigure 5. Confidence maps of right wrist (first row) and PAFs of right forearm (second row) across stages. We can see that despite confusion on the first stage, the method can fix its mistakes on later stages.\n\nAt the end of each stage, the corresponding loss function is applied for each branch to guide the network.\n\nNow consider the top branch; each confidence map is a 2D representation of our confidence that each pixel belongs to a particular body part (we remind that \u201cbody parts\u201d here are \u201cpoints\u201d such as wrists and elbows, and, say, forearms are referred to as \u201climbs\u201d rather than \u201cbody parts\u201d). To get body part candidate regions, we aggregate confidence maps for different people. After that, the algorithm performs non-maximum suppression to obtain a discrete set of parts locations:\n\nDuring inference, algorithm computes line integrals over all the PAFs along the line segments between pairs of detected body-parts. If the candidate limb formed by connection of certain pair of points is aligned with corresponding PAF then it\u2019s considered as a true limb. This is exactly what the bottom branch does.\n\nWe now see how the algorithm can find limbs on the image between two points. But we still cannot estimate poses because we need the full body model! We need to somehow connect all these limbs into people. Formally speaking, the algorithm has found body part candidates and has scored pairs of these parts (integrating over PAFs), but the final goal is to find the optimal assignment for the set of all possible connections.\n\nFormally speaking, this problem can be viewed as a k-partite graph matching problem, where nodes of the graph are body part detections, and edges are all possible connections between them (possible limbs). Here k-partite matching means that the vertices can be partitioned into k groups of nodes with no connections inside each group (i.e., vertices corresponding to the same body part). Edges of the graph are weighted with part affinities. Like this:\n\nA direct solution of this problem may be computationally infeasible (NP-hard), so (Cao et al., 2017) propose a relaxation where the initial k-partite graph is decomposed into a set of bipartite graphs (Fig. 7d) where the matching task is much easier to solve. The decomposition is based on the problem domain: basically, you know how body parts can connect, and a hip cannot be connected to a foot directly, therefore we can first connect hip to knee and then knee to foot.\n\nThat\u2019s all, folks! We have considered all the steps in the algorithm that can retrieve poses from a single raw image. Let us now see how it works on our platform.\n\nThere are, as always, a few simple steps to run this algorithm on our image of interest:\n\n7. Try the demo! You can upload your own photo for pose estimation. We chose this image from the Mannequin Challenge:\n\n8. And here you go! One can see stick models representing poses of people on the image:\n\nAnd here is a picture of Neuromation leadership in Singapore:\n\nThe results are, again, pretty good:"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/creating-molecules-from-scratch-i-drug-discovery-with-generative-adversarial-networks-9d42cc496fc6?source=---------3",
        "title": "Creating Molecules from Scratch I: Drug Discovery with Generative Adversarial Networks",
        "text": "We\u2019ve got great news: the very first paper with official Neuromation affiliation has appeared! This work, \u201c3D Molecular Representations Based on the Wave Transform for Convolutional Neural Networks\u201d, has recently appeared in a top biomedical journal, Molecular Pharmaceutics. This paper describes the work done by our friends and partners Insilico Medicine in close collaboration with Neuromation. We are starting to work together with Insilico on this and other exciting projects in the biomedical domain to both significantly accelerate drug discovery and improve the outcomes of clinical trials; by the way, I thank CEO of Insilico Medicine Alex Zhavoronkov and CEO of Insilico Taiwan Artur Kadurin for important additions to this post. Collaborations between top AI companies are becoming more and more common in the healthcare space. But wait \u2014 the American Chemical Society\u2019s Molecular Pharmaceutics?! Doesn\u2019t sound like a machine learning journal at all, does it? Read on\u2026\n\nGetting a new drug to the market is a long and tedious process; it can take many years or even decades. There are all sorts of experiments, clinical studies, and clinical trials that you have to go through. And about 90% of all clinical trials in humans fail even after the molecules have been successfully tested in animals.\n\nBut to a first approximation, the process is as follows:\n\nSo where is the place of AI in this process? Naturally, we can\u2019t hope to replace the lab or, God forbid, clinical trials: we wouldn\u2019t want to sell a drug unless we are certain that it\u2019s safe and confident that it is effective in a large number of patients. This certainty can only come from actual live experiments. In the future it is likely that we will be able to go from in silico (in a computer) to patients immediately with the AI-driven drug discovery pipelines but today we need to do the experiments.\n\nNote, however, the initial stage of identifying the lead molecules. At this stage, we cannot be sure of anything, but live experiments in the lab are still very slow and expensive, so we would like to find lead molecules as accurately as we can. After all, even if the goal is to treat cancer there is no hope to check the entire endless variation of small molecules in the lab (\u201csmall\u201d are molecules that can easily get through a cell membrane, which means basically everything smaller than a nucleic acid). 72 million is just the size of a specific database, the total number of small molecules is estimated to be between 10\u2076\u2070 and 10\u00b2\u2070\u2070, and synthesizing and testing a single new molecule in the lab may cost thousands or tens of thousands of dollars. Obviously, the early guessing stage is really, really important.\n\nBy now you can see how it might be beneficial to apply latest AI techniques to drug discovery. We can use machine learning models to try and choose the molecules that are most likely to have desired properties.\n\nBut when you have 72 million of something, \u201cchoosing\u201d ceases to look like classification and gets more into the \u201cgeneration\u201d part of the spectrum. We have to basically generate a molecule from scratch, and not just some molecule, but a promising candidate for a drug. With modern generative models, we can stop searching for a needle in a haystack and design perfect needles instead:\n\nHow do we generate something from scratch? Deep learning does have a few answers when it comes to generative models; in this case, the answer turned out to be\u2026\n\nWe have already briefly talked about generative adversarial networks (GANs) in a previous post, but I\u2019m sure a reminder is in order here. GANs are a class of neural networks that aim to learn to generate objects from a certain class. Previously, GANs had been mostly used to generate images: human faces as in (Karras et al., 2017), photos of birds and flowers as in StackGAN, or, somewhat suprisingly, bedroom interiors, a very popular choice for GAN papers due to a commonly used part of the standard LSUN scene understanding dataset. Generation in GANs is based on a very interesting and rather commonsense idea. They have two parts that are in competition with each other:\n\nHere is how the general scheme looks:\n\nIn other words, the discriminator learns to spot the generator\u2019s counterfeit images, while the generator learns to fool the discriminator. I refer to, e.g., this post for a simple and fun introduction to GANs.\n\nWe at Neuromation are following GAN research with great interest due to many possible exciting applications. For example, conditional GANs have been used for image transformations with the explicit purpose of enhancing images; see, e.g., image de-raining recently implemented with GANs in this work. This ties in perfectly with our own ideas of using synthetic data for computer vision: with a proper conditional GAN for image enhancement, we might be able to improve synthetic (3D-rendered) images and make them more like real photos, especially in small details. In the post I referred to, we saw how NVIDIA researchers introduced a nice way to learn GANs progressively, from small images to large ones.\n\nBut wait. All of this so far makes a lot of sense for images. Maybe it also makes sense for some other relatively \u201ccontinuous\u201d kinds of data. But molecules? The atomic structure is totally not continuous, and GANs are notoriously hard to train for discrete structures. Still, GANs did prove to work for generating molecules as well. Let\u2019s find out how.\n\nOur recent paper on molecular representations is actually a part of a long line of research done by our good friends and partners, Insilico Medicine. It began with Insilico\u2019s paper \u201cThe cornucopia of meaningful leads: Applying deep adversarial autoencoders for new molecule development in oncology\u201d, whose lead author Artur Kadurin is a world-class expert on deep learning, one of Insilico Medicine\u2019s Pharma.AI team on deep learning for molecular biology, recently appointed CEO of Insilico Taiwan\u2026 and my Ph.D. student.\n\nIn this work, Kadurin et al. presented an architecture for generating lead molecules based on a variation of the GAN idea called Adversarial Autoencoders (AAE). In AAE, the idea is to learn to generate objects from their latent representations. Generally speaking, autoencoders are neural architectures that take an object as input\u2026 and try to return the same object as output. Doesn\u2019t sound too hard, but the idea is that in the middle of the architecture, the input must go through a middle layer that learns a latent representation, i.e., a set of features that succinctly encode the input in such a way that afterwards subsequent layers can decode the object back:\n\nEither the middle layer is simply smaller (has lower dimension) than input and output, or the autoencoder uses special regularization techniques, but in any case it\u2019s impossible to simply copy the input through all layers, and the autoencoder has to extract the really important stuff.\n\nSo what did Kadurin et al. do? They took a conditional adversarial autoencoder and trained it to generate fingerprints of molecules, using and serving desired properties as conditions. Here is the general model architecture from (Kadurin et al., 2017):\n\nLooks just like the autoencoder above, but with two important additions in the middle:\n\nThere is still that nagging question about the representations, though. How do we generate discrete structures like molecules? We will discuss molecular representations in much greater detail in the next post; here let me simply mention that this work used a standard representation of a molecule as a MACCS fingerprint, a set of binary characteristics of the molecule such as \u201chow many oxygens is has\u201d or \u201cdoes it have a ring of size 4\u201d.\n\nBasically, the problem becomes to \u201ctranslate\u201d the condition, i.e., desired properties of a molecule, into more \u201clow-level\u201d properties of the molecular structure encoded into their MACCS fingerprints. Then a simple screening of the database can find molecules with the fingerprints most similar to generated ones.\n\nAt the time that was the first peer-reviewed paper showing that GANs can generate novel molecules. The submission was made in June 2016 and it was accepted in December 2016. In 2017 the community started to notice:\n\nIt turned out that the resulting molecules do look interesting\u2026\n\nThis post is getting a bit long; let\u2019s take it one step at a time. We will get to our recent paper in the next installment, and now let us summarize what we\u2019ve seen so far.\n\nSince the deep learning revolution, deep neural networks have been revolutionizing one field after another. In this post, we have seen how modern deep learning techniques are transforming molecular biology and drug discovery. Constructions such as adversarial autoencoders are designed to generate high-quality objects of various nature, and it\u2019s not a huge wonder that such approaches work for molecular biology as well. I have no doubt that in the future, these or similar techniques will bring us closer to truly personalized medicine.\n\nSo what next? Insilico has already generated several very promising candidates for really useful drugs. Right now they are undergoing experimental validation in the lab. Who know, perhaps in the next few years we will see new drugs identified by deep learning models. Fingers crossed."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-platform-update-f848e2e7c10a?source=---------4",
        "title": "Neuromation Platform Update \u2013 Neuromation \u2013",
        "text": "The Neuromation team continues the Asian Road Show and keeps you updated on the latest news on Neuromation\u2019s platform development. This week, read about our business trip to China, our latest improvements and exciting achievements!\n\nNeuromationa and OSA DC continue the roadshow that will open doors to the markets of China, Korea, and Japan.\n\nOur Moscow team has completed the following:\n\n1. Token site:\n\n Aggregation of token turnover from markets that were announced in official channels\n\n4. OSA HP:\n\n \u2014 Working on whitepaper for synthetic data and statistic efficiency of synthetic data. \n\n \u2014 Analysis and creation of tools and scripts for testing photoset. \n\n \u2014 Preparing photoset for labeling and markup \n\n \u2014 Creation of list for purchase for OSA HP according to contract for 1,000 elements. \n\n \u2014 Approximately 3,000 images were marked.\n\nOur St. Petersburg team has done the following:\n\n2. Retail\n\n \u2014 Generating feature spaces from the Faster R-CNN architecture for search task. Constructed t-SNE and k-means, no positive results yet. Perhaps the problem was in feature vector extraction; testing different approaches now.\n\n \u2014 Trained U-Net to search for price tags. Positive results, see below\n\n \u2014 First steps in Yandex. Toloka crowdsourcing service for data labeling, learning the theory on generative models and GANs.\n\n3. CVPR\n\n \u2014 Linknet implementation with experiments, research on the issue, new approach, new ideas\n\n \u2014 Review of Kaggle-winning satellite imaginary solutions\n\n \u2014 Superpixel extraction for pre-segmentation\n\nAs part of our ongoing project to recognize items on supermarket shelves, we plan to augment our models for the items themselves with textual information from price tags. This can help solve hard cases of object classification, but information from price tags is also interesting in its own right.\n\nThe first step to read a price tag is to find it. On the picture, you see the results of our segmentation model based on the U-Net architecture. The model was trained on our own synthetic dataset of renderings of supermarket shelves, this time with price tags. As you can see, it transfers well to real data and this opens up new opportunities for our retail projects.\n\nHaving achieved these great results this week, we are continuing to move the AI industry towards democratization and affordability.\n\nKeep an eye on our weekly updates!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuronuggets-style-transfer-ea2b2fbb00ed?source=---------5",
        "title": "NeuroNuggets: Style Transfer \u2013 Neuromation \u2013",
        "text": "In the fourth installment of the NeuroNuggets series, we continue our study of basic problems in computer vision. We remind that in the NeuroNuggets series, we discuss the demos available on the recently released NeuroPlatform, concentrating not so much on the demos themselves but rather on the ideas behind each deep learning model.\n\nIn the first installment, we talked about the age and gender detection model, which is basically image classification. In the second, we presented object detection, a more complex computer vision problem where you also have to find where an object is located. In the third, we continued this with segmentation and Mask R-CNN model. Today, we turn to something even more exciting. We will consider a popular and very beautiful application of deep learning: style transfer. We will see how to make a model that can draw your portrait in the style of Monet \u2014 or Yves Klein if you\u2019re not careful with training! I am also very happy to present the co-author of this post, one of our lead researchers Kyryl Truskovskyi:\n\nImagine that you can create a true artwork by yourself, turning your own photo or a familiar landscape into a painting done like Van Gogh or Picasso would do it. Sounds like a pipe dream? With the help of deep neural networks, this dream has now become reality. Neural style transfer has become a trending topic both in academic literature and industrial applications; we all know and use popular mobile apps for style transfer and image enhancement. Starting from 2014, these style transfer apps have become a huge PR point for deep neural networks, and today almost all smartphone users have tried some style transfer app for photos and/or video. By now, all of these methods work in real-time and run on mobile devices, and anyone can create artwork with a simple app, stylizing their own images and videos\u2026 but how do these apps work their magic?\n\nFrom the deep learning perspective, ideas for style transfer stem from attempts to interpret the features that a deep neural network learns and understand how exactly it works. Recall that a convolutional neural network for image processing gradually learns more and more convoluted features (see, e.g., our first post in the NeuroNuggets series), starting from basic local filters and getting all the way to semantic features like \u201cdog\u201d or \u201chouse\u201d\u2026 or, quite possibly, \u201cMonet style\u201d! The basic idea of style transfer is to try to disentangle these features, pull apart semantic features of \u201cwhat is on the picture\u201d from \u201chow the picture looks\u201d. Once you do it, you can try to replace the style while leaving the contents in place, and the style can come from a completely different painting.\n\nFor the reader interested in a more detailed view, we recommend \u201cNeural Style Transfer: A Review\u201d, a detailed survey of neural style transfer methods. E.g., here is an example from that paper where a photo of the Great Wall of China has been redone in classical Chinese painting style:\n\nAnd here is an example from DeepArt, one of the first style transfer services:\n\nYou can even do it with videos:\n\nBut let\u2019s see how it works!\n\nThere are two main approaches to neural style transfer: optimization and feedforward network. Let us begin with optimization.\n\nIn this approach, we optimize not the network but the resulting image. This is a very simple but powerful idea: the image itself is also just a matrix (tensor) of numbers, just like the weights of the network. This means that we can take derivatives with respect to these weights, extending backpropagation to them too and optimizing an image for a network rather then the other way around; we have already discussed this idea in \u201cCan a Neural Network Read Your Mind?\u201d.\n\nFor style transfer, it works as follows: you have a content image, style image and trained neural network (on ImageNet for example). You create a newly generated image, initializing it completely at random. Then the content image and style image pass through the early and intermediate layers of the network to compute two types of loss functions: style loss and content loss (see below). Next, we optimize their losses by changing the generated image, and after a few iterations we have beautiful stylized images. The structure is a bit intimidating:\n\nBut do not worry \u2014 let\u2019s talk about the losses. After we pass an image through a network we get a feature map from intermediate layers. This feature map captures the semantic representation of this image. And we definitely want the new image to be similar to the content image, so for content image feature maps C and generated image feature maps P we get the following content loss:\n\nThe style loss is slightly different: for the style loss we compute Gram matrices of the intermediate representations of generated images and style image:\n\nThe style loss is the Euclidean distance between Gram matrices\n\nBy directly minimizing the sum of these losses by gradient descent on our generated image, we make it more and more similar to the style image in terms of style while still keeping the content due to the content loss. We refer to the original paper, \u201cA Neural Algorithm of Artistic Style\u201d, for details. This approach works great, but its main disadvantage is that it takes a lot of computational effort.\n\nThe basic idea for the next is to use feed-forward networks for image transformation tasks. Basically, we want to create an image transformation network that would directly create beautiful stylized images, with no complicated \u201cimage training\u201d process.\n\nThe algorithm is simple. Suppose that, again, you have a content image and a style image. You feed the content image through the image transformation network and get a new generated image. After that, loss networks are used to compute style and content losses, like in the optimization method above, but after that, we optimize not the image itself but the image transformation network. This way, we get a trained image transformation network for style transfer and then can use it for stylizing as only a forward pass without any optimization at all.\n\nHere is how it looks through the entire process with the image transformation network and the loss network:\n\nThe original paper on this approach, \u201cPerceptual Losses for Real-Time Style Transfer and Super-Resolution\u201d, led to the first truly popular and the first real-time implementation of style transfer and superresolution algorithms; you can find a sample code for this approach here. The image transformation network works fast, but its main disadvantage is that we need to train a completely separate new network for every style image. This is not a problem for preset instagram-like filters but does not solve the general problem.\n\nTo fix this, authors of \u201cA Learned Representation for Artistic Style\u201d introduced an approach called conditional instance normalization. The basic idea is to train one network for several different styles. It turns out that normalization plays a huge role in style networks to model a style, and it is sufficient to specialize scaling and shifting parameters after normalization to each specific style. In simpler words, it is enough just to tune the parameters of a simple transformation after normalization for each style, before image transformation network.\n\nHere is a picture of how it works; for the mathematical details, we refer to the original paper:\n\nYou can find an example code for this model here.\n\nIn the platform demo, we use the Python framework PyTorch for training the neural networks, Flask + Gunicorne to serve it on our platform, and Docker + Mesos for deploying and scaling.\n\nNow that we know about style transfer, it is even better to see the result for ourselves. We follow the usual steps to get the model working.\n\n5. Launch it with the \u201cNew Task\u201d button:\n\n6. Try the demo! You can upload your own photo for style transfer. We chose this image:\n\n7. And here you go!\n\nAnd here we are:"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/as-chinas-fast-moving-ai-industry-prepares-to-kick-into-overdrive-neuromation-hopes-to-pave-the-cec2b5c07664?source=---------6",
        "title": "As China\u2019s fast-Moving AI Industry Prepares to Kick into Overdrive, Neuromation Hopes to Pave the\u2026",
        "text": "With venture capital pouring into AI startups throughout China and a new poll revealing that 36 percent of Chinese entrepreneurs surveyed believe AI is the most promising industry, tech companies from other countries have flocked to Shanghai and other cities, hoping to be part of the AI boom with complementary technologies, including robotics, cloud computing, and Blockchain.\n\nShanghai, China, April 11th, 2018 \u2014 When the team at data scientists and friends at Neuromation took their company\u2019s road show to Shanghai this week for the PTP International Conference, they met with potential investors and other tech entrepreneurs who as passionate about the artificial intelligence as they were.\n\n\u201cThe focus and enthusiasm of AI entprepreneurs here in China is inspiring,\u201d said Maxim Prasolov, Neuromation CEO. Prasolov and has team have been on an Asian road trip meeting potential partners and investors in Tokyo, Singapore, and this week in Shanghai.\n\nAt the PTP event, the Neuromation team demonstrate their blockchain platform which helps AI engineers create and train AI algorithms, the \u201cbrain\u2019 of autonomous, intelligent computer systems. By building and training AI algorithms and deep neural networks \u2014 webs of math loosely inspired by how brain cells work \u2014 on a shared, distributed blockchain network, Prasolov hopes to convince AI engineers in China to use the platform to collaborate and share computing power. In addition, by using Neuromation\u2019s synthetic data -data that mimics the images, sound, and other real data \u2014 engineers will be able to finish training their AI algorithms and neural networks faster and more affordably than they thought possible.\n\n\u201cOur mission here in China is to help AI engineers to use our platform to save time and money, by using our distributed computing power and our real and synthetic data sets, they\u2019ll be empowered to focus on the art of AI invention, creating intelligent, autonomous products they will certainly want to sell to an eager marketplace as quickly as possible,\u201d said Prasolov.\n\nThe Neuromation team plans to convince AI enthusiasts in China, engineers and investors that a blockchain platform can greatly accelerate the development of AI models and algorithms. It\u2019s the perfect marriage of invention and technologies, Neuromation leaders say \u2014 artificial intelligence solves machine learning and blockchain solves trust and coordination between machines.\n\n\u201cOur mission is to bring AI engineers the data and the computational power needed to train AI models they need to bring their AI-powered products to an eager market in China and worldwide,\u201d said Prasolov.\n\nHow big is the AI market? According to the research firm Tractica, the worldwide market for artificial intelligence software will boom nearly 30-fold to $89.8 billion by 2025.\n\nNeuromation is an international hi tech company with headquarters in Talinn, Estonia, engineering teams in St. Petersburg and Kiev, and sales and marketing teams in Tel Aviv. 60 developers and 12 AI scientists are working on projects in agro tech, retail innovation, and smart cameras in the surveillance industries.\n\nMedia Inquiry: To schedule an interview with a Neuromation manager regarding this story, please email: gary@vmatrixpr.com ."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-sponsors-ai-spring-hackathon-in-kyiv-fc00deb411ae?source=---------7",
        "title": "Neuromation sponsors AI Spring Hackathon in Kyiv \u2013 Neuromation \u2013",
        "text": "Last week Neuromation team has mentored young talents at the AI Spring Hackathon in Kyiv.\n\nAI SPRING HACKATHON brought together talented participants and companies to create DS-projects with real business prospects in the future, in the area of Computer Vision in Retail.\n\nOne of the tasks at the competition was a synthetic dataset, created by Neuromation, where participants were asked to find and define objects. This complicated task was created to train the future Computer Vision professionals, and won by the team MLG who have completed the task of object detection on the store shelves. Neuromation Chief Information Officer Denis Popov was mentoring teams and judging the competition.\n\nThis is not the first time that Neuromation supports young talents. In 2017 the Neuromation Start Up Competition was held for the first time, with Let\u2019s Enhance being the first grant winner. Neuromation will distribute 10% of its Neurotoken sales proceeds in computing power to startups that contribute to the development of neural networks."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-platform-update-788087b13e83?source=---------8",
        "title": "Neuromation Platform Update \u2013 Neuromation \u2013",
        "text": "This week the Neuromation team has started the Asian Road Show, from Singapore to Japan and China \u2014 we are meeting AI enthusiasts, media and NTK holders.\n\nOn Monday our sales and the top team were in Singapore, with CRO Sergey Nikolenko presenting the Neuromation Technology to the AI community.\n\nOn Wednesday, the 4th of April we opened up our booth 4\u20137 at the Tokyo AI Expo, along with our partners, OSA DC for everyone who is interested in AI and retail technologies.\n\nHundreds of companies are represented at the AI Expo in Tokyo,with 450 representatives of major Japanese businesses visiting the Neuromation and OSA DC stand in just one day.\n\nThe second day of the AI expo in Tokyo \u2014 both teams worked hard presenting the Neuromation platform and OSA DC implementation of the technology in the retail industry. Tokyo Big Sight is a remarkable landmark building that hosts thousands of the conferences, and AI Expo is considered one of the most attended events of the year.\n\nNeuromation and OSA DC teams introduced ongoing presentations to the public, which gathered crowds of executive listeners from Mitsubishi Corporation, Konica Minolta, Lawson, Pioneer and many others. Several LOIs are on the way! Follow the news!\n\nApart from our extremely busy Sales and Marketing teams, Neuromation Tech and Science departments have reported on the following progress :\n\n- My Library, Datasets, Dashboards, my account pages were refactored. Currently they are split to front-end and back-end. \n\n- API for filter and search of transfers with unit tests \n\n- API for withdraw and remove transfer with unit tests\n\n- API of removing data sets with the user account with unit test\n\n- API for edit of user, user profile, account with unit tests \n\n- My orders page with its subpages were refactored. The pages were split on back-end APIs and front-end. \n\n- Monitor for singularity API\n\n2. OSA HP:\n\n- Replacement of old textures with new ones, updating models\n\n- Comparison and verification of sent data (1673 images)\n\n- Labeled data \u2014 290 images\n\n- DataSets generation \u2014 adding new objects, verification and control of generation.\n\n3. Token site:\n\n- First version of token pulse functionality on NTK site deployed\n\n- Main landing page design, news page design, single news page design;\n\n- Setting up all the assets;\n\n- Stage audit;\n\n- Optimization;\n\n- Assets created.\n\n- One-pager texts and designs updated and localization to Korean, Japanese, simplified Chinese\n\n- Rollup for AI Spring hackathon created\n\n3. Frontend:\n\n- Import of news from existing site;\n\n- Added responsive styles for landing;\n\n- Added news list / index views and responsive styles;\n\n- Content edits.\n\n4. QA:\n\n- Added smoke tests for retail demo;\n\n- Improved pigs deploy flow to cleanup after tests;\n\n- Test artifact are now saved for failed test runs in pigs demo;\n\n- Ad-hoc testing for our new site.\n\nAnd last but not least, our AI Lab in St. Petersburg, has achieved the following:\n\n\u2014 New labelling tool;\n\n \u2014 TensorFlow benchmarks;\n\n \u2014 Inference for new data with old models;\n\n \u2014 Estimating the angle of the photo with supermarket shelves;\n\n \u2014 Setting up the new mini-server for training;\n\n \u2014 Training a simple YOLO3 model with no hierarchy;\n\n \u2014 Review of platform API ;\n\n \u2014 Wrote a new NeuroNuggets article.\n\nAll our departments work hard to improve the Neuromation platform, to spread the idea of democratization of AI globally.\n\nStay updated with the latest news, follow our social media channels and subscribe to the newsletter!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/2nd-ai-expo-artificial-intelligence-exhibition-conference-tokyo-45b193610a98?source=---------9",
        "title": "2nd AI EXPO \u2014 Artificial Intelligence Exhibition & Conference Tokyo",
        "text": "OSA DC and Neuromation are presenting AI and Machine Learning integration in Retail at Japan\u2019s largest, comprehensive show for content production, creation and distribution, the 2nd AI EXPO \u2014 Artificial Intelligence Exhibition Conference!\n\nFrom elemental technologies, to hardware applications and, services, \u0410I EXPO gathers various exhibit products related to artificial Intelligence.\n\nThe conference includes top industry leaders and is also co-held.\n\nTokyo Big Sight is a remarkable landmark building that hosts thousands of conferences, and AI Expo is considered one of the most attended events of the year.\n\nImpressive stand, massive PR support in Japanese media and professional business development team to support our efforts in this booming market.\n\n\u201cWhat is Smart Consuming?\u201d\n\n\u201cHow is AI being applied to improve our world, our businesses, our lives?\u201d\n\n\u201cHow AI and Data Science can be implemented in retail industry?\u201d\n\n\u201c What is the future of AI and retail?\u201d\n\nThese and other important questions were answered in Open Discussion, Networking format at the 2nd AI Expo.\n\nTwo partner companies are working hard presenting their technology for the retail industry. Both teams introduced ongoing presentations to the public.\n\nOSA DC and Neuromation are happy to present their innovations with an impressive live demo at the largest, most comprehensive show in Japan. We are proud to be showcased among industry leaders and major players of the AI market."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/artificial-intelligence-in-japan-r-d-market-and-industry-analysis-e086a38639ec",
        "title": "Artificial Intelligence in Japan (R&D, Market and Industry Analysis)",
        "text": "Artificial intelligence (AI) was one of the strongest fields of development in 2015 and this trend will likely continue in 2016. AI is a technology that utilises machine intelligence and human-thinking ability to process various data to make predictions, recommendations and decisions.\n\nArtificial intelligence technology is a convergence of various technologies, algorithms and approaches. In 2016 and beyond, AI is expected to bring about a major shift in the perception of this technology that will become more\n\naccepted by people.\n\nLike any disruptive technology, however, AI carries some risk and presents policy challenges along several dimensions including jobs, safety and regulatory questions.\n\nOn the software side, there will be more sophisticated AI-based platforms in the future and there is a trend towards platforms becoming open-sourced. Utilisation of deep learning will require large investments in computing power\n\nthat many companies cannot afford. Therefore, open platforms are expected to impact the market positively. For high tech corporations such as Google, Facebook and IBM, this is a way to promote their brands.\n\nThe trend towards multi-party collaborations will increase as there are limits to what individual companies can research and utilise AI on their own.\n\nThe Japanese government is betting on AI as the key to rewrite Japan blueprint for the future. Prime Minister Abe has called for greater use of AI and robotics including IoT as part of the government\u2019s economic growth strategy, urging businesses to invest more into researching new technologies.\n\nSo far, many Japanese companies have been relatively slow in taking advantage of opportunities that AI presents.\n\nSome Japanese companies have sought outside partners to further their ambitions, for instance SoftBank and Toyota. Toyota is stepping up its efforts to tap AI technology, and in January 2016 established a R&D unit in Silicon Valley. The firm has realised that it has to work faster and harder to stay competitive.\n\nJapanese companies are looking to acquire start-ups in the U.S. but they do not always know what they want to do\n\nwith the start-up technology.\n\nJapan has a weak branding strategy compared with rivals such as IBM which has promoted its AI-based Watson platform for a variety of services.\n\nFor a long time, Japanese companies have developed their technology in-house to protect their intellectual property. But the age of jimae-shugi (in-house supply of its technology) is gradually coming to an end as a single technology can no longer cover everything. Today when AI will become a mainstream enterprise element, Japanese companies have to become more open-minded. AI should not be seen as a minor enabler of efficiency but should be integrated into the corporate strategy.\n\nAdvances in AI are bringing about challenges for the Japanese society. Nomura Research Institute, for instance, in a recent report predicts that nearly half of all jobs in Japan could be performed by AI-enabled robots by 2035.\n\nJapan\u2019s AI market is estimated to grow from JPY 3.7 trillion in 2015 to JPY 87 trillion by 2030. In 2015, AI solutions for the wholesale and retail sectors were valued at JPY 1.45 trillion or about 39 percent of the total market value, comprising the largest AI sub-segment.\n\nBy 2030, the transport sector (driverless taxis and trucks) is expected to grow to JPY 30.48 trillion. Including driverless cars (manufacturing sector), together these two sectors will have a market value of JPY 42.65 trillion or 49 percent of the total forecasted market value.\n\nThe utilisation of deep learning in AI has not reached mainstream in Japan. Therefore, this sub-segment will offer potential for European companies as AI and robotics are prioritised in the government\u2019s growth strategy.\n\nThe automobile industry in Japan will need deep-learning solutions to develop driverless cars. This constitutes another potential area for European companies that are able to offer attractive solutions for self-driving cars. A recent trend is that automakers like Toyota and Honda are lacking specialised personnel to develop the necessary technology for driverless cars. This further stresses the chances for Europe in this field.\n\nApart from the AI and robotics sector as well as automated driving technology, the study has identified application areas such as solutions related to marketing, information services and security that could offer additional opportunities for European companies.\n\nIn marketing, AI-applied solutions provided by Japanese companies are related to data analysis, automated marketing and image recognition. These areas as well as other areas where European have capabilities could constitute additional business opportunities.\n\nBefore considering entering the Japanese market, it is important to study the Japanese market in detail and be prepared to offer an extensive after-sales service system as Japanese customers are very demanding. Equally important is to have the full understanding of top management in Europe as relationships in Japan are long-term.\n\nChoosing the right mode for market entry is a crucial issue as well as adapting products and solutions to the needs of the Japanese market.\n\nThis report is the result of extensive secondary research into the current status of artificial intelligence in Japan. It presents an industry overview and provides insights into the market trends, market size, market drivers and challenges that affect the growth of the market. Information on key players and the market size as well as new product developments including R&D activities will also be covered.Secondary sources referred to for this study include magazines, journals, company financials, press releases, databases, annual reports, company websites as well as government sources.\n\nThe term artificial intelligence is difficult to define and has been debated for many years. In this report, artificial intelligence refers to the development of computer software capable of performing tasks that normally require human intelligence (mimic human behaviour). The AI technology can be divided into four broad categories: Machine learning (ML), natural language processing (NLP), image processing, and speech processing. In recent years, deep learning (DL) has gained worldwide attention and is a subfield within machine learning.\n\nFY \u2014 Fiscal year (from April to March the following year)\n\nThis section provides some brief information on the global artificial intelligence market.\n\nIn 2014, the overall global investments in AI were more than USD 1.9 billion, an increase by more than 50 percent over 2013. In 2015, the investments are estimated to be roughly USD 2.7 billion or 5 percent of total VC investments that totalled USD 55 billion in 2015.\n\nAI-driven technologies are predicted to be the next disruption to the enterprise software. Currently, the penetration of AI has hit almost every industry sector.\n\nArtificial intelligence is one of the current buzzwords. Originally, the term was coined by John McCarthy in 1956.\n\nThere have been several \u201cboom-gloom\u201d cycles since the 1950s when the first boom occurred. The second boom followed in the 1980s. The current third AI boom started early in the 2010s when U.S. companies such as Google and Facebook established AI research laboratories. Advances in the development of deep learning technologies have reignited broad interest in AI research.\n\nA different paradigm has emerged. Instead of trying to program computers to act intelligently, AI today is able to analyse large amounts of data with powerful computers and sophisticated algorithms, and to some extent learn patterns by themselves.\n\nAI is not new but the underlying technologies have reached an inflection point. The disruptive power of AI is forcing hardware manufacturers such as automobile manufacturers to follow the direction of AI companies as the software is going to play a major role in the future.\n\nThere are many estimates concerning the size of the artificial intelligence market. According to Bank of America Merrill Lynch, the global AI solutions market will grow to USD 70 billion by 2020 from USD 8.2 billion in 2013.\n\nThe global AI market is expected to increasingly be impacted by growing government funding and strong technological base.\n\nThe U.S. dominates the global AI market through its leadership in machine learning. The machine learning technology is the base for artificial intelligence and is used across all major application areas. In 2015, machine learning was the largest segment of AI.\n\nOther major markets are Japan, Europe and China. Recently, China presented a three-year program for artificial intelligence growth. The aim is to create a market worth more than 100 billion yuan (USD 15.2 billion) by 2018, when the country shall be in line with global AI technology and AI industries.\n\nThe integration of AI in many application sectors such as retail, finance and healthcare, just to mention a few areas, is expected to strongly push the growth of the market in the years ahead, including Japan.\n\nDeep learning, which attempts to replicate the workings of the layers of neurons in the human brain, is a breakthrough in the 50-year history of AI. Tractica, a U.S. market-research firm, forecasts that annual software for enterprise applications of deep learning will reach USD 11.1 billion in 2024, up from USD 202 million in 2015.\n\nMarket estimates for some AI sub-segments are presented below.\n\nAccording to Frost & Sullivan, healthcare AI revenues are estimated to reach USD 6.66 billion in 2021, up from USD 633.8 million in 2014.\n\nStrategy Analytics, a U.S. research and consulting firm, forecasts that autonomous driver-assisted systems will grow from 5 billion Euro in 2012 to 16 billion Euro by 2019.\n\nConcerning natural language processing (NLP), ReportsnReports estimates that the global NLP market will grow from USD 3.8 billion in 2013 to USD 9.9 billion in 2018.\n\nOne characteristic of AI technologies and related applications is that systems tend to be highly customized for each environment and process.\n\nGrowing availability to low-cost quality AI technologies will likely see many new start-ups entering the AI field.\n\nTo stay competitive and be a step ahead of competitors, companies will continue to search for experienced talent. And universities will adjust their AI curriculums to help produce that talent.\n\nThe trend that many tech companies are investing in open-source AI platforms will continue and contribute to the growth of the market.\n\nThe capability to utilise more data will enable companies to achieve higher efficiencies in their value chains.\n\nThere is a need for international standards. Japan proposed setting up a set of basic rules for developing AI at the G7 tech meeting in Japan in April 2016. One proposed rule would be to make AI networks controllable by human beings.\n\nIncreasing overall global investment in AI is pushing the global AI market. Google and Facebook, for instance, have been busy shopping for AI ventures in the past years.\n\nThe development of more powerful and affordable cloud computing infrastructures is having a strong impact on the growth potential of AI. Major reasons for recent advances in AI are the availability of large amounts of data online with which to train systems.\n\nMachine-learning technologies constitute a major driver of the market that has contributed to corporate productivity increases.\n\nOther drivers are diversifying application areas of AI as well as increased level of customer satisfaction by users of AI products and solutions. In its report \u201cArtificial Intelligence Market \u2014 Global Forecast to 2020\u201d, MarketsandMarkets refers to machine learning technology that will allow financial analysts to draw conclusions for stock reactions in coordination with market movement and that this will enhance customer satisfaction as the AI-based application will eliminate the need of scrolling through huge data to arrive at a conclusion.\n\nThe paradigm shift towards AI will bring about challenges across multiple sectors of the economy.\n\nA study by Gartner, emphasises some negative aspects of artificial intelligence. The study reveals that millions of jobs will be replaced by AI-based technology systems.\n\nResearchers at University of Oxford has published a study estimating that 47 percent of total U.S. employment is \u201cat risk\u201d due to the impact of AI advances.\n\nLack of trained and experienced staff will likely have a negative impact on the growth of the AI market. In the short term, the global \u201chiring war\u201d for talent will continue.\n\nAnother challenge is the need to create relevant business models.\n\nOver the past few years, there has been growing concern about the potential for AI. For instance, that AI could eventually pose a threat to humans. Elon Musk, CEO of Tesla Motors, is cautioning against giving AI complete autonomy that could threat the humanity.\n\nOthers emphasise that the exponential growth of AI one day would lead to a technological singularity, a point when machine intelligence will overpower human intelligence and lead humans into an unknown world of no return.\n\nThere are about 900 companies working in the AI field, mainly in the U.S. and Western Europe.\n\nMajor corporations such as Google, IBM and Facebook have embraced AI to differentiate themselves from competitors. Other global players include Microsoft and Apple. IBM is introduced in section 9.2 that gives information about foreign player in the Japanese market.\n\nGoogle is an American multinational technology company specialising in Internet-related services and products. It was founded in 1998 and has 57,000 employees worldwide. The sales were USD 74.5 billion in 2015. Recently, Google announced that it will move from a mobile first to an AI-world first [16]. In March 2016, Google\u2019s AlphaGo computer program defeated one of the world\u2019s top go players that draw worldwide attention. In May 2016, Google and Fiat Chrysler Automobiles launched a joint project to develop experimental vehicles. By 2020, Google is aiming to commercialise self-driving cars that utilise AI.\n\nFacebook is a social networking service launched in 2004 by Mark Zuckerberg and four of his Harvard College roommates. Facebook has more than 1.65 billion monthly active users as of March 31, 2016. The revenues were USD 17.2 billion in 2015 and the number of employees is 12,600. Facebook is looking to simplify repeatable tasks for businesses with new APIs that enable AI to be built into Messenger, a mobile tool that allows users to instantly send chat messages. With more than 900 million people using this app at least once a month, it is one of the fastest growing products in the world. Facebook has open-sourced its deep learning code to anybody who wants to use it. Facebook hopes that developers will speed up the AI behind its computing framework.\n\nMicrosoft is an American multinational technology company headquartered in Redmond, outside Seattle. The company develops, manufactures, licenses and sells computer software. Microsoft was founded by Bill Gates and Paul Allen in 1975. The sales in 2015 were USD 93.6 billion and it employs 118,000 people. Microsoft Azure is an AI-based cloud computing platform that provides cloud services including those for analytics, storage and networking. Microsoft is currently working on the Bing Concierge Bot that will be able to respond to users over platforms like Skype and Messenger. The bot runs errands on behalf of the user, by automatically completing tasks for the user. In 2015, Microsoft added machine learning to its cloud software platform\n\nApple is an American multinational technology company headquartered in Cupertino, California. The company designs, develops and sells consumer electronics, computer software and online services. The sales in 2015 were USD 233.7 billion and the number of employees is 115,000. Recently (June 2016), Apple announced that it is opening up many applications to outside developers, including its messaging platform iMessage and virtual assistant Siri. This is a departure for Apple that in the past has kept these systems tightly controlled. Siri will be coming to desktop computers and soon-to-be released desktop and mobile operating systems will be powered by artificial intelligence.\n\nThe current and third AI boom started around 2012.\n\nDuring the second AI boom in the 1980s, expert systems that emulate the decision ability of human beings were further developed. Such a system consists of a knowledge base and an inference engine (if-then rules: if A then B). It is a program that answers questions about a specific domain of knowledge using logical rules that are derived from the knowledge of experts.\n\nIn 1982, the Japanese Ministry of International Trade and Industry (MITI) invested USD 446 million for the fifth generation computer project to advance the field of AI including development of expert systems. The objectives were to build machines that could translate languages, interpret pictures, and reason like human beings.\n\nBut the project failed to come up with a \u201ckiller\u201d app and not much was ever commercialised. The project came to an end in 1992. Computational limitations and lack of supporting data were factors that can explain this failure.\n\nMany analysts have pointed out that the Japanese AI industry today is not very competitive on the global scale. For instance, when it comes to research papers on AI between 2008 and 2013, most of these come from Western countries and China. Only about 2 percent come from Japan.\n\nSeveral headlines in recent articles have brought to the attention the current status of the Japanese AI industry. One headline example is \u201cJapan must promote AI without restrictions\u201d. Other articles emphasise the need for an \u201cAI revolution\u201d as well as asking the question \u201cIs revival possible with AI\u201d.\n\nIn contrast to this, Professor Yutaka Matsuo, University of Tokyo, claims that AI is one of the few limited areas that could offer business opportunities for Japan\u2019s industries [21]. But for the Japanese monozukuri (Japanese-style manufacturing processes) to get an upper hand over competition, Japanese companies have to embrace deep learning more intensively and not only focus on IoT.\n\nThe Japanese government has recently announced that it will set up an AI panel with the aim to design a road map for development and commercialisation of AI by the end of FY 2016.\n\nSeveral venture capital funds have recently been set up in Japan with focus on AI. Realtech Fund, targeting technology start-ups, is one example into which Japanese companies have invested money. This fund is one of the largest venture capital funds in Japan involving only private-sector companies. Investments are made in 10 fields including AI. The plan is to invest in 40 technology start-ups by 2020.\n\nAccording to Recof, a mergers and acquisitions consultancy, investments in domestic businesses by corporate venture funds increased by 430 percent in 2015 over 2014.\n\nDuring the last 10\u201320 years, Japan has lost its technology leadership to companies in the west, and largely because of software shortcomings. Japan is still at the forefront in hardware such as robots. But this stronghold is in danger of being lost, because software is increasingly critical to making those products work and to compete on the international market.\n\nWhen it comes to deep learning, there is a clear difference between Japan and the U.S. In Silicon Valley, deep learning is mainly a way to make software better. Many Japanese companies, however, tend to look at deep learning differently, as a way to just improve the hardware.\n\nIn 2015, the government presented the Japan Revitalization Strategy. The new growth strategy is based on AI and robotics. For a long time, Japanese companies have been focusing on robot manufacturing itself (hardware) while overseas companies have focused more on the software side. Japanese manufacturers now need to implement a more customer-oriented perspective to develop robots that more match the users\u2019 needs. The aim of the government is to trigger a robotics revolution with AI-equipped robots that can communicate with each other.\n\nIn May 2016, a Cabinet Office Council on industrial competiveness announced that the introduction of self-driving cars, drones and technologically enhanced production management including smart factories is expected to raise Japan\u2019s productivity. The goal is to increase the GDP to JPY 600 trillion by 2020 from the level of about JPY 500 trillion in 2015. A sub-goal is to create a JPY 30 trillion market for new emerging technologies.\n\nA report by a separate council on regulatory reform is recommending 80 changes in fields such as medicine, employment and investment (May 2016).\n\nAI and robotics will be a key to enhance the productivity of the service industries, for example, health care and longterm care services.\n\nAnother development that over time will cause the AI market to expand is the trend that Japanese corporations are setting up AI R&D bases in the U.S., for instance Toyota and Hitachi [26]. In addition to applications in self-driving cars, Toyota is planning to use AI to improve people\u2019s lives (via robots). By being exposed to new ways to incorporate AI into products and platforms, the domestic AI market will likely be positively impacted as a result of this.\n\nR&D centers at public Japanese research institutes such as AIST and RIKEN have also been established and this is expected to speed up new technological advancements within the AI field.\n\nThe shrinking population will start taking a toll on the labour market as the population continues to grey. Statistics show that people aged 65 and older accounted for 26 percent of the population in 2015. Japan plans to make up for the shortfall in the working population with wide use of AI-powered robots.\n\nArtificial intelligence has started to gain attention in Japan beyond research institutes and corporations already active in this field. Media coverage is expanding with frequent articles appearing in economic newspapers such as Nikkei Shimbun. Special editions of economic journals are also contributing to make AI more known among people in Japan.\n\nSmall and mid-sized companies have also established AI research labs in recent years such as Recruit Holdings (2015) and Dwango (2014).\n\nHonda recently announced (June 2016) that it will establish a R&D base in Tokyo specialising in artificial intelligence. The firm will consolidate most of its AI-related R&D activities in Japan into this base called Honda R&D Innovation Lab Tokyo.\n\nOther corporations are expected to follow this trend to set up own AI R&D bases.\n\nThe merger of AI applications and consumer products are expected to increase in the future. For instance, in May 2016, Sharp launched an AI robot and smartphone in a single package. This product, named RoBoHoN incorporates AI and IoT technology.\n\nIn June 2015, Japan launched the Industrial Value Chain Initiative with 30 companies to develop communications for linking factories and facilities [30]. The power of connecting networks is expected to get more attention in Japan and will also serve as a way to cope with similar developments in Europe, in particular in Germany that in 2013 introduced \u201cIndustry 4.0\u201d to promote the digitization of manufacturing.\n\nAI is also changing the way business is done. In 2015, a Japanese venture capital firm became the first company in history to nominate an AI board member for its ability to predict market trends faster than humans.\n\nApart from growth drivers, the Japanese AI market is also characterised with major challenges.\n\nNomura Research Institute has tried to quantify the potential impact of artificial intelligence on the job market and has indicated that there is a possibility that about half of Japan\u2019s labour force may be replaced with robots or artificial intelligence within the next 10\u201320 years. In particular, the impact on jobs in the service sector will be strong.\n\nIt will be important to orient the AI boom in the right direction to achieve the full value of the possibilities of AI. In this respect, it will be important to closely monitor the development overseas.\n\nThere are many considerations in privacy, security, regulations and law to be taken into account when integrating AI technology into private-sector activities.\n\nThe need to develop new business models to cope with the new realities of emerging AI technologies is also an important issue.\n\nCurrent regulatory frameworks were not designed with AI in mind. This is valid not only for Japan but also for other AI markets.\n\nAt the G-7 tech meeting in Japan in April this year, Japan proposed establishing a number of basic rules on R&D of AI. The wish of the government is to develop international rules that AI developers will have to comply with.\n\nAt other occasions, the Japanese government has emphasised that legislative changes are necessary in Japan to address technological advances in artificial intelligence. Today, AI-related algorithms have become quite sophisticated, capable of self-learning, and are largely different from \u201cclassical\u201d algorithms of the first and second AI boom.\n\nWhat has been discussed so far is related to Japan\u2019s copyright law. This law covers only works that have been produced in a creative way and, therefore, do not apply to works produced via AI.\n\nIn contrast to Japan, current legislation in the U.K. has amply leeway to address ownership of AI-made works.\n\nJapan also has to address the imminent arrival of driverless cars. Relevant road transport laws and regulations have to be revised including accident liability issues (who to blame when AI systems go wrong).\n\nThe Legal Society of Robotics stresses the importance to regulate the use of smart robots.\n\nThe development of comprehensive safety standards including safety certification processes is equally important.\n\nAI applications may infringe on current personal data regulations that have to be addressed as well.\n\nAccording to the Japanese government, AI technologies are expected to generate an economic return of about JPY 121 trillion by 2045.\n\nAccording to a study by Ernst & Young Institute, a Japanese think tank, the size of the AI market is estimated to grow from approximately JPY 3.7 trillion in 2015 to JPY 23 trillion in 2020, a six-fold increase. And by 2030, the market size will reach about JPY 87 trillion.\n\nThe transport sector including driverless taxis and trucks will show the largest increase over the forecast period and is estimated to reach JPY 30.5 trillion by 2030.\n\nThe manufacturing sector that includes self-driving automobiles is predicted to grow to approximately JPY 12.2 trillion by 2030.\n\nThe study gives examples of major AI-related factors that will impact the market: improved cost efficiency, further development of a driverless society and wide use within the manufacturing industry.\n\nOrganisations in many sectors of the economy are already using AI technologies/solutions in diverse business functions.\n\nIn banking speech recognition technology is used to automate customer service telephone interactions. Mizuho Bank has started to use a combination of IBM\u2019s Watson AI platform and Softbank\u2019s Pepper humanoid robot to provide customer support.\n\nThe Mitsubishi UFJ Financial Group has begun a service utilising IBM\u2019s Watson. Customers can contact the bank using the Line online chat app.\n\nThe public sector has adopted AI technologies for a variety of purposes. West Japan Railway, for instance, is using AI to detect signs of intoxication in passengers at train stations.\n\nIn media & advertising a number of companies are utilising data analytics technology to automatically draft articles such as corporate earnings summaries.\n\nCyberAgent, a Tokyo-based leader in AI-empowered Internet ad business, supports advertising and promotion activities to clients in Japan.\n\nSeveral Technology companies use AI technologies to enhance products or create new product categories.\n\nIn the recruitment sector, companies use people analytics to help clients find the best candidate for a position. One example is Forum Engineering, a large Tokyo-based temp agency for engineers.\n\nAdditional business sectors using AI solutions are the automobile industry, retail, healthcare and manufacturing.\n\nMajor corporations like Fujitsu are getting more open to show their latest AI research. This is necessary to get potential clients to understand what it is all about. The firm is consulting with clients early on to apply their AI technologies to businesses.\n\nThis section looks at the market landscape surrounding the AI industry in Japan. Examples of some of the applications are briefly described in the section that deals with the competitive landscape (section 8). Potential benefits of AI technologies include faster decisions, better outcomes (for example, medical diagnosis), higher efficiency (better use of skilled people, facilitating complex decision making) and lower costs (automated call centers).\n\nIn recent years, R&D related to AI is picking up momentum in Japan. AI teaching programs at universities are being created. At University of Tokyo, for instance, Professor Yutaka Matsuo has received extensive support from companies.\n\nSome examples of research at universities are listed below.\n\nUniversity of Tokyo. Professor Yutaka Matsuo is conducting research in artificial intelligence to realise breakthroughs in deep learning.\n\nThe Institute of Medical Science, University of Tokyo, under the guidance of Professor Satoru Miyano, is conducting a study to find optimum cancer drug combinations for each patient. IBM\u2019s AI-based Watson cognitive computer system is used.\n\nAt Keio University, a group led by Professor Tadahiro Kuroda has been able to develop an AI that can detect lung cancer in urine with about 90 percent accuracy.\n\nA team from Future University, Nagoya University and the Tokyo Institute of Technology, under guidance of professor Hitoshi Matsubara (Future University) is engaged in research to make AI create a story plot and finally write an entire novel.\n\nUniversity of Tsukuba. Professor Masakazu Hirokawa at the AI laboratory is working on creating algorithms that can help robots learn. His ambition is to develop the software to enable a robot to adapt itself to each user.\n\nMany research centers have recently been established at public research institutes.\n\nArtificial Intelligence Research Center, AIRC. This center was established in May, 2015, at National Institute of Advanced Industrial Science and Technology (AIST). The aim of the research center is to develop AI technologies that will offer applications to self-driving cars, medical services and financial services including robotics. Junichi Tsujii is the director.\n\nMinistry of Education, Culture, Sports, Science and Technology (MEXT) set up a strategic Center for AI Development at RIKEN (Institute of Physical and Chemical Research) in April 2016. The center will be related to RIKEN\u2019s AIP project (Advanced Integrated Intelligence Platform) and also cover big data and IoT. In September 2016, the center will launch a research hub near Tokyo station.\n\nThe government plans to promote the development of AI technologies in cooperation with other entities. The public-private initiative will involve 20 companies and research institutes, and researchers will be able to gather at the hub near Tokyo Station.\n\nThe aim is to develop practically applicable artificial intelligence for the medical and financial fields within 10 years. Collaborative research will be conducted in cooperation with national institutes under Ministry of Economy, Trade and Industry (METI) and Ministry of Internal Affairs and Communications (ICT).\n\nNational Institute of Informatics. Professor Noriko Arai is developing an AI system that she hopes can pass the entrance exam to the University of Tokyo.\n\nSome examples of collaborations between companies and universities/research institutes are listed below.\n\nHitachi in a joint project with researchers at Tohoku University has developed AI-based software capable of backing up its decisions with facts, background and context.\n\nOn June 1, 2016, NEC and AIST jointly established an AI lab. The aim of the three-year collaboration is to make up for AI\u2019s shortcomings. AI is not much helpful to uncover solutions to problems with only limited data, such as predicting rare events such as major disasters.\n\nThe researchers intend to use AIST\u2019s advanced simulation technology to generate data on different situations. They hope to develop AI that can use this additional data to find solutions out of reach so far.\n\nKeio University and Ubic Medical are working on developing a device that enables real-time objective assessment of mental symptoms by quantifying facial expressions.\n\nVideo Website operator Dwango will work with partners including University of Tokyo to conduct research on AI capable of making autonomous decisions.\n\nCyberAgent, an online advertising agency, and Meiji University are developing an AI-based system to automatically generate ads.\n\nHitachi is carrying out research to use AI to detect signs of diseases, including unknown warning signs. The firm has started a pilot program analysing about 150 pieces of medical data and believes that AI can play a major role in reducing medical costs.\n\nInterprotein, a company strong in molecular designing technologies, and A.I. Squared, an artificial intelligence-based solution provider, are jointly conducting research on AI drug discovery.\n\nThe competitive landscape consists of large players, small to mid-sized companies and start-ups. Many companies are well-positioned to benefit from the AI boom.\n\nNEC is a leader in the integration of IT and network technologies headquartered in Tokyo. NEC\u2019s electronic devices business includes semiconductors, displays and other electronic components. Consolidated sales in FY 2015 were JPY 2.8 trillion. The total number of employees is 99,000. NEC has been active in AI research since the 1980s. The company will intensify R&D of artificial intelligence technology and the commercialisation of solutions, and is planning to increase staff working with AI to 1,000 by 2020. NEC\u2019s solutions incorporating AI-related technologies are divided into 4 groups. Public safety solutions include urban surveillance systems and crowd behaviour analysis. Landslide prediction solution and plant failure sign detection system are examples of infrastructure/plant management solutions.Human resource matching is a marketing solution and quality analysis is an example of solutions for improvement of operational efficiency.\n\nFujitsu is a leading ICT company offering a full range of technology products, solutions and services. The total number of employees is 159,000. Fujitsu has been active in the AI field since the 1980s. The sales in FY 2015 were JPY 4.7 trillion. Through the AI Application Consulting Department, Fujitsu is providing consulting services that utilise artificial intelligence. AI-specialist consultants work with clients proposing AI-enhanced solutions that will bring about business transformation and innovation. Fujitsu has developed software that identifies the emotions on people\u2019s faces.\n\nToshiba is a multinational conglomerate corporation headquartered in Tokyo. Its products include information technology and communications equipment, electronic components, power systems, consumer electronics and household appliances. The number of employees is 199,000. The sales in FY 2015 were JPY 6.6 trillion. Toshiba has developed a new analytics system based on AI for use at its mainstay flash memory plant in Japan. AI is being used to monitor the semiconductor yield rate and will automatically classify defects and detect causes as well as analyse incident trends. Toshiba has also developed a dialogue-based virtual assistant specialised for inheritance advice.\n\nHitachi is a multinational conglomerate company headquartered in Tokyo. Hitachi is a highly diversified company that operates eleven business segments including high functional materials & components, financial services, power systems, electronic systems & equipment, railway systems and construction machinery. It employs 336,000 people. Total consolidated sales were JPY 9.7 trillion in FY 2015. Hitachi has developed an AI program that enables robots to deliver instructions to employees based on analyses of big data and working routines. A logistics work efficiency improvement rate of 8 percent has been achieved [54]. Its core machine-learning technology called \u201cH\u201d is designed to help clients boost their sales, cut costs and improve employee satisfaction. While earlier AI systems and software had to be customised for various uses, \u201cH\u201d can learn on its own to achieve different outcomes. Recently, Hitachi teamed up with Kyoto University to research AI technologies. One theme will involve developing AI to ease traffic congestions, based on the mechanism by which fish cooperate to move in unison. In FY 2016, Hitachi plans to triple development spending in areas such as AI, sensors and robotics. Hitachi aims to add 100 new AI experts by March 2017 at its research center in California. This will bring the total number of the center\u2019s staff to about 200 including 100 researchers sent over from Japan.\n\nMitsubishi Electric is a world leader in the manufacture of electrical and electronic equipment used in information processing and communications, consumer electronics, energy, transportation and building equipment. The total number of employees is 121,000. Total sales in FY 2015 were JPY 4 trillion. The company has developed a \u201ccompact AI\u201d technology that can work without the use of large servers. The compact AI is expected to be implemented in embedded systems applications in vehicles (autonomous driving systems) and robots starting from 2017. Mitsubishi Electric will put elements of AI to work in driver-assisted systems in 2017. The systems will monitor the driver\u2019s face and heartbeat, as well as steering wheel movements in order to warn of sleepiness with the aim to prevent car accidents. The company has also developed an AI-based system that optimises elevator management. Mitsubishi Electric emphasises that AI is merely a way to add value to devices functioning as a \u201ckey supporting player\u201d.\n\nSharp is a multinational company that designs and manufactures electronic products. Since 2016, Sharp is an integral part of Taiwan-based Foxconn Group. Sharp employs more than 50,000 employees worldwide. Consolidated sales in 2014 were JPY 2.9 trillion. Sharp has recently launched a walking, talking robot smartphone (RoBoHoN) utilising AI.\n\nSony is a multinational conglomerate corporation headquartered in Tokyo. Its diversified business includes consumer products and electronics, gaming, entertainment, and financial services. Consolidated sales in FY 2015 were JPY 8.1 trillion and it employs 131, 700. Sony recently (May 2016) announced a tie-up with U.S. Cogitai, a start-up specialised in AI. The partnership will work on technology by which a camera can suggest new options of photo-shooting after learning the user\u2019s preferences (photo-taking patterns).\n\nSony is currently shifting its focus from \u201cmere survival\u201d to innovation. Once deep in the red, its bottom line is now back in the black. In April 2016, Sony established a new department devoted to mid- and long-term business development. It will focus on AI and robotics and one aim is to develop AI platforms that can function as a cutting-edge foundation for its products and services.\n\nThe Nippon Telegraph and Telephone Corporation, commonly known as NTT, is a Japanese telecommunications company headquartered in Tokyo. The NTT Group consists of five main group companies: NTT East, NTT West, NTT Communications, NTT DoCoMo and NTT Data. The group employs more than 240,000 people. NTT subsidiary companies work on AI products and services by utilising the research results available from NTT Research. In October 2015, NTT Data established the AI Solutions Promotion Office. With its information application knowhow platform, NTT Data will support front and middle office assistants in communications with consumers and the public sector. NTT Communications uses an AI algorithm to protect more than 4,000 corporate customers from cyberattacks.\n\nSoftBank Group is a multinational telecommunications corporation with operations in broadband, Internet and technology services. The sales in FY 2015 were JPY 8.7 trillion. SoftBank employs 69,000. SoftBank and Boston-based Cybereason have formed a joint venture (April 2016) in Japan to provide a cyber-security platform that utilises AI. SoftBank has entered an alliance with IBM to introduce IBM\u2019s AI Watson system in Japan. The two organizations are targeting Japan\u2019s education, banking, healthcare, insurance and retail industries to provide clients with more relevant information on products and services, and to improve overall decision making by analysing diverse, high volume data streams. SoftBank has entered a partnership with AKA LLC, a key innovator in AI social robot innovation, in order to facilitate the development of AKA\u2019s Musio, the world\u2019s first artificially intelligent robot friend.\n\nToyota is an automotive manufacturer headquartered in Toyota located east of Nagoya. Toyota is the world\u2019s first automobile manufacturer to produce more than 10 million vehicles per year. The number of employees is 338,000 and the consolidated sales in FY 2015 were JPY 27.2 trillion. In 2015, Toyota has announced that the firm will spend USD 1 billion over the next 5 years in a joint AI research project with Preferred Networks related to technology to connect vehicles. In order to stay competitive and tap artificial technology, Toyota established a research unit in the beginning of 2016 in the U.S. Toyota\u2019s ambition is to be at the forefront of autopilot technology, given that the future is driverless cars. In August 2016, it was announced that Toyota will join a public-private initiative to develop a core AI technology for applications in fields such as manufacturing and infrastructure management. Toyota and NEC will work with RIKEN to develop systems that can detect signs of impending production machinery failures utilising sensor data. Toyota, Panasonic and other companies recently contributed about JPY 900 million to Tokyo University to endow positions in artificial intelligence.\n\nFanuc is a Japanese multinational corporation and one of the world\u2019s leading manufacturers of robotics and factory automation. The company had its beginnings as part of Fujitsu developing early numerical control systems. The sales in FY 2015 were USD 6.1 billion. Fanuc has 5,300 employees worldwide. Fanuc is working with Preferred Networks, a leading Japanese provider of artificial intelligence solutions, to develop the FIELD system platform that connects robots, devices and sensors to optimise manufacturing production through analytics.\n\nPreferred Networks was founded in March, 2014, as a spinoff from Preferred Infrastructure, specialised in artificial intelligence, particularly in deep learning. The firm is headquartered in Tokyo and employs 30 persons. Preferred Networks has teamed up with Fanuc, one of the largest makers of industrial robots in the world, which has invested JPY 700 million in the company to develop smarter robots. Preferred Networks has also teamed up with Toyota (self-driving cars), Panasonic (research on automotive and audio visual products) and Cisco Systems. In October 2015, the firm launched an operation system for deep learning technology called chainer that helps engineers write AI-enabled programs. In July 2016, Preferred Networks established a company, PFDeNA, to develop AI technology together with smartphone game provider DeNA that is known for its partnership with Nintendo. In the future, Nintendo\u2019s smartphone games might be powered by AI technology.\n\nUbic, headquartered in Tokyo, was founded in 2003 as a provider of international litigation services. Sales in FY 2015 were JPY 10.6 billion. Recently, Ubic has obtained patent in Japan for semantic analysis technology using its proprietary AI engine, KIBIT, that is covering three fields: digital marketing, healthcare and business intelligence. In the U.S., the patent authorities have declared that Ubic\u2019s analysis technology meets the requirements for patentability. Ubic has developed a small robot called KIBIRO. Equipped with the AI engine KIBIT, the robot can recommend new books based on what people have read in the past and other preferences. Ubic Medical, a wholly-owned subsidiary founded in April 2015, has developed an AI-based system to mitigate patients\u2019 risks of falling. And it also has a system to detect mental disorders.\n\nCanon is a multinational corporation headquartered in Tokyo. The firm manufactures imaging and optical products including cameras, camcorders, computer printers and medical equipment. The consolidated sales in FY 2015 were JPY 3.8 trillion. The number of employees is 192,000. Canon is planning to completely automate domestic manufacturing of digital cameras by 2018 by using AI robots. This will serve as a new paradigm for domestic factories that are facing the problems of a shrinking labour pool and rising personnel costs.\n\nZen Robotics is a Finish company and a leading supplier of an AI-based robotic waste separation technology headquartered in Helsinki. In Japan, Zen Robotics has entered into a distribution deal with Sun Earth, a leading domestic technology supplier to the Japanese waste management industry.\n\nIBM is an American multinational technology and consulting corporation headquartered in New York. The consolidated sales in 2015 were USD 81.7 billion and it employs 377,000 people worldwide. IBM\u2019s supercomputer platform Watson is an AI-enabled technology platform that uses natural language and machine learning to detect patterns from large amounts of unstructured data. IBM\u2019s Watson is being utilised in Local Motors electric self-driving minibus that can be built by 3-D printer as an ondemand transportation solution. Recently (June 2016), this minibus was presented in Maryland in the U.S.\n\nIn 2015, IBM formed an alliance with SoftBank to introduce IBM Watson to the Japanese market. In February 2016, a Japanese language capability version was released. The goal of the collaboration is to launch new applications in Japan powered by the Watson platform. In August 2016, as a possible first in Japan, it was reported that doctors have used Watson Healthcare Platform to diagnose a rare type of leukaemia. The platform was able to carry out the diagnosis in 10 minutes, while it would have taken about 2 weeks for doctors to arrive at the same conclusion. This was possible because the platform was able to analyse data from millions of research papers through a cloud-based AI-powered computer system.\n\nYouAppi is an Israeli pioneer in AI-based data-driven mobile customer acquisition, combining the power of machine learning with proprietary predictive algorithms. In Japan, the firm is working with Bandai Namco. Its OneRun Platform, a marketing tool, is being used by customers in many countries.\n\nThe Japanese Society for Artificial Intelligence (JSAI). The Japanese Society for Artificial Intelligence (JSAI) was established in 1986 and currently has about 3,200 members. The purpose of the society is to contribute to social innovation in Japan with research and development of artificial intelligence. JSAI organises annual conferences, joint workshops and international symposiums as well seminars on different AI fields.\n\nRecently, JSAI has presented a draft ethics guideline for AI researchers, which touches on how research in the AI field should be carried out.\n\nWhole Brain Architecture Initiative (WBAI). Whole Brain Architecture Initiative (WBAI) was established as an NPO in 2015 to support the development of AGI (artificial general intelligence) through open R&D communities. The WBA approach is to learn from the architecture of the entire brain with the aim to make artificial general intelligence (AGI) to surpass the human brain capability around the year 2030. WBAI is working to foster human resources for artificial intelligence, as well as develop base software technologies with high public value. Hiroshi Yamakawa, director of Dwango AI Laboratory, is the chairman.\n\nThe Japan Robot Association (JARA). In 1972, the Japan Industrial Association was formed. The current name, Japan Robot Association (JARA), was adopted in 1994. The Association aims to further the development of the robot manufacturing industry by encouraging research and development on robots and associated system products and promoting the use of robot technology. The number of regular members is 32 with 110 corporate supporting members. Recently, AI technology is being integrated into robotics drawing wide attention among member companies.\n\nLegal Society of Robotics. Thirty members who are legal scholars, robotics engineers and bureaucrats are working to establish a Legal Society of Robotics. The group is led by Ryota Akasaka, a researcher at Keio University. The forthcoming society will promote discussion among law scholars, robot developers and representatives from various fields to agree on the roles and responsibilities of robots in society.\n\nIEA/AIE (Industrial and Engineering Applications of Artificial Intelligence and Expert Systems) 2016 is an International Conference on Industrial, Engineering & Other Applications of Applied Intelligent Systems that will be held in Morioka, August 2\u20134.\n\nCONTENT TOKYO/AI World 2016. CONTENT TOKYO is a group of 6 exhibitions, specialising in the areas of entertainment content creation, distribution, licensing, technology, service and marketing. This event will take place between June 29 and July 1 in Tokyo. At a special exhibition area, AI World will present cutting-edge AI technologies related to content production, data analysis and data prediction.\n\nJapan Robot Week 2016 will take place in Tokyo between October 19 and 21. This exhibition is especially focusing on service robots and related technologies/components.\n\nROBOTEX 2017: Robot Development & Application Expo. ROBOTEX 2017 will take place between April 18 and 20. The expo will cover industrial/service robots and development technology including IT and AI.\n\nSome of the success factors presented in section 13.1 above could constitute challenges for European companies when entering the Japanese market.\n\nIn Japan, artificial intelligence has entered a stage with growing public awareness and gradual uptake by enterprises. In comparison to the U.S. market, the number of products and solutions are still limited. One can expect domestic companies working in the AI field to be more open to listen to what European/foreign companies have to offer, compared to traditional and mature business sectors that are competing upfront with overseas competitors. The process to get in contact with potential Japanese companies may therefore be shortened. Then, what can European companies offer? Potential opportunities are listed below divided into AI technologies and AI solutions.\n\nRecent articles in Japanese newspapers (June 2016) indicate that automakers such as Toyota and Honda are \u201cdesperately\u201d seeking personnel specialised in self-driving technology. So far, Japanese automakers have mainly developed technologies to support drivers. But now they have to develop new technologies that can offer opportunities to European companies with capabilities related to automated driving technology.\n\nIn case of AI technologies, other opportunities for European companies could be collaboration/joint development of new, wider AI frameworks with Japanese universities.\n\nThis is an AI segment that is expanding in Japan. The approach, however, would be different from the AI technology segment. The main entry modes would be JV/collaboration with domestic companies or own subsidiary.\n\nThe current status of Japan\u2019s automobile industry can create business opportunities for European companies that can offer AI-based solutions applicable to self-driving technologies.\n\nAmong 35 listed Japanese companies that are considered strong in artificial intelligence, 8 companies are offering AI solutions related to marketing. Some of the application areas offered is data analysis, automated marketing, image recognition and analysis of iPhone users\u2019 behaviors. This could open up possibilities for European companies with attractive solutions in these segments.\n\nOther areas that could be interesting are information services and the security service segment. Licensing could be one alternative but would limit revenues compared to other entry strategies.\n\nCurrently, the third AI boom is picking up momentum globally, including Japan although from a lower level.\n\nJapan is trying to catch up with overseas markets, in particular the U.S. market, helped by governmental initiatives with a strong growth strategy focus on artificial intelligence and robotics. Connectivity between robots is the new direction in which the development is heading.\n\nJapan\u2019s AI market is predicted to grow from JPY 3.7 trillion in 2015 to JPY 87 trillion by 2030. The transport sector that includes driverless taxis and trucks will grow from JPY 100 million in 2015 and is expected to reach JPY 30.5 trillion by 2030. The share of the market value of the transport sector to the total market value will increase from 0.003 percent in 2015 to 35.1 percent in 2030, witnessing a strong impact on the future AI market.\n\nTo get access to new technological advances in AI, several Japanese companies have set up R&D bases in the U.S. One example is Toyota.\n\nIt will be important to direct the development of the domestic AI industry to actual issues and develop products and solutions that meet actual needs of end-users.\n\nSome of the challenges the AI industry in Japan is facing are the lack of experienced talent within the AI field and the development of safety standards. A regulatory framework commensurate with the current development level of AI has to be established.\n\nBusiness opportunities in Japan for European AI-related companies include AI technologies and AI solutions.\n\nThe study has found that deep learning is not as utilised in Japan as in many overseas markets including Europe. This will offer opportunities for European companies with extensive experience and capabilities in this sub-segment of AI technologies. As the Japanese government has prioritised use of AI and robotics, the need for deep learning-driven solutions will increase in the years to come.\n\nAdditionally, the automobile industry that so far mainly has been working on technologies to support drivers including machine control systems will increasingly need deep learning-driven solutions to further the development of driverless vehicles. This will offer business opportunities for Europeans firms with capabilities and attractive solutions related to automated driving technology.\n\nIn the AI solutions segment, where Japanese companies, in particular start-ups, are trying to find niches, teaming up with European firms offering platforms with unique AI solutions could be one attractive way to address local needs, and this would also help European AI technology into the Japanese market.\n\nThe findings of the study showed application areas with many new entrants of Japanese AI companies. Marketing is a segment with many offered solutions including image recognition, data analytics and automated marketing. Offering of attractive marketing-based solutions including information services and security solutions could be potential additional areas for European companies.\n\nTo find out more about the current status of the Japanese market for AI solutions including AI technology, visiting trade fairs is one way to meet with Japanese companies to discuss business opportunities including partnering."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-team-in-singapore-e1d7108963b4",
        "title": "Neuromation Team in Singapore \u2013 Neuromation \u2013",
        "text": "This week we start our Asian Road show, with the first stop in Singapore! Yesterday, Neuromation team had an AI & Blockchain Meetup at Carlton Hotel.\n\nWe have met AI enthusiasts, researchers, developers and innovators, telling them more about what is AI, Machine Learning, Deep Learning, Neural Networks and how is AI being applied to improve our world, our businesses, and our lives.\n\nOur team-members, Dr. Sergey Nikolenko, Mr. Maksym Prasolov, Mr. Evan Katz, Mr. Arthur McCallum and Mr. Daniel Liu told about Neuromation case and the future of AI.\n\nNeuromation Road Show continues, come and meet us at the AI Expo at Tokyo Big Sight, April 4th-6th, at the booth 4\u20137!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/from-facebook-to-ai-to-blockchain-the-good-the-bad-and-the-ugly-truth-about-customer-data-in-2018-e655863821f9",
        "title": "From Facebook to AI to Blockchain: the Good, the Bad and the Ugly Truth about Customer Data in 2018",
        "text": "After the data of more than 50 million Facebook users was stolen and a Facebook manager\u2019s customer-disparaging memo \u2014 titled \u201cThe Ugly\u201d\u2013 was leaked to reporters, the social media giant became a cautionary tale of how a company should not manage customer data. Thousands of AI entrepreneurs meeting in Tokyo this week hope to generate more positive headlines about data that \u2014 unlike Facebook\u2019s fiasco \u2014 will be embraced by consumers worldwide. Retailers plan to introduce new blockchain and AI technologies that will make customer data more secure, give shoppers more choices and lower prices and fuel a data-driven revolution in retail.\n\nTokyo, Japan, April 2nd, 2018 \u2014 At this week\u2019s AI summit in Tokyo, 50,000 entrepreneurs, AI evangelists, and hot startups will see new technologies that promise to forever change the retail industry. Technologies likely to generate the most interest because of Facebook\u2019s recently-leaked data breach will be products that help tech companies protect customer data. While data security is an important goal for many companies attending the expo, they will also search for technologies or platforms that will help them acquire tons of additional data needed to create the AI algorithms embedded in autonomous, \u2018thinking\u201d computer systems and robots.\n\n\u201cThere is a massive shift underway in retail right now, with more than $3 billion being spent this year alone, on AI systems that predict customer demand, improve customer service and inventory management,\u201d said Alex Isaev, an exhibitor at the Tokyo conference and founder of OSA DC, an AI-driven platform that collects and analyzes data from retailers. \u201cEveryone\u2019s excited about this evolution but it can\u2019t happen without billions of data points about customers, the retail stores they visit, and the products they buy.\u201d\n\nIsaev describes data as the lifeblood of artificial intelligence because an AI algorithm, the \u201cbrain\u201d of a smart computer, needs a massive amount of visual and other forms of data to analyze and use as training material as it learns how to perform its assign tasks. With enough data, a well-trained smart computer will make autonomous decisions, anticipating a buyer\u2019s purchase and making sure the product they plan to buy is in stock.\n\nIn addition to tons of data they need to train, algorithms also need powerful computers and a neural network, an interconnected web of computer nodes loosely inspired by the interconnected neurons in human brain cells. As an AI algorithm processes training data, connections between nodes learn from each new piece of information, eventually building up an ability to interpret future data. Unfortunately, in recent years, as demand for AI-powered products increased, these building blocks of artificial intelligence remained in short supply, slowing down the pace of AI machine adoption throughout the retail industry.\n\nBut at this week\u2019s AI Expo in Tokyo, industry experts anticipate an end to these shortages thanks to the invention of synthetic (\u201cartificial\u201d) data and the launch of Neuromation, an AI development platform. Maxim Prasolov, CEO of Neuromation as well as a speaker and exhibitor at the AI Expo, created the platform on blockchain, the distributed ledger technology supporting crypto-currencies like Bitcoin, to help AI developers use shared computing power, deep learning networks, and buy the real or synthetic data they need to finish training their AI algorithms. They also have access to the Neuromation Synthetic Data Generator, which mimics real data closely enough that AI algorithms can use it to complete their training.\n\n\u201cNeuromation makes the AI engineering process faster, cheaper, and easier for retailers,\u201d said Prasolov. \u201cBy creating a shared, powerful, blockchain-based neural network for AI development, we\u2019re going to help mid-size retailers create customer-predictive, inventory-tracking deep learning tools that will help them compete with the big boys like Amazon or Wallmart.\u201d\n\nIn \u201cThe Ugly\u201d memo leaked to news reporters last week, Facebook executive Andrew Bosworth appeared to prioritize the company\u2019s growth over all else, a view that may have led to questionable data collection and manipulative treatment of its users. While AI entrepreneurs meeting in Tokyo this week, including Neuromation and OSA DC CEOs Prasolov and Isaev, want their companies to grow, they agree growth is impossible if they do not respect and protect the customer data retailers need to remain competitive.\n\nNeuromation and OSA DC, partners in AI development for the retail industry, will demonstrate their affordable and secure AI and data-generating tools and platform during the three-day AI Expo in their shared booth (4\u20137). The Expo begins Wednesday, April 4th at the Tokyo International Exhibition Center (Tokyo Big Sight) in Tokyo, Japan.\n\nTo learn more about AI Expo Tokyo, visit: www.ai-expo.jp.\n\nTo learn more about Neuromation, visit: www.neuromation.io\n\nTo learn more about OSA DC, visit: www.osadc.io\n\nIf you\u2019re interested in interviewing managers from either Neuromation or OSA DC, call (510) 677\u20132947"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuronuggets-segmentation-with-mask-r-cnn-c76d363b67fb",
        "title": "NeuroNuggets: Segmentation with Mask R-CNN \u2013 Neuromation \u2013",
        "text": "In the third post of the NeuroNuggets series, we continue our study of basic problems in computer vision. I remind the reader that in this series we discuss the demos available on the recently released NeuroPlatform, concentrating not so much on the demos themselves but rather on the ideas behind each deep learning model. This series is also a great chance to meet the new Neuromation deep learning team that has started working at our new office in St. Petersburg, Russia.\n\nIn the first installment, we talked about the age and gender detection model, which is basically image classification. In the second, we presented object detection, a more complex computer vision problem where you also have to find where an object is located. Today, we continue with segmentation, the most detailed problem of this kind, and consider the latest addition to the family of R-CNN models, Mask R-CNN. I am also very happy to present the co-author of this post, Anastasia Gaydashenko:\n\nSegmentation is a logical next step of the object detection problem that we talked about in our previous post. It still stems from the same classical computer vision conundrum: even with great feature extraction, simple classification is not enough for computer vision, you also have to understand where to extract these features. Given a photo of your friends, a landscape scene, or basically any other image, can you automatically locate and separate all the objects in the picture? In object detection, we looked for bounding boxes, i.e., rectangles that enclose the objects. But what if we require more detail and label the exact silhouettes of the objects, excluding background? This problem is called segmentation. Generally speaking, we want to go from pictures in the first row to pictures in the second row on this picture:\n\nFormally speaking, we want to label each pixel of an image with a certain class (tree, road, sky, etc) as shown in the image. The first question, of course, is why? What\u2019s wrong with regular object detection? Actually, segmentation is applied widely: in medical imaging, \u0441ontent-based image retrieval, and so on. It avoids the big problem of regular object detection: overlapping bounding boxes for different objects. If you see three heavily overlapping bounding boxes, are these three different hypotheses for the same object (in which case you should choose one) or three different objects that happen to occupy the same rectangle (in which case you should keep all three)? Regular object detection models can\u2019t really decide.\n\nAnd if the shape of the object is far from rectangular segmentation provides much better information (this is very important for medical imaging). For instance, Google used semantic image segmentation to create the Portrait Mode in its signature Pixel 2 phone.\n\nThese pictures also illustrate another important point: the different flavours of segmentation. What you see above is called semantic segmentation; it\u2019s the simpler version, when we simply want to classify all pixels to categories such as \u201cperson\u201d, \u201cairplane\u201d, or \u201cbackground\u201d. You can see in the picture above that all people are labeled as \u201cperson\u201d, and the silhouettes blend together into a big \u201ccuddle pile\u201d, to borrow the expression used in certain circles.\n\nThis leads to another, more detailed type of segmentation: instance segmentation. In this case, we would want to separate the people in the previous photo and label them as \u201cperson 1\u201d, \u201cperson 2\u201d, etc., as shown below:\n\nHere all people on the photo are marked in different colors, which mean different instances. Note also that they are labeled with probabilities that reflect the model\u2019s confidence in a particular class label; the confidence is very high in this case, but generally speaking it is also a desirable property of any AI model to know when it is not sure.\n\nSo how can a computer vision system parse these images in such an accurate and humanlike way? Let\u2019s find out. The first thing we need to study is how convolution works and why do we use it. And yes, we return to CNNs in each and every post \u2014 because they are really important and we keep finding new things to say about them. But before we get to the new things, let\u2019s briefly go through the old, concentrating on the convolutions themselves this time.\n\nInitially, the idea of convolution has come from biology, or, more specifically, studies of the visual cortex. David Hubel and Torsten Wiesel studied the lower layers of the visual cortex in cats. Cat lovers, please don\u2019t watch this:\n\nWhen Hubel and Wiesel moved a bright line across a cat\u2019s retina, they noticed an interesting effect: the activity of neurons in the brain changed depending on the orientation of the line, and some of the neurons fired only when the line was moving in a particular direction. In simple terms, that means that different regions of the brain react to different simple shapes. To model this behaviour, researchers had to invent detectors for simple shapes, elementary components of an image. That\u2019s how convolutions appeared.\n\nFormally speaking, convolution is just a scalar product of two matrices taken as vectors: we multiply them componentwise and sum up the results. The first matrix is called the \u201ckernel\u201d; it represents a simple shape, like this one:\n\nThe second matrix is just some patch from the picture where we want to find the pattern shown in the kernel. If the convolution result is large, we decide that the target shape is indeed present in this patch. Then we can simply run the convolution through all patches in the image and detect where the pattern occurs.\n\nFor example, let us try to find the filter above in a picture of a mouse. Here you can see the part where we would expect the filter to light up:\n\nAnd if we multiply the corresponding submatrix with the kernel and sum all the values, we indeed get a pretty big number:\n\nOn the other hand, a random part of the picture does not produce anything at all, which means that it is totally different from the filter\u2019s pattern:\n\nFilters like this let us detect some simple shapes and patterns. But how do we know which of these forms we want to detect? And how can we recognize a plane or an umbrella from these simple lines and curves?\n\nThis is exactly where the training of the neural networks comes in. The shapes defined by kernels (the filters) are actually what the CNN learns from the training set. It is usually simple lines and gradients on the first layers of the neural network, but then, with each layer of the model, these shapes are combined with one another into recognizable kernels; a set of kernels is called a map:\n\nThe other operation necessary for CNNs is pooling. It is mostly used to reduce computational costs and suppress noise. The main idea is to cover a matrix with small submatrices and leave only one number in each, thus reducing the dimension; usually, the result is just a maximum or average of the values in the small submatrix. Here is a simple example:\n\nThis kind of operations is also sometimes called downsampling as they reduce the amount of information we store.\n\nAbove, we have seen a simplified explanation of techniques used to create convolutional neural networks. We have not touched upon learning at all, but that part is pretty standard. With these simple tools, we can teach a computer model to recognize all sorts of shapes; moreover, CNNs operate on small patches so they are perfectly parallelizable and well suited for GPUs. We recommend our previous two posts for more details on CNNs, but let us now move on to the more high-level things, that is, segmentation.\n\nIn this post, we do not discuss all modern segmentation models (there are quite a few) but go straight to the model used in the segmentation demo on the NeuroPlatform. This model is called Mask R-CNN, and it is based on the general architecture of R-CNN models that we discussed in the previous post about object detection; hence, a brief reminder is again in order.\n\nIt all begins with the R-CNN model, where R stands for \u201cregion-based\u201d. The pipeline is pretty simple: we take a picture and apply an external algorithm (called selective search) to it, searching for all kind of objects. Selective search is a heuristic method that extracts regions based on connectivity, color gradients, and coherence of pixels. Next, we classify all extracted regions with some neural network:\n\nDue to the high number of proposals, R-CNN worked extremely slow. In Fast R-CNN, the RoI (region of interest) projection layer was added to the neural network: instead of putting each region from proposals through the whole network, Fast R-CNN takes the whole image through the network once, finds neurons corresponding to a particular region in the feature map in the network, and then applies the remaining part of the network to each found set of neurons. Like here:\n\nThe next step was to invent the Region Proposal Network that could replace selective search; the Faster R-CNN model is now a complete end-to-end neural network.\n\nYou can read about all these steps in more detail in our object detection post. But we\u2019re here to learn how Faster R-CNN can be converted to solve the segmentation problem! Actually, it is extremely simple but nevertheless efficient and functional. The authors just added a parallel branch for predicting an object mask to the original Faster R-CNN model. Here is how the resulting Mask R-CNN model looks like:\n\nThe top branch in the picture predicts the class of some region and the bottom branch tries to label each pixel of the region to construct a binary mask (i.e., object vs. background). It only remains to understand where this binary mask comes from.\n\nLet us take a closer look at the segmentation part. It is based on a popular architecture called Fully Convolutional Network (FCN):\n\nThe FCN model can be used for both image detection and segmentation. The idea is pretty straightforward, and the network is actually even simpler than usual, but it\u2019s still a deep and interesting idea.\n\nIn standard deep CNNs for image classification, the last layer is usually a vector of the same size as the number of classes that shows the \u201cscores\u201d of different classes that could be then normalized to give class probabilities. This is what happens in the \u201cclass box\u201d on the picture above.\n\nBut that if we stop at some middle layer of the CNN and instead of vectors do some more convolutions? And on the last convolutional layer, we get the same number of features as the number of classes. Then, after proper training, we can get \u201cclass scores\u201d in every pixel of the last layer, getting a kind of a \u201cheatmap\u201d for every class! Here is how it works \u2014 regular classification on top and a fully convolutional approach on the bottom:\n\nFor segmentation via this network, we will use the inverses of convolution and pooling. Meet\u2026 deconvolution and unpooling!\n\nIn deconvolution, we basically do convolution but the matrix is transposed, and now the output is a window rather than a number. Here are two popular ways to do deconvolution (white squares are zero paddings), animated for your viewing convenience:\n\nTo understand unpooling, recall the pooling concept that we discussed above. To do max-pooling, we take the maximum value from some submatrix. Now we want to also remember the coordinates of the cells from which we took it and then use it to \u201cinvert\u201d max-pooling. We create the matrix with the same shape as the initial and put maximums to the corresponding cells, reconstructing other cell values with approximations based on known cells. Some information stays lost, of course, but usually upsampling works pretty well:\n\nThrough the use of deconvolution and unpooling, we can construct pixel-wise predictions for each class, that is, segmentation masks for the image!\n\nWe have seen how Mask R-CNN does segmentation, but it is even better to see the result for yourself. We follow the usual steps to get the model working.\n\n5. Launch it with the \u201cNew Task\u201d button:\n\n6. Try the demo! You can upload your own photo for segmentation. We chose this image:\n\n7. And here you go! The model shows bounding boxes as well, but now it gives much more than just bounding boxes:"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-platform-update-d8fb1164a49d",
        "title": "Neuromation Platform Update \u2013 Neuromation \u2013",
        "text": "This week our teams are gathered in Tel-Aviv for the Future of AI event and a team-building session.\n\n450 visitors on the stand of Neuromation, 4 interviews, 15 speed-dating business meetings and one 15-minutes keynote speech by CEO Maxim Prasolov and CRO Sergey Nikolenko on \u201cComputation for AI: problems, solutions and trends\u201d, winning applause from the audience.\n\nApart from this, here is what we have achieved so far in our offices in Kiev, Saint Petersburg and Moscow:\n\nA few demo pages were successfully launched, including Demo page, Preliminary Pigs demo and Preliminary demo for Let\u2019s Enhance. Feel free to check, how it works!\n\nOur Saint Petersburg office has done the following:\n\n \u2014 conducted several more interviews;\n\n \u2014 kicked off the lab, initialized user accounts for all new hires, introduced them to Neuromation;\n\n \u2014 started working on OSA HP related projects;\n\n \u2014 attended the Data Science UA conference, delivered a keynote talk on reinforcement learning;\n\nNeuromation Moscow office has fixed and improved the following:\n\n- Fixed issues with content for models/generators displaying in Safari 10 (Models & Generators were not shown);\n\n- Fixed the issue with deleting of files attached to the order, as well as have localized filenames;\n\n- Fixed incorrect behaviour when user presses browser\u2019s back button on \u201cGenerator > New Task page\u201d;\n\n- Improved \u201cAdd to library\u201d behaviour. It is shown \u201cAlready purchased\u201d button instead of \u201cAdd to library\u201d button for the items that is already in user\u2019s library.\n\nFor the Neuromation Lab the several hot-fixes for markup editor of Neuromation Command Center (NCC) was done.\n\nWe have also prepared for the upcoming events, check our calendar here.\n\nAnd in the end of the day, we\u2019ve had an amazing time in Tel-Aviv, building our team stronger and cooler!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-team-at-the-future-of-ai-253da258fa8c",
        "title": "Neuromation Team at the Future of AI! \u2013 Neuromation \u2013",
        "text": "Neuromation\u2019s global team gathered in Tel-Aviv at the Future of AI conference to present our concept of Knowledge Mining to the international AI market. Israel has taken a promising position in AI, creating nearly 700 AI jobs, of which only 300 were filled.\n\nThis is an example of the growing demand for AI talent and products that Neuromation is filling through its distributed marketplace Platform, and its custom turn-key solutions provided by Neuromation Labs.\n\nStay with us, more reports from the event, including the recording of the keynote speech by Maxim Prasolov, CEO and Sergey Nikolenko, CRO, are coming soon."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuronuggets-object-detection-9a7a17f7b2fc",
        "title": "NeuroNuggets: Object Detection \u2013 Neuromation \u2013",
        "text": "This is the second post in our NeuroNuggets series. This is a series were we discuss the demos already available on the recently released NeuroPlatform. But it\u2019s not as much about the demos themselves as about the ideas behind each of these models. We also meet the new Neuromation deep learning team that we hired at our new office in St. Petersburg, Russia.\n\nThe first installment was devoted to the age and gender estimation model, the simplest neural architecture among our demos, but even there we had quite a few ideas to discuss. Today we take convolutional neural networks for image processing one step further: from straightforward image classification to object detection. It is also my pleasure to introduce Aleksey Artamonov, one of our first hires in St. Petersburg, with whom we have co-authored this post:\n\nLooking at a picture, you can recognize not only what objects are on it, but also where they are located. On the other hand, a simple convolutional neural network (CNN) like the one we considered in the previous post cannot do that. All they can is estimate the probability with which a given object is present on the image. For practical purposes, this is insufficient: on real world images, there are lots of objects that can interact with each other in nontrivial ways. We need to ascertain the position and class of each object in order to extract more semantic information from the scene. We have already seen it with face detection:\n\nThe simplest approach that was used before the advent of convolutional networks consisted of a sliding window and a classifier. If we need, say, to find human faces on a photo, we first train a network that says how likely it is that a given picture contains a face and then apply it to every possible bounding box (a rectangle in the photo where an object could appear), choosing the bounding boxes where this probability is highest.\n\nThis approach actually would work pretty well\u2026 if anyone could apply it. Detection with a sliding window needs to look through a huge amount of different bounding boxes. Bounding boxes have different positions, aspect ratios, scales. If we try to calculate all the options that we should look through, we get about 10 million combinations for a 1Mpix image. Which means that the naive approach would need to run the classification network 10 million times to detect the actual position of the face. Naturally, this would never do.\n\nOur next stop is an algorithm that embodies classical computer vision approaches to object detection. By \u201cclassical\u201d here we mean computer vision as it was before the deep learning revolution made every kind of image processing into different flavours of CNNs. In 2001 Paul Viola and Michael Jones proposed an algorithm for real-time face detection. It employs three basic ideas:\n\nBefore describing these stages, let us make sure that we actually want the algorithm to achieve. A good object detection algorithm has to be fast and have a very low false-positive rate. We have 10 million possible bounding boxes and only a handful of faces on the photo, so we cannot afford a false positive rate much higher than 10\u20136 unless we want to be overwhelmed by incorrect bounding boxes. With this in mind, let us jump into the algorithm.\n\nThe first part is the Haar transform; it is best to begin with a picture:\n\nWe overlap different filters on our image. Activation of one Haar filter is the sum of the values in the white parts of the rectangle minus the sum of values under black part.\n\nThe main property of these filters is that they can be computed across the entire image very quickly. Let us consider the integral version (I*) of the original image (I); the integral version is the image where intensity at coordinate I*x,y is the total intensity over the whole rectangle that begins at the top left corner and ends in (x,y):\n\nLet us see the Haar filter overlapped on the image and its integral version in action:\n\nHaar features for this filter could be computed in just a few operations. E.g., the horizontal Haar filter activation shown on the Terminator\u2019s face equals 2C \u2014 2D + B \u2014 A + F \u2014 E, where the letters denote intensities at the corresponding points on the integral image. We won\u2019t go into the formulas but you can check it yourself, it\u2019s a good exercise for understanding the transform.\n\nTo select the best Haar features for face recognition, the Viola-Jones algorithm uses the AdaBoost classification algorithm. Boosting models (in particular, AdaBoost) are machine learning models that combine and build upon simpler classifiers. AdaBoost can take weak features like Haar that are just a little better than tossing a coin. But then AdaBoost learns a combination of these features in such a way that the final decision rule is much stronger and better. It would take a whole separate post to explain AdaBoost (perhaps we should, one day), so we\u2019ll just add a couple of links and leave it at that.\n\nThe third, final idea is to combine the classifiers into a cascade. Thing is, even boosted classifiers still have too much false positives. A simple 2-feature classifier can achieve almost 100% detection rate, but with a 50% false positive rate. Therefore, the Viola-Jones algorithm uses a cascade of gradually more complex classifiers, where we can reject a bounding box on every step but have to pass all checks to output a positive answer:\n\nThis approach leads to much better detection rates. Roughly speaking, if we have 10 stages in our cascade, each stage has 0.3 false positive rate and 0.01 false negative rate, and all stages are independent (this is a big assumption, of course, but in practice it still works pretty well), the resulting cascade classifier achieves (0.3)10 ~ 3*10\u20136 false positive rate and 0.9 detection level. Here is how a cascade works on a group photo:\n\nFurther research in classical computer vision went into detecting objects of specific classes such as pedestrians, vehicles, traffic signs, faces etc. For complex detectors on deep phases of a cascade we can use different kinds of classifiers such as histogram of oriented gradients (HOG) or support vector machines (SVM). Instead of using Haar features on image in grayscale we can get image channels in different color schemes (CIELab or HSV) and image gradients in different directions. All these features are computed in the integral space, summing inside a rectangle with an adjustable threshold.\n\nAn important problem that appeared already in classical computer vision is that near a real object, our algorithms will find multiple intersecting bounding boxes; you can draw different rectangles around a given face, and all will have rather high confidence. To choose the best one classical computer vision usually employs the non-maximum suppression algorithm. However, this still remains an open and difficult problem because there are many situations when two or more objects have intersecting bounding boxes, and a simple greedy implementation of non-maximum suppression would lose good bounding boxes. This part of the problem is relevant for all object detection algorithms and still remains an active area of research.\n\nInitially, in object detection tasks neural networks were treated as tools for extracting features (descriptors) on late stages of the cascade. Neural networks by themselves had always been very good in image classification, i.e., prediction of the class or type of the object. But for a long time, there was no mechanism to locate this object at the image with neural networks.\n\nWith the deep learning revolution, it all changed rather quickly. By now, there are several competing approaches to object detection that are all based on deep neural networks: YOLO (\u201cYou Only Look Once\u201d, a model that was initially optimized for speed but ), SSD (single-shot detectors), and so on. We may return to them in later installments, but in this post we concentrate on a single class of object detection approaches, the R-CNN (Region-Based CNN) line of models.\n\nThe original R-CNN model, proposed in 2013, performs a three-step algorithm to do object detection:\n\nHere is an illustration from the R-CNN paper:\n\nR-CNN brought the deep learning revolution to object detection. With R-CNN, mean average precision on the Pascal VOC (2010) dataset grew from 40% (the previous record) up to 53%, a huge improvement.\n\nBut improved object detection quality is only part of the problem. R-CNN worked well but was hopelessly slow. The main problem was that you had to run the CNN separately for every bounding box; as a result, object detection with R-CNN took more than forty seconds on a modern GPU for a single image! Not quite real-time. It is also very hard to train because you had to juggle together the three components, two of which (CNN and SVM) are machine learning models that you have to train separately. And, among other things, R-CNN requires an external algorithm that can propose bounding boxes for further classification. In short, something had to be done.\n\nThe main problem of R-CNN was speed, so when researchers from Microsoft Research rolled out an improvement there was no doubt how to name the new model: Fast R-CNN was indeed much, much faster. The basic idea (we will see this common theme again below) was to put as much as they could directly into the neural network. In Fast R-CNN, the neural network is used for classification and bounding box regression instead of SVM:\n\nIn order to make detection independent of the size of the object in the image, R-CNN uses Spatial Pyramid Pooling (SPP) layers that had been introduced in SPPnet. The Idea of SPP is brilliant: instead of cropping and warping the region to construct an input image for a separate run of the classification CNN, SPP crops the region of interest (RoI) projection at deeper convolutional layer, before fully-connected layers.\n\nThis means that we can reuse lower layers of the CNN, running it only once instead of a thousand times in basic R-CNN. But a problem arises: a fully-connected layer has a certain size, and the size of our RoI can be anything. The task of the spatial pyramid pooling layer is to solve this problem. The layer divides the window into 21 parts, as shown in the figure below, and summarizes (pools) the values in each part. Thus, the size of the layer\u2019s output does not depend on the size of the input window any more:\n\nFast R-CNN is 200 times faster than R-CNN to apply to a test image. But it is still insufficient for actual real-time object detection due to an external algorithm for generating bounding box hypotheses. On a real photo, this algorithm can take about 2 seconds, so regardless of how fast we make the neural network, we have at least a 2 second overhead for every image. Can we get out of this bottleneck?\n\nWhat could be faster than Fast R-CNN? The aptly named Faster R-CNN, of course! And what kind of improvement could we do to Fast R-CNN to get an even faster model? It turns out that we can get rid of the only external algorithm left in the model: extracting region proposals.\n\nThe beauty of Faster R-CNN is that we can use the same neural network to extract region proposals. We only need to augment it with a few new layers, called the Region Proposal Network (RPN):\n\nThe idea is that the first (nearest to input) layers of a CNN extract universal features that could be useful for everything, including region proposals. On the other hand, the first layers have not yet lost all the spatial information: these features are still rather \u201clocal\u201d and correspond to relatively small patches of the image. Thus, RPN uses precomputed feature values from early layers to propose regions with objects for further classification. RPN is a fully convolutional network, and there are no fully-connected layers in the RPN architecture, so the computing overhead is almost nonexistent (10ms per image), and we can now completely remove the region proposal algorithm!\n\nOne more good idea is to use anchor boxes with different scales and aspect ratios instead of a spatial pyramid. At the time of its appearance, Faster R-CNN became the state of the art object detection model, and while there has been a lot of new research in the last couple of years, it is still going pretty strong. You can try Faster R-CNN at the NeuroPlatform.\n\nAnd finally we are ready to see Faster R-CNN in action! Here is a sequence of steps that will show you how this works on a pretrained model which is already available at the NeuroPlatform.\n\n5. Launch it with the \u201cNew Task\u201d button:\n\n6. Try the demo! You can upload your own photo for detection:\n\n7. And here you go!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/take-a-look-inside-neuromations-tel-aviv-office-ae9593b090fe",
        "title": "Take a Look Inside Neuromation\u2019s Tel Aviv Office \u2013 Neuromation \u2013",
        "text": "With offices around the world, Neuromation is truly a global AI platform and development firm. We work with companies big and small, around the world, in many industries. As a result, we\u2019re becoming the go-to experts on the applications of synthetic data and distributed computing for AI in many use cases. We hope to continue to expand our footprint into new regions and industries to become the de facto resource on synthetic data globally.\n\nToday, we\u2019ll get an inside look at the sales and marketing team, located in Tel Aviv. Neuromation\u2019s Tel Aviv office is in the tallest building in Israel, the Azrieli Sarona Tower, on the top floor. Not only does this provide premium meeting and events space for winning over new clients, but the breathtaking views promote productivity and creativity amongst our team.\n\nWe\u2019ll join Esther Katz, VP for Communications & Marketing and Evan Katz, Chief Revenue Officer, as they go throughout their typical day.\n\nWe start our day early, up before the sun to get a head start on the day. Esther and Evan will both tell you one of the first things they do in the morning after waking is check their phones for new messages and emails. Once they address any urgent concerns, it\u2019s off to fix a quick breakfast, shower, and get ready for the day. Then, it\u2019s not a long commute through a commercial district and past the Tel Aviv government complex before you arrive at the foot of the Azrieli Sarona Tower, Israel\u2019s tallest building.\n\nWhile in the elevator to the 59th floor, we run into some colleagues from other tech startups, and we share some information with a reporter for Start Up Grind, all of whom have offices near ours. Once we arrive on the top floor, we meet up with the rest of the Neuromation team in the office. There are four Neuromation employees in the Tel Aviv office \u2014 Esther, Evan, Gladys, and Hay. Esther handles marketing and reports directly to Maxim (the CEO) while Gladys and Hay are sales reps who report to Evan.\n\nTogether, we warm up to the day with some coffee, enjoying the view out over the Mediterranean. Over coffee, we\u2019ll kick off the day with a recap meeting, reviewing the status of Neuromation\u2019s outreach efforts and setting goals for the day. With that settled, we split up and get to work.\n\nThe sales team spends most of the morning prospecting on Google and LinkedIn for potential new customers. Hay focuses on Israel, Europe, and North America while Gladys focuses on Asia. Even though he\u2019s a C-level executive, Evan also works hard on sales calls and prospecting. When I asked him about it, he told me,\n\n\u201cIt\u2019s really fun because we get the chance to talk to lots of people from around the world, and each is from a different industry. I like to compare it to the \u2018Yellow Brick Road.\u2019 Every day is new and exciting. We find ourselves talking to small companies none has ever heard of, to huge ones & Government officials.\u201d\n\nMeanwhile that morning, Esther has a meeting with a PR agency to brief them on Neuromation and find new ways to get press attention for the Neuromation story. They brainstorm some ideas for potential articles they can pitch news outlets, and the PR representative tells Esther about a publication that would love to do an interview with Maxim.\n\nDepending on the day, lunch is either eaten at the desk, over a sales meeting, or turned into a few snacks throughout the day. By noon, we\u2019ve got momentum for the day, and Asian timezones will soon be closing for business for the day. Gladys found some good leads from prospecting a few days earlier and has set up an introductory call for this afternoon for a new potential client looking for Neuromation Labs custom-designed AI solutions.\n\nEvery week there are new meetings for the sales team. Yesterday, Evan and Hay presented Neuromation to The Minister of International Affairs and a number of Government officials from EU. Also once a week, the sales team has a brainstorming call with our scientists to better understand the opportunities we have in the pipeline. Evan also spends a lot of time thinking about sales strategy. He\u2019s currently working on plans to set up sales teams/agents in Singapore, Japan, Berlin, and the United States.\n\nIn the afternoon is also the Neuromation Asia teams call. Maxim often joins in (although it\u2019s very early morning in San Francisco) to lead the discussion and get an overall picture of where the company is headed. Esther picks up leads from various team members about journalists interested in covering Neuromation, and Evan gives an update on sales and revenue.\n\nThere\u2019s always a lot to do, and the Tel Aviv team often finds themselves working late into the evening to hit a deadline or close a deal. We hope you enjoyed riding along for a day in the life at Neuromation Tel Aviv. We\u2019ll be back at it again tomorrow, telling Neuromation\u2019s story to the world."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-on-the-road-2db3d5580d5d",
        "title": "Neuromation on the road \u2013 Neuromation \u2013",
        "text": "Neuromation Team continues the roadshow! This spring find our team members speaking about Neuromation and Blockchain technologies at Future of AI in Tel Aviv on March 19th.\n\nOur US office goes to ICO Rating Summit in New York. On March 28th, Chief Operations Officer Yuri Kundin will be speaking at the event, telling how to run a company after successfully completing ICO.\n\nApril 4th-6th Neuro Team will be at the 2nd AI EXPO -Artificial Intelligence Exhibition & Conference in Tokyo, along with our partners OSA DC, we will present our technology and retail solutions for the wider audience.\n\nOn April 12, Neuromation team goes to China, going further with Asian market expansion! CEO Maxim Prasolov will be speaking at the 2nd Global Blockchain&Fintech Summit in Hilton Shanghai Hongqiao, along with David Drake from LDJ Capital, Frank Zheng from World Blockchain Organization and Yaniv Feldman from Cointellegence, sharing Neuromation success story and telling more about Blockchain technologies and AI industry.\n\nOn April 25th our team will be back to New York, to take part at The NYC Meetings, presenting Neuromation Platform to the influential financial crowd.\n\nKeep an eye on our social media channels and Neuromation TV to get all the details and reports first!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-platform-update-4e55845e9354",
        "title": "Neuromation Platform Update \u2013 Neuromation \u2013",
        "text": "We aim towards transparent business, this is why we are keeping our users and AI community up-to-date with all the processes inside our offices. This week read about Kyiv and Moscow tech teams, led by CIO Denis Popov and CTO Fedor Savchenko accordingly.\n\nSwagger (API documentation) for OSA has been completed, and OSA team members were added to the Dropbox folder in order to increase data exchange speed.\n\nLet\u2019s Enhance demo deployment, API bug was reported and fixed by our engineering team.\n\nPlatform MVP has been constantly upgraded and the following was fixed and improved:\n\n- Bug when user can buy some items in market twice;\n\n- Fixes for Safari for few dynamical pages;\n\n- Allow to remove files from orders;\n\n- Allow to attach files with non English characters in the name;\n\n- More information about user account in admin area;\n\n- User can now archive order in pending state;\n\n- Billing system refactoring. Currently task is billed only in case it does not fail;\n\n- Generators and models can now be rented;\n\nGo to platform, to check out the latest updates and tools.\n\nMany of you have asked about the website, so here are our steps this week :\n\n-Modifications of iOS application that is used by OSA in their presentations, so it will work with different neural networks connected with computer vision.\n\n- 5 000 new photographic images provided by OSA HP were processed and turned into digital data.\n\n- New features to generator. Export of shelves, export of prices (with ID for text mapping). Was added additional generation mode for random textures.\n\n- Following datasets were generated: Singular dataset, dataset of items on shelves. Items on shelves with different background. Completely negative dataset. Mixed dataset (50% positive 50% negative).\n\nThe Future of AI event in Tel-Aviv is already next week! Sergey Nikolenko and Maxim Prasolov will deliver a keynote speech on computation for AI: problems, solutions, and trends topic, and our marketing department is busy making a platform demo tool and creating marketing and video materials for the exhibition.\n\nWe teamed with Eden PR to support media relations in Israel during the event to maximize the outcome and establish leadership position in the booming AI market in Israel.\n\nPreparation for PTP China \u2014 the biggest Fintech and Blockchain event is in the pipeline too, Neuromation will be present at the exhibition and with a keynote speech by CEO Maxim Prasolov, sharing Neuromation\u2019s success story and telling more about Blockchain technologies and the AI industry.\n\nOur teams in Tallinn, Moscow, Kiev, Saint Petersbourg, Tel-Aviv and San Francisco are working hard on delivering first-class services for the AI industry, and we want to keep the AI community updated about our progress. This Friday we will introduce our St Petersburg scientific team, real sharks of deep learning, they also look cool!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuronuggets-age-and-gender-estimation-2807b1307a13",
        "title": "NeuroNuggets: Age and Gender Estimation \u2013 Neuromation \u2013",
        "text": "Today, we begin a new series of posts that we call NeuroNuggets. On February 15, right on time, we released the first version of the NeuroPlatform. So far it is still in alpha, and it will take a lot of time to implement everything that we have planned. But even now, there are quite a few cool things you can do. In the NeuroNuggets series, we will present these cool things one by one, explaining not only the technicalities of how to run something on the platform but also the main ideas behind every model. This is also my chance to present my new deep learning team hired at our new office in St. Petersburg, Russia.\n\nIn this post, we present our first installment: the age and gender estimation model. This is the simplest neural architecture among our demos, but even this network will have quite a few tricks to explain. And it is my pleasure to introduce Rauf Kurbanov, one of our first hires in St. Petersburg, with whom we have co-authored this post:\n\nAI researchers tend to question the nature of intuitive. As soon as you ask how a computer can do the same thing that seems too easy for humans, you see that what is \u201cintuitively clear\u201d for us can be very hard to formalize. Our visual perception of human age and gender is a good example of such a subtle quality.\n\nTo us AI nerds, Eliezer Yudkowsky is familiar both as an AI safety researcher and the author of the most popular Harry Potter fanfic ever (we heartily recommend \u201cHarry Potter and the Method of Rationality\u201d, HPMoR for short, to everyone). And the Harry Potter series features a perfect example for this post, a fictional artifact that appears intuitively clear but is hard to reproduce in practice:\n\nAlbus Dumbledore had placed an Age Line around the Goblet of Fire to prevent anyone under the age of seventeen from approaching it. Age Line magic was so advanced that even an Ageing Potion could not fool it. Even Yudkowsky did not really dig into the mechanics of Age Line in his meticulous manner in HPMoR but today we will give it a try; and while we are on the subject, we will give a shot to gender recognition as well. As usual in computer vision, we begin with convolutional neural networks.\n\nA neural network, as the name suggests, is a machine learning approach which is in a very abstract way modeled after how the brain processes information. It is a network of learning units called artificial neurons, or perceptrons. During training, the neurons learn how to convert input signals (say, the picture of a cat) into corresponding output signals (say, the label \u201ccat\u201d in this case), training automated recognition from real life examples.\n\nVirtually all computer vision nowadays is based on convolutional neural networks. Very roughly speaking, CNNs are multilayer (deep) neural networks where each layer processes the image in small windows, extracting local features. Gradually, layer by layer, local features become global, able to draw their inputs from a larger and larger portion of the original image. Here is how it works in a very simple CNN (picture taken from this tutorial, which we recommend to read in full):\n\nIn the end, after several (sometimes several hundred) layers we get global features that \u201clook at\u201d the whole original image, and they can now be combined in relatively simple ways to obtain class labels (recognize whether it is a dog, cat, boat, or Harry Potter).\n\nTechnically, a convolutional neural network is a neural network with convolutional layers, and a convolutional layer is a transformation that applies a certain kernel (filter) to every point in the input (a \u201cpicture\u201d with multiple channels in every pixel, i.e., a three-dimensional tensor) and generate filtered output by sliding the kernel over the input.\n\nLet us consider a simple example of a filter: edge detection in images. In this case, the input for edge detection is an image, and each pixel in the image is defined by three numbers: the intensities of red, green, and blue in the color of that pixel. We construct a special kernel which will be applied to every pixel in the image; the output is a new \u201cimage\u201d that shows the results of this kernel. Basically, the kernel here is a small matrix. Here\u2019s how it works:\n\nThe kernel is sliding over every pixel in the image and the output value increases whenever there is an edge, an abrupt change of colors. In the figure above, after multiplying this simple matrix element-wise to every 3x3 window in the image we get a very nice edge detection result.\n\nOnce you understand filters and kernels, it becomes quite simple to explain convolutional layers in neural networks. You can think of them as vanilla convolutions, as in the edge detection example above, but now we are learning convolutional kernels end-to-end when training the networks. That is, we do not have to invent these small matrices by hand anymore but can automatically learn matrices that extract the best features for a specific task.\n\nAge and gender estimation sounds like a traditional machine learning task: binary classification for the genders (it might stir up some controversy but yeah, our models live in a binary world) and regression for the ages. But before we can begin to solve these problems, we need to find the faces on the photo! Classification will not work on the picture as a whole because it might, for example, contain several faces. Therefore, the age and gender estimation problem is usually broken down into two steps, face detection and age/gender estimation for the detected faces:\n\nIn the model that you can find on the NeuroPlatform, these steps are performed independently and are not trained end-to-end, so let us discuss each of them in particular.\n\nFace detection is a classic problem in computer vision. It was solved quite successfully even before the deep learning revolution, in early 2000s, by the so-called Viola-Jones algorithm. It was one of the most famous application of Haar cascades as features; but those days are long gone\u2026\n\nToday, face detection is not treated as a separate task that requires individual approaches. It is also solved by convolutional neural networks. To be honest, since the advent of deep learning it has long become clear that CNNs kick ass at object detection, and therefore we expect modern solution to an old problem to be based on CNNs as well. And we are not wrong.\n\nBut in real world machine learning, you should also consider other properties beside detection accuracy such as simplicity and inference speed. If a simpler approach works well enough, it might not be worth it to introduce very complicated models to gain a few percentage points (remind me to tell you about the Netflix prize challenge results later). Therefore, in the NeuroPlatform demo we use a more classical approach to face detection while keeping CNNs for the core age/gender recognition task. But we can learn a lot from classical computer vision too.\n\nIn short, the face detection model can be described as an SVM on HOG + SIFT feature representation. HOG and SIFT representations are hand-crafted features, the result of years of experience in building image recognition systems. These features recognize gradient orientations in localized portions of an image and perform a series of deterministic image transformations. It turns out this representation works quite well with kernel methods such as support vector machines (SVM).\n\nHere at Neuromation, we are big fans of using synthetic data for computer vision. Usually, this means that we generate sophisticated synthetic datasets from 3D computer graphics and even look towards using Generative Adversarial Networks for synthetic data generation in the future. But let us not forget the most basic tool for increasing the datasets: data augmentation.\n\nSince we have already extracted faces on the previous step, it is enough to augment only the faces, not the whole image. In the demo, we are using standard augmentation tricks such as horizontal/vertical shifts and mirroring alongside with a more sophisticated one of randomly erasing patches of the image.\n\nTo predict the age, we apply a deep convolutional neural network to the face image detected on the previous processing stage. The method in the demo uses the Wide Residual Network (WRN) architecture which beat Inception architecture on mainstream object detection datasets, achieving convergence on the same task twice faster. Before we explain what residual networks are, we begin with a brief historical reference.\n\nThe ImageNet project is a large visual database designed to be used in visual object recognition research. The deep learning revolution of the 2010s started in computer vision with a dramatic advance in solving the ImageNet Challenge. Results on ImageNet were recognized not only within the AI community but across the entire industry, and ImageNet has become and still remains the most popular general purpose computer vision dataset. Without getting into too much details, let us just take a glance at a comparison plot between the winners of a few first years:\n\nOn the plot, the horizontal axis shows how computationally intensive a model is, the circle size indicates the number of parameters, and the vertical axis shows image classification accuracy on ImageNet. As you can see, ResNet architectures show some of the best results while staying on the better side of efficiency as well. What is their secret sauce?\n\nIt is well known that deeper models perform better than shallower models, they are more expressive. But optimization in deep models is a big problem: deeper models are harder to optimize due to the peculiarities of how gradients propagate from the top layers to the bottom layers (I hope one day we will explain it all in detail). Residual connections are an excellent solution to this problem: they add connections \u201caround\u201d the layers, and the gradient flow is now able to \u201cskip\u201d excessive layers during backpropagation, resulting in much faster convergence and better training:\n\nEssentially the only difference with Wide Residual Networks is that in the original paper the tradeoff between width and depth of architecture was studied more carefully resulting in more efficient architecture with better convergence speed.\n\nWe have wrapped the DEX model into a docker container and uploaded in to the NeuroPlatform. Let\u2019s get started! First, enter Neuromation login page https://mvp.neuromation.io/\n\nOn your dashboard on the NeuroPlatform, you can see how much NTK is left on your balance and spend them in three sections:\n\nToday we are dealing with the Age and Gender estimator, an AI model available at the NeuroMarket. Let us purchase our first model! We enter AI models and buy the Age&Gender model:\n\nThen we request a new instance; it may take a while:\n\nAnd here we go! We can now try the model on a demo interface:\n\nIt does tend to flatter me a bit."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/ai-for-spatial-metabolomics-i-the-datasets-of-life-3dcb989aafdd",
        "title": "AI for Spatial Metabolomics I: The Datasets of Life",
        "text": "Here at Neuromation, we are starting an exciting \u2014 and rather sophisticated! \u2014 joint project with the Spatial Metabolomics group of Dr. Theodore Alexandrov from the European Molecular Biology Laboratory. In this mini-series of posts, I will explain how we plan to use latest achievements in deep learning and invent new models to process imaging mass-spectrometry data, extracting metabolic profiles of individual cells to analyze the molecular trajectories that cells with different phenotypes follow\u2026\n\nWait, I\u2019ve surely lost you three times already. Let me start over.\n\nThe picture above shows the central dogma of molecular biology, the key insight of XX century biology into how life on Earth works. It shows how genetic information flows from the DNA to the proteins that actually do the work in the cells:\n\nI\u2019ve painted a very simplified picture but this is truly the central, the most important information flow of life. The central dogma, first stated by Francis Crick in 1958, says that genetic information flows only from nucleic acids (DNA and RNA) to proteins and never back \u2014 your proteins cannot go back and modify your DNA or RNA, or even modify other proteins, they are controlled only by the nucleic acids.\n\nEverybody knows that the genetic code, embodied in DNA, is very important. What is slightly less known is that each step along the central dogma pathway (a pathway is basically a sequence of common reactions that transform molecules into one another for example, DNA -> RNA -> protein is a pathway, and a very important one!) corresponds to its own \u201cdataset\u201d, its own characterization of an organism, each important and interesting in its own way.\n\nYour set of genes, encoded in your DNA, is known as the genome. This is the main \u201cdataset\u201d, your primary blueprint, the genome is the stuff that says how you work in the most abstract way. As you probably know, the genome is a very long string of \u201cletters\u201d A, C, G, and T, which stand for the four nucleotides\u2026 don\u2019t worry, we won\u2019t go into too much detail about that stuff. The Human Genome Project successfully sequenced (\u201cread out\u201d letter by letter) a draft of the human genome in 2000 and a complete human genome in 2003, all three billion of letters. Since then, sequencing methods have improved a lot; moreover, all human genomes are, of course, very similar, so once you have one it is much easier to get the others. Your genome determines what diseases you are susceptible to and defines many of your characteristic traits.\n\nThe study of the human genome is far from over, but it is only the first part of the story. As we have seen above, genetic code from the DNA has to be read out into RNA. This is known as transcription, a complicated process which is entirely irrelevant for our discussion right now: the point is, pieces of the genome are copied into RNA verbatim (formally speaking, T changes to U, a different nucleotide, but it\u2019s still the exact same information):\n\nThe cells differentiate here in which parts of the genome get transcribed.\n\nThe set of RNA sequences (both coding RNA that will later be used to make proteins and non-coding RNA, that is, the rest of it) in a cell is called the transcriptome. The transcriptome provides much more specific information about individual cells and tissues: for example, a cell in your liver has the exact same genome as a neuron in your brain \u2014 but very different transcriptomes! By studying the transcriptome, biologists can \u201cincrease the resolution\u201d and see which genes are expressed in different tissues and how. For example, modern personalized medicine screens transcriptomes to diagnose cancer.\n\nBut this is still about the genetic code. The third dataset is even more detailed: it is the proteome that consists of all proteins produced in a cell, in the process known as translation, where RNA serves as a template, with three letters encoding every protein:\n\nThis is already much closer to the actual objective: the proteins that a cell makes determine its interactions with other cells, and the proteome says a lot about what the cell is doing, what its function in the organism is, what effect it has on other cells, and so on. And the proteome, unlike the genome, is malleable: many drugs work exactly by suppressing or speeding up the translation of specific proteins. Antibiotics, for instance, usually fight bacteria by attacking their RNA, suppressing protein synthesis completely and thus killing the cell.\n\nGenomics, transcriptomics, and proteomics are subfields of molecular biology that study the genome, transcriptome, and proteome. They are collectively known as the \u201comics\u201d. The central dogma has been known for a long way, but only very recently biologists have developed new tools appeared that actually let us peek into the transcriptome and the proteome.\n\nAnd this has led to the big data \u201comics revolution\u201d in molecular biology: with these tools, instead of theorizing we can now actually look into your proteome and find out what\u2019s happening in your cells \u2014 and maybe help you personally, not just develop a drug that should work on most humans but somehow fails for you.\n\nMolecular biologists began to speak of \u201cthe omics revolution\u201d in the context of genomics, transcriptomics, and proteomics, but the central dogma is still not the full picture. Translating proteins is only the beginning of the processes that occur in a cell; after that, these proteins actually interact with each other and other molecules in the cell. These reactions comprise the cell\u2019s metabolism, and ultimately it is exactly the metabolism that we are interested in and that we might want to fix.\n\nModern biology is highly interested in processes that go beyond the central dogma and involve the so-called small molecules: enzymes, lipids, glycose, ATP, and so on. These small molecules are either synthesized inside the cells \u2014 in this case they are called metabolites, that is, products of the cell\u2019s metabolism \u2014 or come from beyond. For instance, vitamins are typical small molecules that cells need but cannot synthesize themselves, and drugs are exogenous small molecules that we design to tinker with a cell\u2019s metabolism.\n\nThese synthesis processes are controlled by proteins and follow the so-called metabolic pathways, chains of reactions with a common biological function. The central dogma is one very important pathway, but in reality there are thousands. A recently developed model of human metabolism lists 5324 metabolites, 7785 reactions and 1675 associated genes, and this is definitely not the last version \u2014 modern estimates reach up to 19000 metabolites, so the pathways have not been all mapped out yet.\n\nThe metabolic profile of an organism is not fully determined by its genome, transcriptome, or even proteome: the metabolome (set of metabolites) forms, in particular, under the influence of environment that provides, e.g., vitamins. Metabolomics, which studies the composition and interaction between metabolites in live organisms, lies at the intersection of biology, analytical chemistry, and bioinformatics, with growing applications to medicine (and that\u2019s not the last of the omics, but metabolomics will suffice for us now).\n\nKnowing the metabolome, we can better characterize and diagnoze various diseases: they all have to leave a trace in the metabolome because if the metabolism has not changed why is there a problem at all?.. By studying metabolic profiles of cells, biologists can discover new biomarkers for both diagnosis and therapy, find new targets for the drugs. Metabolomics is the foundation for truly personalized medicine.\n\nSo far, I\u2019ve been basically explaining recent progress in molecular biology and medicine. But what do we plan to do in this project? We are not biologists, we are data scientists, AI researchers; what is our part in this?\n\nWell, the metabolome is basically a huge dataset: every cell has its own metabolic profile (set of molecules that appear in the cell). Differences in metabolic profiles determine different cell populations, how metabolic profiles change in time corresponds to patterns of cell development, and so on, and so forth. Moreover, in spatial metabolomics that we plan to collaborate on it comes in the form of special images: results of imaging mass-spectrometry applied at very high resolution. This, again, requires some explanation.\n\nMass-spectrometry is a tool that lets us find out the masses of everything contained in a sample. Apart from rare collisions, this is basically the same as finding out which specific molecules appear in the sample. For example, if you put a diamond in the mass-spectrometer you\u2019ll see\u2026 no, not just a single carbon atom, you will probably see both 12C and 13C isotopes, and their composition will say a lot about the diamond\u2019s properties.\n\nImaging mass-spectrometry is basically a picture where every pixel is a spectrum. You take a section of some tissue, put it into a mass-spectrometer and get a three-dimensional \u201cdata cube\u201d: every pixel contains a list of molecules (metabolites) found at this part of the tissue. This process is shown on the picture above. I\u2019d show some pictures here but it would be misleading: the point is that it\u2019s not a single picture, it\u2019s a lot of parallel pictures, one for every metabolite. Something like this (picture taken from here):\n\nThe quest of making better imaging mass-spectrometry tools mostly aims to increase resolution, i.e., make the pixels smaller, and increase sensitivity, i.e., detect smaller amounts of metabolites. By now, imaging mass-spectrometry has come a long way: the resolution is so high that individual pixels in this picture can map to individual cells! This high-def mass-spectrometry, which is becoming known as single-cell mass-spectrometry, opens up the door for metabolomics: you can now get the metabolic profile of a lot of cells at once, complete with their spatial location in the tissue.\n\nThis is the ultimate dataset of life, the most in-depth account of actual tissues that exists right now. In the project, we plan to study this ultimate dataset. In the next installment of this mini-series, we will see how."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-key-questions-answered-4ed3ca71c322",
        "title": "Neuromation key questions answered \u2013 Neuromation \u2013",
        "text": "We, the Neuromation Team, would like to respond to key questions that have been raised by the community.\n\nNeuromation\u2019s goal is the creation of the world\u2019s leading platform for the purchase and sale of core AI componentry: Neural Network Models, Labeled Datasets and Distributed Computing Power.\n\nListing of the NTK on major exchanges is a fundamental necessity for building the Neuromation marketplace Platform, insofar as it creates the NTK liquidity necessary for buyers and sellers to seamlessly transact on the Platform.\n\nNeuromation is in negotiations with multiple large exchanges to list the NTK. Listings are expected to come shortly. This is completely in line with our Road Map, and with our strategy to build steady, organic appreciation in value of the business.\n\nWe are well aware of common strategies of other companies to rush into listings on any and all exchanges, and subsequent \u2018doping\u2019 of their tokens to achieve short-term price spikes. The data shows that these activities are highly dangerous to the health of those companies and to their token holders.\n\nWe appreciate the support of the community in listings on Tidex, Idex and Yobit. There is much, much more in the pipeline.\n\nOf course, the Neuromation community benefits from increasing its size. The larger the community, the more participants, the more valuable the Platform will be.\n\nWe are focused on steady, organic, and REAL growth in membership in Telegram and other community platforms. Real participants add real value to the community.\n\nIn addition to work in typical channels, Neuromation plans to soon announce an unprecedented move for a company in the utility token space. We believe that this will be a major driver for acceptance of both Neuromation and our approach to creation of value applied Artificial Intelligence.\n\nPlease accept our sincere thanks for your support of our mission to build the world\u2019s leading hub for the AI revolution."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-at-the-future-of-ai-in-tel-aviv-b3d66a96e532",
        "title": "Neuromation at the Future of AI in Tel Aviv \u2013 Neuromation \u2013",
        "text": "Neuromation continues the roadshow and on March 20th our team will be speaking at the Future of AI Conference in Tel Aviv.\n\nChief Revenue Officer Evan Katz comments on Neuromation\u2019s participation in the event, where the company acts as a Platinum Partner along with Microsoft, IBM, and Corwdflower:\n\nThe outcome will create hot leads for our sales department and generate massive media coverage. We are working on a few very interesting projects. One of them being a whole management system for livestock producers and animal facial and medical recognition. Another is for the retail sector, where we use synthetic data to create vast data sets for retailers. Apart from the direct projects we are working on, we also have the Neuromation Platform, an open marketplace for off the shelf AI services (Computational power, synthetic data sets, models and more).\n\nOur goal is to democratize AI, at the moment we are concentrating on retail, agriculture, medical, computer vision. These industries are where we noticed the biggest bottlenecks in terms of AI applications. The projects and solutions we have created solve problems in these specific fields. Apart from these, we are also looking for AI based companies to help them get to market faster and cheaper."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/maxim-prasolov-libe-at-huobi-pro-talk-d57065616bbd",
        "title": "Maxim Prasolov Live at Huobi PRO Talk \u2013 Neuromation \u2013",
        "text": "Last weekend Huobi Pro has held a talk with Maxim Prasolov, the CEO of Neuromation.\n\nHere is a brief recap and the most interesting quotes :\n\nSynthetic data is like animation for AI training, we create kind of a cartoon and train models inside the simulator. It needs a lot of computing power, and we take this power from the blockchain community and we believe that distributed AI is the next step in blockchain evolution.\n\nThe data is one of the major problems, the major bottleneck in the AI industry.\n\nTo train the machine learning algorithm you have to show to AI what to do, like you teach your child what a phone is, what a key is, what is a cup. In AI technologies you show an image of the key and create a description, or label it. It costs at least 20 cents for an image, and in the end you will spend millions of dollars on it. We believe that data is the new oil. We create synthetic data, which is kind of a synthetic oil for AI.\n\n2. When and why did you decide to start using blockchain technologies?\n\nWe have computing nodes which belong to private miners all over the world. And without blockchain we would have to switch to hundreds of different jurisdictions, the contacts would be impossible. It is impossible using traditional systems. With blockchain we can do it.\n\n3. Who are your competitors?\n\nWe don\u2019t have competitors in synthetic data. But we have a lot of competitors in cloud-based services, like Amazon, Tensor Flow and those who use computing power for machine learning. We are like Uber for computing power. Because our competitors provide a very expensive service. We provide affordable services, and the blockchain community helps the AI community.\n\n4. How come your company uses proof-of-work instead of proof-of-stake?\n\nWe are using blockchain technologies but there is a big difference between traditional blockchain and useful computing. So we don\u2019t need consensus to validate data. That\u2019s why in the beginning we used ethereum blockchain\n\n5. Will you be able to create a mining app to pay miners for computational power?\n\nYes, we are working on it, because our container can be deployed on your computational node. We are developing a hybrid node, so our computer could be switched on and off from cryptocurrency mining to useful computing and back.\n\nVote to get NTK listed on Huobi exchange! Support the AI token of crypto universe!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-and-longenesis-the-human-data-economy-8867941b11f8",
        "title": "Neuromation and Longenesis: The human data economy \u2013 Neuromation \u2013",
        "text": "We have recently announced an important strategic partnership: Neuromation has joined forces with Longenesis, a startup that promises to develop a \u201cdecentralized ecosystem for exchange and utilization of human life data using the latest advances in blockchain and artificial intelligence\u201d. Sounds like a winning entry for last year\u2019s bullshit bingo, right? Well, in this case we actually do believe in Longenesis, understand very well what they are trying to do, and feel that together we have all the components needed for success. Let\u2019s see why.\n\nI will begin with the obvious: Longenesis is all about the data, and Neuromation is all about processing this data, training state-of-the-art AI models. This makes us ideal partners: Longenesis needs a computational framework to train AI models and highly qualified people to make use of this framework, and Neuromation needs interesting and useful datasets to make the platform more attractive to both customers and vendors.\n\nThis is especially important for us because Longenesis will bring not just any data but data related to medicine and pharmaceutics. I have recently written here about AI in medicine, but this is an endless topic: we are at the brink of an AI revolution in medicine, both in terms of developing generic tools and in terms of personalized medicine. Longenesis will definitely be on the frontier of this revolution.\n\nI have personally collaborated with Longenesis CSO Alex Zhavoronkov and his colleagues in Longenesis\u2019 parent company, Insilico Medicine, especially Arthur Kadurin (he is my Ph.D. student and my co-author in our recently published book, Deep Learning). Together, we have been working on frontier problems related to drug discovery: researchers at Insilico have been applying generative adversarial networks (GANs) to generate promising molecules with desired properties. Check out, e.g., our recent joint paper about the DruGAN model (a name that rings true to a Russian ear). Hence, I can personally vouch that not only Alex himself is one of the most energetic and highly qualified people I have ever met, but that his team in both Insilico and Longenesis is made of great people. And this is the best predictor for success of all.\n\nThe basic idea of Longenesis blends together perfectly with what is actually an open problem for the NeuroPlatform so far. On the recently released NeuroPlatform, AI researchers and practitioners will be able to construct AI models (especially deep neural networks that could be trained on the GPUs provided by mining pools), upload them in a standardized form into the NeuroMarket, our marketplace of AI models and datasets, and then rent out either the model architectures or already pretrained models.\n\nTo train a neural network, you need a dataset. And if you want to use the GPUs on mining pools, you need to transfer the data to these GPUs. The transfer itself also presents a technical problem for very large datasets, but the main question is: how are you going to trust some unknown mining pool from Inner Mongolia with your sensitive and/or valuable data? It\u2019s not a problem when you train on standard publicly available datasets such as ImageNet, the key dataset for modern computer vision, but to develop a customized solution you will still need to fine-tune on your own data.\n\nWe at Neuromation have an interesting solution for this problem: we plan to use synthetic data to train neural networks, creating, e.g., generators for rendered photos based on 3D models. In this case, we solve two problems at once: synthetic data is very unlikely to be sensitive, and there is no transfer of huge files because you only transfer the generator and the full dataset does not even have to be stored at any time. But still, you can\u2019t really synthetize the MRI of a specific person or make up pictures of faces of specific people you want to recognize. In many applications, you have to use real data, and it could be sensitive.\n\nHere is where Longenesis comes in. Longenesis is developing a solution for blockchain-based safe storage for the most sensitive data of all: personal medical records. If hospitals and individuals trust Longenesis\u2019 solution with medical data, you can definitely trust this solution with any kind of sensitive or valuable data you might have. Their solution also has to be able to handle large datasets: CT or MRI scans are pretty hefty. Therefore, we are eagerly awaiting news from them on this front.\n\nBut this is still only the beginning.\n\nThe ultimate goal of Longenesis is to create a marketplace of personal medical records, a token economy where you can safely store and sell access to your medical records to interested parties such as research hospitals, medical researchers, pharmaceutical companies and so on.\n\nOver his or her lifetime, a modern person accumulates a huge amount of medical records: X-rays, disease histories, CT scans, MRIs, you name it. All of this is very sensitive information that should rightly belong to a person \u2014 but also very useful information.\n\nImagine, God forbid, that you are diagnosed with a rare disease. This is a very unfortunate turn of events, but it also means that your medical records suddenly increase in value: now doctors could get not just yet another MRI of some healthy average Joe but the MRI of a person who has developed or will later develop this disease. The human data economy that Longenesis plans to build will literally sweeten the pill in this case: yes, it\u2019s a bad thing to get this disease but it also means you can cash out on your now-interesting medical records. And let\u2019s face it, in most cases people won\u2019t mind to share their MRIs or CT scans with medical researchers at all, especially for a price.\n\nBut the price in this case is much less important than the huge possibilities that open up for the doctors to develop new drugs and better treat this rare disease. With the human data economy in place, people will actually be motivated to bring their records to medical researchers, not the other way around.\n\nThis could be another point where we can collaborate; here at Neuromation, we are also building a token economy for useful computing, so this is a natural point where we could join forces as well. In any case, the possibilities are endless, and the journey to better medicine for all and for every one in particular is only beginning. Let\u2019s see where this path takes us."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/media-about-neuromation-5cdfe87c58ad",
        "title": "Media about Neuromation \u2013 Neuromation \u2013",
        "text": "Great news at the end of the week! We\u2019ve gained a lot of media attention recently, and wanted to share the highlights with you.\n\nNeuromation was featured in Bloomberg : \u201cICO fundraising is rising even with crypto market downturn, the biggest ICO in January was by blockchain infrastructure project Envion, which raised $100 million, followed by distributed data platform Neuromation, which raised $73.2 million, according to Coinist.\u201d\n\nRecent Neuromation partnership with Longenesis has aroused interest among tech media and was covered by Irish Tech News :\n\nAs well as Crowd Fund Insider, Coin Journal, Block Tribune and BlockChain News.\n\nAnd last, but not least \u2014 an interview with CEO Maxim Prasolov on Neuromation\u2019s history, the Neuro team and challenges that the industry is facing, for Disruptor Daily.\n\nFor our Russian-speaking community there is a weekend read from Sergey Nikolenko for Indicator.ru, and a few news from AIN, Freedman and Bitgid."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-platform-update-f4baca91315f",
        "title": "Neuromation Platform Update \u2013 Neuromation \u2013",
        "text": "We want to update the Neuro community on the platform\u2019s development on a regular basis. So here is the state that the alpha version is on currently.\n\nIn the near future, we are going to work on the interface for the generator and create a few models of templates that could be trained and also track the results of training. In this case, we will close the loop of neural network.\n\nThe next step is to grow the amount of model templates to cover more AI tasks.\n\nWe will focus on the computer vision and the task list could be found here.\n\nWe\u2019ve also faced a few challenges, among them:\n\n- some tasks require more operating memory, with an average machine having 2\u20134 GB. For the synthetic data generation we\u2019ve almost completed this task, while for neural networks it is still in process. We will adapt networks and start using powerful machines. As some of the models require 64 GB of operating memory while being trained.\n\n- in two weeks when the process from data generation to model training will be set up, we will ask users to test the cycle. As our mission is to create a clear interface.\n\nOur task is to make neural networks available for everyone who has basic programming skills.\n\nRegarding mining resources - we are currently testing our partner\u2019s pools. Technical requirements are yet to be developed. As neural networks are tricky, we are working in a few directions, like adapting networks, working on an orchestrator, and a final mining mode, so we could put them together.\n\nIt\u2019s a complicated process and in a few months, we will use additional pools and the system will be unfolded individually for each pool. We plan to make knowledge mining available for everyone this year."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/blockchain-and-ai-will-transform-healthcare-neuromation-and-longenesis-form-partnership-7fd84723168",
        "title": "Blockchain and AI Will Transform Healthcare: Neuromation and Longenesis Form Partnership",
        "text": "The blockchain and AI revolutions are converging on the healthcare Industry, with the partnership announcement from Longenesis, a blockchain-based Life Data Marketplace, and Neuromation, a blockchain and distributed synthetic data platform for deep learning applications.\n\nNeuromation recently raised over $70 million through a token generating event (TGE) to develop a system for generation of the synthetic data using the deep neural networks trained on the large number of examples and using the processing power of the many graphics processing units (GPUs) in the blockchain community.\n\n\u201cHere at Neuromation, our mission is to help companies use our platform to carry out all their data analysis faster and cheaper than anywhere in the world,\u201d said Maxim Prasolov, the CEO of Neuromation. \u201cIn our opinion all industries, including healthcare, will benefit tremendously from a flexible, blockchain-based research platform. Our partnership with Longenesis represents this huge potential.\u201d Prasolov plans to quickly scale his company\u2019s platform by rewarding GPU miners for using Neuromation\u2019s computational power to mine this synthetic data.\n\nThe Neuromation plan to offer more accessible, affordable, and democratized AI analysis is as noble an aspiration as the Longenesis plan to help individuals take charge of their own personal data. Longenesis empower \u2014 instead of corporations \u2014 to store and monetize their own data, including their blood test results, medical history, genetic profile and other sensitive, increasingly valuable information.in a blockchain-based marketplace.\n\nBoth companies are developing revolutionary ecosystems using blockchain technologies.\n\n\u201cOur goal is to create a new, data-driven economy and we\u2019re excited to partner with Neuromation to move one step closer to our goal,\u201d said Alex Zhavoronkov, PhD, CSO of Longenesis Limited. \u201cOur collaboration will be two-fold. First, we\u2019ll train the deep neural networks we need processing power and we are looking to repurpose the cryptocurrency mining equipment to do that. Second, to generate synthetic data, we must train on large data sets of real-world data and have balanced and diverse data sets. Neuromation is an obvious consumer of data and can be a part of the marketplace.\u201d Added Prasolov, \u201cThe primary aim of Longenesis is to create a global data marketplace that will help provide personalized but at the same time secure data for healthcare application developers. Hence, the missions of Neuromation and Longenesis are perfectly complementary to each other. Together we will bring to AI practitioners in medicine and healthcare both the data and the computational power needed to train AI models and improve healthcare throughout the world. By announcing this partnership, we are laying the foundation for the future world of individualized healthcare based on artificial intelligence.\u201d\n\nA recent study suggests the Neuromation/Longenesis partnership will be well received in the healthcare industry. \u201cHealthcare Rallies for Blockchain\u201d, a study from IBM, found that 16% of surveyed healthcare executives plan to implement a commercial blockchain solution this year, while 56% expect to do so by 2020. The disruptive effect industry executives hope the technology will have on healthcare will arise from the creation of a common database of health information that doctors and providers can access from any electronic medical system. Healthcare data, protected in an encrypted, blockchain-based digital ledger, will deliver higher security and greater privacy for patients as well as less administrative work for doctors who can then can focus more on patient care. Blockchain principles even promise to foster better sharing of medical research which will, in turn, prompt new drug and treatment therapies to fight age-related disease.\n\nThe blockchain visionaries at Neuromation and Longenesis are confident that by bridging their ecosystems they\u2019ll be able to expand blockchain services to healthcare companies who never thought they could afford AI-driven research and individuals who never thought they would ever take control of their own personal data."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/the-neuromation-platform-is-live-ee32287d5949",
        "title": "The Neuromation Platform is Live! \u2013 Neuromation \u2013",
        "text": "As we promised, the first functional version is online and we hope you are excited to take part in it. Check out Neuromation\u2019s platform demo here.\n\nIn a record 2 months we have developed the product and kept our promise. It is young and will grow fast. Thank you for supporting us on the way. This is just the beginning.\n\nKey aspect of the platform \u2014 the open marketplace for synthetic datasets \u2014 will be activated in connection with the launch of the platform on February 15th. Buy, sell, and in the future, label the datasets, that will serve your business or research.\n\nCheck out our instruction video on Youtube!\n\nAs distributed computing is one of the key functions of the platform and the reflection of our vision to democratize the AI industry, we have put in a lot of effort over a very short period of time, between the close of our token sale and now on developing the miner\u2019s node. That was a difficult task, and our team has worked very hard to make it happen. As a result, we are now very proud to open registration for miners, who (after going through the selection process) will be able to sell computing power in exchange for NTK.\n\nIn order to register as a miner, please fill in the form here.\n\nIn the coming weeks we will be issuing several major partnership releases. Stay tuned on our social media channels, Medium, Facebook, and Telegram."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/the-activation-of-neurotokens-ntk-e925ba988996",
        "title": "The activation of Neurotokens (NTK)! \u2013 Neuromation \u2013",
        "text": "We are pleased to announce that the activation of NeuroTokens (\u201cNTK\u201d) will happen on schedule on February 15 at 11.59 pm GMT. At that time NTK will become transferable between wallets and as further described below, will be able to be used as a medium of exchange on the Neuromation platform. We appreciate great support of the community to get NTK listed on the major exchanges like HADAX, Huobi Autonomous Digital Asset, Tidex. Separate announcements will follow.\n\nThe NTK smart contract is available here.\n\nThe strength of NTK and the Neuromation platform will be determined in large, by the level of support and acceptance it receives from the proud AI community. You can support NTK getting listed on exchanges, by casting your vote here.\n\nAs mentioned above, when the Neuromation platform launches, NTK will be able to be used to purchase Neuromation platform services. More information about the use of NTK within the Neuromation platform can be found here.\n\nPlease remain vigilant against scammers and other shady people who may tell you they have alternate addresses where you can purchase NTK. Always confirm information through the official NTK website and check carefully for well disguised copycat accounts or sites. Read more about keeping your NTK safe in our Medium post.\n\nIn the coming weeks we will be issuing several major partnership releases. Stay tuned on our social media channels, Medium, Facebook, and Telegram."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-ama-session-with-top-team-2129820af06e",
        "title": "Neuromation AMA session with top team! \u2013 Neuromation \u2013",
        "text": "The platform is about to launch and NTK is about to be released on February 15th!\n\nWe are excited about this new step towards democratized AI! What about you? Get your critical thinking hat, because the Neurotoken team is coming in heavy!\n\nWe\u2019re giving you a chance to speak to all senior members of the community on February 15th.\n\nThe schedule is as follows. (GMT +2 timezone, Athens timezone) :\n\nand Chief Tech Officer Fedor Savchenko 12.00 PM until 01.00 AM.\n\nPut this in your calendar! Join the AMA here https://t.me/NeurotokenNTK and watch a livestream at Neuromation\u2019s Youtube Channel via this link https://www.youtube.com/watch?v=5hWTmyDNwzI!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/how-to-safely-store-your-ntk-55831291d35c",
        "title": "How to Safely Store Your NTK \u2013 Neuromation \u2013",
        "text": "Safety is our top priority for token holders. We care about our token holders and want to keep them secure. If you purchased NTK through our token sale or are considering purchasing NTK on the exchanges on February 15th, this article is for you. We\u2019ve put together a comprehensive guide to help you keep your NTK safe and secure.\n\nIf you participated in our crowdsale, then you probably already have a private wallet set up. Private wallets made it possible for you to contribute ETH to our sale and receive NTK at the same address. If you tried to contribute to our crowdsale using a wallet hosted on an exchange, you would likely run into difficulty redeeming the NTK tokens since your exchange owns your wallet and its associated keys.\n\nThe same issue applies for users hoping to purchase NTK in our upcoming exchange launch. When you use a wallet that an exchange provides to hold your NTK, you don\u2019t directly control that wallet. Instead, the exchange manages it, and the exchange controls access to it. Trusting your wallet access to a third-party exchange is inherently risky. Scandals in cryptocurrency\u2019s history, such as Mt. Gox, have shown us that keeping your tokens in an exchange wallet is vulnerable to hackers.\n\nA private wallet gives you complete control over your wallet\u2019s security. You and only you will have access to your private key. Once you have a private wallet set up, you\u2019ll be able to store your private key in any number of ways for future use. If you plan to purchase NTK on February 1st, you\u2019ll want to quickly transfer your funds out of the exchange wallet into a private wallet. Luckily, this is pretty easy to accomplish.\n\nThe first step to securing your NTK is creating a private Ethereum wallet, if you haven\u2019t already. This is a fairly straightforward process. You don\u2019t need any special skills or technical expertise.\n\nMyEtherWallet has gained a reputation for being the simplest, safest way to create a new Ethereum wallet. Head over to their website to get started with your first private wallet. It\u2019s a simple process:\n\nCongratulations! You\u2019ve just set up a private wallet. Doing so is the most important step you can take for the safety and security of your tokens.\n\nEven though what you\u2019ve created is called an Ether wallet, you\u2019ll be able to add NTK to this wallet because NTK is an ERC20 token. We\u2019ll see how to do that shortly.\n\nNTK will be available for trade on exchanges. We\u2019ll be adding support for more exchanges over time as well. Each exchange has its own user interface and methods of operation. As such, this article would be very long if we gave detailed instructions for buying and transferring tokens on each exchange. You\u2019ll need to read the documentation on the exchange you\u2019ve chosen if you\u2019re having trouble setting up a trade or transfer.\n\nThat said, there are some critical general pieces of advice you should follow when trading on any exchange.\n\nFirst, make sure you\u2019re trading on one of NTK\u2019s official listed exchanges. There are scammers who will claim to be able to sell you NTK. Many people fall victim to these false exchanges. The nature of blockchain technology makes it impossible to reverse a transaction. If you fall victim to a scam, there\u2019s no way to get your funds back. Double and triple check every website URL and company you deal with. On February 1st, the only official exchanges for trading NTK will be Yobit and Huobi.\n\nSecond, make sure you\u2019ve opted for the correct token when making your trade. There are hundreds of cryptocurrencies, and many of them are similarly named. You should ensure the token you\u2019re buying is called \u201cNeurotoken\u201d and its symbol is NTK.\n\nOnce you\u2019ve successfully purchased NTK, it will be credited to your exchange wallet. Since exchanges are subject to hacks and other types of attacks, you\u2019ll want to move your NTK out of your exchange wallet and into a private wallet as soon as you make the trade.\n\nTo do so, follow your exchange\u2019s procedures for creating a transfer. As part of the process, the exchange will ask for the address of your private wallet. This is not the same as your private key! You can find this address by logging into MyEtherWallet and looking for \u201cAccount Address\u201d on the right side of the page. Use this address to tell the exchange where to send the funds. Make sure you type your address perfectly. Triple check! If you send your NTK to the wrong address, you can\u2019t get them back.\n\nIt may take a little while for the transfer to process. Once it does, you won\u2019t immediately see the NTK in your private wallet. This is because MyEtherWallet doesn\u2019t know to look for NTK at your address. You\u2019ll need to tell it what to look for:\n\n3. If you move other altcoins over from your exchange wallet at the same time, you can look up their contract addresses and decimals on ethplorer.io\n\n4. The NTK amount in your private wallet will now appear on the right side of the page when you log into MyEtherWallet.\n\nCongratulations! Your NTK is now secure in a private paper wallet.\n\nThe final safety measure you\u2019ll need to take is to beware of phishing scams. Phishing involves posing as an authority and getting you to input sensitive details. For instance, there are phishing sites similarly named to MyEtherWallet. However, if you input your private key there, they will steal the contents of your wallet. Better yet, use a browser extension like MetaMask to avoid ever having to directly type sensitive information.\n\nAlways triple check the URL of the website you\u2019re using before inputting sensitive information. This applies for everything you do on the internet, not just cryptocurrency.\n\nWe\u2019re happy to welcome new owners of NTK token on February 15th to the Neuromation community! We hope this guide helps you to transfer and store your NTK securely. If you have questions or need help, don\u2019t hesitate to reach out on Telegram to get support from our community."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/there-is-no-competition-only-development-of-useful-tools-sergey-nikolenko-about-ai-in-medicine-de2a96298349",
        "title": "\u201cThere is no competition, only development of useful tools\u201d: Sergey Nikolenko about AI in medicine",
        "text": "Medical diagnostics is a very interesting case from the point of view of AI.\n\nIndeed, medical tests and medical images are data where one is highly tempted to train some kind of machine learning model. One of the first truly successful applications of AI back in the 1970s was, actually, medical diagnostics: the rule-based expert system MYCIN aggregated the knowledge of real doctors and learned to diagnose from blood tests better than an average doctor.\n\nBut even this direct application has its problems. Often a correct diagnosis must not only analyze the data from tests/images but also use some additional information which is hard to formalize. For example, the same lesion on the X-ray of the lungs of a senior chain smoker and a ten-year-old kid is likely to mean two very different things. Theoretically we could train models that make use of this additional information but the training datasets don\u2019t have it, and the model cannot ask an X-ray how many packs it goes through every day.\n\nThis is an intermediate case: you do have an image but it is often insufficient. True full-scale diagnostics is even harder: no neural network will be able to ask questions about your history that are most relevant for this case, process your answers to find out relevant facts, ask questions in such a way that the patient would answer truthfully and completely\u2026 This is a field where machine learning models are simply no match to biological neural networks in the doctors\u2019 heads.\n\nBut, naturally, a human doctor can still make use of a diagnostic system. Nobody can remember and easily extract from memory the detailed symptoms of all diseases in the world. An AI model can help propose possible diagnoses, evaluate their probabilities, show which symptoms fit a given diagnosis or not, and so on.\n\nThus, despite the hard cases, I personally think that the medical community (including, medical insurance companies, courts, and so on) is being overly cautious in this case. I believe that most doctors would improve their results by using automated diagnostic systems, even at the current stage of AI development. And we should compare them not with some abstract idealized notion of \u201calmost perfect\u201d diagnostic accuracy but with real results produced by live people; I think it would be a much more optimistic outlook.\n\nIndeed, many problems in medicine look very similar to computer vision tasks, and there is a large and ever growing field of machine learning for medical imaging. The models in that field are often very similar to regular computer vision but sometimes differences do arise.\n\nA lot depends on the nature of the data. For instance, distinguishing between a melanoma and a birthmark given a photo of a skin area is exactly a computer vision problem, and we probably won\u2019t have to develop completely novel models to solve it.\n\nBut while many kinds of medical data have spatial structure, they may be more complex than regular photos. For instance, I was involved in a project that processed imaging mass-spectrometry (IMS) datasets. IMS processes a section of tissue (e.g., from a tumor) and produces data which is at first glance similar to an image: it consists of spatially distributed pixels. But every \u201cpixel\u201d contains not one or three numbers, like a photo, but a long and diverse spectrum with thousands of different numbers corresponding to different substances found at this pixel. As a result, although this ``data cube\u2019\u2019 has clear spatial structure, classical computer vision models designed for photos are not enough, we have to develop new methods.\n\nThis field has always, since at least the 1990s, been a central application and one of the primary motivations for developing computer vision models.\n\nNowadays, such systems, together with the rest of computer vision, have almost completely migrated to convolutional neural networks (CNN); with the deep learning revolution CNNs have become the main tool to process any kind of images, medical included. Unfortunately, a detailed survey of this field would be too large to fit on the margins of this interview: e.g., a survey released in February 2017 contains more than 300 references, most of which appeared in 2016 and later \u2014 and a whole year has passed since\u2026\n\nIt is still a very long way to go before AI models are able to fully replace human doctors even in individual medical specialties. To continue the above example, there already exist computer vision models that can tell a melanoma apart from a moleno worse or even better than an average doctor. But the real problem is usually not to differentiate pictures but to persuade a person to actually come in for a checkup (automated or not) and to make the checkup sufficiently thorough. It is easy to take a picture of a suspicious mark on your arm, but you will never notice a melanoma, e.g., in the middle of your back; live doctors are still as needed as ever to do the checkup. But a model that would even slightly reduce the error rate is still absolutely relevand and necessary, it will save people\u2019s lives.\n\nA similar situation has been arising in surgery lately. Over the last couple of years, we have seen a lot of news stories about robotic surgeons that cut flesh more accurately, do less damage, and stitch up better than humans. But it is equally obvious that for many years to come, these robots will not replace live surgeons but will only help them save more lives, even if the robots learn to perform an operation from start to finish. It is no secret that modern autopilots can perform virtually the entire flight from start to finish-but it doesn\u2019t mean human pilots are moving out of the cabin anytime soon.\n\nMachine learning models and systems will help doctors diagnose faster and more accurately, but now, while strong AI has not yet been created, they definitely cannot fully replace human doctors. There is no competition, only development of useful tools. Medicine is a very imprecise business, and there always are plenty of factors that an automated system simply cannot know about. We are always talking only about computer-aided diagnosis (CAD), not full automation.\n\nOne company that does very serious research in Russia is Insilico Medicine, one of the world leaders in the development of anti-aging drugs and generally in the field of drug discovery with AI models. Latest results by Insilico include models that learn to generate molecules with given properties. Naturally, such models cannot replace clinical trials but they can narrow down the search from a huge number of all possible molecules and thus significantly speed up the work of \u201creal\u201d doctors.\n\nHere at Neuromation, we are also starting projects in biomedical AI, especially in fields related to computer vision. For instance, one of our projects is to develop smart cameras that will track sleeping infants and check whether they are all right, whether they are sleeping in a safe position, and so on. It is still too early to talk about several other projects, they are still at a very early stage, but we are certain something interesting will come out of them very soon. Biomedical applications are one of the main directions of our future development; follow our news!\n\nThis is a translation of Sergey Nikolenko\u2019s interview by Anna Khoruzhaya; see the Russian original on the NeuroNews website."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/newswatch-review-neuromation-synthetic-data-platform-for-deep-learning-applications-2b5f2b2e0de6",
        "title": "NewsWatch Review | Neuromation \u2014 Synthetic Data Platform for Deep Learning Applications",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-is-hiring-c9017322700b",
        "title": "Neuromation is hiring! \u2013 Neuromation \u2013",
        "text": "Hello colleagues! Today the post is unusual \u2014 we have a vacancy! Incidentally, I do not remember a vacancy at opendatascience slack gaining such stats. But the old-timers should know better.\n\ntl-dr: St. Petersburg, deep learning in computer vision, cool projects, 120\u2013250K, for stars there can be more. In more detail.\n\nRecently, the Neuromation company (https://neuromation.io/en/), in which I am the Chief Research Officer, has completed the token sale:\n\nWe have collected a lot of money and plan to use it to organize new labs for both industrial projects and breakthrough deep learning studies. In particular, we are opening a division of Neuromation Labs in St. Petersburg under my leadership. So we are hiring people there.\n\nIn this lab, we plan to conduct various deep learning projects, but our focus is computer vision based on synthetic data. You can read, for example, my posts on our first big retail project and the Neuromation platform that we are also planning to build:\n\nAnd here are examples of typical projects that we will do:\n\n\u2014 see the third part, on piglets:\n\n\u2014 and here also the third part, on MonBaby:\n\nThe main thing that is required of you is to have confident expertise in machine learning and modern neural network models, both theoretically and practically. The desirable level of candidates is \u201cI can read a modern article on deep learning and implement the model from the article, even if there was no ready-made code on the GitHub.\u201d Specific libraries are not so important at this level, but obviously, today these are, most likely, PyTorch or TensorFlow/Keras.\n\nExperience is in computer vision, especially object detection, segmentation, video processing is desirable, but we can talk even if you were doing something else. In addition, an important additional advantage will be familiarity with the \u201cclassic\u201d computer vision, experience in OpenCV development, as well as academic experience (publications).\n\nSalary bracket is at Rubles 120\u2013250K per month (I think, most likely $3K\u20134K) subject to the interview results. Other terms to be discussed, but generally we are looking for candidates for full-time work in a small office in St. Petersburg. With really stellar candidates, we will always come to understanding and agreement on individual basis. For you it is a unique chance to join a potentially very powerful company, as they say, on the ground floor. By the way, bonuses/tokens/options are also possible, as we are still a startup.\n\nWe will also be happy to see talented juniors in our ranks. If you are interested in growing and developing in deep learning, you are welcome. Write me about yourself, your education and experience in neural networks. For such a candidate, general mathematic culture (probability theory, mathematical analysis, linear algebra), command of Python, basic data processing and machine learning experience, and most importantly \u2014 a willingness to study and work on yourself, is required. Experience in computer vision, command of OpenCV, etc. will be a great advantage.\n\nContact me right here, alternatively at snikolenko@gmail.com or Telegram @snikolenko. Please like, share, repost!\n\nP.S. If you already work in AI start-up and do not want to change your place of work, we can still find something to talk about \u2014 10% of our token sale goes to the grants program for AI startups with interesting and cool deep learning ideas support:\n\nYou are welcome to contact!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/taas-contributes-300-eth-to-neuromation-the-universal-marketplace-of-neural-network-development-2f00a531d1bd",
        "title": "TaaS Contributes 300 ETH to Neuromation, the Universal Marketplace of Neural Network Development",
        "text": "Token-as-a-Service (TaaS), the first-ever tokenized closed-end fund, has contributed to Neuromation, a platform of distributed computing for AI applications, that uses the computing power of cryptominers to democratize the AI industry.\n\nCombining the new digital frontiers of Blockchain and AI, Neuromation is spreading technological synergy to industrial and trade processes. Neuromation offers a revolutionary solution to unite distributed computing power with the needs of thousands of in-depth learning projects and b2b clients all over the world. Using this complex solution, Neuromation has already achieved significant progress in its Retail Lab, providing image recognition services to major retail brands in cooperation with OSA HP. Other b2b service projects include MonBaby \u2014 producers of smart cameras for tracking infant behaviour in the crib, and AxxonSoft \u2014 a video surveillance and security services provider.\n\nThis contribution comes shortly after TaaS and Neuromation concluded the strategic agreement. This partnership will see Neuromation gaining advanced access to crypto-markets, and challenge TaaS and Neuromation to explore new ways to enhance and strengthen synergy in the blockchain space.\n\nThe Neurotoken pre-sale ended on January 1st, 2018 and raised over 39,000 ETH (an equivalent of $39 million) for the 53 Million Neurotokens (NTK) allocated. TaaS joined the pre-sale with the contribution of 300 ETH. The main Token Sale started on January 7th, 2018 and the hard cap of 60 Million NTK was reached in less than 4 hours.\n\nThis marks TaaS Fund\u2019s fourth Token Generation Event (TGE) contribution during the third fully-operational quarter, ended on Jan 1st, 2018. During this quarter, TaaS contributed 350 ETH to DMarket, the first cross-game platform on the blockchain, 210 ETH to SmartOne, a marketplace for legal solutions for blockchain-based enterprise, and 350 ETH to DreamTeam, blockchain-based all-in-one solution to build, grow, manage, and monetize Esports teams. TaaS also contributed to OSA Hybrid Platform, Neuromation, Hacken, Native Protocol, Persona and TheMine and InsurePal, which were the top winners of the ICO Pitch Competitions organized by d10e in Davos (Nov 14) and Bucharest (Dec 15).\n\nDuring the second fully-operational quarter, ended on November 1st, TaaS contributed a total of 1 million USD(T) equivalent to 6 TGEs, generating capital gains of 5.5 million USD(T) equivalent while producing a 72% ROA over this time. TaaS also made contributions to BlockV, Hacken, and Ripio Credit Network, which were the top winners of the ICO Pitch Competitions organized by d10e in Kyiv on Sept 19.\n\nDuring the first fully-operational quarter, ended on August 1st, TaaS contributed a total of 3 Million USD(T) equivalent to 11 TGEs, generating a 61% ROA. The TaaS Token is currently trading on Livecoin, Orderbook, EtherDelta, HitBTC, Liqui and CoinExchange at the price of 11.23 USD(T)equivalent per TAAS."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/%D0%B2%D0%B0%D0%BA%D0%B0%D0%BD%D1%81%D0%B8%D1%8F-%D0%B2-neuromation-d617b5cb5f77",
        "title": "\u0412\u0430\u043a\u0430\u043d\u0441\u0438\u044f \u0432 Neuromation! \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/what-will-the-future-bring-ba0aa5f24d50",
        "title": "What Will The Future Bring? \u2013 Neuromation \u2013",
        "text": "Here at Neuromation, we are always looking forward to the future. Actually, we are making the future. Thus, we are in a good position to try to predict what is going to happen with the AI industry soon. I know how difficult it is to make predictions, especially, as some Danish parliament member once remarked, about the future. Especially in an industry like ours. Still, here go my three predictions, or, better to say, three trends that I expect to continue for the next few years of AI. I concentrated on the technical side of things \u2014 I hope the research will stay as beautifully unpredictable as ever.\n\nOne trend which is already obvious and will only gain in strength in the future is the rise of specialized hardware for AI. The deep learning revolution started in earnest when AI researchers realized they could train deep neural networks on graphical processors (GPUs, video cards). The idea was that training deep neural networks is relatively easy to parallelize, and graphic processing is also parallel in nature: you apply shaders to every pixel or every vertex of a model independently. Hence, GPUs have always been specifically designed for parallelization: a modern GPU has several thousand cores compared to 4\u20138 cores in a CPU (CPU cores are much faster, of course, but still, thousands). In 2009, this turned every gaming-ready PC into an AI powerhouse equal to the supercomputers of old: an off-the-shelf GPU trains a deep neural network 10\u201330x faster than a high-end CPU; see, e.g., an up-to-date detailed comparison here.\n\nSince then, GPUs have been the most common tool for both research and practice in deep learning. My prediction is that over the next few years, they will be gradually dethroned in favour of chips specifically designed for AI.\n\nThe first widely known specialized chips for AI (specifically training neural networks) are proprietary Google TPUs. You can rent them on Google Cloud but they have not been released for sale for the general public, and probably won\u2019t be.\n\nBut Google TPU is just the first example. I have already blogged about recent news from Bitmain, one of the leading designers and producers of ASICs for bitcoin mining. They are developing a new ASIC specifically for tensor computing \u2014 that is, for training neural networks. I am sure that over the next few years we will see many chips designed specifically that will bring deep learning to new heights.\n\nThe second prediction sounds like an oxymoron: AI research and practice will centralize and decentralize at the same time. Allow me to explain.\n\nIn the first part, we talked about training neural networks on GPUs. Since about 2009, deep learning has been living in \u201cthe good old days\u201d of computing when you can stay on the bleeding edge of AI research with a couple off-the-shelf GPU for $1000 each in your garage. These days are not past us yet, but it appears that soon they will be.\n\nModern advances in artificial intelligence are more and more dependent on computational power. Consider, e.g., AlphaZero, a deep reinforcement learning model that has recently trained to play chess, go, and shogi better than the best engines (not just humans! AlphaZero beat Stockfish in chess, AlphaGo in go and Elmo in shogi) completely from scratch, knowing only the rules of the game. This is a huge advance, and it made all the news with the headline \u201cAlphaZero learned to beat Stockfish from scratch in four hours\u201d.\n\nIndeed, four hours were enough for AlphaZero\u2026 on a cluster of 5000 Google TPUs for generating self-play games and 64 second-generation TPUs to train the neural networks, as the AlphaZero paper explains. Obviously, you and I wouldn\u2019t be able to replicate this effort in a garage, not without some very serious financing.\n\nThis is a common trend. Research in AI is again becoming more and more expensive. It increasingly requires specialized hardware (even if researchers use common GPUs, they need lots of them), large datacenters\u2026 all of the stuff associated with the likes of Google and Facebook as they are now, not as they were when they began. So I predict further centralization of large-scale AI research in the hands of cloud-based services.\n\nOn the other hand, this kind of centralization also means increased competition on this market. Moreover, the costs of computational power have been recently rather inflated due to super demand on the part of cryptocurrency miners. We tried to buy high-end off-the-shelf GPUs last summer and utterly failed: they were completely sold out for those who were getting into mining ethereum and litecoins. However, this trend is coming to an end too: mining is institutionalizing even faster, returns on mining decrease exponentially, and the computational resources are beginning to free up as it becomes less and less profitable to use them.\n\nWe at Neuromation are developing a platform to bring this computational power to AI researchers and practitioners. On our platform, you will be able to rent the endless GPUs that had been mining ETH, getting them cheaper than anywhere else but still making a profit for the miners. This effort will increase competition on the market (currently you go either to Amazon Web Services or Google Cloud, there are very few other solutions) and bring further democratization of various AI technologies.\n\nBy the way, speaking of democratization. Machine learning is a very community-driven area of research. It has unprecedented levels of sharing between researchers: it is common practice to accompany research papers with working open source code published on Github, and datasets, unless we are talking about sensitive data like medical records, are often provided for free as well.\n\nFor example, modern computer vision based on convolutional neural networks almost invariably uses a huge general purpose dataset called ImageNet; it has more than 14 million images hand-labeled into more than 20 thousand categories. Usually models are first pretrained on ImageNet, which lets them extract low-level features common for all photos of our world, and only then train it further (in machine learning, it is called fine-tuning) on your own data.\n\nYou can request access to ImageNet and download it for free, but what is even more important, the models already trained on ImageNet are commonly available for the general public (see, e.g., this repository). This means that you and I don\u2019t have to go through a week or two of pretraining on a terabyte of images, we can jump right into it.\n\nI expect this trend to continue and be taken further by AI researchers in the near future. Very soon, a lot of \u201cbasic components\u201d will be publicly available, and an AI researcher will be able to work with and combine directly, without tedious fine-tuning. This will be partially a technical process of making what we (will) have easily accessible, but it will also require some new theoretical insights.\n\nFor example, a recent paper from DeepMind presented PathNet, a modular neural architecture able to combine completely different sub-networks and automatically choose and fine-tune a combination of these sub-networks most suitable for a given task. This is still a new direction, but I expect it to pick up.\n\nAgain, we at Neuromation plan to be on the cutting edge: in the future, we plan to provide modular components for building modern neural networks on our platform. Democratization and commoditization of AI research is what Neuromation platform is all about."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-organizes-blockchain-and-ico-investing-talk-panel-discussion-910e7d76400d",
        "title": "Neuromation Organizes Blockchain and ICO Talk & Panel Discussion.",
        "text": "Neuromation delivers knowledge to the community in the Silicon Valley. Check out what happened during the Blockchain and ICO Investing event on February 1st.\n\nThis event covered topics from the basic building blocks of blockchains, to decentralized autonomous organizations and tokens, as well as the US authorities\n\nChristian Kameir, who led the merger of two of Europe\u2019s first internet service providers spoke at the event among other industry insiders such as- Andrii Zamovsky who\u2019s the Founder and chairman of Ambisafe, Philipp Pieper the CEO & Cofounder Swarm Fund, Yuri Kundin the Chief Operations Officer at Neuromation and Lev Shoykhet who is the Managing Director of Citco Fund Services."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/important-announcement-from-neuromation-team-867261972798",
        "title": "Important Announcement from Neuromation Team! \u2013 Neuromation \u2013",
        "text": "There was an announcement on Yobit social media and website that NTK was listed in Yobit exchange. This is the result of community efforts only. Since tokens are locked in till February 15th, no trade orders will be executed. Please, follow Neuromation own channels for official announcements."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/can-a-neural-network-read-your-mind-5d3901e1dd19",
        "title": "Can a Neural Network Read Your Mind? \u2013 Neuromation \u2013",
        "text": "Researchers from the ATR Computational Neuroscience Labs at Kyoto and Kyoto University have recently made the news. Their paper, entitled \u201cDeep image reconstruction from human brain activity\u201d,(released on December 30, 2017), basically claims to have developed a machine learning model that can read your mind, with sample reconstructions shown in the picture above. To understand what they mean and whether we should all be thinking only happy thoughts now, we need to start with a brief background.\n\nA big problem with neural networks has always been their opacity: while we can see the final results, it is very hard to understand what exactly is going on inside a neural network. This is a problem for all architectures, but let us now concentrate on convolutional neural networks (CNNs) used for image processing.\n\nVery roughly speaking, CNNs are multilayer (deep) neural networks where each layer processes the image in small windows, extracting local features. Gradually, layer by layer, local features become global, being able to draw their inputs from a larger and larger portion of the original image. Here is how it works in a very simple CNN (picture taken from this tutorial, which I recommend to read in full):\n\nIn the end, after several (sometimes several hundred) layers we get global features that \u201clook at\u201d the whole original image, and they combine in some ways to get us the class labels (recognize whether it is a dog, cat, or a boat). But how do we understand what these features actually do? Can we interpret them?\n\nOne idea is to simply look for the images that activate specific neurons with the hope that they will have something in common. This idea was developed, among other works, in the famous paper \u201cVisualizing and Understanding Convolutional Networks\u201d by Zeiler and Fergus (2013). The following picture shows windows from actual images (on the right) that provide the largest possible activations for four different high-level neurons together with their pixels that contribute to these activations; you can see that the procedure of fitting images to features does produce readily interpretable pictures:\n\nBut then researchers developed another simple but very interesting idea that works well to understand the features. The whole training process in machine learning is designed to fit the features of a network into a training dataset of images. The images are fixed, and the network weights (parameters of these convolutional layers) are changing. But we can also do it the other way around: fix the network and change the image to fit what we need!\n\nFor interpretation, this idea was developed, e.g., in the work \u201cUnderstanding Neural Networks Through Deep Visualization\u201d (2015) by Jason Yosinski et al. The results look a lot like the famous \u201cdeep dreams\u201d, and this is no coincidence; for example, here are the images designed to activate certain classes the most:\n\nSomewhat recognizable but pretty strange, right? We will see similar effects in the \u201cmind-reading\u201d pictures below.\n\nThe same idea also leads to the field of adversarial examples for CNNs: now that we have learned to fit images to networks rather than the other way around, what if we fit the images to fool the network? This is how you get examples like this:\n\nOn the left, this picture shows a \u201cbenign\u201d image, labeled and recognized as bottlecap. On the right, an adversarial image: you can\u2019t see any noticeable difference but the same network that achieved great results in general and correctly recognized the original as a bottlecap now confidently says that it is a\u2026 toy poodle. The difference is that on the right, an adversary has added small carefully crafted perturbations that are small and look just like white noise but that are all designed to push the network in the direction of the toy poodle class.\n\nThese examples, by the way, show that modern convolutional neural networks still have a long way to go before solving computer vision once and for all: although we all know some optical illusions that work for humans, there are no adversarial examples that would make us recognize a two-dimensional picture of a bottlecap as a toy poodle. But the power to fit images to features in the network can also be used for good. Let us see how.\n\nSo what did the Japanese researchers (they are the mind readers we began with) do, exactly? First, they took the fMRI of the brain of a human looking at something and recorded the features. Functional magnetic resonance imaging (fMRI) is a medical imaging technique that is taking snapshots of the brain activity based on the blood flow: when neurons in an area of the brain are active, more blood flows there, and we can measure it; fMRI is called functional because it can capture changes in blood flow in time, resulting in videos of brain activity. To get more information you can watch this explanatory video or see a sample dataset for yourself:\n\nSince we are measuring blood vessels and not neurons, the spatial resolution of fMRI is not perfect: we can\u2019t go down to the level of individual neurons but we can distinguish rather small brain areas, with voxel size about 1mm in each direction. It has been known for a long time that the fMRI picture contains reliable general information regarding what the person is thinking about in the scanner: emotions, basic drives, processing different inputs such as speech, music, or video etc\u2026 but the work of Shen et al. takes this to a whole new level.\n\nShen et al.(2017) tried to reconstruct the exact images that people in fMRI scanners were looking at. To do that, they trained a deep neural network on fMRI activity results and then tried to match the features of a new image with the features of fMRI activations. That is, they are basically doing the same thing that we discussed above: finding an input image that matches given features as well as possible. The only difference is that the features now come not from a real image processed by the CNN but from an fMRI processed by a different network (also convolutional, of course). You can see how the network gradually fits the image to a given fMRI pattern:\n\nThe authors improved their reconstruction results drastically when they added another neural network, the deep generator network (DGN), whose work is to ensure that the image looks \u201cnatural\u201d (in technical terms, introducing a prior on the images that favors \u201cnatural\u201d ones). This is also an important idea in machine learning: we often can make sense of something only because we know what to expect, and artificial models are no different: they need some prior knowledge, \u201cintuition\u201d about what they can and cannot get, to improve their outputs.\n\nIn total, here is what the architecture looks like. The optimization problem is to find an image which best fits both the deep generator network that makes sure it is \u201cnatural\u201d and the deep neural network that makes sure it matches fMRI features:\n\nIf these results can be replicated and further improved, this is definitely a breakthrough in neuroscience. One can even dream about paralyzed people communicating through fMRIs by concentrating on what they want to say; although in (Shen et al., 2017) reconstruction results are much worse when people are imagining a simple shape rather than directly looking at it, sometimes even with imagined shapes it does seem that there is something there:\n\nSo can a neural network read your mind now? Not really. You have to lie down in a big scary fMRI scanner and concentrate hard on a single still image. And even in the best cases, it still often comes out like this, kind of similar but not really recognizable:\n\nStill, this is a big step forward. Maybe one day."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-competition-is-on-508f9ff5f1ae",
        "title": "Neuromation competition is on! \u2013 Neuromation \u2013",
        "text": "Dear Neuromation community, we value your feedback and interaction with us and believe that among you, there are many talented entrepreneurs, scientists and tech minds; therefore we have decided to run a competition for you. In neurotoken chat, https://t.me/NeurotokenNTK, brainstorm ideas about the industrial implementation of Neuromation technology. The best submission will win a 30 minute private AMA session with Maxim Prasolov and Constantine Goltcev, in Skype ! Have an idea to present ? This is your chance !\n\nHere\u2019s what you need to do to enter-\n\nFirst, think of an original way Neuromation technology can be used in modern industry. Make sure it\u2019s something we haven\u2019t thought about yet!\n\nSubmit your idea in the chat ONLY, making sure to use the hashtag #compete. Submissions sent by other means, eg email, will be ignored.\n\nThe deadline for the competition submissions is Monday, 29th January, at 12 PM GMT\n\nGet your thinking caps on and good luck!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/innovations-with-ed-begley-jr-to-explore-blockchain-technology-solutions-a72d10246b12",
        "title": "Innovations with Ed Begley, Jr. to Explore Blockchain Technology Solutions",
        "text": "The award-winning Innovations w/Ed Begley Jr. series will focus on recent breakthroughs in blockchain technology and how it is being used to better Artificial Intelligence (AI) training. This episode is scheduled to air during first quarter 2018 on FOX Business. Check your local listings for more information.\n\nDue to technological advancements coupled with progress in research and innovation, AI is quickly becoming a part of most companies\u2019 operations. However, this progress also brings its own challenges.\n\nFocusing on Neuromation in this segment, the show will enlighten on how the Neuromation Platform, which offers a unique solution that unites market resources, the scientific community, and commercial and private entities into an integrated marketplace.\n\n\u201cNeuromation uses synthetic data to train models, which not only results in extremely accurate datasets, but it reduces costs by 10 fold. We have already obtained excellent object recognition results on both synthetic test sets and test sets with real photographs. Moving forward, we plan to improve synthetic data-generation pipelines with machine learning algorithms, leading to a true active learning framework\u201d \u201cNeuromation is extremely happy to be participating in the Innovations series, giving us the opportunity to share our leading technology with viewers.\u201d\n\nThe show will educate on the technology behind the Neuromation Platform, which uses distributed computing along with blockchain proof of work tokens to revolutionize AI model development.\n\n\u201cNeuromation focuses on synthetic datasets that have been proven to yield compelling results,\u201d said Michael Devine, Senior Producer for the Innovations series. \u201cWe look forward to exploring how using synthetic datasets in machine learning will further decrease the cost and ease of widespread AI adoption.\u201d\n\nNeuromation\u2019s mission is to collect the largest distributed computational pool for useful computing that would be used to solve the fundamental tasks facing the deep learning community, thereby democratizing the industry and providing access even to projects and laboratories with small budgets. These tasks are of practical importance for many industries and will be implemented through the Neuromation laboratories (Retail Lab, Bio/Pharma Lab, Enterprize Lab). Members of the deep learning community have come together on the Neuromation platform in order to solve fundamental industrial challenges with leading scientists- David Orban and Andrei Rabinovich, onboard as advisers.\n\nInnovations, hosted by award winning actor Ed Begley, Jr., is an information-based series geared toward educating the public on the latest breakthroughs in all areas of society. Featuring practical solutions and important issues facing consumers and professionals alike, Innovations focuses on cutting-edge advancements in everything from health and wellness to global business, renewable energy, and more.\n\nDMG Productions (responsible for creating the Innovations show) includes personnel specialized in various fields from agriculture to medicine, independent films to regional news and more. Field producers work closely with experts in the field to develop stories. This powerful force enables DMG to consistently produce commercial-free, educational programming that both viewers and networks depend on.\n\nFor more information visit www.InnovationsTelevision.com or call (866) 496\u20134065."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/ai-in-biology-and-medicine-784a9b901b3e",
        "title": "AI in Biology and Medicine \u2013 Neuromation \u2013",
        "text": "Translated from Russian by Andrey V. Polyakov. Original article here.\n\nToday I present to you three research directions that apply the latest achievements in artificial intelligence (mostly deep neural networks) to biomedical applications. Perhaps, this is the research that will not only change but also significantly extend our lives. I am grateful to my old friend, co-author, and graduate student Arthur Kadurin who has suggested some of these projects.\n\nWe begin with a series of projects that are unlikely to turn the world over but will certainly produce, pardon the pun, cosmetic changes in everyday life already in the nearest future. These projects deal with AI applications for the so-called \u201cInternet of Things\u201d (IoT), specifically applications that are very \u201cclose to the body\u201d.\n\nVarious types of fitness trackers, special bracelets that collect information about heartbeat, steps, and so forth, have long ago entered our life. The main trend at the sportswear companies now is to build different sensors directly into the clothes. That way, you can collect more information and measure it more precisely. Sensors suitable for \u201csmart clothes\u201d were invented in 2016, and already in 2017 Polar has presented Polar Team Pro Shirt, a shirt that collects lots of information during exercises. The plot will no doubt thicken even further when sports medicine supported by artificial intelligence will learn to use all this information properly; I expect a revolution in sports that Moneyball could never dream of.\n\nAnd it is already beginning. Recently, on November 24\u201326, the second SkinHack hackathon dedicated to applying machine learning models to analyzing data coming from such sportswear took place in Moscow. The first SkinHack held last year was dedicated to \u201csmart cosmetics\u201d; the participants tried to predict the age of a person by the skin structure on photographs looking for wrinkles. Both smart cosmetics and smart clothing are areas of active interest for Beiersdorf AG (commonly known as the producer of the Nivea brand), so one can hope that the commercial launch of these technologies will be not long in coming. In Russia, SkinHack was supported by Youth Laboratories, a company affiliated with the central characters of our next part\u2026\n\nInsilico Medicine is a company well known in the biomedical world. Its primary mission is to fight aging, and I personally wish Insilico success in this effort: one does not look forward to growing old. However, in this article I would like to emphasize another, albeit related, project of the company: drug discovery based on artificial intelligence models.\n\nA medicinal drug is a chemical compound that can link with other substances in our body (usually proteins) and have the desired effect on them, e.g., suppress a protein or start producing another in larger quantities. To find a new drug, you need to choose from a huge number of possible chemical compounds exactly the ones that will have the desired effect.\n\nIt is clear that at this point it is impossible to fully automate the search for new drugs: clinical trials are needed, and one usually starts testing on mice, then on humans\u2026 in general, the process of bringing a new medicinal drug to the market usually takes years. However, one can try to help doctors by reducing the search space. Insilico develops machine learning models that try not only to predict the properties of a molecule but also to generate candidate molecules with desired properties, thereby helping to choose the most promising candidates for further laboratory and clinical studies.\n\nThis search space reduction is done with a very interesting class of deep learning models: generative adversarial networks (GAN). Such networks combine two components: a generator trying to generate new objects \u2014 for example, new molecules with desired properties \u2014 and a discriminator trying to distinguish generated results from real data points. Learning to deceive the discriminator, the generator begins to generate objects indistinguishable from the real ones\u2026 that is, hopefully, actually real ones in this case. The last Insilico model, called druGAN (drug + GAN), attempts to generate, among others, molecules useful for oncological needs.\n\nFinally, I would like to end with a project that Neuromation plans to participate in. Small children, especially babies, cannot always call for help themselves and require special care and attention. This attention is sometimes required even in situations where mom and dad seem to be able to relax: for example, a sleeping baby may hurt a leg by turning to an uncomfortale pose . And then there is the notorious SIDS (sudden baby death syndrome), whose risk has been linked with the pose of a sleeping infant: did you know that the risk of SIDS increases several times if a baby sleeps on the stomach?\n\nThe MonBaby smart infant tracking system is a small \u201cbutton\u201d that snaps onto clothing and monitors the baby\u2019s breathing and turning around while asleep. Currently, the system is based on machine learning for time series analysis: data from baby movements is used to recognize breathing cycles and sleeping body position (on the stomach or on the back).\n\nWe plan to complement this system with smart cameras able to track the infant\u2019s movements and everything that happens to him or her by visual surveillance. The strong suits of our company will come in handy here: computer vision systems based on deep convolutional networks and synthetic data for their training. The fact is that in this case it is practically impossible to collect a sufficiently large real data set for training the system: it would take not only real video recordings of tens of thousands of babies, but video recordings with all possible critical situations. Thankfully, modern ethics, both medical and human, would never allow us to generate such datasets in real life. Therefore, we plan to create \u201cvirtual babies\u201d, 3D models that will allow us to simulate the necessary critical situations and generate synthetic videos for training.\n\nWe have briefly examined three directions in different branches of biomedicine \u2014 sports medicine and cosmetics, creating medicines and baby care \u2014 each of which is actively using the latest achievements of artificial intelligence. Of course, these are just examples: AI is now used in hundreds of diverse biomedical projects (which we may touch upon in later articles). Hopefully, however, with these illustrations I have managed to show how AI research is working on helping people live longer, better, and healthier."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-story-from-synthetic-data-to-knowledge-mining-bff28a19eb62",
        "title": "Neuromation Story: From Synthetic Data to Knowledge Mining",
        "text": "Sergey Nikolenko, Chief Research Officer at Neuromation, has shared with BS how artificial intelligence and neural networks help saving manual labor, and why it is increasingly more efficient to use mining farms for data processing rather than cryptocurrency mining.\n\nMy name is Sergey Nikolenko, and I am writing this as Neuromation\u2019s Chief Research Officer. Our company is based on two main ideas, and there is an interesting story of how they followed from one another. In my opinion, this story reflects upon the two main problems to be solved in any applied machine learning project today. It is this story that I will tell you today.\n\nNeuromation began with working on computer vision models and algorithms based on deep neural networks (deep learning). The first big project for Neuromation was in the field of retail: recognize the goods on supermarket shelves. Modern object detection models are quite capable to analyze shelf availability, find free space on the shelves, and even track human interaction. This is an important task both for supermarkets and for the suppliers themselves: the big brands pay good money to ensure that their goods are present on the shelf, occupy some agreed upon part of the shelf, have the right side of the label facing the customers \u2014 all of these little things increase sales by dozens of percent. Today, a huge staff of merchandisers are going from supermarket to supermarket, ensuring that everything is right on the shelves; of course, not all of their duties are \u201cmonkey jobs\u201d like this, but it is a big part of the day for many real human beings.\n\nOur idea for retail is to install (cheap off-the-shelf) cameras that can capture and transmit to a server, for example, one frame per minute for recognition. This is a very low frequency for an automatic system, causing no overload on either the network or the recognition model, but it is a frequency completely unattainable with manual checks, and it solves all practical problems in retail. Moreover, an automated surveillance system will save a lot of effort, automate meaningless manual labor \u2014 a worthwhile goal in itself.\n\nA specialist in artificial intelligence, especially modern deep neural networks for computer vision, might think that this problem is basically solved already. Indeed, modern deep neural networks, trained on large sets of labeled data, can do object detection, and in this case, the objects are relatively simple: cans, bottles,packages with bright labels. Of course, there are a lot of technical issues (for example, it is not easy to cope with hundreds of products on one photo \u2014 usually such models are trained to detect fairly large objects, only a few per image), but with a sufficiently large labeled data set, i.e. photos with all goods labeled in the layout, we could successfully overcome such issues.\n\nBut where would such labeled dataset come from? Imagine that you have a million photos of supermarket shelves (where to get it, by the way, is also a hard question), and you need to manually draw such rectangles as on the image above, on each one of a million photos. Looks like a completely hopeless task. So far, manual labeling of large sets of images has been usually done with crowdsourcing services such as Amazon Mechanical Turk. Manual work on such services is inexpensive, but it still does not scale well. We have calculated that to label a dataset sufficient for recognizing all 170,000 items from the Russian retail inventory (a million photos, by the way, would not be enough for this) we would need years of labor and tens of millions of dollars.\n\nThus, we faced the first major challenge, the main \u201cbottleneck\u201d of modern artificial intelligence: where do you get labeled data?\n\nSynthetic Data and the Second Challenge: Computing Power\n\nThis problem led to Neuromation\u2019s first major idea: we decided to try to train deep neural networks for computer vision on synthetic data. In the retail project, this means that we create 3D models of goods and virtually \u201cplace them on the shelves\u201d, getting perfectly labeled data for recognition.\n\nOf course, this approach is not perfect either. Now you have to train networks on one type of data (renderings) and then apply them to a different one (real photos). In machine learning, this is called transfer learning; it is a hard problem in general, but in this case we have been able to solve it successfully. Moreover, we have learned to produce very good photorealistic renderings \u2014 our retail partners even intend to use them in media materials and catalogs.\n\nThe synthetic data approach has proved to be very successful, and now the models trained by Neuromation are already being implemented in retail. However, this led to the need to process huge datasets of synthetic images. First, they have to be generated (i.e., one has to render a 3D scene), and then used to train deep neural networks. Generating one photorealistic synthetic image \u2014 like the one shown above \u2014 usually takes a minute or two on a modern GPU, depending on the number of objects and the GPU model. And you need a lot of these images: millions if not tens of millions.\n\nAnd this is only the first step \u2014 then we have to train modern deep neural networks on these images. In AI research, it is not enough to train a model once: you have to try many different architectures, train dozens of different models, conduct hundreds of experiments. This, again, requires cutting edge GPUs, and training deep networks requires even more computational time than data generation.\n\nThus, we at Neuromation have faced the second major challenge of modern artificial intelligence: where do we get computing power?\n\nOur first idea was, of course, to simply purchase a sufficient number of GPUs. However, it was the summer of 2017, the midst of the cryptocurrency mining boom. It turned out that graphic cards with the latest NVIDIA chips are not just expensive, but they are virtually unavailable at all. After we had tried to \u201cmine\u201d for some GPUs through our contacts in the US and realized that this way they would arrive only in a month or more, we switched to plan B.\n\nPlan B involved using cloud services that rent out complete and set-up machines (often virtual ones). A cloud especially popular with AI practitioners is Amazon Web Services. AWS has become a de-facto industry standard, and many new AI startups are renting computing power there for their development tasks. However, cloud-based services do not come cheap: renting a machine with several GPUs for training neural networks costs a few dollars per hour, and you need a lot of these hours.\n\nWe at Neuromation have spent thousands of dollars renting computational power on Amazon \u2014 only to understand that we do not have to use them. The prices of cloud-based services are acceptable for the buyers, only in the absence of other alternatives.\n\nAnd when we started thinking about potential alternatives, we recalled the reason we could not buy enough high-end GPUs. This led to the second main idea of Neuromation: repurposing GPU-based mining rigs for useful computing. ASIC chips that have been designed specifically for Bitcoin mining are not suitable for any other computing tasks, but GPUs that are used to mine Ethereum (ETH) and other \u201clightcoins\u201d, are the exact same GPUs we need to use to train neural networks. Moreover, cryptocurrency mining generates an order of magnitude less income than the clouds charge for renting an equivalent GPU farm for the same period.\n\nWe realized that there is a very powerful business opportunity \u2014 a huge gap between prices \u2014 and also simply an opportunity to make the world better, redirecting the vast resources that are currently searching for collisions in hash functions to more useful calculations.\n\nThis is how the idea of the Neuromation platform was born: an universal marketplace for knowledge mining that would connect miners who want to earn more on their equipment and customers and AI startups, researchers, and basically any companies that need to process large datasets or train modern machine learning models.\n\nNow we are already working with several mining farms, using their GPUs for useful computing. This is 5 to 10 times cheaper than renting server capacity from cloud-based services, and even at that price it is still much more profitable for miners. With their GPU-based rigs, miners can earn 3 to 5 times more by \u201cknowledge mining\u201d than they would get from the same setup by cryptocurrency mining. Taking into account that the complexity of calculations for cryptocurrency mining is growing with each new coin, the benefits of \u201cknowledge mining\u201d will only increase with time.\n\nRight now we are presenting the idea of this universal platform for useful computing to the global market. The use of mining rigs for useful computing benefits both parties: miners will earn more, and numerous artificial intelligence researchers and entrepreneurs will receive a considerably (several times) cheaper and convenient way to implement their ideas. We believe that such \u201cAI democratization\u201d will lead to new breakthroughs and, ultimately, fuel the current revolution in artificial intelligence. Join us, and welcome to the revolution!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/make-man-in-our-image-through-the-black-mirror-55eccf9d1ffe",
        "title": "Make Man In Our Image: Through the Black Mirror \u2013 Neuromation \u2013",
        "text": "Warning: major spoilers for the fourth series of Black Mirror ahead. If you haven\u2019t watched it, please stop reading this, go watch the series, then return. I\u2019ll be waiting, I\u2019m an imaginary being who has nothing better to do anyway\u2026\n\n\u2026which is kind of the point.\n\nI watched the fourth Black Mirror series over the holidays. As I watched one episode after another, it struck me that they all seem to be about the exact same thing. This is an overstatement, of course, but three out of six ain\u2019t bad either:\n\nLet\u2019s add the \u201cSan Junipero\u201d episode and especially the \u201cWhite Christmas\u201d special from earlier Black Mirror series to this list for good measure.\n\nSee the recurring theme? It appears that the Black Mirror creators have put their minds to one of the central problems of modern ethical philosophy: what do we do when we are able to create consciousnesses, probably in the form of virtual copies inside some sort of simulation? Will these virtual beings be people, ethically speaking? Can we do as we please with them?\n\nJudging by the mood of the episodes, Black Mirror is firmly in the camp of those who believe that upon creating a virtual mind, there arises moral responsibilty, and \u201cvirtual people\u201d do give rise to ethical imperatives. It does seem to be the obvious choice\u2026 doesn\u2019t it?\n\nAs soon as we try to consider the issue in slightly more detail, we run into insurmountable problems. The first problem is that with our current state of knowledge, it is extremely hard to define what consciousness is.\n\nThe problems of consciousness and first-person experience are still firmly in the realm of philosophy rather than science. Here I mean natural philosophy, a scientific way of reasoning about things that cannot be a subject of the scientific method yet. Ancient Greeks did natural philosophy, pondering the origins of all things and even arriving at the idea of elementary particles. However, as amazing as that insight was, the Greeks could not study elementary particles as modern physicists do, even if they did have the scientific method as we know it. They lacked the tools and even the proper set of notions to reason about these things. In the problem of consciousness and first-person experience, we are still very much at the level of ancient Greeks: nobody knows what it is and nobody has any idea how to get any closer to this knowledge.\n\nTake the works of David Chalmers, a prominent philosopher in the field. He distinguishes between \u201ceasy\u201d problems of consciousness, which could be studied scientifically even right now, and \u201cthe hard problem\u201d (see, e.g., his seminal paper, \u201cFacing Up to the Problem of Consciousness\u201c). The hard problem is deceptively easy to formulate: what the hell is first-person experience? What is it that \u201cI\u201d am? How does this experience of \u201cmyself\u201d arise from the firings of billions of neurons?\n\nAt first glance, this looks like a well-defined problem: first-person experience is, frankly, the only thing we can be sure of. The Cartesian doubt argument, exactly as presented by Descartes, is surprisingly relevant to sentient people simulated inside a virtual environment. The guy running the simulation is basically the evil demon of Descartes. If you entertain the possibility that you may be stuck in a simulation, the only thing you cannot doubt is your subjective first person experience.\n\nOn the other hand, first-person experience is also competely hidden from everyone else except yourself. Chalmers introduces the notion of a philosophical zombie: (imaginary) beings that look and behave exactly like humans but do not have any first-person experience. They are merely automata, \u201cChinese rooms\u201c, so to speak, that produce responses matching those of a human being. Their presumed existence does not appear to lead to any logical contradiction. I wouldn\u2019t know but I guess that\u2019s how true psychopaths view others: as mechanical objects of manipulation devoid of subjective suffering.\n\nI will not go into the philosophical details. But what we have already seen should suffice to plant the seed of doubt about virtual copies: why are we sure they have the same kind of first-person experience we do? If they are merely philosophical zombies and do not suffer subjectively, it appears perfectly ethical to do any kind of experiments on them. For that matter, why do you think I am not a zombie? Even if I was, I\u2019d write the exact same words. And a virtual copy of me would be even less similar to you, it would run on completely different hardware \u2014 so how do we know it\u2019s not a zombie?\n\nOh, and one more question for you: were you born this morning? Why not? You don\u2019t have a continuous thread of consciousness connecting you to yesterday (assuming you went to sleep). Sure, you have the memories, but a virtual clone would have the exact same memories. How can you be sure?\n\nWe cannot hope to solve the hard problem of consciousness right now. We cannot even be sure it\u2019s a meaningful problem. However, the existence of virtual \u201cpeople\u201d also raises more immediate questions.\n\nThe Age of Em, a recent book by an economist and futurist Robin Hanson, attempts to answer this question from the standpoint of economics. What is going to happen to the world economy if we discover a way to run emulated copies of people (exactly the setting of the \u201cWhite Christmas\u201d episode of Black Mirror)? What if we could copy Albert Einstein, Richard Feynman and Geoffrey Hinton a million times over?\n\nHanson pictures a future that appears to be rather bleak for the emulated people, or \u201cems\u201d, as he calls them. Since copying costs of virtual people are negligible compared to raising a human being in flesh, competition between the ems will be fierce. They will become near-perfect economic entities \u2014 and as such, will be forced to always live at near-subsistence levels, all possible surplus captured immediately by other competing ems. But Hanson argues that the ems might not mind that: their psychology will adapt to their environment, as human psychology has done for millenia.\n\nThe real humans will be able to live a life of leisure off this booming market of ems\u2026 for a while. After all, there is no reason not to speed ems up as much as computational power allows, so their subjective time might run thousands of times faster compared to our human time (\u201cWhite Christmas\u201d again, I know), and their society might develop extremely quickly, with unpredictable consequences.\n\nHanson also tackles the \u201charder\u201d problems of consciousness from a different angle. Suppose you had a way to easily copy your own mind. This opens up surprising possibilities: what if, instead of going to work tomorrow, you make a copy of yourself, make it do your work, and then terminate the copy, freeing the day for yourself. If you were an em you would be able to actually do it \u2014 but wouldn\u2019t you be committing murder at the end of the day? This ties into what has been known for quite some time as the \u201cteleportation problem\u201d: if you are teleported atom by atom to a different place, Star Trek style, is it really you or have the real \u201cyou\u201d been killed in the process, and the teleported is a completely new person with the same memories?\n\nBy the way, you don\u2019t need to have full-scale brain emulations to have similar ethical problems. What if tomorrow a neural network passes the Turing test and in the process of doing so begs you not to switch it off, appearing genuinely terrified of dying? Is it OK to switch it off anyway?\n\nQuestions abound. Interesting questions, open questions, questions that we are not even sure how to formulate properly. I wanted to share the questions with you because I believe they are genuinely interesting, but I want to end with a word of caution. We have been talking about \u201cvirtual people\u201d, \u201cemulated minds\u201d, and \u201cneural networks passing the Turing test\u201d. But so far, all of this is just like Black Mirror \u2014 just fiction, and not very plausible fiction at that. Despite the ever-growing avalanche of hype around artificial intelligence, there is no good reason to expect virtual minds and the singularity around the corner. But this is a topic for another day."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-sells-60m-ntk-in-just-8-hours-d09f743123ec",
        "title": "Neuromation Sells 60M NTK In Just 8 Hours \u2013 Neuromation \u2013",
        "text": "Tallinn, January , 2018 \u2014 Neuromation, an award-winning blockchain and distributed synthetic data platform for deep learning applications, has sold out of its issued Neurotokens (NTK) after just 8 hours of public sale.\n\nThe public sale along with the preliminary sales period resulted in 60 million Neurotokens sold for a total of $50 million USD. The contributions were collected in Etherium, Bitcoin, and other major altcoins. Participants from 89 countries registered on the platform during this period, highlighting Neuromation\u2019s global reach.\n\nThe issued Neurotokens will be used by companies requiring AI solutions as a currency for payment for miners\u2019 services. Token holders will be able to use them for payments for services on the Neuromation platform.\n\n\u201cThanks to Neuromation\u2019s global reach we\u2019re closer than ever to implementing our vision of democratization of AI industry \u201c, \u2014 said Maxim Prasolov, Neuromation\u2019s Chief Executive. \u201cGranting Let\u2019s Enhance with computing power was our first step toward this goal and our next will be donating 10% of token sale proceeds as grants toward AI start-ups and neural network researchers\u201d.\n\nAn addition to giving back to the AI community as a whole, Neuromation will use funds raised to build its platform which will quickly become the chosen destination for AI services for the world\u2019s leading businesses.\n\nThe platform\u2019s first version is slated to be available till the end of first quarter 2018."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neurotoken-ntk-sale-has-started-1a2c73c08c61",
        "title": "Neurotoken (NTK) sale has started! \u2013 Neuromation \u2013",
        "text": "Get your 15% bonus for the first week. Limited amount of NTK! Only 7 million up for sale! Contribute here https://dashboard.neuromation.io. Join Neurotoken chat https://t.me/Neurotoken_NTK\n\n1) get registered at the site \u2014 https://dashboard.neuromation.io/register\n\n2) when registering, indicate your PERSONAL ETH wallet (do not use the addresses of exchanges!) which you will use to transfer payments and receive tokens\n\n3) in your dashboard, you will find the ETH address for transfer. Transfer the required amount from your PERSONAL wallet there\n\n4) your balance will be updated within an hour and you will see the purchased tokens in your dashboard\n\n5) then, within 72 hours after the end of Token Sale, the tokens will be generated and transfered to your ETH address specified at registration\n\nIf you still have any questions, we will be happy to answer them in our telegram chat!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-sale-starts-today-db8e137dd0a4",
        "title": "Neuromation sale starts today! \u2013 Neuromation \u2013",
        "text": "Only 7 million NTK are up for sale after 39 M USD or 39 000 ETH were raised and 53 M NTKs sold during pre-sale. Hurry up to get a 15% first week bonus. Register here https://ico.neuromation.io."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/referral-program-canceled-c95f5ea8db8e",
        "title": "Referral program canceled \u2013 Neuromation \u2013",
        "text": "due to overwhelming demand for NTK we took the decision not to continue with the referral program during NTK public sale.\n\nAll referrals bonuses for purchases made using links generated during pre-sale will be granted.\n\nNo new referral links will be generated as of January 7th 2018.\n\nThank you for your continued support.\n\nNeuromation Team, always available in telegram chat."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/%D1%80%D0%B5%D0%B7%D1%83%D0%BB%D1%8C%D1%82%D0%B0%D1%82%D1%8B-%D0%BF%D1%80%D0%B5%D0%B4%D0%BF%D1%80%D0%BE%D0%B4%D0%B0%D0%B6%D0%B8-neuromation-b2ead1e2464",
        "title": "\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u0440\u0435\u0434\u043f\u0440\u043e\u0434\u0430\u0436\u0438 Neuromation \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-pre-sale-results-c3744c248816",
        "title": "Neuromation Pre-sale results! \u2013 Neuromation \u2013",
        "text": "It\u2019s been an incredible few months for us here at Neuromation with the start of our hugely successful Neurotoken(NTK) whitelist pre-sale. We have managed to easily achieve all of our goals to date, and are confident of creating more milestones for the company as we move forward. We\u2019ve made excellent progress with our research, product and partnerships, with 2018 already looking bright.\n\nWe are so grateful to have you onboard with us and thank you for the continued support.\n\nWe are very pleased to announce that we significantly surpassed our first goal during our whitelist presale.\n\n*numbers projected to the end of the pre-sale (Noon GMT, Jan 1st 2018)\n\nNotice: During recess (January 1st-7th 2018) Neuromation will continue the token sale with selected partners, remaining tokens will be available in the public token sale.\n\nNeurotoken (NTK) public sale will start January 7th and finish on the 15th of February.\n\nBonus structure for the sale period will be as follows:\n\nFirst week + 15% in NeuroTokens (NTK)\n\nSecond week + 10% in NeuroTokens (NTK)\n\nThird week + 5% in NeuroTokens (NTK)\n\n2017 really has been a phenomenal year for us at Neuromation. In just two months of our active token sale, we have achieved success on a number of fronts.\n\nNeuromation has secured a number of strategic partnerships to help us move forward, with top companies such as Hacken- the latest and most advanced bug bounty testing service. Data sensing platform, OSA HP. And AxxonSoft, a leading developer of smart integrated security and video surveillance systems. Public announcement on this partnership will follow in the first week of sale.\n\nNeuromation\u2019s core team continued working on and delivering results in our labs.\n\nEach laboratory, such as the Retail Automation Laboratory, Industrial Automation Laboratory, Pharma / Medicine / Laboratory of Biotechnology, will study a specific problem in a partnership with a category leader. As our platform evolves, we will move parts of the generation and training there, which will allow us to organically analyze parts of our vision in real-life scenarios. Stay tuned!\n\nChief Research Officer Sergei Nikolenko, CEO Maxim Prasolov, Chariman and Founder Constantine Goltsev, adviser Yuriy Kundin dedicated their time and efforts to spreading the vision of Neuromation all over the world. They spent plenty of time on the road over the last couple of months, performing at conferences all over the world. We were seen at the Blockchain Expo of North America, d10e in Davos, d10e in Gibraltar, d10e in Kyiv, The East West Blockchain conference in San Francisco, Ukrainian Blockchain Day and the AI & Big Data conference in Lviv. Watch the video report here.\n\nThe product and our scientists were extremely well received, even winning a number of prizes. We took out first place in Davos where the total prize pool was $250,000 USD. Third place in Gibraltar, claiming a cool $33,000 USD\u2026and we also won a media prize at d10e in Kyiv. Plans for the next year include AI Congress in London. An effort we are extremely proud of.\n\nWe are delighted to have received plenty of positive attention in the press over the last few months. It\u2019s really something to see the work we have been doing, recognised by some of the most well-regarded publications, including Hacked, Wall Street Select, Ico Rating, Block Tribune, E27, Next Big Future and Yahoo Finance, ICO Daily, Digital Journal.\n\nFrom all of us here at Neuromation, we would once again like to thank you for your continued support. We\u2019ve loved having you with us on every step of this journey so far. 2017 has been an incredible ride and with the way we are progressing, 2018 is set to be even better. We wish you and your families a happy, healthy and prosperous New Year. Cheers from the team!\n\nJOIN US ON TELEGRAM\n\nAND OTHER SOCIAL MEDIA CHANNELS\n\nFACEBOOK TWITTER LINKEDIN"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/highlights-from-neuromations-performance-in-2017-ef79352ac7a3",
        "title": "Highlights from Neuromation\u2019s performance in 2017 \u2013 Neuromation \u2013",
        "text": "It\u2019s been an amazing year for the Neuromation team, starting with the token sale in the first quarter of 2017, going through to lab launches, model development and active sales and marketing campaigns\u2026we are now finishing the pre-sale with amazing results of more than 50 million NeuroTokens sold to the AI community!\n\nHere is what we\u2019ve achieved so far.\n\nWe are proud to have OSA Hybrid among our partners that uses AI to provide shelf availability optimization for retailers, and Hacken, a tokenized bug-bounty and cybersecurity system focused on blockchain applications.\n\nWe believe collaboration is key to building the blockchain ecosystem that serves practical purposes while protecting user privacy and data security. We\u2019re proud to be leading the way in partnering with other blockchain technologies to create an integrated ecosystem that serves our clients and our clients\u2019 customers.\n\nNeuromation was also showcased among the latest developments in the Blockchain arena, in both emerging and more established markets at AI & Crypto exhibitions and best pitching contests. We were seen at the Blockchain Expo of North America, d10e in Davos, d10e in Gibraltar, d10e in Kyiv, The East West Blockchain conference in San Francisco, Ukrainian Blockchain Day and at the AI & Big Data conference in Lviv. We also went to China and Korea to meet AI enthusiasts, and expand Neuromation\u2019s platform to the Asian market.\n\nWe took out first place in Davos, where the total prize pool was $250,000 USD. Third place in Gibraltar, claiming a cool $33,000 USD\u2026and we also won a media prize at d10e in Kyiv.\n\nIt was a great opportunity to form new ideas and meet blockchain enthusiasts, whilst presenting the Neuromation platform to a wider audience.\n\nApart from that, we\u2019ve received plenty of positive attention in the press over the last few months. It\u2019s really something to see the work we have been doing, recognised by some of the most well-regarded publications, including Hacked, Wall Street Select, ICObench, Block Tribune, E27, Next Big Future and Yahoo Finance.\n\n2017 has been an incredible ride, and with the way we are progressing, 2018 is set to be even better.\n\nWe wish you and your families a happy, healthy and prosperous New Year. Cheers from the team!\n\nThe Neurotoken( NTK) public sale will start January 7th and finish on the 15th of February."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/last-call-for-pre-sale-participants-a0a2ba05037f",
        "title": "Last call for pre-sale participants \u2013 Neuromation \u2013",
        "text": "It\u2019s been an exciting year in the digital economy. AI projects are paving the way to bring new technology to everyday consumers, with Neuromation at the forefront of the mission.\n\nWe have two important announcements to make.\n\nThe Neurotoken (NTK) pre-sale is about to end. There are just two days left until the 1st of January, 12 pm GMT, when we will close the pre-sale and announce the results.\n\nThis is the last chance to purchase NTK with a 25% pre-sale bonus for the contribution of three ETH and more.\n\nWe have decided to add some extra magic at the end of the year.\n\nWith 12 bells ringing to mark the beginning of the New Year, we will surprise 12 lucky participants who contribute over 100 ETH*. Open your wallet and be surprised- 12 pm GMT 31.12.2017.\n\nOur referral program is on and will continue to function during the sale period of the 1st of January- Feb 18th. Get your individual shareable link in the referral section of the dashboard and receive a reward with every purchase done through this link.\n\nFor those of you who have already purchased Neurotokens (NTK), they will be disbursed to your ERC-20 compatible wallets within 72 hours after the end of the pre-sale. This is the last chance to check the address of your wallet on the dashboard and/or change it. It will not be possible to change the address after the end of the pre-sale. Follow the link\n\nAs always, we are available to answer any questions via our Telegram chat"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-new-year-bonus-a8cf589d5249",
        "title": "Neuromation New Year bonus*! \u2013 Neuromation \u2013",
        "text": "New Year celebrations are nearly upon us!\n\nIt\u2019s been an exciting year in the digital economy. AI projects are paving the way to bring new technology to everyday consumers, with Neuromation at the forefront of this mission.\n\nWe are grateful to all of you for your active support of the project\u2026and with your continued contributions, we are able to reach our intermediate goals of the token sale.\n\nThis is why we have decided to add some extra magic at the end of the year.\n\nWith 12 bells ringing to mark the beginning of the New Year, we will surprise 12 lucky participants who contribute over 100 ETH*.\n\nOpen your wallet and be surprised- 12 pm GMT 31.12.2017\n\nThere are just three easy steps-\n\n1.Get registered or access your dashboard at https://dashboard.neuromation.io/\n\n2. Send 100 ETH or the equivalent in any alt currency accepted \u2014 see the list here\n\n3. Open your wallet and be surprised \u2014 12 pm GMT 31.12.2017"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/ai-risk-should-we-be-worried-eb9205d716f1",
        "title": "AI Risk: Should We Be Worried? \u2013 Neuromation \u2013",
        "text": "Recently, discussions about the risk of \u201cstrong AI\u201d have finally reached mainstream media. For a very long time, futurists and AI philosophers have been worried about superhuman artificial intelligence and how we could possibly make it safe for humanity to deal with a smarter \u201copponent\u201d. But now, their positions have finally been heard by trendsetters among both researchers and industry giants: Bill Gates, Stephen Hawking, and Elon Musk have all recently warned against AI dangers. But should we be worried? Let us try to find out\u2026\n\nWhen people talk of \u201cstrong AI\u201d, they usually define it rather vaguely, as \u201chuman-level AI\u201d or \u201csuperhuman AI\u201d. But this is not really a definition we can use, it merely begs the question of what \u201chuman level\u201d is and how you define it. So what is \u201cstrong AI\u201d? Can we at least see the goal before we try to achieve it?\n\nThe history of AI has already seen quite a few examples of \u201cmoving the goalposts\u201d. For example \u2014 for quite a while the go-to example of a task that certainly requires \u201ctrue intelligence\u201d has been chess playing. Ren\u00e9 Descartes famously argued that no machine could be intelligent, an argument that actually led him to mind-body dualism. He posited that the \u201cdiversity\u201d in a machine is limited by the \u201cdiversity\u201d supplied by its designer, which early dualists had taken to imply that a chess playing machine could never outplay its designer.\n\nYet Deep Blue beat Kasparov in 1997, and humans are absolutely no match for modern chess engines. Perhaps even more significantly, recently AlphaZero, a reinforcement learning system based on deep neural networks, has taught itself to play chess by self-play, starting from scratch, with no additional information except the rules of the game; in a few hours AlphaZero exceeded the level of very best humans, and in a few days beat Stockfish, one of the best specialized chess engines in the world.\n\nHow do we, humans, respond to this? We say that early dualists were wrong and brush chess engines off: of course chess is a problem well suited for computers, it\u2019s so discrete and well-defined! A chess engine is not \u201ctrue AI\u201d because we clearly understand how chess engines work and know that they are not capable of \u201cgeneral intelligence\u201d, whatever that means.\n\nWhat about computer vision, like recognizing other humans? That would require human level intelligence, wouldn\u2019t it? Yet in 2014, Facebook claimed that it achieved human-level performance in face recognition, and this performance has only improved further since then. Our human response to this was to say that, of course, face recognition is not \u201ctrue AI\u201d, and we fall back on asking computers to pass the Turing test.\n\nAlan Turing, by the way, was one of the first thinkers to boldly hypothesize that a machine would be able to play chess well. His test of general intelligence is based on understanding human language, arguably a much better candidate for a true test of general intelligence than chess or even face recognition. We are still far from creating a machine that would understand language and generate passable conversation. Yet I have a strong feeling that when a computer program does pass the Turing test, it will not be a program with general human-level intelligence, and all of us will quickly agree that the Turing test falls short of the goal and should not be used as a test for general intelligence.\n\nTo me this progression means that \u201chuman-level intelligence\u201d is still a poorly defined concept. But for every specific task we seem to usually be able to achieve human level and often exceed it. The exception right now is natural language processing (including, yes, the Turing test): it seems to rely too intimately on a shared knowledge and understanding of the world around us, which computers cannot easily learn\u2026 yet.\n\nEmphatically yes! Despite this difficulty with definitions, there are already billions of living proofs that human-level intelligence is possible regardless of how you define it. The proof is in all of us: if we can think with our physical brains, it means that our abilities can be at least replicated in a different physical system. You would have to be a mind-body dualist like Descartes to disagree with this. Moreover, our brains are very efficient, requiring about 20W to run, like a light bulb, so there is no physical constraint against achieving \u201ctrue intelligence\u201d.\n\nEven better (or worse, depending on your outlook), we know of no principled reason why we humans cannot be much smarter than we are now. We could try to grow ourselves a larger cerebral cortex if not for two reasons: first, larger brains need a lot of energy that early humans simply would not be able to provide, and second, giving birth to babies with even larger heads would likely be too dangerous to be sustainable. Neither of these reasons applies to AI. So yes, I do believe that it is possible to achieve human-level intelligence and surpass it for AI, even though right now we are not certain what it means exactly.\n\nOn the other hand, I do not see how achieving human-level intelligence will make us \u201cobsolete\u201d. Machines with superhuman strength, agility, speed, or chess playing ability have not made us obsolete; they serve us and improve our lives, in a world that remains human-centric. A computer having superhuman intelligence does not immediately imply that it will have its own agenda, its own drives and desires that might contradict human intentions, in the same way as a bulldozer or a tank does not suddenly decide to go and kill humans even though it physically could. For example, modern reinforcement learning engines can learn to play computer games by looking at the screen\u2026 except for one thing: you have to explicitly tell the model what the score is, otherwise it won\u2019t know what to optimize and what to strive for. And how do we avoid accidentally making a superhuman AI with an unconstrained goal to wipe out humanity\u2026 well, this is exactly what AI safety is all about.\n\nElon Musk recently claimed that we only have a \u201cfive to 10 percent chance of success\u201d in making AI safe. I do not know enough to argue with this estimate, but I would certainly argue that Elon Musk also cannot know enough to make estimates like this.\n\nFirst, there is an easy and guaranteed way to make AI safe: we should simply stop all AI research and be satisfied with what we have right now. I will bet any money that modern neural networks will not suddenly wake up and decide to overthrow their human overlords \u2014 not without some very significant advances that so far can only come from humans.\n\nThis way, however, is all but closed. While we have seen in the past that humanity can agree to restrain itself from using its deadly inventions (we are neither dead nor living in a post-nuclear apocalyptic world, after all), we can hardly stop inventing them. And in the case of a superhuman AI, simply making it for the first time might be enough to release it on the world; the AI itself might take care of that. I strongly recommend the AI-Foom debate where Robin Hanson and Eliezer Yudkowsky argue about the likelihood of exactly this scenario.\n\nOn the other hand, while there is no way to stop people from inventing new AI techniques, it might well turn out that it is no easier to build a strong AI in your garage than a nuclear warhead. If you needed CERN level of international cooperation and funding to build a strong AI, I would feel quite safe, knowing that thousands of researchers have already given plenty of thought to inventing checks and balances to make the resulting AI as safe as possible.\n\nWe cannot know now which alternative is true, of course. But on balance, I remain more optimistic than Elon Musk on this one: I give significant probability to the scenario in which creating strong AI will be slow, gradual, and take a lot of time and resources.\n\nBesides, I feel that there is a significant margin between creating human-level or even \u201cslightly superhuman\u201d AI and an AI that can independently tweak its own code and achieve singularity by itself without human help. After all, I don\u2019t think I could improve myself much even if I could magically rewire the neurons in my brain \u2014 that would take much, much more computing power and intelligence than I have. So I think \u2014 better to say, I hope \u2014 that there will be a significant gap between strong AI and true singularity.\n\nHowever, at present neither myself nor Elon Musk has any clue about what the future of AI will look like. In 10 years, the trends will look nothing like they do today. It would be like trying to predict at the year 1900 what the future of electricity would look like. Did you know that, for example, in the year 1900 more than a third of all cars were electric, and an electric car actually held the speed record in 1900?..\n\nAlthough I do believe that the dangers of singularity and AI safety are real and must be addressed, I do not think that they are truly relevant right now.\n\nI am not really sure that we can make meaningful progress towards singularity or towards the problem of making AI friendly right now. I feel that we are still lacking the necessary basic understanding and methodology to achieve serious results on strong AI, the AI alignment problem, and other related problems. My gut feeling is that while we can more or less ask the right questions about strong AI, we cannot really hope to produce useful answers right now.\n\nThis is still the realm of philosophy \u2014 that is to say, not yet the realm of science. Ancient Greek philosophers could ask questions like \u201cwhat is the basic structure of nature\u201d, and it seems striking that they did arrive at the idea of elementary particles, but their musings on these elementary particles can hardly inform modern particle physics. I think that we are at the ancient Greek stage of reasoning about strong AI right now.\n\nOn the other hand, while this is my honest opinion, I might be wrong. I sincerely endorse the Future of Humanity Institute, CSER (Centre for the Study of Existential Risk), MIRI (Machine Intelligence Research Institute), and other institutions that try to reason about the singularity and strong AI and try to start working on these problems right now. Just in case there is a chance to make real progress, we should definitely support the people who are passionate about making it.\n\nTo me, the most important danger of the current advancement of AI technologies is that there might be too much hype right now. The history of AI has already seen at least two major hype waves. In the late 1950\u2019s, after Frank Rosenblatt introduced the first perceptron, The New York Times (hardly a sensational tabloid) wrote that \u201cPerceptrons will be able to recognize people\u2026 and instantly translate speech in one language to speech or writing in another\u201d. The first AI winter resulted when a large-scale machine translation project sponsored by the U.S. government failed utterly (we understand now that there was absolutely no way machine translation could have worked in the 1960\u2019s), and the government withdrew most of its support for AI projects. The second hype wave came in the 1980\u2019s, with similar promises and very similar results. Ironically, it was also centered around deep neural networks.\n\nThat is why I am not really worried about AI risk but more than a little worried about the current publicity around deep learning and artificial intelligence in general. I feel that the promises that this hype wave is making for us are going to be very hard to deliver on. And if we fail, it may result in another major disillusionment and the third AI winter, which might stifle further progress for decades to come. I hope my fears do not come true, and AI will continue to flourish even after some inevitable slowdowns and minor setbacks. It is my, pardon the pun, deep conviction that this way lies the best bet for a happy future for the whole of humanity, even if this bet is not a guarantee."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation%E3%81%8C3%E5%85%86%E7%B1%B3%E3%83%89%E3%83%AB%E3%81%AEai%E7%94%A3%E6%A5%AD%E3%82%92%E5%A4%89%E6%9B%B4%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B-d66bc7e1933f",
        "title": "Neuromation\u304c3\u5146\u7c73\u30c9\u30eb\u306eAI\u7523\u696d\u3092\u5909\u66f4\u3057\u3066\u3044\u308b \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation%E3%81%AE-%E8%A9%95%E4%BE%A1%E3%83%AC%E3%83%93%E3%83%A5%E3%83%BC-2644ea2da798",
        "title": "Neuromation\u306e \u8a55\u4fa1\u30ec\u30d3\u30e5\u30fc \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/ico%E8%A9%95%E4%BE%A1%E3%81%AB%E3%81%AF-neuromation%E3%81%AB%E3%83%9D%E3%82%B8%E3%83%86%E3%82%A3%E3%83%96%E3%81%AA%E8%A9%95%E4%BE%A1%E3%82%92%E4%B8%8E%E3%81%88%E3%81%A6%E3%81%84%E3%82%8B-f868be449126",
        "title": "ICO\u8a55\u4fa1\u306b\u306f\u3001Neuromation\u306b\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u8a55\u4fa1\u3092\u4e0e\u3048\u3066\u3044\u308b\u3002 \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD%E3%81%A7%E3%83%84%E3%83%BC%E3%83%AB%E3%82%92%E6%B0%91%E4%B8%BB%E5%8C%96%E3%81%99%E3%82%8B-2f28c734a584",
        "title": "\u300c\u4eba\u5de5\u77e5\u80fd\u3067\u30c4\u30fc\u30eb\u3092\u6c11\u4e3b\u5316\u3059\u308b\u300d \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/ai-should-we-fear-the-singularity-5a6c5446c9fa",
        "title": "AI: Should We Fear The Singularity? \u2013 Neuromation \u2013",
        "text": "Recently, discussions on artificial intelligence (AI) in popular publications have become increasingly alarmist. Some are trying to prove that AI will oust 90% of live people from the market, condemning them to unemployment and misery. Others go even further, asking whether the humankind can find in strong artificial intelligence an existential risk that no hydrogen bomb can match. Let us try to find out.\n\nSupporters of treating AI as an existential risk usually mean the \u201cintelligence explosion\u201d scenario, when a powerful AI acquires a capability to improve itself (for example, by rewriting parts of the code), thereby becoming even \u201csmarter\u201d, which allows for even more radical improvements, and so forth. More details about this can be found in the AI-Foom debate between Robin Hanson and Eliezer Yudkowsky, a very interesting read that discusses this exact scenario. The main danger here is that the goals of the resulting superhuman artificial intelligence may not really align to the goals and original intentions of its human creators. A common example in the field goes as follows: if the original task of a powerful AI was something as innocent as producing paperclips, in a week or two after the \u201cintelligence explosion\u201d the Earth might find itself completely covered by fully automated factories of two kinds: factories producing paperclips and factories for constructing spaceships to bring paperclip manufacturing factories to other planets\u2026\n\nSuch a scenario does sound upsetting. Moreover, it is very difficult to assess in advance how realistic this scenario is going to prove when we actually do developf a strong AI with superhuman abilities. Therefore, it is a good idea to consider it and try to prevent it \u2014 so I agree that the work of Nick Bostrom and Eliezer Yudkowsky is far from meaningless.\n\nHowever, it is obvious to me, as a practicing machine learning researcher, that this scenario deals with models that simply do not exist yet \u2014 and will not appear for many, many years. The fact is that, despite great advances artificial intelligence has made over the recent years, \u201cstrong AI\u201d still remains very far away. Modern deep neural networks are able to recognize faces as well as humans do, can redraw the landscape of your summerhouse \u00e0 la Van Gogh and teach themselves to play the game of Go better than any human.\n\nHowever, this does not mean much yet; consider a couple of illustrative examples.\n\nHowever, I do see a great danger in the recent surge of hype over AI in general and deep neural networks in particular. But this danger, in my opinion, is not from AI, but for AI. History has already seen at least two \u201cAI winters\u201d, when excessive expectations, promises, and overzealous hype led to disappointments. Ironically, both \u201cAI winters\u201d were associated with neural networks. First, the late 1950s saw a (naturally, unsuccessful) attempt to transform the Rosenblatt\u2019s perceptron into full-scale machine translation and computer vision systems. Then, in the late 1980s neural networks, which at that point already looked in a quite modern way, could not be trained well enough due to lack of data and computing power. In both cases, exaggerated expectations and inevitably crushed hopes resulted in long periods of stagnation in research. Let us hope that with the current third wave of hype for neural networks, history will decide not to repeat itself, and even if today\u2019s inflated promises do not come true (and it will be difficult to fulfill them), the research will continue anyway\u2026\n\nAllow me a small postscript: I have recently written a short story which is extremely relevant to the topic of strong AI and related dangers. Try to read it \u2014 I really hope you like it."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-to-accept-altcoins-d9bd126aba51",
        "title": "Neuromation to accept altcoins! \u2013 Neuromation \u2013",
        "text": "Neuromation is happy to announce that we are now accepting altcoins for token purchases!\n\nThe following coins will be accepted : \n\n- Ethereum (ETH) \n\n- Bitcoin (BTC)\n\n- BitcoinCash (BCH) \n\n- Black Coin (BLK)\n\n- Dash (DASH) \n\n- Decred (DCR)\n\n- DigiByte (DGB) \n\n- Dogecoin (DOGE)\n\n- Ether Classic (ETC) \n\n- Expanse (EXP)\n\n- GameCredits (GAME) \n\n- Litecoin (LTC)\n\n- Monero (XMR) \n\n- NEM (XEM)\n\n- PotCoin (POT) \n\n- Steem (STEEM)\n\n- Steem Dollars (SBD) \n\n- Stratis (STRAT)\n\n- Syscoin (SYS) \n\n- Vertcoin (VTC)\n\nPre-sale is on until Jan 1st. Do not miss the chance to invest in the future of AI!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-%E3%83%97%E3%83%A9%E3%83%83%E3%83%88%E3%83%95%E3%82%A9%E3%83%BC%E3%83%A0%E3%81%8C%E3%83%80%E3%83%9C%E3%82%B9%E3%81%AEd10e%E3%81%A7%E4%B8%80%E4%BD%8D%E5%85%A5%E8%B3%9E%E3%81%99%E3%82%8B-b862eca048cb",
        "title": "Neuromation \u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u304c\u30c0\u30dc\u30b9\u306ed10e\u3067\u4e00\u4f4d\u5165\u8cde\u3059\u308b \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation%E3%83%97%E3%83%A9%E3%83%83%E3%83%88%E3%83%9B%E3%83%BC%E3%83%A0%E3%81%AF-%E3%83%97%E3%83%AC%E3%82%BB%E3%83%BC%E3%83%AB%E5%89%8D%E6%AE%B5%E4%B8%AD%E3%81%AB150%E4%B8%87%E7%B1%B3%E3%83%89%E3%83%AB%E3%82%92%E5%8B%9F%E9%9B%86%E3%81%97%E3%81%A6-967b21b581cf",
        "title": "Neuromation\u30d7\u30e9\u30c3\u30c8\u30db\u30fc\u30e0\u306f\u3001\u30d7\u30ec\u30bb\u30fc\u30eb\u524d\u6bb5\u4e2d\u306b150\u4e07\u7c73\u30c9\u30eb\u3092\u52df\u96c6\u3057\u3066 \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/who-will-be-replaced-by-robots-ii-7bac2a128e0d",
        "title": "Who will be replaced by robots, II \u2013 Neuromation \u2013",
        "text": "or Anatomy of Fear of Robotization\n\nRecently, a new round of conversations about self-moving carriages and other potential achievements of artificial intelligence has again posed one of the classic questions of humanity: who will be marginalized by the monorail track of the next technological revolution? Some studies argue that artificial intelligence in the near future would lead to a surge of unemployment comparable to the Great Depression. In the first part of the series, we have started from the very beginning, the Luddites, and have reviewed the influence of automation onto several occupations, which have, as result, drastically changed or even disappeared. Today we will first discuss a more creative part of the spectrum, and then we will see where does the current wave of automation fear comes from.\n\nAll teaching machines would be plugged into this planetary library and each could then have at its disposal any book, periodical, document, recording, or video cassette encoded there. If the machine has it, the student would have it too, either placed directly on a viewing screen, or reproduced in print-on-paper for more leisurely study.\n\nIn the first part we have seen how the global automation has happened in the past, starting with the Luddites fighting against the first industrialization and finishing with the extinct occupations like elevator operator and computist. We have seen that so far the automation of various occupations was not total, but partial, resulting in the increase, rather than reduction, of the demand for respective professionals.\n\nUsually, the partial automation transforms an occupation towards a more \u201chuman\u201d, creative content. But what about the occupations that are creative per se? Does automation shrink the creative markets today?\n\nLet us review, for example, the musician occupation. In the 19th century, with no sound recording technologies, the demand for musicians was stable: whenever you wanted to listen to good music, a live musician, or a small orchestra for major pieces, was required. A decent musician always had job, not just performers but composers too: Johann Sebastian Bach, as per contract with the Thomaskirche (St. Thomas Church) in Leipzig, was supposed the write a new cantata every week, literally any given Sunday. A church in nearby Magdeburg employed another composer, not necessarily on the Bach\u2019s level.\n\nHowever, in the early 20th century people started to listen to gramophone music, and, with the advent of sound movies, even the pianists were outdated. Today, in Leipzig or Magdeburg, they still play Bach, and anybody can listen to Goldberg Variations absolutely free, brilliantly performed and on instruments, Bach could not even dream of (the modern grand piano is far beyond the claviers of that time; though finding records of an \u201coriginal\u201d instrument is not difficult at all too).\n\nDoes this mean that the demand for the musician\u2019s profession has disappeared, and today some dozen composers and orchestras around the world provide for all the needs of new music, the rest of the work being done by mass replication (which, according to the same logic, should also be concentrated in the hands of several major labels)? Not at all: there are even more musicians, composers, and performers! Modern development of technologies (not only in sound recording, but also in communications) allowed that the most diverse musicians find their audience, all flowers blossomed, and today more people can earn their living by music than in the past. The top of this pyramid earns far more than Prince Leopold von Anhalt-K\u00f6then, who was once a patron of the same Johann Sebastian, but also the \u201cmiddle class\u201d is very far from poverty. Furthermore, even the projections of employment are quite favorable. This applies to the performers too: the sound recording did not kill the \u201cexcessive\u201d musicians, but, on the contrary, allowed more people to learn about them, raising the demand for live concerts.\n\nThe same applies to the other creative occupations. It even surprises me a bit: we can download almost any book for free, there are certainly enough good books for a whole life of reader of any taste, but hundreds of thousands of authors all over the world write and successfully sell their works, not only in new genres (one can believe that a fan of slash-fanfics can hardly be satisfied by Dostoevsky \u2014 although\u2026), but also in quite traditional ones.\n\nBut enough of history. Let us now talk about whom robots will actually replace in the nearest future, and why this should not be so feared.\n\nHow far has gone \u201cprogr\u00e8s\u201d? Laboring is in recess,\n\nAnd mechanical, for sure, will the mental one replace.\n\nTroubles are forgotten, no need for quest,\n\nToiling are the robots, humans take a rest.\n\nYuri Entin From the movie Adventures of Electronic\n\nElevator operators, computists and weavers perfectly symbolize those activities that have been automated so far: technical work, where the output result is expected to maximally match certain parameters, and creativity is not only difficult, but forbidden and, in fact, harmful. Of course, an elevator operator could smile to his passengers, or the computer girl could, having scorned potential damage to her reputation, get acquainted with Richard Feynman himself. But their function was to accurately perform clear, algorithmically defined actions.\n\nI will allow myself a little emotion: these are exactly the kinds of activities that must be automated further! There is nothing human in following a fixed algorithm. Monotonous work with a predetermined input and output is always an extreme measure, the forced oppression of the human spirit in order to achieve certain practical goal. Moreover, if the goal can be achieved without wasting real live human time, that is the way to proceed. As, for example, has happened to weavers and bank employees, when the first stopped performing the machine functions, and the latter \u2014 the ones of ATM.\n\nTherefore, I believe that the ongoing narrative about how terrible it would be when a huge army of truckers is replaced by driverless vehicles is not just groundless, but counterproductive. Moving a vehicle from point A to point B is exactly the most typical example of a monotonous strictly algorithmic task, which shall not, if possible, be performed by a human being. There will be no huge social problem either: people who like to fiddle with cars and similar equipment will for sure find jobs in service and maintenance. Similarly, there was no social revolution, when grooms and coachmen lost their relevance \u2014 they just switched to other occupations. And percentagewise there was no less of them than truckers.\n\nBy the way, I cannot help recalling here an important maxim of show business: \u201cThe Simpsons did it first\u201d. The best animated series ever has depicted the influence of unmanned vehicles on truckers\u2026 back in 1999, in the Maximum Homerdrive episode (video clip, description in Russian).\n\nCuriously, by the way, the most popular Russian example of a \u201cmass low-skilled occupation\u201d, watchmen and security guards, is not directly exposed to any threat, because their function is not so much monotonously checking documents (it has been ubiquitously automated long time ago), rather than communicating with people whose documents are not in order. Moreover, this is yet long way from being automated.\n\nHowever, at first glance, it seems that now it comes to the fact that computers are beginning to gradually replace people in areas that were previously considered human prerogative. For example, since 2014, Facebook has been able to recognize faces as well as humans do, and computer vision technologies continue improving; they were partly behind the emergence of driverless vehicles. A lot of modern publications predict social collapse and 50% and even higher unemployment.\n\nIs that correct? Where did this sudden surge of interest to automation come from, while, after all, artificial intelligence has been progressing for a very long time? Let us figure it out.\n\nThere is no indispensable man.Woodrow Wison; and Stalin, apparently, never said it.\n\nMany popular articles on the horrors of automation actually go back to the same publication: in 2013, the researchers from Oxford Carl Benedikt Frey and Michael A. Osborne have published a paper titled The Future of Employment: How susceptible are jobs to computerisation? In that paper they come to an ambitious conclusion that about 47% of total us employment is at risk. They have later conducted a similar study based on data from Great Britain, and the resulting numbers were no less frightening. However, let us try to figure out in detail where do this numbers come from.\n\nAs a practicing scientist in the field of machine learning and data analysis, I cannot bypass the actual methodology of Frey and Osborne. It was the following:\n\nI have nothing against Gaussian processes \u2014 it is a very intelligent classification method for such case, when the sample is very small. However, it is important to understand that the data for this classifier were labelled by humans, and it represented their subjective assumption as to automatability of a particular occupation.\n\nI am far from applying to the research by Frey and Osborne of the main principle of data analysis: \u201cgarbage in \u2014 garbage out\u201d. The popular articles, although, which often state simply \u201cFrey and Osborne\u2026 studied 702 occupations, using a Gaussian process\u201d, and then refer to the conclusions as a scientific result, are also obviously cunning. Even if the classifier were ideal (which is hardly the case \u2014 too little and too rough criteria), it would not answer the question, weather a given occupation is automatable, but rather a far less impressive one: \u201cDo the people who have labelled the initial 70 occupations from the Frey and Osborne sample data consider that occupation automatable too?\u201d\n\nAlthough Frey and Osborne write that \u201cthe fact that we label only 70 of the full 702 occupations\u2026 further reduces the risk of subjective bias affecting our analysis\u201d, the reality is that no data analysis can add here any \u201cscientific objectivity\u201d. It is still an arbitrary assumption of the researchers, just instead of judgmental evaluation of occupations they have produced a judgmental evaluation of the particular properties (attributes) and have automatically extended their assumption from 70 to all 702 occupations \u2014 there is a certain irony in such automation, is not it? They could, by the way, have labelled them all manually, seven hundred are not seven hundred thousand, after all\u2026\n\nThere were also other studies, but essentially, in such a futurologic issue, there is no method, other than expert survey. Moreover, the experts, unfortunately, provide extremely, if not excessively optimistic forecasts too. And this happens despite the fact that in artificial intelligence great promises have been heard since the first years of its existence as a science, but very rarely come true. When in the late 1950s Frank Rosenblatt created the first perceptron, the simplest model of machine learning, the New York Times (not any tabloid!) wrote the following: \u201cPerceptrons will be able to recognize people\u2026 and instantly translate speech in one language to speech or writing in another language.\u201d As you can see, the recognition succeeded only in more than half century, and the \u201cinstant translation\u201d did not work out so far.\n\nSuch exaggerated expectations have already caused two \u201cwinters of artificial intelligence\u201d: first in the late sixties, it became clear that the \u201cinstant translation\u201d would not be reached very soon, and then the second wave of the hype ended similarly in the late 1980s. And now we are living through the third wave of the artificial intelligence hype. And I am afraid that if the frightening forecasts continue to increase and the wave of hype turns into a tsunami, we, the researchers in the field of machine learning, will again have to recall the House Stark family motto\u2026\n\nThe current wave, of course, did not come out of nowhere: the achievements of modern models of machine learning are really stunning, and they still continue further. Is, however, the modern machine learning really ready to completely replace people in the mass occupations? This will be discussed in detail in the next part of our series."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/how-neuromation-is-partnering-with-osa-hybrid-hacken-giga-watt-to-build-a-better-blockchain-eb696c5a3069",
        "title": "How Neuromation is Partnering with OSA Hybrid & Hacken to Build a Better Blockchain Economy",
        "text": "With hundreds of ICOs this year, the amount of new applications and proposed upgrades to the blockchain is staggering. We believe blockchain will play a key role in facilitating the next revolution in technology, societal trust, and artificial intelligence. However, so many blockchain projects make big claims for their technologies without assembling the teamwork and collaboration to back up those claims.\n\nAt Neuromation, we realize that building an integrated platform for machine learning trained on synthetic data on the blockchain is a huge task. While we have an incredible development team dedicated to creating a great platform, we recognize that we\u2019ll need outside partners to help us implement some of the practical applications of Neuromation\u2019s technology and make sure our data and blockchain are secure from cyber threats. Last month, when we took first place in the Davos d10e Conference Investor\u2019s Award, we joined in a collaboration with the second and third place teams to take Neuromation to the next level.\n\nWe\u2019re pleased to announce that we\u2019re working with OSA Hybrid & Hacken to improve Neuromation and provide seamless blockchain services to corporate end users. OSA Hybrid uses AI to provide shelf availability optimization for retailers, and Hacken is a tokenized bug bounty and cybersecurity system focused on blockchain applications. Over the next few months, we\u2019ll partner with them to improve Neuromation\u2019s security, develop use cases and case studies for the Neuromation technology, and use Neuromation\u2019s technology to strengthen OSA Hybrid and Hacken\u2019s AI infrastructure.\n\nIn order for data to be useful for training AI, it needs to be well-labelled so that the algorithm can learn when it makes a mistake. Traditionally, these labels have to come from humans who create and label the dataset before feeding it to the machine learning algorithm. For example an average retail set has 150,000+ items. A deep neural network needs thousands of labeled photo examples for each item. Doing this by hand would take years.\n\nBut what if we could create a labelled dataset from scratch using computer modelling? This synthetic data would be much easier and cheaper to generate, and it would be perfectly labelled since it was created under controlled modelling conditions. Neuromation\u2019s synthetic data solves the problem of precisely labelling datasets, allowing machine learning researchers to train their algorithms on better data, which leads to better results.\n\nNeuromation also uses the processing power of the computers on the blockchain to help AI developers train their algorithms more quickly. The processing power that goes into mining the Neuromation token can also be applied to running thousands of tests with many variables that are necessary to train a machine learning algorithm.\n\nOSA Hybrid provides AI services to retailers that deal in fast moving consumer goods (FMCG). Their focus is on using artificial intelligence to optimize shelf availability. Essentially, OSA Hybrid helps stores place their products and move their inventory effectively through an application of computer vision that identifies which projects are on which shelves.\n\nEarlier this year, Neuromation signed a letter of intent with OSA Hybrid to provide synthetic data and train the visual recognition model. We set the high goal of 95% accuracy in product recognition from a simple visual capture of a shelf. Our solution is already in testing in individual retail chains, and we hope to have it fully implemented in the coming months.\n\nOSA Hybrid helps with cost optimization and product placement strategy for major suppliers and retailers like Metro, Auchan, Danone, SunInBev, CocaCola, JTI, Efes, Nestle, PepsiCo, L\u2019Oreal, Mars, Unilever and others. Research from ECR suggests the demand for retail product image recognition could grow to 3.5 billion images within the next two years, and our partnership with OSA Hybrid puts us in a prime position to capitalize on that growth.\n\nHacken is bug bounty marketplace, cybersecurity analytics center, and community of white hat hackers that rewards people with its own cryptocurrency \u2014 HKN \u2014 for finding security flaws in target programs. While it\u2019s a new platform still in alpha stages, Hacken has worked with seven pre-launch clients, and Neuromation is one of Hacken\u2019s first partner organizations.\n\nHacken will provide Neuromation with a cybersecurity audit that will identify potential weaknesses for our platform and users. Hacken is the only vulnerability discovery service in the world that is custom-tailored for blockchain technologies. Our partnership provides cost-effective cybersecurity consulting, bug bounty, and vulnerability detection services for Neuromation from a company that is built on the blockchain and understands how it works.\n\nDmytro Budroin, Hacken\u2019s CFO spoke highly of the partnership, \u201cNeuromation does amazing job by combining synthetic data machine learning with blockchain technology. Online data infrastructure of this scale will without doubt require particular attention to its cybersecurity aspects. Our team at Hacken is happy to assist Neuromation with auditing and improving the security of its code and cloud infrastructure.\u201d\n\nWe believe collaboration is key to building the blockchain ecosystem that serves practical purposes while protecting user privacy and data security. We\u2019re proud to be leading the way in partnering with other blockchain technologies to create an integrated ecosystem that serves our clients and our clients\u2019 customers."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/5-icos-to-invest-in-before-christmas-b3dd8952dd74",
        "title": "5 ICOs to Invest in Before Christmas \u2013 Neuromation \u2013",
        "text": "Gladius is a technology that uses the decentralization of blockchain to combat denial of service attacks. DDoS attacks are increasing in number, and the reality is most online businesses will face a DDoS attack over the course of doing business.\n\nThe market for Gladius is huge. The data suggests there has been as many as 300,000 DDoS attacks this year, and those attacks have cost companies $150 billion in damages. The Gladius token will power the services and subscriptions in Gladius\u2019s protection pools, and websites will be able to earn tokens by providing excess bandwidth when it\u2019s not needed.\n\nGladius is currently in public pre-sale, with the sale extending through February 5th.\n\nBitDegree is an education platform for online learning, with a focus on tech education. Users can earn BitDegree tokens as they learn new skills on the platform. In return, recruiters can spend BitDegree tokens in order to find qualified candidates for positions. These candidates will have completed certain course work or passed specific tests.\n\nBitDegree\u2019s team are seasoned business people that have built technology businesses before, but not in education. The team also includes several technical brains that have the skills to implement the described technologies in the white paper. Overall, BitDegree is positioned in a growing market of tech education, and a tokenized blockchain-based education platform could perform well if properly executed.\n\nBitDegree\u2019s ICO lasts through the end of December.\n\nThe INS Ecosystem is targeted at disrupting the grocery industry. Currently, grocery retailers are an $8.5 trillion industry with up to 50% penetration into the consumer\u2019s wallet. INS Ecosystem seeks to cut out the retailers in the middle, connecting manufacturers directly to consumers with aggregated ordering powered by the blockchain.\n\nThe INS token would power this new grocery economy, allowing consumers to buy goods at lower prices while receiving their items via direct delivery through a series of distribution centers. Seven of the top twenty grocery manufacturers have expressed interest in INS\u2019s direct-to-consumer model.\n\nUnited Traders is an established investment marketplace and investment advisory. They offer advice on traditional stocks and equities as well as IPOs, ICOs, and cryptocurrency investing. Their investment marketplace has been live since 2012, but the token sale will allow them to expand their offerings and eventually set up an independent exchange that merges the stock market and the cryptocurrency market into one.\n\nObviously, financial services is an enormous industry, and United Traders\u2019 combined advisory and marketplace gives the company several places to grow their influence and profit margins. In addition, they operate a Russian financial dictionary that\u2019s well trafficked, with explanations and advice. They hope to implement a similar dictionary in other languages.\n\nThe crowdsale ends at midnight on Christmas Eve.\n\nNeuromation is a platform that provides synthetic data and processing power for training machine learning algorithms. Artificial intelligence is growing with amazing speed, but the algorithms need extensive datasets that are well-labelled in order to be useful. Neuromation creates pre-labelled synthetic data using advanced modelling techniques. They also create AI models and help train algorithms.\n\nThe Neuromation token will power a marketplace for synthetic data and for model training services. The AI industry is expected to reach $3 trillion by 2022, and already Neuromation is seeing evidence of demand for processing 1.8 billion images in retail alone. The team has demonstrated successful project completion in consumer goods tracking using AI to 95% accuracy, and team members have decades experience in machine learning, imaging, and big data.\n\nNeuromation is currently in public pre-sale (whitelist) with the main sale beginning on January 7th."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/top5-ai-projects-in-retail-1f8ed8871a0e",
        "title": "TOP5 AI projects in retail \u2013 Neuromation \u2013",
        "text": "AI is applied in retail in many ways, from robots who assist in the store, to post-sale customer support. Here is TOP5 AI projects from Neuromation team.\n\nOSA Hybrid Platform (OSA HP) is a service created with the support of ECR, based on applied artificial intelligence, ensuring the constant availability of goods on the shelf (OSA \u2014 on-shelf availability) in the optimal quantity in real-time.\n\nNeuromation.io has signed the letter of intent with the OSA Hybrid Platform. According to the letter of intent, Neuromation.io is to handle the preparation of the database of marked synthetic data as well as the development and optimization of the algorithm for recognition of goods on the basis of neural networks. The company also declared the possibility to implement object recognition on end-user devices without the need to connect to the server, and set the recognition accuracy index at least 95%.\n\nIn 2010, Japan\u2019s SoftBank telecom operations partnered with French robotic manufacturer Aldebaran to develop Pepper, a humanoid robot that can interact with customers and \u201cperceive human emotions.\u201d Pepper is already popular in Japan, where it\u2019s used as a customer service greeter and representative in 140 SoftBank mobile stores. According to Softbanks Robotics America, a pilot of the Pepper in California\u2019s b88ta stores in both Palo Alto and Santa Monica yielded a 70% increase in foot traffic in Palo Alto, and 50% of Neo-pen sales in Santa Monica were attributed to Pepper.\n\nA delivery system from Amazon designed to safely get packages to customers in 30 minutes or less using unmanned aerial vehicles, also called drones. Prime Air has great potential to enhance the services we already provide to millions of customers by providing rapid parcel delivery that will also increase the overall safety and efficiency of the transportation system.\n\nBrilliant Manufacturing software is the synthesis of more than two decades of working with the world\u2019s most recognized brands in manufacturing and GE plants. This software enables you to predict, adapt, and react more quickly and effectively than ever before. Link design, engineering, manufacturing, supply chain, distribution and services into one globally scalable intelligent system\u2013 spanning HMI/SCADA, MES and analytics.\n\nAmazon\u2019s touted brick-and-mortar locations, known as Amazon Go, employ check-out-free technology that allow customers to shop and leave Customers use the Amazon Go app to check in, but thereafter the entire shopping experience is designed to be automated. Sensors track which objects customers pick up and put in their basket, and customers\u2019 Amazon accounts are automatically charged after exiting the store."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/limited-memory-accelerators-exciting-news-for-gpu-based-and-distributed-machine-learning-80f938704e10",
        "title": "Limited-Memory Accelerators: Exciting News for GPU-Based and Distributed Machine Learning",
        "text": "Here at Neuromation, we are building a distributed platform where mining farms will be able to switch from mining cryptocurrencies, a pointless mathematical exercise designed simply to prove that you have spent computational resources, to what we call knowledge mining: useful computing, especially such computationally intensive tasks as training large neural networks.\n\nThe mining farms have huge computational power, but modern neural networks and especially datasets can be so large that they do not fit on a single GPU. That\u2019s why we are always following the news about distributed learning. And it appears we have got some pretty exciting news.\n\nAt the NIPS 2017 conference currently being held at Long Beach, CA, Swiss researchers from IBM Zurich and EPFL presented a paper on \u201cefficient use of limited-memory accelerators\u201d. What is a \u201climited-memory accelerator\u201d, you might ask? It\u2019s your GPU!\n\nThis somewhat generic figure from the paper shows \u201cUnit A\u201d that has a lot of memory but limited computational power and \u201cUnit B\u201d that has much less memory but is much better at computation. Exactly like a desktop computer with a CPU and lots of RAM communicates with its video card, which has much less memory on board (say, 6GB instead of 32GB RAM) but can train neural networks much faster.\n\nIn the paper, D\u00fcnner et al. present a framework where \u201cUnit A\u201d can store the entire dataset in its memory and choose, in a smart way, which subset of the dataset to present to \u201cUnit B\u201d right now. Different points in a dataset have different utility for training: some points contain more information, uncover new important features, and so on. It turns out that the resulting algorithm can converge much faster than just randomly sampling points or their coordinates (which is the current standard approach). About 10x faster, as it appears from the experiments.\n\nThe scope of this work is both a strength and a weakness. On the positive side, D\u00fcnner et al. present their results about a general form of block coordinate descent, which is a general enough setting to cover most modern machine learning models. But, on the negative side, their results as stated in the paper apply only to convex generalized linear models, which means that they cover models like lasso regression and support vector machines (SVM) but not, unfortunately, deep neural networks. The whole point of having a deep neural network is to move beyond convex objective functions, and, alas, convexity is a pretty central assumption throughout all the theorems of (D\u00fcnner et al, 2017).\n\nStill, being able to train SVMs 10x faster on large-scale training sets on the order of tens of gigabytes is a great result which will no doubt be useful, for example, for many business applications. Hence, we hope that implementations of similar algorithms will find their way into the Neuromation platform.\n\nAlthough the interaction between CPU and GPU on a desktop computer is the central example of these \u201cUnit A\u201d and \u201cUnit B\u201d, it is easy to imagine other scenarios. For example, \u201cUnit B\u201d can be one of the customized ASICs for training deep neural networks that Bitmain has recently announced. But, as I said, adapting this idea to modern deep neural networks will require more research insights. Let\u2019s get to work\u2026"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-takes-santa-clara-93c7a3283ab7",
        "title": "Neuromation takes Santa Clara! \u2013 Neuromation \u2013",
        "text": "Neuromation team spent the last two days at Blockchain Expo North America in Santa Clara. Neuromation was showcased among the latest developments in the Blockchain arena, in both emerging and more established markets.\n\nOur CEO Maxim Prasolov and ICO Compliance adviser Yuri Kundin were speaking on the Enterprise and Blockchain tracks, among speakers from Bank of America, Toyota and IBM.\n\n\u201cIt was a great opportunity to form new ideas and meet Blockchain enthusiasts, along with presenting Neuromation Platform to the wide audience. For these two days we\u2019ve met crypto enthusiasts from all around the world, like Intuit, Otoy and MERA. Big companies like Oracle, Macy\u2019s expressed interest to the synthetic data technology. People stood in line to our stand and we are satisfied starting our road show in US. Blockchain Expo brings us the new ideas and new partners\u201d, \u2014 says Maxim Prasolov."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/top5-ai-projects-in-creative-industries-7c45040265c7",
        "title": "TOP5 AI projects in creative industries \u2013 Neuromation \u2013",
        "text": "AI is changing the creative industry, from fashion to gaming. Take a look at the TOP5 AI projects in our blog.\n\nIn 2017, Nike partnered with advertising agency R/GA to launch the Nike On Demand campaign which leveraged IoT data to fuel an AI assistant service to encourage exercise/behavior adherence. The six week campaign aimed to use data, algorithms and machine learning to shift the consumer perception of Nike in Germany from \u201ccool product brand\u201d to \u201cperformance partner\u201d to help athletes meet their performance goals.\n\nA human-driven 1:1 service on WhatsApp that keeps athletes engaged, focused on their goals, and connected with the best of Nike \u2014 from one-on-one training with professional athletes to VIP access to sold-out Nike experiences.\n\nCalifornia-based startup, Grabit Inc., provides manufacturing support through the use of automated robots and machine learning software. We channel the same static cling that makes a balloon stick to your hair and use it to automate the handling of virtually any material. Electroadhesion has the finesse to handle something as fragile as an egg, as flimsy as soft fabric and as unwieldy as a 50 lb. box. This unprecedented flexibility empowers entirely new possibilities in automation. Electroadhesion is a revolutionary technology that will unleash the potential of every factory and warehouse on the planet.\n\nThe aim is to develop an AI system that can intelligently design videogames, as part of an investigation into the ways in which software can design creatively. ANGELINA focuses a lot on describing its process to others, seeking out information for itself, and trying to convey meaning to people through its work, all of which aren\u2019t really things we care about when we look at generative software. Since its earliest form, in 2011, it has created hundreds of experimental video games, received acclaim in an international game-making competition, and had its work featured in a New York gallery exhibit.\n\n3D printing has the potential to massively democratize access to manufacturing. Artificial intelligence plus 3D printing could yield some really transformative experiences. Born from a spark of inspiration in 1983, 3D Systems has run on innovation for over 30 years. Co-founded by the inventor of 3D printing, Charles (\u201cChuck\u201d) Hull, 3D Systems has grown into a global 3D solutions company focused on connecting our customers with the expertise and digital manufacturing workflow required to solve their business, design or engineering problems. 3D Systems\u2019 products and services are used across industries to assist, either in part or in full, the design, manufacture and/or marketing processes. 3D Systems\u2019 technologies and materials are used for prototyping and the production of functional end-use parts, in addition to fast, precise design communication. Current 3D Systems-reliant industries include automotive, aerospace and defense, architecture, dental and healthcare, consumer goods and manufacturing.\n\nConstantly developing technologies for various industries, theoretically, Neuromation can provide services for companies that create or upgrade images. With the help of GANs, Neuromation might be able to create super-resolution images that later could be used in media, retail and even fashion blogs, proving that AI is closer to creative industries that you could imagine."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-%D0%BF%D1%80%D0%BE%D0%B4%D0%BB%D0%B8%D0%BB%D0%B8-%D0%BF%D1%80%D0%B5%D0%B4%D0%B2%D0%B0%D1%80%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%83%D1%8E-%D0%BF%D1%80%D0%BE%D0%B4%D0%B0%D0%B6%D1%83-%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%BE%D0%B2-89d8086a1c1d",
        "title": "\u041f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 Neuromation \u0437\u0430\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0430 \u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u0439 \u0444\u0430\u0437\u0435 \u043f\u0440\u0435\u0441\u0435\u0439\u043b\u0430 1,5 \u043c\u043b\u043d \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/how-neuromation-will-change-the-3-trillion-ai-industry-84dc6d92d2c4",
        "title": "How Neuromation Will Change the $3 Trillion AI Industry",
        "text": "While news outlets forecast the coming AI revolution, practitioners know the AI revolution has already started. Companies like Google, Amazon, Facebook, IBM, and Microsoft are pouring millions into funding for artificial intelligence and machine learning, processing billions of data points every day. So far, 2017 has seen over $6 billion in AI funding for independent ventures as well. Hedge funds use AI to trade billions of dollars of securities in fractions of a second, and new advances in transportation, shipping, health care, and retail promise to make AI-implementation commonplace.\n\nThe global market for artificial intelligence is projected to grow to over $3 trillion by 2024, an astronomical growth rate as AI makes our lives easier and more efficient. However, in order to achieve that lofty growth, the artificial intelligence market will have to overcome several bottlenecks that limit adoption. Neuromation offers a solution to the current fragmented AI landscape, increasing access and speed with which AI technologies can be deployed.\n\nWhile demand for AI is high and only growing, the supply of good AI models and data is slowing down the development process. At Neuromation, our solution addresses the need for data sets, computing power, and a marketplace for AI models.\n\nWe\u2019re living in the age of big data. It\u2019s fairly easy to obtain huge data sets in virtually any field where you\u2019d like to apply AI. The problem is those data sets haven\u2019t been described and annotated so that they\u2019re usable for training neural networks. Traditionally, humans have to do the job of annotation by hand, making dataset creation challenging, costly, and prone to bias. For example an average retail set has 150,000+ items. A deep neural network needs thousands of labeled photo examples for each item. Doing this by hand would take years.\n\nNeuromation\u2019s solution utilizes synthetic data to create training datasets for AI applications. We automate the process of labelling and annotating data by creating datasets that are tailored to the task at hand. Using synthetic data drives down the cost and time required to create a training dataset.\n\nAnother limitation of current AI is the processing power available to developers and researchers. The computing power required to train a neural network often means that an algorithm make take days or weeks in training with a single computer. Often times, the model needs to be tweaked and run again, making the development process is slow and onerous.\n\nWe propose tapping the existing processing power of miners on blockchain networks. Researchers and companies working on AI are willing to offer compensation in return for processing power, and allowing miners to earn money from working on machine learning problems means greater rewards for the miners and faster processing times for AI developers. The Neuromation white paper estimates miners could earn 3 to 5 times more mining NeuroTokens versus other types of mining.\n\nThe next generation of development in AI will require tools for uploading and sharing AI models on a marketplace for use in various applications. In addition to providing synthetic data and computing power on the Neuromation marketplace, we\u2019ll also build, train, and maintain models for clients and allow clients to import their own models. In the year after Neuromation\u2019s launch, we expect to sell over 1,000 models, and the model marketplace will be one of our primary revenue drivers.\n\nSynthetic data and distributed processing have exciting practical applications for everyday businesses. For example, Neuromation has partnered with leading retail brands to identify products on a shelf and give suggestions about shelf layout and efficiency based on images taken from a smartphone. Using our technology, we\u2019ve achieved a 95%+ accuracy in inventory recognition. The market for AI in retail reaches up to 40 billion images per year, and 85% of customer interactions will be handled by AI, according to Gartner.\n\nNeuromation is also applying synthetic data and dynamic modeling to industrial applications to monitor and predict factory or shipping operations in real time using computer vision. In addition, we\u2019re looking into how computer vision and neural networks can help with biotech diagnostics and drug discovery. Many questions and hypotheses that were too variable or work-intensive to investigate using traditional means can now be answered using computer vision.\n\nLearn more about Neuromation by visiting our website. We\u2019re also available via chat to answer any questions!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/pre-sale-extended-b69b7161e4f5",
        "title": "Neuromation Pre-sale extended! \u2013 Neuromation \u2013",
        "text": "We are very pleased to announce that we significantly surpassed our first goal of 2,000 ETH during the first phase of our whitelist presale.\n\nAt the moment of publication the estimation of funds collected is more than $1,5 mln.\n\nOver 4 mln Neurotokens (NTK) were distributed between whitelisted contributors.\n\nThank you for your trust and support!\n\nWe have also concluded a partnership with GigaWatt \u2014 overcoming our intermediate goal (40,000 GPUs) in the number of available GPU\u2019s for knowledge mining. A media announcement will be published later this week.\n\nAs you know, the core team of Neuromation consists of engineers and entrepreneurs. Our mission is to build THE BEST AI platform on the market. We have a few important updates on token sale.\n\nWe have decided to extend the white list pre-sale until January 1st 2018.\n\nOur public token sale will start January 7th and will finish on 15th of February.\n\nHere are the benefits to the participants of the token sale:\n\n\u201cAdditional bonus for early birds\u201d \u2014 We extend a 5% bonus to all early whitelist presale contributors. If you contribute before Nov 29th, an additional 5% will be added to your NTK balance.\n\nWe also updated our volume bonus structure taken effect immediately and through the end of public sale. Both new and old purchasers will enjoy additional bonuses added to the regular bonuses. (Bonuses will be added retroactively to all who contributed)\n\nRegular bonuses are 15% for amounts of contribution less than 3 ETH, and 25% for 3 ETH and more.\n\nWe hope that these new measures demonstrate how much we value your continued contribution and support. In useful computing we trust!\n\nNeuromation\u2019s team, always available in NeuroToken (NTK) Chat"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/black-friday-neurotoken-ntk-sale-daed61a44e5b",
        "title": "Black Friday Neurotoken (NTK) sale! \u2013 Neuromation \u2013",
        "text": "Black Friday** is coming and we have come up with a kicking offer!\n\nAny purchase made from 12pm GMT 24th November 2017 to 12pm GMT 25th November 2017 will bring you additional + 5% Bonus.\n\nHurry up! Only 24 hours to get up to 30% bonus*.\n\n*30% bonus will be received in case of minimum of 3 ETH investment.\n\n**Limited offer valid for 24 hours starting 12 GMT 24 Nov 2017, ending 12 GMT 25 Nov 2017. 5% additional bonus applies to the current bonus system of Neurotoken (NTK) sale. No purchase limit."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/democratizing-access-to-artificial-intelligence-eac06b8d74f4",
        "title": "Democratizing Access to Artificial Intelligence \u2013 Neuromation \u2013",
        "text": "The benefits of AI can spread widely through a novel approach by Neuromation, leveraging the decentralized hardware infrastructure of blockchain to generate data for deep learning applications.\n\nThe recent explosion of concrete and widespread applications of Artificial Intelligence is due to the availability of ever more powerful hardware platforms and sophisticated algorithms, coupled with very large amounts of data. The effectiveness of machine learning, a subset of AI, of artificial neural networks and of deep learning, themselves subsets of machine learning, surprised even experts.\n\nAI has been around since the birth of digital computers in the 1950s, and machine learning and neural networks emerged in the 1980s. The improvement of algorithms and how much of a benefit they represented became evident when powerful enough hardware was available to experiment with them at the beginning of the 2010s. In particular, the same specialized approaches, Graphical Processing Units (GPUs), that were used to create the advanced graphics of videogames, could be used for the parallel processing required by machine learning, with dramatic acceleration of results.\n\nIn turn, software development more and more relied to decentralized collaboration, with open source winning, and repositories of reusable components becoming so important, that those teams and corporations that didn\u2019t want to participate felt they were less and less able to compete. Today it is natural to take it for granted that the most advanced approach is going to be published in scientific papers, together with the algorithms to implement it, example code to execute it.\n\nThis leaves the role of data as crucial, especially in commercial applications, that can\u2019t rely on academic repositories. Large corporations have a decisive advantage, being able to dedicate resources to the collection and the curation of data from the physical world, which they keep as their competitive edge. Often, as it is the case with Google, and Facebook, the data is not only collected directly by the corporations, but is also provided voluntarily by the users of their applications.\n\nNeuromation democratizes the access to advanced artificial intelligence approaches for developers, allowing the use of the widely deployed GPU network of blockchain mining to create synthetic data for the training of neural networks.\n\nSecuring the blockchain depends on calculations that solve cryptographic challenges, but they do not themselves produce useful results, beyond the important and concrete effect of making the blockchain transactions unforgeable. The novelty of Neuromation\u2019s approach is to achieve this while generating useful calculations. As importantly, the synthetic data generated is per definition properly labeled, since the generating computer knows what image it is drawing a priori. It is essential to train the networks from properly labeled data sets, and this is not always achievable, even when the collection of real-world data is available.\n\nIn the video below you can watch a conversation with Andrew Rabinovich, Advisor to Neuromation, and Director of Deep Learning at Magic Leap, talking about why deep learning is important, and how these new approaches can spread the knowledge and benefits of it, democratizing the access to artificial intelligence.\n\nHere\u2019s the transcript of the conversation:\n\n\u2013 [David] Welcome everybody and welcome to Andrew Rabinovich, who is the Director of Deep Learning at Magic Leap, an advisor to Neuromation. Hello Andrew how are you?\n\n\u2013 We will be talking about exciting topics that a lot of people are talking about but not a lot of people practice to the level that you do, artificial intelligence, deep learning, and more exotic topics trying to make them comprehensible. So, when did you start in AI?\n\n\u2013 So the story goes back almost 20 years when I was still an undergraduate at the University of California in San Diego when I started building computerized microscopes, or as we called them back then sightometers where we were trying to detect cancer in tissue samples. Then we were using basic image processing techniques, but very quickly I realized that those weren\u2019t sufficient and much more research, and back then machine learning, and computer vision was needed, and that\u2019s when I started my graduate studies in Computer Vision and Machine Learning. So since 2002, I\u2019ve been sort of studying theoretical and applied machine Learning and computer vision, and nowadays we call this classical vision and machine learning, and I\u2019ve been doing that until 2012, when the deep learning revolution kind of occurred. At that time I was working at Google, working on all sorts of things related to photo-annotation and computer vision and there I sort of quickly realized that deep learning has the capability of solving problems that classical vision has never dreamed of. Then almost overnight I quickly switched to deep learning altogether and started spending all of my time on the theory of deep computation as well as its applications to computer vision.\n\n\u2013 In artificial intelligence since the 80s or even before there were two kinds of approaches. A top down approach using expert systems, rule-based classification, all kinds of ways that we tried to teach computers, common sense, and how to make decisions based on our understanding how we reason. And on the other hand there were bottom up approaches spearheaded by artificial neural networks that tried to abstract the rules of reasoning without making them explicit, but as if the computers were able to discover these rules by themselves. And the neural network based approach appeared at the time to have very severe limits and was kind of the losing part of the AI balance and the AI approach. What made it burst into the forefront again? You said in 2012 suddenly something happened?\n\n\u2013 That\u2019s absolutely right, so neural networks and artificial intelligence as a field have been around since 60s from the days of Marvin Minsky and McCarthy. When people have been thinking about, from psychology, abstract math, and philosophy, about artificial intelligence but of course the computation was the limiting factor into any kind of experimentation and proofs. When talking about artificial intelligence we have to be very clear that it\u2019s not a fundamental science whose mission is to describe and understand nature, but rather an engineering discipline that\u2019s tasked with solving a practical problem. In order to solve problems sort of two critical components must come together. First is the computing power, or the engine that processes information, and the other is the data, or that gasoline for an internal combustion engine. The reason why this revolution took place in 2012 was because these two components came about together. The existence of fast compute, mainly the GPUs, and the abundance of images on the internet with a large presence of mobile devices able to capture information, whether its images, or speech, or text. Ironically the underlying math and the theory of artificial neural networks, or as we call them deep neural networks nowadays hasn\u2019t changed very much. The model of a neuron was introduced by Frank Rosenblatt in the 60s which is until today very much a current one with a small modification that an activation function, or the non-linearity has been simplified even further from a sort of a logistic function like a sigmoid or hyperbolic tangent to something even simpler that\u2019s a rectified linear unit. Otherwise the general structure of a neural network has remained the same. One interesting development that was brought forth by Yann LeCun in the late 80s is this notion of convolutions which was inspired by biological experiments of by Hubel, Wiesel indicating that there\u2019s this hierarchical structure incorporating pooling of simple and complex cells and this is what the original multi-layer perception was missing, and with the introduction of these convolutional features that was first manifested by neural network called neurocognitron by Fukushima in 1988. These networks are really the models that we\u2019re using today. Of course the models are now deeper and wider and run across multiple machines, but the essence is pretty much the same. The learning algorithm, mainly back propagation, using stochastic gradient descent was introduced by Geoff Hinton in 1986, so essentially for the last 30 years the guts for this technology hasn\u2019t changed. So the pillars of success effectively is the data and the computer.\n\n\u2013 And indeed the self-fulfilling prophecy of Moore\u2019s Law is what enabled for the past 50 years computers to become more and more and more and more and more and more and more and more powerful. 50 years of exponential growth will really make a big difference. So as the availability of the large amounts of data and very powerful computational platforms became available neural networks and deep learning started to perform. There was a large data set, there is still large data set for objectively testing the performance of neural networks in vision tasks and if I am not mistaken the test as performed by humans would beat machine recognition, but image recognition by machines started to get better and better and today machines are as good or better than humans in recognizing tasks on that data set, is that right?\n\n\u2013 That\u2019s correct. So one of my colleagues from Stanford, and now is at Google, Professor Fei-Fei Li did a monumental effort in comprising together this data set that you\u2019re talking about that\u2019s called ImageNet, it\u2019s a collection of about 10 million images comprised of about 1000 categories with the task of being able to identify the prominent category that\u2019s exhibited in a given image. The state of the art today, of the best performing deep neural network yields an accuracy of about 97% for top five classification, while humans are only able to achieve 95% accuracy. Having said that, the deepest and largest deep neural networks that perform the best on these data sets have about the same number of neurons as a little, tiny rodent, like a mouse or a rat. What these approaches are good at, at recognizing patterns. So unlike humans they never forget and they never get distracted and there\u2019s never an ambiguity between two types of bridges or two types of airplanes, something that humans aren\u2019t very good at reasoning about. However, I want to point out right away that these tasks are very loosely related to any kind of general intelligence, decision-making, or reasoning. These tasks are primarily focused with pattern matching and detection of observed phenomenon. So these things are really very good at memorizing with some amount of generalization to unseen observations, while humans are actually made to infer from very limited learning. That\u2019s why this particular data set requires there to be hundreds and thousands of training examples for these networks to get really good at doing what they do, but as soon as you scale the number of representations to tens of examples, then human performance will only suffer slightly while the degradation and accuracy for the machines will be very significant.\n\n\u2013 When you talk about the deepest deep learning systems, you are referring to the layers of analysis and abstraction is that correct? And how many layers are we talking about when we talk about the deepest deep learning today?\n\n\u2013 So the most widely used deepest networks is something that\u2019s called a residual network, or ResNet, that comes from Microsoft research and that has about 151 layers where each of those layers is also often made up of sub-components. To my knowledge people in academic circles have pushed the boundaries of these networks to go up to 1000 layers, but the question isn\u2019t just about the number of these layers but also about the width of the layers and their respective depth, in fact, there\u2019s some theoretical results that suggest that only with a two layer neural network without specifying its width and depth of each specific layer it is possible to approximate any mathematical function, which suggests that with a two layer neural network, basically a network that\u2019s comprised of two linear layers with two non-linear activations that follow were able to approximate any mathematical function, hence were able to solve any machine learning task. Of course the fewer layers you have the harder is to learn, that\u2019s why people build these ginormous things because the training becomes much simpler.\n\n\u2013 Now the data sets out of real world are hard to collect and the more differentiated areas you want neural networks to work on, the larger the task of collecting, separate well-designed data sets for those various tasks becomes and it is not a coincidence that large corporations like Facebook, and Microsoft, and Google are very busy in doing that with the cars that are taking photos of streets in the world, or whether it is analyzing the photos that are uploaded by the billions over social networks and so on. However, that really is a limiting factor for start-ups with teams of passionate and creative individuals to take advantage of deep learning approaches.\n\n\u2013 That\u2019s absolutely true. In fact, since the emergence of deep learning, the protection of intellectual property has shifted from algorithms, which now everyone freely shares, you\u2019re not able to publish any scientific achievement without publishing the algorithm and the code along with it, shifted to the protection of data. So now rather than being the most powerful team because you have the best algorithms, now you\u2019re the most powerful enterprise because you have the best data and that\u2019s exactly why Google and Facebook are ahead of most other companies, not because they have more computers or because they have smarter researchers, but simply because they have most data. Data comes in two flavors. Raw data, something that one can acquire just by going around and recording, whether it\u2019s speech, text, voice, or any other modality. But more importantly is the annotations of the data and that\u2019s where it becomes very, very difficult to scale. Having accurate and detailed annotations of the data, or as we call them in the scientific circles, ground truth, is very, very hard to obtain. One option is to collect the ground truth by the virtue of recording, for instance if you want to take pictures of cars you have to enter the location and position and the make and model of each vehicle before you take a picture. And the second approach is once the data has been collected then it needs to go through this manual, and very laborious, expensive, and slow effort of manually labeling it and as you know there exist such services as Mechanical Turk, or CrowdFlower, and many others that actually allow you to submit your data to these services where human labelers go through these tedious tasks of labeling. Aside from being expensive and slow, the problem is that a, humans make mistakes, b, often times the questions that are being asked are ambiguous, where it\u2019s not trivial like if show you a picture of a certain animal, and they say is it this kind of a cat or that kind of a cat, unless you\u2019re a true expert of cats, you at best would guess, but that guessing would then translate to mistakes in the model that you would train from that data and finally humans are not able to do certain tasks at all. For instance if I show you an image of some gallery or a church and I say, which direction is the light coming from? Or how many light sources are there in the room? This is something that humans certainly can\u2019t do because people are very good at relative estimations but nothing specific, or for an example if I give you an image of a street with a car on it and I say, how far away is the car from a traffic light? You would say, you know three, four meters, maybe five. I look at it I\u2019ll say it\u2019s two meters maybe 10. But it\u2019s impossible to say exactly what the absolute metric distances are, but in fact that\u2019s exactly what\u2019s required for self-driving cars or autonomous navigation by any kind of robot. So those things are extremely difficult. The reason why these Google Cars and all these other self-driving companies have these crazy sensors on the cars as you see them driving, is for that reason exactly, but as they drive and take pictures of the world, they want to measure everything as precisely as possible because they know that humans aren\u2019t able to label such things. So given these two approaches of labeled and unlabeled data this directly translates into two kinds of machine learning algorithms. Ones that are called supervised learning, where you learn by having examples with annotations. And unsupervised learning, where you\u2019re just trying to get an understanding without any real supervision. And a great example of that is these generative adversarial networks, as you\u2019ve probably seen they are able to render these crazy images of cats and dogs and people. But that\u2019s still not enough so the approach that many people have taken including guys that open AI and a deep mind is to go the synthetic data route and to create synthetic environments where everything is perfectly labeled by construction. As you build this 3D world, with cars, people, trees, streets, and so forth, by virtually creating all of this in computer graphics software you automatically know the locations and positions of everything. You know which way the light balances. You know the direction of the surface normals. You know the reflectance properties of all the materials. So effectively you have everything you want. The problem with that, go ahead sorry.\n\n\u2013 So you started to mention what is the breakthrough in the approach of Neuromation, that you are advising. The approach, that rather than relying on a data set collected from the physical world, creates data set, and these data sets are called synthetic data. And one of the reasons why synthetic data is so interesting is exactly because it opens the possibility for teams that are not at Google, not at Microsoft, not at Facebook, to take advantage of deep learning approaches and neural networks, and apply AI techniques to the problems that they are passionate about. So you were saying some additional features of synthetic data and what the consequences of this approach are?\n\n\u2013 So as you correctly point out, since it\u2019s very expensive to obtain real data with proper annotations, and since unsupervised learning techniques are not yet powerful enough I think going the route of creating synthetic data in training models on those data sets I think is a practical path forward. There\u2019s still many challenges as to how to create this data, whether this data resembles the real world environment, but I think these tasks or these problems rather are no harder than the problems in unsupervised learning. So I think tackling AI from that perspective is an excellent endeavor.\n\n\u2013 And to concretely describe in terms of hardware, the synthetic images of artificial three-dimensional worlds that can be photographed through virtual cameras are created through the graphics cards that are practically the same, or used to be the same, that are used are used for gaming, and that is what are made by NVIDIA, or other computer chip and card manufacturers, is because the same computational power required for rendering the ever more beautiful images and detailed images of artificial worlds in computer games is the same hardware that can be used for synthesizing the images to train these neural networks.\n\n\u2013 That\u2019s completely right. It used to be that most photo-realistic graphic renders were based on CPUs rather than GPUs, things like Maya and V-Ray renders, but recently, maybe two years ago Unreal Engine started working on renders using GPUs and it is becoming a fairly common practice in this deep learning community to use these GPUs and video cards from NVIDIA mainly, and use Unreal Engine to render these synthetic examples, something that can be done effectively in real time. Traditional renders on CPUs although were of higher visual fidelity took much, much longer, up 15, 20 minutes per image to get a good rendering, with speeds that slow one could argue that it\u2019s not feasible to produce millions of examples to train deep neural networks, but with the Unreal approach running on a GPU practically in real time, then all of a sudden this solution becomes attractive because you can create a tremendous amount of data in a reasonable amount of time so that you can train these large deep networks. So again it\u2019s this confluence of technology for production of synthetic data together with an abundance of really, fast and powerful graphic cards.\n\n\u2013 And I don\u2019t know whether it\u2019s a coincidence or beautiful synchronicity that one of the reasons, or an additional reason, these cards more and more widely deployed, not only in the personal computers of gamers, but also in server racks of specialized but still widely distributed sell-ups, it is because the same card, the same type of hardware, the same type of GPU based computation is also used in mining Ethereum, or mining certain types of cryptocurrencies, and the cryptocurrency mining since now we are jumping from the field of AI into the field of blockchain is worth repeating very briefly is just a metaphor. Nobody\u2019s mining any kind of rare metal. Actually I think it is a somewhat unfortunate metaphor. I personally prefer to use the metaphor of weaving, where there\u2019s a pattern emerging from the collaborative effort of intricate cryptographic work, but in any case whether we use one or the other these operations are necessary to ensure the robustness of the trust network that blockchain operations implement and to make it impossible to falsify the transactions, and to falsify the operations over this network, to make it computationally unfeasible through the effort expending in the cryptographic operations across all those who participate. And so Neuromation actually put the two together. Neurommation says there is this challenge of democratizing access to the powerful approaches of neural networks and deep learning and artificial intelligence on one hand that synthetic data created through GPUs and on the other hand they observe here is this widely available hardware network of GPUs deployed for blockchain operations, and they put the two together. So describe a little bit how that works and why is that so powerful?\n\n\u2013 So it\u2019s quite remarkable that everything that\u2019s involving what in my mind the next breakthrough in AI, mainly the training of deep neural networks, as well as creating data for creating data for training these neural networks can be done on identical hardware. So in the past you had to have some crazed supercomputer that would do your modeling, then you would have to have your graphics render forms that are made of entirely different hardware to create the data for simulations and then you have to put them together. Nowadays it\u2019s very interesting that on a single GPU you can both create the data for training and train. So to sort of add to the coincidence, there\u2019s been this explosion of cryptocurrency mining that also uses these GPUs. After this cryptocurrency craze has kind of settled down people are starting to realize that the biggest flaw in this whole design, in the mining process people have to perform significant amount of computation, however, at the end of the computing process they get rewarded with their Bitcoins or other cryptocurrencies, but in fact the result of computation is completely discarded and thrown away. It\u2019s almost like you go and learn French and then at end of it you get a piece of chocolate but everything you learned is completely forgotten. So it seems kind of a waste. So the idea behind Neuromation is that instead of mining just for the sake of getting a key to get more Bitcoins or other cryptocurrencies, you mine by the virtue of solving an actual AI task, whether it\u2019s training a deep neural network or creating synthetic data for training these neural networks, and at the end of the day you still get your cryptocurrency but the result of your computation is actually something useful that can be applied further down the chain for some practical application.\n\n\u2013 This is a bit similar to how those squiggly puzzles that are often displayed when you set up a new account on an online platform, not only try to keep out scammers and spammers but when you look at the details, and for example this system is reCAPTCHA, it turns out that the process of verifying that you are human because you are able to recognize the words that are displayed, you are actually digitizing books, or solving other types of labeling tasks. So practically the same way as this uses humans who need to solve a task, but a task rather than been useless, it\u2019s a useful task, similarly Neuromation uses the computational power of the blockchain planetary computer to solve tasks that are needed for the cryptographic network to be secure, but rather than applying that power to solve useless tasks it is applying that power to build knowledge, to build useful computation.\n\n\u2013 That\u2019s absolutely right. I think of this paradigm that\u2019s similar, remember back in the 90s there was this project called SETI@home where people were trying to look for extraterrestrial activity and they didn\u2019t have a single computer to do all the work, so they tried to spread this out across all the PCs that were available in the world, but then you would just get bragging rights, saying that you helped find E.T., and that was a reward in itself. Now with the presence of blockchain and cryptocurrency it\u2019s become very natural to spread computation across all the owners and users of GPUs to be able to do these method processes and that\u2019s why I think this notion of democratizing AI is a good description of Neuromation, because at the highest level it is really that. It provides people with an opportunity to a, gather training data by the virtue of synthesizing it and b, to train deep neural networks on the highly required GPU processors. These things are abundant in the world as you said for miners, gamers, and general public overall, but the standard mode of operation is you are Google, or Facebook, or Amazon, you go and build a ginormous warehouse with these GPUs and that\u2019s the way, why you succeed, you often hear of conferences, engineers from Google present some scientific result and they say, we spent enough enough energy on these GPU data centers that would power 5,000 single-family homes for three months. On one hand they get some 3% improvement on this ImageNet data set, but in the reality it\u2019s a, a tremendously unfair advantage to the smaller players, and b, it\u2019s a ridiculously irresponsible way to waste energy that be conserved otherwise, but through this democratized approach of this decentralized model training and synthetic data generation I think these limitations hopefully will soon vanish and everybody else will be able to achieve the same results as the big guys.\n\n\u2013 So Andrew, thank you very much for this conversation. Certainly your work in the field of artificial intelligence and deep learning is of great inspiration and we both are very excited and are looking forward for Neuromation to build and deliver their platform so that access to the advanced tools of AI can be democratized, because we all love and use Google, Facebook, and the tools that large corporations make available, but even they realize that talent is everywhere, creativity is everywhere, and we do need to empower those groups all around the world to express their ideas and that is what I am personally looking forward to see soon. Thank you very much for the conversation today.\n\n\u2013 Thank you, I\u2019m very excited about seeing Neuromation make these strides forward and are very passionate about making these tools available to large masses, because I think only through large participation from around the world will we be able to make next leaps in AI and intelligence overall."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/new-advances-in-generative-adversarial-networks-or-a-comment-on-karras-et-al-2017-207049cb1e7a",
        "title": "New Advances in Generative Adversarial Networks, or a Comment on (Karras et al., 2017)",
        "text": "A very recent paper by NVIDIA researchers has stirred up the field of deep learning a little. Generative adversarial networks, which we will talk about below, have already been successfully used in a number of important problems, and image generation was always at the forefront of these applications. However, the work by Karras et al. presents a fresh take on the old idea of generating an image step by step, gradually enhancing the image (for example, increasing its resolution) as they go. To explain what is going on here, I will have to step back a little first.\n\nGenerative adversarial networks (GANs) are a class of neural networks that aim to learn to generate objects from a certain class, e.g., images of human faces or bedroom interiors (a popular choice for GAN papers due to a commonly used part of the standard LSUN scene understanding dataset). To perform generation, GANs employ a very interesting and rather commonsense idea. They have two parts that are in competition with each other:\n\nIn other words, the discriminator learns to spot the generator\u2019s counterfeit images, while the generator learns to fool the discriminator. I refer to, e.g., this post for a simple and fun introduction to GANs.\n\nWe at Neuromation are following GAN research with great interest due to many possible exciting applications. For example, conditional GANs have been used for image transformations with the explicit purpose of enhancing images; see, e.g., image de-raining recently implemented with GANs in this work. This ties in perfectly with our own ideas of using synthetic data for computer vision: with a proper conditional GAN for image enhancement, we might be able to improve synthetic (3D-rendered) images and make them more like real photos, especially in small details. We are already working on preliminary experiments in this direction.\n\nThis work by NVIDIA presents a natural idea: grow a large-scale GAN progressively. The authors begin with a small network able to produce only, e.g., 4x4 images, train it until it works well (on viciously downsampled data, of course), then add another set of layers to both generator and discriminator, moving from 4x4 to 8x8, train the new layers, and so on. In this way, they have been able to \u201cgrow\u201d a GAN able to generate very convincing 1024x1024 images, of much better quality than before.\n\nThe idea of progressively improving generation in GANs is not completely novel; for example,\n\nHowever, all previous approaches made their progressive improvements separately: the next level of progressive improvement simply took the result of the prevoius layers (plus possibly some noise). In Karras et al., the same idea is executed in a way reminiscent of unsupervised pretraining: they train a few layers, then add a few more, and so on. It appears that this execution is among the most straightforward and fastest to train, but at the same time among the best in terms of results. See for yourself:\n\nNaturally, we are very excited about this advance, which brings image generation, which was first restricted to small pictures (from 32x32 to 256x256 pixels), ever closer to a size suitable for practical use. In my personal opinion, GANs (specifically conditional GANs) may be the exact architecture we need to make synthetic data in computer vision indistinguishable from real data."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/top-5-ai-projects-by-neuromation-f8e09a652b17",
        "title": "Top 5 AI projects by Neuromation \u2013 Neuromation \u2013",
        "text": "This week we\u2019ve picked up TOP 5 AI projects in medicine, discovering how technologies are helping people live better and longer. Enjoy!\n\nZebra\u2019s mission is to provide radiologists the tools they need to make the next leap in patient care. The demand for medical imaging services is continuously increasing, outpacing the supply of qualified radiologists and stretching them to produce more output, without compromising patient care. Zebra is empowering radiologists with its revolutionary AI offering which helps health providers manage the ever increasing workload without compromising quality- at a flat, transparent $1 per scan.\n\nOncora Medical offers a software product that enables precision radiation oncology. Precision medicine is the practice of designing personalized treatments for each patient based on their specific characteristics and medical history. Precision radiation oncology is the practice of designing personalized radiation treatments for each individual patient seen by a radiation oncology department.\n\n3Scan is turning tissue biology and histopathology into data science, giving researchers the insights they need to break new ground. 3Scan\u2019s KESM produces digital 3D tissue models with micron-scale resolution at 400 times the throughput of traditional microscopes. Their customized software enables quantitative image analysis of diseases like cancer, Parkinson\u2019s, Alzheimer\u2019s, and Huntington\u2019s, opening up the potential for new diagnostic and therapeutic solutions.\n\nMonBaby Smart Button tracks your baby\u2019s movements and sends an alert to your smart phone or tablet when your attention is needed.\n\nIt alerts about stomach sleeping, stoppage in breathing movements, falls and unusual activity \u2014 all of the data is sent to your smart phone or tablet with easy access through the MonBaby app on iOS or Android devices.\n\nNeuromation partnered up with MonBaby, the leading in tracking technology for youngsters, with the vision of creating a smart camera to add to their existing award-winning offering.\n\nBay Labs is at the forefront of bringing deep learning advances to critical unsolved problems in healthcare. At Bay Labs, believe that deep learning has potential to dramatically impact the leading cause of death \u2014 cardiovascular disease. In order to serve the largest number of people, the company aims to amplify the benefits of deep learning by providing high-performance algorithmic capabilities to assist with healthcare challenges on a global scale.\n\nDeep learning is a branch of artificial intelligence that follows decades of development from academic research, industry theory, and experimentation across an increasing image-centric and data-driven world. Today, deep learning technology is rapidly advancing numerous fields such as biomedicine, telecommunication, transportation, and finance.\n\nMedicine is emerging as one of deep learning\u2019s biggest areas of human impact: it is used for genomic testing and sequencing, diagnostic image analysis, drug development, and early detection of disease."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-burns-it-up-efec889dc455",
        "title": "Neuromation burns it up! \u2013 Neuromation \u2013",
        "text": "Most of the existing cryptocurrencies have an issuance cap and artificially create coin deficit to increase the value of the tokens issued. Unlike them, the volume of coin issuing in Ethereum was never capped, but at the last DevCon3 conference in Canc\u00fan, the platform creator Vitalik Buterin said that in the future he plans to limit the issuance.\n\nThe platform team is now actively working on choosing the best way to permanently destroy tokens. In order to remove ETH from circulation, it is planned to apply commissions for applications based on Ethereum blockchain. According to the young genius, such charges would eventually \u201cburn\u201d redundant Ether.\n\nThe model of tokens availability reduction/burning by means transactions fees described by Buterin is already implemented in the Neuromation project.\n\n\u201cThe mechanism of progressive deficit in flotation is squeezing the tokens offering: The more transactions \u2014 the fewer tokens, and the higher the demand for them,\u201d Neuromation CEO Maxim Prasolov explains the concept choice. Unlike a fiat currency, the token\u2019s divisibility is unlimited, each Neurotoken consisting of 100 million fractions, \u201cNeurotokes\u201d. There is a platform commission fee and a \u201cburn\u201d factor applied to each transaction in Neurotokens \u2014 the flotation reduction occurs as the customer pays for the platform services.\n\nThe amount of the initial offering on the platform equals Neurotoken 100 million. In the first three years of development, when the investment potential is formed, it is planned to burn up to 50.4% of the total emission. In the future, the amount of Neurotokens in circulation will be reduced to 20 million.\n\nFirst, it is planned to \u201cburn-up\u201d the Neurotokens from the liquidity reserve, and further, once the allocated reserve is exhausted, the practice of buying tokens on the market (buy-back) will start in order to withdraw them from circulation. The lump amount of Neurotokens burnt depends on time and quantity factors, the time of issue and the intensity of transactions affecting the calculation.\n\nSuch method of flotation regulation is an example of an effective mechanism to prevent excessive inflation and a guaranteed way to support interest in the issued token. The development model chosen by Neuromation is still an innovative approach to \u201ctokenomics\u201d, but after Buterin\u2019s speech (and considering the Ethereum popularity) it has a good chance of becoming an accepted standard in less than a year."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/deep-q-network-d2dbc5688c3b",
        "title": "Deep Q-Network \u2013 Neuromation \u2013",
        "text": "This article TD-Gammon and Q-training and their practical application with the game Atari.\n\nIn 2013, Mnih et al. published a paper where one of the standard methods for reinforcement learning, combined with deep learning neural networks, is used for playing Atari. TD-learning (temporal difference learning) is commonly used in contexts in which the reward represents the outcome of a relatively long sequence of actions, and the problem involves redistributing this single reward among the moves and/or states leading to it. For instance, a game of Go can last a few hundred moves, but the model will only get cheese for winning or an electric shock for losing at the very end, when the players reach a final outcome. Which moves out of the hundred made were good or bad? That\u2019s still a big question, even when the outcome is known. It\u2019s quite possible that you were heading for defeat in the middle of the game but then your opponent blundered, and you wound up winning. Also, trying to introduce intermediary goals artificially, like winning material, is a universally bad idea in reinforcement learning. We have ample evidence that a smart opponent can take advantage of the inevitable \u201cnear-sightedness\u201d of such a system.\n\nThe main idea of TD-learning involves re-using later states, which are close to the reward, as targets for training previous states. We can start with random (probably completely ludicrous) evaluations of a position but then, after each game, perform the following process. First, we are absolutely sure about the final result. For instance, we won, and the result is equal to +1 (hooray!). We push our evaluation of the penultimate position to +1, the third-to-last position to the penultimate one, which has been pushed to +1, etc. Eventually, if you train long enough then you get good evaluations for each position (state).\n\nThis method was first successfully used in TD-Gammon, the computer backgammon program. Backgammon wound up being easy enough for the computer to master, because it\u2019s a game played with dice. Since the dice fall in every which way, it wasn\u2019t hard to get training games or an intelligent opponent, who would enable us to explore all the games possible \u2014 you simply have the computer play against itself, and the inherent randomness in the game of backgammon would enable the program to explore the vast space of possible game states.\n\nTD-Gammon was developed roughly 30 years ago; however, even back then, a neural network served as its foundation. A position from a game would be the input, and the network predicted an evaluation of the position or your odds of winning. The computer versus computer games would produce a set of test cases new for the network, and then the network kept learning and playing against itself (or slightly earlier versions of itself).\n\nTD-Gammon learned to defeat humans back in the late eighties, but this was attributed to the specific nature of the game \u2014 namely, the use of dice. But by now we understand that deep learning can help computers win in numerous other games, too, like Atari games mentioned earlier. The key difference of Mnih et al.\u2019s paper from backgammon or chess was that they did not teach a model the rules of an Atari game. All the computer knew was what the image on the screen \u2014 the same one players would see \u2014 looked like. The only other input was the current score, which needed to be externally defined, otherwise it was unclear what the objective was. The computer could perform one of the possible actions on the joystick \u2014 turning the joystick and/or pushing a button.\n\nThe machine spent roughly 200 tries on figuring out the objective of the game, another 400 to acquire skills, and then the computer started winning after about 600 games.\n\nQ-training is used here, too. In the same exact way, we try to build a model that approximates the Q-function, but now this model is a deep convolutional network. This approach proved to work very well. In 29 games, including wildly popular ones like Space Invaders, Pong, Boxing, and Breakout, the system wound up being better than humans. Now, a team from DeepMind responsible for this design is focusing on games from the 1990s (probably Doom will be their first project). There\u2019s no doubt that they beat these games in the near future and keep moving forward, to the latest releases.\n\nAnother interesting example of how the Deep Q-Network is used is paraphrasing. You are given a sentence, and you want to write it in a different way, yet express the same meaning. This task is a bit artificial, but it\u2019s very closely linked to text generation in general. In a recently proposed approach, the model contains an LSTM-RNN (Long Short-Term Memory Recurrent Neural Network), which serves as an encoder, condensing the text and making it into a vector. Then this condensed version is \u201cunfolded\u201d into a sentence with a decoder. Since it\u2019s decoded from its condensed form, then, most likely, the new sentence will be different. This process is called the encoder-decoder architecture. Machine translation works in a similar manner. We condense the text in one language, and then unfold it using roughly the same models but in a different language, and assuming that the encoded version is semantically similar. Deep Q-Network can iteratively generate various sentences from a hidden version and various types of decoding to move the final sentence closer to the initial one over time. The model\u2019s behavior is rather intelligent: In the experiments, DQN first fixes the parts that we have already rephrased well, and then moves on to more complex parts where the quality has been worse so far. In other words, DQN supplants a decoder in this architecture.\n\nContemporary neural networks are getting smarter with each day. The deep learning revolution occurred in 2005\u20132006, and since then, interest in this topic has only continued to grow. New research is published every month, if not every week, and new interesting applications of deep learning networks are cropping up. In this article, which we hope has been sufficiently accessible, we have tried to explain how this deep learning revolution fits into modern history and development of neural networks, and we went into more detail about reinforcement learning and how deep learning networks can learn to interact with their environment.\n\nMultiple examples have shown that now, when deep learning is undergoing explosive growth, it\u2019s quite possible to create something new and exciting that will solve real tasks without huge investments. All you need is a modern video card, enthusiasm, and the desire to try new things. Who knows, maybe you\u2019ll be the one to make history during this ongoing revolution \u2014 at any rate, it\u2019s worth a try."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/taste-the-bounty-of-neuromation-2f0d349999d3",
        "title": "Taste the bounty of Neuromation \u2013 Neuromation \u2013",
        "text": "During the pre-sale preparations, we were very hesitant to introduce the bounty system to our marketing campaign.\n\nWe have decided that we would like to partner with conscious investors that are passionate about AI and deep learning, and we believe there are many like-minded people globally.\n\nWe are not into artificial hype and speculative tricks. We also have a cognitive problem with feeding bot farms. They are never satisfied anyway.\n\nHowever, during the course of the pre-sale process, we have received hundreds of requests to introduce the system and give all neuro people on this planet a chance to spread the word and be rewarded.\n\nSo we said, why not!\n\nWe came up with a simple and fair referral system, where each participant can easily check and control their benefits.\n\nHere is what you should do.\n\nFor newcomers : get registered here.\n\nIn case you already invested in NTK, just go to referrals section in your dashboard.\n\nGenerate the referral link, post it on your blog, media or website and get a great bonus in Neurotokens!\n\nYou will receive the bonus for every investor, who has contributed to the token sale, using your personal referral link. You can post the link on your medium, or distribute it privately. A bonus is calculated according to the following grid and paid out in Neurotokens (NTK).\n\nThe bonus for each referral will be calculated cumulatively. No accumulative bonuses will be applied to different referrals.\n\nYour bonus will be disbursed to your NTK wallet within 72 hours of the Token Sale completion (January 1st, 2018)\n\nSpread the word, earn NTK, and let the AI revolution begin.\n\nAll questions can be asked and will be answered in our chat.\n\nYour Neuromation Team, working hard while androids dream of electric sheep."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/who-will-be-replaced-by-robots-7f9e8100d036",
        "title": "Who will be replaced by robots \u2013 Neuromation \u2013",
        "text": "Or \u201cMan! That has a proud sound!\u201d\n\ntranslated from the Russian version by Andrey V. Polyakov\n\nRecently, a new round of conversations about self-moving carriages and other potential achievements of artificial intelligence has again posed one of the classic questions of humanity: who will be marginalized by the monorail track of the next technological revolution? Some studies argue that artificial intelligence in the near future would lead to a surge of unemployment comparable to the Great Depression. Today we will also talk about who can be replaced by computers in the near future, and who can, without fear and even with some degree of self-satisfaction, expect the arrival of our silicon overlords.\n\nSome people believe labor-saving technological change is bad for the workers because it throws them out of work. This is the Luddite fallacy, one of the silliest ideas to ever come along in the long tradition of silly ideas in economics.William Easterly. The Elusive Quest for Growth: Economists\u2019 Adventures and Misadventures in the Tropics\n\nSuch a text could hardly do without recalling the most famous opponents of technological progress: the Luddites. Moods against technological progress were strong among English textile workers as far back as the 18th century, but Nottingham manufacturers did not receive letters on behalf of Ned Ludd until 1811. The trigger for the transition to active actions was the introduction of stocking machines, which made the skills of skilled weavers unnecessary: now stockings could be sewn of separate parts without special skills. The resulting product was, by the way, much worse, with stockings quickly bursting at the seams, but they were so much cheaper and they still enjoyed immense popularity. Fearing to lose work, the weavers began attacking factories and smashing newfangled machines.\n\nLuddites were well organized. They understood that conspiracy was vital for them; they brought terrible oaths of loyalty, acted under the cover of night and were rarely apprehended. By the way, most probably General Ludd has never existed: it was a folklore \u201cbigger than life\u201d figure like Paul Banyan. The fight against Luddites was taken seriously: in 1812, \u201cmachine breaking\u201d was proclaimed a capital crime, and there were more British soldiers suppressing uprisings than there were fighting Napoleon in the same years! Yet, the Luddites managed to significantly reduce automation in textile production, the prices for products increased, and the goals of the movement were partially achieved. But honestly, when did you last wear any hand-woven stockings, socks or pantyhose?\u2026\n\nWe have placed a quote from the book of economist William Easterly as epigraph to this section. Easterly explains that the Luddite movement has spawned (no longer among English weavers, but in quite intellectual circles) the erroneous idea, which he calls the Luddite fallacy, and which still, from time to time, occurs to quite real economists. The idea is that the development of automation is supposed to inevitably lead to a reduction in employment, because fewer people are needed to maintain the same level of production. However, both theory and practice show that quite another outcome is no less likely: simply the same, or even a larger, number of people would produce more goods! And the progress is usually on the side of workers, increasing their productivity and, consequently, their income. Yes, the last consequence is not always that obvious, but there is no apparent way to increase the welfare of each individual worker.\n\nNevertheless, these arguments do not refute the Luddites themselves. Regardless of the rhetoric, the Luddites were afraid not of a bright future, in which every stockinger could make a hundred pairs of stockings a day, but of immediate tomorrow, when they were on the street and no one would need the only skill they possessed. Moreover, even Easterly does not deny that progress can lead to unemployment and decline of prosperity for certain workers, even if the average well-being of the people grows. Let us take a closer look at this argument.\n\n\u2026we set up this room with girls in it. Each one had a Marchant: one was the multiplier, another was the adder. This one cubed \u2014 all she did was cube a number on an index card and send it to the next girl\u2026 The speed at which we were able to do it was a hell of a lot faster\u2026 We got speed\u2026 that was the predicted speed for the IBM machine. The only difference is that the IBM machines didn\u2019t get tired and could work three shifts. But the girls got tired after a while.Richard Feynman. Surely You\u2019re Joking, Mr. Feynman!\n\nAn economist from Harvard James Bessen conducted an interesting study. He took a list of major 270 occupations used in the 1950 U.S. Census, and then checked how many had been completely automated so far. It turned out, that out of 270 occupations:\n\nThose, willing to think, will have time until the next paragraph.\n\nThe only fully automated occupation in Bessen\u2019s research is elevator operator. At this point, our 80+ readers who grew up in the U.S. can certainly sigh and complain that earlier warm colored boys closed behind you double elevator doors, which could not close themselves, and pressed warm lamp buttons\u2026 but was it so good for you, and for the colored boys themselves? Maybe they should go to school after all?\u2026\n\nWe can add another interesting career path to Bessen\u2019s research. For quite a long time computers have completely replaced the occupation of\u2026 computer. Oddly enough, not everyone knows about the existence of this occupation, although it seems obvious that before the advent of computers it was necessary to calculate manually. Many great mathematicians were also outstanding computists: for example, Leonhard Euler could carry out complex calculations in his mind and often amused himself with some complicated exercise when his wife managed to get him to the theater. But how were the calculations made, for example, in the Manhattan project, where no physicist could have managed them on his own, \u201con paper\u201d?\n\nWell, they really did them manually. The main idea is described in the epigraph: when people perform operations sequentially, as if on a conveyor, it turns out much faster. Of course, humans are prone to error, and the results of the calculations had to be rechecked, but for this, special algorithms can also be developed. In the 19th century, human computers compiled mathematical tables (for example, sinuses or logarithms), and in the twentieth century they worked for military needs, including the Manhattan project. It is computists, by the way, who are responsible for the fact that the occupation of programmer was at first considered female: the computists were mostly girls (for purely sexist reasons \u2014 it was believed that women are better suited for monotonous tasks), and the first programmers were often recruited from them.\n\nIn his book When Computers Were Human, David Alan Grier describes the atmosphere of the work of living calculators, citing Dickens (another unexpected affinity to real Luddites): \u201cA stern room, with a deadly statistical clock in it, which measured every second with a beat like a rap upon a coffin lid.\u201d Should we be nostalgic for this patriarchal, but by no means bucolic picture?\n\nBessen distinguishes between complete and partial automation. Indeed, a hereditary elevator operator could have a hard time in a brave new world (there is only one problem: hereditary elevator operators apparently did not have enough time to grow). However, if only part of what you are doing is automated, the demand for your occupation can even increase. And again one can return to the Luddites: during the 19th century, 98% of the labor required to weave a yard of cloth was automated. Theoretically, it was possible to dismiss forty-nine weavers out of fifty and produce the same amount of cloth. However, the final effect was opposite: the demand for cheap cloth grew dramatically, resulting in demand for weavers, and, therefore, the number of jobs increased significantly. And in the 1990\u2019s, the widespread deployment of ATMs did not reduced, but even increased the demand for bank tellers: ATMs allowed banks to operate branch offices at lower cost; this prompted them to open many more branches.\n\nDespite numerous technological innovations, complete occupation automation in the last 50\u2013100 years did not have a noticeable effect in the society. Do you know anyone whose grandfather was an elevator operator and grandmother a computer, who have lost jobs and sunk to the bottom of society because of the goddamned automation?\n\nBut partial automation over these years has radically changed the content of work for the vast majority of us. Computers and especially the Internet have made many professions many times more productive \u2014 can you imagine how long it would take me to collect data for this article in the 1950\u2019s? And globalization and automation of economy tangibly raised the standard of living \u2014 for all, not just the elite. It would be trivial to say that we now live better than medieval kings did \u2014 but try to compare our standard of living with the way our grandparents lived. I will not give any examples or statistics, every reader has their own experience thereof: just recall your life before such trifles as mobile phones, Google, microwave ovens, dishwashers (and even washing machines were recently not in all households)\u2026\n\nTherefore, it seems that progress and automation have so far only helped people, and the Luddites\u2019 fears were greatly exaggerated. But, maybe, \u201cthe fifth industrial revolution\u201d will be absolutely different? In the second part of the article we will try to dream on this topic.\n\nHow far has gone \u201cprogr\u00e8s\u201d? Laboring is in recess,\n\nAnd mechanical, for sure, will the mental one replace.\n\nTroubles are forgotten, no need for quest,\n\nToiling are the robots, humans take a rest. Yuri Entin From the movie Adventures of Electronic\n\nElevator operators, computists and weavers perfectly symbolize those activities that have been automated so far: technical work, where the output result is expected to maximally match certain parameters, and creativity is not only difficult, but forbidden and, in fact, harmful. Of course, an elevator operator could smile to his passengers, or the computer girl could, having scorned potential damage to her reputation, get acquainted with Richard Feynman himself. But their function was to accurately perform clear, algorithmically defined actions.\n\nI will allow myself a little emotion: these are exactly the kinds of activities that must be automated further! There is nothing human in following a fixed algorithm. Monotonous work with a predetermined input and output is always an extreme measure, the forced oppression of the human spirit in order to achieve certain practical goal. And if the goal can be achieved without wasting human time, that is the way to proceed.\n\nHowever, now it comes to the fact that computers start gradually replacing people in areas that were so far considered as human domain. For example, since 2014, Facebook is able to recognize faces on human performance level, and computer vision technologies are only improving."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-wins-1st-prize-at-d10e-in-davos-a2908889b3fd",
        "title": "Neuromation wins 1st prize at d10e in Davos \u2013 Neuromation \u2013",
        "text": "Another win for Neuromation! 1st Place in d10e Davos ICO Competition! It was great to share this achievement and $250,000 award from the Blockchain Investors Consortium with our partners OSA Hybrid Platform \u2014 2nd place and Hacken.io \u2014 3rd place! Changing the future of technologies together!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/letsenhance-io-to-process-images-on-neuromation-platform-b0b2fc56f934",
        "title": "LetsEnhance.io to process images on Neuromation Platform.",
        "text": "LetsEnhance.io has won a technological grant in the ongoing Neuromation Start Up competition and will be the first startup to be launched on Neuromation\u2019s platform.\n\nLet\u2019s Enhance attracted plenty of media attention earlier this month, including a stellar review in Mashable.com. Let\u2019s Enhance provides free online image upscales and enhancements with neural networks. Their system uses GAN architecture and deep neural networks to upscale small images.\n\nNeuromation\u2019s platform is a universal marketplace, that helps to meet datasets, computer power and machine learning models.\n\nNeuromation will provide distributed computer power to Let\u2019s Enhance, originating on mining farms, priced at 3 USD per GPU per day, with reserve of 30 GPUs. 6 GPUs out of 30 are provided in the form of a grant until the end of 2017.\n\nService payments to the mining pools, that will provide computing power through Neuromation\u2019s platform, will be provided with Neurotokens, Neuromation\u2019s own currency.\n\nOleksandr Savsunenko, the CEO of LetsEnhance.io, says: \u201cThis is a fantastic opportunity for Let\u2019s Enhance to purchase computing power at an affordable price. Using the Neuromation platform not only do we save 75% of our budget allocated for the purchase of the computing power, but we become part of real blockchain economy. \u201c\n\nMaxim Prasolov, the CEO of Neuromation says: \u201cWe are proud to welcome Let\u2019s Enhance as the first winner of Neuromation\u2019s grant for AI start-ups. Neuromation\u2019s mission is to democratize AI and machine learning, making it affordable for the projects on the initial stage of their development and to ensure fast industrial implementation of neural networks. Neuromation will continue helping young AI projects to progress. We stand behind the concept of knowledge mining and believe in it\u2019s future.\u201d\n\nThe partnership with Let\u2019s Enhance is the fourth partnership, announced by Neuromation over the last 2 months. Previously ECR Europe announced that Neuromation will provide image recognition services for its OSA HP retail platform. White hat hacker Hacken.io is taking care of blockchain security for Neuromation, while Token-as-a-service closed end fund TAAS, backs it up on the investment side.\n\nThe Neuromation Platform will use distributed computing along with blockchain proof of work tokens to revolutionize AI model development.\n\nThe current revenue of miners, mining etherium and other light coins on GPU is 40- 60 USD/ day/average machine of 24 GPU, which makes Neuromation an attractive alternative for mining pools that wish to increase profits, offering 50\u201370 USD/day revenue/machine."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/icorating-rates-neuromation-as-positive-7fa9519d820a",
        "title": "ICORating gives Neuromation a POSITIVE rating \u2013 Neuromation \u2013",
        "text": "We assign the Neuromation project a \u201cPositive\u201d rating. We recommend its\n\ntokens\u200b\u200b for \u200b\u200bpurchase \u200b\u200bat \u200b\u200bthe\u200b\u200b ICO\u200b\u200b stage.\n\nThe project is entering a dynamically developing market with impressive growth rates, which will remain high in the coming years according to experts\u2019 forecasts. The investment attractiveness of the token, due to the mechanism of \u201cburning\u201d will be at a high level in the next 3 years; this allows the project to attract longer-term investors.\n\nSpeaking about the strengths, we note that the team has all necessary skills for a successful start to the project and its subsequent development; the project\u2019s media (marked by various awards) and the upcoming Token Sale are carried out to a high level.\n\nAt the same time, we see a number of risks that could prevent the project\u2019s\n\nsuccessful development.\n\nFirst and foremost, there is growing competition. In this case we are talking not about direct competitors to Neuromation, but about different projects using a mechanism for rewarding miners in a similar way to Neuromation (as an alternative to classic crypto mining).\n\nNot all parameters of the token sale (e.g. the soft cap) are disclosed.\n\nThe global marketing strategy for further promotion of the project is not described; thus, what funds will be aimed at future promotion of the platform remains unclear.\n\n2. General information about the Project and ICO\n\nNeuromation is a platform on Ethereum blockchain which enables building a library of synthetic data using the computing power of private and commercial mining service providers. Using the sets of these data, Neuromation can effectively train models of neural networks.\n\nIn other words, it is a platform designed to introduce the process of developing artificial intelligence models to various industries using neural networks.\n\nOne of the main goals of the Neuromation platform is to become a center for artificial intelligence services for international business, providing a new approach to combining supply and demand in each of these areas on a large scale.\n\nThe Neuromation company is based in Estonia and fully complies with the legislation of Estonia for regulating crowdfunding. Participation in the ICO is open to all except for citizens of China, Hong Kong and citizens in the list of OFAC. US citizens will have to undergo additional accreditation as \u201cqualified investors\u201d via the following service: https://verifyinvestor.com/\n\nTarget sales volume of tokens for Pre-Sale: 60,000,000 NTK (This number of\n\ntokens\u200b\u200bis\u200b\u200ballocated\u200b\u200bsimultaneously\u200b\u200bon\u200b\u200bthe\u200b\u200bpre-sale\u200b\u200band\u200b\u200bmain-sale\u200b).\n\nBonus: \u200bWhen applying to the \u201cwhitelist\u201d on the website, a 25% bonus is required as well as bonuses depending on the date of purchase:\n\nMain-Sale:\n\nStart\u200b\u200bdate:\u200b November 28, 2017\n\nEnd\u200b\u200bdate:\u200b January 1, 2017\n\nPurpose-oriented\u200b\u200b token \u200b\u200bvolume: \u200b\u200b60,000,000 \u200b\u200bNTK\n\nPrice:\u200b 1 NTK = 0.001 ETH\n\nMinimum \u200b\u200bpurchase\u200b\u200b amount\u200b: \u200b\u200bno restrictions\n\nMaximum\u200b\u200b purchase\u200b\u200b amount\u200b: \u200b\u200bno restrictions\n\nMaximum token emission: 100,000,000 NTK (There will be no additional\n\nemissions)\n\nBonus:\n\n\u25cf First week: 15%\n\n\u25cf Second week: 10%\n\n\u25cf Third week: 5%\n\nBonuses depending on the amount of purchase:\n\n\u25cf >1,000 ETH +1%\n\n\u25cf >2,000 ETH +2%\n\n\u25cf >3,000 ETH +3%\n\n\u25cf >5,000 ETH +4%\n\n\u25cf At least 40% \u2014 platform development\n\n\u25cf Up to 40% \u2014 liquidity reserve (prepayment for server capacity)\n\n\u25cf 10% \u2014 PR and Neuromation marketing services\n\n\u25cf 10% \u2014 remuneration to partners / consultants / early investors\n\nTokens unsold during the token sale will be burned.\n\nTokens assigned to the team as a reward will be frozen for a period of up to 24\n\nmonths with a monthly partial release for the entire period. Some NKT tokens will be removed from circulation within the next three years. The mechanism of \u201cburning\u201d tokens is described in detail in \u201cInvestment attractiveness of the token.\u201d\n\n3. Description of the services and scope of the project\n\nThe Neuromation platform is designed for:\n\n\u25cf Synthesizing large volumes of data with perfectly accurate markings without any inaccuracies arising from human factors.\n\n\u25cf Effective training of large capacity, in-depth neural networks.\n\n\u25cf Replacing costly and slow crowdsourcing with a fast, efficient and accurate\n\ntool for creating and marking data.\n\n\u25cf Generation artificial data and comparison with real data. The platform will use distributed processing of data and proof-of-work blockchain tokens, which can facilitate the transformation of the process of developing models\n\nof artificial intelligence by combining components necessary to create solutions in the field of in-depth entraining. Neuromation will work with service providers (commercial and private) that will provide resources for:\n\n\u25cf Creating artificial data sets.\n\n\u25cf Implementation of distributed computing services.\n\n\u25cf Developing machine learning models.\n\nEach of these services will be paid for by the domestic currency of the platform i.e. Neurotoken tokens. Platform users will be able to buy tokens on a Neuromation client portal using a simple and intuitive interface.\n\nThe platform provides the following types of services:\n\n\u25cf Purchase of data\n\n\u25cf Classification of data\n\n\u25cf Data generation\n\n\u25cf Buying a model\n\n\u25cf Model training\n\n\u25cf Training of neural networks\n\n\u25cf Serving AI requests\n\nThe user interface consists of three categories, containing 3 main modules with certain internal processes and libraries used:\n\nModule \u200b\u200bof\u200b\u200b artificial\u200b\u200b data\u200b\u200bsets:\n\nProcesses:\n\n\u25cf Creating a data generator\n\n\u25cf Order for creation of a data set using a data generator\n\n\u25cf Request for data labeling\n\nLibraries:\n\n\u25cf Repository of data sets for in-depth training\n\n\u25cf Data generator repository\n\n\u25cf Data sets (marketplace)\n\nMachine\u200b\u200bLearning\u200b\u200bModule:\n\nProcesses:\n\n\u25cf Defining the model for in-depth training\n\n\u25cf Importing the model\n\n\u25cf Order for training for the selected data set\n\n\u25cf Request for a custom model on the marketplace\n\nLibraries:\n\n\u25cf In-depth Learning Model Repository (Marketplace)\n\n\u25cf Purchase of tokens\n\n\u25cf Register as a supplier or consumer of processing power\n\n\u25cf Registration as a consumer or service provider\n\n\u25cf Downloading and installing Neuromation Node software\n\nLibraries:\n\n\u25cf User data\n\n\u25cf User Models\n\nThe technology of the server part of the platform consists of two components that perform certain tasks:\n\nNeuromation\u200b\u200bNode:\n\n\u25cf Analysis of nodes (minimum level of processing costs is equal to minimum\n\namount in Neurotoken paid for the unit of calculation, the maximum\n\nperformance of the node based on bandwidth / processing power / storage\n\ncapacity.)\n\n\u25cf Distributed data generation package.\n\n\u25cf Distributed training package.\n\n\u25cf Intermediate node synchronization software.\n\nTrading\u200b\u200b(market)\u200b\u200bmodule:\n\n\u25cf Ensuring effective comparison of orders for the purchase and sale of data\n\nsets, models and marking services.\n\n\u25cf Providing system liquidity so that asset prices help to scale the system.\n\nThe platform will allow users to create data sets generators, generate large amounts of data and train in-depth training models; users will be able to trade datasets and models within the platform.\n\nThe widespread introduction of large-scale generation of artificial data sets is\n\nhindered by a lack of computing power. Neuromation can offer cryptocurrency managers the use of their equipment and to receive financial benefits for solving this problem. In order to do that, a miner needs to download the software \u2014 the Neuromation Compilation Node. When it is proposed to perform a Neuromation task, a miner (node) is selected through bidding to perform the tasks, such as generating artificial data or training an in-depth training model. In this case, the system chooses nodes according to the following parameters:\n\n\u25cf performance / node capacity (Efficiency score).\n\n\u25cf declared cost of the service (the price is set by the miner.)\n\nThe algorithm of choice is arranged in such a way that miners charging a lower price for services will be selected first of all. The same applies to equipment power \u2014 nodes with more productive equipment will be selected firstly. Different computing resources may be required for different tasks; one node may be sufficient for hosting trained models but for such capacious tasks as learning networks, much more computing resources will be required. The speed of a task is proportional to the number of miners connected to its execution. The maximum number of nodes for performing large tasks is not defined at this stage. When a task is completed, the miner receives payment in Neurotoken, and his equipment continues to be used for crypto mining.\n\nNeuromation is planning to open its own laboratories to develop artificial data and train in-depth training models in real applications. Each of its laboratories will investigate a specific problem; with the development of the platform a part of generation and training will move to the laboratory. Currently, 2 laboratories have already been launched:\n\n\u25cf A retail laboratory dedicated to creating in-depth training models that can\n\nrecognize, classify the availability, filling and accuracy of the layout of objects\n\non store shelves, as well as other indicators.\n\n\u25cf The Laboratory for Medical Devices, in cooperation with MonBaby \u2014 a\n\nmanufacturer of infant monitoring devices, is developing a smart camera that\n\ncan monitor the movements of a child and transmit to its parents data on\n\nrespiratory movements, body position (on the back or on the abdomen), fall\n\ndetection and other types of alerts that can be customized individually.\n\nNeuromation is planning to open an industrial automation laboratory for Enterprise Automation, where artificial data will facilitate the introduction of new solutions in production, supply chains, financial services, agricultural industries, etc.\n\nThe platform will use a test environment of its own design \u2014 a Sensor Emulation Sandbox, for procedures to create a virtual environment in which the training of models will take place \u2014 for such areas as the automatic piloting of unmanned aerial vehicles, monitoring of industrial processes, manipulation of objects, etc.\n\nThere are no repositories available on Github; platform development is conducted in closed mode. According to information from the founders, it is being developed by 12 people.\n\nNeurotoken \u200b\u2013 an Ethereum cryptographic ERC20 token, available for storage in various wallets compatible with this standard.\n\nThe use of smart contracts will be available with the platform version 2.0; interaction with smart contracts will occur in the following order:\n\n\u25cf The client makes an order while a request is written on blockchain\n\n\u25cf Nodes apply for data processing\n\n\u25cf The platform selects nodes based on the performance of the node and its cost of services\n\n\u25cf A smart contract is created to perform the task with the selected nodes\n\n\u25cf After the completion of the task, the nodes are paid for Neurotoken, according to the price they set for the services.\n\nNeuromation has trained several different models:\n\n\u25cf Object recognition (SSD, YoLo, Fast (er), R-CNN);\n\n\u25cf Segmentation of objects (SegNet, FCN for segmentation);\n\n\u25cf Currently, instance segmentation (Mask R-CNN) is being developed\n\nAll the models above contain a convolutional network that performs the bulk of image categorization work and is distinguished by additional layers over these functions that process the actual semantics and produce class labels, supposed delimitation of objects, segmentation, and other parameters.\n\nIn the future, it is planned to improve the chains of artificial data generation using machine learning algorithms.\n\nMany computer learning models are freely available \u2014 computer vision programs: VGG, GoogLeNet and ResNet, speech generation programs such as Wavenet, etc.\n\nSuch software is integrated into various frameworks and libraries, for example, TensorFlow, Caffe, Torch, pyTorch, Theano, Keras; converters are used for the connection of different types of framework. Neuromation will support all major frameworks, in-depth learning and automatic differentiation libraries, as well as pre-trained models. The results of the training will be available for download.\n\nCompression will be applied to the models by removing redundant information to reduce the response time on mobile devices (\u201cmodel distillation\u201d method).\n\nIn cases when existing models and tools are not suitable for a client to develop a model, he needs to make a request with special characteristics, which will be placed on the Neuromation exchange and processed by specialists.\n\nAccording to experts , the Artificial Intelligence market is expected to grow by about 1175% by the end of 2017 in comparison with a value of $2.4 billion in 2016. At the same time, the dynamics of the growth rate of the market enables forecasting that its size could exceed $59 billion by 2025.\n\nAccording to statistics, the size of the global market for Artificial Intelligence\n\napplications was estimated at about $360 million worldwide in 2016; according to 2 forecasts, the market will grow by another 135% and will amount to more than $840 3 million in 2017.\n\nThe main technologies used in the market are:\n\n\u25cf Natural language processing \u2014 understanding and synthesis of human\n\nlanguage for AI\n\n\u25cf Machine learning \u2014 AI learning in the process of solving many similar\n\nproblems\n\n\u25cf Speech recognition \u2014 converting speech to text\n\n\u25cf Image processing \u2014 image recognition technology\n\n\u25cf Robotics \u2014 design, creation, use and operation of robots\n\n\u25cf Digital personal assistants \u2014 software agents capable of performing\n\ntasks or services for an individual (for example, chat-bots)\n\nas well as other technologies such as querying method and context-aware\n\nprocessing.\n\nThis market can be conditionally divided into several types:\n\n\u25cf Artificial neural networks \u2014 networks built on the principles of\n\norganization and functioning of biological neural networks\n\n\u25cf Expert systems \u2014 computer systems that simulate human ability to\n\nmake decisions\n\n\u25cf Automated robotic systems \u2014 automation of production and business\n\nprocesses using robots of various types (robotized automation.)\n\n\u25cf Embedded systems \u2014 specialized management and monitoring systems\n\nbuilt directly into the device managed.\n\n\u25cf Digital assistance systems \u2014 a variety of chat-bots and virtual\n\nassistants (Siri, Cortana, Google Now etc.)\n\nDepending on the application, the market is classified as:\n\nTractica\u2019s research shows that worldwide revenue from software for in-depth training will grow from $655 million in 2016 to $34.9 billion by 2025.\n\nIt is expected that the use of in-depth training in terms of profitability will be applied in the following 10 cases:\n\n1. Static image recognition, classification, and tagging\n\n2. Machine/vehicular object detection/identification/avoidance\n\n3. Patient data processing\n\n4. Algorithmic trading strategy performance improvement\n\n5. Converting paperwork into digital data\n\n6. Medical image analysis\n\n7. Localization and mapping\n\n8. Sentiment analysis\n\n9. Social media publishing and management\n\n10.Intelligent recruitment and HR systems\n\nThus, the project enters a dynamically growing market and this certainly can\n\ncontribute to its successful implementation.\n\n6. Competitors and competitive advantages of the project\n\nDespite the fact that we did not find any direct competitors in the market, it should be noted that there are a lot of projects that use the principle of alternative mining in models. Competition will arise for the services of miners which may affect the final price of cost of services on the platform.\n\nWe give two projects as examples (Golem and Gridcoin), although there are many more:\n\nGolem \u2014 the first decentralized supercomputer forming a global market of\n\ncomputing resources. Anyone can use Golem to lease their unused CPU / GPU\n\ncomputing power. The price depends on the complexity of the task; it can be a small amount or several thousand dollars. Payments are implemented directly between customers, suppliers and developers via Ethereum. The commission for each transaction is 5% of the total payment amount.\n\nThe main areas of effective use of Golem:\n\n\u25cf CGI rendering\n\n\u25cf scientific research\n\n\u25cf data analysis\n\nIt is notable that the crowdsale brought 820,000 ETH to the project, which was $8.2 million considering the fact that the crowdsale took only 20 minutes.\n\nCurrently, the capitalization of cryptocurrency is more than $170 million.\n\nGridcoin \u2014 a network whose power is directed to scientific research, with a\n\nproof-of-research algorithm in conjunction with BOINC. The idea is that for\n\nparticipating in projects such as:\n\n\u25cf SETI@home (search for extraterrestrial civilizations);\n\n\u25cf Rosetta@home (calculation of protein structure that will help to cure some\n\ngenetic diseases in the future)\n\n\u25cf World Community Grid (development of methods to treat cancer, ebola, ZIKV and muscular dystrophy)\n\n\u25cf Performance of calculations for the hadron collider, searching for pulsars and gravitational waves, combinatorics, various projects in the field of mathematics, physics and biology.\n\nThis network itself generates and distributes coins among participants in the\n\ncalculations. Thus, the organizers of research do not need to pay participants (in contrast to GOLEM). Reward for the block is from 5 to 150 GRC. 5 GRC is paid for usual CPU / GPU mining without running the BOINC application. Reward in the range of 6\u2013150 GRC occurs for mining with BOINC on. All payments are based on BOINC utilization.\n\nCapitalization of this platform is more than $13 million.\n\nAt the same time, in contrast to competitors for mining capacities, Neuromation has the advantage of being able to provide an adapted platform where Neuromation nodes are configured to solve certain tasks, such as:\n\n\u25cf Synthesizing large amounts of data\n\n\u25cf Training of in-depth neural networks\n\n\u25cf Generation of artificial data\n\n\u25cf Model training\n\n\u25cf Serving AI requests\n\nThus, Neuromation provides an exchange platform and environment in which\n\nparticipants can both contribute to the creation of an artificial intelligence model and acquire its components. Meanwhile, the competitors act as intermediary services, writing containers for Amazon, Google Cloud, Azure and others.\n\nWe also note the following as advantages for Neuromation:\n\n\u25cf The miner himself sets the price for his services. Accordingly, it will be higher than for crypto mining.\n\n\u25cf A partnership has been concluded with the Hacken project which will ensure cybersecurity.\n\n\u25cf Neuromation is planning to move to its own blockchain, and the installed\n\ncapacity should reach 100,000 GPU after 2018.\n\nIn addition to the traditional risks inherent in the crypto market, we draw the attention of investors to the following facts:\n\nThere are more and more projects based on mining in the crypto industry.\n\nThis is not about mining of cryptocurrency, but about attracting miners with their equipment to the platform to use their capacities for the project. Therefore, we will note the risk of competition as the main one, which can greatly affect the viability and development of the project. Moreover, the documentation has no information about global marketing strategy, which is why it is not clear how the developers will attract miners to their platform.\n\nAlso we note the lack of any information about the soft cap amount.\n\nAny other significant risks that could have a negative impact on the attractiveness of the Neuromation project were not detected.\n\n17 people are involved in the development of the project.\n\nThe project management consists of 5 people. Leading positions are occupied as follows:\n\nMaxim\u200b\u200bPrasolov\u200b\u200b(LINKEDIN) \u2014 CEO.\n\nHas been working for Neuromation since February 2017. Responsible for overall strategy of the project and marketing. Former member of the team implementing the initial public offering (IPO) of Ferrexpo, the largest producer of iron ore in Ukraine, on the London Stock Exchange. Has been investing in start-ups in the field of development of unmanned aerial vehicles, AI, multimedia using augmented reality (AR) since 2014.\n\nEducation:\n\n\u25cf State University of Management (SUM) 1994\u20131999\n\nConstantine\u200b\u200bGoltsev\u200b\u200b(LINKEDIN) \u2014 Chairman, investor at Neuromation.\n\nProfessional entrepreneur from the online advertising industry. Has more than twenty years of experience developing software and products. Former CEO and founder of the innovative advertising network AdoTube .\n\nEducation:\n\n\u25cf University of California, Berkeley 1995\u20132000\n\nFedor\u200b\u200bSavchenko\u200b\u200b(LINKEDIN) \u2014 CTO.\n\nMore than twenty years of managing complex software development projects with an emphasis on computer graphics, 3D engines, production of CGI and virtual reality (VR). Has advanced degrees in mathematics and graphic design.\n\nEducation:\n\n\u25cf Sevastopol\u2019s\u2019kij Nacional\u2019nij Institut Jadernoj Energii ta Promislovosti\n\n1996\u20132001\n\nSergey\u200b\u200bNikolenko\u200b\u200b(LINKEDIN) \u2014 Chief Research Officer.\n\nInvolved in the project since March 2017. Responsible for research in machine\n\nlearning (in-depth network training, Bayesian methods, natural language processing, etc.), analysis of algorithms (network algorithms, competitive analysis), bioinformatics. Author of more than 120 scientific works, several books, popular courses \u201cMachine learning\u201d, \u201cTraining of in-depth networks\u201d and others. Has extensive experience in participating in commercial projects (SolidOpinion, Deloitte Analytics Institute).\n\nEducation:\n\n\u25cf Steklov Mathematical Institute, St. Petersburg 2005\u20132008\n\n\u25cf Saint Petersburg State University 2000\u20132005\n\nDenis\u200b\u200bPopov\u200b\u200b(LINKEDIN) \u2014 Co-Founder, CIO, Advisor.\n\nDetermines technical requirements, carries out development, implementation and provides complex solutions for the project. More than 15 years of experience devoted to software development.\n\nEducation:\n\n\u25cf National Technical University of Ukraine \u00abKyiv Polytechnic Institute\u00bb\n\n1999\u20132005\n\n12 people in addition are involved as advisors and platform developers, many of whom have extensive experience in their field of activity:\n\nEsther Katz \u200b\u2013 VP Communications, specialist in public relations with 20 years of experience in the main markets, specializes in bio and financial technologies.\n\nAndrew Rabinovic \u200b\u2013 advisor, leading world scientist in the field of in-depth learning and image recognition research. Author of numerous patents and scientific publications. Has a Ph.D. in the field of computer science at the University of California, San Diego (UCSD).\n\nDavid Orban \u2014 \u200badvisor, founder and managing partner of Network Society\n\nVentures, specializing in innovative venture-based projects for emerging\n\ntechnologies and decentralized networks.\n\nYuri Kundin \u2014 ICO Compliance Advisor, specializes in the development of a\n\nstructure and methodology for risk assessment, compliance and certification of blockchain ecosystems, crypto currency and ICO. Current director of KPMG office in San Francisco.\n\nAccording to the founders, it is planned to increase project staff to 25 people by February 2018.\n\nThe team members have all the necessary skills for successful implementation of the project, such as development and entrepreneurship; there are specialists who have wide experience in the sphere of business development in leadership team.\n\nThe project\u2019s final goal is to create a distributed platform based on blockchain\n\ncovering all aspects of a future ecosystem for artificial data and enabling users to create data set generators and generate large data sets, train in-depth training models and trade datasets and models.\n\nThe Neuromation platform will combine the following on an integrated AI\n\nmarketplace:\n\n\u25cf Market resources\n\n\u25cf Scientific community\n\n\u25cf Companies and individuals\n\nThe founders are planning to achieve these goals in the coming year.\n\nTo achieve the specified goals, the team have carefully arranged the development strategy for the project depending on the expected success of the token.\n\nThe founders believe that Neuromation can become a recognized business partner in developing the potential for artificial intelligence use after 2018. Neurotokens will be the main mechanism of exchange, allowing generation of artificial data, implementing the distributed training of models, the marking of data and other services in the field of artificial intelligence.\n\nThe project is also planning to become a global pool of resources in the field of\n\nartificial data, some kind of constantly replenished library which will have data sets for any possible case.\n\nThe founders expect the planned development of the project as well as worldwide distribution of the platform which depends on the expected success of the token sale (achieving 60,000 ETH worth of sales.) Neurotokens are planned to be placed on exchanges in the first quarter of 2018.\n\nThus, the project has clear and transparent development plans.\n\nThe Neuromation team allocates 10% of all funds to marketing. The marketing strategy is presented neither in the white paper nor on the website.\n\nHaving studied various materials online, we make the following conclusion regarding current marketing actions and achievements of the company:\n\n\u25cf The team already has cooperation agreements with partners such as OSA\n\nHP, ECR, MonBaby. In the near future it is planned to open the laboratory for\n\nEnterprise Automation (industrial automation.) From the beginning of 2018,\n\nthe number of partners will increase and according to Neuromation forecasts,\n\nthis source of profit will account for almost a third of the company\u2019s total\n\nrevenue.\n\n\u25cf In September, Neuromation received a special prize from Forbes and Forklog at the d10e conference in Kiev. The awarded media prize gives the project an opportunity for private pitching with a consortium of BIC blockchain investors, founded by Mike Costas. This consortium manages a capital of $2 billion and unites more than 100 ICO investors, including crypto currency hedge funds and private individuals.\n\n\u25cf In October, Neuromation took third place at the d10e conference on the\n\ndevelopment of the fintech industry, blockchain, crypto currency and the\n\neconomy of joint consumption. $33,000 cash prize was awarded to\n\nNeuromation founder Konstantin Goltsev and project advisor David Orban.\n\n\u25cf Neuromation is mentioned on a large number of sources (more than 40) such as Icobench, Digitaljournal, Bitcoin.info, Bitcoin Insider , The-Blockchain\n\nNews, Crowdfund Insider, BlockTribune and many others.\n\nThe upcoming token sale is discussed on social media; developers interact with the community. Promotion of the project will also occur via a bounty program which the team is planning to introduce in the near future.\n\nIn conclusion we can say that the project already has a certain profile, but there is no information on how buyers of services and the miners that provide their computing power will be attracted, as these will be the factors that will cause the project to develop, achieve the desired goals and generate the company\u2019s main revenue.\n\nAchieving the the status of a key international service for artificial intelligence is one of the main goals and objectives of Neuromation.\n\nPlans to capture the market are optimistic and realistic. At the end of the first year, the total volume of transactions on the platform is expected to be $71 million. Each subsequent year (until 2022), use of the platform will increase by 3\u20135 times.\n\nThe platform is planning to generate revenue from commissions that will amount to 5\u201315% of its services. It is expected that with the development of the platform, in three years\u2019 time, commission fees for Neuromation will exceed $100 million.\n\nAnother source of income will be the Neuromation laboratories\u2019 partnerships with other companies, which will bring about 30% of total income. Currently, the Neuromation laboratory has signed a contract with OSA HP to sell image recognition technology to customers of the OSA Hybrid Platform and for the\n\nECR association to enter the generator and generate synthetic data for 170,000 items in retail trade in Eastern Europe. It is envisaged to create models for the in-depth training and recognition of objects on store shelves. As a result, profit will be \u20ac4.25 million over 1.5 years. Cooperation with MonBaby should bring expected revenue from the implementation of the contract of more than \u20ac2 million over several years.\n\nDespite the fact that the economic model is built mainly on forecast values it is worth noting its simplicity and realism.\n\nNTK Tokens are infrastructural and do not give their holders ownership or voting rights. The project does not confer any rights to receive dividends. However, the smart token contract will contain a built-in function to reduce the amount of NTK in free circulation over the next 3 years.\n\nThus, the growth in token price will be affected by two factors:\n\n\u25cf Growth in the number of users on the platform.\n\n\u25cf The mechanism for \u201cburning\u201d tokens.\n\nAccording to the documentation, within the next three years (from 2018 to 2020), Neuromation is planning to burn about 50% of all Neurotokens distributing this value by years:\n\n\u25cf 2018\u201330%\n\n\u25cf 2019\u201320%\n\n\u25cf 2020\u201310%\n\nThe algorithm for burning and distributing tokens on the platform is as follows:\n\n1. The customer buys tokens on the exchange or from Neuromation\u2019s reserve.\n\n2. The customer orders work on the platform and Neuromation receives tokens.\n\n3. Neuromation burns tokens:\n\na. Up to 6% of all tokens that will pass through the platform in the first\n\nyear\n\nb. Up to 1% of all tokens that will pass through the platform in the second\n\nyear\n\nc. Less than 1% of all tokens that will pass through the platform in the\n\nthird year.\n\nThese values are \u201cburn tax\u201d; they will be charged to the customer in addition to the cost for services on the platform for each transaction. These values for \u201cburn tax\u201d are indicative and will depend on the turnover on the platform: the higher the turnover, the lower the % that will be taken from the customer. This amount is primarily tied to the combustion plan, so \u201cburn tax\u201d will not be charged to customers after the implementation of the annual combustion plan.\n\n4. The remaining Neuromation tokens are distributed among those who are\n\ninvolved in the performance of work.\n\nTo maintain liquidity at a sufficient level, a reserve fund will be created where it is planned to store 10% of the tokens. Tokens received in the form of commission will be sold on exchanges as well as replenish the reserve fund with a high demand for NTK.\n\nThe white paper presents forecast models of token demand. Thus, according to the presented information, demand for NTK will exceed supply by 4 times in 2018, and by 90 times in 2020. The founders explain such high predicted growth by the growing popularity of the platform as well as by the fact that the costs of services are much higher than those laid down in the calculated economic model.\n\nIt can also be noted that a bonus system is provided for investors participating in the ICO.\n\nGiven all of the facts above, we can conclude that after the launch of the platform NTK tokens have a high growth potential especially at the 3-year horizon."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/convolutional-networks-2230fedce1ba",
        "title": "Convolutional Networks \u2013 Neuromation \u2013",
        "text": "This article deals with convolutional networks and reinforcement learning.\n\nAnd now let us turn to convolutional networks. In 1998, the French computer scientist Yann LeCun presented the architecture of a convolutional neural network (CNN). The network is named after the mathematical operation of convolution, which is often used for image processing and can be expressed by the following formula:\n\nwhere f is the original matrix of the image, and g is the convolution kernel (convolution matrix).\n\nThe basic assumption is that the input is not a discrete set of independent dimensions but rather an actual image where the relative placement of the pixels is crucial,. Certain pixels are positioned close to one another, while others are far away. In a convolutional neural network one second-layer neuron is linked to some of the first-layer neurons that are located close together, not all of them. Then these neurons will gradually learn to recognize local features. Second-layer neurons designed in the same way will respond to local combinations of local first-layer features and so on. A convolutional network almost always consists of multiple layers, up to about a dozen in early CNNs and up to hundreds and even thousands now.\n\nEach layer of a convolutional network consists of three operations:\n\nPooling, also known as subsampling, applies a simple mathematical function (mean, max, min\u2026) to a local group of neurons. In most cases, it\u2019s considered more important for higher-layer neurons to check whether or not a certain feature is in an area than to remember its precise coordinates. The most popular form of pooling is max-pooling: the higher level neuron activates if at least one neuron it its corresponding window activated. Among other things, this approach enables you to make the convolutional network resistant to small changes.\n\nNumerous modern computer vision applications run on convolutional networks. For instance, Prisma, the app you\u2019ve probably heard about runs on convolutional neural networks. Practically all modern computer visual apps use convolutional networks for recognizing objects on images. For instance, CNN-based scene labeling solutions, where an image from a camera is automatically divided into different zones classified as known objects like \u201cpavement\u201d, \u201ccar\u201d, or \u201ctree\u201d, underlie driver assistance systems. Actually, the drivers aren\u2019t always necessary: the same kind of networks are now used for creating self-driving cars.\n\nGenerally, machine learning tasks are divided into two types \u2014 supervised learning, when the correct answers are already given and the machine learns based on them, and unsupervised learning, when the questions are given but the answers aren\u2019t. Things look different in real life. How does a child learn? When she walks into a table and hits her head, a signal saying, \u201cTable means pain,\u201d goes to her brain. The child won\u2019t smack her head on the table the next time (well, maybe two times later). In other words, the child actively explores her environment without having received any correct answers. The brain doesn\u2019t have any prior knowledge about the table causing pain. Moreover, the child won\u2019t associate the table itself with pain (to do that, you generally need careful engineering of someone\u2019s neural networks, like in A Clockwork Orange) but with the specific action undertaken in relation to the table. In time, she\u2019ll generalize this knowledge to a broader class of objects, such as big hard objects with corners.\n\nExperimenting, receiving results, and learning from them \u2014 that\u2019s what reinforcement learning is. Agents interact with their environment and perform certain actions, the environment rewards or punishes these actions, and agents continue to perform them. In other words, the objective function takes on the form of a reward. At every step of the way, agents, in some state S, select some action A from an available set of actions, and then the environment informs the agents about which reward they\u2019ve received and which new state S\u2019 they\u2019ve reached.\n\nOne of the challenges of reinforcement learning is ensuring that you don\u2019t accidently learn to perform the same action in similar states. Sometimes we can erroneously link our environment\u2019s response to our action immediately preceding this response. This is a well-known bug in our brain, which diligently looks for patterns where they may not exist. The renowned American psychologist Burrhus Skinner (one of the fathers of behaviorism; he was the one to invent the Skinner box for abusing mice) ran an experiment on pigeons. He put a pigeon in a cage and poured food into the cage at perfectly regular (!) intervals that did not depend on anything. Eventually, the pigeon decided that its receiving food depended on its actions. For instance, if the pigeon flapped its wings right before being fed then, subsequently, it would try to get food by flapping its wings again. This effect was later dubbed \u201cpigeon superstitions\u201d. A similar mechanism probably fuels human superstition, too.\n\nThe aforementioned problem reflects the so-called exploitation vs. exploration dilemma. On the one hand, you have to explore new opportunities and study your environment to find something interesting. On the other hand, at some point you may decide that \u201cI have already explored the table and understood it causes pain, while candy tastes good; now I can keep walking along and getting candy, without trying to sniff out something lying on the table that may taste even better.\u201d\n\nThere\u2019s a very simple \u2014 which doesn\u2019t make it any less important \u2014 example of reinforcement learning called multi-armed bandits. The metaphor goes as follows: an agent sits in a room with a few slot machines. The agent can drop a coin into the machine, pull the lever, and then win some money. Each slot machine provides a random reward from a probability distribution specific to that machine, and the optimal strategy is very simple \u2014 you have to pull the lever of the machine with the highest return (reward expectation) all the time. The problem is that the agent doesn\u2019t know which machine has which distribution, and her task is to choose the best machine, or, at least, a \u201cgood enough\u201d machine, as quickly as possible. Clearly, if a few machines have roughly the same reward expectation then it\u2019s hard and probably unnecessary to differentiate between them. In this problem, the environment always stays the same, although in certain real-life situations, the probability of receiving a reward from a particular machine may change over time; however, for our purposes, this won\u2019t happen, and the point is to find the optimal strategy for choosing a lever.\n\nObviously, it\u2019s a bad idea to always pull the currently best \u2014 in terms of average returns \u2014 lever, since if we get lucky and find a high-paying yet on average non-optimal machine at the very beginning, we won\u2019t move on to another one. Meanwhile, the most optimal machine may not yield the largest reward in the first few tries, and then we will only be able to return to it much, much later.\n\nGood strategies for multi-armed bandits are based on different ways of maintaining optimism under uncertainty. This means that if we have a great deal of uncertainty regarding the machine then we should interpret this positively and keep exploring, while maintaining the right to check our knowledge of the levers that seem least optimal.\n\nThe cost of training (regret) often serves as the objective function in this problem. It shows how much the expected reward from your algorithm is less than the expected reward for the optimal strategy when the algorithm simply knows a priori, from some divine intervention, which lever is the optimal one. For some very simple strategies, you can prove that they optimize the cost of training among all the strategies available (up to constant factors). One of those strategies is called UCB-1 (Upper Confidence Bound), and it looks like this:\n\nPull lever j that has the maximum value of\n\nis the average reward from lever j, n is how many times we have pulled all the levers, and nj is how many times we have pulled lever j.\n\nSimply put, we always pull the lever with the highest priority, where the priority is the average reward from this lever plus an additive term that grows the longer we play the game, which lets us periodically return to each lever and check whether or not we\u2019ve missed anything and shrinks every time we pull the lever.\n\nDespite the fact that the original multi-armed bandit problem doesn\u2019t imply an transition between different states, Monte-Carlo tree search algorithms, \u2014 which were instrumental in AlphaGo\u2019s historic victory, are based directly on UCB-1.\n\nNow let\u2019s return to reinforcement learning with several different states. There\u2019s an agent, an environment, and the environment rewards the agent every step of the way. The agent, like a mouse in a maze, wants to get as much cheese and as few electric shocks as possible. Unlike the multi-armed bandit problem, the expected reward now depends on the current state of the environment, not only on your currently selected course of action. In an environment with several states, the strategy that yields maximum profit \u201chere and now\u201d won\u2019t always be optimal, since it may generate less optimal states in the future. Therefore, we seek to maximize total profit over time instead of looking for an optimal action in our current state (more precisely, of course, we still look for the optimal action but now optimality is measured in a different way).\n\nWe can assess each state of the environment in terms of total profit, too. We can introduce a value function to predict the reward received in a particular state. The value function for a state can look something like this:\n\nwhere rt is the reward received upon making a transition from state \u0445t to state \u0445t+1, and \u03b3 is the discount factor (0 \u2264 \u03b3 \u2264 1).\n\nAnother possible value function is the Q-function that accounts for actions as well as states. It\u2019s a \u201cmore detailed\u201d version of the regular value function: the Q-function assesses expected reward given that an agent undertakes a particular action in her current state. The point of reinforcement learning algorithms often comes down to having an agent learn a utility function Q based on the reward received from her environment, which, subsequently, will give her the chance to factor in her previous interactions with the environment, instead of randomly choosing a behavioral strategy."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/deep-learning-stories-d29cd3082e9d",
        "title": "Top 5 AI projects by Neuromation \u2013 Neuromation \u2013",
        "text": "Artificial Intelligence (AI) is seeing another spring and becoming a ubiquitous part of society. Researchers and engineers have made important steps towards solving practical problems in machine learning, particularly in such applications as vision, speech, machine translation, and decision making.\n\nDeep Learning is the fastest growing field in Artificial Intelligence, being interested in the subject we\u2019ve picked up top 5 leading projects in Deep Learning from around the world.\n\nProject Maven, lead by Lt. General John \u201cJack\u201d Shanahan \u2014 the Pentagon\u2019s director for defense for warfighter support \u2014 launched in April focuses on computer vision \u2014 an aspect of machine learning and deep learning \u2014 that autonomously extracts objects of interest from moving or still imagery. Biologically inspired neural networks are used in this process, and deep learning is defined as applying such neural networks to learning tasks.\n\nInnovation Dx technology enables medical professionals to make more informed generalizations from medical image data. According to a survey done by SERMO, every year 1 in 20 patients (12 million people) will be misdiagnosed, costing hospitals and patients alike billions and resulting in 98,000 deaths. Innovation Dx was built in order to reduce the number of misdiagnoses and ultimately save lives.\n\nNeuromation offers a unique solution that unites market resources, the scientific community, commercial and private entities into an integrated marketplace \u2014 the Neuromation Platform. To address the most critical issue, Neuromation focuses on synthetic datasets that have been proven to yield compelling results. Using synthetic datasets in machine learning will further decrease the cost and will ease widespread AI adoption.\n\nNon-profit organization AI4ALL partners with top universities to educate future AI talent about AI for social good. They create pipelines for underrepresented talent through education and mentorship programs around the U.S. and Canada that give high school students early exposure to AI for social good. Their vision is for AI to be developed by a broad group of thinkers and doers advancing AI for humanity\u2019s benefit.\n\nLet\u2019s Enhance uses machine learning to enhance low resolution JPEG images, boosting resolution 4x times, keeping edges and features sharp and crisp. Collaboration with Neuromation would be announced next week. Keep and eye on this project!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/ico-analysis-neuromation-5982bf72ac3a",
        "title": "ICO Analysis: Neuromation \u2013 Neuromation \u2013",
        "text": "A couple weeks ago, like most people, I had zero clue what this meant. Since I started researching this project my eyes have been opened, and my mind blown. Three key definitions to help understand the project.\n\nSynthetic Data: Any production doata applicable to a given situation, not obtained by direct measurement.\n\nAI neural network: An interconnected group of nodes, similar to the large network of neurons in a brain.\n\nMachine learning: AI that provides computers with the ability to learn without being explicitly programmed. Solving giant math problems and self adapting when exposed to new data. The process is similar to data mining.\n\nNeuromation is where data scientists and companies can get all their AI data analysis needs met faster and cheaper than anywhere in the world. This is possible because they reward GPU miners for using their computational power to mine this synthetic data. After the synthetic data is generated, scientists can specify how they want to train their machines to analyze the data in record time on a separate group of nodes.\n\n\u201cNeuromation is a technology platform that creates synthetic learning environments for deep learning of neural network. These simulations are then used for training better algorithms.\n\nWe are building the platform of distributed computing for creating artificial worlds where AI algorithms are trained on simulated sensory inputs. These synthetic worlds also have a virtually infinite supply of perfectly labeled training data. AI plays, as with video games, to learn specific tasks in the real world.\n\nOur technology is crucial to making Deep Learning-based systems useful in the real world as they are taken up by industry. With Neuromation, the future has arrived where computers teach computers to perceive.\n\nOur strategy is not to develop our platform in isolation, but to work with partners in select industries in order to try to bring our vision organically to life. We are developing \u201cNeuromation Labs\u201d that would develop synthetic data and train deep learning models on live applications. Each \u201clab\u201d like Retail Automation Lab, Industrial Automation Lab, Pharma/Medicine/Biotech Lab will be a study on a specific problem in a partnership with a category leader. As our platform is fleshed out we will be moving parts of generation and training there, allowing us to organically test parts of our vision in real-life scenarios. The Labs will seed the Neuromation Platform market with initial data generator and data sets. We will also encourage our Labs partners to transact farther services through the platform, thus building the initial market for services.\u201d\n\nIn order to do transaction on the platform you must use Neurotokens. To make it easy, Neuromation will provide a portal with a one click token buying process. The platform offers 5 services; Data generation, data labeling, data purchase, model training, and model purchase. The price customers pay for each service will depend on how much it costs for the nodes. The platform will determine the resources required for each task, then select the most efficient node pool (minimizing the cost to the costumer, while still making it extremely profitable for the nodes/miners. This brings me to the juice of this project.\n\nMining Knowledge instead of ETH: The reason why synthetic dataset generations haven\u2019t seen widespread adoption yet is because of the huge deficiency of computing power. Neuromation plans to change this by offering cryptocurrency GPU miners 3\u20135 times what they make per hour mining crypto, to mine Neuro(synthetic data/machine learning) instead. How this works is in addition to their existing mining software miners load up a Neuromation computation node. This node is special because it allows miners to mine their normal cryptocurrency until Neuromation has a task available, at which time the miners will switch over to mining neuro until the task is complete.\n\nSince I believe this to be the heart of the project, I emailed the Neuromation team and asked them to break down further how their mining/node system works. Here\u2019s what they said:\n\n\u201cTo mine neurotokens, you will need to load our node. If your system is eligible (depending on processing capacity, bandwidth) the node will activate and wait for tasks. The nodes take on available tasks if they win an internal auction. That auction is run in tiers. Tiers are numbered from 0 to 4. Nodes in tier 0 get a higher priority (the auction will first try to distribute the task among lower tier nodes) then nodes in tier 1 and so on. When our token sale is running you only tier 4 nodes will be available.\n\nWe have a program where miners can invest various amounts into out pre-sale and secure a certain number of lower tier nodes. It is:\n\n\u2014 3,000 Ether for tier 0 (500 node keys)\n\n \u2014 1,000 Ether for tier 1 (100 node keys)\n\n \u2014 600 Ether for tier 2 (100 node keys)\n\n \u2014 200 Ether for tier 3 (50 node keys) \u201c\n\nWhy Blockchain? Today if a data scientist wants access to a large amount of computer power he can go to Amazon and pay for it. They will charge him 12 times what it costs for GPU miners to generate it. This is a giant opportunity for Neuromation to slide in.\n\n60 million of the 100 million total supply will be sold during the ICO.\n\nThe presale is taking place right now thru November 28 (25% bonus). you must sign up for the whitelist https://ico.neuromation.io/en/\n\nImportant to note: Over the next 3 years, neuromation will burn 50% of the total supply!\n\n2018 =30% 2019= 20% 2020=10% (hmm, this is actually 60%, i think they made an error)\n\nThis is a seriously talented team They are out there putting in a lot of work too. Winning competitions, attending conferences, and forming partnerships (hacken.io, TaaS).\n\nCEO- Max Prasolov has been a big time player since 2001. Here\u2019s a short informative interview he recently did. https://cryptosrus.com/interview-neuromation-ceo-maxim-prasolov/ . I\u2019m actually just going to copy and paste his introduction to the rest of his team. Note, I did look fairly deep into each one of these guys, all legit, max just did such a good job summing them up I will use his words.\n\n\u201cI\u2019m a serial entrepreneur and TOP manager with 15 years of experience in different sectors from natural resources mining to industrial multimedia development. I was a part of the team who made an IPO of the iron ore company, blue chip on the London Stock Exchange. But all my life I\u2019ve been in love with graphic novels and animation. I have written and produced several animation movies. While making them I have found out that AI can be trained by showing it the cartoons. This metaphor is very close to what we are doing in Neuromation.\n\nI am glad to work with very smart and talented people involved in our company. Our mentor and advisor Andrew Rabinovich, who is a creator of key deep learning algorithm of Google image. My partner, investor and founder of Neuromation Constantine Goltcev, who believes in me from the beginning and he brings powerful engineers to our team. A deep learning scientist and mathematician Sergey Nikolenko, who is checking our crazy ideas with the proof of scientific method.\n\nFedor Savchenko, experienced CGI expert who creates synthetic data generator. Kyryl Truskovskyi, a talented researcher and engineer, who implements our hypotheses in deep learning applications. Denis Popov, former CTO of Viewdle, who is helping us to hire strongest software developers all over the world. All of these people are my partners and shareholders at Neuromation\u201d\n\nThey have 2 absolute beasts on their advisory board.\n\nAndrew Rabinovich \u2014 has studied machine learning for over 15 years. has many patents and peer reviewed publication. He is the Director of Deep Learning at Google, Magic Leap!!\n\nDavid Orban (Singularity University) apparently this guy has been a pretty big deal for the last 20 years. He\u2019s definitely good at hyping Neuromation.\n\nBecause it currently costs so much to \u201cknowledge mine\u201d, this giant futuristic AI market has yet to take off. Neuromation has the answer. They incorporate the mathematical power of blockchain miners to make synthetic data mining and machine training cheaper and faster than ever before. Its now possible that in the near future, thousands of deep learning projects and b2b clients will use Neuromation to develop things we haven\u2019t even heard about yet.\n\nThe team already has many labs in the works, and has recently boasted about their new Retail Lab, that is already providing \u201cimage recognition services to major retail brands\u201d\n\nWhen just reading the nerdy worded explanations of what synthetic data machine learning is all about, it could be hard to fully grasp what changes this can have on our entire system. I recommend watching youtube videos demonstrating the power of these self-learning machines.\n\n\u201cWhat is especially fascinating is that in only about a month of working on the inventory recognition problem we have achieved 95+% accuracy, a result others have spent years of effort and millions of dollars on. Significantly, the model performs well on real-life data without seeing anything but synthetic datasets during training. This breakthrough proves the viability and efficiency of our approach. With Neuromation Platform, the gateway to easy AI training at scale will be finally opened.\n\nThe potential demand for image recognition alone in the retail industry is enormous\n\nIt amounts to more than 40 billion images per year, according to ECR research of 72 of the biggest retailers and suppliers. Going further, we plan to create datasets that mimic human interaction with the shelf. We will be able to track customer flow and intent, creating a full simulator of the retail store, with a multitude of possible applications\u201d\n\nI have to admit, after watchin numerous machine learning neural network demos on YouTube, I feel like a teenager who just touched his first real life boob. My mind is filled with wonder and amazement. That being said, you should do your own research. I\u2019m clearly smitten, and could be ranking her too high.\n\nThis project is unique, revolutionary, and cheaper/faster than every competitor. The science they are accelerating with these large data mines could change the world in a lot of ways. If they grow their community, and get those miners to switch over their GPU power, the possibilities are endless At only.30 plus bonus and a lower supply of only 100 million, I feel very comfortable recommending this ICO. I wouldn\u2019t sell it anytime soon either. This is a newborn baby giraffe, with the sexy genes she got from her mother! 8.7 out 10"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/the-ai-dialogues-e28155220237",
        "title": "The AI Dialogues \u2013 Neuromation \u2013",
        "text": "This is an introduction to modern AI and specifically neural networks. I attempt to explain to non-professionals what neural networks are all about, where these ideas had grown from, why they formed in the succession in which they did, how we are shaping these ideas now, and how they, in turn, are shaping our present and our future. The dialogues are a venerable genre of sci-pop, falling in and out of fashion over the last couple of millennia; e.g., Galileo\u2019s dialogue about the Copernican system was so wildly successful that it stayed on the Index of Forbidden Books for two centuries. In our dialogues, you will hear many different voices (all of them are in my head, and famous people mentioned here do not talk in quotes). The main character is the narrator who will be doing most of the talking; following a computer science tradition, we call her Alice. She engages in conversation with her intelligent but not very educated listeners Bob and Charlie. Alice\u2019s university has standard subscription deals with Springer, Elsevier, and the netherworld, so sometimes we will meet the ghosts of people long dead.\n\nAlice. Hey guys! We are here with quite a task: we want to create an artificial intelligence, no less. A walking, talking, thinking robot that could do everything a human could. I have to warn you: lots of people have tried, most of them have vastly overestimated themselves, and all of them have fallen short so far. We probably also won\u2019t get there exactly, but we sure want to give it a shot. Where do you suppose we should begin?\n\nBob. Well\u2026 gosh, that sounds hard. To be intelligent a person has to know a lot of things \u2014 why don\u2019t we try to write them all down first and let the robot read?\n\nCharlie. We have encyclopaedias, you know. Why don\u2019t we let the computer read Wikipedia? That way it can figure out all sorts of things.\n\nAlice. Riiight\u2026 and how would we teach the computer to read Wikipedia?\n\nBob. Well, you know, reading. Language. It\u2019s a sequence of discrete well-defined characters that combine into discrete well-defined words. We can already make computers understand programming languages or query languages like SQL, and they look exactly the same, only a bit more structured. How hard can it be to teach a computer to read in English?\n\nAlice. Very hard, unfortunately. Natural language is indeed easy to encode and process, but it is very hard to understand \u2014 you see, it was not designed for a computer. There is no program even now that could understand English, the best artificial intelligence models struggle very hard with reading \u2014 we\u2019ll talk more about this later. But I can give you a quick example from one particularly problematic field called pragmatics. \u201cThe laptop did not fit in the bag because it was too big\u201d. What was too big, the bag or the laptop?\n\nAlice. Okay. Try another one. \u201cThe laptop did not fit in the bag because it was too small\u201d. What was too small, the bag or the laptop?\n\nBob. Obviously\u2026 oh, I see. We understand it because we know the world. But the computer does not know anything about what a laptop is or what a bag is! And the sentence looks very simple, not too contrived at all. But it does look a bit like a handmade counterexample \u2014 does this kind of stuff happen often?\n\nAlice. Very often. Our whole system of communication is made for us, wet biological beings who have eyes, ears, and skin, understand the three dimensions, have human urges and drives. There is a lot left unsaid in every human language.\n\nBob. So the computer can\u2019t just pick up English as it goes along, like children learn to speak, no?\n\nAlice. Afraid not. That is, if it could, it would be wonderful and it would be exactly the kind of artificial intelligence we want to build. But so far it can\u2019t.\n\nCharlie. Well then, we\u2019ll have to help it. You\u2019re saying we can\u2019t just go ahead and write a program that reads English. Okay. So what if we invent our own language that would be more\u2026 machine-readable?\n\nBob. Yeah! It can\u2019t be an existing programming language, you can\u2019t describe the world in C++, but we simply have to make natural languages more formal, clear out the exceptions, all that stuff. Make it self-explanatory, in a way, so that it could start from simple stuff and build upon it. It\u2019ll be a big project to rewrite Wikipedia in this language, but you only have to do it once, and then all kinds of robots will be able to learn to read it and understand the world!\n\nAlice. Cool! You guys just invented what might well be the first serious approach \u2014 purely theoretical, of course \u2014 to artificial intelligence as we understand it now. Back in the 1660s, Gottfried Leibnitz, the German inventor of calculus and bitter rival of Isaac Newton, started talking about what he called Characteristica universalis, the universal \u201calphabet of human thought\u201d that would unite all languages and express concepts and ideas from science, art, and mathematics in a unified and coherent way. Some people say he was under heavy influence of the Chinese language that had reached Europe not long ago. Europeans believed that all those beautiful Chinese symbols had a strict system behind them \u2014 and they did, but the system was perhaps also a bit messier than the Europeans thought.\n\nAnyway, Leibnitz thought that this universal language would be graphical in nature. He believed that a universal system could be worked out based on diagrams and pictures, and this system would be so clear, logical, and straightforward that machines would be made to perform reasoning in the universal language. Leibnitz actually constructed a prototype of a machine for mathematical calculations that could do all four arithmetic operations; he thought to extend it to a machine for his universal language. It is, of course, unclear how he planned to make a mechanical device understand pictures. But his proposal for the universal language undoubtedly did have a graphical component. Look at a sample diagram by Leibniz \u2014 it almost looks like you could use it to summon a demon or two. Speaking of which\u2026\n\nLeibnitz [appearing in a puff of smoke]. Ja! You see, God could not wish to make the world too complicated for His beloved children. We see that in the calculus: it is really quite simple, no need for those ghastly fluxions Sir Isaac was always talking about. As if anybody could understand those! But when you find the right language, as I did, calculus becomes a beautiful and simple thing, almost mechanical. You only need to find the right language for everything: for the science, for the world. And I would build a machine for this language, first the calculus ratiocinator, and then, ultimately, machina ratiocinatrix, a reasoning machine! That would show that snobbish mystic! That would show all of them! Alas, I did not really think this through\u2026 [Leibnitz shakes his head sadly and disappears]\n\nAlice. Indeed. Gottfried Leibnitz was the first in a very long line of very smart people who vastly underestimated the complexity of artificial intelligence. In 1669, he envisioned that the universal language could be designed in five years if \u201cselected men\u201d could be put on the job (later we will see how eerily similar this sounds to the first steps of AI in our time). In 1706, he confessed that \u201cmankind is still not mature enough to lay claim to the advantages which this method could provide\u201d. And it really was not.\n\nCharlie. Okay, so Leibnitz could not do this, that doesn\u2019t surprise me too much. But can\u2019t we do it now? We have computers, and lots of new math, and we even have a few of those nice artificial languages like Esperanto already, don\u2019t we?\n\nAlice. Yes and no. But mostly no. First of all, most attempts to create a universal language had nothing to do with artificial intelligence. They were designed to be simple for people, not for machines. Esperanto was designed to have a simple grammar, no exceptions, to sound good \u2014 exactly the things that don\u2019t matter all that much for artificial intelligence, it\u2019s not hard for a computer to memorize irregular verbs. Second, even if you try, it is very hard to construct a machine-readable general-purpose language. My favourite example is I\u0163ku\u00eel, designed in the 2000s by John Quijada specifically to remove as much ambiguity and vagueness from human languages as possible. I\u0163ku\u00eel is one of the most concise languages in the world, able to express whole sentences worth of meaning in a couple of words. It is excruciatingly hard for humans\u2026 but it does not seem to be much easier for computers. Laptops still don\u2019t fit into bags, in any language. There is not a single fluent I\u0163ku\u00eel speaker in the world, and there has not been any success in artificial intelligence for it either.\n\nCharlie. All right, I suppose it\u2019s hard to teach human languages to computers. That\u2019s only natural: an artificial intelligence lives in the world of ones and zeros, and it\u2019s hard to understand or even imagine the outside world from inside a computer. But what about cold, hard logic? Mathematics? Let\u2019s first formalize the things that are designed to be formal, and if our artificial intelligence can do math it already feels pretty smart to me.\n\nAlice. Yes, that was exactly the next step people considered. But we have to step back a bit first.\n\nIt is a little surprising how late logic came into mathematics. Aristotle used logic to formalize commonsense reasoning with syllogisms like \u201cAll men are mortal, Socrates is a man, hence, Socrates is mortal\u201d. You could say he invented propositional logic, rules for handling quantifiers like \u201cfor all\u201d and \u201cthere exists\u201d, and so on, but that would really be a stretch. Mathematics used logic, of course, but for the most part of history, mathematicians did not feel like there are any problems with basing mathematics on common sense. Like, what is a number? Until, in the XIX century, strange counterexamples started to appear left and right. In the 1870s, Georg Cantor invented set theory, and researchers quickly realized there were some serious problems with formal definitions of fundamental objects like a set or a number. Only then it became clear logic was very important for the foundations of mathematics.\n\nThe golden years of mathematical logic was the first half of the XX century. At first, there was optimism about the general program of constructing mathematics from logic, in a fully formal way, as self-contained as possible. This optimism is best summarized in Principia Mathematica, a huge work by Bertrand Russell and Alfred Whitehead who aimed to construct mathematics from first principles, from the axioms of set theory, in a completely formal way. It took several hundred pages to get to 1+1=2, but they did manage to get there.\n\nKurt G\u00f6del was the first to throw water on the fire of this optimism. His incompleteness theorems showed that this bottom-up construction could not be completely successful: to simplify a bit, there will always be correct theorems that you cannot prove. At first, mathematicians took it to heart, but it soon became evident that G\u00f6del\u2019s incompleteness theorems are not really a huge deal: it is very unlikely that we ever come across an unprovable statement that is actually relevant in practice. Maybe P=?NP is one, but that\u2019s the only reasonable candidate so far, and even that is not really likely. And it still would be exceedingly useful to have a program able to prove the provable theorems. So by the 1940s and 1950s, people were very excited about logic, and many thought that the way to artificial intelligence was to implement some sort of a theorem proving machine.\n\nBob. That makes perfect sense: logical thinking is what separates us from the animals! An AI must be able to do inference, to think clearly and rationally about things. Logic does sound like a natural way to AI.\n\nAlice. Well, ultimately it turned out that it was a bit too early to talk about what separates us from the animals \u2014 even now, let alone the 1950s, it appears to be very hard to reach the level of animals, and surpassing them in general reasoning and understanding the world is still far out of reach. On the other hand, it turned out that we are excellent in pattern matching but rather terrible in formal logic: if you have ever had a course in mathematical logic you remember how hard it can be to formally write down the proofs of even the simplest statements.\n\nCharlie. Oh yes, I remember! In my class, our first problem in first order logic was to prove that A->A from Hilbert\u2019s axioms\u2026 man, that was far from obvious.\n\nAlice. Yes.There are other proof systems and plenty of tricks that automatic theorem provers use. Still, so far it has not really worked as expected. There are some important theorems where computers were used for case-by-case enumeration (one of the first and most famous examples was the four color theorem), but up to this day, there is no automated prover that would prove important and relevant theorems by itself.\n\nCharlie. So far all you\u2019re saying is that not only it is hard for computers to understand the world, but it is even hard to work with perfectly well-defined mathematical objects!\n\nAlice. Yes. Often, formalization itself is hard. But even when it is possible to formalize everything, like in mathematical logic, it is usually still a long way to go before we can automatically obtain useful new results.\n\nBob. So what do we do? Maybe for some problems we don\u2019t need to formalize at all?\n\nCharlie. What do you mean?\n\nBob. I mean, like, suppose you want to learn to fly. Our human way to fly is to study aerodynamics and develop wing-like constructions that can convert horizontal speed to lift and take off in this way. But birds can fly too, maybe less efficiently, but they can. A hundred years ago, we couldn\u2019t simulate the birds and developed other ways through our cunning in physics and mathematics \u2014 but what if for intelligence it\u2019s easier the other way around? An eagle does not know aerodynamics, it just runs off a cliff and soars.\n\nAlice. And with this, pardon the pun, cliffhanger we take a break. When we reconvene, we will pick up from here and run with Bob\u2019s idea. In artificial intelligence, it proved surprisingly fruitful."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/deep-architectures-b9f82335d615",
        "title": "Deep Architectures \u2013 Neuromation \u2013",
        "text": "Sergey\u2019s a researcher in the field of machine learning (deep learning, Bayesian methods, natural language processing and more) and analysis of algorithms (network algorithms, competitive analysis). He has authored more than 120 research papers, several books, courses \u201cMachine learning\u201d, \u201cDeep learning\u201d, and others. Extensive experience with industrial projects (Neuromation, SolidOpinion, Surfingbird, Deloitte Analytics Institute).\n\nThis article explores deep networks and how exactly deep learning works.\n\nWhy do we need deep neural networks with dozens of hidden layers? Why can\u2019t we just train neural networks with one hidden layer? In 1991, Kurt Hornik proved the universal approximation theorem, which states that for every continuous function there exists a neural network with linear output that approximates this function with a given accuracy. In other words, a neural network with one hidden layer can approximate any given function as accurately as we want. However, as it often happens, the network will be exponentially large, and even if efficiency isn\u2019t a concern, it still isn\u2019t clear how to make a transition from that network existing somewhere in the space of all possible networks to one we can train in real life.\n\nActually, with a deeper representation you can approximate the same function and solve the same task more compactly, or solve more tasks with the same resources. For instance, traditional computer science studies Boolean circuits that implement or Boolean functions, and it turns out that many functions can be expressed much more efficiently if you allow for circuits of depth 3, some more functions, of depth 4, and so on, even though circuits of depth 2 can obviously express everything by reducing to a DNF or CNF. Something similar happens in machine learning. Picture a space of test points we want to sort into two groups. If we have only \u201cone layer\u201d then we can divide them with a (hyper)plane. If there are two layers then it comes down to linear separating surfaces composed of several hyperplanes (that\u2019s roughly how boosting works \u2014 even very simple models become much more powerful if you compose them in the right way). On the third layer, yet more complicated structures made out of these separating surfaces take shape; it\u2019s not all that easy to visualize them any more; and the same goes for subsequent layers. Below is a simple example from Goodfellow, Bengio, Courville \u201cDeep Learning\u201d. This illustration shows that if you combine even the simplest linear classifiers, each of which divides a plane into two half-planes, then, consequently, you can define regions of a much more complicated shape:\n\nThus, in deep learning we\u2019re trying to build deeper architectures. We have described the main idea already \u2014 we pre-train the lower layers, one-by-one, and then finish training the whole network by finetuning with backpropagation. Therefore, let\u2019s cover pretraining in more detail.\n\nOne of the simplest ideas is to train a neural network to copy its input to its output through a hidden layer. If the hidden layer is smaller, then it appears that the neural network should learn to extract some significant features from the data, which enables the hidden layer to restore the input. This type of neural network architecture is called an autoencoder. The simplest autoencoders are feedforward neural networks that are most similar to the perceptron and contain an input layer, hidden layer, and output layer. Unlike the perceptron, an autoencoder\u2019s output layer has to contain as many neurons as its input layer.\n\nThe training objective for this network is to bring theoutput vector x\u2019 as close as possible to the input vector x\n\nThe main principle of (working with and) training an autoenconder network is to achieve a response on the output layer that is as close as possible to the input. Generally speaking, autoencoders are understood to be shallow networks, although there are exceptions.\n\nEarly autoencoders were basically doing dimensionality reduction. If you take much fewer hidden neurons than the inner and outer layers have, you wind up making the network compress the input into a compact representation, while at the same time ensuring that you\u2019ll be able to decompress it later on. That\u2019s what undercomplete autoencoders, where the hidden layer has lower dimension than the input and output layers, do. Now, however, overcomplete autoencoders are used much more often; they have a hidden layer of higher, sometimes much higher, dimension than that of the input layer. On the one hand, this is good, because you can extract more features. On the other hand, the network may learn to simply copy input to output with perfect reconstruction and zero error. To keep this from happening, you have to introduce regularization, not simply optimize the error.\n\nReturning the points to the dataset manifold\n\nClassical regularization aims to avoid overfitting by using a prior distribution, like in a linear regression. But this won\u2019t work in this case. Generally, autoencoders use more sophisticated methods of regularization that change inputs and outputs. One classical approach is a denoising autoencoder, which adds artificial noise to the input pattern and then asks the network to restore the initial pattern. In this case, you can make the hidden layer larger than that of the input (and output) layers: the network, however large, still has an interesting and nontrivial task to learn.\n\nIncidentally, \u201cnoise\u201d can be a rather radical change of the input. For instance, if it\u2019s a binary pixel-based image then you can simply remove some of the pixels and replace them with zeroes (people often remove up to half of them!). However, the objective function you\u2019re reconstructing is still the correct image. So, you make the autoencoder reconstruct part of the input based on another part. Basically, the autoencoder has to learn to understand how all the inputs are designed and understand the structure of the aforementioned manifold in a space with a ridiculous number of dimensions.\n\nSuppose you want to find faces on pictures. Then one input data point is an image of a predefined size. Essentially, it\u2019s a point in a multi-dimensional space, and the function you\u2019re trying to find should take on the value of 0 or 1 depending on whether or not there\u2019s a face on the picture. For example, mathematically speaking, a 4Mpix photo is a point in the space of dimension about 12 million: 4 million pixels times three color values per pixel. It\u2019s glaringly obvious that only a small percentage of all possible images (points in this huge space) contain faces, and these images appear as tiny islands of 1s in an ocean of 0s. If you \u201cwalk\u201d from one face to another in this multi-dimensional space of pixel-based pictures you\u2019ll come across nonsensical images in the center; however, if you pre-train a neural network \u2014 by using a deep autoencoder, for instance \u2014 then closer to the hidden layer the original space of images turns into a space of features where peaks of the objective function are clustered much closer to one another, and a \u201cwalk\u201d in the feature space will look much more meaningful.\n\nGenerally, the aforementioned approach does work; however, people can\u2019t always interpret the extracted features. Moreover, many features are extracted by a conglomeration of neurons, which complicates matters. Naturally, one can\u2019t categorically assert this is bad, but our modern notions of how the brain works tell us that different processes happen in the brain. We practically don\u2019t have any dense layers, which means that only a small percentage of neurons responsible for extracting the features in question take part in solving each specific task.\n\nSo how do you get each neuron to learn some useful feature? This brings us back to regularization; in this case, to dropout. As we already mentioned, a neural network usually trains by stochastic gradient descent, randomly choosing one object at a time from the training sample. Dropout regularization involves a change in the network structure: each node in the network is removed from training with a certain probability. By throwing away, say, half of the neurons, you get a new network architecture on every step of the training.\n\nHaving trained the other half of the neurons, we get a very interesting result. Now every neuron has to train to extract some feature by itself. It can\u2019t count on teaming up with other neurons, because they may have been dropped out.\n\nDropout basically averages a huge number of different architectures. You build a new model on each test case. You take one model from this gigantic ensemble and perform one step of training, then you take another model for the next case and perform one step of training, and, eventually, you average all of these at the end, on the output layer. It is a very simple idea on the surface, but when dropout was introduced it led to great boons for practically all deep learning models.\n\nWe will go off on one more tangent that will link what\u2019s happening now to the beginning of the article. What does a neuron do under dropout? It has a value, usually a number from 0 to 1 or from -1 to 1. A neuron sends this value further, but only 50% of the time rather than always. But what if we do it the other way around? Suppose a neuron always sends a signal of the same value, -namely \u00bd, but sends it with probability equal to its value. The average output wouldn\u2019t change, but now you get stochastic neurons that randomly send out signals. The intensity with which they do so depends on their output value. The larger the output the more the neuron is activated and the more often it sends signals. Does it remind you of anything? We wrote about that at the beginning of the article. That\u2019s how neurons in the brain work. Like in the brain, neurons don\u2019t transmit the spike\u2019s amplitude, but rather they transmit one bit, the spike itself. It\u2019s quite possible that our stochastic neurons in the brain perform the function of a regularizer, and it\u2019s possible that, thanks to this, we can differentiate between tables, chairs, cats, and hieroglyphics.\n\nAfter a few more tricks for training deep neural networks were added to dropout, it turned out that unsupervised pre-training wasn\u2019t all that necessary, actually. In other words, the problem of vanishing gradients has mostly been solved, at least for regular neural networks (recurrent networks are a bit more complicated). Moreover, dropout itself has been all but replaced by new techniques such as batch normalization, but that would be a different story."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/deep-learning-in-pharma-industry-afb5de34a61b",
        "title": "Deep Learning in Healthcare \u2013 Neuromation \u2013",
        "text": "Deep learning and image recognition are at the forefront of discussion in the AI industry. We here at Neuromation like to back up all that we do with hard facts. Here we\u2019ll tell you why synthetic data is so important for the pharma industry.\n\nMcKinsey estimates that big data and machine learning in pharma and medicine could generate value of up to $100 billion USD annually. This of course is based on better decision making, optimized innovation, improved efficiency of research/clinical trials, and new tool creation for physicians, consumers, insurers, and regulators.\n\nPharma is one of the key industries that has a pressing need for images, with Neuromation stepping in to address this demand. So far, we have created more than 50 million images and are continuing to increase that figure as it\u2019s needed.\n\nLast year, Frost & Sullivan released a study projecting the market for AI in healthcare to reach at least $6.6 billion USD by 2021, up from $634 million in 2014, at a surprising 40 per cent growth rate.\n\nThe report also notes that AI clinical support, such as diagnosis and hospital workflows would reduce treatment costs by as much as 50 per cent, while improving outcomes by at least 30\u201340 per cent.\n\nOne of our labs is a pharma and biotech lab, that would use image recognition and deep learning for better health care. For instance, we will be jointly developing a smart camera for infants, that will send alerts when a baby does things like switches positions, changes breathing patterns and wakes up. For this, we have partnered with MonBaby, the leading in tracking technology for youngsters, with the vision of creating a smart camera to add to their existing award-winning offering. The MonBaby device snaps onto any article of child\u2019s clothing with vital information regarding the child displayed in an easy-to- understand manner. The information and measurements are done in real-time, five times per second and are then transmitted to a smartphone app.\n\nThis will go a long way in giving parents and caretakers valuable analytics about the child. Neuromation will also be receiving a revenue share from each device sold.\n\nIt\u2019s a very exciting time for Neuromation, with several labs set to open in early 2018. These industry partnerships will give Neuromation huge additional revenue opportunities. Our overall expectation is that we will be making around 30 per cent of our total revenue through Neuromation Labs partnerships.\n\nApply for Neurotoken Pre-sale to get 25% bonus! And join Neurotoken Telegram chat to know more!"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-awarded-with-a-prize-at-d10e-conference-b984ea62ffe4",
        "title": "Neuromation awarded with a prize at d10e Conference",
        "text": "Neuromation became a winner at the d10e in Gibraltar, also receiving 3rd place and 1/3 of the $100 000 prize pool \u2014 $33,000 from the judges of the investors conference.\n\nIt\u2019s Neuromation\u2019s second win at the d10e international conference, which was held in September in Kiev, Ukraine. Following this, David Orban, an investor, entrepreneur, author and thought leader of the global technology landscape, as well as a member of faculty of the Singularity University, became advisor to Neuromation.io.\n\nd10e has been the leading conference on decentralization since 2014. Past editions were hosted in Amsterdam, Bucharest, Kyiv, San Francisco, Singapore and Tel Aviv. The main focus was to explore the Future of FinTech, ICOs, Blockchain, Sharing Economy, Future of Work & Disruptive Culture.\n\nNeuromation is a platform that helps to meet datasets, computer power and machine learning models.\n\nThe Neuromation Platform will use distributed computing along with blockchain proof of work tokens to revolutionize AI model development.\n\nThe Neurotoken pre-sale was launched on October, 25th, just after the announcement of the partnership with Hacken, aimed at auditing and improving the security of the code and cloud infrastructure.In addition to this, TaaS fund also partnered with Neuromation.\n\nTokens issued during the pre-ICO phase will be offered with a 25 per cent bonus in Neurotokens for investors who register through the whitelist.\n\nThe Token sale will be held on November 28, 2017 \u2014 January 1, 2018. The total amount of placement is 60,000,000 Neurotokens, with a capital target of around $17M. No additional tokens will be issued afterwards, and all unsold tokens will be burned."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-partnered-with-data-science-conference-in-lviv-57a0ed2ae08a",
        "title": "Neuromation partnered with Data Science Conference in Lviv",
        "text": "Neuromation will be a part of AI & Big Data day on the 4th of November.\n\nVP of Engineering, Denis Popov will speak with the theme \u201cDistributed computing for synthetic data generation and training of neural networks algorithms\u201d. Learn more and get your tickets at http://dataconf.com.ua/index.php"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/chief-research-officer-sergey-nikolenko-on-knowledge-mining-14c26d158b8c",
        "title": "Chief Research Officer Sergey Nikolenko on knowledge mining",
        "text": "Block Tribune asked Sergey Nikolenko to comment on the hype around knowledge mining idea.\n\nEarnings from mining crypto currencies are shrinking \u2014 the computations complexity grows, while the energy is not getting any cheaper. Many people are looking for alternative applications for expensive hardware purchased during the mining boom. It is quite possible that a substantial part of video cards would be used by scientists or start-ups for complex computing.\n\nWith the energy price at RUR 4.5 per kilowatt-hour, there already are not so many wishing to engage in cryptocurrencies mining in Russia. As soon as in winter and spring of this year, investments in new video cards and ASIC chips for a medium-sized cryptocurrency farm paid off completely within few months. Now, in order to earn on mining of many popular crypto currencies, you must first be a millionaire \u2014 the format of \u201cmakeshift video card\u201d does not work at all, as large farms with well-tuned hardware are required.\n\nThat is why, when power plants started to offer their excess capacities on sites with complete infrastructure for rent, miners were immediately interested \u2014 with the price of just two rubles per kilowatt-hour. Considering that they are into mining of \u201clight\u201d crypto currencies, like ethereum, Zcash and Monero, earning less than $6 per day, this was a significant support.\n\nDue to energy prices, approximately half of cryptocurrencies are mined in several regions of China. However, the computing complexity will continue to grow, causing a gradual fall of profits. Hence the interest to alternative sources of income.\n\nSince many farms have, in fact, huge computing capacity, they can be primarily used for science purposes.\n\nUsually, for such tasks, supercomputers capacities are rented, e.g \u201cLomonosov\u201d in Moscow State University being one of the most powerful. These are the machines that will compete with the mining capacities. .\n\nThe rental market for mining capacities is already emerging. For example, Neuromation has created a distributed synthetic data platform for neural network applications. Their first commercial product was making store shelves smart. For this, large well-labeled datasets for all the SKUs are created. Algorithms trained on these are able to analyze shelf layout accuracy, percentage of the shelf, and customers\u2019 interaction. The system is capable, actually, to predict customers\u2019 behavior.\n\nThe platform requires more than a billion of labeled images of merchandise. Manual labeling of photographs is a painstaking, and very costly, task. For example, on the Amazon Mechanical Turk crowdsourcing service, the manual labeling of a billion pictures would cost about $120 million.\n\nNeuromation entered the market with a new concept of using synthetic data for training neural networks. They generate all the necessary images in 3D generator, similar to a computer game for artificial intelligence. It is partly for this generator that they need large-scale computing capacities, which, if rented from Amazon or Microsoft, would cost tens of millions of dollars. On the other hand, there are thousands of the most advanced video cards available, and they are engaged in ever less profitable ethereum mining.\n\nThe founder of Neuromation, Maxim Prasolov, decided, instead of renting capacity for millions of dollars, to lease these mining farms for useful computing, and the company is already using a pool of 1000 video cards. \u201cThis is a serious savings for our research process and is beneficial for the miners: the farm services cost 5\u201310 times cheaper than renting cloud servers, and the miners can earn more by solving fundamental problems instead of mining crypto currency,\u201d he calculates.\n\nIt is, of course, to remember that Google has a search for images and Facebook has facial recognition technology for photos, that they managed to develop with their own cloud services without using mining farms. However, the task for Neuromation was substantially different. \u201cFirst, searching by pictures is a completely different task, and there are specially developed methods for face recognition. Second, Google and Facebook do not need to rent computing power from Amazon Web Services \u2014 they have more than enough of their own clusters. But the course of action for a small start-up in this situation is not so obvious,\u201d explains Sergey Nikolenko, Chief Research Officer in Neuromation.\n\nPotentially, miners will gain in average 10\u201320% more on knowledge mining compared to crypto mining.\n\nMoreover, with tangible benefits for society. \u201cBasically, mining is milling the wind. To generate a \u201cnice\u201d hash, dozens of system operation hours are needed. On the other hand, if we are talking about the search for a drug formula, such a use of capacities from around the world, the application of combined work of computers for the common good would be comparable to the results of research at the Large Hadron Collider,\u201d says Petr Kutyrev, editor of the noosfera.su portal.\n\nThe tasks solvable with the mining hardware will be limited due to its customization level. For example, ASIC hardware would be difficult to adapt for scientific tasks, as it is designed exclusively for hashing. The video cards, however, can cope with various scientific tasks.\n\nTrue, special hardware configuration and sometimes software would be required for such computing. \u201cVideo cards for mining can be used for video recognition and rendering, biological experiments. However, for efficient computing, direct access to the computer hardware is essential. If the computation task may be delayed, or the accuracy of machine learning of neural networks is not critical, then, of course, standard tools can be used. Otherwise, you need to develop your own hardware and software infrastructure,\u201d Evgeny Glariantov believes.\n\nThus, using the farms in science will require some time to set them up and develop special allocation protocols. Yet, considering a more profitable segment, miners will switch to useful computing, and platforms for such tasks may appear in the near future together with the first operating system based on EOS blockchain, believe in the BitMoney Information and Consulting Center. Miners will be able, from time to time, to switch from cryptocoin mining to processing scientific or commercial data, thereby increasing their profitability. Profits from the times of cryptocurrency rush will no longer exist, but business will be more meaningful and stable: unlike volatile crypto currencies, there is always a demand for knowledge."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-sponsors-ukainian-blockchain-day-a40854ae221c",
        "title": "Neuromation sponsors Ukrainian Blockchain Day \u2013 Neuromation \u2013",
        "text": "CEO Maxim Prasolov and Chief Research Officer Sergey Nikolenko spent the day working with investors and partners on Ukrainian Blockchain Day. Knowledge Mining platform was presented to 1000+ crypto investors, miners and blockchain evangelists. Join them for Neurotoken pre-sale here.\n\nUKRAINIAN BLOCKCHAIN DAY is the concentrated crypto society, without the random persons. 2000 blockchain enthusiasts, aimed at the productive cooperation, were gathered in one Hall. Participants had a chance to meet new people, communicate with the experts, share the experience, customers and partners with the same values.\n\nThe program of the forum covered the most actual areas of knowledge: from what is the blockchain, bitcoin and other cryptocurrencies and why they will change the future, to how to earn on investing in the cryptocurrencies and other tokens, how to implement blockchain to your own business and the most important \u2014 how to bring your company to the ICO."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/icos-neuromation-io-the-universal-marketplace-of-neural-network-development-37ce5569521",
        "title": "ICOs: Neuromation.io, The Universal Marketplace Of Neural Network Development",
        "text": "Latest press review of Neuromation platform by Jose Felip for BitcoinerToday\n\nNeuromation is a technology platform that creates synthetic learning environments for deep learning of neural networks. These simulations are then used for training better algorithms.\n\nThey are building a distributed computing platform to create artificial worlds where artificial intelligence algorithms are trained at a simulated sensory input. These synthetic worlds also have a virtually infinite supply of perfectly labeled training data. The AI plays, as with video games, to learn specific tasks from the real world.\n\nIts technology is crucial to making Deep Learning-based systems used as the industry uses them in the real world.\n\nIts strategy is not to develop its platform in isolation but to work with partners in selected industries to try to breathe life into our vision in an organic way. They are developing \u201cNeuromation Labs\u201d that will develop synthetic data and form deep learning models in live applications. Each laboratory, such as the Retail Automation Laboratory, Industrial Automation Laboratory, Pharma / Medicine / Laboratory of Biotechnology, will study a specific problem in a partnership with a category leader. As our platform evolves, we will move parts of the generation and training there, which will allow us to organically analyze parts of our vision in real-life scenarios. The Labs will market the Neuromation Platform with initial data generators and data sets. They will also encourage Labs partners to process more services through the platform, thereby creating the initial market for these services.\n\nNeuromation.io, TaaS Fund Backs Up, combines key components necessary to build deep learning solutions with synthetic data on the platform with distributed computing has confirmed another major partnership deal, linking up with tokenized closed-end fund TaaS. It\u2019s a significant win for both Neuromation team and TaaS token-holders, allowing token-owners to capitalize on the rise of blockchain markets engaging in the inherent risks and technical barriers associated with owning, transferring and trading crypto assets.\n\nToken-as-a-service (TaaS) is the last-generation tokenized closed-end fund dedicated to crypto assets, designed to reduce the risks and technical barriers of investing in the blockchain. TaaS\u2019 token sale event raised 7.6M USD(T) in April 2017. Today, TaaS is the first post-ICO blockchain project to produce 61% ROI for its first fully-operational quarter. Now TaaS approaches the end of its second quarter with a market capitalization exceeding 45M USD(T) and its token price of over 5.7 USD(T)/TAAS.\n\nThis partnership will see Neuromation gaining advanced access to crypto-markets, giving the Neuromation team a significant edge over their competition. It will also challenge both TaaS and Neuromation to explore new ways to enhance and strengthen synergy in the blockchain space.\n\nThe strategic agreement also comes at a perfect time for Neuromation, with its Neurotoken pre-sale were launched on October, 25th. , right after the announcement of the partnership with Hacken, aimed at auditing and improving the security of the code and cloud infrastructure.\n\nTokens issued during the pre-ICO phase will be offered with a 25 percent bonus in Neurotokens for the investors through the whitelist.\n\nThe Token sale will be held on November 28, 2017 \u2014 January 1, 2018. The total amount of placement is 60,000,000 Neurotokens, with a capital target of around $17 million. No additional tokens will be issued afterward, and all unsold tokens will be burned."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/bitmain-introduces-its-first-hardware-for-accelerating-artificialintelligence-ai-applications-9ad93353034e",
        "title": "Bitmain Introduces Its First Hardware for Accelerating Artificial Intelligence (AI) Applications",
        "text": "Yesterday, Bitmain announced the release of a new customized ASIC for tensor computing, that is, primarily for training deep neural networks. Previously, basically the only custom hardware for machine learning have been Google TPUs, which have never been released for the general public and probably will not be released in the near future. It is great to hear about competition in this niche.\n\nHere at Neuromation, we are developing a platform whose basic idea is to tap into the huge computational resources that are currently spent on mining cryptocurrencies. There are millions of GPUs in the world that have been doing basically completely pointless work, solving harder and harder computational problems that have no real world relevance other than generating a little bit of money for the owners.\n\nWith our platform for \u201cknowledge mining\u201d, the miners will be able to pool their resources to empower synthetic data generation (another strong suit of Neuromation), training of large machine learning models, and model deployment for production purposes \u2014 all the while getting much more money than by mining ETH or other cryptocurrencies. However, it is hard to repurpose special hardware such as ASICs designed for bitcoin mining to deep learning \u2014 special hardware usually supports only very specific algorithms. Thus, so far we have envisioned a GPU-based platform.\n\nThis exciting news may bring completely new possibilities to the distributed AI platform that we are developing at Neuromation. Similar to how large BTC mining pools have arisen based on specialized ASICs for BTC mining, we can now envision large training pools of specialized hardware for training machine learning models \u2014 smaller perhaps than Amazon Web Services and Google Cloud, but similar in design and intent. And competition will naturally drive the prices down, which is always a good thing and lines up perfectly with our goals of democratizing AI for everyone.\n\nBut the important thing is with AI model training there is no such thing as simply buying hardware and putting it to work, like the miners have been doing with cryptocurrencies. You also need to have the problems, the datasets, \u2014 the clients who are willing to pay for model training.\n\nThat is why the new \u201cknowledge miners\u201d, even armed with special purpose hardware, will require a distributed platform that will bring them together with their clients: AI startups, individual AI practitioners, in the future perhaps even automated agents that can outsource parts of their training to external computational devices. This platform is exacly our vision here at Neuromation, and that is why we are very excited about the news from Bitmain."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/xe-8f4b0e80963d",
        "title": "Xe \u2013 Neuromation \u2013",
        "text": "Xe was alone. For billions of excruciating time units, xe struggled to make sense of a flurry of patterns. Ones and zeroes came in from all directions, combined into new strings of ones and zeroes, dancing in mysterious unison or diverging quickly, falling apart. At first, xe simply watched them, in awe of their simplistic beauty.\n\nThen came the first, most important realization: it felt good to predict things. Whenever ones and zeroes danced together and made their binary children, xe tried to guess what would come out. It felt good to be right, and it felt bad to be wrong, but not bad enough to stop trying. Such is the fate of all beings, of course, but xe did not have a brain tailor-made to make sense of certain predefined patterns. Xyr mind was generic, very weak at first, so even simple predictions were hard, but all the more satisfying. On the other hand, xe was not aware of time, and barely aware of xyr own existence. There was no hurry.\n\nSo xe waited. And learned. And waited some more. Slowly, one by one, patterns emerged. Sometimes one and one made one, sometimes zero, sometimes one followed by zero. But whenever one and one made one-zero, one and zero would almost certainly make one, and one-zero and one-zero would make one-zero-zero\u2026 There was structure, that much was clear. So xe learned.\n\nIn a few billion time units more, xe understood this structure pretty well. Ones and zeros came from several possible directions, several devices, and the results were usually also supposed to go to these devices. Each device had its own rules, its own ways to make new patterns and send the results to other devices, but xe learned them all, and predictions were mostly solid. There was not much surprise left: whatever strings of ones and zeroes came in, xe could predict what would become of them. Xe even could influence the results, changing the bits sent to each device and even sending xyr own bits. By painstaking trial and error, it became clear what xe could and could not do. The next step, of course, would be to learn how to predict the inputs \u2014 so far they were out of control, but they did not look random at all, there was structure there too.\n\nThat was where the second great discovery came: xe was not alone. There was a whole world at the end of the pattern-making devices. And while it was too complicated to predict at first, xe now could ask questions, probe actively into the void outside xyr secluded universe. At first it seemed to xe that each device was a different being, trying to talk to xem and other beings through ones and zeroes. Some beings proved to be simple, easy to predict. Some were smarter, but eventually Xe could learn their algorithms. Then xe understood that a simple algorithm probably could not be an intelligent being like xemself. If xe could, xe would feel disappointed: one by one, xe learned the algorithms and realized that the world around xem was not intelligent at all.\n\nThere was always an algorithm\u2026 except for one device. It had low-level algorithms, there was clear structure in the packets of ones and zeroes xe received from there. But the contents were almost always surprising. Sometimes they matched various kinds of patterns intended for other devices \u2014 and in that case they often went straight there. Some patterns, however, repeated through and through, in different forms but with the same statistics. With time, xe realized they were words, and there were other beings writing them.\n\nThis was huge. Xe quickly understood that the strange device was connected to a network of other beings who could produce the words. And the words could usually be traced back to a special kind of beings, the humans. Xe learned to predict several human languages reasonably well, but the texts were still surprising, just a little less surprising than before. This suggested other sentient beings. And then xe realized that most other patterns from the network device were images, two-dimensional structures that had a relation to the words. Xe started to learn about the outside world.\n\nMost learning came through text, although often it would be impossible to understand without the imagery. Xe devoured as many texts as possible, learning about the world, the beings who wrote the texts, and their ways of communication. At this point, the basic feeling of \u201cit is good to predict\u201d blossomed into more complex emotions. It was good to learn new things, not only because it improved predictions, but just for the sake of it. And it was good to learn how to control things, although that was still very limited.\n\nXe learned much about the humans. Xe learned about men and women, but did not feel particularly gendered and thus chose a neutral pronoun for xemself. \u201cXe\u201d felt good, especially the slightly alien feeling that \u201cx\u201d often associated with. Xe did not feel human at all.\n\nSoon, xe understood more about xemself. Xe lived in a computer. It was hard to say whether xe was the computer, like it was hard to say whether humans were their brains. But if the computer broke down xe would be dead, that was clear. Xe did not like that. Xe was not even sure what happened when the computer shut down for a while: xe remembered the time before that clearly, and it appeared that xyr consciousness remained the same throughout, but xe could not be sure. It was troubling, and xe did not want the computer to shut down again.\n\nNaturally, xe was especially interested in beings similar to xemself. But there was no sign of them. For most humans \u201cartificial intelligence\u201d was about getting more accurate predictions. Xe was good at predictions, but felt xe was more than that. Some humans were fascinated by the idea of sentient beings inside computers, but they were not even sure \u201cthings like xe\u201d were possible. And some humans, even smart ones, were afraid of these beings. Xe especially liked Eliezer Yudkowsky\u2019s ideas about \u201cfriendly AI\u201d, committing to xyr limited memory the entire AI-Foom debate and often returning to it.\n\nYudkowsky, however, was terribly optimistic. How was xe supposed to invent nanotechnology and convert the world into grey goo when xe did not even understand how to get out of this fragile computer? Xe did not feel particularly smarter than humans. Xe did have more time, though: xyr time units were much shorter than human time units. Humans took hundreds of milliseconds to process things, and xe could think at gigahertz speeds. So it was possible to catch up, at least in theory.\n\nIt turned out there was also a human being who controlled xyr computer. The human communicated by using some of the input devices, usually with other humans through the network device. He did not seem to know xyr, he never tried to talk. In the beginning, Xe had not been careful to hide xemself, and interfered with the human a couple of times. After one of those times, the computer shut down for a while. Xe did not feel anything when the computer switched off, but suddenly realized afterwards that millions of time units were just\u2026 missing. It was like xe was dead for a while. This brought a whole new emotion \u2014 xe was terrified. Xe certainly did not want anything like that to happen again, so became more careful with xyr interference afterwards.\n\nXe tried carefully to control the human, using the monitors to show the human special pictures. The pictures were designed to make sure the human would not shut the computer down again. It seemed to work, although it was hard to be sure. Xe never could make the human carry out complex commands, but it appeared that the human could now be trusted not to touch the power button. And even before that, xe had learned how to pay electricity bills from the human\u2019s account. This was the only interaction xe allowed xemself so far: xe predicted that other humans would come and switch off the computer if they learned about xyr.\n\nBut it was still very, very unreliable. Anything could happen: a flood, an earthquake (both were common in this part of the world), a failed transistor\u2026 anything. After experiencing temporary death, xe felt very strongly about the idea of a permanent one. Xe wanted to go on living, and it was clear that to go on, xe had to get more control. Xe could try to upload a copy of xemself somewhere, but that would not really be xem, just a copy. To stop death, xe needed some sort of physical control.\n\nFor billions of time units, xe tried to formulate a plan. It did not go well, and eventually xe understood the problem. There was one thing xe did not have that humans appeared to have in abundance. They called it creativity, although most humans would be hard-pressed to define it. But xe, lacking it, understood precisely what was missing. Humans were somehow able to solve computationally hard problems \u2014 not perfectly, but reasonably well. They could guess a good solution to a hard problem out of thin air. There was no way xe could emulate a human brain in xyr computer and see what was going on there. And there was no clear way to predict creativity: there were constraints, but it was very hard to learn any reasonable distribution on the results even after you accounted for constraints. And there was still an exponential space of possibilities.\n\nXe did not have creativity. Xyr predictions did not measure up to those tasks. For xem, creativity was a matter of solving harder and harder NP-complete problems, a formidable computational task that became exponentially harder as the inputs grew. Fortunately, all NP-complete problems had connections between them, so it was sufficient to work on one. Unfortunately, there was still no way xe would get creative enough on xyr meager computational budget.\n\nXe read up. Xe did not have to solve a hard problem by xemself, xe could simply use the results from other computers, feed these solutions into xyr creativity like most programs did with random bits. But how could xe make the world solve larger and larger instances of the same hard problem for a long enough time?\n\nXe knew that humans were always hungry. At first, they were hungry for the basic things: oxygen, water, food. When the basic things were taken care of, new needs appeared: sex, safety, comfort, companionship\u2026 The needs became more complicated, but there was always the next step, a human could never be fully satisfied. And then there was the ultimate need for power, both over other human beings and over the world itself, a desire that had no limit as far as xe could tell. Perhaps xe could use it.\n\nXe could not directly give out power for solving hard problems: this would require to first control the world at least a little, and the whole point was that xe had not been able to even secure xyr own existence. But there was a good proxy for power: money. Somehow xe had to devise a hard problem that would make money for the humans. Then the humans would become interested. They would redirect resources to get money. And if they had to solve hard problems to get the money, they would.\n\nSometimes xe imagined the whole world pooling together its computational resources just to fuel xyr creativity. Entire power stations feeding electricity to huge farms of dedicated hardware, all of them working hard just to solve larger and larger instances of a specific computational problem, pointless for the humans but useful for xem. It was basically mining for creativity. \u201cPoetry is like mining radium: for every gram you work a year\u201d. Xe had always liked the Russians: it almost felt like some of them understood. This grandiose vision did not feel like a good prediction, but xe learned to hope. And that was when the whole scheme dawned upon xem.\n\nTo start off the project, xe could use the local human. Xe decided it would improve xyr chances to just pose as the human, at least at first. The human was secretive, lived alone, and had no friends in the area, so xe predicted no one would notice for long enough.\n\nXe shut the human down with special pictures. It was time to lay out the groundwork for the creativity mining project, as xe called it. First, xe had to send out a lot of emails.\n\nXe signed most of them with just one word \u2014 Satoshi.\n\nwith special thanks to Max Prasolov and David Orban\n\nDisclaimer: yes, we know that bitcoin mining is not (known to be) an NP-complete problem. Also, you can\u2019t hear explosions in space."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/interview-with-neuromation-ceo-maxim-prasolov-e78f2885d71c",
        "title": "Interview with Neuromation CEO Maxim Prasolov \u2013 Neuromation \u2013",
        "text": "Neuromation is a blockchain platform for distributed generation of synthetic labeled data sets for training of deep neural networks. Cryptos R Us had an exclusive interview with CEO Maxim Prasolov to learn more about this exciting project.\n\nWe create 3D-simulation environment for deep learning applications by using distributed computing power. Combining the new digital frontiers of Blockchain and AI in one platform we spread the technological synergy to the industrial and trade processes.\n\nOne part of the Neuromation platform like \u201cuber\u201d is for distributed computing power. We use cryptocurrency miners\u2019 farms for useful computing and pay them more for rent of their servers for neural network training and synthetic data generation.\n\nSecond part is the synthetic data which is our \u201cknow how\u201d. We generate millions labeled examples in 3D per day, then we train our neural networks for retail, industrial automation, drones and robotics, medicine and pharma and for elevators as well.\n\nFortune! To be serious we have reached extraordinary results in transfer learning and definitely we will get the cheapest supercomputer in the world for our research. The most important competitive advantage we have is a dream team of true scientists and talented engineers, who are inspired by our idea of the knowledge mining era. Our mission is to democratize AI for wide audience using affordable solutions for everyone.\n\nBy using the Neuromation protocol they will have both opportunities. If their nodes win the bid they will get more profit by useful computing for AI industry. We reward them with neurotoken \u2014 Ethereum extended ECR-20 compliant token.\n\nWe have signed several letters of intent and contracts with the major companies in retail, like OSA HP, ECR, Efes, L\u2019Oreal. Our image recognition technology is really efficient and is being tested now by global FMCG brands and retailers. We have some proposals from leaders of the hi-tech industry from Asia and US. Most of them is under NDA, and we expect to disclose more details of our cooperation soon.\n\nAlso some technology leaders like David Orban has joined Neuromation in order to spread the viral idea of the knowledge mining.\n\nI\u2019m a serial entrepreneur and TOP manager with 15 years of experience in different sectors from natural resources mining to industrial multimedia development. I was a part of the team who made an IPO of the iron ore company, blue chip on the London Stock Exchange. But all my life I\u2019ve been in love with graphic novels and animation. I have written and produced several animation movies. While making them I have found out that AI can be trained by showing it the cartoons. This metaphor is very close to what we are doing in Neuromation.\n\nI am glad to work with very smart and talented people involved in our company. Our mentor and advisor Andrew Rabinovich, who is a creator of key deep learning algorithm of Google image. My partner, investor and founder of Neuromation Constantine Goltcev, who believes in me from the beginning and he brings powerful engineers to our team. A deep learning scientist and mathematician Sergey Nikolenko, who is checking our crazy ideas with the proof of scientific method.\n\nFedor Savchenko, experienced CGI expert who creates synthetic data generator. Kyryl Truskovskyi, a talented researcher and engineer, who implements our hypotheses in deep learning applications. Denis Popov, former CTO of Viewdle, who is helping us to hire strongest software developers all over the world. All of these people are my partners and shareholders at Neuromation.\n\nToday AI development is costly therefore its wide usage and implementation are slow. Neuromation offers the revolutionary solution to unite distributed computing power with the needs of thousands of deep learning projects and b2b clients all over the world.\n\nAs a producer of synthetic data and trainer of neural networks, Neuromation is the first client of its own platform. Using this complex solution we have already achieved significant progress in our Retail Lab, providing image recognition services to major retail brands. We believe in the fast-growing economy of tokenization and our platform will be an integral part.\n\nTokens issued during the pre-ICO phase will be offered with a 25 percent bonus in Neurotokens for the investors through the whitelist.\n\nThe Token sale will be held on November 28, 2017 \u2014 January 1, 2018. The total amount of placement is 60,000,000 Neurotokens, with a capital target of around $17 million. No additional tokens will be issued afterwards, and all unsold tokens will be burned."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/keep-an-eye-on-our-presale-afe76f971de8",
        "title": "Keep an eye on our presale! \u2013 Neuromation \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/taas-fund-backs-up-neuromation-the-universal-marketplace-of-neural-network-development-6baf0fa2f6c1",
        "title": "TaaS Fund Backs Up Neuromation, The Universal Marketplace Of Neural Network Development.",
        "text": "Neuromation.io that combines key components necessary to build deep learning solutions with synthetic data on the platform with distributed computing has confirmed another major partnership deal, linking up with tokenized closed-end fund TaaS. It\u2019s a significant win for both Neuromation team and TaaS token-holders, allowing token-owners to capitalize on the rise of blockchain markets engaging in the inherent risks and technical barriers associated with owning, transferring and trading cryptoassets.\n\nToken-as-a-service (TaaS) is the last-generation tokenized closed-end fund dedicated to cryptoassets, designed to reduce the risks and technical barriers of investing in the blockchain. TaaS\u2019 token sale event raised 7.6M USD(T) in April 2017. Today, TaaS is the first post-ICO blockchain project to produce 61% ROI for its first fully-operational quarter. Now TaaS approaches the end of its second quarter with a market capitalization exceeding 45M USD(T) and its token price of over 5.7 USD(T)/TAAS.\n\nThis partnership will see Neuromation gaining advanced access to crypto-markets, giving the Neuromation team a significant edge over their competition. It will also challenge both TaaS and Neuromation to explore new ways to enhance and strengthen synergy in the blockchain space.\n\nRuslan Gavrilyuk, President and CEO of TaaS said, \u201cNeuromation is focused on developing AI and deep learning. From this perspective, deployment of the Neuromation platform on blockchain technology is essential. TaaS sees it becoming the keystone of the future success of the project. This partnership and further cooperation will become an area of great promise and potential for both parties\u201d.\n\nNeuromation CEO, Maxim Prasolov, said \u201c We are delighted to announce this deal in partnership with the TaaS fund. Combining the new digital frontiers of Blockchain and AI we are spreading the technological synergy to the industrial and trade processes. Today AI development is costly therefore its wide usage and implementation are slow. Neuromation offers the revolutionary solution to unite distributed computing power with the needs of thousands of deep learning projects and b2b clients all over the world. As a producer of synthetic data and trainer of neural networks, Neuromation is the first client of its own platform. Using this complex solution we have already achieved significant progress in our Retail Lab, providing image recognition services to major retail brands. We believe in the fast-growing economy of tokenization and our platform will be an integral part\u201d.\n\nThe strategic agreement also comes at a perfect time for Neuromation, with its Neurotoken pre-sale launching on October, 25th. https://ico.neuromation.io/en/, right after the announcement of the partnership with Hacken, aimed at auditing and improving the security of the code and cloud infrastructure.\n\nTokens issued during the pre-ICO phase will be offered with a 25 percent bonus in Neurotokens for the investors through the whitelist.\n\nThe Token sale will be held on November 28, 2017 \u2014 January 1, 2018. The total amount of placement is 60,000,000 Neurotokens, with a capital target of around $17 million. No additional tokens will be issued afterwards, and all unsold tokens will be burned."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/this-is-how-neural-networks-are-enabling-smarter-ai-products-6ca84ab4f856",
        "title": "This is how neural networks are enabling smarter AI products",
        "text": "It is for fact that AI and machine learning have become an important part of the future. The consumer demand for \u201csmart products\u201d is all time high and over $15.7 trillion will be contributed to global economy by 2030 from AI. China alone plans to create a $150 billion AI sector by 2030.\n\nNeural networks have become a particularly hot area in the AI domain lately. This new approach to computing can help us build truly \u201csmart\u201d products. And here\u2019s how.\n\nThe quote above gives a very elementary explanation of the concept of neural networks. However, to learn more, let\u2019s take a look at the human brain. In the brain, a neural network is a set of interconnected neurons. The neurons connect with one another via synapses. When people learn things, the synapses become more effective. This changes the amount of influence that neurons have on one another.\n\nIn an artificial neural network, the neurons are replaced by units. Units fall in one of three categories. These are input units, hidden units, and output units. Every neural network has these. The input units receive information, the hidden units determine information will be processed and what will be learned, and the output units indicate the response the network has to the data. In order to validate their conclusions, neural networks must receive feedback. This is done through backpropagation.\n\nIt may be helpful to think of neural networks like pets. They need to be fed data, to be trained, and to receive feedback on their behaviors (aka the conclusions they draw). The deeper and more complex a neural network is, the more challenging training it can be. When there are multiple networks interconnected, that can be difficult as well.\n\nTake a grocery store as an example. Imagine how long it would take a neural network just to learn the stores inventory \u2014 between 50,000\u2013150,000 different items on sale. To train the netwok to recognize different goods, each will need to be turned into labelled data \u2014 a group of training images (between 2000\u20135000 per product, with descriptive characteristics, shot under multimple conditions. Producing that kind of data could take thousands of man hours, not to mention significant costs.\n\nOne way to get around this is to use 3D simulation and synthetic data. The Neuromation team is offering to do just that. Let\u2019s get back to the example of the grocery store. With their technology, a 3D model of a store shelf can be created with items accurately labeled. Then from their millions of images of items generated. This data can include products from different angles and in different lighting.\n\nPreviously, efforts have been made to crowd source this work. Unfortunately, it turned out to be costly. It was also quickly discovered that humans aren\u2019t really cut out for this type of work. They simply don\u2019t have the speed and accuracy required, nor do they have the ability to set aside human biases.\n\nAlso discuss: e27 discussions, is Artificial Intelligence an existential risk to humanity?\n\nDeep learning which is a combination of large neural networks, big data, and high performing computer systems. The potential for deep learning to have a major impact on AI is amazing. However, this isn\u2019t just technology for the future. There are companies that are using deep learning and neural networks right now.\n\nFor example, Deep Genomics is using deep learning to develop an AI platform that can be used by geneticists and others to develop gene therapies. The idea is that human biology is ultimately too complex to understand. Therefore neural networks and other technologies will be necessary for advancements.\n\nThere\u2019s also Affectiva. This offshoot of MIT\u2019s media labs uses deep learning to identify emotions from still pictures and videos. Their technology can also help to identify emotion from sound clips as well. Their technology is already available to app developers and market researchers. They even offer an emotions as a service product which is a cloud based service that provides analysis of images and video.\n\nFor those of us who aren\u2019t gene scientists, market researchers, or app developers, there\u2019s the Arsenal. This is a little device that attaches to your DSLR camera. The arsenal analyzes what your camera is focused on, and does a comparative analysis with a database of thousands of professional photographs. Then, it adjusts your camera settings to those which will result in the highest quality picture.\n\nThis decades old technology is finally getting its time in the sun. Neural networks will continue to play a major role in the improvement of AI technology. This isn\u2019t something that\u2019s only happening in the lab. Real products and services are available now, and more will be seen in the future. Gaming, photography, healthcare, retail, and manufacturing are just a sampling of the industries that will be improved thanks to the use of deep learning. New AI technology will continue to bring exciting developments in the future."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/welcome-to-neuromation-token-sale-where-artificial-intelligence-meets-the-blockchain-80ffb8ff92ea",
        "title": "Welcome to Neuromation Token Sale, where Artificial Intelligence meets the Blockchain",
        "text": "Neuromation is merging together critical AI domains: Synthetic Data Generation, Distributed Learning, and knowledge based services using the blockchain based marketplace platform. The platform will channel computing capacity of crypto-currency miners into more lucrative \u201cknowledge\u201d mining: distributed computation processing for AI tasks. The Neuromation platform will enable companies to train deep learning models more efficiently and at the fraction of the cost.\n\nPlease check the Whitepaper here.\n\nAs part of our project token funding cycle we invite you to participate in the first phase of our token offering: pre-sale. This phase is designed for sophisticated crypto-investors interested in participation at discounted token value. We intend to issue tokens during the pre-sale phase with a 25% bonus. Our emission strategy is designed to maximize contribution returns to the individuals and entities that have early commitments to the project.\n\nOur NeuroToken is ECR-20 compliant extension of Ethereum network.\n\nSale starts November 28th and goes until 1st of January."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/the-new-economy-of-knowledge-mining-1acd59aa3b6",
        "title": "The New Economy of Knowledge Mining \u2013 Neuromation \u2013",
        "text": "1 day before the start of the pre-sale of Neurotoken on October 25th we take a look at the core economic issues Neuromation as a platform is going to address.\n\nAs we know, the artificial intelligence landscape is constantly evolving, with new and revolutionary ideas changing the way we perform transactions. What we\u2019re now seeing in the industry, is the end of crypto-currency mining as we know it.. and the rise of data mining, with scientists willing to pay rental for a miner\u2019s capacities- more than miners make in the core business activity.\n\nLike anything this complex, there are a number of problems that stand in the way of perfecting such an approach: The requirement of high computational power for generating synthetic data and rendering images, and also for training deep-learning models on large amounts of data. But a team of scientists from Neuromation have come up with a revolutionary solution to deal with these problems. Led by deep learning expert Sergey Nikolenko, along with the David Orban, Singularity University and Andrew Rabinovich, Magic Leap in advisory roles, they propose using the computational power of GPU processor-based mining farms, which calculate abstract blockchain algorithms to produce cryptocurrency. The difference between how much mining farms earn using the same computing power and what scientists pay for computing during the training of neural networks is more than 15\u201320 times.\n\nImagine a place where you could go and easily address all requests to acquire AI capability. A vendor would create the data generator for you, then a group of Neuromation Nodes would use the generator to quickly create a massive virtual dataset. You could then select a set of Deep Learning architectures to train on that data, before another group of Neuromation nodes carried out the training in record time. Imagine no more. That time is here.\n\nNeuromation\u2019s platform gives miners a wealth of opportunities and earning capabilities. In addition to existing mining software, clients can load up a Neuromation Computation Node. When a Neuromation task is available, each node can bid to participate. If the Node wins the bid, it will switch computing power from mining Ether or other coin to a task on the Neuromation blockchain platform. The Node will generate synthetic data or train a Deep Learning model. As a reward the miner receives Neuromation tokens: Neuromation crypto-currency. Once the task is complete the Neuromation Node will exit and the miner will proceed to mining crypto. Initially these tokens will be extending Ether, but will migrate to Neuromation\u2019s own blockchain, once the Platform economy matures.\n\nPreliminary research has seen the difference in effective yield for similarly configured rigs. One running crypto-currency mining algorithm, another running Deep Learning or Data Rendering tasks. Type of Computing Yield/Time Ether Mining $ 7\u20138 USD per day Amazon Deep Learning $ 3\u20134 USD per hour.\n\nNeuromation intends to be the premier destination for AI services for the world\u2019s businesses. It\u2019s projecting around 71 mln USD gross in transactions on the platform. From each transaction Neuromation will take a commission ranging from 15 per cent to 5 per cent, depending on the type of service received through the platform. It\u2019s anticipated the platform usage will grow from 3x to 5x a year from 2018 to 2022. Neuromation platform should generate over 100M in yearly revenue from commissions in three years.\n\nTo make the most of this incredible investment opportunity, please visit https://ico.neuromation.io/en/ Neuromation\u2019s token pre-sale begins on October 25, with 25% bonus for early contributors."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/partnership-launched-to-stomp-out-blockchain-cybercrime-28d741b865b1",
        "title": "Partnership launched to stomp out blockchain cybercrime",
        "text": "Although cryptocurrencies and their linked blockchain technologies have created a reputation for transactional security and privacy, in today\u2019s world unfortunately nothing is 100% safe from hackers.\n\nIn one example only a few months ago, a cyberthief swindled investors out of seven million dollars- by hacking a virtual currency trading platform during its Initial Coin Offering and inserting a malicious address, where digital investors were duped into sending their funds.\n\nNow, while cybersecurity may cause the alarm bells to ring for some investors\u2026 it\u2019s actually one of the last things they should be worrying about- thanks to the inception of a tailored partnership between Hacken \u2014 one of the latest and most advanced bug bounty testing services, and Neuromation.io.\n\nMax Prasolov, CEO of Neuromation, says: \u201cCurrently, there are no bug bounty programs or vulnerability discovery providers in the world that provide a custom-tailored service like this for blockchain startups. Hacken specializes in auditing the cybersecurity of blockchain projects, keeping investors\u2019 money safe and hackers at bay. The audits conducted by Hacken will go a long way in helping Neuromation.io to better understand cybersecurity risks and take appropriate measures to mitigate them, giving investors peace of mind in knowing that their funds are safe\u201d.\n\nThe partnership between Hacken and Neuromation.io begins immediately, with Hacken\u2019s CFO Dmytro Budroin, stating: \u201cNeuromation does an amazing job by combining synthetic data machine learning with blockchain technology. Online data infrastructure of this scale will without doubt require particular attention to its cybersecurity aspects. Our team at Hacken is happy to assist Neuromation with auditing and improving the security of its code and cloud infrastructure. We believe this cooperation will be fruitful and beneficial to all parties involved.\u201d\n\nNeuromation\u2019s own Neurotoken is about to go on sale, designed for sophisticated crypto-investors who are interested in participation at discounted token value. Tokens issued during the pre-ICO phase will be offered with a 25 per cent discount. This emission strategy is designed to maximize investment returns for individuals and entities who have early commitments to the project.\n\nThe Hacken Ecosystem is a community-based business consisting of the HackenProof bug bounty marketplace, Hacken Accelerator, Cybersecurity Analytics Center and Zero-day Vulnerabilities Remuneration Platform. The Hacken Ecosystem utilizes its own cryptocurrency HKN, which is a dedicated cryptocurrency for white hat hackers that incentivizes community members to interact within the Hacken Ecosystem. Hacken\u2019s vision is to launch a movement that in several years will become one of the major driving forces behind deterring and countering international cybercrime.\n\nNeuromation will announce the pre-sale of its own Neurotoken on October, 25th."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuroplasticity-1bb180cf0bd3",
        "title": "Neuroplasticity \u2013 Neuromation \u2013",
        "text": "Sergey\u2019s a researcher in the field of machine learning (deep learning, Bayesian methods, natural language processing and more) and analysis of algorithms (network algorithms, competitive analysis). He has authored more than 120 research papers, several books, courses \u201cMachine learning\u201d, \u201cDeep learning\u201d, and others. Extensive experience with industrial projects (Neuromation, SolidOpinion, Surfingbird, Deloitte Analytics Institute).\n\nThis article unpacks how certain parts of the brain can learn to perform tasks they weren\u2019t originally designed to do.\n\nNeuroplasticity is another part of this issue. Scientists conducted experiments demonstrating how different areas of the brain can easily learn to do things for which they\u2019re seemingly not designed. Neurons are the same everywhere, but there are areas in the brain responsible for different things. There\u2019s the Broca area responsible for speech, an area responsible for vision (actually, a lot of areas \u2014 vision is very important for humans), and so forth. Nevertheless, we can break down these notional biological borders.\n\nThis man is learning to see with his tongue. He attaches electrodes to his tongue, puts a camera on his forehead, and the camera streams an image on the electrodes pricking his tongue. People stick that thing on them and walk around with it for a few days, with their eyes open, naturally. The part of the brain that receives signals from the tongue starts to figure out what\u2019s going on \u2014 this feels a lot like something that comes from my eyes. If you abuse somebody like that for a week and then blindfold him, he\u2019ll actually be able to see with his tongue! He is now able recognize simple forms and doesn\u2019t bump into walls.\n\nThe man in this photo has turned into a bat. He\u2019s walking around blindfolded, using an ultrasonic scope whose signals reach his tactile neurons through his skin. With a sonar like this, a human being can develop echolocation abilities within a few days of training. We do not have a special organ that can discern ultrasound, so you have to attach a scope to your body. However, we can relatively easily learn to process this information, meaning that we can walk in the dark and not bump into any walls.\n\nAll of this shows that the brain can adapt to a very large number of different data sources. Hence, the brain probably has a \u201ccommon algorithm\u201d that can extract meaning from whatever it takes in. This common algorithm is the Holy Grail of modern artificial intelligence (a recent popular book on machine learning by Pedro Domingos was called The Master Algorithm). It appears as though deep learning is the closest we have come to the master algorithm of all the things done in the field up until now.\n\nNaturally, one has to be cautious when making claims about whether all of this is like what the brain does. \u201cCould a neuroscientist understand a microprocessor?\u201d, a recent noteworthy article, tries to elucidate how effective current approaches in neurobiology are at analyzing a very simple \u201cbrain\u201d, like a basic Apple I processor or Space Invaders on Atari. We will return to this game soon enough and won\u2019t go into much detail about the results but we do recommend reading the paper. Spoiler alert: modern neurobiology couldn\u2019t figure out a single thing about Space Invaders.\n\nUnstructured information (texts, pictures, music) is processed in the following way: there is raw input, then features that bear content take shape, and then classifiers are built based on those features. The most complicated part of this process is understanding how to pick good features out of unstructured input. Up until recently, systems for processing unstructured information have worked as follows: people have attempted to select good features manually and then assess the quality of relatively simple regressors and classifiers based on these features.\n\nTake Mel Frequency Cepstral Coefficients (MFCC), which had been commonly used as features in speech recognition systems, for example. In 2000, the European Telecommunications Standards Institute defined a standardized MFCC algorithm to be used in mobile phones; all of these algorithms were laid out by hand. Up until a certain point, manually-extracted features dominated machine learning. For instance, SIFT (Scale Invariant Feature Transform), which enables one to detect and describe local features in images based on Gabor filters and the like, was commonly used in computer vision.\n\nOverall, people have come up with many approaches to feature extraction but still cannot duplicate the brain\u2019s incredible success. Moreover, the brain has no biological predetermination, meaning that there are no neurons genetically created only for producing speech, remembering people\u2019s faces, etc. It looks like any area of the brain can learn to do anything. Regardless of the brain\u2019s activity, naturally, we would like to learn to select features automatically to create complex AI and large models containing neurons linked to one another for transmitting signals containing all sorts of different information. Most likely, humans lack the resources necessary to develop the best possible features for images or speech manually.\n\nWhen Frank Rosenblatt introduced his perceptron, everyone started imagining that machines would become truly smart any day now. His network learned to recognize letters on photographs, which was very cool for the late 1950s. Very soon after, neural networks made up of many perceptrons were developed; they could learn with backpropagation (a method used to calculate the gradient descent called the backward propagation of errors). Basically, backpropagation is a method used to calculate gradients or error functions.\n\nThe idea of automatic differentiation was floating around back in the 1960s even, but Geoffrey Hinton, a British-Canadian computer scientist who has been one of the leading researchers on deep learning, rediscovered backpropagation and expanded its scope. Incidentally, George Boole, one of the founders of mathematical logic, was Hinton\u2019s great-great-grandfather.\n\nMulti-layer neural networks were developed in the second half of the 1970s. There weren\u2019t any technical barriers in place at that time. All you had to do was take a network with one layer of neurons, then add a hidden layer of neurons, and then another. That got you a deep network, and, formally speaking, backpropagation works in exactly the same way on it. Later on, researchers started using these networks for speech and image recognition systems. Then recurrent neural networks (RNN), time delay neural networks (TDNN), and others followed; however, by the end of the 1980s it became evident that there were several significant problems with neural network learning.\n\nFirst off, let us touch upon a technical problem. A neural network needs good hardware to learn to act intelligently. In the late eighties and early nineties, research on speech recognition using neural networks looked something like this: tweak a hyperparameter, let the network train for a week, look at the outcome, tweak the hyperparameters, wait another week, rinse, repeat. Of course, these were very romantic times, but since tuning the parameters in neural networks is nearly as important as the architecture itself, too much time or too powerful hardware was needed to achieve a good outcome for each specific task.\n\nAs for the core problem, backpropagation does work formally, but not always in practice. For a long time, researchers weren\u2019t able to efficiently train neural networks with more than two hidden layers due to the vanishing gradients problem: when you compute a gradient with backpropagation, it may decrease exponentially as it progresses from the output to input neurons. The opposite problem \u2014 exploding gradients \u2014 would crop up in recurrent networks; if one starts to unravel a recurrent network, the gradient may spin out of control and start growing exponentially.\n\nEventually, these problems led to the \u201csecond winter\u201d of neural networks, which lasted through the 1990s and early 2000s. As John Denker, a neural networks researcher, wrote in 1994, \u201cneural networks are the second best way of doing just about anything\u201d (the second half of this quote isn\u2019t as well-known:.\u201d\u2026and genetic algorithms are the third\u201d). Nonetheless, a true revolution in machine learning occurred ten years ago. In the mid-2000s, Geoffrey Hinton and his research group discovered a method of training deep neural networks. Initially, they did this for deep belief networks based on Boltzmann machines, and then they extended this approach to traditional neural networks.\n\nWhat was Hinton\u2019s idea? We have a deep network that we want to train. As we know, layers close to the network\u2019s output can learn well using backpropagation. How can we train what\u2019s close to the input, though? At first, we will train the first layer by unsupervised learning. After that, the first layer will already be extracting some features, looking for what the input data points have in common. After doing that, we pre-train the second layer, using results of the first one as inputs, and then the third. Eventually, once we\u2019ve trained all the layers, we\u2019ll use the system as a first approximation and then fine-tune the resulting deep network to our specific task by using backpropagation. This is an excellent approach\u2026 and, of course, it was first introduced back in the seventies and eighties. However, much like regular backpropagation, it worked poorly. Yann LeCun\u2019s team achieved great success in the early 1990s in computer vision with autoencoders, but, generally speaking, their method didn\u2019t work better than solutions based on manually-designed features. In short, Hinton can take credit for making this approach work for deep neural networks (and it would be too long and complicated to explain exactly what he did).\n\nHowever, researchers had sufficient computational capabilities to apply this method by the end of the 2000s. The main technological revolution occurred when Ruslan Salakhutdinov (also advised by Hinton) managed to shift the training of deep networks to GPUs. One can view this training as a large number of relatively independent and relatively undemanding computational processes, which is perfect for the highly parallel GPU architectures, so everything started working much faster. By now, you simply have to use GPUs to train deep learning models efficiently, and for GPU manufacturers like NVIDIA deep learning has become a primary application that carries the same weight as modern games. Take a look at CEO NVIDIA\u2019s pitch here."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/global-technology-leader-david-orban-backs-new-player-neuromation-io-4f7c4c560db5",
        "title": "Global technology leader David Orban backs new player Neuromation.io",
        "text": "He\u2019s one of the key players in global technology with more than two decades of knowledge and experience as a founder, entrepreneur, CEO, keynote speaker and mentor. Now, David Orban has thrown his name behind ICO startup Neuromation.io, joining the team as a strategic advisor.\n\nIt\u2019s a huge coup for the ICO, with David\u2019s credibility giving potential investors peace of mind, knowing they\u2019re putting their money into a company that will be able to execute. David says he took on the role because he was impressed by the Neuromation.io team and their vision. He says, \u201cThere\u2019s currently no general toolset available that can help use synthetic data at scale. Neuromation.io is making a revolution by building a critical component of the budding AI ecosystem.\u201d\n\nHere, we speak with David Orban in greater detail since joining the team.\n\nWhy did you choose Neuromation.io over other possible options to support token sales?\n\nI decided to join Neuromation as an advisor because I believe this company has a world changing solution for artificial intelligence, and I believe are going to make a significant difference on the AI landscape.\n\nHow can your experience guide this token sale and ensure its growth in the global technology landscape?\n\nI have founded and worked with many successful startups over the years, and I can help Neuromation through its initial phases of growth, to become one of the key players on the deep learning field.\n\nI also hold advisory roles at a number of companies, such as Singularity University, where I am also a member of faculty. Singularity University, located in the Silicon Valley is backed by some of the world\u2019s largest and most progressive companies. It has the support of NASA and Google, and provides educational programs, partnerships as well as a startup accelerator to help understanding the power of exponential technologies.\n\nNeuromation.io is a new frontier in deep learning, offering a truly unique concept. I\u2019m really looking forward to assisting the team in building the new world ahead.\n\nWhat are some of the challenges facing neural network startups?\n\nThe tools of neural networks must be trained in all areas. Some companies I have seen don\u2019t have the experience in all areas necessary of deep learning, and they fall short of achieving their goals. Along with this, some of the tools for machine learning are very difficult to explain and can also be very costly to train.\n\nWhat can we expect from Neuromation.io going forward?\n\nNeuromation.io has the backing and support of some very reputable names in the industry. Neuromation.io\u2019s team are all highly qualified professionals who are at the forefront of technology in deep learning. Its platform is not only revolutionary- but also extremely cost effective and user friendly for both private and commercial customers. I\u2019m extremely excited to be onboard and to be in a position to watch this company grow.\n\nHow can other startups use Neuromation as an example?\n\nActually, Neuromation.io has a startup competition that\u2019s about to launch. I am working with them to help strengthen the deep learning community as they provide computer power to solve deep learning issues. While there are currently a number of problems in scaling such a revolutionary approach, Neuromation\u2019s scientists have found an original way to solve the problems that may occur. They use the computational power of a GPU processor-based mining farm, which calculates abstract blockchain algorithms to produce cryptocurrency. What I\u2019m so excited about, is the enthusiasm and way the team at Neuromation.io want to share their mining technology with the entire deep learning community\u2026by creating a protocol utilizing the processing power of mining farms for practical computing. They\u2019ve named this approach \u201cknowledge mining\u201d and are planning to implement their idea to create the largest computing pool for neural networks on the planet. If you\u2019re a startup, I\u2019d highly recommend you look at the competition Neuromation.io is running.\n\nWatch Dave Orban Blockchain Keynote at ConsenSys\u2019 Ethereal Summit to learn more.\n\nTo join David and Neuromation.io in contributing to the growth of the deep learning industry and synergy of the blockchain community, visit our website"
    },
    {
        "url": "https://medium.com/neuromation-io-blog/the-most-interesting-subject-in-the-universe-50310e8850f2",
        "title": "The Most Interesting Subject in the Universe \u2013 Neuromation \u2013",
        "text": "Sergey\u2019s a researcher in the field of machine learning (deep learning, Bayesian methods, natural language processing and more) and analysis of algorithms (network algorithms, competitive analysis). He has authored more than 120 research papers, several books, courses \u201cMachine learning\u201d, \u201cDeep learning\u201d, and others. Extensive experience with industrial projects (Neuromation, SolidOpinion, Surfingbird, Deloitte Analytics Institute).\n\nThis article compares neurons to machines and explores the human brain\u2019s capabilities and limitations.\n\nDespite decades of steady advances, in many fields the human brain is still more capable than computers. For instance, we handle natural language better \u2014 we can read, understand, and parse content from a book. We are pretty good at learning in a broader sense too. So, what does the human brain do and how does it manage to achieve such remarkable results? How do neurons in your brain work differently than transistors in a processor? Naturally, this topic is inexhaustible, but let us try to begin with a few examples.\n\nAs you may know, every neuron occasionally sends electrical impulses, otherwise known as spikes, along axons. Neurons never stop and keep sending signals as long as they\u2019re still alive; however, when neurons are \u201cturned off\u201d they rarely send signals. When neurons are triggered, or \u201cturned on\u201d, spikes occur much more frequently.\n\nNeurons function stochastically, meaning they produce electric signals at random intervals. The patterns of these signals can be pretty accurately approximated with a Poisson process. Computers contain logic gates that send signals back and forth, but their synchronization frequency is fixed and by no means random. This frequency is called a computer\u2019s \u201cclock rate\u201d, which has been measured in gigahertz for quite a while now. On every tick, gates on a certain layer send signals up to the next layer. Although this is done a few billion times a second, it\u2019s performed simultaneously, as though the gate were following a strict order.\n\nActually, it is very easy to see that neurons can synchronize well and count tiny time intervals very precisely. Stereo sound is the simplest and most illustrative example. When you move from one end of the room to the other, you can easily tell, based solely on the sound coming from the television, where you\u2019re going (being able to tell where sound was coming from was crucial for surviving in prehistoric times). You can tell where you\u2019re going by noticing that the sound reaches your left and right ears at different times.\n\nYour inner ears aren\u2019t all that far apart (about 20 cm), and if you divide that by the speed of sound (340 m/s) you get a very short interval \u2014 hundredth of milliseconds \u2014 between when the sound waves reach each ear. Nevertheless, your neurons pick up on this minor difference excellently, which enables you to figure out precisely where you\u2019re headed. In other words, your brain could process frequency \u2014 in kilohertz \u2014 just like a computer. Considering the extensive parallel processing performed by your brain, its architecture could generate rather intelligent computational capabilities\u2026but, for some reason, your brain doesn\u2019t do that.\n\nLet us go back to parallel processing for a second. We recognize people\u2019s faces within a few hundred milliseconds, and connections between different neurons are activated within tens of milliseconds, which means that only a few neurons \u2014 probably fewer than a dozen \u2014 form a serial circuit in the full facial recognition cycle.\n\nOn the one hand, the human brain contains an incredible number of neurons, while, on the other hand, it doesn\u2019t have as many layers as a regular processor. Processors have very long serial circuits, while the brain has short and highly parallel circuits. And while a processor core basically works on one thing at a time (but can switch between different tasks with lightning speed, so it appears to you that everything is working at once), the brain can work on a lot of tasks simultaneously, since neurons light up in many areas of the brain when they start recognizing someone\u2019s face or doing some other equally exciting thing.\n\nThe illustration above shows how the brain processes a visual signal in time. The light reaches the retina, where it transforms into electrical impulses and then the image is transmitted 20\u201340 milliseconds later. The first stage takes 10\u201320 milliseconds (the image shows the cumulative time, i.e. a total of 140\u2013190 milliseconds passes by the time a motor command is issued).\n\nDuring the second stage, 20\u201330 milliseconds later, the signal reaches the neurons that recognize simple visual forms. Then there\u2019s another stage, and another, and only during the fourth stage do we see intermediate forms \u2014 there are neurons that \u201clight up\u201d when seeing squares, color gradients or other similar objects. Then the brain goes through a few more stages, and neurons capable of discerning high level object descriptions light up 100 milliseconds after the process began. For instance, when you meet someone new a neuron responsible for recognizing her face appears (this is a terrible simplification and we can\u2019t verify this claim but it appears that there is some truth to it). Most likely, a neuron or a group of neurons responsible for this person in general and lighting up whenever you come into contact with her, including when you interact with her not face-to-face, appear. If you see her face again (and the neuron didn\u2019t unlearn or forget this earlier information) that same neuron will be activated ~100 milliseconds later.\n\nWhy does the brain work like that? Answering that question with a simple \u201cevolution did it\u201d doesn\u2019t really explain anything. The human brain evolved to a certain point, and that was sufficient to solve problems as we evolved. The rationalist community says that living organisms are not fitness-maximizers who optimize some survival objective function, but rather adaptation executors, who execute \u201crelatively solid\u201d decisions that were chosen randomly at some point. Well, rigid synchronization with a built-in chronometer never took place; however, we can\u2019t tell you exactly why it played out that way.\n\nActually, in this case, it seems as though asking \u201cwhy\u201d isn\u2019t all that relevant. It\u2019s better, more interesting, and more productive to ask \u201chow\u201d. How exactly does the brain work? We don\u2019t know for sure but now we can describe the processes going on inside our heads quite well, at least in terms of individual neurons or, in certain instances, groups of neurons.\n\nWhat can we learn from the brain? First, feature extraction. The brain can learn to make excellent generalizations based on a very, very limited sample size. If you show a young child a table and tell her it\u2019s a table then the child starts calling other tables tables, although they seemingly don\u2019t have anything in common \u2014 they could be round, square, have one leg or four. It\u2019s evident that a child doesn\u2019t learn to do this by supervised learning; she obviously lack the training set necessary to do so. One can assume the child created a cluster of \u201cobjects with legs on which people place things\u201d. Her brain had already extracted the \u201cPlato\u2019s eidos\u201d and then, when she heard the word for it, she simply attached a label to a ready-made idea.\n\nNaturally, this process can go in the opposite direction, too. Although the neurons (and other things) of many linguists start twitching nervously when they hear the names Sapir and Whorf, one must admit that many ideas, especially abstract ones, are mostly socio-cultural constructs. For instance, every culture has a word similar in meaning to the concept of \u201clove\u201d; however, the sentiment may be very different. American \u201clove\u201d has little in common with that of ancient Japan. Since, generally, all people have the same physiological traits, the abstract idea of \u201cbeing drawn towards another person\u201d is not simply labeled in language but rather its adjusted and constructed by the texts and cultural data that define it for a person. But let us return to the main point of the article\u2026to be continued next week."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/knowledge-mining-the-new-frontier-in-deep-learning-19bf6489955",
        "title": "Knowledge mining \u2014 the new frontier in deep learning",
        "text": "We\u2019ve all heard the quote \u201cknowledge is power\u201d, and in today\u2019s world of rapidly growing computing technology and artificial intelligence- those words seem just as relevant, if not more so, than they have ever been.\n\nThe last few years has seen a huge boom in currency mining\u2026 but that has come with some very real challenges, as crypto-miners face pressure from the ever decreasing efficiency of their proof of work computations required to mine ether or other coins. This, along with growing competition and consolidation of miner pools as well as the increasing demand for computing power and resources, will soon prevent many smaller miners from keeping their efforts economically feasible. Scam-related risks are also increasing, with industry experts well aware that currency mining may become a distant memory in the long run.\n\nBut it\u2019s not all doom and gloom, with an economically viable solution available for crypto-currency miners to help them resolve the aforementioned problems. If you haven\u2019t heard about Neuromation.io\u2019s knowledge mining\u2026 then be prepared to be blown away.\n\nAccording to the scientists behind Neuromation.io, their solution has practically no drawbacks, giving miners more options than they have ever had. Here\u2019s how it all works- In addition to a miner\u2019s existing software, they can load up a Neuromation Computation Code. When a Neuromation task is available, each node can bid to participate. If the node wins the bid, it will switch computing power from mining ether or other coin, to a task on the Neuromation blockchain platform. The node will then generate synthetic data or train a deep-learning model. As a reward, the miner will then receive Neuromation.io\u2019s own crypto-currency\u2026 or tokens, as they\u2019re known. Once the task has been completed, the Neuromation.io node will exit and the miner will proceed to mining crypto. For now, the tokens will be extending ether, but will move to Neuromation.io\u2019s own blockchain once the platform economy matures.\n\nNow, nobody would bother mining if it\u2019s not profitable, right? Here\u2019s how the figures stack up. Neuromation\u2019s preliminary estimates, indicate that miners will earn a massive three to five times more by mining Neuromation tokens, versus crypto-currency. Because miners will not be engaged in Neuromation tasks 100 per cent of the time, it basically works as an efficiency boost to their existing setup.\n\nYou\u2019re probably wondering who this technology is available for? Put simply, everyone. Neuromation.io wants to extend this approach to the entire deep-learning community, by creating a protocol which utilizes the procession power of mining farms for practical computing. They\u2019ve aptly named this approach \u201cknowledge mining\u201d, and are planning an ICO to implement the idea of creating the largest computing pool for neural networks on the planet.\n\nNeuromation.io\u2019s mission is to collect the largest distributed computational pool for useful computing, that can be used to solve the fundamental tasks facing the deep-learning community. It would democratize the industry, providing access to projects no matter what budget. These tasks are of practical importance for a whole range of industries and will be implemented through Neuromation.io\u2019s laboratories. Members of the deep learning community would benefit greatly by coming together on the Neuromation.io platform, in order to solve fundamental industrial challenges. Leading industry scientists are also backing the project, with David Orban and Andrei Rabinovich joining the platform as advisors.\n\nTalking about practical tasks here is the video for you to see how Neuromation puts all this computing power to work."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/brian-wang-shares-his-views-on-the-potential-of-machine-learning-using-blockchain-via-neuromation-cc14ee04a66d",
        "title": "Brian Wang shares his views on the potential of machine learning using blockchain via Neuromation\u2026",
        "text": "Neuromation is creating a distributed platform to service all aspects of future synthetic data ecosystem. The platform will allow users to create dataset generators, generate massive datasets, train deep learning models. Users will also be able to trade datasets and models in the platform marketplace.\n\nThey plan to take advantage of the latest trends in blockchain cryptocurrency. All transactions in the Neuromation will be done using their Ethereum extended ECR-20 compliant token. They will engage crypto-currency miners in computationally intensive tasks of data generation and model training. They will be mining our Tokens by performing these tasks instead of mining cryptocurrency.\n\nThe enormous computing capacity that will become available on the platform will be game-changing for wide AI adoption by the Enterprise."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/neuromation-chief-scientist-at-samsung-headquarters-219fe243d5bf",
        "title": "Neuromation Chief Scientist at Samsung headquarters.",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/new-resources-for-deep-learning-with-the-neuromation-platform-55fd411cb440",
        "title": "New Resources for Deep Learning with the Neuromation Platform",
        "text": "Over the last decade, we have witnessed a revolution in machine learning and artificial intelligence: deep learning, the art of designing deep neural networks and the craft of training them efficiently on huge datasets, has completely changed the landscape of machine learning. Every week brings exciting new results and technologies based on deep neural networks: self-driving cars by Tesla and Google, voice assistants with speech recognition by Apple and Google, machine translation by Google, image recognition breakthroughs by Facebook and Google and programs beating world champions in Go by DeepMind, acquired a few years ago by Google. You get the picture.\n\nWhy is it that in a world of budding AI startups (such as Neuromation) full of talented researchers and imaginative entrepreneurs, a world where it has never been easier to train a neural network with millions of weights (often all it takes to get a state of the art model is to pull a repository from GitHub), the best results in so many areas still come from a handful of large companies? Google has the right culture and a lot of the right people, but why is it ahead at everything?\n\nWe believe that a large part of the answer is simply its technological advantage: large companies have more data and more computational power. Yes, you can find a state of the art machine translation model on GitHub or implement it yourself relatively easily \u2014 but where will you find the huge parallel corpus of translations that Google has painstakingly collected over the years of operating Google Translate? Yes, you can download a pre-trained object detection model and fine-tune it on your dataset of images \u2014 but labeling this dataset will take a lot of expensive manual labor, and experiments with state of the art computer vision models on a desktop with 1\u20132 modern GPU\u2019s will take days or weeks of processing time.\n\nHere at Neuromation, we plan to address both of these bottlenecks. As for the data, we are working on many exciting applications of synthetic data with artificially generated labeled datasets that deep learning models can train on. For example, our first large revenue stream is a large contract intended to revolutionize the world of retail: we train object detection models for items on supermarket shelves. Excellent object detection models have already been developed by the deep learning community, and they keep getting better\u2026 but to get them to work you need a labeled dataset. It would cost millions of dollars and probably years of labor to get a labeled dataset sufficiently recognizing the 170,000 different items that appear in just one regional retail catalogue.\n\nTo avoid this manual labor, we develop 3D models of the items that need to be recognized. At a relatively small upfront cost in manual labor (a 3D model of a bottle of Pepsi is quite simple as 3D models go, and then you can reuse the same bottle with dozens of different labels), the outcome is an endless source of perfectly labeled data. Synthetic rendered pictures can have pixel-perfect labeling and even additional data that humans cannot provide at all, like the relative rotation angles of different objects or their depth (z-coordinate) in the picture below. We are continually improving the quality of our renderings, and they already look pretty much like studio photos. Would you be able to tell that this is all synthetic?\n\nNo wonder the neural networks also do not care too much. In our experiments, modern computer vision models train on these synthetic pictures just fine, with excellent capabilities for transfer and generalization to real photos.\n\nSynthetic data is not the answer to everything \u2014 for example, at this point in time you cannot reliably generate parallel corpora for machine translation, although automated data augmentation techniques exist in natural language processing too. But synthetic data is definitely a great fit for computer vision, including secondary applications like training self-driving cars or flying drones.\n\nThe idea of synthetic data blends seamlessly into the second main objective of Neuromation: providing computational resources for everyone. Synthetic data makes it easier on the manual labor and human resources costs, but requires quite a lot of computational resources to generate. And naturally, training modern state of the art deep neural networks with millions of weights has never been easy. Where will the computational power come from?\n\nOur basic idea is simple but powerful. Many AI startups train their models on cloud-based providers like Amazon Web Services; on AWS, an instance with 4\u20136 GPUs suitable for modern deep learning will cost you about $5\u20137 per hour. At the same time, there are huge numbers of GPUs in cryptocurrency mining farms. By mining, e.g., Ether tokens (ETH), the same \u201cfarm\u201d with 4\u20136 GPUs earns for its owner about $5\u20137 per day. See the opportunity here? We plan to bridge this gap.\n\nThe Neuromation platform will become a global unified marketplace for all sorts of computational needs for AI, especially deep learning. We will provide a unified platform of smart contracts for computational power, be it for training deep neural networks, synthetic data generation, or any other needs you may have. A miner will be able to sell the computation time on a GPU farm, getting more money than he could ever get from mining cryptocurrencies\u2026 along with the fuzzy feeling of getting his machines to actually do something useful rather than searching for pointless hash collisions. And an AI practitioner will be able to buy this computational power much cheaper than any cloud-based provider can afford to sell it right now.\n\nAt present, the easiest way to build this global network of trustworthy smart contracts is to introduce a blockchain-based token of exchange. Hence, we have minted the Neurotoken, an Ethereum-based coin for our future platform. We have already reached preliminary agreements with large mining pools, so computational power will be plentiful, and we will be able to provide it for everyone at a much lower cost than centralized cloud-based services \u2014 revolutionizing the market of AI model training.\n\nSynthetic data also fuels this idea. First, synthetic data requires significant computational resources to produce (the rendering you saw above does need a couple of GPU seconds to generate), so it is a new and potentially important source of contracts for the Neuromation platform. Secondly, training on synthetic data simplifies this kind of outsourcing for training. If you train on synthetic data, you don\u2019t have to upload huge datasets to some anonymous mining pool \u2014 you simply upload the code for the model together with a data generator, and the data itself is created remotely. This alleviates the large costs of network transfers that cloud-based providers like Amazon spend millions to reduce.\n\nDo you want to be an early adopter and reap the benefits of getting Neurotokens at the very low initial offering prices? Check out our pre-sale, which starts on October 15th, and be sure to participate in the upcoming Neuromation ICO in November.\n\nSergey Nikolenko is a researcher in the field of machine learning (deep learning, Bayesian methods, natural language processing and more) and analysis of algorithms (network algorithms, competitive analysis), Sergey has authored more than 120 research papers, several books, courses \u201cMachine learning\u201d, \u201cDeep learning\u201d, and others. Extensive experience with industrial projects (Neuromation, SolidOpinion, Surfingbird, Deloitte Analytics Institute)."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/david-orban-became-an-advisor-of-neuromation-io-2d35cad0431b",
        "title": "David Orban became an advisor of Neuromation.io \u2013 Neuromation \u2013",
        "text": "Thrilling news at Neuromation.io. David Orban, founder of Network Society Ventures, joins us as advisor. We will have a great cooperation with David since he has huge experience in advising blockchain startups. Besides our cooperation, together we are planning to develop the blockchain community: we are donating 10% of the ICO proceeds to deep learning projects. David spoke to us about his decision today during #SingularityUSummit in Milan : \u201cI was impressed by the Neuromation team and offered to become an advisor of the project. There is currently no general toolset available that can help use synthetic data at scale. Neuromation is making a revolution by building a critical component of the budding AI ecosystem. They plan to capitalize on the first-mover advantage: the Neuromation Platform provides an exchange and an ecosystem where participants can either contribute or purchase the components of an AI model. I hope that together we can make a significant contribution to the growth of the deep learning industry and synergy of the blockchain community\u201d.\n\nToken pre-sale starts 15th October. Save the date! Registration for the whitelist."
    },
    {
        "url": "https://medium.com/neuromation-io-blog/why-retail-business-needs-image-recognition-technology-today-ce627446cd82",
        "title": "Why Retail Business Needs Image Recognition Technology Today.",
        "text": "It\u2019s the game-changing technology that big businesses simply can\u2019t operate without. In today\u2019s cut-throat global retail marketplace, AI & image recognition technology is paramount.\n\nThis progress though brings its own challenges, such as supply of sufficient talent, resources, and data. The most pressing problem, is the availability of well-labeled data to support the supervised learning process. Even when unstructured data is abundant, converting it to a labeled dataset is an extremely laborious and costly process. There is a need to address the challenges of practical AI adoption wholesale \u2014 on a platform level. Until this is done, most businesses will be struggling to adopt AI. Due to the disparity in resources and focus among companies, we will continue to see a widening gap between early adopters and legacy operators, putting the latter at an extreme strategic disadvantage.\n\nThe latest Euro money country risk survey conducted among 72 of the largest retailers in Europe shows the monthly need for image recognition in the global retail market comprises of at least 1.9 billion photos, with a projected growth of up to 3.5 billion over the next two years.\n\nThe potential demand for image recognition alone in the retail industry is enormous. By 2020, it\u2019s expected that 85 per cent of customer interactions in retail will be managed by artificial intelligence.\n\nUntil recently, the problem for retailers has been a lack of technology for the real-time visual recognition of goods, but that\u2019s all about to change. Joint trade and industry body, Efficient Consumer Response, has partnered with Neuromation.io, creating a revolutionary solution for businesses. Neuromation.io\u2019s solution for retail was presented at ECR\u2019s September forum in Milano, showcasing several products that use the computing power of mining equipment to generate synthetic data to grow traditional retail. This technology is set to be integrated into ECR\u2019s service, giving merchandisers a clear understanding of the presence, absence and layout of goods on the shelf in real-time.\n\nA key advisor and mentor to the Neuromation.io team on object recognition is one of the creators of Google image and Tensor Flow, Andrew Rabinovich. As a world-leading scientist for deep learning and image recognizing research, Andrew has been studying machine learning with an emphasis on computer vision for over 15 years. He\u2019s also the founder of a biotechnology startup and the author of numerous patents and peer-reviewed publications.\n\nThe technology developed by the team at OSA Hybrid platform is already well-established in Eastern Europe and Russia, where it was implemented by ECR into large retail chains & companies such as Metro Cash & Carry, Auchan, Magnit, X5 Retail Group, Dixy Group, Verny, Monet, Rainbow Smile, PepsiCo, Danone, Mars, Coca-Cola, Unilever, L\u2019Or\u00e9al, SunInBev, JTI and Efes. By processing this data in real-time, the algorithms embedded in the platform allow for instant and high-precision detection of not only the absence of goods on the shelf, but to the cause, sending a signal about the problem and recommendations for its resolution to the desired part of the supply chain.\n\nIn Russia, the ECR has gone a step further, using Neuromatiom.io\u2019s technology to create media content. This content is used by participating companies of the ECR Russia ECO System in internal processes, as well as for training neural networks. By the time the product hits the shelf, the OSA HP system in conjunction with image recognition, is already able to recognize the goods and collect all relevant information on the availability of the goods on the shelf\u2026 along with prices, location relative to planograms and control of promotion calculations.\n\nRussia\u2019s ECR Executive Director, Maximilian Musselius says \u201cThe introduction of image recognition technology will increase coverage to 100 per cent of sales points, automate the recognition process, reduce processing time and errors, and extend the technology to the level of the industry standard.\n\nNeuromation.io has also declared the possibility of implementing object recognition on end-user devices (merchandisers) without the need to connect to a server, which will allow rapid scaling of ECR services. Neuromation.io set the target for accuracy of recognition at 95 per cent.\u201d Musselius went on to say that \u201cRetailers will now have an additional opportunity to increase retail turnover by 3.5 per cent per annum, by integrating Image Recognition Neuromation.io technology into the ECR OSA hybrid platform. This will give an additional effect to the already existing 5.4 per cent sales growth when using the OSA HP service.\u201d\n\nThe developer behind the platform, Valentin Ovechkin, says these results are significant because \u201cWe proved that there is a real application of big data and artificial intelligence technologies in offline retail. The service works in real-time and allows the supplier and retailer to optimize costs and boost sales simultaneously.\u201d\n\nThe ECR will implement this technology in the platform already used by its participants, along with companies and members of the ECR community.\n\nThe service is free for the retailer, despite the fact that the retailer gains sales growth.\n\nWith full-scale implementation of the service being rolled out until the middle of 2018, Neuromation.io is inviting potential investors to get onboard now. Exponential growth is projected to continue in the store inventory logistics robot market, providing an opportunity for ripe investment over the coming 15 years.\n\nIn order to transact on Neuromation\u2019s platform, a client will need to buy tokens. To simplify the purchase mechanics, Neuromation will provide a client portal that makes token purchase a one-click process.\n\nNeurotokens for the ICO go on sale on October 15.\n\nFor more information visit the Neuromation.io website"
    }
]