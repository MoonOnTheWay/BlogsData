[
    {
        "url": "https://medium.com/@julsimon/talks-aws-dev-days-munich-bbdeae2d99ed?source=user_profile---------1----------------",
        "title": "Talks: AWS Dev Days Munich \u2013 Julien Simon \u2013",
        "text": "I recently had the pleasure to speak at the AWS Dev Day in Munich. I hope these three talks will help you find your way into Deep Learning and learn how AWS can help you build more agile and more scalable Machine Learning workflows.\n\nHappy to answer questions here or on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/amazon-ai-monthly-april-2018-e5779723c877?source=user_profile---------2----------------",
        "title": "Amazon AI Monthly \u2014 April 2018 \u2013 Julien Simon \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@julsimon/a-quick-look-at-the-swish-activation-function-in-apache-mxnet-1-2-79d9ff9d1673?source=user_profile---------3----------------",
        "title": "A quick look at the Swish activation function in Apache MXNet 1.2",
        "text": "Apache MXNet 1.2 is right around the corner. As hinted at by the change log, this looks like a major release, but in this post, we\u2019ll focus on a new activation function: Swish.\n\nIn deep neural networks, the purpose of the activation function is to introduce non-linearity, i.e. to enforce a non-linear decision threshold on neuron outputs. In a way, we\u2019re trying to mimic \u2014 in a simplistic way, no doubt \u2014 the behavior of biological neurons which either fire or not.\n\nOver time, a number of activation functions have been designed, each new one trying to overcome the shortcomings of its predecessors. For example, the popular Rectified Linear Unit function (aka ReLU) improved on the Sigmoid function by solving the vanishing gradient problem.\n\nOf course, the race for better activation function never stopped. In late 2017, a new function was discovered: Swish.\n\nBy automatically combining different mathematical operators, Prajit Ramachandran, Barret Zoph and Quoc V evaluated the performance of a large number of candidate activation functions (\u201cSearching for Activation Functions\u201d, research paper). One of them, which they named Swish, turned out to be better than others.\n\nAs you can see, if the \u03b2 parameter is small, Swish is close to the linear when the \u03b2 parameter is small and close to ReLU when it\u2019s large. The sweet spot for \u03b2 seems to be between 1 and 2: it creates a non-monotonic \u201cbump\u201d for negative values which seems to have interesting properties (more details in the research paper).\n\nAs highlighted by the authors: \u201csimply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNet-A and 0.6% for Inception-ResNet-v2\u201d.\n\nThis sounds like an easy improvement, doesn\u2019t it? Let\u2019s test it on MXNet!\n\nSwish is available for the Gluon API in MXNet 1.2. It\u2019s defined in incubator-mxnet/python/mxnet/gluon/nn/activations.py and using it in our Gluon code is as easy as: nn.Swish().\n\nIn order to evaluate its performance, we\u2019re going to train two different versions of the VGG16 convolution neural network on CIFAR-10:\n\nThis is pretty straightforward: starting from the master branch, we simply create a vggswish.py file and replace ReLU by Swish, e.g.:\n\nThen, we plug this new set of models into incubator-mxnet/python/mxnet/gluon/model_zoo/__init__.py and voila!\n\nHere\u2019s the full diff if you\u2019re interested. There\u2019s really not much to it.\n\nMXNet contains an image classification script which lets us train with a variety of network architectures and data sets (incubator-mxnet/example/gluon/image_classification.py). Exactly what we need!\n\nWe\u2019ll use SGD with epochs steps, dividing the learning rate by 10 each time.\n\nHere are the results.\n\nIt\u2019s always difficult to draw conclusions from a single example (although I did run a bunch of different trainings with consistent results). Here, we can see than the Swish version seems to train faster and validate as well as the ReLU version.\n\nTop validation accuracies are respectively 0.866186 at epoch #14 and 0.866001 at epoch #19. A minimal difference, but then again VGG16 isn\u2019t a very deep network.\n\nSo now we have another activation function in our arsenal. It\u2019s still quite new, so we need to experiment and learn when to use it or not. In case you\u2019re curious, MXNet 1.2 also adds support for the ELU and SELU functions, so why not read about these as well? A Deep Learning engineer\u2019s day is never done :)\n\nThat\u2019s it for today. Thank you for reading. Happy to read your feedback here or on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033?source=user_profile---------4----------------",
        "title": "Using Chalice to serve SageMaker predictions \u2013 Julien Simon \u2013",
        "text": "Amazon SageMaker makes it easy to train and deploy Machine Learning models hosted on HTTP endpoints. However, in most cases you won\u2019t expose these endpoints directly. Pre-processing and post-processing steps are likely to be required: authentication, throttling, data transformation and enrichment, logging, etc.\n\nIn this post, we will use AWS Chalice to build a web service acting as a front-end for a SageMaker endpoint.\n\nI\u2019ve already written a couple of posts (here and here) on training and deploying SageMaker models, so I won\u2019t go into these details again. For our purpose, we\u2019ll use the built-in algorithm for image classification to train a model on the Caltech-256 data set, as presented in this tutorial.\n\nOnce training and deployment are complete, the endpoint is ready for prediction.\n\nFirst, we need to figure out what data format the algo expects: as explained in the documentation, we need to post binary data with the application/x-image content type.\n\nWe\u2019ll do this with the InvokeEndpoint API, which you\u2019ll find it in your favourite AWS SDK: we simply need to provide an endpoint name and of course the body.\n\nHere\u2019s an example with boto3.\n\nOK, this worked. The response contains 257 probabilities, one for each of the CalTech-256 categories (256 object categories plus a catch-all category). As our image represents a floppy disk (category #75), the highest probability is the 74th one. You can check if you feel like it :)\n\nNow, it\u2019s time to write a front-end web service for this endpoint.\n\nChalice is an AWS Open Source project that lets developers build Python-based web services with minimal fuss. The programming model is extremely close to Flask, so you should feel right at home. Installation is a breeze (pip install chalice) and the CLI is idiot-proof (*I* can use it).\n\nSimple services require very little configuration, if any. Based on the boto3 calls in your code, Chalice generates an IAM policy which will be attached to the Lambda function\u2019s role. You\u2019ll generally want to tweak it a bit, but it\u2019s a nice starting point and a great time saver. You can also bring your own policy (which we\u2019ll do later on).\n\nYou can add Python dependencies by listing them in the requirements.txt file. Just remember that Lambda functions can\u2019t exceed a zipped size of 50MB, so you should be conservative!\n\nWhen it comes to deployment (\u2018chalice deploy\u2019), Chalice automatically creates a Lambda function running your code, as well as an API in the API Gateway to trigger it. If you\u2019d rather deploy with SAM, that\u2019s possible too: \u2018chalice package\u2019 will build both the deployment package and the SAM template.\n\nLast but not least, you can run your service locally (\u2018chalice local\u2019) which obviously helps debugging.. and allows you to code on planes ;)\n\nAlright, let\u2019s get to it.\n\nAs we just saw, the endpoint returns a raw prediction, which contains probably more information than we need to send back to the client. Thus, our service will only return the top k classes and probabilities, in descending order of probability.\n\nThe body of our POST request will contain:\n\nFor more flexibility, let\u2019s store the SageMaker endpoint name in an environment variable for the Lambda function.\n\nHere are the steps we need to take:\n\nPretty straightforward, I think. Most of the work is actually to convert response data into something that we process. Indeed, the response body is a byte array representing a bracketed array of comma-separated probabilities, which is inconvenient to work with.\n\nIn order to turn it into a proper Python array, we can evaluate it as a Python expression with ast.literal_eval(). And voila: a Python array! Nice trick.\n\nWe can then build a Numpy array holding the category indexes in ascending order of probability, reverse the array and take the first top k elements. The last step is to build the response body.\n\nPfeew. That was a little more Python plumbing than I originally expected, but it\u2019s actually a good example of post-processing raw predictions.\n\nConfiguration is pretty simple and is stored in .chalice/config.json:\n\nUsing \u2018*\u2019 as a resource selector for InvokeEndpoint is ok here. However, in production, I would strongly recommend using the actual endpoint ARN instead.\n\nLet\u2019s test the service locally by running \u2018chalice local\u2019 and then curl to invoke it.\n\nNice. Our service seems to work :)\n\nNow it\u2019s time to deploy on AWS by running \u2018chalice deploy\u2019. Let\u2019s run the same test.\n\nAll good. That\u2019s it, then: we successfully built a serverless microservice invoking a SageMaker endpoint (code and test script on Github).\n\nThis is pretty fun, so why not write another service?\n\nComputer vision models require input images to have the same size as training images. I believe the SageMaker built-in algorithm handles image resizing for us, but it\u2019s definitely something we\u2019d have to take care of if we worked with your own custom model.\n\nHere\u2019s how you could do this with the OpenCV library (code and test script on Github)."
    },
    {
        "url": "https://medium.com/@julsimon/aws-summit-san-francisco-ai-ml-recap-be6389880514?source=user_profile---------5----------------",
        "title": "AWS Summit San Francisco: AI/ML recap \u2013 Julien Simon \u2013",
        "text": "Outside of re:Invent, the San Francisco summit is one of the bigger events when it comes to announcing AWS new services.\n\nThe tradition was upheld again this year. Let\u2019s take a quick look at the different AI/ML announcements.\n\nAvailable in preview at re:Invent 2017, Amazon Translate is now generally available in the US East (N. Virginia), US East (Ohio), US West (Oregon), and EU (Ireland) regions.\n\nWe also announced that new languages would be supported in the coming months: Japanese, Russian, Italian, Traditional Chinese, Turkish, and Czech.\n\nAvailable in preview at re:Invent 2017, Amazon Transcribe is also generally available in the US East (N. Virginia), US East (Ohio), US West (Oregon), and EU (Ireland) regions.\n\nThe TensorFlow and Apache MXNet containers using for training and prediction by Amazon SageMaker have been open-sourced!\n\nThis is great news for all SageMaker users, who are now able to:\n\nNew notebook instance sizes have been introduced. You can now go as big as ml.m4.16xlarge and ml.p3.16xlarge, which will help you run heavier data pre-processing on the notebook instance itself.\n\nAvailable in preview at re:Invent 2017, AWS Greengrass ML Inference is now generally available, in the same regions as AWS Greengrass: US East (N. Virginia), US West (Oregon), EU (Frankfurt) APAC (Tokyo) and APAC (Sydney) regions.\n\nThe latest version of the MXNet Model Server (v0.3) now lets you use pre-built CPU and GPU containers stored on the Docker Hub.\n\nHappy to answer any questions. Feel free to reach out on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/webinar-how-machine-learning-on-aws-can-transform-your-education-business-c80de84f8c21?source=user_profile---------6----------------",
        "title": "Webinar: How Machine Learning on AWS Can Transform Your Education Business",
        "text": "Machine Learning is a hot topic in the education industry. In this webinar, you will learn how EdTechs and universities are using ML on AWS to improve the learning outcome.\n\nGoing through many use cases, we\u2019ll introduce you to the growing family of API-driven ML services that provide organizations with everything they need to innovate and build transformative learning solutions for the education marketplace."
    },
    {
        "url": "https://medium.com/@julsimon/webinar-building-your-smart-applications-with-amazon-ai-4f8e10375bad?source=user_profile---------7----------------",
        "title": "Webinar: Building Your Smart Applications with Amazon AI",
        "text": "In this webinar, you will learn how to easily add Amazon AI services to your own applications. I\u2019l show you how to access image and video analysis, text to speech, speech to text, translation, natural language processing: all of which are just an API call away.\n\n \n\n Through code-level demos of Amazon SageMaker, Amazon Translate, Amazon Polly, Amazon Transcribe, Amazon Comprehend, Amazon Rekognition, you\u2019ll see how to quickly get started with these services, with zero AI expertise required."
    },
    {
        "url": "https://medium.com/@julsimon/talk-deep-learning-at-the-edge-and-aws-deeplens-3a46430a9d33?source=user_profile---------8----------------",
        "title": "Talk: Deep Learning at the Edge and AWS DeepLens \u2013 Julien Simon \u2013",
        "text": "I had to pleasure to give this talk at Papis.io London today.\n\nAWS DeepLens is a wireless video camera and API that lets you get hands-on experience with Deep Learning and develop your own computer vision applications. In this talk, we\u2019ll look under the hood of Deep Lens and discuss its different building blocks: Intel hardware, Intel Inference Engine, Apache MXNet, AWS Greengrass and Amazon SageMaker.\n\nYou can view the video below. Demo included, of course!"
    },
    {
        "url": "https://medium.com/@julsimon/aws-tel-aviv-summit-2018-speed-up-your-machine-learning-workflows-with-built-in-algorithms-aacada0d83d8?source=user_profile---------9----------------",
        "title": "Talk: Speed up Your Machine Learning Workflows with Built-In Algorithms",
        "text": "I recently had the pleasure to speak at the AWS Tel Aviv Summit. In this session, I\u2019m focusing on the built-in algorithms available in Amazon Sagemaker, which I write about previously (time series with DeepAR, movie recommendation with Factorization Machines).\n\nYou\u2019ll learn why these algorithms scale to extremely large datasets and I\u2019ll show you some pretty impressive benchmarks as well."
    },
    {
        "url": "https://medium.com/@julsimon/aws-tel-aviv-summit-2018-machine-learning-state-of-the-union-d97d190271b1?source=user_profile---------10----------------",
        "title": "Talk: Machine Learning State of the Union \u2013 Julien Simon \u2013",
        "text": "I recently had the pleasure to speak at the AWS Tel Aviv Summit. This session will give you an up to date overview of Machine Learning services available on AWS and of how our customers use them.\n\nHappy to answer any question that you may have!"
    },
    {
        "url": "https://medium.com/@julsimon/amazon-ai-monthly-march-2018-def51be77ba1?source=user_profile---------11----------------",
        "title": "Amazon AI Monthly \u2014 March 2018 \u2013 Julien Simon \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@julsimon/webinar-enabling-deep-learning-in-iot-applications-with-apache-mxnet-4361e2a6726c?source=user_profile---------12----------------",
        "title": "Webinar: Enabling Deep Learning in IoT applications with Apache MXNet",
        "text": "A few months ago, I wrote a post explaining why Apache MXNet is a good choice for Deep Learning at the Edge.\n\nDays ago, I also recorded a webinar where I discuss the different options available to developers, using services such as Apache MXNet, Deep Learning AMI, Amazon SageMaker, AWS IoT, AWS Greengrass (ML), AWS DeepLens."
    },
    {
        "url": "https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-2-bed3be4761d3?source=user_profile---------13----------------",
        "title": "Tumbling down the SGD rabbit hole \u2014 part 2 \u2013 Julien Simon \u2013",
        "text": "In the first part of this post, we studied the Stochastic Gradient Descent optimizer and discussed five problems that could hamper training of Deep Learning models:\n\nIn this post, we\u2019ll discuss solutions to these problems and how to apply them.\n\nThe problem posed by local minima in deep neural networks has been debated for a long time. However, intuition tells us that they should be rare. Indeed, the number of parameters in such networks is very large (millions at least): for a point to be a local minimum, all dimensions should have a positive slope. The probability of this happening would be the inverse of 2 to the power of the number of dimension. The inverse of 2^1,000,000? Of 2^10,000,000? Pretty slim chance\u2026\n\nBased on this, one could also conclude that using larger-than-needed models would actually be a good idea: more parameters would mean lower probability for local minima, right? Indeed, a 2015 paper by Itay Safran and Ohad Shamir (\u201cOn the Quality of the Initial Basin in Overspecified Neural Networks\u201d) shows that this is the case: \u201chigher dimensions also means more potential directions of descent, so perhaps the gradient descent procedures used in practice are more unlikely to get stuck in poor local minima and plateaus\u201d.\n\nAnother paper published in 2014 paper by Ian J. Goodfellow, Oriol Vinyals and Andrew M. Sax (\u201cQualitatively characterizing neural network optimization problems\u201d) concludes empirically that local minima are not a problem when training deep neural networks. Yes, they may exist but in practice they\u2019re hardly ever encountered during SGD. Even when they are, it does just fine escaping them, thank you.\n\nWhen all is said and done, please remember one thing: the purpose of the training process is not to find *the* global mimimum \u2014 this is a NP-hard problem, so forget about it. It is to find *a* minimum that generalizes well enough, yielding a test accuracy compatible with our business problem.\n\nThese problems are related: for very high dimension problems such as deep neural networks, it will take a long time to reach an acceptable minimum if you use a small fixed learning rate.\n\nOver the years, a number of improvements were designed to speed up SGD (Robbins and Monro, 1951). Techniques like Momentum (Polyak, 1964) and Nesterov Accelerated Gradient (Nesterov, 1983) were designed to accelerate progress in the direction of steepest descent.\n\nAs Deep Learning gained popularity and as networks grow larger, researchers realized that some dimensions had steep slopes while some exhibited vast plateaus. Thus, they worked on adapt to these different conditions by not only modifying the learning rate but also by using different learning rates for different dimensions.\n\nAdaGrad (2011, PDF) came first. For a given parameter, it increases the learning rate if it receives small updates (i.e. speed up on plateaus) and decrease it if it receives larges updates (i.e. slow down on steep slopes). This is achieved by dividing the learning rate by the sum of all squared past gradient updates (aka the l2 norm): larger gradient updates will reduce the learning rate while tiny gradient updates will increase it.\n\nAdaGrad has a problem, however: as the sum grows monotonically, the learning rates will end up converging to zero for large models and long trainings, effectively preventing any further progress.\n\nRMSProp (2012) and AdaDelta (2012) solve this problem by decaying the accumulated sum before adding the new gradient. This prevents the sum from exploding and the learning rates from going to zero.\n\nAdam (2015) adds momentum to AdaDelta, further focusing progress in the direction of steepest descent.\n\nHere\u2019s a great visualization. The adaptive optimizers race to the minimum, momentum and NAG go for a walk in the park before heading out to the right spot\u2026 and Grand Pa SGD gets there too but really slowly.\n\nAll these optimizers are available in your favorite Deep Learning library. Adam is a popular choice as it seems to work well in most situations. Does it mean that it can\u2019t be improved? Of course not: research never sleeps and this nice post by Sebastian Ruder lists the latest developments.\n\nOne of these developments is surprising. Some researchers now question that adaptive optimizers are the best choice for deep neural networks, as illustrated by \u201cThe Marginal Value of Adaptive Gradient Methods in Machine Learning\u201d (Ashia Wilson et al., 2017).\n\nThis paper show that well-tuned SGD with learning rate decay at specific epochs ends up outperforming all adaptive optimizers: \u201cDespite the fact that our experimental evidence demonstrates that adaptive methods are not advantageous for machine learning, the Adam algorithm remains incredibly popular. We are not sure exactly as to why, but hope that our step-size tuning suggestions make it easier for practitioners to use standard stochastic gradient methods in their research\u201d.\n\nNow, of course, figuring out the learning rate decay schedule is another complex problem in itself, so I wouldn\u2019t bury Adam just yet :)\n\nSo far, we\u2019ve only studied SGD and its variants. Of course, there are other techniques out there. One of them is the Follow the Moving Leader algorithm aka FTML (\u201cFollow the Moving Leader in Deep Learning\u201d by Shuai Zheng and James T. Kwok, 2017, PDF).\n\nI won\u2019t go into details in this already long post, but let\u2019s take a quick look at some of the results: learning CIFAR-10 with LeNet and ResNet-110.\n\nIs there a new sheriff in Optimizer City? You\u2019ll find more results in the research paper (tl;dr: FTML matches or surpasses Adam on CIFAR-100 and on LSTM tasks).\n\nFTML is available in Apache MXNet 1.1. Using it is as simple as:\n\nIn 2015, Rong Ge et al. proposed a noisy version of SGD (\u201cEscaping From Saddle Points \u2014 Online Stochastic Gradient for Tensor Decomposition\u201d): when updating parameters, adding a tiny amount of noise on top of the gradient is enough to nudge SGD to a slightly lower point instead of getting stuck at the saddle point. Very cool idea :)\n\nIn 2016, Lee et al. (\u201cGradient Descent Converges to Minimizers\u201d) showed that even without adding noise, random initialization of parameters lets SGD evade first-order saddle points: \u201cgradient descent with a random initialization and sufficiently small constant step size converges to a local minimizer or negative infinity almost surely\u201d.\n\nAll Deep Learning libraries will let you do this. For example, you could use either of these in Apache MXNet:\n\nNoisy SGD and random initialization save us from computing the Hessian, which is a costly operation slowing down the training process. Still, in the previous post, I gave you an example of a third-order saddle point defeating the Hessian. Is there hope for these?\n\nIn 2016, Anima Anandkumar (a Principal Scientist at AWS) and Rong Ge proposed the first method to solve third-order saddle points: \u201cEfficient approaches for escaping higher order saddle points in non-convex optimization\u201d. I\u2019m not aware of any Deep Learning library implementing this technique, but please feel free to get in touch if there\u2019s one :)\n\nLast month, Jeremy Bernstein et al. proposed two variant of SGD called SignSGD and Signum (adding momentum) : \u201cSIGNSGD: compressed optimisation for non-convex problems\u201d. They solve the gradient size problem by sending only the sign of the gradient, not its 32-bit value! Thus, this reduces the amount of data to send by a factor of 32. The network says thanks ;)\n\nAmazingly, these algorithms converge at the same rate or even faster than Adam, only losing out to highly-tuned SGD (hmmm, again). Here are some results:\n\nSignSGD is available in Apache MXNet 1.1. Using it is as simple as:\n\nWe covered a lot of ground again. Let\u2019s try to sum things up.\n\nThat\u2019s it for today. I hope that you learned a lot and that these techniques will help you build better models.\n\nAs always, thanks for reading! Please feel free to reach out on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/tumbling-down-the-sgd-rabbit-hole-part-1-740fa402f0d7?source=user_profile---------14----------------",
        "title": "Tumbling down the SGD rabbit hole \u2014 part 1 \u2013 Julien Simon \u2013",
        "text": "As could be feared, my Deep Learning excursions have led me in increasingly strange places. Lately, I\u2019ve become quite obsessed with optimizers, maybe the single most important factor in successfully training a Deep Learning model.\n\nThis post assumes a basic knowledge of Deep Learning (if you\u2019re new to the topic, this previous post is probably a better one to read).\n\nWe shall start with a basic explanation of Stochatic Gradient Descent, the most commonly used optimization function used in Deep Learning. And then, we\u2019ll fall into darkness :D\n\nThat\u2019s it. Zooming in on the last item, what we\u2019re really trying to achieve is find a set of parameters for which the loss function (which computes the difference between ground truth and predicted values) reaches a small enough value.\n\nThis set of parameters cannot be computed algebraically: it has to be approximated iteratively using an optimization function.\n\nStochastic Gradient Descent was invented in 1951 by Robbins and Monro. It\u2019s still very widely used today. All it requires is the ability to derive a function, plus a single hyper-parameter called the learning rate (noted \u2018lr\u2019 below: 0.1 is a typical value).\n\nHere\u2019s the SGD update rule when the function has a single parameter called \u2018w\u2019: w = w-(lr*f\u2019(w))\n\nIn plain-speak: we compute the function derivative at our present location. This tells us what the slope is and which way is down. Accordingly, we then take a small step down and repeat until we get to the minimum value for f().\n\nAn example is the best way to explain it :)\n\nLet\u2019s apply this to a very simple function : f(x) = x\u00b2. I hope that you\u2019ll remember from high-school that its derivative is 2x ;)\n\nAt x=1 (our starting point for this example), the derivative is equal to 2. The slope is positive, which means that if we have to decrease x to get closer to the minimum (If the slope was negative, we\u2019d increase x to get closer to the minimum)\n\nFor x=0.5, the derivative is equal to 1. Let\u2019s run another round of SGD:\n\nx = x-(0.25*1) =0.25.\n\nAfter a few iterations, this is what we get.\n\nAs you can see, we gradually get closer and closer to the function\u2019s minimum, i.e. x=0. SGD works, woohoo.\n\nIn a deep neural networks, each weight is a parameter. As a consequence, the network\u2019s function has as many dimensions as the network has weights\u2026 which could be millions and even tens of millions. Mind-boggling.\n\nStill, SGD works the same. However, for additional performance, we don\u2019t want to update weights one at a time: during the training phase, backpropagation should instead update all the weights in a given layer at once (starting from the last layer and moving on to the previous one until it reaches the input layer).\n\nTo do so, let\u2019s define a vector storing the weights for this layer: \n\nL = [w1, w2, \u2026]\n\nSimilarly, let\u2019s define a vector storing the partial derivatives of our function. This vector is called the gradient (yes, that\u2019s the \u2018G\u2019 in \u2018SGD): it informs us about the slope in each dimension.\n\nFortunately, Deep Learning libraries like Apache MXNet or TensorFlow know how to compute partial derivatives automatically, so we\u2019ll never have to do that ourselves. Pfeew.\n\nThe SGD update now becomes: L = L-(lr*grad(f)(L))\n\nHere\u2019s a simulated example with two parameters, where we literally \u201cwalk down the mountain\u201d step by step to reach a low point in the \u201cvalley\u201d.\n\nSee? It\u2019s not that complicated: you\u2019ve now conquered SGD. No matter the number of dimensions, we have a simple iterative algorithm that lets us find the smallest value possible for our function.\n\nDoes this really work every single time? Well\u2026 no :)\n\nA function may exhibit many local minima. Look at the weird beast below.\n\nLots of very shallow local minima, some deeper ones and a global one. As we iterate on SGD, we\u2019d definitely want to avoid getting stuck in any of the shallow ones \u2014 this could happen if we used a really small learning rate, preventing us from crawling out. Ideally, we\u2019d like to end up that global minimum, but we could end up falling into a deep local one.\n\nIs this a theoretical problem? Do these situations actually take place with deep neural networks? If so, would there be any way to catapult ourselves out of these local minima?\n\nImagine a part of the parameter space where the slope would be close to zero in every dimension. Let\u2019s call this a plateau, even if the word doesn\u2019t really make sense in high-dimension spaces.\n\nThere, all components of the gradient would be close to zero, right? Hardly any slope. The consequence would be near-zero updates for all weights, which in turn would mean that we would hardly move towards the minimum. We\u2019d be stuck on that plateau for a long time and training would be extremely slow, no matter how much hardware we\u2019d throw at it. Definitely an undesirable situation (unless we\u2019d reached a nice minimum, of course).\n\nCould we speed up, i.e. increase the learning rate when slope is minimal?\n\nShould we really expect all dimensions to have roughly the same slope? Or could there be steep dimensions \u2014 where we\u2019d make good progress quickly \u2014 and flatter dimensions \u2014 where we\u2019d move much slower?\n\nSurely that\u2019s a reasonable hypothesis. As SGD uses the same learning rate for all parameters, this would surely cause uneven progress and slow down the training process.\n\nCould we have adapt the learning rate to the slope of each dimension?\n\nNow, imagine we\u2019d reach a specific point in the parameter space where all components of the gradient are actually equal to zero (yes, these points actually exist, more on this in a minute). What would happen then? No more weight updates! We\u2019d be stuck there for all eternity. Doom on us.\n\nLet\u2019s look at an example: f(x,y) = x\u00b2- y\u00b2. This does look like a horse saddle, doesn\u2019t it?\n\nNow, let\u2019s compute the partial derivatives: df/dx = 2x, df/dy = -2y. Hence, the gradient of this function is [2x, -2y]. Obviously, at the point (0,0), both components of the gradient are equal to zero.\n\nLooking at the graph above, indeed we see that this point is a minimum along the x axis and a maximum along the y axis. Such points are called saddle points: a minimum or a maximum for every dimension.\n\nShould SGD lead us to this exact place, our training process would be toast as weights would not be updated any longer. Our model would probably be no good either since we\u2019d definitely not have reached a minimum value.\n\nSaddle points have been proven to be very common in Deep Learning optimization problems (we\u2019ll see in the next post how to try and limit their occurence).For now, let\u2019s see if we can break out!\n\nOur problem here is that we\u2019re only looking at unidirectional slopes. Instead, if we looked at the curvature around the saddle point (i.e. how much the surface deviates from a plane), then maybe we\u2019d figure out that there\u2019s actually a way down along th y axis.\n\nStudying the curvature of a function requires the computation of second-order partial derivatives (ah come on, don\u2019t quit on me now. Hang on!). They\u2019re stored in a square matrix called the Hessian.\n\nLet\u2019s do this for our previous example:\n\nThus, the Hessian for our function is:\n\nBy multiplying this matrix with unit vectors along the x and y axes, we\u2019re going to find out what the curvature looks like:\n\nWhat do we see here? Multiplying H by a unit vector along the x axis yields a positive multiple of the vector: curvature is positive, we can only go up. Indeed, (0,0) is a minimum along the x axis.\n\nOn the contrary, multiplying H by a unit vector along the y axis yields a negative multiple of the vector. This indicates a negative curvature, which means that there is a way down. Victory!\n\nNow we know how to break out of saddle points. The only problem is that computing the Hessian is quite expensive. There are other ways to avoid saddle points, albeit less rigorous ones (more on this in the next post).\n\nAre we out of the woods when it comes to saddle points? Far from it, I\u2019m afraid.\n\nHere\u2019s an example, called the monkey saddle: f(x,y) = x\u00b3- 3xy\u00b2.\n\nLet\u2019s compute the gradient and the Hessian again.\n\nThe gradient is [3x\u00b2- 3y\u00b2, -6xy], which is equal to [0, 0] at point (0,0). This is a saddle point again.\n\nAccordingly, the Hessian is:\n\nH(0,0) is a zero matrix! Multiplying it by a unit vector (or any vector, for that matter) will result in a zero vector: we\u2019re unable to figure out curvature. Neither the gradient nor the Hessian provide any information on which way is down: such points are degenerate saddle points. Oh, the agony\u2026\n\nShould we lose all hope? Are there more advanced techniques to detect these conditions?\n\nThis last problem is different from the previous ones. Imagine that we\u2019re working with a very large data set: distributing training \u2014 splitting computation across multiple instances \u2014 would surely deliver a nice speed up.\n\nThe way this typically works is quite straightforward. Each instance in the training cluster grabs a batch of samples, processes it (forward propagation to compute the loss for the batch, then backprogation to compute gradients for all layers). Then, each instance would send the computed gradients for this batch to all other instances (or to a central location in charge of propagating them). Each instance would then update their weights accordingly.\n\nAll good, right? Not quite. For large models, gradients are actually very heavy: close to 100MB for ResNet-50, a popular image classification model. This means that each instance would have to send 100MB to all other instances after each round of backpropagation! Even with a limited number of instances, this could quickly become a performance bottleneck, stalling our computing efforts and slowing down the training process.\n\nCould we reduce the gradient size? Compress this data, maybe?\n\nIn this post, we looked at the mathematical foundation of SGD. We learned about mathematical tools that help us find which way is down in the quest for a nice minimum. This was a bit math-heavy, but hopefully you made it to the end. Remember, you can do this! Stephen Hawking passed away yesterday and he once said:\n\nI feel that this is great advice when trying to understand Deep Learning.\n\nIn the next post, we\u2019ll look at solutions for the issues discussed above\u2026and how to apply them in practice.\n\nThank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/amazon-ai-monthly-february-2018-46345ccec0f4?source=user_profile---------15----------------",
        "title": "Amazon AI Monthly \u2014 February 2018 \u2013 Julien Simon \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@julsimon/yet-another-10-deep-learning-projects-based-on-apache-mxnet-da09f30cca38?source=user_profile---------16----------------",
        "title": "Yet another 10 Deep Learning projects based on Apache MXNet",
        "text": "In previous articles, I listed 10 Deep Learning projects based on Apache MXNet\u2026. and then 10 more\u2026 and what do you know, here is another batch!\n\nThis is an implementation of the architecture described on the self-titled paper by Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan and Jiashi Feng.\n\nQuoting from the paper: \u201cOn the ImageNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed\u201d.\n\nThis is an implementation of the architecture described on the self-titled paper by Jie Hu, Li Shen and Gang Sun.\n\nThis project implements the CapsNet architecture presented in the \u201cDynamic Routing Between Capsules\u201d paper by Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. In a nutshell, capsule networks are an exciting new development designed to overcome the limitations of convolutional neural networks.\n\nThis code achieves 99.71% accuracy on the MNIST dataset, which is in line with the scores reported in the paper.\n\nThis project also implements the CapsNet architecture, but it does so using the imperative Gluon API (here\u2019s an introduction to Gluon if you\u2019re not familiar with it).\n\nThis implementation achieves 99.53% accuracy on MNIST, which the author suggests could be improved by adding more data augmentation.\n\nThis is an implementation of the architecture described in \u201cMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\u201d by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto and Hartwig Adam.\n\nQuoting from the paper: MobileNets are \u201ca class of efficient models (\u2026) for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks\u201d.\n\nA model pre-trained on ImageNet is provided, with a top-5 accuracy of 90.15%.\n\nThis is an implementation of the architecture described in \u201cArcFace: Additive Angular Margin Loss for Deep Face Recognition\u201d by Jiankang Deng, Jia Guo, and Stefanos Zafeiriou.\n\nInsightFace is a new face recognition method, which achieves state-of-the art scores of 99.80%+ on LFW and 98%+ on Megaface.\n\nThis is an implementation of the architecture described in \u201cDeep Speech 2: End-to-End Speech Recognition in English and Mandarin\u201d by Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan and Zhenyao Zhu (pfew!).\n\nThis is a great project if you want to build a speech-to-text model. Please bear in mind that you will need a very large dataset. Quoting from the original paper: \u201cour English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours. We use data synthesis to further augment the data during training\u201d.\n\nThis is an implementation of the architecture described in \u201cEnd-to-end 3D face reconstruction with deep neural networks\u201d by Pengfei Dou, Shishir K. Shah and Ioannis A. Kakadiaris.\n\nThanks to this project, you can build a 3D model of a a face using only a single 2D image. Quite impressive!\n\nDeepo is a set of pre-built containers for Deep Learning. It supports MXNet as well as other frameworks. Containers will run on Linux (CPU/GPU), Windows (CPU) and MacOS (CPU) with either Python 2.7 or Python 3.6.\n\nThis is pretty handy if you want to work locally, and of course on AWS with one of our Docker services: ECS, EKS or Fargate.\n\nThis tool simplifies the process of fine-tuning an image classification dataset on your own dataset (here\u2019s an introduction to fine-tuning if you\u2019re unfamiliar with this technique).\n\nIt wil automatically build RecordIO files from a tree of images, download pre-trained models, replace the last layer according to the number of classes in your dataset, add data augmentation, run fine-tuning, visualize results, etc."
    },
    {
        "url": "https://medium.com/@julsimon/classifying-fashion-mnist-with-gluon-8d14fce03b9?source=user_profile---------17----------------",
        "title": "Classifying Fashion-MNIST with Gluon \u2013 Julien Simon \u2013",
        "text": "In a previous post, we took a first look at the Gluon API, a high-level API for built on top of Apache MXNet.\n\nIn this post, we\u2019ll keep exploring Gluon but first we need a cool dataset to work with.\n\nPut together by e-tailer Zalando, Fashion-MNIST is a drop-in replacement for the well-known (and probably over-used) MNIST dataset: same number of samples, same number of classes and same filenames! You\u2019ll find plenty of details in this technical report.\n\nInstead of digits, this data set contains the following fashion items: t-shirt/top, trouser, pullover, dress, coat, sandal, shirt, sneaker, bag and ankle boot. Some items look very similar, which is likely to make our classification job harder.\n\nThanks to the Gluon vision API, it couldn\u2019t be simpler to load the training set and the validation set. Each sample is a 28x28 greyscale image shaped (28, 28, 1). We\u2019ll use a simple transform function to reshape it to (1,28,28).\n\nWe\u2019re going to try out a number of network architectures, so let\u2019s write a function that lets us build a variety of CNNs from the following Gluon layers:\n\nBy default, we\u2019ll use the ReLU activation function but let\u2019s also plan for Leaky ReLU, which is a separate layer in Gluon.\n\nThanks to this function, we can now build a variety of CNNs with one single line of code. Here\u2019s a simple example.\n\nThe flexibility of the Gluon API is really great here. I must admit that this would have been more work with the symbolic API in MXNet :)\n\nWe have to initialize weights, pick an optimizer and set its parameters. Nothing unusual. Let\u2019s settle for Xavier initialization, but feel free to try something else.\n\nDuring training, we\u2019d like to measure training and validation accuracy. Let\u2019s use an MXNet metric to compute them.\n\nOur training loop is the standard Gluon training loop:\n\nIn the process, we\u2019re also computing accuracies and storing their values for plotting purposes.\n\nOnce training is complete, let\u2019s plot training accuracy and validation accuracy vs epochs. As usual, we\u2019ll use matplotlib. Pretty standard stuff, right?\n\nNetwork architecture, hyper parameters, layer parameters: so many combinations to explore\u2026 Let\u2019s try to set some guidelines.\n\nWe\u2019ll start from a basic CNN, known to work well on the MNIST dataset. We\u2019ll apply it as is to Fashion-MNIST to get a baseline.\n\nFirst, we\u2019ll work on getting the best training performance possible, making sure that the network is large enough to learn the training dataset.\n\nWe\u2019ll probably end up overfitting it in the process, which is why we\u2019ll then work on improving validation accuracy.\n\nVery well then, let\u2019s get to work!\n\nThe following network scores 99.2% validation accuracy on MNIST.\n\nWe\u2019ll use ReLU for all activation layers.\n\nTop validation accuracy is 92.42%. This is significantly lower than the MNIST score (99.2%), which goes to show that Fashion MNIST is indeed more difficult to learn. Good :->\n\nOn the bright side, it does look like this network is capable of learning the dataset. It also scored much higher than all non Deep Learning based techniques benchmarked on Fashion-MNIST (the top one is a variation of Support Vector Machines at 89.7%).\n\nSo, hurrah for Deep Learning, but let\u2019s improve this score, shall we?\n\nWe used SGD with a fixed learning rate, which is ok to get a quick feeling for how the network performs. However, more modern optimizers will definitely improve performance.\n\nPopular choices includes AdaDelta (paper), AdaGrad (pdf) or Adam (paper). Which one should we pick? It looks like everyone tends to rely on Adam, so let\u2019s try it for 50 epochs (training log).\n\nTop validation accuracy is 93.05% at epoch #10 (!). Adam does learn very fast indeed.\n\nBatch Normalization (paper) is a technique that helps train faster and avoid overfitting by normalizing values for each training batch. The authors recommend applying it to the inputs of activation layers.\n\nLet\u2019s update our network accordingly and use this technique for both the convolutional layers and fully connected layers.\n\nCompared to previous runs, this one learned even faster. With respect to validation accuracy, we got a small improvement at 93.31%.\n\nTraining performance is now very good. Let\u2019s now work on improving validation accuracy. Batch Normalization did help a bit, but we should be able to do even better by adding Dropout layers.\n\nDropout (paper) is a technique that randomly sets to zero a configurable fraction of connections between two layers. By throwing this wrench into the training process, we slow it down, make it work harder at figuring out unexpected inputs and hopefully help the model generalize better.\n\nLet\u2019s add 30% dropout after each convolution block and before each Dense layer. That\u2019s a lot of Dropout: training should be much slower, so we\u2019ll train for 100 epochs.\n\nHere\u2019s the training log. The best validation accuracy is reached at epoch #72: 94.39%. Dropout helped us squeeze an extra 1% accuracy!\n\nI\u2019m sure we could go higher if we kept experimenting: tuning dropout , trying out different activation functions like Leaky ReLU, using data augmentation, maybe adding more convolution kernels and so on. This is a rather long post already, so let\u2019s stop there :) However, please keep tweaking, it\u2019s the best way to learn (pun not intended) and the Gluon API makes it particularly easy to build networks programatically.\n\nAs always, thanks for reading. Happy to answer questions here or on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/deep-learning-videos-from-aws-dev-days-ba8eededd737?source=user_profile---------18----------------",
        "title": "Deep Learning videos from AWS Dev Days \u2013 Julien Simon \u2013",
        "text": "My talks at AWS Dev Day Stockholm are now online.\n\nWe have more developer events planned across Europe in 2018. The next ones will take place in Manchester, Edinburgh and Dublin. Stay tuned for other countries!\n\nThis is the same session that I did at AWS re:Invent. It wasn\u2019t filmed at the time, so there :)"
    },
    {
        "url": "https://medium.com/@julsimon/gluon-building-blocks-for-your-deep-learning-universe-4bce4e56ef55?source=user_profile---------19----------------",
        "title": "Gluon: building blocks for your Deep Learning universe",
        "text": "Launched in October 2017, Gluon is a new Open Source high-level API for Deep Learning developers. Right now, it\u2019s available on top of Apache MXNet.\n\nYet another API? Well, not quite. Here are ten reasons why you should take a good look at Gluon.\n\nI\u2019m not exaggerating. Calling it documentation doesn\u2019t do it justice: Gluon actually comes with a full-fledged book on Deep Learning!\n\nConcepts, how to implement them from scratch, how to implement them with Gluon, pretty much all network architectures from perceptrons to Generative Adversial Networks\u2026 and a ton of notebooks.\n\nVERY impressive work by my colleague Zach Lipton. If you\u2019d like to help him out, I\u2019m sure he\u2019d be happy to review your pull requests ;)\n\nGluon includes an extensive collection of pre-defined layers: from basic ones (Dense, Activation, Dropout, Embedding, etc.) to Convolution (2D, 3D, transposed) to Pooling (average, max and global max in 1D, 2D and 3D).\n\nYou\u2019ll also find layers for recurrent networks (RNN, LSTM, GRU), as well as individuals cells. The latter allow you full control over your networks should you need to build them cell by cell.\n\nIn addition, you\u2019ll find a collection of experimental features contributed by the Gluon community, such as convolutional recurrent cells.\n\nLast but not least, Gluon also includes a nice collection of loss functions, from basic ones to more advanced ones like the Triplet Loss function used to build face recognition models.\n\nFor reference, this is how we\u2019d define a simple network with the symbolic API in Apache MXNet.\n\nHere\u2019s the same network defined with Gluon. All we have to do is to add layers sequentially.\n\nAs you can see above, we don\u2019t have to define the input shape when building a network. With Gluon, all we have to do is initialize parameters and forward data to the network.\n\nFor instance, this is how we\u2019d apply the network above to a 256-float vector.\n\nThis is an advantage over Keras where we\u2019d have to build the input shape into the model definition.\n\nGluon makes it intuitive to explore network layers, as well as their parameters.\n\nHere\u2019s how we can iterate through layers.\n\nThe Data API provides convenient methods to load datasets stored in NDArrays (which is how MXNet stores tensors), numpy arrays, RecordIO files and image folders.\n\nWe can also download popular datasets like MNIST, Fashion MNIST, CIFAR-10 and CIFAR-100.\n\nTransformations can be applied at loading time by providing a transform function. For example, here\u2019s how we would normalize pixel values for the MNIST dataset.\n\nThe Gluon model zoo is more complete than its counterparts in Apache MXNet, Keras and PyTorch.\n\nAt the time of writing, you can grab pre-trained versions of AlexNet, DenseNet, Inception V3, ResNet V1, ResNet V2, SqueezeNet, VGG and MobileNet, in multiple depths and configurations.\n\nAll of these will come in handy for transfer learning and fine-tuning. Downloading a model couldn\u2019t be simpler.\n\nIn traditional Deep Learning frameworks like Tensorflow and Apache MXNet, network definition and training run in symbolic mode (aka define-then-run).\n\nThere are good reasons for doing this! Since a symbolic network is pre-defined, its execution graph can be optimized for speed and memory prior to training, then run with highly-efficient C++ primitives: all of this makes it more efficient than its imperative counterpart written in Python. However, it comes at the expense of flexibility (networks cannot be modified) and visibility (networks are hard / impossible to inspect).\n\nIn contrast, Gluon relies exclusively on imperative (aka define-by-run) programming: network definition and training loop are based on Python code, allowing us to use all language features (loops, conditional execution, classes, etc.) for maximal flexibility.\n\nThanks to imperative programming, it\u2019s possible to debug every step of the training process: inspecting parameters, saving them to disk, tweaking them if certain conditions happen, etc. Even inside of Jupyter notebooks, we can use the Python debugger by inserting a single line of code. This is invaluable when trying to understand why training goes wrong.\n\nGluon makes it very easy to define your own objects. Here\u2019s a class for a multi-layer perceptron. Once again, the imperative programming style allows us to define the forward() operation exactly the way we want it: we could apply conditional processing based on network parameters, number of epochs, etc.\n\nWe can also define custom layers, as highlighted by this example taken from the Gluon documentation. As you can see, they can be seamlessly integrated with the rest of the Gluon API, so we still rely on existing objects to make our life easier.\n\nWe discussed earlier the benefits of imperative programming while noting that performance would be inferior to symbolic programming.\n\nLet\u2019s run a quick test by predicting 1,000 MNIST images with this simple multi-layer perceptron (for the sake of brevity, I\u2019ll just show the network definition)\n\nNow, let\u2019s change replace the Sequential object with its hybrid equivalent. This will allow Gluon to compile the network to symbolic form and to use optimized lower-level primitives.\n\nThis time, total prediction time is 0.21 second, almost 2x faster. Is there a catch? Well, yes: you lose the flexibility to write a custom forward() function as well as the ability to debug it. Still, once you\u2019ve successfully built and trained a network, hybridizing it is a easy way to improve inference performance.\n\nFor reference, let\u2019s run the same test with the symbolic API of Apache MXNet.\n\nPrediction time is 0.16 second, more than 30% faster than the hybridized version. When top speed is required \u2014 for inference and even more so for training \u2014 the highly-optimized primitives of MXNet remains the best option.\n\nGluon has a lot going for it. I think it improves on symbolic MXNet and even on Keras in several respects . The documentation and the model zoo alone are worth the price of admission, especially if you\u2019re beginning with Deep Learning. Go try it out and tell me what *you* think :)\n\nAs always, thanks for reading. Happy to answer questions here or on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/amazon-ai-monthly-january-2018-f6ee46182f80?source=user_profile---------20----------------",
        "title": "Amazon AI Monthly \u2014 January 2018 \u2013 Julien Simon \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@julsimon/predicting-world-temperature-with-time-series-and-deepar-on-amazon-sagemaker-e371cf94ddb5?source=user_profile---------21----------------",
        "title": "Predicting world temperature with time series and DeepAR on Amazon SageMaker",
        "text": "Predicting time-based values is a popular use case for Machine Learning. Indeed, a lot of phenomena \u2014 from rainfall to fast-food queues to stock prices \u2014 exhibit time-based patterns that can be successfully captured by a Machine Learning model.\n\nIn this post, you will learn how to predict temperature time-series using DeepAR \u2014 one of the latest built-in algorithms added to Amazon SageMaker. As usual, a Jupyter notebook is available on Github.\n\nDeepAR is an algorithm introduced in 2017. It\u2019s quite complex and I won\u2019t go into details here. Let\u2019s just say that unlike other techniques that train a different model for each time-series, DeepAR builds a single model for all time-series and tries to identify similarities across them. Intuitively, this sounds like a good idea for temperature time-series as we could expect them to exhibit similar patterns year after year.\n\nIf you\u2019d like to know more about DeepAR, please refer to the original research article as well as the SageMaker documentation.\n\nThis dataset contains a very large number of temperatures recorded across the globe since the 18th century. Here, I will use the Daily Land dataset, which holds a daily temperature measure from 1880 to 2014. Temperatures are reported as a variation (aka anomaly) from the 1951\u20131980 average (8.68\u00b0C), as visible in the sixth column.\n\nBefore we go any further, we have to decide how to build our time-series. Given data resolution (one data point per day), we should probably build yearly series: having long enough series (hundreds of samples at least) is one of the requirements for a successful model. Thus, we\u2019re going to build 135 series of 365 samples (or 366 for leap years) using the second and sixth columns in our file.\n\nIn addition, we should decide how many samples we\u2019d like to predict: let\u2019s go for 30, i.e. predicting a month\u2019s worth of temperatures.\n\nVery little dataset preparation is required (woohoo!). Once we\u2019ve downloaded and cleaned up the file (remove header and empty lines), we can directly load it into a list (for plotting) and a dictionary (for training). Note that we\u2019re also adding the average temperature to all samples in order to work with actual temperatures, not variations.\n\nLet\u2019s take a quick look at our dataset.\n\nI see an upward trend, but I\u2019ll let each of you come to their own conclusions.\n\nWe\u2019re not going to split 80/20 like we usually would. Things are a bit different when working with time series:\n\nThe input format for DeepAR is JSON Lines: one sample per line, such as this one.\n\nEasy enough, let\u2019s take care of it.\n\nNext, let\u2019s upload our data to S3. The SageMaker SDK has a nice little function to do this.\n\nAs usual with built-in algorithms, we need to select the container corresponding to the region we run in and then create an Estimator. Nothing unusual here.\n\nNow let\u2019s look at hyper parameters.\n\nHyper parameters for DeepAR are detailed in the documentation. Let\u2019s focus on the required ones:\n\nIn addition, after a unreasonable number of different tests, I ended up getting better results with only two layers (default is three), a smaller learning rate and a large number of epochs.\n\nTraining stops early and the best epoch is selected (#206). Here are my three metrics: loss for p50 and p90 (which tell us how accurate the predicted distribution is), as well as Root Mean Square Error.\n\nOK, now let\u2019s deploy this model and use it.\n\nNothing complicated here: create an endpoint hosting our model and create a RealTimePredictor to send requests to.\n\nAccording to the inference format for DeepAR, here\u2019s what we should send to the endpoint:\n\nHere\u2019s an example request, where we provide the first 30 data points.\n\nOnce we get prediction results, we need to extract each time series for plotting. Obviously, we\u2019re not interested in the 100 raw sample series, let\u2019s just pick one at random.\n\nPretty graphs: everyone loves them. They\u2019ll also help us get a sense of how well we\u2019re predicting. We\u2019ll throw in ground truth for good measure.\n\nAll right, time to put all of this to work!\n\nFirst, let\u2019s predict the last 30 days of 1984 and compare to ground truth.\n\nLet\u2019s try another example. This time, suppose that we have data samples for the first 90 days of 2018 (we\u2019ll just use random values here) and that we want to predict the next 30 days. Here\u2019s how you would do it.\n\nAs you can see, built-in algorithms like DeepAR are a great way to get the job done quickly: no training code to write, no infrastructure drama to endure. We can thus focus on experimenting with our time series and hyper-parameters to get the best result possible.\n\nIf you\u2019re curious about other SageMaker built-in algorithms, here are some previous posts on:\n\nAs always, thank you for reading. Happy to answer questions on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/building-a-movie-recommender-with-factorization-machines-on-amazon-sagemaker-cedbfc8c93d8?source=user_profile---------22----------------",
        "title": "Building a movie recommender with Factorization Machines on Amazon SageMaker",
        "text": "Recommendation is one of the most popular applications in Machine Learning. In this post, you will learn how to build a movie recommendation model based on Factorization Machines \u2014 one of the built-in algorithms of Amazon SageMaker \u2014 and the popular MovieLens dataset.\n\nAt the time of writing, this use case is not covered by the very nice collection of SageMaker sample notebooks, so I figured it could help many of you out there if I put one together. Get ready to learn about one-hot encoding, sparse matrices, protobuf files and more :)\n\nSeveral of my AWS colleagues provided excellent advice as well as debugging tips, so please let me thank Sireesha Muppala, Yuri Astashanok, David Arpin and Guy Ernest.\n\nFactorization Machines (FM) are a supervised Machine Learning technique introduced in 2010 (research paper, PDF). FM get their name from their ability to reduce problem dimensionality thanks to matrix factorization.\n\nThey can be used for classification or regression and are much more computationally efficient on large sparse data sets than traditional algorithms like linear regression.This property is why FM are widely used for recommendation: user count and item count are typically very large although the actual number of recommendations is very small (users don\u2019t rate all available items!).\n\nHere\u2019s a toy example, where a sparse rating matrix (dimension 4x4) is factored into a dense user matrix (dimension 4x2) and a dense item matrix (2x4). As you can see, the number of factors (2) is smaller than the number of columns of the rating matrix (4). In addition, this multiplication also lets us fill all blank values in the rating matrix, which we can then use to recommend new items to any user.\n\nThis dataset is a great starting point for recommendation. It comes in multiples sizes and in this post, we\u2019ll use ml100k: 100,000 ratings from 943 users on 1682 movies. As you can see, the ml100k rating matrix is quite sparse (93.6% to be precise) as it only holds 100,000 ratings out of a possible 1,586,126 (943*1682).\n\nHere are the first 10 lines in the data set: user #754 gave movie #595 a 2-star rating and so on.\n\nAs explained earlier, FM work best on high-dimension datasets. As a consequence, we\u2019re going to one-hot encode user ids and movie ids (we\u2019ll ignore timestamps). Thus, each sample in our data set will be a 2,625 boolean vector (943+1682) with only two values set to 1 with respect to the user id and movie id.\n\nWe\u2019re going to build a binary recommender (i.e. like/don\u2019t like). 3-star, 4-star and 5-star ratings are set to 1. Lower ratings are set to 0.\n\nOne last thing: the FM implementation in SageMaker requires training and test data to be stored in float32 tensors in protobuf format. Yes, that sounds complicated :) However, the SageMaker SDK provides a convenient utility function that takes cares of this, so don\u2019t worry too much about it.\n\nOK, then: here are the steps we need to implement:\n\nml-100k contains multiple text files, but we\u2019re only going to use two of them to build our model:\n\nBoth files have the same tab-separated format:\n\nAs a consequence, we\u2019re going to build the following data structures:\n\nOur training matrix is now even sparser: Of all 237,746,250 values (90,570*2,625), only 181,140 are non-zero (90,570*2). In other words, the matrix is 99.92% sparse. Storing this as a dense matrix would be a massive waste of both storage and computing power!\n\nTo avoid this, let\u2019s use a scipy.lil_matrix sparse matrix for samples and a numpy array for labels.\n\nWe should check that we have approximately the same number of samples per class. An unbalanced data set is a serious problem for classifiers.\n\nSlightly unbalanced, but nothing bad. Let\u2019s move on!\n\nNext, we\u2019re going to write the training set and the test set to two protobuf files stored in S3. Fortunately, we can rely on the write_spmatrix_to_sparse_tensor() utility function: it writes our samples and labels into an in-memory protobuf-encoded sparse multi-dimensional array (aka tensor).\n\nThen, we commit the buffer to S3. Once this step is complete, we\u2019re done with data preparation and can now focus on our training job.\n\nHere\u2019s our training set in S3: only 5.5MB. Sparse matrices FTW!\n\nLet\u2019s start by creating an Estimator based on the FM container available in our region. Then, we have to set some FM-specific hyper-parameters (full list in the documentation):\n\nThe other ones used here are optional (and quite self-explanatory).\n\nFinally, let\u2019s run the training job: calling the fit() API is all it takes, passing both the training and test sets hosted in S3. Simple and elegant.\n\nA few minutes later, training is complete and we can check out the training log either in the notebook or in CloudWatch Logs (in the /aws/sagemaker/trainingjobs log group).\n\nAfter 50 epochs, test accuracy is 71.5% and the F1 score (a typical metric for binary classifier) is 0.75 (1 indicates a perfect classifier). Not great but with all that sparse matrix and protobuf excitement, I didn\u2019t spend much time tuning hyper-parameters. Surely you can do better :)\n\nWe have one last step to cover: model deployment.\n\nAll it takes to deploy the model is a simple API call. In the old days (2 months or so ago), this would have required quite a bit of work, even on AWS. Here, just call deploy() and voila!\n\nWe\u2019re now ready to invoke the model\u2019s HTTP endpoint thanks to the predict() API. The format for both request and response data is JSON, which requires us to provide a simple serializer to convert our sparse matrix samples to JSON.\n\nWe\u2019re now able to classify any movie for any user: just build a new data set, process it the same way as the training and test set and use predict() to get results. You should also experiment with different prediction thresholds (set prediction to 1 above a given score and to 0 under it) and see what value gives you the most efficient recommendations. The MovieLens data set also includes movie titles, so there\u2019s plenty more to explore :)\n\nAs you can see, built-in algorithms are a great way to get the job done quickly, without having to write any training code. There\u2019s quite a bit of data preparation involved, but as we saw, it\u2019s key to make very large training jobs fast and scalable.\n\nIf you\u2019re curious about other SageMaker built-in algorithms, here are a couple of previous posts on:\n\nIn addition, if you\u2019d like to know more about recommendation systems, here are a few resources you may find interesting.\n\nAs always, thank you for reading. Happy to answer questions on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/resurrecting-a-bricklens-5596f38931b3?source=user_profile---------23----------------",
        "title": "Resurrecting a BrickLens \u2013 Julien Simon \u2013",
        "text": "A unfortunate Linux kernel update has recently wreaked havoc on many DeepLens cameras. While some lucky users have been able to revert this update, many of them (myself included) have found themselves the proud owner of, well\u2026 a BrickLens :D\n\nHere\u2019s a detailed procedure to perform a full system restore on DeepLens. It builds upon the official troubleshooting guide.\n\n \n\nDISCLAMER: THIS WILL ERASE ALL EXISTING CONTENT ON THE CAMERA.\n\nYou\u2019ll also need the ability to format the micro-SD card with NTFS:\n\nThe flashing.py Python script uses dd to perform a raw copy the system image (image.bin) to the DeepLens internal storage (partition 2 on /dev/mmcblk1 aka /dev/mmcblk1p2). It then resizes the partition to the correct size.\n\n \n\nA word of warning: there\u2019s no error management in the script. To avoid messing up your filesystem and having to flash it again, I would suggest commenting out the filesystem commands.\n\nOnce you\u2019ve run the modified script and flashed the image, you could run the filesystem commands one by one, carefully checking that each one has completed successfully before running the next.\n\n \n\nIf you think I\u2019m just old-fashioned and paranoid, just go ahead and run the script as-is.\n\nThe whole process takes about 15 minutes. Once it\u2019s complete, you can shut DeepLens down.\n\nDisconnect the USB key and remove the micro-SD card. Store them somewhere safe, chances are you\u2019ll need them again ;)\n\n \n\nYour DeepLens is now operational. Power it on and follow the setup instructions in the AWS documentation.\n\n \n\nOne last thing: unless you really enjoyed running this procedure, may I suggest that you disable automatic updates, either at registration time or by running:\n\nHave fun with DeepLens!\n\n \n\nQuestions and comments welcome on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/options-for-machine-translation-on-aws-da819d8a4887?source=user_profile---------24----------------",
        "title": "Options for Machine Translation on AWS ? \u2013 Julien Simon \u2013",
        "text": "Here\u2019s another question I got recently: \u201cWhat are the different options for Machine Translation on AWS?\u201d\n\nYou have three options. Let\u2019s go through each one of them.\n\nAmazon Translate is a high-level AI service based on Deep Learning technology. It delivers fast, high-quality, and affordable language translation.\n\nAt the time of writing, it supports 12 language pairs (more coming): English-Arabic, English-Chinese (simplified), English-French, English-German, English-Portuguese, English-Spanish.\n\nIf you want to see Amazon Translate in action, please refer to this previous post.\n\nAmazon SageMaker is a fully-managed service that enables developers and data scientists to quickly and easily build, train, and deploy Machine Learning models at scale.\n\nOne of the main features of SageMaker is its collection of built-in Amazon-implemented algorithms. One of these algorithms is \u2018seq2seq\u2019, a popular choice to build custom Machine Translation models. In particular, this could help you train a model for a language pair not yet available in Amazon Translate.\n\nIf you want to see \u2018seq2seq\u2019 in action, please refer to this Jupyter notebook that shows you how to train a model translating English to German. It is also available on Sagemaker notebook instances under sample-notebooks/introduction_to_amazon_algorithms/seq2seq_translation_en-de.\n\nSockeye is an open source project that lets you train Machine Translation models. Sockeye also uses the \u2018seq2seq\u2019 algorithm and is based on Apache MXNet, a open source library for Deep Learning.\n\nWith Sockeye, you can train custom models on any infrastructure. Using GPU instances from the P3 family and the Deep Learning AMI on AWS would make sense, but obviously you can use it anywhere else, including on your own servers.\n\nTo see Sockeye in action, please refer to this detailed tutorial on translating German to English.\n\nAs you can see, the choice is yours!\n\nHave another question? Please ping me on Twitter."
    },
    {
        "url": "https://medium.com/@julsimon/image-classification-on-amazon-sagemaker-9b66193c8b54?source=user_profile---------25----------------",
        "title": "Image classification on Amazon SageMaker \u2013 Julien Simon \u2013",
        "text": "In a previous video, I introduced you to Amazon SageMaker, a fully-managed service for Machine Learning. You learned about the different ways you could use SageMaker: using built-in algorithms, bringing your training script, bringing your own model and even bringing your own custom training and prediction code.\n\nToday, I\u2019m going to focus on using the built-in algorithm for image classification. In this code-level video, you will learn how to:\n\nHere we go!"
    },
    {
        "url": "https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-2-3ec757908c5e?source=user_profile---------26----------------",
        "title": "10 steps on the road to Deep Learning (part 2) \u2013 Julien Simon \u2013",
        "text": "In the first part, I told you about the first five steps you could take to get started with Deep Learning (DL):\n\n1 \u2014 You can do it\n\nLet\u2019s look at the five next ones.\n\nDL is code. Most of us learned to code by reading someone else\u2019s code and by figuring out what it does. This applies to DL too.\n\nImage classification is a fun problem to start with. Reasonably-sized data sets such as Dogs and cats, MNIST, CIFAR or German traffic signs are a good starting point.\n\nAs you probably know, there are many different neural network architectures. Initially, you should focus on fully connected networks (aka multi-layer perceptrons aka MLPs). Yes, they\u2019re basic but they\u2019re easy to understand. Early on, don\u2019t worry about building the best model possible: focus on understanding the concepts (training set, validation set, loss function, optimization, hyper parameters) and the steps (data preparation, training loop, etc.) required to train and use a model\n\nSo, grab some examples based on your favorite library, read them, figure out what each bit does, run them and by all means tweak them: add layers, resize layers, change hyper-parameters (batch size, learning rate, number of epochs, etc.) , predict with your own data, etc. Building intuition on how a given change may affect the performance of a model is extremely important.\n\nOnce you\u2019re comfortable with fully connected networks, I\u2019d suggest looking at Convolutional Neural Networks (aka CNNs). Don\u2019t let operations like convolution and pooling scare you away. Use the black box approach we discussed earlier.\n\nLater on, you\u2019ll be ready to look at other architectures like LSTM, GAN, etc. They are pretty weird beasts, so please don\u2019t start with these :)\n\nAfter a while, you\u2019ll be ready to expand your horizon and start exploring additional techniques used by DL practitioners. Here are a few that I found very valuable to study in detail.\n\nMost libraries come with a model zoo, i.e. a set of models pre-trained on large data sets (such as ImageNet for image classification models). You can download the models and use them right away. No training needed!\n\nThese models can be further trained on your own data. This is a very powerful technique that lets you reuse the initial training while specializing the model for your own purpose. All the cool kids do it, e.g. Expedia or Conde Nast.\n\nData augmentation is another powerful technique which creates additional samples from existing ones, e.g. cropping images, resizing them, distorting them, etc. This is especially useful if your initial data set is too small to train from scratch or even for successful fine-tuning. DL libraries provide data augmentation APIs. You can also use the imgaug standalone library.\n\nFor instance, you could look at special layers like Dropout and Batch Normalization, both of which help network learn and generalize better.\n\nYou could also dive deeper on activation functions such as sigmoid, ReLU, LeakyRelu and Softmax: try to understand how they differ and when you\u2019d use one or the other. Same thing for loss functions and regularization.\n\nLast but not least, you should spend some time learning about optimization algorithms. Chances are you\u2019ve used SGD, but there are many more: Adagrad, Adadelta, Adam, etc.\n\nAnd don\u2019t forget: code or it didn\u2019t happen. Try adding one of these Lego blocks to your toy networks, see what happens and try to figure it out.\n\nI took a lot of Math classes during my studies (French engineers will know what \u2018a lot\u2019 means). I thought that I had forgotten everything over the years, but surprisingly it came back pretty fast. Depending on your own studies, you\u2019ll either fly through this stuff, or sweat and curse all the way. Stay focused, move at your own pace and remember, YOU CAN DO IT!\n\nHaving said that, here\u2019s a list of topics you\u2019ll have to study if you want to start understanding how DL building blocks really work.\n\nSince DL is able to extract features automatically from data, I felt that mastering statistics and probabilities was less important than for ML where they play a major role in feature engineering.\n\nStill, you can\u2019t really ignore this stuff. The material below is pretty basic but it will certainly come in handy at some point.\n\nHaving a good grasp on linear algebra, matrices, etc. is mandatory in understanding how forward propagation, backward propagation and so on are implemented.\n\nDerivatives are the foundation on which optimization algorithms \u2014 such as SGD \u2014 built. Again, this is mandatory study if you want to understand them in detail.\n\nIf you\u2019re really rusty (or learning this for the first time), start with this one.\n\nThen, you should study this as well (the first 3 chapters should be enough). You need to know partial derivatives in order to grasp the cosmic beauty of back propagation :*)\n\nOnce you\u2019ve (re)sharpened your Math skills, you will be ready to figure out how these mysterious DL Lego blocks work. Black boxes no more!\n\nHere are some selected topics you should now be able to crack:\n\nA great way to put your knowledge to the test is to stop using high-level APIs such as model.fit() or model.train(). Try switching to examples based on a custom training loop, where every step of the training process is explicitly defined. Once again, read, understand and hack away!\n\nEvery now and then, you\u2019ll stumble on a Math concept that you don\u2019t know well. Just go and learn it. You\u2019ve gone too far to let it stop you, right?\n\nThere are a million resources out there, but here are a few that I found very valuable. When embarking on such a learning adventure, it definitely helps to rely on structured content. Blog posts and Stack Overflow will only take you so far :)\n\nExactly what the title says and a great combination of tools of techniques you\u2019ll need along the way: Python essentials (probably not enough if you\u2019ve never worked with it before), linear algebra, statistics, probability, plotting data, machine learning algorithms, etc.\n\nThe book doesn\u2019t rely on any existing ML/DL framework. Pure Python all the way. Buy it with your eyes closed.\n\nJason runs machinelearningmastery.com, possibly my favorite ML/DL blog. Tons of well-explained, pragmatic examples on how to solve problems! He also authored multiple books based on his posts.\n\nThis one is a really good introduction to ML algorithms, but I\u2019m sure they\u2019re all equally good.\n\nThis is possibly the best online DL book you\u2019ll come across.\n\nThis guide is a no-nonsense introduction to DL: algorithms, network architectures, etc. Zach starts by explaining general concepts and then shows you how to implement them with MXNet and Gluon. Even if you\u2019ve decided to work with a different library, the former is extremely valuable.\n\nDon\u2019t let the simple title fool you. This as BRUTAL as it gets (and I mean that in a good way). The authors are world-expert researchers on Deep Learning and the book is math-heavy (quite an understatement). Being able to read this is a goal in itself. Awesome, but definitely NOT for beginners.\n\nThe book is available online, as well as in printed form (Amazon.com).\n\nJeremy and Rachel have built two free 7-week courses, which are equally brilliant. You\u2019ll be coding from day 1 (mostly on Keras), which is definitely inline with what we\u2019ve been discussing so far: less focus on Math, more focus on code. Production quality is a bit rough at times, but I cannot recommend these courses enough.\n\nAndrew is one of the leading experts in Deep Learning. He authored two reference courses on Coursera, one on Machine Learning and one on Deep Learning.\n\nThe Machine Learning course will take you through all major Machine Learning algorithms. Unlike the fast.ai course, the focus here is on algorithms and there\u2019s definitely more Math involved than you\u2019ll probably like :) There are some programming assignments based on GNU Octave. I understand why you\u2019d pick Octave over Matlab, but IMHO this course is screaming for a new revision based on Python. Having said that, it\u2019s extremely interesting!\n\nThe Deep Learning course was launched a few months ago. It will take you through neural networks, all major network architectures, etc. Here too, the focus is on understanding algorithms, although programming assignments are based in Python, Keras and Tensorflow. A great quality course, led by a brilliant instructor.\n\nThere\u2019s a zillion of them, but here are a few that are definitely worth your time.\n\nIt\u2019s a long road for sure and it will probably take you many months of hard work to go through of all this. If you have focus and dedication, you will make it and acquire these skills. Good luck on your journey to Deep Learning!\n\nAs always, thank you for reading. Please reach out on Twitter if you have questions."
    },
    {
        "url": "https://medium.com/@julsimon/10-steps-on-the-road-to-deep-learning-part-1-f9e4b5c0a459?source=user_profile---------27----------------",
        "title": "10 steps on the road to Deep Learning (part 1) \u2013 Julien Simon \u2013",
        "text": "One of the questions I often get after my talks is: \u201cI\u2019m a developer. How can I get started with this stuff?\u201d. Here\u2019s how I worked my way into Deep Learning. By no means am I claiming that this is \u201cThe Way\u201d, if such a thing even exists.\n\nIn this post and the next, I\u2019ll go through 10 steps that will hopefully help you learn in the right order and at your own pace.\n\nI will also share some commented resources (MOOCs, books, blogs, etc.) at the end of the second post.\n\nA lot of people fear that Machine Learning (ML) and Deep Learning (DL) are too complicated for them and that they\u2019re lacking skills to even get started.\n\nNo, no and no. If I did it, so can you.\n\nYour first goal should be to understand what this technology is, what it can or cannot do and how you can use it in our own applications. They are plenty of data sets, algorithms and models available off the shelf. You should start by figuring out what these Lego blocks are and how you can combine them to start building cool stuff. Save the nitty-gritty details for later: isn\u2019t this how we work with code anyway?\n\nTip #1: DL is not dark magic. It\u2019s code. Treat it as such: no more, no less.\n\nForward propagation, backward propagation, loss functions, gradient descent and what not. The Math and jargon are pretty overwhelming at first.\n\nYes, Math is the foundation of ML. No, it should not be your starting point. Who wants to go through hours of statistics and algebra before writing even one line of code? Not me.\n\nThis is typically how you kickstart the training process in a DL library:\n\nYou can either spend weeks understanding the underlying Math, or \u2014 for a while \u2014 you can just accept that it works the way it does and move on.\n\nUsing a black box approach is a great way to get started with minimal angst. Over time, you\u2019ll build a better understanding on how each block works and you\u2019ll be able to go deeper. But not now. Peel the DL onion layer by layer.\n\nTip #2: abstraction is your friend. Treat anything you don\u2019t understand as a black box and focus on interfaces.\n\nWe established that DL is code. So, what language should you master?\n\nIf you have to pick ONE, it\u2019s Python, hands down. Chances are, you already know it ;) Not only is it the dominant language for Data Science, ML and DL, it\u2019s also a useful language for pretty much everything else, as illustrated by the Python AWS SDK aka boto3.\n\nIf you haven\u2019t used it in a while, or if you\u2019re learning it, make sure you have a really good understand of lists, tuples, dictionaries, etc. These data structures are everywhere in DL code and you\u2019ll definitely struggle if you\u2018re not comfortable with them.\n\nIn addition, to the core language, I would very strongly suggest that you also learn the basics of these Python tools:\n\nTip #3: Invest in Python 3 and enjoy a lifetime of dividends.\n\nI\u2019ll spare you the Venn diagram on AI, ML and DL. Still, DL is a subset of ML, so shouldn\u2019t you know a bit about ML before jumping into DL? :)\n\nYou should definitely spend time learning the ropes of \u201ctraditional\u201d ML: data sets, main problem classes (regression, classification, clustering, etc), popular algorithms for each problem class, etc. Neural networks are just one technique and there are many more. Having a wider understanding of ML will help you see when to use DL or not.\n\nIt\u2019s a good thing that you learned Python, because now you can get introduced to ML with scikit-learn. This library is easy to use and supports a large number of ML algorithms: definitely the more gentle way to learn what they are and what they\u2019re good at.\n\nIf you\u2019re more advanced (or more ambitious), you could also take a look at PySpark (Spark\u2019s Python API) and Spark MLlib (Spark\u2019s Machine Learning library). Spark\u2019s powerful data management features are an asset if you have to deal with large data sets. They\u2019re overkill during your learning phase, but keep them in mind as you start building production apps.\n\nTip #4: First and foremost, Deep Learning is Machine Learning. Learn them in the right order.\n\nRead my lips: at this point, it doesn\u2019t really matter which DL library you pick\u2026 As long as it has a Python API and is friendly enough to hide the nasty low-level stuff, you\u2019ll be fine.\n\nSo, in no particular order, here are some options:\n\nA word of warning: Theano is very low-level and going into maintenance mode. Tensorflow is low-level, hard to learn and known to generate quite a bit of frustration. I would not recommend these to a beginner.\n\nCheck out the documentation, run some tutorials, make up your own mind. One of these libraries will feel more comfortable to you. That\u2019s the one you should pick.\n\nTip #5: Ignore the hype. Use the library that will help you learn faster."
    },
    {
        "url": "https://medium.com/@julsimon/mixing-spark-with-sagemaker-d30d34ffaee7?source=user_profile---------28----------------",
        "title": "Mixing Spark with Sagemaker ? \u2013 Julien Simon \u2013",
        "text": "This short post comes from a question asked by Manel Maragal (thanks!) on my YouTube channel. It\u2019s a really good question and hopefully my answer doesn\u2019t suck\u2026 so why not share both with everyone?\n\nIn the line of \u201cBuilding your own algorithm container\u201d, is it possible to use Spark code entirely (and distributively) on SageMaker? What I get from the documentation is that I\u2019m supposed to do ETL in my Spark Cluster. And then, when fitting the data to the model, use sagemaker_pyspark that will create a Sagemaker job. Moving the dataframe into S3 with protobuf format, to then train with a new Sagemaker instance cluster.\n\nThe question is: if I already have my dataframe loaded into my distributed cluster, why would I want to use Sagemaker? I might as well use Spark ML for it, which has a better algorithm support, and avoids creating an additional cluster. Maybe I got the whole thing wrong\u2026\n\nSpark and Spark ML are great tools, more power to both :) Still, I see a few reasons why combining them with Sagemaker would make sense:\n\n1 \u2014 Decoupling ETL infrastructure from training infrastructure. They could have different sizing requirements (compute and/or storage) and you wouldn\u2019t want to oversize your Spark cluster just because one part of the overall process requires more capacity. Sure, you can resize an EMR cluster dynamically, but it\u2019s extra work and potentially troublesome if you have a lot of data. SageMaker will start and terminate instances automatically, which I find cleaner and simpler :)\n\n2 \u2014 In the same vein, imagine you need GPUs for training. It would be costly and sub-optimal to run your EMR cluster on GPU instances just for this.\n\n3 \u2014 Spark ML is great and has indeed a lot of algos. SageMaker is just getting started and we\u2019ll keep adding scalable implementions of state-of-the-art algos, so maybe one of them will actually catch your eye, such as DeepAR (released a couple of days ago).\n\n4 \u2014 Deploying models in production on managed infrastructure. To me, this is the single most important feature in SageMaker: call a single API and deploy an endpoint.\n\nIf you\u2019re curious about combining SageMaker and Spark, this Github repository has several examples.\n\nKeep the questions coming and have fun testing :)"
    },
    {
        "url": "https://medium.com/@julsimon/amazon-ai-monthly-december-2017-5532dff32eaa?source=user_profile---------29----------------",
        "title": "Amazon AI Monthly \u2014 December 2017 \u2013 Julien Simon \u2013",
        "text": "From now on, you\u2019ll get every month a video recap of all things Amazon AI: new features, cool blog posts, new customer stories.\n\nI hope you\u2019ll enjoy it. So, without further ado, here\u2019s episode 1 :)"
    },
    {
        "url": "https://medium.com/@julsimon/amazon-ai-the-christmas-post-bacf76346bd9?source=user_profile---------30----------------",
        "title": "Amazon AI: the Christmas post \u2013 Julien Simon \u2013",
        "text": "Let\u2019s start with a proper business problem: we want to build the capacity to automatically turn billboard text into multi-language speech. No doubt that this has to be a billion dollar business.\n\nApparently, this ad did run for a while in Las Vegas before being taken down due to a number of complaints. Complaining about alcohol abuse in Las Vegas??? Consider moving, dude ;)\n\nAnyway, our goal today is to:\n\nAs these services are quite recent, we need to make sure we have the latest AWS Python SDK aka boto3. We\u2019ll also need Pygame to play sounds in a portable way.\n\nText detection in Amazon Rekognition allows us to detect words, lines and their respective positions in the picture. Here\u2019s, we only care about full lines of text, so we\u2019re going to append all lines into a single string.\n\nAmazon Comprehend has a dedicated API for this. We\u2019ll simply call it and return the language code.\n\nAmazon Translate is straightforward: provide the text string, the source language (detected by Comprehend) and the destination language. We\u2019re returning a text string holding the translation.\n\nLast but not least, let\u2019s use Amazon Polly (combined to Pygame) to speak the translated text.\n\nWe have solved our business problem. No doubt the Executive Committee will be pleased :D\n\nJoking aside (for a minute), I find it pretty amazing that we\u2019re able to build this kind of application with so little code and without knowing anything about AI. Amazon AI rocks!\n\nThat\u2019s it for today. Have a great Christmas, everybody. Party like a wild animal if that\u2019s your thing but please stay safe, ok?"
    },
    {
        "url": "https://medium.com/@julsimon/optimizing-apache-mxnet-models-for-deeplens-on-amazon-sagemaker-a9ec5673e8ce?source=user_profile---------31----------------",
        "title": "Optimizing Apache MXNet models for DeepLens on Amazon SageMaker",
        "text": "In a previous post, we explored how Apache MXNet models are in fact optimized for AWS DeepLens thanks to the Intel Deep Learning Inference Engine.\n\nIn this article, I will show you how to automate this process during a typical training process we would perform on Amazon SageMaker.\n\nAs usual, all code is available in a Jupyter notebook on Github.\n\nFirst of all, we need to register and get a download link for the toolkit. Then, let\u2019s grab the toolkit and extract it.\n\nIn order to be able to perform unattended installation, let\u2019s edit the configuration file named silent.cfg. We simply need to set:\n\nNow, let\u2019s archive the toolkit and copy it to an S3 bucket, which we\u2019ll use for deployment to SageMaker instances.\n\nOk, now we\u2019re ready to deploy the toolkit to SageMaker and convert models. Let\u2019s open a notebook instance and start a new Jupyter notebook.\n\nFirst, let\u2019s define S3 locations for the toolkit and the model to convert: if you used Amazon SageMaker to train the model (and why wouldn\u2019t you?), your model is already in S3 anyway :)\n\nNext, we have to set the location where the Intel Toolkit will be installed, as well the optimization parameters we\u2019d like to use.\n\nNow we\u2019re ready to download and install the toolkit. A few lines of script is all it takes.\n\nThe next step is to install Python dependencies required by the Model Converter.\n\nConda makes it easy to create isolated environments, so let\u2019s build one for the Intel toolkit. This will save us from clobbering the environment we use for training.\n\nOf course, you\u2019ll only need to run these steps once per instance. The second (slightly cryptic) line is required to see the new kernel listed in the Jupyter menu.\n\nNothing fancy here: simply copy the trained model from its S3 location.\n\nWe saw how to do this in the previous post. More information on parameters here.\n\nAnd we\u2019re done! Now you can copy the converted model back to S3 and deploy it to Deep Lens.\n\nThis was surprisingly easy, don\u2019t you think? Now you can optimize your models and deploy them to DeepLens. Please let me know about your projets, happy to share and retweet!\n\nAs always, thank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/a-quick-look-at-machine-translation-with-amazon-translate-d7ca66983439?source=user_profile---------32----------------",
        "title": "A quick look at Machine Translation with Amazon Translate",
        "text": "Amazon Translate is a new service announced at AWS re:Invent 2017. At the time of writing, it is available in preview. Please consider joining it and sending us feedback!\n\nThis is what Amazon Translate is capable of right now:\n\nLet\u2019s try it on a few examples. Please keep in mind that the service is still in preview and that it\u2019s constantly learning: imperfections will quickly be fixed thanks to customer feedback.\n\nFirst paragraph on this article in today\u2019s Le Monde:\n\nThe date is a little off, but the rest of the translation is fine.\n\nFirst paragraph of this article in today\u2019s El Mundo:\n\nI don\u2019t speak Spanish, but the English definitely makes sense to me :)\n\nFirst paragraph on this article from the BBC:\n\nI don\u2019t speak Chinese either. Although some parts of the translation need improvement, the meaning of the article is quite clear: more taxes :-/\n\nCurious about this works? Read on :)\n\nseq2seq is a supervised learning Deep Learning algorithm where the input is a sequence of tokens (such as words from a text) and the output generated is another sequence of tokens. It is a popular choice for Machine Translation applications.\n\nLet\u2019s take a look, shall we?\n\nSockeye includes a nice tutorial on building a model to translate German to English. It relies on the WMT news dataset, a great resource if you want to train other language pairs.\n\nFollowing the instructions and letting the model train for 5\u20136 hours on a p2.8xlarge instance, I obtained my first model without any problem. Great job on the tutorial, Sockeye team :)\n\nI threw the actual translation commands into a small shell script. Here are some samples taken from Wikipedia and German news websites.\n\nPretty good results. I\u2019m sure this would be even better with longer training times.\n\nSo there you have it. You can either use Amazon Translate as a fully managed translation service, or you can build and train your own with Sockeye. Choice is king!\n\nThat\u2019s it for today. Thank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/a-quick-look-at-natural-language-processing-with-amazon-comprehend-238b8d9ec11d?source=user_profile---------33----------------",
        "title": "A quick look at Natural Language Processing with Amazon Comprehend",
        "text": "Amazon Comprehend is a new service announced at AWS re:Invent 2017. At the time of writing, it is available in the US (Virginia, Ohio, Oregon) and in Europe (Ireland).\n\nThis is what Amazon Comprehend is capable of right now:\n\nLet\u2019s look at the first four features with a simple example in the console. Here\u2019s a sample text from an Associated Press data set:\n\n\u201cA suspect bit the ear of a 4-year-old police dog and injured the animal\u2019s neck during a chase and arrest, police said today. The dog, Rex, was on patrol with Constable Philip Rajah in the Natal provincial capital during the weekend when they came across two suspicious individuals,\u2019\u2019 police said. While Rajah searched one man, Rex chased the other and got the worst of it when his quarry turned on the animal and bit him. Rajah had to yank the man off the dog, police said. They said the dog was being treated for a serious neck injury at a veterinary clinic. The man who bit the dog may face a charge of malicious injury to state property\u201d.\n\nWhat does Comprehend make of this crazy story?\n\nAs you can see, key entities are properly identified: the police officer, Rex, the two individuals, the date and the location. Language is also detected as \u2018English\u2019.\n\nOnce again, key phrases are understood. It\u2019s quite likely that indexing them in a backend would let us find this article quickly and accurately.\n\nNow, what about sentiment?\n\nTone is indeed pretty neutral, although I\u2019m quite sure poor Rex would think this is a pretty negative story :D\n\nLet\u2019s take another shot at language detection. Comprehend is able to detect 100 languages, so let\u2019s use tweets in three different languages: Swahili, Tagalog and Ukrainian. As you can see, these are rather short sentences, which are more difficult to detect than longer texts.\n\nAll three are correctly detected. Good job, Comprehend!\n\nNow, let\u2019s take a look at topic modeling, a powerful technique allowing us to automatically:\n\nFirst of all, we need a data set. We\u2019ll use a collection of 2,246 news items from Associated Press (source), stored in a TSV file.\n\nFirst, we have to remove the first two columns (we don\u2019t need them) and save the file in UTF-8 format, which is what Comprehend expects.\n\nCreating the job is straightforward: S3 bucket, file format, number of topics to detect, etc. Once the job is complete, we can download the results from S3.\n\nThe output file contains two CSV files: doc-topics.csv and topic-terms.csv.\n\nThe doc-topics.csv file stores the list of topics and weights detected in each document (here each line in the file),\n\nFor example, let\u2019s look at topics for document #648:\n\nHere are the first few lines of this document: \u201cSavings institution depositors withdrew more money from their accounts than they deposited in May for the first time in seven months, while losses continued to erode the ailing industry\u2019s capital, the government said Monday. The Federal Home Loan Bank Board said net withdrawals at the nation\u2019s 3,102 federally insured S &Ls totaled $941 million in May. But the decline came after seven consecutive months of net deposit gains totaling $28 billion\u201d.\n\nLooks like \u2018finance\u2019 is the key topic here.\n\nThe topic-terms.csv file stores the list of topics that have been detected in the document collection. For each topic, we see a list of words and their weights.\n\nThe dominant topic in document #648 is topic #3: let\u2019s check what it\u2019s about.\n\nThese words could reasonably be associated with a \u2018finance\u2019 topic, so it\u2019s a pretty good match for the document above.\n\nCurious about this works? Read on :)\n\nLDA is an unsupervised learning algorithm which is commonly used to discover a user-specified number of topics shared by documents within a text collection.\n\nIf you want to dive deeper (and I mean \u2018deeper\u2019) on topic modeling and LDA, I strongly recommend this re:Invent video by my colleague Anima Anandkumar.\n\nThat\u2019s it for today. Thank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/exploring-ahem-aws-deeplens-fcad551886ef?source=user_profile---------34----------------",
        "title": "Exploring (ahem) AWS DeepLens \u2013 Julien Simon \u2013",
        "text": "AWS DeepLens was one of the most surprising launches at at re:Invent 2017. Built on top of AWS services such as Lambda and Greengrass, this Intel-powered camera lets developers experiment with Deep Learning in a fun and practical way.\n\nOut of the box, a number of projects can be deployed to the camera in just a few clicks: face detection, object recognition, activity detection, neural art, etc.\n\nThe overall process looks like this:\n\nThis is seriously cool and fun, but I want to know how this really works. Don\u2019t you? Yeah, I thought so :)\n\nIn the DeepLens console, we can easily see that project models are deployed from S3. Here\u2019s how it looks for the face detection project.\n\nLet\u2019s take a look, then.\n\nHuh? This is not what an MXNet looks like. As explained before, we should see a JSON file holding the model definition and PARAMS file storing model weights.\n\nLet\u2019s ssh to the camera and try to figure this out.\n\nIntel Deep Learning Deployment Toolkit. This sounds exciting. A few seconds of googling later, we learn here that this SDK includes:\n\nThis makes a lot of sense. Although it\u2019s perfectly capable to run in resource-constrained environments \u2014 as demonstrated by my Raspberry Pi experiment \u2014 Apache MXNet is not the best option here. First, it carries a lot of code (training, data loading, etc.) which is useless in an inference context. Second, it simply cannot compete with a platform-specific implementation making full use of dedicated hardware, special instructions and so on.\n\nSo now, the model files in S3 make sense. The XML file is the model description and the BIN file is the model in IR form.\n\nWhat kind of hardware is it optimized for ? Let\u2019s look at the hardware for a second.\n\nThis baby comes with an Intel HD Graphics 500 chip, so we have a GPU in there too. This one has 12 \u201cexecution units\u201d capable of running 7 threads each (SIMD architecture). 84 \u201ccores\u201d, then: not a monster, but surely better than running inference on the Atom itself.\n\nNow it\u2019s starting to make sense. The Inference Engine is certainly able to leverage specific instructions on the Atom (with Intel MKL, no doubt) as well as the GPU architecture (with OpenCL).\n\nNow what about the model optimizin\u2019 thing?\n\nSays the Intel doc: \u201c(the model optimizer) performs static model analysis and automatically adjusts deep learning models for optimal execution on end-point target device\u201d.\n\nOK. It optimizes. Nice job explaining it :-/ Let\u2019s figure it out.\n\nAfter a bit of installing (and cursing at python), we\u2019re able to run the optimizer.\n\nMost parameters make sense, but two are a bit intriguing.\n\nOK, this explains the model name we saw earlier.\n\nThis model uses 16-bit floating values for weights (and probably activation functions too). Obviously, 16-bit arithmetic is both faster and more energy-efficient than 32-bit arithmetic, so this makes sense.\n\nOK, now let\u2019s run this thing on existing models. First, we\u2019ll download Inception v3 and VGG-16 from the MXNet model zoo. Then, we\u2019ll convert them.\n\nLet\u2019s now compare the original models to their optimized version.\n\nAs you can see, moving to FP16 definitely halves model size. Less storage, less RAM, less compute!\n\nNow there\u2019s only question? Do these models work? Should we give this Inference Engine a spin? Of course we should.\n\nThe toolkit comes with a bunch of samples, let\u2019s build them.\n\nOur models are classification models, so let\u2019s try this.\n\nLet\u2019s grab an image and resize it to 224x224."
    },
    {
        "url": "https://medium.com/@julsimon/a-quick-look-at-automatic-speech-recognition-with-amazon-transcribe-8e84fa31d51e?source=user_profile---------35----------------",
        "title": "A quick look at Automatic Speech Recognition with Amazon Transcribe",
        "text": "Amazon Transcribe is a new service announced at AWS re:Invent 2017.\n\nAt the moment, the service is only available in preview (which you can apply for), but this shouldn\u2019t prevent us from taking a look, should it? ;)\n\nThis is what Amazon Transcribe is capable of right now:\n\nThis is a high-level service in the vein of Polly and Rekognition, so the API is as easy as it gets: ListTranscriptionJobs, StartTranscriptionJob and GetTranscriptionJob.\n\nNo CLI for now, so we have to test either with the AWS Console or with the SDK. Let\u2019s keep this simple and try the console.\n\nI recorded a sound file, saved it as a FLAC file and uploaded to S3. I also updated the bucket policy to allow access for Transcribe (you\u2019ll find the policy in the documentation).\n\nHeading out to the console, it\u2019s pretty easy to create a job. Just make sure you use a proper URL to point to your sound file, not an S3 URI.\n\nClick on the \u201cCreate\u201d button, wait for a few minutes and you\u2019ll see the output of the job.\n\nLet\u2019s download the JSON file (complete file) and look at the raw output.\n\nPretty good results. Some commas are missing and funny enough, Transcribe doesn\u2019t understand its own name. Apart from this, this is an accurate transcription of my sound file.\n\nThe output file also contains the time stamps for each words, the confidence score and possible alternatives, e.g.:\n\nCool new service. Please join the preview and send us feedback. I\u2019m very curious to see what you\u2019re going to build with this!\n\nAs always, thank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/building-a-spam-classifier-pyspark-mllib-vs-sagemaker-xgboost-1980158a900f?source=user_profile---------36----------------",
        "title": "Building a spam classifier: PySpark+MLLib vs SageMaker+XGBoost",
        "text": "Our raw data set is composed of 1-line messages stored in two files:\n\nIn order to classify these messages, we need to build an intermediate data set with two classes. For this purpose, we\u2019re going to use a simple but efficient technique called Feature Hashing:\n\nOnce we\u2019re done, our intermediate data set will be:\n\nWe\u2019ll split it 80/20 for training and validation and run in through a number of classification algorithms.\n\nFor prediction, the process will be similar: hash the message, send the word vector to the model and get the predicted result.\n\nNot that difficult, hey? Let\u2019s get to work!\n\nOur first step is to load both files and split the messages into words.\n\nThen, we\u2019re hashing each message into 1,000 word buckets. As you can see, each message is turned into a sparse vector holding bucket numbers and occurrences.\n\nThe next step is to label our features: 1 for spam, 0 for non-spam. The result is a collected of labeled samples which are ready for use.\n\nFinally, we split the data set 80/20 for training and test and cache both RDDs as we will use them repeatedly.\n\nNow we\u2019re going to train a number of models with this data set. To measure their accuracy, here\u2019s the scoring function we\u2019re going to use: simply predict all samples in the test set, compare the predicted label with the real label and compute accuracy.\n\nWe\u2019re going to use the following classification algorithms:\n\nLet\u2019s start with Logistic Regression, the mother of all classifiers.\n\nWhat about SVMs, another popular algorithm?\n\nNow let\u2019s try three variants of tree-based classification. The API is slightly different from previous algos.\n\nLast but not least, let\u2019s try the Naives Bayes classifier.\n\nIt is vastly superior to all other algos. Let\u2019s try to predict a couple of real-life samples.\n\nThey were predicted correctly. This looks like a pretty good model. Now why don\u2019t try to improve these scores? I\u2019ve used default parameters for most of the algorithms, surely there is room for improvement :) You\u2019ll find links to all APIs in the notebook, so feel free to tweak away!\n\nSo far, we\u2019ve only worked locally. This raises some questions:\n\nThese questions \u2014 scalability and deployment \u2014 are often the bane of Machine Learning projects. Going from \u201cit works on my machine\u201d to \u201cit works in production at scale 24/7\u201d usually requires a lot of work.\n\nThere is hope. Read on :)"
    },
    {
        "url": "https://medium.com/@julsimon/amazon-sagemaker-a9028ab9874e?source=user_profile---------37----------------",
        "title": "Amazon SageMaker \u2013 Julien Simon \u2013",
        "text": "Yesterday, Andy Jassy announced a lot of new services during his re:Invent keynote. One of the most exciting ones is Amazon SageMaker, a fully-managed service that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale.\n\nI just recorded a 30-minute walkthrough, so let\u2019s get to it. Plenty of code and examples for you to enjoy :)\n\nThank you for viewing. Build cool stuff and let me know!"
    },
    {
        "url": "https://medium.com/@julsimon/speeding-up-apache-mxnet-part-3-lets-smash-it-with-c5-and-intel-mkl-90ab153b8cc1?source=user_profile---------38----------------",
        "title": "Speeding up Apache MXNet, part 3: let\u2019s smash it with C5 and Intel MKL",
        "text": "In a previous post, I showed you how to speed up MXNet inference with the NNPACK Open Source library.\n\nSince then, AWS has released a new instance family named c5, based on the new Intel Skylake architecture. This architecture notably uses the AVX-512 SIMD instruction set, which is designed to boost math operations involved in Deep Learning. To maximize performance, developers are encouraged to use the Intel Math Kernel Library which provides a highly optimized implementation of these math operations.\n\nHow about we combine C5 and MKL and get smashin\u2019?\n\nThe procedure is pretty simple. Let\u2019s start with a c5.4xlarge instance 16 vCPUs, 32GB RAM) running the latest Deep Learning AMI.\n\nFirst, we need to configure the MXNet build in ~/src/mxnet/config.mk. Please make sure that the following variables are set correctly.\n\nNow, let\u2019s build the latest version of MXNet (0.12.1 at the time of writing). The MKL library will be downloaded and installed automatically.\n\nWe\u2019re good to go. Let\u2019s run our benchmark.\n\nLet\u2019s use the script included in the MXNet sources: ~/src/mxnet/example/image-classification/benchmark_score.py. This benchmark runs inference on a synthetic data set, using a variety of Convolutional Neural Networks and a variety of batch sizes.\n\nAs nothing is ever simple, we need to fix a line of code in the script. C5 instances don\u2019t have any GPU installed (which is the whole point here) and the script is unable to properly detect that fact. Here\u2019s the modification you need to apply. While we\u2019re at it, let\u2019s add additional batch sizes.\n\nOK, now let\u2019s run the benchmark. After a little while, here are the results.\n\nFor comparison, here are the same results for vanilla MXNet 0.12.1 running on the same c5 instance.\n\nA picture is worth a thousand words. The following graphs illustrate the speedup provided by MKL for each network: batch size on the X axis, images per second on the Y axis.\n\nDo we need one? :) Thanks to Intel MKL, we get massive inference speedup for all models, anywhere from 7x to 10x. We also get some scalability as AVX-512 kicks into high gear when batch size increases.\n\nThe comparison with vanilla MXNet 0.11 running on a c4.8xlarge instance is even more impressive, as speedup consistently hits 10x to 15x. If you\u2019re running inference on c4, please move to c5 as soon as you can: you\u2019ll definitely get more bang for your buck.\n\nOne last thing: what about training? Using the same c5 instance, a quick training test on CIFAR-10 (using the train_cifar10.py script) shows a 10x speedup between MKL-enabled MXNet and vanilla MXNet. Not bad at all. On smaller data sets, c5 could actually be a cost-effective solution compared to GPU instances. Your call, really\u2026 which is the way we like it ;)\n\nThat\u2019s it for today. Thanks for reading."
    },
    {
        "url": "https://medium.com/@julsimon/generative-adversarial-networks-on-apache-mxnet-part-1-b6d39e6b5df1?source=user_profile---------39----------------",
        "title": "Generative Adversarial Networks on Apache MXNet, part 1",
        "text": "In several previous posts, I\u2019ve shown you how to classify images using a variety of Convolution Neural Networks. Using a labeled training set and applying a supervised learning process, AI delivers fantastic results on this problem and on similar ones, such as object detection or object segmentation.\n\nImpressive as it is, this form of intelligence only deals with understanding representations of our world as it is (text, images, etc). What about inventing new representations? Could AI be able to generate brand new images, convincing enough to fool the human eye? Well, yes.\n\nIn this post, we\u2019ll start to explore how!\n\nA breakthrough happened in 2014, with the publication of \u201cGenerative Adversarial Networks\u201d, by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville and Yoshua Bengio.\n\nRelying on an unlabeled data set and an unsupervised learning process, GANs are able to generate new images (people, animals, landscapes, etc.) or even alter parts of an existing image (like adding a smile to a person\u2019s face).\n\nThe original Goodfellow article uses the art forger vs art expert analogy, which has been rehashed to death on countless blogs. I\u2019ll let you read the original version and I\u2019ll try to use a different analogy: cooking.\n\nYou\u2019re the apprentice and I\u2019m the chef (obviously!). Your goal would to cook a really nice Boeuf Bourguignon, but I wouldn\u2019t give you any instructions. No list of ingredients, no recipe, nothing. My only request would be: \u201ccook something with 20 ingredients\u201d.\n\nYou\u2019d go the pantry, pick 20 random ingredients, mix them together in a pot and show me the result. I\u2019d look at it and of course the result would be nothing like what I expected\n\nFor each of the ingredients you selected, I\u2019d give you a hint which would help you get a little bit closer to the actual recipe. For example, if you picked chicken, I could tell you: \u201cWell, there is no chicken in this recipe but there is meat\u201d. And if you used grape juice, I may say: \u201cHmm, the color is right but this is the wrong liquid\u201d (red wine is required).\n\nResolved to improve, you\u2019d go back to the pantry and try to make slightly better choices. The result would still be far off, but a little bit closer anyway. I\u2019d give you more hints, you\u2019d cook again and so on. After a number of iterations (and a massive waste of food), chances are you\u2019d get very close to the actual recipe \u2014 assuming that I wouldn\u2019t have lost my temper by then :D\n\nLet\u2019s replace the apprentice by the Generator and the chef by the Discriminator. Here is how GANs work.\n\nGANs may be implemented using a number of different model architectures. Here, we will study a GAN based on Convolutional Neural Networks, as published in \u201cUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\u201d, by Alec Radford, Luke Metz and Soumith Chintala (2016).\n\nLet\u2019s take a look at the Generator model. I\u2019ve slightly updated the illustration included in the original article to reflect the exact code that we will use later on.\n\nDon\u2019t panic, it\u2019s not as bad as you think. We start from a random vector of 100 values. Using 5 transposed convolution operations (more on this in a minute), this vector is turned into an RGB 64x64 image (hence the 3 channels).\n\nNow let\u2019s look at the Discriminator. Wait, it\u2019s almost identical (don\u2019t forget to start from the left this time). Using 5 convolution operations, we turn an RGB 64x64 image into a probability: true for valid samples, false for invalid samples.\n\nStill with me? Good. Now what about this convolution / transposed convolution thing?\n\nThere are plenty of great tutorials out there. The best I\u2019m aware of is part of the Theano documentation. Extremely detailed with beautiful animations. Read it and words like \u201ckernel\u201d, \u201cpadding\u201d and \u201cstride\u201d will become crystal clear.\n\nIn a nutshell, convolution is typically used to reduce dimensions. This is why this operation is at the core of Convolutional (duh) Neural Networks: they start from a full image (say 224x224) and gradually shrink it through a series of convolutions which will only preserve the features that are meaningful for classification.\n\nThe formula to compute the size of the output image is actually quite simple.\n\nWe can apply it to the Discriminator network above and yes, it works. Woohoo.\n\nTransposed convolution is the reverse process, i.e. it increases dimensions. Don\u2019t call it \u201cDeconvolution\u201d, it seems to aggravate some people ;)\n\nThe formula to compute the size of the output image is as follows.\n\nApplying it to the Generator network gives us the correct results too. Hopefully, this is starting to make sense and you now understand how it\u2019s possible to generate a picture from a vector of random values :)\n\nApache MXNet has a couple of nice examples implementing this network architecture in R and Python. I\u2019ll use Python for the rest of the post, but I\u2019m sure R users will follow along.\n\nHere\u2019s the code for the Discriminator network, based on the illustration above. You\u2019ll find extra details in the research article, e.g. why they use the LeakyRelu activation function and so on.\n\nHere\u2019s the code for the Generator network, based on the illustration above.\n\nOK, now let\u2019s take care of the data set. As you probably know, the MNIST data set contains 28x28 black and white images. We need to:\n\nNothing MXNet-specific here, just good old Python data manipulation.\n\nDuring Discriminator training, this data set will be served by a standard NDArray iterator.\n\nWe also need to provide random data to the Generator. We\u2019ll do this with a custom iterator.\n\nWhen getdata() is called, this iterator will return an NDArray shaped (batch size, random vector size, 1, 1). We\u2019ll use a 100-element random vector, so through multiple transposed convolutions, the Generator will indeed build a picture from a (100, 1, 1) sample.\n\nTime to look at the training code. This time, we cannot use the Module.fit() API. We have to write a custom training loop taking into account the fact that we\u2019re going to use the Discriminator gradients to update the Generator.\n\nHere are the steps:\n\nQuite a mouthful! Congratulations if you got this far: you understood the core concepts of GANs.\n\nThe MXNet sample includes code to visualize the images coming out of the Generator . The simplest way to view them is to copy the code in a Jupyter notebook and run it :)\n\nAfter a few minutes (especially if you use a Volta-powered p3 instance), you should see something similar to this.\n\nAs you can see, random noise gradually turns into well-formed digits. It\u2019s just math, but it\u2019s still amazing\u2026\n\nCommon training metrics like accuracy mean nothing here. We have no way of knowing whether Generator images are getting better\u2026 except by looking at them.\n\nAn alternative would be to generate only fives (or any other digit), to run them through a proper MNIST classifier and to measure accuracy.\n\nThere is also ongoing research to use new metrics for GANs, such as the Wasserstein distance. Let\u2019s keep this topic for another article :)\n\nThanks for reading. This is definitely a deeper dive than usual, but I hope you enjoyed it."
    },
    {
        "url": "https://medium.com/@julsimon/c5-instances-fast-and-then-some-2a31d35a6a76?source=user_profile---------40----------------",
        "title": "C5 instances: fast\u2026 and then some \u2013 Julien Simon \u2013",
        "text": "In a previous post, I used the largest i3 instance to build FreeBSD 11 in 10 minutes and 54 seconds.\n\nThis was a massive improvement over the i2 family, thanks to super-fast NVMe storage. Now that the c5 family is available, let\u2019s see if we can do even better :)\n\nAs usual, c5 instances come in different sizes.\n\nLet\u2019s pick the largest one and see how the new Intel Skylake architecture performs. For storage, we\u2019ll use a 100GB EBS SSD volume with 5,000 IOPS.\n\nWe\u2019ve done this a few times already, here are the commands. Let\u2019s start 4 threads per vCPU, hopefully this will be enough to keep them busy.\n\n8 minutes and 30 seconds. Holy hell. 21% faster than i3. For reference, the same test on the largest c4 (c4.8xlarge) takes 11 minutes and 32 seconds. The c5 instance is 26% faster in this case.\n\nThis is amazing. I can\u2019t wait to test this again once NVMe storage is available on c5!\n\nThanks for reading and happy testing."
    },
    {
        "url": "https://medium.com/@julsimon/10-more-deep-learning-projects-based-on-apache-mxnet-a2dababe455f?source=user_profile---------41----------------",
        "title": "10 (more) Deep Learning projects based on Apache MXNet",
        "text": "In a previous article, I listed 10 cool Deep Learning projects based on Apache MXNet. Well, here are 10 more, a nice mix of model implementations and applications.\n\nIf you have an MXNet project that I haven\u2019t listed to far, please get in touch!\n\nThis is an implementation of the DenseNet-BC architecture as described in the Densely Connected Convolutional Networks, by by Gao Huang, Zhuang Liu, Kilian Q. Weinberger and Laurens van der Maaten.\n\nThis architecture contains shorter connections between layers close to the input and those close to the output. They help models train more efficiently and predict more accurately.\n\nThis project implement Binary Neural Networks, as described in BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet by Haojin Yang, Martin Fritzsche, Christian Bartz and Christoph Meinel.\n\nThese networks use weights are binary values! At the cost of minimal accuracy loss, these networks are both much smaller and much faster than their floating-point counterparts.\n\nThis is an implementation of the Mask R-CNN architecture, based on the self-titled paper by Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r and Ross Girshick\n\nThis architecture is an evolution of Fast R-CNN and does a very good job at object segmentation. If case you didn\u2019t know, TuSimple build autonomous driving systems :)\n\nThis project performs object detection based on the YOLO9000: Better, Faster, Stronger research paper by Joseph Redmon and Ali Farhadi.\n\nAt 40 frames per second, YOLOv2 gets 78.6 mean average precision, \u201coutperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster\u201d.\n\nThis project implements the model described in STN-OCR: A single Neural Network for Text Detection and Text Recognition, by Christian Bartz, Haojin Yang and Christoph Meinel.\n\nThis model is a simple CNN that does a good job at detecting head poses.\n\nThis project implements an MXNet version of the model described in Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields by Zhe Cao, Tomas Simon, Shih-En Wei and Yaser Sheikh.\n\nThis tutorial shows you how to build a sentiment analysis model , based on Convolutional Neural Networks for Sentence Classification by Yoon Kim. The author provides clear notebooks using both Keras and MXNet. Very nicely done!\n\nLet\u2019s not forget mobile application developers. These twin projects show you how to use an MXNet model in Android and iOS apps.\n\nThat\u2019s it for today. Thanks for reading."
    },
    {
        "url": "https://medium.com/@julsimon/enabling-deep-learning-in-iot-applications-with-apache-mxnet-dde112ec060f?source=user_profile---------42----------------",
        "title": "Enabling Deep Learning in IoT applications with Apache MXNet",
        "text": "Apache MXNet [1] is an Open Source library for Deep Learning. Thanks to a high-level API available in several languages (Python, C++, R, etc.), software developers and researchers can build Deep Learning models to help their applications make smarter decisions.\n\nHowever, many state of the art models have hefty compute, storage and power consumption requirements which make them impractical \u2014 or even impossible \u2014 to use on resource-constrained devices. For example, the 500MB+ VGG-16 Convolution Neural Network (CNN) model is too large to fit on a Raspberry Pi 3, which is not a tiny IoT device in itself. Other models may fit, but they usually suffer from pretty slow inference times, a significant problem when fast processing is required.\n\nIs IoT then doomed to helplessly watch the AI revolution go by?\n\nOf course not. Apache MXNet is IoT-friendly in several ways. In addition, several AWS services also make it pretty easy to deploy MXNet models at the Edge. Let\u2019s dive deeper.\n\nOne of the key features of Apache MXNet is lazy evaluation: data processing operations are only executed when strictly necessary. This allows many optimization techniques to be applied, such as avoiding unnecessary calculations or reusing memory buffers. Obviously, this behavior greatly contributes to speed and memory efficiency, both key advantages for IoT projects.\n\nIn the last few years, significant advances have been made in shrinking Deep Learning models without losing accuracy. Thanks to operations like pruning (removing useless connections), quantization (using smaller weights and activation values) and compression (encoding weights and activation values), researchers have managed to compress large CNNs by a factor of 35 or more [2]. Even complex models now end up in the 5\u201310MB range, definitely within reach of smaller IoT devices.\n\nAmazingly, some of these bleeding edge techniques are available in Apache MXNet. You can use mixed precision training, which relies on 16-bit floats instead of 32-bit floats to deliver 2x compression with no loss of accuracy [3]. Thanks to a recent research project, it\u2019s also possible to use Binary Neural Networks, where weights are encoded using only +1 and -1 values. This technique yields 20x to 30x compression, with only limited loss of accuracy [4].\n\nOf course, as they involve less computation, smaller models also tend to be faster. Another technique to speed up MXNet is to use an acceleration software library such as Intel MKL or NNPACK.\n\nThese libraries provide accelerated math processing routines that are critical to the performance of Deep Learning. Both support the Intel architecture, with NNPACK also supporting ARM v7 and v8, both popular choices for embedded applications. In the same vein, MXNet can leverage performance-oriented libraries for image processing and memory allocation, namely libjpeg-turbo, gperftools and jemalloc.\n\nAll these need to be configured at build time. You\u2019ll find detailed instructions on the MXNet website.\n\nPerformance is great, but what about deployment? How can IoT devices living at the edge of the network use Deep Learning capabilities?\n\nMXNet models can be combined with AWS Lambda, a compute service that lets you run code without provisioning or managing servers. Lambda functions can be triggered by many different types of events, such as an IoT message matching a rule defined on the AWS IoT gateway. Thus, embedded applications that may be too constrained or too rigid to support on-device deployment can rely on cloud-based models in a simple and scalable way.\n\nOn more powerful IoT devices, AWS Greengrass provides a local Lambda execution environment able to sync back and forth with the Cloud when network connectivity is available: you\u2019ll find a detailed example in this blog post. This service can be used to deploy and update MXNet models embedded in Lambda functions, now allowing IoT devices to run inference locally without any cloud-based support.\n\nAs you can see, Deep Learning and IoT are not worlds apart. Quite the contrary, in fact. We can\u2019t wait to see the devices and applications that will be built on top of Apache MXNet and AWS services. Science is definitely catching up with fiction: exciting times!\n\nThank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-7-son-we-need-to-talk-5a910aa642d1?source=user_profile---------43----------------",
        "title": "Johnny Pi, I am your father \u2014 part 7: son, we need to talk",
        "text": "Previously, we used an AWS IoT button to trigger object detection on our robot. However, this is a pretty poor way to interact. Wouldn\u2019t it be just simpler and more fun to speak to it? Of course!\n\nIn this post, I\u2019ll walk you through an Alexa skill that I wrote to send voice commands to the robot.\n\nYou can definitely set up Alexa on the Pi itself: the AlexaPi project lets you connect a Pi to the Alexa Voice Service (AVS) and process voice commands, provided that you connected a microphone on one of the USB ports (duh).\n\nThe setup process is a little hairy, but I did get things to work. Here\u2019s a quick demo.\n\nHowever, the overall results were pretty poor because my cheap USB microphone is absolute crap!\n\nAnyway, even with a decent mike, the robot obviously needs to be close enough to hear you\u2026 and I wanted to be able to talk to a remote robot as well :) For these reasons, I decided not to go the AlexaPi way. Instead, I opted for an Amazon Echo device with a custom skill.\n\nAs you probably know, an Alexa skill requires two components:\n\nHere, the interaction model is pretty simple. I\u2019ll either ask the robot to move around or to look at objects or faces. Thus, we\u2019ll need to define two intents: DirectionIntent and SeeIntent.\n\nAs far as the Lambda function is concerned, it will use AWS IoT to send the proper MQTT messages to the robot, just like we\u2019ve done with the joystick and the IoT button. It worked well so far and if it ain\u2019t broken, I sure ain\u2019t gonna try to fix it :*)\n\nThe movement commands we\u2019d like to send the robot are what you\u2019d expect: right, left, forward, backward, hold, slower, faster. Let\u2019s define a custom slot, called {Direction}, with all these values.\n\nWe\u2019ll also ask the robot to look at objects and faces. For this purpose, let\u2019s create a second custom slot, called {Target}, holding two possible values: \u2018object\u2019 and \u2018faces\u2019.\n\nOnce you\u2019re done, this is how your intent schema should look.\n\nThen, we need to come up with a number of different utterances that we\u2019re likely to use. Here are some examples:\n\nSome combinations do sound a little weird, such as \u201cjust turn forward\u201d, but nothing that would really require us to create different intents for left/right and forward/backward.\n\nAgain, we have to create a custom slot, called {Target}, holding two possible values: \u2018object\u2019 and \u2018faces\u2019. Here are some of the utterances:\n\nI\u2019m sure you\u2019ll have additional ideas, just add them to your list.\n\nWe\u2019re finished with the interaction model. Let\u2019s now take care of the Lambda function which will perform the actual processing.\n\nStarting from a vanilla skill and customizing is the simplest way to get this done. Thus, I will only highlight the parts that are specific to this skill.\n\nFirst, we need to create AWS IoT credentials for the skill (certificate, key pair, IAM policy). We\u2019ve done this many times before and the process is always the same: just repeat the same steps and download all credentials on your local machine.\n\nHere\u2019s the IAM policy you should attach to the skill thing in AWS IoT and to the Lambda function itself.\n\nTime to write the function itself. First, we have import the AWS IoT SDK and the IoT credentials. Let\u2019s also define a couple of helper functions.\n\nThis is pretty easy :) Just change the messages in the vanilla skill functions.\n\nNothing really complicated here:\n\nIn this context, I found that it was more reliable to connect and disconnect every time. Not sure why, although the latency is hardly noticeable. Feel free to try something different.\n\nThere\u2019s a tiny hack in this code that I was too lazy to fix. We can\u2019t use the word \u2018stop\u2019, as it\u2019s reserved by Alexa to stop the skill. That\u2019s why I use \u2018hold\u2019 instead: although I\u2019m actually sending \u2018stop\u2019 to the robot as this is the command I\u2019ve implemented there ;)\n\nThe handler function for SeeIntent has a similar structure, but it is a little more complex. Indeed, we need to handle two different cases:\n\nIn both cases, we\u2019re also instruction the robot to tweet the picture. You can see past pictures here.\n\nThe last bit we need to implement is the intent dispatcher. Pretty straightforward.\n\nWe\u2019re now done with the Lambda function. Time to package and deploy it.\n\nHere are the corresponding commands, which you need to run in the directory holding the function code.\n\nAll right, all the pieces are in place, let\u2019s test them. Here\u2019s a recent video from the AWS Loft in London. My session was an introduction to Deep Learning and the actual demo starts at the 59:30 mark.\n\nPretty cool, don\u2019t you think? As usual, you\u2019ll find all code on Github.\n\nThat\u2019s it for today. In the next and possibly last part of this story, we\u2019ll have the robot send text strings back to the Alexa skill instead of speaking through Amazon Polly. Until then, thank you for reading and keep building."
    },
    {
        "url": "https://medium.com/@julsimon/building-fpga-applications-on-aws-and-yes-for-deep-learning-too-643097257192?source=user_profile---------44----------------",
        "title": "Building FPGA applications on AWS \u2014 and yes, for Deep Learning too",
        "text": "Field Programmable Gate Arrays (FPGA) are not shiny new technology: indeed, the first commercial product dates back to 1985. So how could they be relevant to a bleeding edge topic like Deep Learning? Then again, neural networks themselves go back to the late Forties, so... There might be something afoot, then. Read on :)\n\nUntil quite recently, the world of computing has been unequivocally ruled by CPUs. However, for a while now, there have been ever-growing doubts on how sustainable Moore\u2019s Law really is.\n\nTo prevent chips from melting, clock speeds have been stagnating for years. In addition, even though lithography processes still manage to carve smaller and smaller features, we\u2019re bound to hit technology limits rather sooner than later: in the latest Intel Skylake architecture, a transistor is 100 atoms wide.\n\nAnother factor is helping foment the coming coup against King CPU: the emergence of new workloads, such as genomics, financial computing or Deep Learning. As it happens, these involve staggering amounts of mathematical computation which can greatly benefit from massive parallelism (think tens of thousands of cores). Sure, it\u2019s definitely not impossible to achieve this with CPU-based architectures \u2014 here\u2019s a mind-boggling example \u2014but in recent years, a very serious contender has emerged: the Graphics Processing Unit (GPU), spearheaded by Nvidia.\n\nEquipped with thousands of floating-point cores, a typical GPU is indeed a formidable crunching machine, able to deliver proper hardware parallelism at scale. Soon enough, researchers have understood how these chips could be applied to Machine Learning and Deep Learning at scale.\n\nAnd thus began the Age of the GPU, leading up to the design of computing monsters such as the Nvidia V100: 21.1 billion transistors, 815 square millimeters (1.36 square inch for my US friends), 5120 CUDA cores, 640 tensor cores. Surely, this should be enough for anyone\u2026 right?\n\nWhen it comes to brute force computing powers, GPUs are unmatched.\n\nHowever, for some applications, they don\u2019t deliver the most bang for your buck. Here are some reasons why you might not want to use a GPU:\n\nWhat about Deep Learning specifically? Of course, we know that GPUs are great for training: their massive parallelism allows them to crunch large data sets in reasonable time. To optimize throughput and put all these cores to good use, we don\u2019t forward single samples through the model: we use batches of samples instead.\n\nHowever, training is only half the story: what about inference? Well, it depends. If your application can live with the latency required to collect enough samples to forward a full batch, then you should be fine. If not, then you\u2019ll have to run inference on single samples and it\u2019s likely that throughput will suffer.\n\nIn order to get the best inference performance, the logical step would be to use a custom chip. For decades, the choice has been pretty simple: either build an Application Specific Integrated Circuit (ASIC) or use an FPGA.\n\nAn ASIC is a fully custom design, which is mass-produced and deployed in devices. Obviously, you get to tweak it and optimise it in the way that works best for your application: best performance, best power efficiency, etc. However, designing, producing and deploying an ASIC is a long, expensive and risky process. You\u2019ll be lucky to complete it in less than 18 months.\n\nThis is the route that Google took for their TPU chip. They did it in 15 months, which is impressive indeed. Just wonder how long it would take you.\n\nOf course, ASICs are inflexible: if your application requirements change significantly, you have to start all over again.\n\nAs their name implies, FPGAs are (re)programmable logic circuits. A typical FPGA includes hundreds of thousands and sometimes millions of logic cells, thousands of Digital Signal Processing (DSP)\u201cslices\u201d as well as very fast on-chip memory.\n\nNot everyone enjoys digital circuits and Boolean algebra, so let\u2019s keep this simple: FPGAs are Lego digital architecture. By mixing and matching the right logic blocks, a system designer armed with the right tools can pretty much implement anything\u2026 even an Apple ][ :)\n\nHistorically, building FPGA applications has required the purchase of costly software and hardware tools in order to design, simulate, debug, synthesize and route custom logic designs. Let\u2019s face it, they made it hard to scale engineering efforts.\n\nDesigning custom logic for FPGAs also required the mastery of esoteric languages like VHDL or Verilog\u2026 and your computer desktop would look something like this. Definitely not for everyone (including myself).\n\nFortunately, developers now have the option to build FPGA applications in C/C++ thanks to the SDAccel environment and OpenCL. The programming model won\u2019t be unfamiliar to CUDA developers :)\n\nThey rely on the Xilinx Ultrascale+ VU9P chip. Here are some of the specs (PDF): over 2.5 million System Logic Cells (specs \u2014 PDF) and 6,840 DSP slices (specs \u2014 PDF). Yes, it\u2019s a beast!\n\nIn order to simplify FPGA development, AWS also provides an FPGA Developer AMI coming with the full Xilinx SDx 2017.1 tool suite\u2026 and a free license :) The AMI also includes the AWS SDK and HDK to help you build and manage your FPGA images: both are Open Source and available on Github.\n\nThe overall process would look something like this:\n\nAt the core of Neural Networks lies the \u201cMultiply and Accumulate\u201d operation, where we multiply inputs by their respective weights and add all the results together. This can be easily implemented using a DSP slice. Yes, I know its a very simple example, but more complex operations like convolution or pooling could be implemented as well.\n\nOf course, modern FPGAs have tons of gates and they\u2019re able to support very large models. However, in the interest of speed, latency and power consumption, it would make sense to try to minimize the number of gates.\n\nThere\u2019s a lot of ongoing research to simplify and shrink Deep Learning models with minimal loss of accuray. The three most popular techniques are:\n\nQuantization, i.e. using integer weights (8, 4 or even 2-bit) instead of 32-bit floats. Less power is required: less gates are required to implement the model, and integer operations are cheaper than floating-point operations. Less memory is also required, as we save memory and shrink model size.\n\nPruning, i.e. removing connections that play little or no role in predicting successfully. Computation speed goes up, latency goes down. Less memory is required, as we save memory and reduce model size.\n\nCompression, i.e. encoding weights, as they\u2019re now integer-based and exhibit a smaller set of possible values. Less memory is required, as we save memory and reduce model size even further.\n\nAs a bonus, models may shrink so much that on-chip SRAM could become a viable option. This would help in saving even more power (as SRAM is much more efficient than DRAM) as well as speeding up computation (as on-chip RAM is always faster to access than off-chip RAM).\n\nUsing these techniques and more, researchers have obtained spectacular results.\n\nSome of you may still remember the ill-fated LISP machines and shiver at the thought of AI hardware. However, times have changed and researchers are moving fast here as well.\n\nThe topic picked up even more speed when Nvidia recently announced a new initiative, Nvidia Hardware for Deep Learning. In a nutshell, this includes Open Source hardware blocks implemented in Verilog that may be used to build Deep Learning accelerators for IoT applications:\n\nThis FPGA-based initiative is coming from the company that brought us GPUs in the first place, which should definitely raise a few eyebrows. In my humble opinion, we should definitely pay attention: no one would know more than Nvidia about GPUs strengths and weaknesses and about speeding up Deep Learning computations. A exciting and clever move indeed.\n\nI don\u2019t have a crystal ball, but here are a few closing predictions based on extensive analysis of my gut feelings :-P\n\nWell, we made it. No code this time, but I hope you still enjoyed this :)"
    },
    {
        "url": "https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-6-now-im-pushing-your-button-ha-7a591c46ab74?source=user_profile---------45----------------",
        "title": "Johnny Pi, I am your father \u2014 part 6: now I\u2019m pushing your button, ha!",
        "text": "In previous posts, we gave our robot the gift of sight thanks to Amazon Rekognition and Apache MXNet, as well as the gift of speech thanks to Amazon Polly.\n\nIn this post, I\u2019ll show you how to send orders to the robot with an AWS IoT button. Specifically, pushing the button will trigger object detection using the MXNet model.\n\nJust like for the robot and the joystick, we need to register the IoT button in AWS IoT: create a certificate, a key pair, an IAM policy, etc. The process is always the same, so just repeat the same steps and download all credentials on your local machine.\n\nThis couldn\u2019t be simpler. Just press the button for 5 seconds and a new Wi-Fi hotspot will appear. Connect to it, enter the Wi-Fi settings and the endpoint of the IoT gateway, upload the credentials and you\u2019re done.\n\nNow let\u2019s take care of the Lambda function which will be invoked when we press the button.\n\nYou didn\u2019t think you\u2019d escape IAM,did you? ;) Of course, we need to create a role \u2014 let\u2019s call iotButtonLambdaRole \u2014 and a policy allowing the Lambda function to connect to AWS IoT and publish a message to the MQTT topic. Just go the the IAM console, create the role as a Lambda service role and attach the following policy.\n\nThe last statement is important. It allows the Lambda function to log in CloudWatch logs and you might need it for debugging later on ;)\n\nWe\u2019ll keep things very simple: whenever the button is pushed and whatever the click type (short, long or double), we\u2019ll post an message to the JohnnyPi/see MQTT topic.\n\nHere\u2019s the code for the Lambda function.\n\nAs you can see, this is pretty straightforward. We\u2019re reusing the same IoT code as for the robot. Just make sure you used the right IoT credentials in iot_config.py.\n\nWe need to embed the AWS IoT SDK in the function package, so let\u2019s first install it locally.\n\nThen, we simply need to zip everything\u2026\n\n\u2026and we can create the function with the AWS CLI.\n\nOr you can click in the console, you devil you!\n\nWe\u2019re missing one last part: the IoT rule which will trigger the Lambda function whenever a MQTT message is sent by the IoT button.\n\nJust head out to the IoT console and create the rule. It couldn\u2019t be simpler: select every single message\u2026\n\nJust push the button and see what happens :) If you run into issues, here are some troubleshooting tips:\n\nAt some point, it\u2019s going to work and it should look something like this. The video below is in French but you\u2019ll get the point, I\u2019m sure. Make sure to enable automatic subtitles for extra fun!\n\nThat\u2019s it for today. As usual, you\u2019ll find all code on Github. In the next post, we\u2019ll build an Alexa skill to issue voice commands!"
    },
    {
        "url": "https://medium.com/@julsimon/imagenet-part-2-the-road-goes-ever-on-and-on-578f09a749f9?source=user_profile---------46----------------",
        "title": "ImageNet \u2014 part 2: the road goes ever on and on \u2013 Julien Simon \u2013",
        "text": "In a previous post, we looked at what it took to download and prepare the ImageNet dataset. Now it\u2019s time to train!\n\nThe MXNet repository has a nice script, let\u2019s use it right away.\n\nEasy enough. How fast is this running?\n\nAbout 400 images per second, which means about 53 minutes per epoch. Over three and a half days for 100 epochs. Come on, we don\u2019t want to wait this long. Think, think\u2026 Didn\u2019t we read somewhere that a larger batch size will speed up training and help the model generalize better? Let\u2019s figure this out :)\n\nIn this context, the largest batch size means the largest that will fit on one of our GPUs: each of them has 11439MB of RAM.\n\nUsing the nvidia-smi command, we can see that the current training only uses about 1500MB. As we didn\u2019t pass a batch size parameter to our script, it\u2019s using the default value of 128. That\u2019s not efficient at all.\n\nBy trial and error, we can quickly figure out that the largest possible batch size is 1408. Let\u2019s give it a try.\n\nThat\u2019s more like it: the GPU RAM is maxed out. Training speed should be much higher\u2026 right?\n\nNope. Something is definitely not right. Let\u2019s pop the hood.\n\nGPU RAM is fully used, but what about the actual GPU cores? As it turns out, there\u2019s an easy way to find out. Let\u2019s look at the \u201cvolatile GPU information\u201d returned by nvidia-smi, it\u2019ll give us an idea on how hard GPUs actually work.\n\nThat\u2019s not good. One second, our GPUs are running at 100% and the next they\u2019re idle.\n\nIt looks like they\u2019re stalling over and over, which probably means that we can\u2019t maintain a fast enough stream of data to keep them busy all the time. Let\u2019s take a look at our Python process\u2026\n\nThe RecordIO files storing the training set are hosted on an EBS volume (built from a snapshot, as explained before). Our Python script reads the images using a default value of 4 threads. Then, it performs data augmentation on them(resizing, changing aspect ratio, etc.) before feeding them to the GPUs. The bottleneck is probably in there.\n\nIdle time is extremely high (id=80.5%), but there are no I/O waits (wa=0%). It looks like this system is simply not working hard enough. The p2.16xlarge has 64 vCPUs, so let\u2019s add more decoding threads.\n\nOur GPUs look pretty busy now.\n\nWhat about our python process?\n\nIt\u2019s working harder as well, with all 32 threads running in parallel. Still no I/O wait in sight, though (thank you EBS). In fact, we seem to have a nice safety margin when it comes to I/O: we could certainly add more threads to support a larger batch size or faster GPUs if we had them.\n\nWhat about training speed? It\u2019s nicely crusing at a stable 700+ images per second. That\u2019s a 75% increase from where we started, so it sure was worth tweaking.\n\nAn epoch will complete in 30 minutes, which gives us just a little over 2 days for 100 epochs. Not too bad.\n\nAt on-demand price in us-east-1, 50 hours of training would cost us $720. Ouch. Surely we can optimize this as well?\n\nLet\u2019s look at spot prices for the p2.16xlarge. They vary a lot from region to region, but here\u2019s what I found in us-west-2 (hint: using the describe-spot-price API should help you find good deals really quick).\n\nYes, ladies and gentlemen. That\u2019s an 89% discount right there. Training would now cost something like $80.\n\nAs you can see, there is much more to Deep Learning than data sets and models. Making sure that your infrastructure runs at full capacity and at the best possible price also requires some attention. Hopefully, this post will help you do both.\n\nIn the next post, I think we\u2019ll look at training ImageNet with Keras, but I\u2019m not quite sure yet :D\n\nAs always, thank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/imagenet-part-1-going-on-an-adventure-c0a62976dc72?source=user_profile---------47----------------",
        "title": "ImageNet \u2014 part 1: going on an adventure \u2013 Julien Simon \u2013",
        "text": "When it comes to building image classifiers, ImageNet is probably the most well known data set. It\u2019s also used for the annual ILSVRC competition, where researchers from all over the world compete to build the most efficient models.\n\nAs previously discussed, models are frequently trained on ImageNet before being fine-tuned on other image sets. Fine-tuning is a much faster process and a great way to get very good accuracy in just a few epochs.\n\nStill, what does it take to actually grab ImageNet and prepare it for training. Let\u2019s find out. We\u2019ll start with Apache MXNet, but who knows where we\u2019ll end up?\n\nClocking in at 150 GB, ImageNet is quite a beast. It holds 1,281,167 images for training and 50,000 images for validation, organised in 1,000 categories. We\u2019re pretty far from MNIST or CIFAR-10!\n\nThis creates all kinds of interesting problems that need to be solved before training even starts, namely:\n\nWe\u2019re going to look at all these steps. As always, the devil is in the details.\n\nWe need to download 150GB. That\u2019s gonna take a while (think days) and it\u2019s gonna fill, well, just about 150GB of disk space. So you\u2019d better plan ahead.\n\nHere\u2019s how I did it: I started a t2.large instance (but t2.micro should work too), which is more than powerful enough to handle the download. I also attached a 1000GB EBS volume to it (because we\u2019ll need additional space later on). As I/O performance is not important here, I picked the least expensive volume type (sc1).\n\nThen I ssh\u2019ed into the instance, formatted the volume, mounted it on /data and chown\u2019ed it recursively to ec2-user:ec2-user. I hope you don\u2019t need me to show you these steps, but here\u2019s the tutorial, just in case ;)\n\nIt starts easy. Head out to the ImageNet website, register, accept conditions and voila. You\u2019ll get a username and an access key which will let you download the data set, which is composed of several very large files: no way we can right click and \u201csave as\u201d.\n\nFortunately, one of the Tensorflow repositories includes a nice download script, download_imagenet.sh. It\u2019s quite straightforward, this is how to use it.\n\nAnd then, you have to wait for a bit: my download took about 5 days\u2026\n\nOnce the script has completed, your directory should look like this.\n\nLet\u2019s take a look at the data set. If you list the imagenet/train directory, you\u2019ll see 1,000 directories, each of them holding images for a given ImageNet category. Yes, they have weird names, which is why you need to grab this file to know what\u2019s what, e.g. n02510455 is the category for giant pandas.\n\nIf you list the imagenet/validation directory, you see 50,000 images in a single directory. That\u2019s not really practical, we\u2019d like to have them in 1,000 directories as well. This script will take care of it: simply run it inside the validation directory.\n\nDuring training, we could definitely load images from disk. However, this would require a lot of I/O, especially when using multiple GPUs: these beasts are very hungry and you need to keep feeding them with data at the proper throughout. Failing to do so will stall the GPUs and your training speed will drop (more on this in a future post).\n\nAnother problem arises when distributed training is used, i.e. when multiple GPU instances are learning the same data set. Sure, we could copy the full data set to each instance, but that may be impractical for huge ones.\n\nIn order to solve both issues, we\u2019re going to convert the data set into RecordIO files. This compact format is both I/O efficient and easily shareable across instances.\n\nThe process is pretty simple: we\u2019re going to pack the training set and the validation set in their own RecordIO file. We\u2019re also going to resize and compress the images a bit to save some space: this won\u2019t have any impact on training quality, since most of the ImageNet models require 224x244 images. Feel free to create plenty of threads to maximize throughput :)\n\nLet\u2019s start with the validation set. It only takes a couple of minutes.\n\nNow, let\u2019s do the same thing for the training set. This is going to run for a while (about 1h30 on my t2.xlarge).\n\nAt this point, losing all this would suck beyond belief, wouldn\u2019t it? Let\u2019s make sure we back everything up in S3. Just create a bucket and sync it with /data. Yes, that\u2019s going to take a while.\n\nOnce the backup is over, you should:\n\nDeploying the data set to a new GPU instance is now as easy as:\n\nThis will only take a few seconds and can easily be scripted and scaled to as many instances as needed (aws ec2 create volume, aws ec2 attach-volume). For full automation, you could perform these operations as User Data commands at instance launch.\n\nSure, it takes a while to download ImageNet, but thanks to the flexibility of EBS, we\u2019re now able to deploy it as many times as needed in just a few seconds. Of course, you can easily apply this fast and cost-effective technique to other data sets :)\n\nIn the next post, we\u2019ll train a model from scratch and focus on making sure that we get as much performance as possible from our GPU instance.\n\nThat\u2019s it for today. As always, thank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-5-adding-mxnet-for-local-image-classification-bc27a5fd2c27?source=user_profile---------48----------------",
        "title": "Johnny Pi, I am your father \u2014 part 5: adding MXNet for local image classification",
        "text": "In the previous post, we learned how to use Amazon Rekognition to let our robot detect faces and labels in pictures taken with its own camera. This is a very cool feature, but could we do the same thing with local resources only, i.e. without relying on a network connection and a cloud-based API?\n\nOr course we can. In this post, I\u2019ll show you how to classify images using a local Deep Learning model. We\u2019ll use the Apache MXNet library, which I\u2019ve covered extensively in another series. Let\u2019s get to work.\n\nWe\u2019re going to use the same MQTT topic as for Rekognition, namely JohnnyPi/see: we\u2019ll just add an extra word to specify whether Rekognition or MXNet should be used.\n\nSince we\u2019re not invoking any additional AWS API, there is no additional IAM setup required (and there was much rejoicing!).\n\nWe have two options: build from source (as explained in this post) or install directly with pip. Let\u2019s go for the second option, which is obviously easier and faster.\n\nNow what about the model? Given the limited processing power and storage capabilities of the Pi, it\u2019s obvious we\u2019re never be able to train a model with it. Fortunately, we can pick pre-trained models from MXNet\u2019s model zoo. As we found earlier, we need a model that is small enough to fit in the Pi\u2019s memory (only 1GB). Inception v3 is such a model.\n\nOur version of Inception v3 has been pre-trained on the ImageNet dataset, which holds over 1.2 million pictures of animals and objects classified in 1,000 categories. Let\u2019s fetch it from the model zoo.\n\nOK, now it\u2019s time to write some code!\n\nWith MXNet added into the mix, here\u2019s what should now happen when we send an MQTT message to the JohnnyPi/see topic.\n\nFirst, we\u2019ll take a picture with the Pi camera. If the message starts with \u2018reko\u2019, we\u2019ll use Rekognition just like we did in the previous post. No changes here.\n\nIf the message starts with \u2018mxnet\u2019, then we will:\n\nThis is what the updated callback looks like.\n\nI\u2019ve already covered these exact steps in full detail in a previous post, so I\u2019ll stick to a quick summary here.\n\nIn inception.load_image(), we use OpenCV and Numpy to load the image and transform it a 4-dimension array corresponding to the input layer on the Inception v3 model: (1L, 3L, 224L, 224L) : one sample with three 224x244 images (red, green and blue).\n\nIn inception.predict(), we forward the sample through the model and receive a vector of 1,000 probabilities, one for each class.\n\nIn inception.get_top_categories(), we sort these probabilities and find the top 5 classes.\n\nThis part is almost identical to what we did with Rekognition, so no need to repeat ourselves.\n\nOnce again, we\u2019ll use MQTT.fx to send commands to the JohnnyPi/see topic:\n\nLet\u2019s try another one.\n\nAnd let\u2019s tweet a last one:\n\nThe output is indeed:\n\nOur robot is now able to move, measure distance to objects, detect faces and classify objects. In the next part, I\u2019ll show you how to use an IoT button to send commands to the robot. Once this this complete, we\u2019ll move on to building an Amazon Echo skill for voice control. Stay tuned :)\n\nAs always, thanks for reading."
    },
    {
        "url": "https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-raspberry-pi-edition-e444b446a180?source=user_profile---------49----------------",
        "title": "Speeding up Apache MXNet with the NNPACK library (Raspberry Pi edition)",
        "text": "In a previous post, I showed you how to add the NNPACK library to Apache MXNet and how this did speed up CPU-based inference for networks such as Alexnet or VGG by a factor of 2 to 3. Definitely worth the effort.\n\nI also mentioned that NNPACK supports ARM 7 processors with the NEON instruction set (similar to AVX for Intel chips), as well as ARM v8 processors (which include NEON by default).\n\nAs the Raspberry Pi 3 is built on the Cortex-A53 CPU (ARM v8-based), I figured I\u2019d give it a go and see how NNPACK could help us for embedded applications.\n\nThe steps required to build NNPACK and MXNet are strictly identical to the ones we used on our AWS instance, so please refer to the previous article.\n\nGiven the limited RAM available on the Raspberry Pi 3, we won\u2019t be able to load larger models like VGG16 (as I found out when I first tried loading models on the Pi). Thus, I had to stick to the multi-layer perceptron and the Alexnet convolutional network, with the largest batch sizes possible.\n\nWow. When processing a single image, we get a consistent 4x speedup. Then, things tend to level (not sure why) before improving again when we really start processing larger batch sizes: 4x speedup is reached again at batch size 256.\n\nNow, let\u2019s try Alexnet now and see what performance boost we get for convolutional networks.\n\nVery impressive. We still get 4x speedup for single-image prediction and exceed 6x speedup for bulk prediction.\n\nWe did get a nice speedup on Intel processors, but the optimised code of NNPACK really shines on the less powerful ARM v8. MXNet inference gets a massive performance boost for both single images and larger batches. Kudos to Marat Dukhan, the author of NNPACK.\n\nThat\u2019s it for today, thanks for reading!"
    },
    {
        "url": "https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-4-adding-cloud-based-vision-8830c2676113?source=user_profile---------50----------------",
        "title": "Johnny Pi, I am your father \u2014 part 4: adding cloud-based vision",
        "text": "In the previous post, we learned how to use Amazon Polly to let our robot speak. I hope you had fun with that :)\n\nIn this post, I\u2019ll show you how to take picture with the robot\u2019s camera and how to use Amazon Rekognition to identify faces and objects\u2026 and yeah, we\u2019ll send some tweets.\n\nOnce again, all commands will be sent through a dedicated MQTT topic, named JohnnyPi/see. We have to update the thing\u2019s IAM policy to allow it to subscribe to this new topic.\n\nJust go to the IAM console, locate the proper policy and add the following statement.\n\nAs we did for Polly, he have to allow the robot to call the Rekognition API. Let\u2019s use the AWS CLI again and grant the robot the appropriate IAM permissions.\n\nOK, with IAM out of the way, let\u2019s write some code. Here\u2019s what should happen when we send an MQTT message to the JohnnyPi/see topic:\n\nAs usual, we need to add a callback for messages posted to the topic. This is what the function looks like.\n\nIt should be quite explanatory. Let\u2019s look at the important steps in more detail.\n\nThe Pi camera API is nice and simple. Open the camera, take a picture, close the camera. Why can\u2019t programming be always this simple?\n\nFirst, we have to copy the picture to S3. Make sure to use your own bucket in awsUtils.py.\n\nNext, we invoke two Rekognition APIs:\n\nThanks to the face details provided by Rekognition, we\u2019re now able to locate the position of each face in the picture. Using the Pillow library, we\u2019re going to draw a rectangle around each face and add a legend with the face count (\u2018Face0\u2019, \u2018Face1\u2019, etc.).\n\nDrawing a rectangle around each face requires a bit more work that I\u2019d have liked. First, Rekognition returns fractional coordinates for the bounding box, which need to be converted into absolute pixel values. Second, the Pillow API to draw rectangles doesn\u2019t allow line width to be set: for high resolution pictures, the resulting rectangle tends to be invisible :-/ Thus, I\u2019m drawing lines instead. If you\u2019re curious about Pillow, all code is located in RekognitionUtils.py. If not, no worries, you can happily ignore this. It seems to work fine ;)\n\nOnce rectangles and legends have been added, the new image is saved locally and the number of faces is returned.\n\nUsing the number of faces, we build a text string that Polly will speak. In the same way, we build a text string about labels: more details in generateMessages().\n\nWe simply reuse the code we wrote in part 3.\n\nThe first step is to create a Twitter developer account and get API credentials. Then, we can use the super simple Tweepy library to send a tweet.\n\nThat\u2019s it, we have everything we need. Let\u2019s test!\n\nAs previously, I\u2019m using MQTT.fx to publish messages to the JohnnyPi/see topic. Here\u2019s the output for Rekognition only.\n\n48\u201368? WTF. I should have a chat with the Product Manager about the age range. Or maybe I simply need some sleep :)\n\nHere\u2019s a second try with both Rekognition and Twitter.\n\nAll right, the age range is more like it. I guess the moral of the story is: don\u2019t smile.\n\nAs usual, you\u2019ll find the full code on Github.\n\nIn the next part, we\u2019ll keep expanding our robot vision skills with a pre-trained MXNet model for image recognition. More silliness in sight, no doubt!\n\nUntil then, have fun and as always, thanks for reading."
    },
    {
        "url": "https://medium.com/@julsimon/keras-shoot-out-part-3-fine-tuning-7d1548c51a41?source=user_profile---------51----------------",
        "title": "Keras shoot-out, part 3: fine-tuning \u2013 Julien Simon \u2013",
        "text": "Using Keras and the CIFAR-10 dataset, we previously compared the training performance of two Deep Learning libraries, Apache MXNet and Tensorflow.\n\nIn this article, we\u2019ll continue to explore this theme. I\u2019ll show you how to:\n\nUsing the same script (~/keras/examples/cifar10_resnet50.py), I trained a Resnet-50 model on CIFAR-10 using first MXNet 0.11, then Tensorflow 1.2. Here is the setup I used:\n\nEven with 8 GPUs, training takes time: about 2h for MXNet and about 3h30 for Tensorflow. Pretty heavy lifting! I uploaded the models to my own personal model zoo: feel free to grab them and run your own tests :)\n\nNow let\u2019s see how we can load our models.\n\nDepending on the backend configured in ~/.keras/keras.json, we have to load one model or the other. Using keras.backend.backend(), it\u2019s easy to figure which one to pick. Then, we simply call keras.models.load_model().\n\nWhat about setting the number of GPUs using for training? For Tensorflow, we\u2019ll use session parameters. For MXNet, we\u2019ll add an extra parameter to model.compile().\n\nOk, now let\u2019s take care of data.\n\nKeras provides convenient functions to load commonly used data sets, including CIFAR-10. Nice!\n\nFirst, we need to extract a given number of samples from two classes. This is the purpose of the get_samples() function:\n\nThe next step is to build the new training and test sets with the prepare_dataset() function:\n\nThe last step before actually retraining is to define the optimizer \u2014 for now, we\u2019ll use the same one as for the original training \u2014 as well as the number of GPUs \u2014 we\u2019ll stick to one, as this is the most likely setup for developers fine-tuning a model on their own machine.\n\nOK, we\u2019re now ready to train the model. First, let\u2019s predict our test set to see what the baseline is. Then, we train the model. Finally, we predict the test set again to see how much the model has improved.\n\nFull code is available on Github. Now, let\u2019s run it!\n\nLet\u2019s fine-tune the model on cars and horses (classes 1 and 7) for 10 epochs. Here are the results.\n\nThis is a MASSIVE accuracy improvement after just a few minutes of training. Just wait, we can do even better :)\n\nThe many layers in our model have already learned the car and horse classes. So, it\u2019s probably a waste of time to potentially retrain all of them. Maybe it would just be enough to retrain the last layer, i.e. the one that actually outputs the probability for the 10 classes.\n\nKeras includes a very nice feature that lets us decide which layers of a model are trainable and which aren\u2019t. Let\u2019s use it to freeze all layers but the last one and try again.\n\nHere are the results.\n\nTraining time is almost identical for both libraries. I guess the work boils down to running backpropagation on a single layer, which both libraries can do equally well on a single GPU. Scaling doesn\u2019t seem to come into play here.\n\nAccuracy is hardly impacted for Tensorflow, but there is a slight hit for MXNet. Hmmm. Maybe our optimisation parameters are not optimal. Let\u2019s try one last thing :)\n\nPicking the right hyper parameters for SGD(learning rate, etc.) is tricky. Here, we\u2019re only training for 10 epochs, which probably makes it even more difficult. One way out of this problem may be to use the AdaGrad optimizer, which automatically adapts the learning rate\n\nHere are the results.\n\nAdaGrad works its magic indeed. It helps MXNet improve its accuracy and deliver the best score in this test. Tensorflow improves as well.\n\nHere, we fine-tuned the model for two classes, but we still output 10 probabilities (one for each class). The next step would be to add an extra layer with only two outputs, forcing the model to decide on two categories only (and not ten).\n\nWe would need to modify our subset labels, i.e. one-hot encode them with only two values. Let\u2019s keep this for a future article :)\n\nFine-tuning is a very powerful way to improve the accuracy of a model in a very short period of time compared to training. It\u2019s a great technique when you can\u2019t or don\u2019t want to spend time and money (re)training complex models on large data sets. Just make sure you understand how the model was trained (data set, data format, etc.) in order to retrain it appropriately.\n\nThat\u2019s it for today. Thank you for reading."
    },
    {
        "url": "https://medium.com/@julsimon/speeding-up-apache-mxnet-with-the-nnpack-library-7427f367490f?source=user_profile---------52----------------",
        "title": "Speeding up Apache MXNet with the NNPACK library \u2013 Julien Simon \u2013",
        "text": "Apache MXNet is an Open Source library helping developers to build, train and re-use Deep Learning networks. In this article, I\u2019ll show you to speed up predictions thanks to the NNPACK library. Before we dive in, let\u2019s first discuss why we would want to do this.\n\nTraining is the step where a neural network learns how to correctly predict the right output for each sample in the data set. One batch at a time (typically from 32 to 256 samples), the data set is fed into the network, which then proceeds to minimise total error by adjusting weights (and sometimes hyper parameters) thanks to the backpropagation algorithm.\n\nGoing through the full data set is called an \u201cepoch\u201d: large networks may be trained for hundreds of epochs to reach the highest accuracy possible. This may take days or even weeks, which is where GPUs step in: thanks to their formidable parallel processing power, training time can be significantly cut down, compared to even the most powerful of CPUs.\n\nInference is the step where you actually use the trained network to predict a result from new data samples. You could be predicting with one sample at a time, for example trying to identify objects in a single picture like Amazon Rekognition does, or with multiple samples, for example trying to track a moving object in a video stream.\n\nOf course, GPUs are equally efficient at inference. However, many systems are not able to accommodate them because of cost, power consumption or form factor constraints (just think of embedded systems).\n\nThis is where the NNPACK library comes into play, as it will help us speed up CPU inference in Apache MXNet.\n\nNNPACK is an Open Source library available on Github. It implements operations like matrix multiplication, convolution and pooling in a highly optimised fashion.\n\nIf you\u2019re curious about the theory and math that help make these operations very fast, please refer to the research papers mentioned by the author in this Reddit post.\n\nNNPACK is available for Linux and MacOS X platforms. It\u2019s optimised for the Intel x64 processor with the AVX2 instruction set, as well as the ARMv7 processor with the NEON instruction set and the ARM v8.\n\nIn this post, I will use a c4.8xlarge instance running the Deep Learning AMI, which already includes some of the dependencies we need. Here\u2019s what we going to do:\n\nNNPACK uses the ninja build tool. Unfortunately, the Ubuntu repository does not host the latest version, so we need to build it from source as well.\n\nNow let\u2019s prepare the NNPACK build, as per instructions.\n\nBefore we actually build, we need to tweak the configuration file a bit. The reason for this is that NNPACK only builds as a static library whereas MXNET builds as a dynamic library: thus, they won\u2019t link properly. The MXNet documentation suggests to use an older version of NNPACK, but there\u2019s another way ;)\n\nWe need to edit the build.ninja file and the \u2018-fPIC\u2019 flag, in order to build C and C++ files as position-independent code, which is really all we need to link with the MXNet shared library.\n\nNow, let\u2019s build NNPACK and run some basic tests.\n\nWe\u2019re done with NNPACK: you should see the library in ~/NNPACK/lib.\n\nNNPACK relies on this library to accurately detect CPU information. Build instructions are very similar.\n\nThen, we need to edit build.ninja and update cflags and cxxflags again.\n\nNow, let\u2019s build cpuinfo and run some basic tests.\n\nWe\u2019re also done with cpuinfo: you should see the library in ~/cpuinfo/lib.\n\nFirst, let\u2019s install dependencies as well as the latest MXNet sources (1.1 at the time of writing). Detailed build instructions are available on the MXNet website.\n\nNow, we need to configure the MXNet build. You should edit the make/config.mk file and set the variables below in order to include NNPACK in the build, as well as the dependencies we installed earlier.\n\nNow, we\u2019re ready to build MXNet. Our instance has 36 vCPUs, so let\u2019s put them to good use.\n\nA few minutes later, the build is complete. Let\u2019s install our new MXNet library and its Python bindings.\n\nWe can quickly check that we have the proper version by importing MXNet in Python.\n\nWe\u2019re all set. Time to run some benchmarks.\n\nBenchmarking with a couple of images isn\u2019t going to give us a reliable view on whether NNPACK makes a difference. Fortunately, the MXNet sources include a benchmarking script which feeds randomly generated images in a variety of batch sizes through the following models: Alexnet, VGG16, Inception-BN, Inception v3, Resnet-50 and Resnet-152.\n\nAs nothing is ever simple, we need to fix a line of code in the script. C4 instances don\u2019t have any GPU installed (which is the whole point here) and the script is unable to properly detect that fact. Here\u2019s the modification you need to apply to~/incubator-mxnet/example/image-classification/benchmark_score.py. While we\u2019re at it, let\u2019s add additional batch sizes.\n\nTime to run some benchmarks. Let\u2019s give 8 threads to NNPACK, which is the largest recommended value.\n\nFull results are available here. As a reference, I also ran the same script on an identical instance running the vanilla 0.11.0-rc3 MXNet (full results are available here).\n\nAs we can see on the graph above, NNPACK delivers a significant speedup for Alexnet (up to 2x), VGG (up to 3x) and Inception-BN (almost 2x).\n\nFor reasons beyond the scope of this article, NNPACK doesn\u2019t (yet?) deliver any speedup for Inception v3 and Resnet.\n\nWhen GPU inference is not available, adding NNPACK to Apache MXNet may be an easy option to extract more performance from your network.\n\nIt would definitely be interesting to run the same benchmark on the upcoming c5 instances (based on the latest Intel Skylake architecture) and on a Raspberry Pi. More articles to be written :)"
    },
    {
        "url": "https://medium.com/@julsimon/keras-shoot-out-part-2-a-deeper-look-at-memory-usage-8a2dd997de81?source=user_profile---------53----------------",
        "title": "Keras shoot-out, part 2: a deeper look at memory usage",
        "text": "In a previous article, I used Apache MXNet and Tensorflow as Keras backends to learn the CIFAR-10 dataset on multiple GPUs.\n\nOne of the striking differences was memory usage. Whereas MXNet allocated a conservative 670MB on each GPU, Tensorflow allocated close to 100% of available memory (a tad under 11GB).\n\nI was a little shocked by this state of affairs (must be the old-school embedded software developer in me). The model and data set (respectively Resnet-50 and CIFAR-10) didn\u2019t seem to require that much memory after all. Diving a little deeper, I learned that this is indeed the default behaviour in Tensorflow: use all available RAM to speed things up. Fair enough :)\n\nStill, a fact is a fact: in this particular setup, MXNet is faster AND memory-efficient. I couldn\u2019t help but wonder how Tensorflow would behave if I constrained its memory usage. Let\u2019s find out, shall we?\n\nAs a number of folks pointed out, you can easily restrict the number of GPUs that Tensorflow uses, as well as the fraction of GPU memory that it allocates (a float value between 0 and 1). Additional information is available in the Tensorflow documentation.\n\nJust take a look at the example below.\n\nWith this in mind, let\u2019s start restricting memory usage. I\u2019m curious to find out how low we can actually go and if there\u2019s any consequence on training time.\n\nI\u2019ll run the same script as in the previous article (keras/examples/cifar10_resnet50.py), with the following parameters:\n\nOur reference point will be MXNet: 658MB of allocated memory, 155 seconds per epoch.\n\nAfter a little while, here are the results for memory usage and epoch time.\n\nAgain, this is a single test and YMMV. Still, a few remarks.\n\nBy default, Tensorflow allocates as much memory as possible, but more memory doesn\u2019t mean faster. So why behave like a hog in the first place?Especially since Tensorflow can actually get to a memory footprint similar to MXNet (although it\u2019s really a trial and error process).\n\nThis behaviour still raises a lot of questions that trouble my restless mind :)\n\nOh boy. More questions than when I started. Typical :) I\u2019ll have to investigate!\n\nAll in all, I guess I\u2019m more comfortable with a library like MXNet that allocates memory as needed and gives me a clear view on how much is left, what the impacts are when parameters are tweaked, etc.\n\nCall it personal preference. And of course, MXNet is quite faster too.\n\nThanks for reading. Stay tuned for more articles!"
    },
    {
        "url": "https://medium.com/@julsimon/getting-started-with-deep-learning-and-apache-mxnet-34a978a854b4?source=user_profile---------54----------------",
        "title": "Getting started with Deep Learning and Apache MXNet",
        "text": "Deep Learning has become one of the hottest \u2014 if not THE hottest \u2014 topic in the IT industry. Indeed, this technology can efficiently solve complex problems such as image recognition or natural language speech, as demonstrated by services such as Amazon Rekognition, Amazon Polly or Amazon Lex.\n\nHowever, some use cases require that customers build their own Deep Learning application. For a long time, this was a very complex task involving arcane knowledge and tools which only expert scientists could master. Unfortunately, this discouraged a lot of software developers from even trying.\n\nNow, high-level Open Source libraries such as Apache MXNet enable non-experts to design, train and re-use state of the art deep neural networks. Thanks to its high-level API, its support of languages such as Python or R and its ability to train quickly on multiple GPUs, Apache MXNet is well-suited for exploration and experimentation. Let\u2019s get you started!\n\nFor a no-nonsense introduction in less than 30 minutes, you should listen to the AWS podcast I recorded a few months ago.\n\nFor a deeper dive into the Apache MXNet API, here are two series of articles I recently wrote. They are specifically targeted at software developers with a basic knowledge of Python and a strong desire to learn: everyone is welcome!\n\n\u00b7 Part 1: installing MXNet and a look at the NDArray API.\n\n\u00b7 Part 2: understanding imperative programming vs. symbolic programming and a look at the Symbol API.\n\n\u00b7 Part 3: training a first model and a look at the Module API.\n\n\u00b7 Part 4: recognizing images with Inception v3, a pre-trained model from the model zoo.\n\n\u00b7 Part 5: using more pre-trained models, VGG16 and ResNet-152.\n\n\u00b7 Part 1: learning the MNIST data set, first with a Multi-Layer Perceptron then with the LeNet on a GPU instance.\n\n\u00b7 Part 2: learning the CIFAR-10 data set with ResNext-101, then fine-tuning a pre-trained network.\n\n\u00b7 Part 4: using the Deep Learning AMI and multiple GPU instances for distributed training.\n\n\u00b7 Part 5: using Amazon EFS to optimize data set storage for distributed training.\n\nI hope that you will enjoy these articles and that they will help you get started on your first Deep Learning project. Please follow me on Medium and Twitter for more AWS and AI content."
    },
    {
        "url": "https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-3-adding-cloud-based-speech-fb6e4f207c76?source=user_profile---------55----------------",
        "title": "Johnny Pi, I am your father \u2014 part 3: adding cloud-based speech",
        "text": "In the previous post, we learned how to control our robot with a joystick connected to an Arduino Y\u00f9n.\n\nIn this post, I\u2019ll show you how to use Amazon Polly to give the gift of speech to our little friend. If you\u2019re not familiar with this service, don\u2019t worry: it\u2019s dead simple to use.\n\nIn order to invoke Polly and play the resulting sound file, we need to install three Python packages on our robot:\n\nJust like for movement, all commands will be sent through a dedicated MQTT topic, named JohnnyPi/speak. There\u2019s nothing to provision on the AWS IoT gateway, but we have to update the thing\u2019s IAM policy to allow it to subscribe to this new topic.\n\nJust go to the IAM console, locate the proper policy and add the following statement.\n\nWe\u2019ve just taken care of IoT security, now let\u2019s make sure the robot is also allowed to call the Polly API. To do so, we\u2019re going to create a new IAM user with the corresponding policy and use its credentials to authenticate.\n\nLet\u2019s use the AWS CLI for once. No, it\u2019s not complicated: just run these commands on your own machine, not on the robot ;)\n\nTake note of the SECRET_KEY and ACCESS_KEY values. As usual, you have two options to deploy them on the robot:\n\nOk, that\u2019s enough IAM for a day. Let\u2019s start to write some code.\n\nWe need to do three things:\n\nHere\u2019s the corresponding code: nothing clever :) It should also run file on your local machine.\n\nNow that we can invoke Polly, it\u2019s time to work on processing the MQTT messages. Here\u2019s what we need to add to our existing server.py code:\n\nWe\u2019re done: you can grab the updated code on Github.\n\nIn order to test all of this, we need to:\n\nIn the next post, I\u2019ll take you through integration with Amazon Rekognition. Of course, we\u2019ll combine this with Polly to let the robot tell us what it sees :)\n\nAs always, thank you for reading and for your support."
    },
    {
        "url": "https://medium.com/@julsimon/keras-shoot-out-tensorflow-vs-mxnet-51ae2b30a9c0?source=user_profile---------56----------------",
        "title": "Keras shoot-out: TensorFlow vs MXNet \u2013 Julien Simon \u2013",
        "text": "A few months, we took an early look at running Keras with Apache MXNet as its backend. Things were pretty beta at the time, but a lot of progress has since been made. It\u2019s time to reevaluate\u2026 and benchmark MXNet against Tensorflow.\n\nThe good folks at DMLC have forked Keras 1.2 in order to implement MXNet support, multi-GPU included. In parallel, they\u2019ve moved the projet to the Apache Incubator and are currently putting the finishing touches to MXNet 0.11. This is pretty impressive work in such a short time frame!\n\nIn addition to the Keras and MXNet codebases, here\u2019s what we\u2019re going to use today:\n\nOnce the instance is running, we first have to update MXNet to the latest version (0.11.0-rc3 at the time of writing). Here, we\u2019re obviously going for GPU support.\n\nUpdating Keras is quite simple too.\n\nLet\u2019s check that we have the correct versions.\n\nOk, looks good. Let\u2019s move on to training.\n\nKeras supports multiple backends for training and it\u2019s very easy to switch from one to the other. Here are the two file versions for Tensorflow and MXNet.\n\nKeras provides plenty of nice examples in ~/keras/examples. We can use cifar10_resnet50.py pretty much as is. Since we\u2019re going to be using all 8 GPUs, let\u2019s just update the batch size to 256, the number of epochs to 100 and disable data augmentation.\n\nHere\u2019s what memory usage looks like, as reported by nvidia-smi.\n\nAs we can see, TensorFlow is a bit of a memory hog, pretty much eating up 100% of available GPU memory . Not really a problem here, but I\u2019m wondering if a much more complex model would still be able to fit in memory. To be tested in a future post, I suppose :)\n\nAfter a while, here\u2019s the result (full log here).\n\nAll right. Now let\u2019s move on to MXNet.\n\nAt the moment, auto-detection of GPUs is not implemented for MXNet in Keras, so we need to pass the list of available GPUs to the compile() API\n\nHoly moly! MXNet is 60% faster: 25 seconds per epoch instead of 61. Very nice. In the same time frame, this would definitely allow us to try more things, like different model architectures or different hyper parameters. Definitely an advantage when you\u2019re experimenting.\n\nWhat about memory usage? As we can see, MXNet uses over 90% less RAM and there is plenty left for other jobs.\n\nHere\u2019s the result after 100 epochs (full log here): 43 minutes, 99.4% training accuracy, 62% test accuracy.\n\nGranted, this is a single example and no hasty conclusion should be drawn. Still, with 8 GPUs and a well-known data set, MXNet is significantly faster, much more memory-efficient and more accurate than Tensorflow.\n\nIf you\u2019d like to dive a bit more into MXNet, may I recommend the following resources?\n\nIn part 2, I\u2019m taking a deeper look at memory usage in Tensorflow and how to optimise it.\n\nIn part 3, we\u2019ll learn how to fine-tune the models for improved accuracy.\n\nThank you for reading :)"
    },
    {
        "url": "https://medium.com/@julsimon/i3-instances-and-nvme-booya-57fbf452a1c1?source=user_profile---------57----------------",
        "title": "I3 instances and NVMe: booya! \u2013 Julien Simon \u2013",
        "text": "In a previous post, I used an i2.8xlarge instance (32 vCPUs, 244GB RAM, 8x800GB local SSD storage, 10Gb networking) to build FreeBSD in 17 minutes. Pretty good, but in the never ending quest for performance, I decided to try again with an i3.8xlarge instance, as the I3 family is now supported by the latest FreeBSD 11.1 AMI released at the beginning of August. Kudos to Colin Percival for maintaining the FreeBSD AMIs!\n\nThe I3 family is more than a generation bump. Along with the FPGA instances (F1 family), they\u2019re the first to support non-volatile memory express (NVMe) SSD instance store volumes, which deliver a massive performance upgrade compared to traditional SSDs. Thanks to this new technology, the I3 family can deliver up to 3.3 million IOPS at a 4 KB block and up to 16 GB/second of sequential disk throughput.\n\nAlong with a CPU upgrade (from Ivy Bridge to Broadwell), we should definitely see a nice improvement in our build time. Let\u2019s get to work.\n\nWe\u2019ll start by launching an i3.16xlarge instance (64 vCPUs, 488GB RAM, 8x1900GB local NVMe storage, 20Gb networking), with the latest FreeBSD 11.1 AMI (ami-ab56bed2 in eu-west-1).\n\nA few minutes later, we can connect to the instance. Let\u2019s take a look at the NVMe devices.\n\nNow, let\u2019s create two ZFS pools with four disks each: one for /usr/src, one for /usr/obj.\n\nOK, now let\u2019s fetch and extract the FreeBSD distribution.\n\nAnd the result is\u2026 10 minutes and 54 seconds.\n\nThis is a 35% speedup compared to the 17 minutes we got on the i2.8xlarge. Granted, the i3.16xlarge has more cores, but given the structure of the FreeBSD build, they\u2019re hardly ever put to work. Amdahl\u2019s Law strikes again.\n\nI\u2019m guessing that most of the improvement does come from the crazy-fast NVMe storage system. To confirm this, let\u2019s run the same test using memory disks.\n\nNo improvement at all. This leads me to think that CPU is now the bottleneck. Indeed, the quest never ends. Now, EC2 team, could be please get NVMe on the upcoming C5 Skylake-based instances? Please please please? :D\n\nThanks for reading and happy testing."
    },
    {
        "url": "https://becominghuman.ai/johnny-pi-i-am-your-father-part-2-the-joystick-db8ac067e86?source=user_profile---------58----------------",
        "title": "Johnny Pi, I am your father \u2014 part 2: the joystick \u2013",
        "text": "In the previous post, we laid the foundations for movement, thanks to simple messages sent to our robot through AWS IoT.\n\nIn this post, I\u2019ll show you how to control the robot with a joystick connected to an Arduino Y\u00f9n. I simply *love* this small device, as it is actually powered by two CPUs:\n\nBoth CPUs are connected by a serial bridge, allowing the Arduino code to transparently invoke Linux APIs. The best of both worlds :) Very clever design!\n\nJust like for our Raspberry Pi robot, we need to register the Arduino in AWS IoT, create a certificate, a key pair, etc. The process is exactly the same, so just repeat the same steps and download all credentials on your local machine.\n\nDitto for the AWS IoT SDK installation. The only difference is that the installation script will install both the SDK and the credentials on the Arduino, so you do need the register the Arduino before installing the SDK. The process is a little more involved that for the Pi, but stick to the instructions and you\u2019ll be fine.\n\nWe\u2019re now ready to fire up the Arduino IDE. First, let\u2019s write aws_iot_config.h, where we\u2019ll define all parameters required to connect. Obviously, you\u2019ll need to replace XXX with the appropriate values, namely:\n\nOk, now let\u2019s take a look at the joystick and how we\u2019ll connect it to the Arduino.\n\nThe joystick has five analog pins. From top to bottom:\n\nAs you can see, we use the three APIs from the AWS IoT SDK: setup(), config() and connect(). If all three succeed, then we\u2019re connected to the message broker. Nothing complicated.\n\nHere, we read the voltage on the X and Y pins, subtract the neutral voltage and figure out in which direction the joystick has moved. Then, we publish the proper message to the JohnnyPi/move MQTT topic using the publish() API. We use QoS 1 for guaranteed delivery.\n\nThat\u2019s all it takes. Not a lot of code, is it? You\u2019ll find everything on Github.\n\nYou can see the whole thing in action in the video below. The actual demo starts at 29:15.\n\nIn the next post, I\u2019ll take you through integration with Amazon Polly. Time for our little friend to speak!\n\nAs always, thank you for reading and for your support :)"
    },
    {
        "url": "https://becominghuman.ai/johnny-pi-i-am-your-father-part-1-moving-around-e09fe95bbfce?source=user_profile---------59----------------",
        "title": "Johnny Pi, I am your father \u2014 part 1: moving around",
        "text": "The introduction showed you the big picture, now let\u2019s get to work. In this post, we\u2019re going to take care of the following tasks:\n\nI\u2019m hoping that some of you will actually build this from scratch, so here\u2019s the shopping list for part 1:\n\nOnce you\u2019ve built your robot and connected it to WiFi, you can ssh to it (user:pi, password: robots1234) and run some basic commands to get familiar with the GoPiGo Python API. It really couldn\u2019t be simpler, here\u2019s a small example inspired by the Logo turtle. Make sure you have enough space around the robot\u2026 and stay clear of stairs or any other fall hazard!\n\nFeel free to play with the other commands. If you have connected the servo and the ultrasonic sensor, you should definitely try the associated commands as well. Once again, it\u2019s all super simple and even Python beginners should feel comfortable very quickly.\n\nOf course, we\u2019re not going to run Python scripts to drive our robot. We\u2019re going to send commands through the AWS IoT message broker: more precisely, the robot is going to subscribe to MQTT topics (one for robot movement, one for servo movement), where we\u2019re going to publish simple commands like \u2018left\u2019 or \u2018right\u2019.\n\nIn order to communicate with the message broker, we first need to install the AWS IoT Python SDK. Just ssh to the robot and issue this single command:\n\nThat\u2019s it. Now let\u2019s register the robot in AWS IoT.\n\nIn AWS IoT, a device (or \u201cthing\u201d, as they\u2019re called) needs:\n\nFor the sake of brevity, I won\u2019t cover how to get through these steps, which are explained in full detail in the AWS documentation.\n\nJust a quick note on the IAM policy: in this context, it\u2019s probably ok to use a very loose policy (iot:* and resource:*), but here\u2019s the one you should really use.\n\nLast but not least, don\u2019t forget to copy the root certificate, the certificate and the private key to the robot, why not in a dedicated certs directory?\n\nWe will definitely need them to connect to the IoT message broker, which happens to be the topic of the next section!\n\nFirst, let\u2019s create the iot_config.py file and define all parameters required to connect. Obviously, you\u2019ll need to replace XXX with the appropriate values, namely:\n\nNext, let\u2019s write the iot_connect.py file, holding two simple functions: one for connection and one for disconnection. Nothing weird here, we\u2019re just using the appropriate AWS APIs.\n\nThat\u2019s all it takes. Make sure to test this before proceeding: just check that you can call the connectIot() function without any error and that you see a connection in the dashboard of the AWS IoT console :)\n\nNow, let\u2019s subscribe to the two MQTT topics. We just have to call the subscribe() API in the AWS IoT SDK, passing:\n\nOur MQTT messages will contain self-explanatory movement commands: \u2018forward\u2019, \u2018backward\u2019, \u2018left\u2019, \u2018right\u2019, \u2018faster\u2019, \u2018slower\u2019, \u2018stop\u2019. Thus, the role of the callback is simply to extract the command from the message payload and call the appropriate GoPiGo API.\n\nThen, the callback sleeps for a second and stops the robot. This is a precaution to make sure that it won\u2019t run out of control, fall off the stage, etc. Feel free to tweak this to your own liking.\n\nOk, that\u2019s all the code we need for today: you can grab it on Github.\n\nIn order to test all of this, we need to:\n\nPlease make sure you test all commands. Check out the server.py output for any potential error.\n\nIn the next post, we\u2019ll continue to work on movement: I\u2019ll show you how to drive the robot using a joystick connected to an Arduino Y\u00f9n :)\n\nUntil then, thank you for reading!"
    },
    {
        "url": "https://medium.com/@julsimon/johnny-pi-i-am-your-father-part-0-1eb537e5a36?source=user_profile---------60----------------",
        "title": "Johnny Pi, I am your father \u2014 part 0 \u2013 Julien Simon \u2013",
        "text": "This is a sneak peek at my next series of articles, where I will show you how to control a Raspberry Pi-based GoPiGo robot with:\n\nThe robot will be able to:\n\nI will share everything on Github as we go.\n\nThere\u2019s a ton of stuff to cover, but since I\u2019m sure you don\u2019t want to wait for 10 or 15 articles to see this thing running, here\u2019s a sneak preview ;)\n\nWell, this should be fun. Now let\u2019s start to write! As always, I\u2019m available at @julsimon for questions and discussion."
    },
    {
        "url": "https://medium.com/@julsimon/aws-podcast-introduction-to-apache-mxnet-on-aws-397132a6ea37?source=user_profile---------61----------------",
        "title": "AWS podcast: Introduction to Apache MXNet on AWS \u2013 Julien Simon \u2013",
        "text": "A while ago, I had the pleasure of recording an AWS podcast about Apache MXNet, what it is and how you can quickly get started with it on AWS.\n\nThe podcast is now online. Thank you to my colleague Simon Elisha for being a great host.\n\nIf you\u2019d like to know more about MXNet after listening to this, please head out to my in-depth technical introduction."
    },
    {
        "url": "https://medium.com/mlreview/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64?source=user_profile---------62----------------",
        "title": "10 Deep Learning projects based on Apache MXNet \u2013 ML Review \u2013",
        "text": "So far, we ran our MXNet code on Amazon EC2 instances, just like any other Python application. As you may know, there are alternative ways to run code on AWS and they can be obviously applied to MXNet.\n\nUsing a CloudFormation template, this project will create an automated workflow that will provision, configure and orchestrate a pipeline triggering deployment of any changes to your MXNet model or application code. You will orchestrate all of the changes into a deployment pipeline to achieve continuous delivery using CodePipeline and CodeBuild. You can deploy new MXNet APIs and make those available to your users in just minutes, not days or weeks.\n\nMore information in the companion blog post:\n\nThis is a reference application that predicts labels along with their probabilities for an image using a pre-trained model with Apache MXNet deployed on AWS Lambda. A Serverless Application Model template (SAM) and instructions are provided to automate the creation of an API endpoint.\n\nYou can leverage this package and its precompiled libraries to build your prediction pipeline on AWS Lambda with MXNet. Additional models can be found in the Model Zoo"
    },
    {
        "url": "https://medium.com/@julsimon/getting-started-with-the-aws-deep-learning-ami-8b776a80b898?source=user_profile---------63----------------",
        "title": "Getting started with the AWS Deep Learning AMI \u2013 Julien Simon \u2013",
        "text": "The Deep Learning AMI is a Amazon Machine Image provided by Amazon Web Services for use on Amazon EC2. It is designed to provide a stable, secure, and high performance execution environment for deep learning applications running on Amazon EC2.\n\nIt includes popular deep learning frameworks, including MXNet, Caffe, Caffe2, TensorFlow, Theano, CNTK, Torch and Keras as well as packages that enable easy integration with AWS, including launch configuration tools and many popular AWS libraries and tools. It also includes Anaconda Data Science Platform for Python2 and Python3. The Deep Learning AMI is provided at no additional charge to Amazon EC2 users."
    },
    {
        "url": "https://medium.com/@julsimon/how-to-configure-an-aws-iot-button-7f43919afd97?source=user_profile---------64----------------",
        "title": "Using an AWS IoT button to get your kids to show up for dinner",
        "text": "The AWS IoT button is now available in Europe. This device is the best way to get started with AWS IoT and build your first project.\n\nIn the first video, I\u2019ll show you how to:\n\nIn the second video, you\u2019ll learn how to:\n\nAnd hopefully, your kids will show up for dinner! Enjoy and have fun :)"
    },
    {
        "url": "https://medium.com/@julsimon/apache-mxnet-support-in-keras-83de7dec46e5?source=user_profile---------65----------------",
        "title": "Apache MXNet support in Keras \u2013 Julien Simon \u2013",
        "text": "Pretty exciting news last week. Fran\u00e7ois Chollet, author of the popular Keras Deep Learning library, has announced that Apache MXNet is now available as a backend, alongside TensorFlow and Theano.\n\nThis is still beta code, but it won\u2019t prevent us from exploring, will it? :)\n\nIn their own words, Keras is \u201ca high-level neural networks API, written in Python and developed with a focus on enabling fast experimentation\u201d.\n\nIndeed, Keras is pretty popular in the Deep Learning community, certainly because it provides a higher level API, allowing beginners and seasoned practitioners alike to quickly build and train networks without messing with low-level details.\n\nKeras is also the main tool used in the equally popular \u2014 and highly recommended \u2014 Deep Learning course by fast.ai.\n\nIn a nutshell, Keras is a wrapper around Deep Learning backends. Initially, it supported TensorFlow and Theano, but sure enough, the community asked for MXNet support.\n\nLo and behold, the good folks at DMLC got to work and MXNet support is now available in their forked repository. If you\u2019re curious about the Keras-to-MXNet API, it\u2019s here. Well done, guys!\n\nIn the next steps, I\u2019m using a g2.2xlarge instance on AWS, running the very latest Deep Learning AMI, Ubuntu edition (lots of updates!).\n\nI\u2019m assuming that you already have MXNet installed: if not, please read this.\n\nFirst, we need to install the forked Keras version. Please use virtualenv if you\u2019re concerned about messing up your Python environment.\n\nThen, we need to declare MXNet as the backend for Keras. Let\u2019s edit ~/.keras/keras.json.\n\nNow, let\u2019s start python and make sure our setup is correct.\n\nLooking good. As far as I can see, the fork is based on Keras v1: until it\u2019s merged with the upstream repository, Keras v2 features won\u2019t be available.\n\nLet\u2019s run a quick example. We\u2019ve talked at length about the MNIST dataset, so how about we use it again? You\u2019ll find the Keras code here and it\u2019s pretty interesting to compare it to its MXNet equivalent. C\u2019mon, isn\u2019t the MXNet version neater? ;)\n\nAll right, that worked. So there you go: MXNet on Keras!\n\nThe previous example ran on the instance CPU. Although Keras does support GPU computing, setup is unfortunately backend-dependent. When it comes to multi-GPU support, setup is awkward at best (long discussion here). Hopefully, this will be fixed elegantly in future releases.\n\nWhen it comes to MXNet, GPU support doesn\u2019t seem to be complete right now. GPUs are not detected automatically (like for Tensorflow) and I couldn\u2019t find a reliable way to enable a GPU context.\n\nCode is definitely in place: here\u2019s the relevant snippet from https://github.com/dmlc/keras/blob/master/keras/engine/training.py\n\nThis should allow me to modify the MNIST example above to use a GPU context.\n\nBut the damn thing segfaults when I run the modified example :-/ Maybe it hasn\u2019t been fully tested on the latest AMI, who knows. Or maybe I\u2019m too dumb to figure it out: that\u2019s always a strong possibility. Or too impatient. Please reach out if you have the answer :)\n\nThis is certainly going to work anytime soon. I\u2019ll keep you posted!\n\nHaving MXNet support in Keras is great news. Thanks again to everyone involved.\n\nI can\u2019t wait to get proper GPU and multi-GPU support for all backends. I think we\u2019d all love a solution as simple and elegant as the MXNet way, i.e. context=(mx.gpu(0), mx.gpu(1), mx.gpu(2)). It would definitely allow Keras users to enjoy the near-linear scalability of MXNet while using their existing Keras code.\n\nAnd once this is available, I\u2019ll run some more benchmarks ;)\n\nThat\u2019s it for today. Thanks for reading."
    },
    {
        "url": "https://becominghuman.ai/create-your-own-basquiat-with-deep-learning-for-much-less-than-110-million-314aa07c9ba8?source=user_profile---------66----------------",
        "title": "Create your own Basquiat with Deep Learning for much less than $110 million",
        "text": "You may have read today that a painting by Jean-Michel Basquiat was just sold for $110 million . Is this a tribute to his genius or a sign of very, very broken times? Probably both!\n\nThis is a beautiful haunting piece. However, most of us don\u2019t have that kind of money to spend on art (or anything else, for that matter), so we can forget about hanging a Basquiat on our walls.\n\nOr can we? Is there a way we could create artwork in the same style as Basquiat automatically? Enter Deep Learning once again :)\n\nThis fascinating research paper was authored by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge in 2015. In their own words, they use \u201cneural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images\u201d.\n\nIn a nutshell, they use a Convolutional Neural Network (VGG-19, which we\u2019ve used before) to extract style elements from a well-known painting and apply them to an arbitrary image.\n\nThis means that we can take any painting by a famous artist and apply its style to one of our own pictures. MXNet provides all the code that we need to do this, so let\u2019s build our own Basquiat right away.\n\nIt couldn\u2019t be simpler. Just fire up a GPU instance running the Deep Learning AMI or run the MXNet Docker image on your own machine. In the MXNet source directory, go to example/neural-style.\n\nNow, you have two options. You can either load the neuralart.ipynb notebook in Jupyter and follow along, or run the nstyle.py script directly from the CLI.\n\nWith the CLI, this is how to run the script. There are a few parameters that you can tweak, but the most important are obviously the path to the style image (in our case, the Basquiat painting) and the path to the content image (here, my silly mug).\n\nOnce you launch the script, it will run for 1000 epochs and save intermediate images to the output directory every 50 epochs. Here are some (resized) examples as well as the final image.\n\nSo there you go. Pretty impressive result! The main elements of Basquiat\u2019s style are clearly visible. I should use this as my profile picture on LinkedIn :D\n\nAnyway, now you can build a picture in the style of your favorite artist in just a few minutes! Maybe one day we\u2019ll see AI-inspired art being sold at auction houses for ridiculous prices, who knows?\n\nUntil then, thanks for reading and have fun with AI art!"
    },
    {
        "url": "https://becominghuman.ai/training-mxnet-part-5-distributed-training-efs-edition-1c2a13cd5460?source=user_profile---------67----------------",
        "title": "Training MXNet \u2014 part 5: distributed training, EFS edition",
        "text": "In part 4, we learned how to speed up training by using multiple GPU instances. We saw that the job launcher, a Python script named launch.py, used rsync to copy the data set to each one of these instances. For a small data set like CIFAR-10, that\u2019s not really a problem. However, it would be slow and wasteful to copy a much larger data set.\n\nIn this article, I will show you how to share the data set across all instances with Amazon EFS, a managed service fully compatible with NFS v4.1.\n\nEverything we did in part 4 still stands. In particular, we do need password-less ssh between all instances, because the launcher uses ssh to start the training script.\n\nSo, if you haven\u2019t done so already, please read part 4 and come back here when you reach the \u201cLaunching distributed training\u201d section :)\n\nThis is very simple. Go the EFS console and click on \u201cCreate file system\u201d. Select the VPC in which your instances are running. Then, select all available AZs and make sure you use the Security Group attached to your instances (not the default Security Group selected by the console).\n\nClick \u201cNext\u201d, select the \u201cGeneral purpose\u201d performance mode and create the file system. After a few minutes, the file system will be ready for mounting. Don\u2019t try to mount it until it\u2019s available, you\u2019ll get a \u201cName or service not known\u201d error. I know, I tried :)\n\nAmazon EFS is NFS-compatible, so mounting the file system is done the way you\u2019d expect it. Mount instructions are available in the console.\n\nObviously, we need to do this on all instances. If your instances are already up, just run the commands above on each of them.\n\nIf they\u2019re not up yet, then you can mount the file system automatically at instance launch thanks to User Data (just copy the commands in the \u201cAdvanced Details\u201d section of the EC2 console).\n\nNow that the file system has been mounted on all instances, it\u2019s time to share the data set. Actually, we\u2019ll share the full MXNet source tree, in order to include all necessary scripts.\n\nYou should now see the mxnet directory on all instances. We\u2019re ready to launch training.\n\nThe command is identical to the one we used in part 4, minus the \u201c \u2014 sync-dst-dir /tmp/mxnet\u201d parameter.\n\nThat\u2019s it! Now you know how to do distributed training using shared storage. As a bonus, we also shared the sources and that will come in handy next time we need to build and install MXNet :)"
    },
    {
        "url": "https://towardsdatascience.com/training-mxnet-part-4-distributed-training-91def5ea3bb7?source=user_profile---------68----------------",
        "title": "Training MXNet \u2014 part 4: distributed training \u2013",
        "text": "In part 3, we worked with the CIFAR-10 data set and learned how to tweak optimisation parameters. We ended up training a 110-layer ResNext model using all 4 GPUs of a g2.8xlarge instance\u2026 which took about 12 hours.\n\nIn this article, I\u2019ll show you how to use multiple instances to dramatically speed up training. Buckle up!\n\nWe\u2019re going to work with p2.8xlarge instances running the Deep Learning AMI, Ubuntu edition. However, you can easily replicate this with any kind of EC2 instance or even on a bunch of PCs running under your desk :)\n\nLet\u2019s get started. We\u2019re going to configure the master node the way we like it and then we\u2019ll clone it to add more instances to our MXNet cluster. The first step is to go to the Marketplace section of the EC2 console and locate the Deep Learning AMI.\n\nThen, select the instance type you\u2019d like to use. Please be mindful of instance costs: a p2.8xlarge is going to cost $7.20 per hour. Don\u2019t worry, you can actually use any instance type, as MXNet is able to use either the CPU(s) or the GPU(s) of the instance. Obviously, GPU instances will be much faster than t2.micros :)\n\nA few more clicks and you\u2019re done. Just make sure the SSH port is open and that you have created a new key pair for the instance (let\u2019s call it ec2). After a few minutes, you can ssh into the master node using the ubuntu user (not the ec2-user).\n\nBy default, distributed training is not enabled in the source distribution, which means we probably have to rebuild MXNet from source. If your build already includes distributed training, you can skip this section.\n\nThe Deep Learning AMI includes the MXNet sources: we just have to make them our own and refresh them to the latest stable version (0.9.5 at the time of writing).\n\nThen, we need to configure our build options. The last one actually enables distributed training.\n\nNow we can build and install the library. No need to add dependencies, as they\u2019re already included in the AMI. I\u2019m running a parallel make on 32 cores because that\u2019s what a p2.8xlarge has.\n\nOnce the library is installed, it\u2019s a good idea to run a quick Python check.\n\nOk, this looks good. Let\u2019s move on.\n\nThe master node and the worker nodes need to talk to one another to share the data set as well as training results. Thus, we need to alter the configuration of our security group to allow this.\n\nThe absolute simplest way to do this is to allow all TCP communication between instances of the MXNet cluster, i.e. instances using the same security group.\n\nTo do this, go to the EC2 console and edit the inbound rules of the security group of the master node. Add a rule allowing all TCP traffic and use the actual name of the security group to restrict source traffic.\n\nOur instance is now ready. Let\u2019s create the worker nodes.\n\nWe\u2019re going to create a new AMI based on the master node. Then, we\u2019ll use it to launch the workers. Locate your instance in the EC2 console and create an image.\n\nAfter a few minutes, you\u2019ll see the new AMI in the \u201cImages\u201d section of the EC2 console. You can now use it to launch your worker nodes.\n\nNothing complicated here: select the instance type, the number of instances you\u2019d like to launch (3 in my case) and the same security group as the master node.\n\nA few more minutes and your instances are ready.\n\nLovely. Write down the private IP adresses of each instance, we\u2019re going to need them in a second.\n\nLet\u2019s log in to the master node, move to the tools directory and look at the launcher.\n\nThis is the tool we\u2019ll use to start training on all nodes (master node included). It does two things:\n\nlaunch.py needs the private IP address of all nodes (including the master node) to be declared in a file. It should look something like this.\n\nWe need password-less ssh access between the master node and the worker nodes. If you already have this in place, you can skip this section.\n\nWe\u2019ll keep things simple by creating a new key pair on our local computer and distributing it across the cluster.\n\nNext, still from our local computer, we\u2019re going to copy the public key to all nodes (including the master node) and the private key to the master node only.\n\nFinally, on the master node, we\u2019ll start ssh-agent and add the mxnet identity.\n\nYou should now be able to log in from the master node to each worker node (including the master node itself). Please make sure that this is working properly before going on.\n\nIf it does, you\u2019re ready to train, buddy :)\n\nHere\u2019s the magic command: 4 nodes listed in the hosts file will receive a copy of the data set in /tmp/mxnet via rsync. Then, the master node will run the train_cifar10.py script on each node, training a 110-layer ResNext model on all 8 GPUs.\n\nThe PS_VERBOSE variable will output extra information. Very useful in case something goes wrong ;)\n\nYou can check progress by logging in on the different nodes and running the \u2018nvidia-smi -l\u2019 command.\n\nSo how fast is this? As I mentioned before, it took about 12 hours to run 300 epochs on the 4 GPUs of a g2.8xlarge instance. The combined 32 GPUs of the 4 p2.8xlarge instances did it in 91 minutes!\n\nThat\u2019s an 8x speedup, which kind of makes sense since we have 8x more GPUs. I had read about it and now I see it with my own eyes: linear scaling indeed! This makes me want to push it to 256 GPUs: it would only require 16 p2.16xlarge after all :D\n\nLast but not least, my colleagues Naveen Swamy and Joseph Spisak wrote a very interesting blog post on how to automate most of this stuff using AWS CloudFormation. This is definitely worth reading if you\u2019re running everything in AWS.\n\nThat\u2019s it for today. Thank you very much for reading and for all the friendly support I\u2019ve been receiving lately. It means a lot to me!"
    },
    {
        "url": "https://medium.com/@julsimon/training-mxnet-part-3-cifar-10-redux-ecab17346aa0?source=user_profile---------69----------------",
        "title": "Training MXNet \u2014 part 3: CIFAR-10 redux \u2013 Julien Simon \u2013",
        "text": "In part 2, we learned about the CIFAR-10 data set and we saw how to easily load it using a RecordIO object. Using this data set, we both trained a network from scratch and fine-tuned a network trained on ImageNet. In both cases, we used a fixed learning rate and we got to a point where validation accuracy plateaued at about 85%.\n\nIn this article, we\u2019re going to focus on improving validation accuracy.\n\nLet\u2019s fine-tune our pre-trained network again. This time, we\u2019re going to reduce the learning rate gradually, which should help the model converge to a \u201clower\u201d minimum: we\u2019ll start at 0.05 and multiply by 0.9 each time 25 epochs have gone by, until we reach 300 epochs.\n\nWe need 2 extra parameters to do this:\n\nThe new fine-tuning command becomes:\n\nA while later, here\u2019s the result.\n\nAs we can see, reducing the learning rate definitely helps reaching higher validation accuracy. Previously, it plateaued at 85% after 25 epochs or so. Here, it\u2019s climbing steadily up to 89% and would probably keep increasing beyond 300 epochs. 4% is a huge difference: on a 10,000 image validation set, it means that an extra 400 images are correctly labelled.\n\nSurely, we could keep tweaking and find an even better combination for the initial learning rate, the factor and the steps. But we could also do without all these parameters, thanks to the AdaDelta optimizer.\n\nAdaDelta is a evolution of the SGD algorithm we\u2019ve been using so far for optimization (research paper). It doesn\u2019t need to be given a learning rate. In fact, it will automatically select and adapt a learning rate for each dimension.\n\nLet\u2019s try to train ResNext-101 from scratch, using AdaDelta. Instead of loading the network, we\u2019re going to build one using resnext.get_symbol(), a Python function available in MXnet. There\u2019s a whole set of functions to build different networks: I suggest that you take some time to look at them, as they will help you understand how these networks are structured.\n\nYou should now be familiar with the rest of the code :)\n\nThese are the results after 300 epochs.\n\nIn our previous article, training the same model from scratch only yielded 80% validation accuracy. Here, not only does training accuracy increase extremely fast, we also reach 86% validation accuracy without having to guess about learning rate or when to decrease it.\n\nIt\u2019s very likely that an expert would achieve better results by tweaking optimization parameters, but for the rest of us, AdaDelta is an interesting option.\n\nMXNet supports a whole set of optimization algorithms. If you\u2019d like to learn more on how they work and how they differ, here\u2019s an excellent article by Sebastian Ruder.\n\nOne last thing. I mentioned earlier that training took \u2018a while\u2019. More precisely, it took 12+ hours using all 4 GPUs of a g2.8xlarge instance.\n\nCould we go faster? Sure, I could use a p2.16xlarge instance. That\u2019s as large as GPU servers get.\n\nEven faster? We need distributed training, which we\u2019ll cover in part 4."
    },
    {
        "url": "https://becominghuman.ai/training-mxnet-part-2-cifar-10-c7b0b729c33c?source=user_profile---------70----------------",
        "title": "Training MXNet \u2014 part 2: CIFAR-10 \u2013",
        "text": "In part 1, we used the famous LeNet Convolutional Neural Network to reach 99+% validation accuracy in just 10 epochs. We also saw how to use multiple GPUs to speed up training.\n\nIn this article, we\u2019re going to tackle a more difficult data set: CIFAR-10. In the process, we\u2019re going to learn a few new tricks. Read on :)\n\nThe CIFAR-10 dataset consists of 60,000 32 x 32 colour images. They are divided in 10 classes containing 6,000 images each. There are 50,000 training images and 10,000 test images. Categories are stored in a separate metadata file.\n\nEach file contains 10,000 pickled images, which we need to turn into an array shaped (10000, 3, 32, 32). The \u20183\u2019 dimension comes for the three RGB channels, remember? :)\n\nLet\u2019s open the first file, save its first 10 images to disk and print their category. Nothing really complicated here, except some OpenCV tricks (see comments in the code below).\n\nJust like we did for the the MNIST data, the CIFAR-10 images and labels could be loaded in NDArrays and then fed to an iterator. This is how we would to it.\n\nThe next logical step would be to bind these arrays to a Module and start training (just like we did for the MNIST data set). However, I\u2019d like to show you another way to load image data: the RecordIO file.\n\nBeing able to load data efficiently is a very important part of MXNet: you\u2019ll find architecture details here. In a nutshell, RecordIO files allow large data sets to be packed and split in multiple files, which can then be loaded and processed in parallel by multiple servers for distributed training.\n\nWe won\u2019t cover how to build these files today. Let\u2019s use pre-existing files hosted on the MXNet website.\n\nThe first file contains 50,000 samples, which we\u2019ll use for training. The second one contains 10,000 samples, which we\u2019ll use for validation. Image resolution has been set to 28x28.\n\nLoading these files and building an iterator is extremely simple. We just have to be careful to :\n\nData is ready for training. We\u2019ve learned from previous examples that Convolutional Neural Networks are a good fit for object detection, so that\u2019s where we should look.\n\nIn previous examples, we picked models from the model zoo and retrained them from scratch on our data set. We\u2019re going to do that again with the ResNext-101 model, but we\u2019re going to try something different in parallel: fine-tuning the model.\n\nFine-tuning means that we\u2019re going to keep all layers and pre-trained weights unchanged, except for the last layer: it will be removed and replaced by a new layer having the number of outputs of the new data set. Then, we will train the output layer on the new data set.\n\nSince our model has been pre-trained on the large ImageNet data set, the rationale for fine-tuning is to benefit from the very large number of patterns that the model has learned while training on ImageNet. Although image sizes are quite different, it\u2019s reasonable to expect that they will also apply to CIFAR-10.\n\nLet\u2019s first start by training the model from scratch. We\u2019ve done this a few times before, so no difficulty here.\n\nThis is the result after 100 epochs.\n\nReplacing layers sounds complicated, doesn\u2019t it? Fortunately, the MXNet sources provide Python code to do this. It\u2019s located in example/image-classification/fine-tune.py. Basically, it\u2019s going to download the pre-trained model, remove its output layer, add a new one and start training.\n\nThis is how to use it:\n\nThis is going to download resnext-101\u20130000.params and resnext-101-symbol.json from the model zoo. Most of the parameters should be familiar. Here\u2019s the result after 100 epochs.\n\nWhat do we see here?\n\nEarly on, fine-tuning delivers much higher training and validation accuracy. This makes sense, since the model has been pre-trained. So, if you have limited time and resources, fine-tuning is definitely an interesting way to get quick results on a new data set.\n\nOver time, fine-tuning delivers about 5% additional validation accuracy than training from scratch. I\u2019m guessing that the pre-trained model generalizes better on new data thanks to the large ImageNet data set.\n\nLast but not least, validation accuracy stops improving after 50 epochs or so. Surely, we can do something to improve this?\n\nYes, of course. We\u2019ll see how in part 3 :)"
    },
    {
        "url": "https://chatbotslife.com/training-mxnet-part-1-mnist-6f0dc4210c62?source=user_profile---------71----------------",
        "title": "Training MXNet \u2014 part 1: MNIST \u2013",
        "text": "In a previous series, we discovered how we could use the MXNet library and pre-trained models for object detection. In this series, we\u2019re going to focus on training models with a number of different data sets.\n\nPlease note that is an updated and expanded version of this tutorial: I\u2019m using the Module API (instead of the deprecated Model API) as well as the MNIST data iterator.\n\nThis data is a set of 28x28 greyscale images representing handwritten digits (0 to 9).\n\nThe training set has 60,000 samples and the test set has 10,000 examples. Let\u2019s download them right away.\n\nHow about we take a look inside these files? We\u2019ll start with the labels. They are stored as a serialized numpy array holding 60,000 unsigned bytes.\n\nThe file starts with a big-endian packed structure, holding 2 integers: magic number, number of labels.\n\nLet\u2019s now extract some images. Again, they are stored as a serialized numpy array, which we will reshape to build 28x28 images. Each pixel is stored as an unsigned byte (0 for black, 255 for white).\n\nThe file starts with a big-endian packed structure, holding 4 integers: magic number, number of images, number of rows and number of columns.\n\nLet\u2019s save the first 10 images to disk.\n\nThis is how they look.\n\nOk, now that we understand the data, let\u2019s build a model.\n\nWe\u2019re going to use a simple multi-layer perceptron (similar to what we built here) : 784 \u2192 128 \u2192 64 \u2192 10\n\nMXNet conveniently provides a data iterator for the MNIST data set. Thanks to this, we don\u2019t have to open the files, build NDArrays, etc. It also has default parameters for filenames and so on. Very cool!\n\nWe can now bind the data to our model. Default batch size is 128.\n\nLet\u2019s start with default settings for weight initialization and optimization (aka hyperparameters) and hope for the best. Here we go!\n\nHmm, things are not going well. It looks like the network is not learning. Actually, it is learning, but real slow: the default learning rate is 0.01, which is too low. Let\u2019s use a more reasonable value such as 0.1.\n\nThat\u2019s more like it. We get to 96.93% accuracy after 10 epochs. What about validation accuracy? Let\u2019s create a metric and score our validation data set.\n\nStill, the first few training epochs are not great: this is caused by default weight initialization. Let\u2019s use something smarter, like the Xavier technique.\n\nThat\u2019s much better: we get to 86% accuracy after only one epoch. We gain almost 1.5% training accuracy and 1% validation accuracy.\n\nCan we get better results? Well, we could always try to train the model longer. Let\u2019s try 50 epochs.\n\nAs you can see, we hit 100% training accuracy after 42 epochs and there\u2019s no point in going any further. In the process, we only manage to improve validation accuracy by 0.4%.\n\nIs this the best we can do? We could try other optimizers, but unless you really know what you\u2019re doing, it\u2019s probably safer to stick to SGD.\n\nMaybe we simply need a bigger boat?\n\nLet\u2019s try this network and see what happens :\n\nWe hit 100% training accuracy after 25 epochs and get to 97.99% validation accuracy, a modest 0.14% increase compared to the previous model. Clearly, a deeper multi-layer perceptron is not the way to go.\n\nWe need a better boat, then.\n\nWe\u2019ve seen that these networks work very well for image processing. Let\u2019s try a well-known CNN \u2014 called LeNet \u2014 on this data set.\n\nHere is the model definition, everything else is identical.\n\nThis is painfully slow. About 45 seconds per epoch, about 30 times slower than the multilayer perceptron. Now would be a good time to try these fancy GPUs, don\u2019t you think?\n\nBy chance, I\u2019ve running this on a g2.8xlarge instance. It has 4 NVidia GPUs ready to crunch data :)\n\nAll it takes to switch from CPU to GPU is this. Amazing!\n\nHere we go again.\n\nNice! Training time has been massively reduced. Accuracy is now 99+% thanks to the more sophisticated model.\n\nDid I mention that there are four GPUs in this box? How about using more than one?\n\nOnce again, this is pretty simple to set up.\n\nWe saved 50% of training time. Let\u2019s go for three GPUs.\n\nAnother 20% saved. Training time is now only 50% more than what it was for the CPU-version of the multi-layer perceptron.\n\nAdding a fourth GPU won\u2019t help. Yes, I tried :) Anyway, we\u2019re pretty happy with our model, so let\u2019s save it for future use.\n\nSaving a model just requires a file name and an epoch number.\n\nThis creates two files (which you should now be familiar with):\n\nJust like we did in previous articles, we\u2019re now able to load this pre-trained model.\n\nHere are the ugly digits I created with Paintbrush :)\n\nI saved them as a 28x28 images, which I can now load as numpy arrays. I need to normalize pixels values and add two dimensions to reshape the array from (28, 28) to (1, 1, 28, 28) : batch size of one, one channel (greyscale), 28 x 28 pixels.\n\nWe\u2019ll predict image by image. To avoid building a data iterator, I\u2019ll use the same trick we\u2019ve seen before (using a namedtuple to provide a data attribute).\n\nNow we\u2019re ready. Let check these digits!\n\nAnd here are the results.\n\nWow. Hardly any doubt on the first 9 digits (probabilities are 99.99+%). Only my ugly 9 scores lower :)\n\nWell, who thought that we\u2019d have so much fun and that we\u2019d cover so much ground using the MNIST dataset? Code and images are available on Github. Hopefully, this will get you started on building and training networks on your own data.\n\nThat\u2019s it for today. Stay tuned for part 2 where we\u2019ll look at another data set!"
    },
    {
        "url": "https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-6-fcdd7521ae87?source=user_profile---------72----------------",
        "title": "An introduction to the MXNet API \u2014 part 6 \u2013 Julien Simon \u2013",
        "text": "In part 5, we used three different pre-trained models for object detection and compared them using a couple of images.\n\nOne of the things we learned is that models have very different memory requirements, the most frugal model being Inception v3 with \u201conly\u201d 43MB. Obviously, this begs the question: \u201ccan we run this on something really small, say a Raspberry Pi?\u201d. Well, let\u2019s find out!\n\nThere\u2019s an official tutorial, but I found it to be missing some steps, so here\u2019s my version. It works fine on a Raspberry Pi 3 running the latest Raspbian.\n\nFirst, let\u2019s add all necessary dependencies.\n\nThen, let\u2019s clone the MXNet repository and checkout the latest stable release. Don\u2019t miss this last step, as I found HEAD to be broken most of the time (Update 30/04/17: the MXNet dev team got in touch and informed me that Continuous Integration is now in place. I can confirm that HEAD now builds fine. Well done, guys).\n\nMXNet is able to load and save data in S3, so let\u2019s enable this feature, it might come in handy later on. MXNet also supports HDFS but you need to install Hadoop locally, so\u2026 no :)\n\nWe could just run make but given the limited processing power of the Pi, the build is gonna take a while: you don\u2019t want to it to be interrupted if your SSH session times out! Screen is going to solve this.\n\nTo speed things up a little, we can run a parallel make on 2 cores (out of 4). I wouldn\u2019t recommend using more, as my Pi became unresponsive when I tried it.\n\nThis should last about an hour. The last step is to install the library and its Python bindings.\n\nOnce we\u2019ve copied the model files to the Pi, we need to make sure that we can actually load them. Let\u2019s reuse the exact same code we wrote in part 5. For the record, the Pi is in CLI mode with about 580MB of free memory. All data is stored on a 32GB SD card.\n\nOuch! VGG16 is too large to fit in memory. Let\u2019s try ResNet-152.\n\nResNet-152 loads successfully in about 10 seconds and predicts in less than 10 milliseconds. Let\u2019s move on to Inception v3.\n\nOn a constrained device like the Pi, model differences are much more obvious! Inception v3 loads much faster iand predicts in a few milliseconds. Even when the model is loaded, there\u2019s plenty of RAM left on the PI to run an actual application, so it\u2019s definitely an interesting candidate for embedded apps. Let\u2019s keep going :)\n\nOne of the best gadgets you can add to the Raspberry Pi is a camera module. It couldn\u2019t be simpler to use!\n\nOf course, I cannot resist running the same picture through Amazon Rekognition using the Python scripts I wrote a while ago (article, code).\n\nGood job, Rekognition. Now\u2026 wouldn\u2019t it be nice it the Pi actually told us what the picture is about? It\u2019s not too complicated to add Amazon Polly to the mix (article).\n\nSo, here\u2019s a video of my Raspberry Pi performing real-time object detection with the Inception v3 model running in MXNet, and describing what it sees with Amazon Polly."
    },
    {
        "url": "https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-5-9e78534096db?source=user_profile---------73----------------",
        "title": "An introduction to the MXNet API \u2014 part 5 \u2013 Julien Simon \u2013",
        "text": "In part 4, we saw how easy it was to use a pre-trained version of the Inception v3 model for object detection. In this article, we\u2019re going to load two other famous Convolutional Neural Networks (VGG19 and ResNet-152) and we\u2019ll compare them to Inception v3.\n\nPublished in 2014, VGG16 is a model built from 16 layers (research paper). It won the 2014 ImageNet challenge by achieving a 7.4% error rate on object classification.\n\nPublished in 2015, ResNet-152 is a model built from 152 layers (research paper). It won the 2015 ImageNet challenge by achieving a record 3.57% error rate on object detection. That\u2019s much better than the typical human error rate which is usually measured at 5%.\n\nTime to visit the model zoo once again. Just like for Inception v3, we need to download model definitions and parameters. All three models have been trained on the same categories, so we can reuse our synset.txt file.\n\nAll three models have been trained on the ImageNet data set, with a typical image size of 224 x 224. Since data shape and categories are identical, we can reuse our previous code as-is.\n\nAll we have to change is the model name :) Let\u2019s just add a parameter to our loadModel() and init() functions.\n\nLet\u2019s compare these models on a couple of images.\n\nGood job on the top two categories, but the other three are wildly wrong. Looks like the vertical shape of the microphone stand confused the model.\n\nVery high on the top category. The other four are all meaningful.\n\nVery similar results to VGG16 for the top two categories. The other three are a mixed bag.\n\nAll three models score very high on the top category. One can suppose that the shape of a violin is a very unambiguous pattern for a neural network.\n\nObviously, it\u2019s impossible to draw conclusions from a couple of samples. If you\u2019re looking for a pre-trained model, you should definitely look at the training set, run tests on your own data and make up your mind!\n\nYou\u2019ll find extensive model benchmarks in research papers such as this one. For developers, the two most important factors will probably be:\n\nTo answer the first question, we could take an educated guess by looking at the size of the parameters file:\n\nAs we can see, the current trend is to use deeper networks with less parameters. This has a double benefit: faster training time (since the network has to learn less parameters) and reduced memory usage.\n\nThe second question is more complex and depends on many parameters such as batch size. Let\u2019s time the prediction call and run our examples again.\n\nThis is what we see (values have been averaged over a few calls).\n\nThat\u2019s it for today! Full code below."
    },
    {
        "url": "https://towardsdatascience.com/an-introduction-to-the-mxnet-api-part-4-df22560b83fe?source=user_profile---------74----------------",
        "title": "An introduction to the MXNet API \u2014 part 4 \u2013",
        "text": "In part 3, we built and trained our first neural network. We now know enough to take on more advanced examples.\n\nState of the art Deep Learning models are insanely complex. They have hundreds of layers and take days \u2014 if not weeks \u2014 to train on vast amounts of data. Building and tuning these models requires a lot of expertise.\n\nFortunately, using these models is much simpler and only requires a few lines of code. In this article, we\u2019re going to work with a pre-trained model for image classification called Inception v3.\n\nPublished in December 2015, Inception v3 is an evolution of the GoogleNet model (which won the 2014 ImageNet challenge). We won\u2019t go into the details of the research paper, but paraphrasing its conclusion, Inception v3 is 15\u201325% more accurate than the best models available at the time, while being six times cheaper computationally and using at least five times less parameters (i.e. less RAM is required to use the model).\n\nQuite a beast, then. So how do we put it to work?\n\nThe model zoo is a collection of pre-trained models ready for use. You\u2019ll find the model definition, the model parameters (i.e. the neuron weights) and instructions (maybe).\n\nLet\u2019s download the definition and the parameters (you may have to change the filename). Feel free to open the first file: you\u2019ll see the definition of all the layers. The second one is a binary file, leave it alone ;)\n\nSince this model has been trained on the ImageNet data set, we also need to download the corresponding list of image categories (1000 of them).\n\nOk, done. Now let\u2019s get to work.\n\nHere\u2019s what we need to do:\n\nThat\u2019s all it takes. Four lines of code! Now it\u2019s take to push some data in there and see what happens. Well\u2026 not quite yet.\n\nData preparation: making our life miserable since the Seventies\u2026 From relational databases to Machine Learning to Deep Learning, nothing has really changed in that respect. It\u2019s boring but necessary. Let\u2019s get it done.\n\nRemember that the model expects a 4-dimension NDArray holding the red, green and blue channels of a single 224 x 224 image. We\u2019re going to use the popular OpenCV library to build this NDArray from our input image. If you don\u2019t have OpenCV installed, running \u201cpip install opencv-python\u201d should be enough in most cases :)\n\nHere are the steps:\n\nDizzy? Let\u2019s look at an example. Here\u2019s our input picture.\n\nOnce processed, this picture has been resized and split into RGB channels stored in array[0] (here is the code used to generate the images below).\n\nIf batch size was higher than 1, then we would have a second image in array[1], a third in array[2] and so on.\n\nWas this fun or what? Now let\u2019s predict!\n\nYou may remember from part 3 that a Module object must feed data to a model in batches: the common way to do this is to use a data iterator (specifically, we used an NDArrayIter object).\n\nHere, we\u2019d like to predict a single image, so although we could use data iterator, it\u2019d probably be overkill. Instead, we\u2019re going to create a named tuple, called Batch, which will act as a fake iterator by returning our input NDArray when its data attribute is referenced.\n\nNow we can pass this \u201cbatch\u201d to the model and let it predict.\n\nThe model will output an NDArray holding the 1000 probabilities, corresponding to the 1000 categories. It has only one line since batch size is equal to 1.\n\nLet\u2019s turn this into an array with squeeze(). Then, using argsort(), we\u2019re creating a second array holding the index of these probabilities sorted in descending order.\n\nAccording to the model, the most likely category for this picture is #546 , with a probability of 58%.\n\nLet\u2019s find the name of this category. Using the synset.txt file, we can build a list of categories and find the one at index 546.\n\nWhat about the second highest category?\n\nSo there you go. Now you know how to use a pre-trained, state of the art model for image classification. All it took was 4 lines of code\u2026 and the rest was just data preparation.\n\nYou\u2019ll find the full code below. Have fun and stay tuned :D"
    },
    {
        "url": "https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-3-1803112ba3a8?source=user_profile---------75----------------",
        "title": "An introduction to the MXNet API \u2014 part 3 \u2013 Julien Simon \u2013",
        "text": "In part 2, we discussed how Symbols allow us to define computation graphs processing data stored in NDArrays (which we studied in part 1).\n\nIn this article, we\u2019re going to use what we learned on Symbols and NDArrays to prepare some data and build a neural network. Then, we\u2019ll use the Module API to train the network and predict results.\n\nOur (imaginary) data set is composed of 1000 data samples\n\nLet\u2019s use a uniform distribution to generate the 1000 samples. They are stored in an NDArray named \u2018X\u2019: 1000 lines, 100 columns.\n\nThe categories for these 1000 samples are represented as integers in the 0\u20139 range. They are randomly generated and stored in an NDArray named \u2018Y\u2019.\n\nNext, we\u2019re splitting the data set 80/20 for training and validation. We use the NDArray.crop function to do this. Here, the data set is completely random, so we can use the top 80% for training and the bottom 20% for validation. In real life, we\u2019d probably shuffle the data set first, in order to avoid potential bias on sequentially-generated data.\n\nOur data is now ready!\n\nOur network is pretty simple. Let\u2019s look at each layer:\n\nIn part 1, we saw that neural networks not trained one sample at a time, as this is quite inefficient from a performance point of view. Instead, we use batches, i.e. a fixed number of samples.\n\nIn order to deliver these batches to the network, we need to build an iterator using the NDArrayIter function. Its parameters are the training data, the categories (MXNet calls these labels) and the batch size.\n\nAs you can see, we can indeed iterate on the data set, 10 samples and 10 labels at a time. We then call the reset() function to restore the iterator to its original state.\n\nOur network is now ready for training!\n\nFirst, let\u2019s bind the input symbol to the actual data set (samples and labels). This is where the iterator comes in handy.\n\nNext, let\u2019s initialize the neuron weights in the network. This is actually a very important step: initializing them with the \u201cright\u201d technique will help the network learn much faster. The Xavier initializer (named after his inventor, Xavier Glorot \u2014 PDF) is one of these techniques.\n\nNext, we need to define the optimization parameters:\n\nAnd finally, we can train the network! We\u2019re doing it over 50 epochs, which means the full data set will flow 50 times through the network (in batches of 10 samples).\n\nAs we can see, the training accuracy rises rapidly and reaches 99+% after 50 epochs. It looks like our network was able to learn the training set. That\u2019s pretty impressive!\n\nBut how does it perform against the validation set?\n\nNow we\u2019re going to throw new data samples at the network, i.e. the 20% that haven\u2019t been used for training.\n\nFirst, we\u2019re building an iterator. This time, we\u2019re using the validation samples and labels.\n\nNext, using the Module.iter_predict() function, we\u2019re going to run these samples through the network. As we do this, we\u2019re going to compare the predicted label with the actual label. We\u2019ll keep track of the score and display the validation accuracy, i.e. how well the network did on the validation set.\n\nThere is quite a bit happening here :)\n\nThen, we\u2019re comparing the number of equal values in label and pred_label using Numpy.sum().\n\nFinally, we compute and display the validation accuracy.\n\nWhat? 9%? This is really bad! If you needed proof that our data set was random, there you have it!\n\nThe bottom line is that you can indeed train a neural network to learn anything, but if your data is meaningless (like ours here), it won\u2019t be able to predict anything. Garbage in, garbage out!\n\nIf you read this far, I guess you deserve to get the full code for this example ;) Please take some time to use it on your own data, it\u2019s the best way to learn."
    },
    {
        "url": "https://medium.com/@julsimon/an-introduction-to-the-mxnet-api-part-2-ce761513124e?source=user_profile---------76----------------",
        "title": "An introduction to the MXNet API \u2014 part 2 \u2013 Julien Simon \u2013",
        "text": "In part 1, we covered some MXNet basics and then discussed the NDArray API (tldr: NDArrays is where we\u2019re going to store data, parameters, etc).\n\nNow that we\u2019ve got data covered, it\u2019s time to look at how MXNet defines computation steps.\n\nThat\u2019s a fair question! Haven\u2019t we all learned that \u201cprogram = data structures + code\u201d? NDArrays are our data structures, let\u2019s just add code!\n\nWell yes, we could to that. We\u2019d have to define all the steps explicitly and run them sequentially on our data. This is called \u201cimperative programming\u201d and it\u2019s how Fortran, Pascal, C, C++ and so on work. Nothing wrong with that.\n\nHowever, neural networks are intrinsically parallel beasts: inside a given layer, all outputs can be computed simultaneously. Independent layers could also run in parallel. So, in order to get good performance, we\u2019d have to implement parallel processing ourselves using multithreading or something similar. We know how that usually works out. And even if we got the code right, how reusable would it be if data size or network layout kept changing?\n\nFortunately, there is an alternative.\n\n\u201cDataflow programming\u201d is a flexible way of defining parallel computation, where data flows through a graph. The graph defines the order of operations, i.e. whether they need to be run sequentially or whether they may be run in parallel. Each operation is a black box: we only define its input and output, without specifying its actual behaviour.\n\nThis might sound like Computer Science mumbo jumbo, but this model is exactly what we need to define neural networks : let input data flow through an ordered sequence of operations called \u201clayers\u201d, with each layer running many instructions in parallel.\n\nEnough talk. Let\u2019s look at an example. This is how we would define E as (A*B) + (C*D).\n\nNo matter what the inputs are (integers, vectors, matrices, etc.), this graph tells us how to compute the output value \u2014 provided that operations \u201c+\u201d and \u201c*\u201d are defined.\n\nOf course, MXNet will use this information for optimisation purposes.\n\nSo now we know why these things are called symbols (not a minor victory!). Let\u2019s see if we can code the example above.\n\nSee? This is perfectly valid code. We can assign a result to e without knowing what a, b, c and d are. Let\u2019s keep going.\n\na, b, c and d are symbols which we explicitly declared. e is different: it is a symbol as well, but one that is the result of a \u2018+\u2019 operation. Let\u2019s try to learn more about e.\n\nWhat this tells us is that:\n\nOf course, we can do much more with symbols than \u2018+\u2019 and \u2018*\u2019. Just like for NDArrays, a lot of operations are defined (math, formatting, etc.). You should take some time to explore the API.\n\nSo now we know how do define our computing steps. Let\u2019s see how we can apply them to actual data.\n\nLet\u2019s continue with the example above. Here, I\u2019d like to set \u2018A\u2019 to 1, \u2018B\u2019 to 2, C to \u20183\u2019 and \u2018D\u2019 to 4, which is why I\u2019m creating 4 NDArrays containing a single integer.\n\nNext, I\u2019m binding each NDArray to its corresponding Symbol. Please note that I have to select the context (CPU or GPU) where execution will take place.\n\nNow, it\u2019s time to let our input data flow through the graph in order to get a result: the forward() function will get things going. It returns an array of NDArrays, because a graph could have multiple outputs. Here, we have a single output, holding the value \u201814\u2019 \u2014 which is reassuringly equal to (1*2)+(3*4).\n\nLet\u2019s apply the same graph to four 1000 x 1000 matrices filled with random floats between 0 and 1. All we have to do is define new input data: binding and computing are identical.\n\nPretty cool, isn\u2019t it? This clean separation between data and computation aims at giving us the best of both worlds:\n\nThat\u2019s it for today. In the next article, we\u2019ll look at the Module API, the last one we need to cover before we can start training and using neural networks!"
    },
    {
        "url": "https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab?source=user_profile---------77----------------",
        "title": "An introduction to the MXNet API \u2014 part 1 \u2013",
        "text": "In this series, I will try to give you an overview of the MXnet Deep Learning library: we\u2019ll look at its main features and its Python API (which I suspect will be the #1 choice). Later on, we\u2019ll explore some of the MXNet tutorials and notebooks available online, and we\u2019ll hopefully manage to understand every single line of code!\n\nIf you\u2019d like learn more about the rationale and the architecture of MXNet, you should read this paper, named \u201cMXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems\u201d. We\u2019ll cover most of the concepts presented in the paper, but hopefully in a more accessible way.\n\nI\u2019ll go as slow and explain as much as I need to. Expect minimal math and minimal jargon, but no (intentional) dumbing down. You won\u2019t become an expert \u2014 I ain\u2019t one anyway\u2014 but I hope you\u2019ll learn enough to understand how you can add Deep Learning capabilities to your own applications.\n\nFirst things first: let\u2019s install MXNet. You\u2019ll find the official instructions here, but here are some additional tips.\n\nOne of the cool features of MXNet is that it can run identically on CPU and GPU (we\u2019ll see later how to pick one or the other for our computations). This means that even if your computer doesn\u2019t have an Nvidia GPU (just like my MacBook), you can still write and run MXNet code which you\u2019ll use later on GPU-enabled systems.\n\nIf your computer has such a GPU, that\u2019s great but you need to install the CUDA and cuDNN toolkits, which tends to turn into a nightmare more often than not. At the slightest incompatibility between the MXNet binary and the Nvidia tools, your setup will be broken and you won\u2019t be able to work.\n\nFor this reason, I would strongly advise you to use the Docker images provided on the MXNet website: one for CPU environments, one for GPU environments (which requires nvidia-docker). These images come pre-installed with everything you need and allow you to get started in minutes.\n\nFor what it\u2019s worth, the Docker images also seem to be more up to date than the Python package available through \u2018pip\u2019.\n\nAWS provides you with the Deep Learning AMI, available both for Linux and Ubuntu. This AMI comes pre-installed with many Deep Learning frameworks (MXNet included), as well as all the Nvidia tools and more. No plumbing needed.\n\nYou can run this AMI either on a regular instance or on a GPU instance. If your computer doesn\u2019t have an Nvidia GPU, this might come in handy later when we start training networks: your most inexpensive option will be to use a g2.2xlarge instance at $0.65 per hour.\n\nFor now, a good old-fashioned CPU is all we need! Let\u2019s get started.\n\nThe first part of the MXNet API we\u2019re going to look at in the NDArray API. An NDArray is an n-dimensional array, containing items of identical type and size (32-bit floats, 32-bit integers, etc).\n\nWhy are these arrays important? As explained in a previous article, training and running neural networks involve a lot of math operations. Multi-dimensional arrays is how we\u2019ll store our data.\n\nLet\u2019s take a simple example: categorizing images. The image below represents an handwritten \u20188\u2019, 18x18 pixels.\n\nThis image represents the same image as an 18x18 matrix, with each cell holding the greyscale value for the corresponding pixel: \u20180\u2019 for white, \u2018255\u2019 for black and values in between for 254 shades of grey. This matrix representation is what we\u2019ll run through a neural network to train it in categorizing digits from 0 to 9.\n\nNow imagine that we use colour images instead of greyscale images. Each image would now be described using 3 matrices, one for each colour, so our input data is now a little more complicated.\n\nLet\u2019s go one step further. Imagine we\u2019re doing real time image recognition for autonomous driving: in order to make decisions on quality up-to-date data, we\u2019re using 1000 x 1000-pixel RGB images at 30 frames per second. Every second, we\u2019ll have to handle 90 1000 x 1000 matrices (30 frames x 3 colours). If each pixel is represented as a 32-bit value, that\u2019s 90 x 1000 x 1000 x 4 bytes, more or less 343 Megabytes. If you have multiple cameras, things add up pretty quick.\n\nThat\u2019s a lot of data to pump through a neural network: for maximal performance (i.e. minimal latency), GPUs don\u2019t process images one by one: instead, they process them in batches. If we use a batch size of 8, our neural network will process input data in chunks of 1000 x 1000 x 24, i.e. a 3-dimensional array holding 8 1000 x 1000 images in 3 colours.\n\nNow that we know why NDArrays are important, let\u2019s look at how they work (yeah, code at last!). If you\u2019ve worked with the numpy Python library, good news: NDArrays are extremely similar and you probably know most of the API, which is fully documented here.\n\nLet\u2019s start with the basics. No explanation needed, I suppose :)\n\nBy default, an NDArray holds 32-bit floats, but we can customize that.\n\nPrinting an NDArray is as easy as this.\n\nAll the math operators you\u2019d expect are available. Let\u2019s try an element-wise matrix multiplication.\n\nHow about an proper matrix multiplication (aka \u2018dot product\u2019)?\n\nLet\u2019s try something a little more complicated:\n\nYou should now know enough to start playing with NDArrays. There are also higher level functions to build neural networks (FullyConnected, etc.) but we\u2019ll study them when we start looking at actual networks.\n\nThat\u2019s it for today. In the next post, we\u2019ll look at the Symbol API, which allows us to define data flows, which is pretty much what neural networks are all about. Thanks for reading and stay tuned."
    },
    {
        "url": "https://medium.com/@julsimon/freebsd-from-cd-rom-to-cloud-1322e5591d99?source=user_profile---------78----------------",
        "title": "FreeBSD, from CD-ROM to Cloud \u2013 Julien Simon \u2013",
        "text": "I\u2019ve been using Open Source for 25 years (man, what a depressing way to start this article). Linux has taken over the world, but I still have a soft spot for the BSD family in general and for FreeBSD in particular. Blame it on the colourful cast of characters who build the BSD OSes: Bill Joy, Kirk McKusick and so on. And yeah, blame it on the Daemon t-shirts ;)\n\nSo, for the sake of having fun and perpetuating the legacy, how about we start a FreeBSD instance on AWS and see where this could lead us?\n\nThe good news is that it couldn\u2019t be easier. Just log in to your AWS account and head for the AWS Marketplace: in a couple of clicks, you\u2019ll find images for FreeBSD 10 & 11. Kudos to Colin Perceval for maintaining them.\n\nNext, we need to pick an instance type. One of the perks of working for AWS is not paying for infrastructure, so why not pick something really large, like i2.8xlarge: 32 vCPUs, 244GB RAM, 8x800GB local SSD storage, 10Gb networking. I\u2019d have gone for the i3 family, but for reasons unknown to me, the FreeBSD AMI doesn\u2019t support it. Probably a driver issue, I\u2019ll look into it.\n\nA few minutes later, the instance is ready.\n\nWhat now? Back in the day, I loved rebuilding everything from source. It was a great way to learn and that mystical \u201cmake world\u201d command fascinated me. It would be fun to do that again, hey? Let\u2019s go!\n\nFirst things first: storage. The source distribution is pretty large and building it requires even more space of course. Well, this instance does have an EBS volume attached, but it also has a ton of ultra-fast local storage (aka instance store in AWS terminology). Isn\u2019t this a great opportunity to use ZFS (another one of my obsessions)? Of course it is.\n\nThis is how these 8 disks appear on the instance:\n\nLet\u2019s switch to root and initialise them with a single slice. First, I need to run an arcane \u2018sysctl\u2019 command allowing me to do this without having to drop to single-user mode (yes, I\u2019m lazy\u2026and I want to keep this short!).\n\nNow we can create our ZFS storage. Let\u2019s use 4 disks for source files and 4 disks for object files. This is all it takes:\n\nLet\u2019s now fetch all FreeBSD 11 sources and extract them:\n\nWe\u2019re ready to build. This instance has 32 vCPUs, so let\u2019s take advantage of them and run a parallel build:\n\nWhile we wait for this to complete, \u2018top\u2019 is interesting to look at. All cores busy, 96% user time, 0.1% idle (instance store FTW). Perfect!\n\n17 minutes. Back in the day, this would take 12+ hours on my high-end PC. I would leave it on all night, only to find out at breakfast that the goddamn build had failed in the middle of the night. And yes, that would ruin my day.\n\nOf course, there would plenty more to explore \u2014 and maybe we\u2019ll dive deeper in future stories \u2014 but that\u2019s it for now. Thanks for reading."
    },
    {
        "url": "https://towardsdatascience.com/fascinating-tales-of-a-strange-tomorrow-72048639e754?source=user_profile---------79----------------",
        "title": "Fascinating Tales of a Strange Tomorrow \u2013",
        "text": "The topic of this workshop was \u201cArtificial Intelligence\u201d, a term coined by McCarthy himself, which he defined this way: \u201cEvery aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it\u201d. Wouldn\u2019t it be great if the Dartmouth workshop had actually be triggered by John McCarthy seeing the \u201cForbidden Planet\u201d and then going home thinking: \u201cLet\u2019s build Robbie\u201d? That\u2019s probably not true at all, though. Oh well. Anyway, the group got to work and laid the foundations of Artificial Intelligence as we know it. In fact, most of the participants devoted their entire career to furthering the state of the art on AI, receiving no less than four Turing Awards in the process: Marvin Minsky[3] in 1969, John McCarthy in 1971, Herbert Simon[4] & Allen Newell[5] in 1975.\n\nDuring the early years of AI, these bright scientists made predictions, such as: \u00b7 1958, Herbert Simon and Allen Newell: \u201cWithin 10 years a digital computer will be the world\u2019s chess champion.\u201d \u00b7 1965, Herbert Simon: \u201cMachines will be capable, within 20 years, of doing any work a man can do.\u201d \u00b7 1967 Marvin Minsky: \u201cWithin a generation, the problem of creating \u2018artificial intelligence\u2019 will substantially be solved.\u201d \u00b7 1970 Marvin Minsky: \u201cIn from 3 to 8 years, we will have a machine with the general intelligence of an average human being.\u201d Predicting the future is always risky business, but still\u2026 This raises an daunting question: how could such brilliant minds be so awfully wrong about what AI would (or wouldn\u2019t) achieve in a reasonable time frame? Don\u2019t worry, we\u2019ll answer this question later on. Unfortunately, repeated failures to achieve significant progress became a trademark of Artificial Intelligence. Expectations were high, little or no results were delivered, funds were cut and projects were abandoned. Unsurprisingly, these multiple \u201cAI winters\u201d discouraged all but the most hardcore supporters. The most glaring symbol of this disillusion came from Marvin Minsky himself. In 2001, he gave a talk named \u201cIt\u2019s 2001: Where is HAL?\u201d referring of course to the HAL computer in Stanley Kubrick\u2019s movie \u201c2001: A Space Odyssey\u201d. This is all the more significant that back in 1968, Minsky actually advised Kubrick during the making of the movie. In this talk, he notably addresses the \u201cCommon Sense issue\u201d in non-ambiguous terms: \u201cNo program today can distinguish a dog from a cat, or recognize objects in typical rooms, or answer questions that 4-year-olds can!\u201d\n\nBottom line: AI is cool to play with in a lab environment, but it will never achieve anything in the real world. Case closed. Meanwhile, on the US West Coast\u2026 While AI researchers despaired in their labs, a number of startups were reinventing the world: Amazon, Google, Yahoo, later joined by Facebook and a few others were growing their web platforms at a frantic pace. In the process, they were acquiring users by the millions and piling up mountains of data. It soon became clear that this data was a goldmine, if it could actually be mined! Using commodity hardware, these companies\u2019 engineers set on a quest to design and build data processing platforms that would allow them to crunch raw data and extract business value that could turn into revenue\u2026 always a key goal for fast-growing startups! A major milestone was reached in December 2004, when Google released the famous Map Reduce paper[6], where they described \u00ab a programming model and an associated implementation for processing and generating large data sets \u00bb. Not to be outdone, Yahoo implemented the ideas described in this paper and released a first version of their project in April 2006: Hadoop[7] was born. Gasoline waiting for a match: the Machine Learning explosion happened and the rest, as they say, is history. 2010 or so: Machine Learning is now a commodity. Customers have a wide range of options, from DIY to Machine Learning as a Service. Everything is great in Data World. But is it really? Yes, Machine Learning helped us make a lot of applications \u201csmarter\u201d but did we make significant progress on Artificial Intelligence? In other words, are we any closer to \u201cbuilding HAL\u201d? Well\u2026 no. Let\u2019s try to understand why. One of the first steps in building a Machine Learning application is called \u201cfeature extraction\u201d. In a nutshell, this is a step where Data Scientists explore the data set to figure out which variables are meaningful in predicting or classifying data and which aren\u2019t. Although this is still mostly a lengthy manual process, it\u2019s now well understood and works nicely on structured or semi-structured data such as web logs or sales data. However, it doesn\u2019t work for complex AI problems such as computer vision or computer speech, simply because it\u2019s quite impossible to define formally what the features are: for example, what makes a cat a cat? And how is a cat different from a dog? Or from a lion? To put it simply, traditional Machine Learning doesn\u2019t solve this kind of problem, which is why new tools are needed. Enter neural networks! New tools? Hardly! In 1957, Frank Rosenblatt designed an electro-mechanical neural network, the Perceptron[8], which he trained to recognize images (20x20 \u201cpixels\u201d). In 1975, Paul Werbos published a article describing \u201cbackpropagation\u201d[9], an algorithm allowing better and faster training of neural networks. So, if neural networks have been around for so long, surely they must be partly responsible for failed AI attempts, right? Should they really be resurrected? Why would they suddenly be successful? Very valid questions indeed. Let\u2019s first take a quick look at how neural networks work. A neuron is a simple construct, which sums multiple weighted inputs to produce an output. Neurons are organized in layers, where the output of each neuron in layer \u2019n\u2019 serves as an input to each neuron in layer \u2018n+1\u2019. The first layer is called the input layer and is fed with the input data, say the pixel values of an image. The last layer is called the output layer and produces the result, say a category number for the image (\u201cthis is a dog\u201d).\n\nThe beauty of neural networks is that they\u2019re able to self-organize: given a large enough data set (say, images as inputs and category labels as outputs), a neural network is able to learn automatically how to produce correct answers Thanks to an iterative training process, it\u2019s able to discover the features which allow images to be categorized, and adjusts weights repeatedly to reach the best result, i.e. the one with the smallest error rate. The training phase and its automatic feature discovery are well adapted to solving informal problems, but here\u2019s the catch: they involve a lot of math operations which tend to grow exponentially as data size increases (think high-resolution pictures) and as the number of layers increases. This problem is called the \u201cCurse of Dimensionality\u201d and it\u2019s one of the major reasons why neural networks stagnated for decades: there was simply not enough computing power available to run them at scale. Nor was enough data available. Neural networks need a lot of data to learn properly. The more data, the better! Until recently, it was simply not possible to gather and store vast amounts of digital data. Do you remember punch cards or floppy disks? A significant breakthrough happened in 1998 when Yann Le Cun invented Convolutional Neural Networks[10], a new breed of multi-layered networks (hence the term \u201cDeep Learning\u201d). In a nutshell, CNNs are able to extract features efficiently while reducing the size of input data: this allows smaller networks to be used for classification, which dramatically reduces the computing cost of network training. This approach was so successful that banks adopted CNN-driven systems to automate handwriting recognition for checks. This was an encouraging accomplishment for neural networks\u2026 but the best was still to come! By the late 2000s, three quasi-simultaneous events made large-scale neural networks possible. First, large data sets became widely available. Text, pictures, movies, music: everything was suddenly digital and could be used to train neural networks. Today, the ImageNet[11] database holds over 14 million labeled images and researchers worldwide use it to compete every year[12] in build the most successful image detection and classification network (more on this later). Then, researchers were able to leverage the spectacular parallel processing power of Graphics Processing Units (GPUs) to train large neural networks. Can you believe that the ones that won the 2015 and 2016 ImageNet competition have respectively 152 and 269 layers? Last but not least, Cloud computing brought elasticity and scalability to developers and researchers, allowing them to use as much infrastructure as needed for training\u2026 without having to build, run or pay for it long term. The combination of these three factors helped neural networks deliver on their 60-year old promise. State of the art networks are now able to classify images faster and more accurately than any human (less than 3% error vs. 5% for humans). Devices like the Amazon Echo understand natural language and speak back at us. Autonomous cars are becoming a reality. And the list of AI applications grows every day. Wouldn\u2019t you like to add yours? Number of layers and error rate of ILSVRC winners How AWS can help you build Deep Learning applications AWS provides everything you need to start building Deep Learning applications: \u00b7 A wide range of Amazon EC2 instances to build and train your models, with your choice of CPU[13], GPU[14] [15] or even FPGA[16]. \u00b7 The Deep Learning Amazon Machine Image[17], a collection of pre-installed tools and libraries: mxnet[18] (which AWS officially supports), Theano, Caffe, TensorFlow, Torch, Anaconda and more. \u00b7 High-level AI services[19] for image recognition (Amazon Rekognition), speech to text (Amazon Polly) and chatbots (Amazon Lex). The choice is yours, just get started and help Science catch up with Fiction! Artificial Intelligence is making progress every day. One can only wonder what is coming next! Will machines learn how to understand humans \u2014 not the other way around? Will they help humans understand each other? Will they end up ruling the world? Whatever happens, these will be fascinating tales of a strange tomorrow. Note: this is an edited transcript of one of my current keynote talks. Original slides are available here."
    },
    {
        "url": "https://read.acloud.guru/from-cloud-computing-to-edge-computing-5c639b1796da?source=user_profile---------80----------------",
        "title": "From Cloud Computing to Edge Computing \u2013",
        "text": "The AWS global infrastructure now offers 16 regions and 73 edge locations \u2014 bringing innovative infrastructure everywhere\n\nSince 2006, Amazon Web Services has been striving to bring customers innovative, highly available and secure infrastructure services.\n\nToday, these services rely on global infrastructure spread across sixteen regions. Two more regions will go live in 2017, one in France and one in China [1].\n\nCrucial as they are, these regions are not the whole story. Since 2008, AWS has also been building their own Content Delivery Network, named Amazon CloudFront. CloudFront enables customers to improve the performance of their applications by serving static and dynamic content as close to end-users as possible.\n\nThanks to CloudFront, in 2012, NASA shared worldwide the pictures of Curiosity\u2019s landing on Mars. A huge event, which generated more traffic than the Olympic Games [2].\n\nThe purpose of the AWS Edge Locations is not only to serve traffic, far from it. They also help customers raise the bar on platform security, as they\u2019re integrated with a DDoS protection service (AWS Shield, launched at re:Invent 2016 [3]) and a Web Application Firewall Service (AWS WAF [4]).\n\nThanks to this defense in depth strategy, it\u2019s possible to mitigate attacks as soon as possible, before they even reach customer infrastructure hosted in the AWS regions.\n\nAt re:Invent 2016, AWS has brought to Edge Locations the ability to run code on incoming and outgoing traffic.\n\nWith this new service, named Lambda@Edge [5], customers can now trigger AWS Lambda functions that process and modify HTTP requests sent to or coming from the origin, i.e. the server hosting the web application.\n\nFor example, Lambda@Edge allows content to be customized depending on terminal properties: if the terminal is a smartphone, it may not be desirable to serve a very large, high resolution image. A smaller, lighter image could do and help optimize user experience by cutting down on transfer time, as well as save money by serving less data.\n\nThis service could also be used to authenticate users and filter unwanted traffic sooner, or even to run A/B tests by showing different content to different groups of users.\n\nLambda@Edge is a first step towards running code outside of AWS regions. Still, customers have very diverse use cases and infrastructure needs, which led AWS to design new services allowing them to process data as close the its source as possible, as soon as it is produced and without any need to transfer it to infrastructure hosted in AWS regions. These services even make it possible to process data in areas where network connectivity is sparse or even non-existent.\n\nLaunched in 2015, AWS Snowball is a portable storage equipment able to hold 100 Terabytes. Targeted at huge data transfers between on-premises infrastructure and AWS, it is used by Digital Globe, one of the main providers of space imagery and geospatial content, to transfer Petabytes to the Cloud.\n\nAt re:Invent 2016, AWS launched AWS Snowball Edge [6], a new version of Snowball able to run code locally thanks to an embedded Lambda architecture named AWS Greengrass (more on this in a minute). Thus, customers can now deploy on-premise storage and compute capabilities working with the same APIs as the ones they use in their AWS infrastructure.\n\nThanks to computing power equivalent to a 16-core, 64 GB RAM server, customers may apply complex processing to their data before sending it to AWS: compression, formatting, ETL, etc. And they don\u2019t have to overload their own infrastructure to do it.\n\nSnowball Edge is already used for mission-critical applications. For instance, Philips Healthcare is deploying it in Intensive Care Units, where medical teams use it to store, process and visualise vital signs of their patients: this guarantees uninterrupted care, even if the hospital faces a major IT outage. The US Department of Defense also relies on Snowball Edge to bring storage and computing capabilities to isolated areas [7].\n\nIntegrated in Snowball Edge, AWS Greengrass is a service targeted at devices with limited hardware resources (1GHz CPU, 128MB RAM). It gives them the ability to run code locally, even if network connectivity is not available.\n\nWith Greengrass, IoT devices can store and process locally the data they collect. They can also talk to one another without going through the Cloud.\n\nOf course, whenever possible, Greengrass will allow IoT devices to connect to the Cloud, in order to synchronize and transfer data that has been processed and aggregated locally. Obviously, this will help cut down on bandwidth requirements and costs.\n\nAs you can see, traditional boundaries between infrastructure and devices are getting fuzzier every day. New use cases require local storage and processing capabilities, close to data sources and even when network connectivity is unavailable.\n\nSince 2006, AWS has been working relentlessly to bring customers the innovative, highly available and secure services they\u2019ve been requesting.\n\nThis is how Lambda@Edge, Snowball Edge and Greengrass were born and we\u2019re impatient to see what customers will build with these new services."
    },
    {
        "url": "https://medium.com/@julsimon/exploring-the-gdelt-data-set-with-amazon-athena-a6f7b1d67a6e?source=user_profile---------81----------------",
        "title": "Exploring the GDELT data set with Amazon Athena \u2013 Julien Simon \u2013",
        "text": "The Global Database of Events, Language and Tone (GDELT) Project monitors the world\u2019s broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organisations, counts, themes, sources, emotions, counts, quotes, images and events driving our global society every second of every day.\n\nData set v1.0 is publicly available in S3 and I figured that this would be as good an excuse as any to play with my new favorite analytics service, Amazon Athena. Yes, Redshift, I still love you, baby ;)\n\nThe data set contains (at the time to writing) 1,542 uncompressed CSV files: 58 columns, 440+ million lines, 140+ GB. It is updated daily.\n\nData is formatted according to a couple of specs (here and here, PDF). A few lookup tables are also available here (country codes, organisation names, etc.).\n\nAll right, let\u2019s get to business. As you may know, Athena is able to query data hosted in S3 : no infrastructure to launch or manage, no data preparation, no loading time. All we have to do is create a table using the Hive DDL.\n\nThis is how we do it. All scripts and queries are available on Github, including how to create lookup tables.\n\nQuite a mouthful, but pretty straightforward: just read the doc, define as many columns as needed with the right type and point to the S3 bucket holding all files. One immediate benefit of this is that whenever we run queries, Athena will automatically use all available files, including the additional one that\u2019s delivered daily. Zero work!\n\nOK, let\u2019s run some queries. I\u2019m using SQL Workbench/J with the Athena JDBC driver.\n\nNone of these took more than 30 seconds and that\u2019s with uncompressed CSV, the least performing data format possible. Converting the data set columnar formats such as Parquet or Orc would yield a massive improvement, but it\u2019s extra work, so why bother? ;)\n\nThat\u2019s it for today. Thanks for reading!"
    },
    {
        "url": "https://medium.com/@julsimon/amazon-polly-hello-world-literally-812de2c620f4?source=user_profile---------82----------------",
        "title": "Amazon Polly: \u201cHello World\u201d\u2026 literally! \u2013 Julien Simon \u2013",
        "text": "Today, AWS announced a new text-to-speech service, called Polly. Well\u2026 I had to try it :D\n\nHere\u2019s a *very* basic example in Python. There is much more to Polly, but this should get you started. You can list all available voices with \u2018aws polly describe-voices\u2019.\n\nAnd yes, I\u2019m sure there are more clever ways to play sound files in Python, but they\u2019re beyond my weak skills, so there ;)\n\nVery fun service. I see a lot of chatty build servers on the horizon, yelling in German at careless developers. Oh yes, pure bliss :D"
    },
    {
        "url": "https://medium.com/@julsimon/a-hands-on-look-at-the-amazon-rekognition-api-e30e19e7d88b?source=user_profile---------83----------------",
        "title": "A hands-on look at the Amazon Rekognition API \u2013 Julien Simon \u2013",
        "text": "Amazon Rekognition is a Deep Learning based image analysis service. Don\u2019t worry though, you won\u2019t have to wade through Machine Learning / Deep Learning mumbo jumbo to work with Recognition. Quite the contrary, as Rekognition provides a very easy-to-use API. \n\n \n\n It allows developers to:\n\nAs usual, this service can be used with the AWS CLI (as in \u2018aws rekognition\u2019 ), or with one of our language SDKs. I\u2019ll show you some CLI examples first and then we\u2019ll use the popular Python SDK, aka boto3.\n\nFirst things first: how do we send images for processing? Two options: send the image as a byte blob or put it in S3. I suspect the most of use will use the second option, so that\u2019s what I\u2019ll use. Time to play!\n\nJSON, the cornerstone of any nutritious service. So, what do we have here? A face has been found with 99.99+% confidence. It\u2019s delimited by the BoundingBox coordinates (top left corner, face width, face height): these are fractional values with respect to the total height and width of the image. Eyes, nose and mouth have been located too (that\u2019s reassuring).\n\nNow, let\u2019s see what Rekognition can tell us about this second picture.\n\n{ \"Labels\": [ { \"Confidence\": 99.29261779785156, \"Name\": \"Human\" }, { \"Confidence\": 99.2958984375, \"Name\": \"People\" }, { \"Confidence\": 99.2958984375, \"Name\": \"Person\" }, { \"Confidence\": 99.2667007446289, \"Name\": \"Book\" }, { \"Confidence\": 99.2667007446289, \"Name\": \"Text\" }, { \"Confidence\": 71.22590637207031, \"Name\": \"Bookcase\" }, { \"Confidence\": 71.22590637207031, \"Name\": \"Furniture\" }, { \"Confidence\": 71.22590637207031, \"Name\": \"Shelf\" }, { \"Confidence\": 52.00172805786133, \"Name\": \"Portrait\" }, { \"Confidence\": 52.00172805786133, \"Name\": \"Selfie\" } ] }\n\nWith a very good level of confidence, this is the picture of a human with books on a bookshelf, possibly a portrait. A pretty good summary. Let's compare the two previous pictures. Is this truly the same person? Spoiler: yes, although I look 15 years older on the first one. Note to self: no more promo shots after 36 sleepless hours :D\n\nSimilarity is 98%. Jet lag or not, I'm always the same me. See how simple this service is? I don't see how they could have made it easier. How long would it take to design, build and *train* something like this on your own? I have really no idea and to I don't intend to find out!\n\nEnough CLI, let\u2019s switch to Python and run more visual examples. For this purpose, I\u2019ve written a couple of scripts (available here), using boto3 and the Pillow image processing library.\n\nAll images must be present with the same name both locally and in S3 . The last parameter for both scripts allows you to skip the copy to S3 if the file is already there. Hopefully, the code reads like well-written prose (hi Uncle Bob). If not, blame jet lag (yes, it\u2019s the root of all evil). Anyway, there\u2019s nothing complicated here, I\u2019m sure you\u2019ll figure it out in no time. Let\u2019s play some more!\n\nSay hi to Romain, C\u00e9dric and Damian, my friendly AWS colleagues. Rekognition sees 4 males, 1 with a beard, 2 with eyeglasses, all of them very happy... and I'm the calmest of the bunch, how about that. Amazingly, Rekognition manages to catch my hardly visible laptop (left edge of the picture, on the table).\n\nHere\u2019s a tougher one (Hallo to my German friends).\n\nWow, 15 people, including partial faces. All genders are correct. Emotions are mostly ok, but we definitely need to add 'DRUNK' to the list ;) The labels are spot on: a crowd of men and women drinking alcohol. Let's try another one. Low res, low quality.\n\nLabels are fine, except for 'American Football'. 83%??? Gimme a break, the training set needs more Soccer images! In addition, I don't think number 4 is wearing eyeglasses, but again this is a low res picture. Apart from this, Rekognition correctly picked up all faces and funny enough, the expressions make sense too: \"sad\" and \"surprised\" are definitely how these guys must have felt against the legendary Diego! A last one for the road: how about this complex abstract-ish nighttime picture of Shinjuku?\n\nNote that I lowered the confidence threshold from 75% to 50% get more labels. Still, Rekognition does a good job. It also gets the girl's face and yes, she does look quite sad. The Anime face isn't detected but I guess this is the desired behavior. Alright, enough detection. Let's now try to match faces, using some of the previous pictures as well as some new ones.\n\nQuite good! The last one is particularly nice, given the distance, the angle and the poor lighting (see actual picture above). These are just a few examples and I'm sure you can't wait to try your own. Hopefully this post has given you a visual, hands-on overview of the Recognition service and how user-friendly it is. I didn't cover face collections, but the API is pretty much what you'd expect (create, delete, etc.).\n\nFeel free to explore and experiment. Until we meet again, keep rockin\u2019."
    },
    {
        "url": "https://medium.com/@julsimon/the-lost-tales-of-platform-design-accaf4004527?source=user_profile---------84----------------",
        "title": "The Lost Tales of Platform Design \u2013 Julien Simon \u2013",
        "text": "This is a presentation I\u2019ve meant to build for a while. I guess I was just waiting for an excuse to spend enough time to do so. My talk at The Family\u2019s Lion program (http://joinlion.co) gave me this excuse and I want to thank them for the opportunity.\n\nIn a nutshell, I\u2019m sick and tired of the current state of Software Engineering. In hindsight, it\u2019s probably the main reason why I decided to step away from CTO roles and try something different. No more babysitting \u201cengineers\u201d who think they know it all and have neither interest nor respect for the vast body of knowledge that has been built over the last 60 years.\n\nSome would say that this is precisely what management is about. Teaching, training, coaxing, blah blah blah. Maybe, maybe not. What I know is that my time is my most precious asset. I will never waste it again on people who are not willing to listen and learn from those who have been there before. Especially when top management doesn\u2019t have the guts to hire, fire or reward accordingly (and of course they\u2019ll say otherwise).\n\nMaybe one day I\u2019ll find a team of like-minded code warriors, or maybe I\u2019ll feel like building one again. I\u2019m not holding by breath. AWS is home :)\n\nDisclaimer: all opinions are my own (what did you expect?)."
    },
    {
        "url": "https://medium.com/@julsimon/a-silly-little-script-for-amazon-redshift-a11ff7f45c67?source=user_profile---------85----------------",
        "title": "A silly little script for Amazon Redshift \u2013 Julien Simon \u2013",
        "text": "One of the great things about Amazon Redshift is that it\u2019s based on PostgreSQL. Hence, our favorite PostgreSQL tools can be used, notably psql. However, building and typing the full connection chain to a Redshift cluster is a bit of a drag.\n\nSo, here\u2019s a simple script (source on Github) for the lazy ones among us. It simply requires a cluster name, a database name and a user name. You will be prompted for a password, unless you have a matching entry in your .pgpass file. As a bonus, your connection will be SSL-enabled. Ain\u2019t life grand?"
    },
    {
        "url": "https://medium.com/@julsimon/a-silly-little-script-for-amazon-ecs-6f05d66b2963?source=user_profile---------86----------------",
        "title": "A silly little script for Amazon ECS \u2013 Julien Simon \u2013",
        "text": "I\u2019m currently spending a lot of time playing with Amazon ECS and Docker.\n\nThe ecs-cli tool is very convenient to manage clusters and tasks, but I needed a few extra things (as always) and crammed them into the ecs-find script.\n\nHopefully, this might come in handy for you too :)"
    },
    {
        "url": "https://medium.com/@julsimon/test-drive-amazon-machine-learning-redshift-ef9250610acf?source=user_profile---------87----------------",
        "title": "Test drive: Amazon Machine Learning + Redshift \u2013 Julien Simon \u2013",
        "text": "Last week, AWS launched their flavor of \u201cMachine Learning as a service\u201d, aka Amazon Machine Learning. It was not a moment too soon, given the number of existing cloud-based ML propositions. To name just a few: BigML, Qubole and yes, Azure Machine Learning (pretty impressive, I\u2019m sorry to admit). So, here it is finally. Let\u2019s take it for a ride.\n\nFirst things first: some data is needed. Time to use a little Java program that I wrote to pump out test data simulating an e-commerce web log (see Generator.java in https://github.com/juliensimon/DataStuff).\n\nHere\u2019s the format, columns are pretty self-explanatory: Nothing fancy, but it should do the trick. Next step: connect to my super fancy 1-node Redshift cluster and create an appropriate table for this data:\n\nNext, let\u2019s generate 10,000,000 lines, write them in a CSV file and upload it to my favorite S3 bucket located in eu-west-1. And now the AWS fun begins! Right now, Amazon ML is only available in us-east-1, which means that your Redshift cluster must be in the same region, as well as the S3 bucket used to output files (as I later found out). Bottom line: if everything is in us-east-1 for now, your life will be easier ;)\n\nLucky me, the only cross-region operation allowed in this scenario is copying data from S3 to Redshift, here\u2019s how: For the record, this took just under a minute for 450MB. That\u2019s about 100MB per second sustained. Not bad :)\n\nLet\u2019s take a quick look: SELECT * FROM mydata LIMIT 10;\n\nLooks good. Time to fire up Amazon ML. The process is quite simple:\n\nCreating the datasource from Redshift is straightforward: cluster id, credentials, table name, SQL statement to build the test data.\n\nOnce connected to Redshift, Amazon ML figures out the schema and data types:\n\nNow, let\u2019s select our target column (the one we want to predict):\n\nNext, we can customize the model. Since this is a numerical value, Amazon ML will use a numerical regression algorithm. If we had picked a boolean value, a different algorithm would have been used. Keep on eye on this in future releases, I\u2019m sure AWS will add more algos and allow users to tweak them much more than today. As you can see, 70% of data is used to build the model, 30% to evaluate it.\n\nNext, Amazon ML ingests the data: In our case, this means 10 million lines, which takes a little while. You can see the different tasks: splitting the data, building the model, evaluating it.\n\nA few coffees later, all tasks are completed. The longest one was by far building the ML model. The whole process lasted just under a hour (reminder: 10 columns, 10 millions lines).\n\nSo, is this model any good? Amazon ML gives limited information for now, but here it is:\n\nThat promising \u201cExplore the model performance\u201d button displays a distribution curve of residuals for the part of the data set used to evaluate the model. Nothing extraordinary.\n\nAs a sidenote, I think it\u2019s pretty interesting to see that a model can be build from totally random data. What does this say about the Java random generator? I\u2019m not sure.\n\nNow, we\u2019re ready to predict! Amazon ML supports batch prediction and real-time prediction through an API. I\u2019ll use batch for now. Using a second data set of 10,000 lines missing the \u2018basket\u2019 column, let\u2019s build a second data source (from S3 this time):\n\nTwo news tasks are created: ingest the data from S3 and predict. After a 3\u20134 minutes, prediction is complete:\n\nA nice distribution curve of predicted values is also available.\n\nActual predicted values are available in a gzip\u2019ed text file in S3: Pretty cool\u2026 but one last question needs to be answered. How much does it cost? Well, I did push the envelope all afternoon and so\u2026\n\nOver a thousand bucks. Ouch! Expensive fun indeed. I guess I\u2019ll expense that one :D\n\nOne thousand predictions cost $0.1. So, the scenario I described (model building plus 10K predictions) only costs a few dollars (thanks Jamie @ideasasylum for pointing it out).\n\nHowever, if you decide to use live prediction on a high-traffic website or if you want to go crazy on data-mining, costs will rise VERY quickly. Caveat emptor. AWS has an history of adapting prices pretty quickly, let\u2019s see what happens. Final words? Amazon ML delivers prediction at scale. Ease of use and documentation are what you\u2019d expect from AWS. Features are pretty limited and the UI is still pretty rough but good things come to those who wait, I guess. Cost rises quickly, so make sure you set and track ROI targets on your ML scenarios. Easier said than done\u2026 and that\u2019s another story :) Till next time, keep crunchin\u2019!"
    },
    {
        "url": "https://medium.com/@julsimon/a-unix-bi-bli-ography-and-then-some-f6b93847aed3?source=user_profile---------88----------------",
        "title": "A UNIX bi(bli)ography\u2026 and then some \u2013 Julien Simon \u2013",
        "text": "Here\u2019s something I\u2019ve been meaning to do for a while and today is the day. Blame it on dark winter boredom, on accidentally stumbling on that dusty cardboard box in the cellar or on mid-life crisis. Whatever. Some of you out there will get it and I salute you.\n\nMy interest in UNIX started in early 1992 while studying for my Computer Science engineering degree. R\u00e9my Card was teaching the UNIX class, so I got a Linux system running from day 1 :) The first release I installed was 0.13, which then became 0.95 in March \u201992 (the first release with X Window support). I still remember downloading the floppy images from Ren\u00e9 Cougnenc\u2019s BBS (RIP friend).\n\nShortly after, R\u00e9my et Ren\u00e9 et myself co-wrote what it possibly the first magazine article ever published on France on Linux (available here).\n\nThere wasn\u2019t a lot of Linux documentation at that time\u2026 and none in French. So I started writing my own guide, called \u201cLe Guide du Rootard pour Linux (GRL)\u201d. I also translated the Linux info sheet with some help from Ren\u00e9. For a while, these two documents were the only Linux documentation in French (here\u2019s a later post from May \u201994 in fr.comp.os.linux).\n\nDuring that time, I got to meet Linus Torvalds a few times. I also attended the 1st Linux Conference in Heidelberg in \u201994 and met Richard Stallman there.\n\nBy then, I had started to work \u2014 which meant that I had less time \u2014 and I was also growing increasingly annoyed by the business circus surrounding Linux. In May \u201995, I decided to stop maintaining the GRL: here\u2019s the farewell thread in fr.comp.os.linux, with R\u00e9my saying that I \u201cmassively contributed to Linux usage in France\u201d. That was 13 years ago. Man! In late \u201996, I somehow found the time to work on a French translation of Kirk McKusick\u2019s 4.4BSD book (see below)\u2026 and I swore I would never do it again. This book consumed all my evenings (and many nights) for months. I guess having the privilege to exchange some e-mails with Kirk was worth it all! And then life went on. I\u2019m too young and too alive for a proper eulogy, so this is where I stop :) Without further ado, here\u2019s my UNIX bibliography, with a couple of other OS books thrown in for good measure. I added links to Amazon to for those of you who may want to complete their collection :)\n\nThis book is a commented printout of the Unix V6 kernel code. Obviously influential\u2026 if you had a copy at the time. Still a great read, showing the simplicity and purity or early UNIX. And definitely nice to have for historical purposes.\n\n2. The Design of the UNIX Operating System, by M.J. Bach (published 1986)\n\nThe first proper book on the UNIX kernel (mostly System V Release 2), covering all major topics: process management, file system, I/O, etc. Hardly any code, but lots of detailed explanations, figures and algorithms. A very good book that helped me a lot in the early days. If you were working on System V UNIX in the early nineties, this was the one to have.\n\n3. The Design and Implementation of the 4.3BSD UNIX Operating System, by S. Leffler, K. McKusick, M. Karels & J. Quaterman (published 1989)\n\nWhat can I say? To UNIX geeks, this is the Ancient Testament. The story has been told many times, but the importance of Berkeley UNIces (4.2, 4.3 & 4.4) cannot be stressed enough, both on a technical and a \u201cphilosophical\u201d point of view. Open Source as we know it would not exist were it not for a bunch of crazies at the University of Berkeley.\n\n4. The Basic Kernel \u2014 Source Code Secrets, by W. Jolitz and L. Jolitz (written in 1991, published 1996)\n\n386BSD, the OS that never really was\u2026 Unfortunately, this book is not a printout of the \u201cPorting UNIX to the 386\u201d series published in Dr Dobbs magazine in 1991. It\u2019s mostly a commentary of low-level kernel code (mostly dealing with process management), which I must say I did not find particularly fascinating.\n\nAlthough this is volume 1, no additional volume was ever published. I think I know why\u2026\n\n5. Advanced Programming in the UNIX Environment, by R. Stevens (published 1992)\n\nThe best UNIX programming book I read, which covers UNIX concepts, system calls, the ANSI C library, etc. Tons of real-life code is also provided. The late R. Stevens also wrote several other fantastic books on TCP/IP and network programming. He truly was a great technical author.\n\n6. The Design of OS/2, by H Deitel and M. Kogan (published 1992)\n\nNot UNIX but still a very good OS, so superior to Windows at the time (remember Windows 3.0???). A dry but very serious book on designing a true multitasking OS for the i386. I learned a lot from this one, especially with OS/2 running on my triple-boot i486. DOS, OS/2, Linux : yeah baby, those were the days :)\n\nStill not UNIX, but getting closer. Microkernels helped redefine the architecture of traditional operating systems and Mach is probably the most widespread: a number of proper UNIces have been built over the Mach microkernel, from OSF/1 to MacOS X!\n\nBeyond Mach programming, this is a good book to understand what the fuss is all about. I used it in \u201894-\u201996 when I was working on the MASIX project, publishing several papers with Franck Mevel in the process:\n\n- \"Overview of the Masix Distributed Operating System\", SIPAR Workshop on Parallel and Distributed Systems, Biel-Bienne, Switzerland, 1995\n\n- \"Distributed communication services in the Masix system\", Proceedings of the 15th International Conference on Computers and Communications, IEEE, Phoenix, pp 172-178, 1996.\n\n- \"Secure Communication Services in the Masix Distributed Operating System\", Proceedings of the IASTED, NETWORKS, Orlando, pp 5-9, 1996.\n\nOver 1000 pages of UNIX tips, tricks and wizardry. Command line tools, scripting, etc: this book has it all. I have the 1st edition, but the book has been revised several times and a lot of it still applies. This book saved me many times (Ubuntu forums and Google didn\u2019t exist, remember?) and on more quiet occasions, it just looked nicely geeky on my coffee table ;)\n\nLots of books have been written on distributed systems and this is simply the BEST (I have the 2nd edition, but a 4th edition has been published in 2005). It has the most complete and clearest descriptions of many complex distributed algorithms (clocks, transactions, etc). It also covers three microkernels used for UNIX-like distributed systems (Mach, Chorus & Amoeba).\n\n10. The Magic Garden Explained \u2014 The Internals of UNIX System V Release 4, by B. Goodheart & J. Cox (published 1994)\n\nThere was before and after this book, a true landmark in UNIX history. For the first time, this book revealed kernel-level information on SVR4, which was at the core of many commercial UNIces. With accurate, up-to-date and crystal-clear descriptions of kernel data structures, algorithms and principles, this is maybe the best and most usable UNIX kernel book ever written.\n\nThis isn\u2019t a technical book, but a detailed account of the early days told by the people who lived them. If you want to learn how it all started (and sometimes how it all went wrong), this is the book to read. And you also get to see pictures of all the Great Ancients, including Biff the Dog.\n\n12. UNIX Internals \u2014 The New Frontiers, by U. Vahalia (published 1996)\n\nAnother very good kernel-level book, which covers all key areas (process management, IPC, filesystems, etc) as well some more exotic topics like distributed UNIX, NFS or kernel memory allocation (superb section!). The particular interest of this book is that all concepts are with several real-life examples taken from all major UNIces : 4.4BSD, SVR4, Solaris, etc. As far as I know, this was the first book to compare different UNIX versions in such detail.\n\n13. The Design and Implementation of the 4.4BSD Operating System, by K. McKusick, K. Bostic, M. Karels & J. Quaterman (published 1996)\n\nThe New Testament, revised and updated for what is the last UNIX version released by the University of Berkeley. The book is quite verbose and sometimes obscure, but it is invaluable. A lot of 4.4BSD code ended up in many UNIces, especially in Open Source versions like NetBSD, FreeBSD, OpenBSD and Linux.\n\nI loved that 4.4BSD book so much that I worked on the French translation\u2026 and I lived to regret it :) I think my English is quite good and my technical skills aren\u2019t too bad either, but try the section \u201cexplaining\u201d virtual memory management on the VAX: this one alone almost made me quit!\n\nA slightly misleading title, as this book solely talks about SCO UNIX System 5 Release 3.2 for i386 architectures. A good kernel book, if you work(ed) on SCO UNIX or if you want(ed) to learn about a given UNIX implementation for the Intel architecture.\n\nFor my purposes, I found the scope a bit narrow and liked the \u201cMagic Garden\u201d book much better. And no, the \u201cSCO vs the world\u201d soap opera has nothing to do with it!\n\nThe first book on the Solaris kernel (2.5 to 2.7). I was working for Sun Microsystems at the time, so I particularly enjoyed it. Sun is good at writing documentation and although the content is highly technical, this book is an easy read, presenting complex kernel features in great detail.\n\nThe same authors have since then written a Solaris 10 / OpenSolaris book which I have yet to read.\n\nI use this book as a quick reference for common System Administration tasks on Red Hat Linux (RHEL, Fedora & CentOS). What I like about it is that it\u2019s well organized, theory-free and goes straight to the point, which is exactly what I was looking for. I don\u2019t think I need another vanilla UNIX book :) Who knows, maybe I\u2019ll actually go and try to pass the certification\u2026\n\nAll these books would have been of little use without actual code to work on. A little more exploration in The Vault revealed a cool series of vintage CDs, from a time where broadband didn\u2019t exist and even universities had slow Internet access. I\u2019m sure these will bring back some memories :) From top to bottom and from left to right:\n\n- 4.4BSD Lite (1 CD, June \u201894): complete 4.4BSD Lite source distribution, with all AT&T files removed. This is where NetBSD, FreeBSD & OpenBSD come from.\n\n- BSDisc (1 CD, November \u201894): complete source & binaries for NetBSD 1.0 and FreeBSD 2.0. As far as I know, this was the first CD release for both OSes.\n\n- FreeBSD 2.2.5 (4 CDs, November \u201897): full distribution (sources & binaries), plus a live CD and a copy of the CVS repository.\n\n- FreeBSD Toolkit (6 CDs, May \u201899): FreeBSD 2.2, 3.1 & 4.0, plus a ton of packages. It was really time for the DVD-ROM to be invented :)\n\n- GNU \u2014 Free Software for UNIX (1 CD, February \u201896): 83 source packages, with pre-compiled binaries for SunOS 4.1.4 and Solaris 2.4. Emacs, gcc, g++, gdb, perl 4 & 5. Says the backcover: \u201cWith this disc, you can bring any barebones system up to a professional working environment\u201d :)\n\nWow, this has turned into a monster post\u2026 and I still have a few more books somewhere: where on Earth is that great book on Windows NT internals? Did I burn it? Anyway, that\u2019s it for today. Thank you very much for reading this far: if you enjoyed this post, please leave a comment and let\u2019s share some UNIX love :)"
    }
]