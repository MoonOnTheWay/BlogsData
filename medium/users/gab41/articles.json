[
    {
        "url": "https://gab41.lab41.org/introducing-the-voices-obscured-in-complex-environmental-settings-voices-corpus-b7990d080176?source=---------0",
        "title": "Introducing the Voices Obscured in Complex Environmental Settings (VOiCES) corpus",
        "text": "Increasingly we are interacting with technology as we interact with other humans \u2014 through speech. Voice recognition interfaces are integrated in cars, cell phones, smart watches, and personal \u201cdo-it-all\u201d assistants like Siri, Alexa, and Google Assistant. Though voice recognition has come a long way, more often than not I get frustrated when attempting to command my devices. More than once I\u2019ve been the victim of misdialing, and I have gone as far as changing names on my contact list to ease the job of my virtual assistants; perhaps you\u2019ve been there too. The fact is natural speech is bound to happen in noisy places, often in a multi-speaker environment. Audio interfacing needs to better adapt to these circumstances. This is especially true as audio user interfaces become a larger part of our everyday lives.\n\nFor acoustic technology to improve, it needs better examples to learn and test upon. The VOiCES corpus, a collaboration between Lab41 and SRI International, provides speech data recorded in acoustically challenging environments. By making this corpus publicly available, we aim to promote acoustic research including:\n\nIn recent years various deep learning approaches have proven effective in both speaker separation and denoising of monaural (single audio microphone) recordings. Deep learning frameworks are able to better adapt and capture data complexity in comparison to more traditional signal processing approaches, such as matrix factorization and probabilistic modeling. This results in improved model performance for a broader breadth of audio and acoustic processing problems.\n\nHowever, deep learning algorithms require large amounts of training data. In deep learning we create a framework that is able to distill patterns or representations from the input data it is exposed to, in order to predict the correct answer, known during training. The framework essentially amounts to an ensemble of \u201cvoters\u201d that adjust their weights (or votes) based on input-output data pairs, until the solution is optimized. To test how well we do, the model is tested against input-output data pairs it did not see during training (validation data). Ideally the training data exposes the algorithms to a broad range of scenarios that closely resemble the use case of a particular task, i.e. you want the training and validation data to closely resemble reality. However, such data is often difficult to find.\n\nFor audio, \u201cdata close to reality\u201d means noisy data with real room acoustic profiles, where speakers are not recorded at close range. Unfortunately, there are no publicly available audio datasets that meet these requirements. Those that are available are often limited in scope and/or available for a hefty fee. While we assume that such datasets have been developed for exclusive use within large companies, publicly available data that encapsulates real life scenarios is scarce. Most publicly available audio corpora are composed of isolated speech recorded at close range microphony (Librispeech, VoxForge, TED-LIUM, VOICE). In absence of noisy speech, synthetic data composed of mixed audio from different sources is used. Typically, clean isolated speech is convolved with noise (UrbanSound8K, DEMAND), and software is used to include reverberation. This synthetic data is then used to train and validate models. Though this provides a great starting point to train models for audio applications, and is our current best approach, these models often under perform or break in the wild. To improve model performance with un-curated speech under natural conditions, we need better data: real data.\n\nLab41 in collaboration with SRI International is excited to announce the Voices Obscured in Complex Environmental Settings (VOiCES) corpus, presenting speech recordings in acoustically challenging environments. Features includes microphones at a distance, background noise, distractor noise, and reverberant room acoustics. The corpus will be publicly available under Creative Commons BY 4.0, making it accessible for commercial, academic, and government use. With this release we aim to provide test and development data that better represents real situations, promoting acoustic research.\n\nData for the VOiCES corpus was recorded in real rooms, having reverb and HVAC system background, where prerecorded foreground and noise was replayed and recorded using twelve microphones placed at strategic locations through the room. Two different rooms were used for recording sessions: room-1 with dimensions 146\u201d x 107\u201d (x 107\u201d height) and room-2 with dimensions 225\u201d x 158\u201d (x 109\u201d height). Both were windowed and carpeted rooms, with mostly bare walls and ceiling. Rooms were furnished with chairs and tables, with window shades pulled up for recordings. During each recording session both isolated speech and noise was played simultaneously. Three different types of noise were used: television, music, or overlapping speech from several speakers, referred to here as \u201cbabble\u201d. Each room had four recording sessions \u2014 one for each noise type played concurrently with isolated speech, and an isolated speech only session. Additionally, one hour of noise only (or ambient room noise) was recorded at the end of each session. This resulted in over 120 hours of recorded speech per microphone, for a total of 374,688 audio files.\n\nIsolated speech was played from a dedicated speaker, placed 43\" from the ground, that sat on a rotating platform. During recording sessions the robotic platform rotated its position by 10 degrees every hour, spanning 180 degrees. As humans, our conversations typically include head movement, and we wanted to include this modality in the data as well. Noise was played from one of three speakers, with the exception of babble, which used all three noise dedicated speakers. The figure below shows the recording configuration for (a) room-1 and (b) room-2. The speaker used for foreground speech, at its 90\n\ndegree position, is shown as the yellow rectangle. The blue squares show the location of the distractor noise speakers, and the green circles represent microphone locations. Twelve microphones were placed in strategic locations throughout the room: 4 cardioid dynamic studio microphones (SHURE SM58), 7 omnidirectional condenser lavalier microphones (AKG 417L), and 1 omnidirectional dynamic lavalier microphone (SHURE SM11). Paired studio and lavalier microphones were placed at four different positions: (1) Behind the foreground loudspeaker (microphones 7 and 8), (2) on a table directly in front of the foreground loudspeaker (microphones 1 and 2), (3) on a table in front of the foreground loudspeaker at a farther distance than (2) (microphones 3 and 4), and (4) across the room from the foreground loudspeaker (microphones 5 and 6). The remaining four lavalier microphones were placed in other locations in the room, either partially or fully obstructed by a physical barrier, to better represent real scenarios.\n\nAll audio sources were taken from public domain or sources with a creative commons agreement, allowing free distribution of data derivatives. Isolated speech was taken from the \u201cclean\u201d data subset of LibriSpeech. A total of 15 hours were selected from this corpus (amounting to 3,903 audio files), encompassing 300 speakers with a 50\u201350 female-male split. At least three minutes of speech was selected from each speaker, with at least one minute from three different book chapters - an amount sufficient for speaker identification tasks. Selected files were randomized and interleaved with 2 seconds of silence, resulting in a 19 hour audio file. Music noise was selected from the MUSAN corpus, and babble noise was constructed from the \u201cus-gov\u201d subset of MUSAN. The latter is composed of public U.S. government meetings. For babble noise, audio from three different meetings was blended for each track, resulting in a total of nine overlapping voices simultaneously getting played during recordings (all three noise dedicated speakers were used for babble noise). Television noise was selected from public domain television and movies, taking five minute excerpts from the middle of the files, with a total of 5\u20138 extracted excerpts per movie or show. All files were amplitude normalized. For noise tracks, data was concatenated into 20 hour audio files per noise type. A pressure meter, located near microphone 1 was used to adjust volumes levels for foreground (~65 dB) and distractor noise (~50 dB).\n\nThe corpus data includes the source audio file ID and orthographics transcriptions, the source files used during recordings (these are corrected for DC offset and amplitude normalized), the retransmitted recorded audio (with and without distractor noise), and mapping between speaker ID, book ID, and chapter ID. At the start of each recording session we recorded audio signals to evaluate the room response; these will also be available for use. All files are labeled such that the speaker ID, recording room ID, distractor noise played, and type and location of the microphone, are easily identified.\n\nWe\u2019re very excited about the release of the VOiCES corpus and trust that by releasing it under a creative commons (CC BY 4.0) agreement it will promote further research in the area of speech and acoustics. The corpus will be hosted on Amazon Web Services and will be available soon. Make sure to check out the VOiCES website for more details on data release dates, publications, and upcoming data challenges."
    },
    {
        "url": "https://gab41.lab41.org/using-machine-learning-to-classify-devices-on-your-network-538264c8b820?source=---------1",
        "title": "Using machine learning to classify devices on your network",
        "text": "In this article, we plan to walk readers through using our machine learning code to classify devices on a network. We have touched on this in previous blog posts about the Poseidon Software Defined Networking (SDN) project and how it relates to detecting lateral movement, as well as using machine learning (ML) to analyze network data. With that in mind, we\u2019ve experimented with classifying devices using packet-capture data. We\u2019ve made a few tools available to make it easier to try on your own network as well. The models that we\u2019ll be using run in combination with the Poseidon SDN project, and if you\u2019d like to try that yourself, you can read about how to build your own Software-Defined Network with Raspberry Pis and a Zodiac FX switch or watch the video.\n\nFor running the machine learning code, you\u2019ll need to have Docker installed, and some way of capturing traffic on the devices that you want to classify. For network data capture , consider using our version of tcpdump that we\u2019ve modified to include flags that strip layer-4 payload information as well as information to external hosts. Once you\u2019ve performed the prerequisites , you can run\n\nto get the machine learning container and we\u2019re ready to start. There are four things that this post will cover:\n\nThe easiest thing to do is to run one of the models that we\u2019ve trained on your own data. To do that, first create a packet capture (our machine learning software PoseidonML expects a pcap) from the device of interest if you don\u2019t have one already. The capture should be at least 15 minutes long, and contain a large amount of internal-internal traffic, but longer captures may produce more reliable results.\n\nNow you can choose between two models to use for classification. The default choice in PoseidonML is a Random Forest model. This model\u2019s performance is encouraging out of the box when trained on new data (more on that in the next section!), so it is a good place to start. To run this model on your pcap:\n\nThe other model is a one layer neural network, which you can try by running:\n\nIn either case, you should get an output with a message that looks something like this:\n\nThis is the message that the ML tools send back to Poseidon. The \u201cclassification\u201d field indicates the labels that the classification model has assigned to the device that the input pcap was sourced from, and the confidence field gives the associated confidences of those labels. In addition, there is a \u201cbehavior\u201d field, which typically should be \u2018normal\u2019 indicating the model did not detect abnormal behaviors, and an \u201cinvestigate\u201d field, which is true if the ML tools are requesting that Poseidon gather more data from this device. The \u201cvalid\u201d field simply indicates if the request to analyze this device was made by Poseidon with the associated metadata, so it should be false in this case. The top 3 device types are returned, which may include an \u2018Unknown\u2019 label.\n\nOne of the central concerns we had in this project is whether or not data from your network looks anything like the data that we trained our models on. If it doesn\u2019t, the machine learning tools won\u2019t work very well. In light of that, we decided that we should try to make it easy to retrain the machine learning models on new data. The hardest part of this is collecting a dataset that will work for this. However, here are a few rules of thumb that we found to work well for our specific problem:\n\n1. Have multiple devices for each label you want to assign.\n\nMultiple devices help the model to learn more general information. If you have only one device, models may learn the specifics of that one device, which means they might not work on new data.\n\n2. Capture traffic in uninterrupted segments of at least 15 minutes long.\n\nOur models work by looking at session level information, so this time window is intended to contain many packets from several sessions.\n\n3. Total length of captures from each device should be at least 5 hours.\n\nThis is intended to sort of \u2018average out\u2019 specific uses of a device, and give a better idea of how it behaves as a whole, and not at one instant in time.\n\nThis is done to evaluate how well the model you train is working. It\u2019s necessary to test on separate data than you trained on, or you will not know if your model has learned anything general.\n\nTo keep things simple, we adopted a naming convention for the capture files of the form\n\nFor example, a one hour capture from an my workstation on Tuesday at 1520 UTC might have a name like this:\n\nAfter doing the collection, you should wind up with a directory of captures. Here are a few examples of what we used in training:\n\nNow, we\u2019ll need to assign the labels that we want to be associated with these captures. To do this, create a JSON file called \u2018label_assignments\u2019 in the directory with your data. This will associate a device name with a label. For the examples above, our label_assignments.json might look like this:\n\nIn many cases, the label is similar to the device name, but this doesn\u2019t have to be the case. Any captures that aren\u2019t assigned a label will be automatically given the \u2018Unknown\u2019 label. This is the basic format that you can use for making both a set of training data and a set of testing data.\n\nNow that you have the training dataset configured, training a model should be pretty straightforward. We\u2019ve automated most of the process, so you should be able to run\n\nThis will train a random forest model, but you could also use lab41/poseidonml:onelayer for the neural network model. We found the random forest model works well on most datasets, but the neural network model can sometimes work better on larger datasets. This step should handle the preprocessing of the dataset, including the selection of features, training of the model, and cross validation. After this runs, you should see an overall F1-score (closer to 1 is better) on a validation set (the training script automatically created one from your training data) and you should have a trained model in the directory you specified. That should be all you have to do to train a model!\n\nAfter training a model, you probably will want to test how it will perform on new data. Fortunately that\u2019s not too different from what we\u2019ve done so far. You\u2019ll need to create a test dataset the same way as described above. This should not include any of the data that was in the training dataset, though. After that, you simply run\n\nAgain, you could also use lab41/poseidonml:onelayer for the other model. After processing the dataset, this should give a result that looks something like the following:\n\nThe table above shows the performance of the model (measured by F1-score) on each label that we defined, and the performance averaged over all labels (mean F1-score). Additionally, you get some statistics on the run time and amount of data processed at the end. This should be all you have to do to train a network device classification model on your own data.\n\nThere\u2019s still a lot left to do, in particular assembling a large shareable network dataset for collaborative research. Hopefully, our tools make it easier to get started running your own experiments. We are really interested in finding out more about how models trained on data from one network work on another, so if you conduct any experiments on that, we would love to hear about them!"
    },
    {
        "url": "https://gab41.lab41.org/pyseal-homomorphic-encryption-in-a-user-friendly-python-package-e27547a0b62f?source=---------2",
        "title": "PySEAL: Homomorphic encryption in a user-friendly Python package",
        "text": "We can all agree that the recent growth of machine learning applications has been driven by innovation in algorithm design, low cost storage for large training datasets, and powerful GPU-driven compute. However, many useful training datasets can never be shared. Consider a biomedical application where a large cohort of patient genomic data needs to be compared to identify previously unknown genetic markers of disease. Of course, we need to safeguard patient data privacy and security and, therefore, cannot openly share their genomic data within and between healthcare and/or research organizations. To date, there have been many successes for identifying genetic markers of some inherited diseases; mostly diseases that are a result of a single gene and can be successfully identified with limited amounts of data. However, more complicated diseases will require advanced analytic approaches, such as machine learning, and likely a substantially greater amount of data. Gaining access to large sets of patient genomic data for a particular disease is challenging due to the necessary legal agreements that need to be in place to obtain that data. Is there another way to address this challenge?\n\nOne solution to this problem is to use fully homomorphic encryption. Fully homomorphic encryption refers to an encryption scheme in which performing computations and analyses (even complex, non-linear ones) on encrypted data. What this means is that if we homomorphically encrypt the DNA sequences of patients, we can then query homomorphically encrypted databases for genetic comparisons. We can then decrypt the final result and get the same answer as we would have gotten using unencrypted DNA sequences.\n\nThis approach is especially attractive because it allows a healthcare organization or research group to compare DNA sequences without ever exposing patient\u2019s DNA sequences to unauthorized parties. It also allows genomic database holders to ensure the privacy of their DNA and potentially protect their intellectual property if the dataset is owned by a commercial company. It\u2019s a neat trick, and one that has gained more attention in recent years.\n\nUnfortunately, most homomorphic encryption schemes are considerably slower than regular encryption methods, the main difficulty in using these schemes. A key area of ongoing research in cryptography focuses on bridging this speed gap, and recent developments pioneered by Microsoft Research\u2019s (MSR) cryptography group have made significant improvements. In December 2017, MSR released version 2.3 of its Simple Encrypted Arithmetic Library (SEAL), a fast C++ implementation of the homomorphic encryption system described by Fan and Vercauteren in their paper \u201cSomewhat Practical Fully Homomorphic Encryption\u201d. The encryption system proceeds in two separate stages: First, numerical data are converted into polynomials and embedded in a specified polynomial ring. Then, the ciphertexts (encryptions) of the polynomials are computed by applying noisy linear transformations involving the public key to each polynomial. Arithmatic operations are then performed on the encrypted data, and as long as the accumulated noise in our computations does not exceed a threshold amount, we can decrypt the computational output with a linear transformation involving the private key and obtain the correct result (see sections 4 and 5 of Fan and Vercauteren\u2019s paper for details). The MSR implementation has a number of nice features in addition to the basic encryption apparatus, such as recommendation methods that provide optimal parameters for the initial encryption setup, and a noise budget that reflects the noise incurred when performing a given computational procedure.\n\nLab41 and B.Next have been engaged in a collaborative effort to tackle the DNA sequence and database encryption problem described in the introduction, which is a first step for a host of biotechnology applications that require secure information sharing and comparative analyses of genomic data. Our recent work on fully homomorphic approaches includes a publicly available port of MSR\u2019s C++ SEAL library to Python, which can easily be imported and used in Python REPL\u2019s and projects. The library, called PySEAL, features the capability to call key classes and methods in Python from MSR\u2019s C++ implementation, common use cases of homomorphic encryption as illustrated in the original SEAL library, and a Docker file that takes care of setting up the right environment and building the required executables.\n\nWe hope that PySEAL will be a useful platform for data scientists and others who are more accustomed to using high-level programming languages for analysis and experimentation. Given the prevalence of Python in the data science community as well as in the general science and engineering community, we felt that it would be valuable to open source a convenient homomorphic encryption package, readily usable by scientists and engineers of all backgrounds. The readme and the examples included in the library explain the specifics of how the main classes fit together and show how to use these classes to implement various encryption schemes. These examples can be used as starting points by researchers who are implementing homomorphic encryption schemes, or who are just interested in exploring new possibilities for preserving the privacy of their data. Homomorphic encryption as a field is still in its early stages, and we look forward with great anticipation to the technologies and advances that the general scientific community will develop in this sphere in the coming years."
    },
    {
        "url": "https://gab41.lab41.org/source-contrastive-estimation-it-removes-noise-too-dc8626835e8e?source=---------3",
        "title": "Source Contrastive Estimation: It Removes Noise, Too!",
        "text": "Imagine you\u2019re recording a weekly company meeting, which you plan to transcribe later. You place your smartphone at the center of your meeting table, hit record, and start deliberating your team\u2019s weekly goals. While transcribing the audio the following day, you\u2019re shocked at how difficult it is to understand at times. What was a lively debate during the meeting turns into a garbled mess because cars are honking in the background. Fluorescent light fixtures and HVAC in the meeting room also contribute to an irritating hum throughout the recording. Halfway through the meeting your colleague let some fresh air in by opening a window. This had the unintended consequence of also letting in loud, distracting barking-dog noise from the kennel across the street.\n\nThe issues touched on in the above scenario are among the central concerns of monaural (single-track) audio processing. The human auditory system is capable of localizing and isolating different audio sources and focusing only on those it determines to be most interesting. We do this subconsciously using our complex array of auditory senses coupled with sophisticated cognitive processing which includes understanding the context in which the sounds occur. A single microphone on a smartphone (in reality, phones typically use arrays, though their offsets are small) lacks this sophisticated machinery humans have of filtering out distractions. Our previous blog post on source contrastive estimation (SCE) for blind source separation shows how deep neural networks can be leveraged to isolate different speakers in monaural audio recordings. This post outlines Lab41\u2019s work on adapting this technique to remove noise sources from monaural recordings as well.\n\nGoing back to the weekly meeting scenario, there are two types of noise present: stationary and non-stationary (or, dynamic). The stationary noise comes from the constant hum originating from the light fixtures. This type of noise is easily detected and removed with traditional signal processing techniques such as Wiener or Kalman filters. These techniques rely on the noise having some statistical regularity and are a standard part of many commercial audio pipelines. Dynamic noises are much more difficult to remove. The noises from the barking dogs are a good example of non-stationary noise. The following figure illustrates the lack of statistical regularity in both the audio waveform and spectrogram of a barking dog versus noise from an HVAC system (white noise).\n\nWhile, dynamic noise reduction is popular in consumer devices (noise cancelling headphones, etc.), they all rely on having a fixed microphone array specially calibrated for certain tasks (e.g., smartphones listen for human voice commands, etc.). This is not the case in many scenarios, including those with distant speakers (recording devices are poorly calibrated for this), an unknown number of distracting noises, rooms having an unknown impulse response, and so on. Removing the stationary noise is a relatively straightforward task. Identifying and extracting the dynamic noise is more difficult and requires modeling and automatically isolating the specific sources of each noise type.\n\nOver the last decade, machine-learning approaches have started to see success in removing dynamic noise. In particular, adaptation of matrix factorization techniques to the processing of spectral representations of audio signals has proven useful. These methods can be difficult to make performant, and in many cases additional complexity is required to model source characteristics accurately. While we explored one such method (sparse non-negative matrix factorization (SNMF)), this algorithm shows suboptimal performance when compared to nonlinear approaches (i.e. neural networks).\n\nRemoving dynamic noise requires a significant amount of modeling to disambiguate both signals (human speakers) and noises (i.e. barking dogs), each of which has very complex temporal patterns. Wouldn\u2019t it be nice if the temporal patterns for each source could be learned rather than resorting to linear approximations or coming up with ad-hoc, handcrafted models? Indeed, deep neural networks are well suited for such tasks as demonstrated in our work using SCE to separate multiple speakers in monaural audio. We leveraged this work to model a set of noise sources along with human voices in order to separate them. However, we made a few improvements that further optimize the source-separating power in the context of denoising.\n\nRecall that SCE learns a dense \u201cembedding vector\u201d representation for each of the time-frequency (t-f) values in an FFT spectrogram. Each t-f value is assigned to a known set of sources through an all-or-nothing masking procedure (every t-f value is assigned to one, and only one, source). During training, vectors that belong to different sources are pulled apart while vectors from the same source are pushed together. To then isolate sources, a vector is computed for every t-f value and clustering is performed using K-means to group t-f values. Each group of t-f values is then back-transformed through an inverse FFT (iFFT) operation into a resulting waveform. This is an optimal procedure when the following criteria are met:\n\n1. An arbitrary number of sources are to be separated\n\n2. The number of sources is constant and known\n\n3. The clusters for all sources in the embedding space are isotropic and well-separated (i.e. conducive to K-means clustering)\n\nOur solution to denoising draws inspiration from recent work on music-singer separation. Here, too, there exist just two broad categories of sources to separate. Instead of clustering the embedding vectors, a nonlinear transformation is learned that takes each vector and learns so-called \u201cratio masks\u201d that proportionally split each t-f value between both \u201csinger\u201d and \u201cmusic\u201d categories. The process of estimating these masks is called mask inference (MI). The spectral magnitude is then multiplied by each ratio mask and run through an iFFT operation to yield the waveforms for each source. During training, a loss term that corresponds to the reconstruction error of the ratio-masked spectrogram is added to a deep clustering (DC) loss. In our implementation we used the SCE loss rather than the DC loss. This two-source architecture is diagramed below.\n\nHere we see that after the spectral magnitude is fed through four bi-directional LSTM (BLSTM) layers and a final linear reshaping layer (Z), the output is funneled to both a clustering objective and ratio mask objective specifically designed to separate only signal (human speech) and noise (e.g.,barking dogs).\n\nWe trained ours and several other network architectures and compared denoising performance using a combination of the LibriSpeech spoken word and the UrbanSound8K noise datasets. The noises involved ranged from periodic (HVAC, jackhammer) to highly non-stationary (dogs barking, street music). As a deep learning baseline, we tested against a denoising autoencoder (DAE) whose objective was to reconstruct a clean spectrogram given a noised version. We also tested mask inference coupled with deep clustering (MI+DC) and mask inference coupled with SCE (MI+SCE). These two networks can perform both clustering \u00a9 and mask inference (MI). Thus, we tested both methods of separating signal from noise.\n\nThe following is the performance of these algorithms in terms of source-distortion ratio (SDR) improvement versus input SNR.\n\nThe performance of the mask inference head of SCE+MI is on par with the DC+MI (+13 dB at an input SNR of [-5,-4] dB) while the clustering performance of SCE (+11.5 dB) is slightly better than the clustering of the SCE+MI and DC+MI algorithms. Therefore, SCE may be more desirable when the number of sources to be separated is arbitrary. The performance of the deep learning-based methods is relatively consistent across input SNRs while SNMF sees more dramatic differences. Also, the improvements in SDR are greatest for inputs with lower in SNR. This can be explained by the fact that at higher input SNRs the signal is already quite prominent, so there is less room for improvement. And now, the performance versus noise type:\n\nIn general, the improvements in SDR are greatest for more statistically stationary noise sources and the deep learning methods perform quite well for both stationary and non-stationary noise sources.\n\nWith such promising results, there are several avenues to explore and improvements to be made. One such exploration is to better align the objectives of the K-means clustering and vector space learning (i.e. criterion 3 in the above discussion). Another method for potential improvement is to use waveforms as both input and output to a network that learns a sparse representation and is able to separate arbitrary numbers of sources."
    },
    {
        "url": "https://gab41.lab41.org/sampling-hotness-in-tensorflow-926bf8136a8d?source=---------4",
        "title": "Sampling Hotness in TensorFlow \u2013",
        "text": "The current libraries are well documented with cost functions on top of sampling distributions. For example, noise contrastive estimation is available:\n\nThis wasn\u2019t always the case in Tensorflow, and it surprises us how recently they\u2019ve been added. For example, was added to the codebase in a relatively recent git pull. However, the documentation (the original request) was uploaded in Dec 2015. This is not isolated in TensorFlow, considering word2vec and most of the other label sampling methods have lacked even GPU support. In fact, look at the example on line 148 in TF 1.1; it still reads:\n\nThe dearth of these kind of implementations was especially problematic for large network layers where you need to sample layers because the size of the weight matrices is proportional to the amount of unique labels. For example in the YFCC corpus, it\u2019s 400k words after pruning down from six million words. Considering affordable GPU memory has only recently topped 16GB, it\u2019s easy to see how a matrix with a single dimension of 400k could be prohibitive.\n\nWe received six Titan X cards, mid-year 2017. With a huge GPU hammer, we looked around for the biggest nail. So late last year, we downloaded the biggest dataset we could get our hands on (100 million images) with the most amount of metadata (14GB), and threw the largest neural network we could at it. The trouble was, while we could find ways to fully use the 16GB of memory, the actual matrix-vector operation still took way too long. Enter candidate sampling, which we describe next.\n\nAgain, sampling functions in Tensorflow are fairly new. The community has been rushing to provide new contributions to the official repository. Specifically for candidate sampling, we\u2019ve gained many capabilities from (this one\u2019s an oldie), , , and all of their variants. All of these functions are conveniently summarized in the table below:\n\nLogs and odds, oh my! The highlight here is that the positive and negative samples (second and third columns) relate to the match or mismatch between the label and the data. The remainder of the table just tells you how to use those samples.\n\nThis is great if what you\u2019re trying to do falls into any one of these categories. But, you might want to do something different, and sometimes cost functions don\u2019t really fit into any of the above forms. Additionally, you may want to sample from a specific distribution, not available in TensorFlow, which only includes the Zipfian distribution, the boring and ever-present uniform distribution, and an empirical distribution (think histogram) based on a record of your past true samples. As mentioned above, TensorFlow is rapidly evolving, but that is hardly a complete set of distributions\n\nSo\u2026what if you want to do all the sampling and cost function creation yourself? What if you had no choice but to do the sampling manually because you needed to manipulate the vectors (embeddings) in some special way that isn\u2019t present in the above table\u2026and you wanted to do it on the GPU? In other words, we\u2019d like to know how to sample off-GPU and then do math operations on-GPU.\n\nWe found the maximum amount of flexibility in the function , and implementing dot products ourselves. Much to our initial consternation, it wasn\u2019t implemented in TF versions before 1.1.\n\nThis comment is April 6, 2017, but the implementation finished a few months before that. It\u2019s a simple fix: you will need to get at least version 1.1, which you can do directly from the TensorFlow Github page. (They do nightly builds.)\n\nThe documentation for is chock-full of examples, but the bottom line of code sums it all up:\n\nFrom a set of vectors (or matrices or tensors, whatever), you can extract samples with complete freedom. And you can do this with all sorts of shapes! Let\u2019s say you have an embedding of 3x2 (i.e. three examples, each with two dimensions), you could train it with the code below:\n\nWhat\u2019s really powerful is that you can train a number of vectors by building a batch and you can repeat vectors any number of times.\n\nSo that\u2019s great, right? Almost. Let\u2019s be clear: neural networks are all about tensor dot products, and navigating dimensions is especially hard. A typical situation occurs when you have a large number of samples (maybe from ) for a single input, and now you want to find the correlation to a feature vector a la word2vec. That is, you want to take a single vector and repeatedly dot it with a whole bunch of other vectors.\n\nTurns out broadcasting works especially well, and the TensorFlow version of broadcasting works exactly the same as the numpy version. The element-wise operator will replicate as needed, and you need only sum the relevant indices. For example:\n\nYou ask, what about multi-dimensional tensors? Neural networks are optimized by batches, right? As it turns out, when your dimension is of size \u201c1\u201d, TensorFlow will expand to the appropriate size. (Think and for all you MATLAB users.) For example -\n\nLet\u2019s put it all together. Let\u2019s say we have a batch of features. For each of the features we want to correlate with three vectors we\u2019ve sampled from a set of vectors. (In reality, we\u2019d sample three vectors 100 times for our SCE algorithm, but with the below code, the extension\u2019s left out for argument\u2019s sake.) This is how you would create that graph:\n\nThe above code is all you need to write to implement your own sampled cost functions! To review, we\u2019ve linked you to candidate sampling. We\u2019ve talked about a few of their cost functions. Then, for those of you who enjoy sampling and implementing new cost functions, we explained TensorFlow\u2019s function to index into your embedding arrays. Then we talked about tricks in broadcasting, namely using \u201c1\u201d in places where you\u2019d like to repeat operations, effectively creating copies of an array for operation.\n\nWe hope you enjoyed our TensorFlow tutorial on sampling. For more information, check out our Lab41 Github pages at: https://github.com/Lab41. Thanks for tuning in!"
    },
    {
        "url": "https://gab41.lab41.org/matching-cars-with-siamese-networks-83646122ad9c?source=---------5",
        "title": "Matching Cars with Siamese Networks \u2013",
        "text": "Lab41 just finished Pelops, a vehicle re-identification project using data from fixed video cameras. Last time I talked about \u201cchipping\u201d, that is extracting an image of a vehicle from a frame of video automatically. We found that background subtraction worked OK based on the small amount of labeled data we had.\n\nIn this post I\u2019ll go over the rest of the pipeline: feature extraction and vehicle matching.\n\nMachine learning algorithms operate on a vector of numbers. An image can be thought of as a vector of numbers \u2014 three numbers to define the color of each pixel \u2014 but it turns out that taking these numbers and transforming them gives a more useful representation. This step of taking an image and creating a vector of useful numbers is called feature extraction. We performed feature extraction using several different algorithms.\n\nOur first method of feature extraction was an old one: Histogram of Oriented Gradients or HOG. HOG was first proposed in the 80s, but has since found uses in identifying pedestrians and was used by our sister lab CosmiQ Works to identify boat headings. HOG effectively counts the number and direction of edges it finds in an image, and as such is useful for finding specific objects. HOG lacks color information, so in addition to the output from HOG, we appended a histogram of each color channel.\n\nOur second method of feature extraction was based on deep learning. We took ResNet50 trained on ImageNet, removed the fully connected layers, and treated the 2048-dimension output of the convolutional layers as our feature vector. It is well known that networks trained on ImageNet, despite being exceptionally good at identifying dogs and cats, are also very good for general image problems. It turns out the edges, shapes, and colors learned for dogs are also, in different configurations, useful for cars. For more details on the ResNet architecture, see my reading group blog post.\n\nOur third method of feature extraction was a fine-tuned ResNet50. Pretrained networks are good at general image tasks, but they can be \u201cfine-tuned\u201d to perform better on specific tasks. For Pelops that specific task was make, model, and color identification of cars in a labeled dataset. It is hoped that making the model better at make, model, and color detection will generate features that are more useful for matching cars. This makes intuitive sense: any human matching cars would use make, model, and color as primary features.\n\nOnce all of the vehicles have feature vectors associated with them, those vectors can be used to match vehicles to each other. There are a few ways to do this, starting with the simplest, which is to calculate a distance between the vectors. This works great if the feature extractors are designed to make the distance meaningful, but this is not generally the case. Neural networks can have cost functions based on distance, but ResNet50 does not. So this method, while attractive in simplicity, is not a good solution.\n\nThe second possible way of matching is to train a traditional (that is, non-deep learning based) classifier. We trained a logistic regression model, a random forest, and a support vector machine on top of each of the types of feature vectors. Each model was given two feature vectors and asked to classify them as coming from the same vehicle, or not. The training data was balanced so that there were as many positive as negative examples. The best accuracy these models achieved was 80%, although most struggled to pass 70%. Accuracy is the number of true results divided by the number of total items tested.\n\nThe third method of matching was to use a neural network as a classifier. Once we added a deep learning classifier on top of our deep learning feature extractor, we had a Siamese neural network. For details about how we trained such a network, and for an overview of its architecture, see our blog post here. The Siamese network performs the feature extraction and matching in one step, and so allows optimizing both portions at the same time. This arrangement achieved the best results by far, hitting nearly 93% accuracy on our test set.\n\nIn order to determine how well our various feature extraction and matching algorithms did, we needed a labeled dataset. We used the VeRi dataset, which contains pictures of 776 uniquely identified vehicles. There are multiple pictures of each vehicle taken from 20 different traffic cameras in China. An example of two VeRi images is show below."
    },
    {
        "url": "https://gab41.lab41.org/object-localization-without-deep-learning-1cf234b4780a?source=---------6",
        "title": "Object Localization without Deep Learning \u2013",
        "text": "Lab41 is just wrapping up a vehicle re-identification project named Pelops. The goal was to be able to identify if the same car drove past a fixed set of video cameras without reading the license plate.\n\nWe broke the project down into three parts:\n\nToday I\u2019ll show you our approach to chipping; feature extraction and matching will follow in a later post.\n\nThere has been a lot of work done on using deep learning for object localization in the past few years. Deep learning based methods currently achieves state of the art results for many localization problems. In fact, our sister lab, CosmiQ Works, explored using these techniques, and even developed a modified version of YOLO called You Only Look Twice (YOLT) to do ship and plane localization in satellite images.\n\nDeep learning seems like the best option for finding cars as well! But we don\u2019t use deep learning, for two reasons:\n\nWe were able to hand label about 200 frames of the traffic camera data in order to test our algorithms, but did not have enough time (or, critically, patience) to label enough vehicles to train or fine-tune a deep learning model. Therefore, we chose an old standard in computer vision: background subtraction.\n\nWhat background subtraction tries to do, at its simplest, is to classify every pixel in an image as either background or foreground. Background pixels are ignored while foreground pixels are taken to be part of an object of interest. To do this classification the algorithms develop background models. We used two different algorithms with the only difference being the method in which they model the background pixels.\n\nThe first method used a very simple background model and was mainly intended as a benchmark. For each pixel it takes the median luminosity of the last ten frames. This gives a crude estimation of what objects are \u201cpermanent\u201d and which are transient.\n\nThe second method was based on a paper by Zivkovic and van der Heijden. It uses a Gaussian mixture model to estimate the distribution of background pixels in RGB space. It learns the distributions based on the previous frames and updates as new frames are processed. We used the implementation in OpenCV called MOG2, cv::BackgroundSubtractorMOG2, which also includes a model for shadow rejection. Shadows are otherwise a tough problem to solve because shadows are real changes in the image, but are generally not ones that are interesting.\n\nAfter the background models are computed, the two algorithms perform the same steps. They subtract their background model from the image leaving black pixels where there was no change and pixels with some luminosity in areas where there was a moving object. We then run a Gaussian blur over the image in order to join incorrectly separated regions (e.g., where the model splits an object in half). After the blur is applied, all pixels below a threshold value are set to black and bounding boxes are drawn around the remaining pixels. For both of our models, the size of the Gaussian kernel and the threshold value are free parameters which we optimize over. The best values for these parameters are discussed in the results section.\n\nThe data we used to test our chipping algorithms was from two sets of traffic cameras. The first set was from San Antonio, Texas, and the second set was from New Orleans, Louisiana. These two datasets cover a wide range of vehicle sizes in terms of number of pixels. The Texas data is low resolution and the cameras are mounted high above the highway giving very small chips, with the largest being roughly 50 pixels on a side. The Louisiana data is taken with higher resolution cameras that are mounted closer to the roadway and so the chips are much larger. Some of the chips are up to 300 pixels on a side. An example of the Texas data and Louisiana data follow:"
    },
    {
        "url": "https://gab41.lab41.org/introducing-source-contrastive-estimation-d63ab9844e3?source=---------7",
        "title": "Introducing source-contrastive estimation \u2013",
        "text": "Like most deep learning-based approaches to source separation, our technique is a form of mask estimation. The spectrogram of a signal contains information about its relative power at different times and frequencies. When audio from two sources appears in a spectrogram, many of its components will be more strongly associated with one source than another, as Figure 2 illustrates below.\n\nIn the past few years, deep learning has begun to be applied to audio source separation. Such work includes Huang et al.\u2019s RNN-based approach , innovative cost functions from Microsoft Research and Aalborg University, and a vector-embedding technique adopted by deep clustering and improved on in deep attractor networks . These techniques have typically added a large improvement over linear matrix decomposition techniques, like non-negative matrix factorization. We plan to put up a blog post in the near future describing some of those approaches. Today, however, let\u2019s dive into Lab41\u2019s unique work in this space, an approach we call source-contrastive estimation (SCE).\n\nThe field of source separation seeks to make signals like this usable by separating speakers onto separate tracks, recovering the original, much more intelligible, components of the mixture. Notice how the individual components of the mixture in Figure 1, on the right, have more orderly harmonics (the really narrow horizontal bands stacked on top of each other) and clearer structure in the vocal tract resonances (the thicker dark bands), compared to the mixture on the left:\n\nListen to an audio recording made in the same environment, however, and you will realize why speech and engineering researchers have been stumped by the \u201c cocktail party problem \u201d ever since the early days of signal processing. If you don\u2019t have your two ears (and whole body, which also conducts sound) in the scene, the reality of a noisy environment like a bar or loud party is one of an undifferentiated din of overlapping speech, reverberation, and interference of all sorts. Recordings made in such environments are often unusable in their original state:\n\nHave you ever been talking with someone in a room crowded with dozens of people talking at full volume and been amazed that you can still hear and understand your conversation partner? I sure haven\u2019t \u2014 it\u2019s part and parcel of being a human with at least average hearing ability. We are remarkably effective at navigating noisy environments, and understanding what we hear in them. For the most part, however, it doesn\u2019t even seem remarkable to us.\n\nWe can recover most of the signal from a source by just masking out the portions of the spectrogram that don\u2019t belong to it. This can be done with a matrix, the same shape as the spectrogram itself, whose values are all between 0 and 1. Element-wise multiplication with the input spectrogram does what we need. Values of zero stop a component from coming through; values of one allow it to pass through unadulterated. Figure 3 below illustrates this process.\n\nHow do we get such a mask, you ask? One part of the answer is the network architecture and postprocessing that yields the given set of masks. The other part is the objective function that tells the model how to learn which masks work and which don\u2019t.\n\nIf We have two bidirectional LSTM layers at the start, with the recurrence going over the time dimension of the spectrogram. Then a feedforward layer generates an embedding \u2014 a matrix with a 40-component vector assigned to each time-frequency bin. The code implementing this in TensorFlow is available on GitHub \u2014 here\u2019s the juiciest bit, excerpted from a larger class definition:\n\nThe insight from deep clustering and deep attractor networks is that representing time-frequency bins in a dense vector space lets you nudge them around, so that components belonging to different speakers are easily separated. When you deploy the model on a mixture, it transforms the spectrogram into a space where the difference between speakers is pretty clear-cut, as Figure 5 shows.\n\nThe vectors in this new space can be clustered using any old algorithm. We used K-means clustering. Cluster memberships can then be used to create masks by assigning 1s to all the time-frequency components in the cluster and -1 to all the other bins.\n\nGetting the vectors in the embedding space to separate, however, is the tricky part, bringing us to the cost function at the heart of SCE. In deep attractor networks, the goal is to get each embedding close to a centroid of the speaker\u2019s overall volume in the embedding space. In deep clustering, the story is less geometrically neat but the embeddings are optimized to approximate a bin-to-bin similarity matrix. We thought we could do even better.\n\nThe \u201csource-contrastive\u201d part of \u201csource-contrastive estimation\u201d makes a nod to noise-contrastive estimation, the engine behind (some instantiations of) word2vec. The intuition is that an embedding can be optimized by simultaneously pushing it toward a target vector representing the speaker it belongs to and by pushing it away from other vectors, representing the other speakers in the mixture. This effects a contrast between bins that should be different and draws bins together when they share an identity. And although speaker identities are used during training, they are not needed during inference \u2014 neat!\n\nHere\u2019s another question: just where do we put the target vectors for the various speakers? We can actually just learn that along the way, too. As bin embeddings get pushed around and away from each other, so too will the speaker embeddings. As a side benefit, you should end up with similar-sounding speakers closer to each other in the embedding space and different-sounding ones further apart.\n\nOur experiments, documented in our arXiv manuscript, indicate that SCE outperforms deep clustering in a number of settings, including mixtures of two women and of one man and one woman. This includes an improvement in signal-to-distortion ratio (SDR) of 1 or more dB when using SCE, compared to other techniques, as shown in Figure 6.\n\nAnother thing we think is really cool about our approach is that it cuts down on training time. Doing updates to a deep clustering model requires some hefty matrix computations (even after the impressive magic tricks its inventors pulled off to make their updates faster). Like its cousin noise-contrastive estimation, SCE presents a relatively lightweight means of estimating an embedding that separates speakers from each other. In our experiments, average wall clock time per minibatch in SCE was almost half that of deep clustering!\n\nThere is a lot more to do \u2014 right now, we are concerned that the margin between speakers in the embedding space is not wide enough, making it hard to separate ambiguous components. We also suspect that the information contained in our model could be put to work for other purposes \u2014 such as speaker identification or diarization. We are also hard at work on ways to make the separation performance of SCE robust to noise, so that it can actually work in real-world scenarios. In any case, we are very encouraged by our results to date, even if it is a long way from solving the cocktail party problem in general. If you are intrigued, please read our manuscript and check out our GitHub repo!"
    },
    {
        "url": "https://gab41.lab41.org/tl-dr-turn-static-portions-of-your-network-into-a-lookup-table-numerically-rank-your-architectures-46cc7750cad6?source=---------8",
        "title": "Iterating through Siamese network designs \u2013",
        "text": "I recently finished evaluating several Siamese network architectures, and wanted to share a method I found useful in reducing model training time. The task was to determine similarity between portions of an image extracted from video. Simplistic comparison methods failed and I needed to move to evaluating several large networks. Time became an issue.\n\nA Siamese neural network is a neural network that contains two or more identical subnetworks. This means the subnetworks start with the same initial conditions (configuration, parameters, and weights), and training maintains weight symmetry.\n\nSiamese networks are often useful in creating a similarity measure between the inputs. Siamese networks are used to determine if images, text or audio contain similar content.\n\nMy current problem requires a good similarity measure between images for later processing.\n\nI started by removing the classification layer of a Resnet50 network. I then ran images through the network and examined the cosine distance between the processed vectors. I then tried using a Support Vector Machine (SVM) to compare the image vectors. Neither method was performing well for my image dataset.\n\nI moved to Siamese networks for performing the comparison. I had many ideas for topologies and wanted to try them all.\n\nMy first Siamese network was built from Resnet50 with the classification layer removed. I added a vector subtraction layer to combine the two Resnet50 outputs, a fully connected layer, and finally a classification layer.\n\nThe team created a function to make experiments. The function returns two images from a set of labeled images and similarity truth between the images. The images and the expected similarity are then used to train the network.\n\nI generated many experiments and fed the experiments in batches through the above model in two phases:\n\nThe problem with this method was iteration time. Getting a model trained to the accuracy I wanted would take about 2 days. I had too many Siamese topologies to explore and little patience waiting for results. I needed to rank my architectures and only refine models providing the best results.\n\nIn terms of improving model training iteration time, a few thoughts come to mind:\n\nUsing these four thoughts I could improve my model training iteration time.\n\nMost of my processing time was spent evaluating Resnet50. To get around paying the Resnet50 tax (twice) I decided to pre-process all my images through Resnet50 and create an image-to-features lookup table.\n\nThen I used the lookup table to feed the top of the Siamese network; instead of calculating the features.\n\nI amortized my Resnet50 evaluation costs over all the potential models I wanted to explore. Pre-processing my image set took half a day. The payoff came when I worked through 10 different architectures for the top of the Siamese network in a day.\n\nIt still took about a day to do the final refinement (performing end-to-end training) as lookups could not be used there."
    },
    {
        "url": "https://gab41.lab41.org/deep-session-learning-for-cyber-security-5ea1cdd1fee3?source=---------9",
        "title": "Deep Session Learning for Cyber Security \u2013",
        "text": "Recently, Lab41 teamed up with Cyber Reboot (a sister lab) to explore the intersection of deep learning (DL) and cyber security in a software defined network (SDN) environment. We called it Poseidon, based heavily on it being a cool word with the letters s, d, and n in order.\n\nThe goal was to use predictions about network traffic to automatically update a network\u2019s posture. This entailed three main objectives: performing deep learning on packet data, setting up an SDN environment, and scheduling a microservice to connect the two (for more information and code visit our Github page). Since I belong to the cult of deep learning, I was tasked with the first objective. But in order to create something meaningful I had to first immerse myself in the world of cyber security, and then break out of some typical analytical norms. Here is my story.\n\nMY KINGDOM TO ANSWER THESE TWO QUESTIONS\n\nI had the privilege of working with some of the best cyber security experts in the field today. They helped guide the analytical research focus by expressing interest in finding bad people already on the network. There are many cyber security companies that focus their efforts on preventing bad people from getting on your computer network, but fewer are focused on finding an intruder who has bypassed such preventions and is already pivoting on the network. In order to identify such unauthorized individuals, we needed to answer two questions: 1) what is on the network? 2) what is it doing? Though, we could philosophically banter how the final algorithm implicitly answers both questions, I will focus on our contributions to answering the second question.\n\nWHAT IS IT DOING?\n\nThrough my literature review of network behavior analysis it became abundantly clear that anomaly detection was the most often used algorithm to identify anomalous events on a network. The reason is that in network data you have a billion examples of normal traffic and only a few examples of abnormal/bad/malicious traffic. It is tough to build a classifier on such an imbalance of class examples, because the classifier would simply label everything as normal and produce a classification accuracy of 99.99999%. Anomaly detection algorithms were created for grossly imbalanced datasets. They ignore the abnormal examples and model only the normal, all the while flagging anything that deviates too far from \u201cnormal\u201d. The hope of this approach is to catch anything not normal, regardless of if it is a new or old type of attack. Unfortunately, there are many drawbacks to only modeling one class. The main drawback is the assumption that you know what normal is. A few years ago, the University of Berkeley and Lawrence Berkeley National Laboratory published a paper on using machine learning for intrusion detection (IDS). They stated, \u201c\u2026traffic often exhibits much more diversity than people intuitively expect, which leads to misconceptions about what anomaly detection technology can realistically achieve in operational environments.\u201d The diversity found in networks makes modeling normality difficult, and it can lead to a high rate of false alarms. I decided to turn this anomaly detection problem into a classification problem. Here is how I did it.\n\n/*BETA FOR DATA SCIENCE FOLKS WHO AREN\u2019T INTO NETWORKING: When two machines are networked together they communicate by sending data packets back and forth. The collection of all of the packets in a conversation between two computers is called a session. This is very similar to how utterances (or data packets) are structured in a dialogue (or a session)*/\n\nFirst, the right inputs needed to be selected. I didn\u2019t want to deal with the complications of deep packet inspection (compute time, encryption, etc.) so I decided to focus only on packet headers. The raw hex dump of the headers offered a beautifully sequential structure with a very small lexicon (256 hex pairs, or words). Not only are the hex pairs in packet headers sequentially ordered (like words in an utterance), but also the packets themselves are sequentially organized in a session (like utterances in a dialogue). This is a perfect recipe for deep learning consumption. The only thing left was to create anomalous sessions for the classifier.\n\nThe trick of switching from anomaly detection to classification is being able to programmatically create or generate anomalies. Recent advances in machine learning (see Generative Adversarial Networks) use competing neural networks to generate examples that are indistinguishable from a training data set. (NOTE: \u201cAdversarial\u201d in this context is not meant to refer to an adversary on the network but rather the competition between the two neural networks.)\n\nIn the spirit of GAN, I manually generated abnormal sessions to look almost indistinguishable from normal sessions using three basic techniques. The first two abnormal sessions that were synthetically created are similar. In the first technique, the order of the source and destination IP and MAC addresses in all of the packets in a session are switched. The second type is similar in that the order of the source and destination ports is switched. The purpose of this approach was to simulate a role reversal between two machines. As an example of machine role reversal, imagine if the server you normally SSH to decides to SSH to your workstation. It is similar to me starting a conversation with my wife by saying, \u201cHoney, I just watched the most moving episode of Grey\u2019s Anatomy;\u201d this is a complete role reversal in a conversation. See the figure below.\n\nThe third abnormal type is accomplished by leaving the source IP in its proper place and swapping out the destination IP address with an IP address the source never talks to (the swap out creates unwanted correlations with within the header \u2014 these will be investigated in follow on work). This is to simulate a conversation that never happens, or should never happen, on the network. It\u2019s similar to me in college telling my friends that I had an engaging conversation with a woman \u2014 a conversation that never happened. See the figure below.\n\nThe implementation is fairly simple. We assume that all of our data is benign. When each session is presented at training time it has a 50/50 chance of remaining as a normal session or being morphed into one of the three abnormal sessions.\n\nThis allows us to have as many examples of normal sessions as we do abnormal sessions. Next, we need to choose the right algorithm and make sure it is assessing the right parts of the packets.\n\nSince both the hexadecimal pairs in a packet header and the packet headers in a session have a beautiful sequential order, a Recurrent Neural Network (RNN) is a natural choice for encoding packets and sessions. We will use two RNNs: one to summarize the hex pairs in a packet header, and one to encode all the packets in a session. We call these the Packet RNN and Session RNN respectively. The Packet RNN starts at the beginning of a header and encodes the first hex pair into a vector of numbers. It moves to the second hex pair and combines its representation with the information passed on from the first. Thus, at any pair in the header the Packet RNN is outputting a summary representation of that pair combined with the information from all the pairs before it. It does this sequentially until the last hex pair. We discard all the outputs from each pair in the sequence except for the last. This final output is a lovely summary of all the information in a packet header (see the red boxes in the figure below).\n\nNow that we have a way of encoding and compressing a packet header into numbers, we need to collect these representations and use them to create a session representation. We use a second RNN, the Session RNN, which takes as input the ordered header representations we just created. It starts with the first header representation and combines its with the representation of the second header, and so on until the last packet in the session (see the blue boxes in the figure below).\n\nIn the end we are left with a real-valued vector that is a compressed and latent representation of the entire session. This paper (including a lovely generative twist) and this one (adding attention mechanisms) are excellent examples of this architecture.\n\nAn attention mechanism is a simple addition to the DL architecture, which allows the user to catch a glimpse into its decision process. It effectively turns the neural black-box, to more of a grey one. The output of the last time step of an RNN, as previously explained, is supposed to be a nice summary of the entire sequence it just digested. But, instead of using 100% of the last output, an attention mechanism creates a weighted sum of all the time step outputs (compare the figure below with the one above). These attention weights are part of the algorithm\u2019s learning process and update as more examples pass through the network. This gives it the ability to ignore parts of the input and emphasize more important parts of the sequence. We use two attention mechanisms: Packet Attention to focus on the most important parts in the header, and Session Attention to focus on the most important headers in the session.\n\nThe figure below is a visualization of the two attention types of the first 8 packets of a session that suffers from destination IP swap out. Since we swapped out the destination IP address and left the source IP alone, we would hope that the Packet Attention mechanism would focus on the destination IP portion of the header. And it does! The darker the blue indicates what part of the header the Packet Attention deemed most important. Interestingly, it also focuses on the destination port, probably thinking they don\u2019t match up very well. The Packet Attention didn\u2019t look at the right parts of every packet in the session, but that is ok. The Session Attention pretty much ignored the packets that didn\u2019t focus on the destination IP address and port areas. The darker red indicates which packet the Session Attention thought was most important.\n\nFollow this link and especially this one for more information on attention.\n\nI tested the accuracy of the classifier on an openly available PCAP file called bigFlows.pcap. The order of the packets in the file is preserved. We use the first 80% of the sessions for model training and the remaining 20% for model testing. Remember, all of the data are presumed to be benign. In reality, some portion of any given network is likely to be compromised. This means the model won\u2019t identify the existing hostility, but it will identify when the attacker tries to spread. The testing data is modified in the same manner as the training. The results are exciting.\n\nWe expect it to do well in this adversarial scenario. Poor results here would have indicated a need for more model tuning. What should really get you fist pumping is the memory capacity of the RNNs. They can remember the relationship between two IP addresses! The next step was to test it out on a labeled IDS dataset.\n\nFinding a useful IDS dataset is difficult. A considerable amount of time was spent looking for an applicable dataset. The University of New Brunswick published an IDS dataset in 2012. It consists of seven days of network traffic PCAP files. The details of the data are in the figure below.\n\nI was only interested in days 1 through 3, thus the other 4 days were discarded. We use the normal traffic from days 1 and 2 to train the model, using only two of the three abnormal types to define abnormal sessions: IP direction switch, and destination IP swap out (i.e. each selected with probability = 0.5/2). Once the model sufficiently learned from the normal and synthetic abnormal data, it was put to the classification test on data from day 3. Below is a confusion matrix of the results from the best model.\n\nWhat this matrix tells us is out of the total actual attack sessions (1608+8346=9954) the model catches 83.8% (8346/9954=0.838) of them, with only a 0.5% (46/(46+8346)=0.00548) false positive rate. Remember, the neural network defines abnormal based on only 2 simple attack types, and zero firewall rules. What if we had 5 attack types, or 10, or 20 to teach the neural network what abnormal is? There is room for improvement. By this time your arm should be sore from fist pumping so much. You can try this out now by downloading the Jupyter Notebooks on our Github repo.\n\nI am aware that these three threat types are pretty basic when it comes to network security, but their effectiveness on the ISCX dataset was surprising. What would make this all the more awesome is the addition of a generative component as described in this paper. This would allow the classifier to go beyond the dataset and be more robust in catching variations of the same attack type.\n\nThe success of this normal/abnormal classifier and attention mechanisms gives hope that this architecture can teach us what is important in headers and sessions for more sophisticated attack types. But finding an interesting event on your network is only part of a complete cyber defense system. Next, you must effectively react to that event. This reaction be the focus of the next phase of our project."
    },
    {
        "url": "https://gab41.lab41.org/jupyter-notebook-sharing-is-caring-5ed4831d7f71",
        "title": "Jupyter Notebook Sharing is Caring \u2013",
        "text": "Here at Lab41, we work with a variety of groups from industry, academia, and even the government. Altair, one of Lab41\u2019s projects this year, involves a close collaboration between Lab41 and the team developing nbgallery, an open-source project from the U.S. Department of Defense. At a high level, Altair is about investigating deep learning techniques for measuring source code similarity. We\u2019re working with the nbgallery team to incorporate these techniques into more informed content-based Jupyter notebook recommendations for nbgallery users. We\u2019ve invited their development team for a quick Q&A to provide more context about Altair and nbgallery itself: Lab41: For those who are unfamiliar, what\u2019s the elevator pitch for nbgallery? nbgallery team: nbgallery is an enterprise Jupyter Notebook sharing and collaboration platform with the goal of making it easier for data scientists and analysts to share and run code-based analytics.\n\nWhy did your team create nbgallery? What use-cases does it address? As huge fans of IPython/Jupyter, we wanted to be able to easily share and collaborate on Jupyter notebooks across our large, distributed organization. Specifically, we wanted to empower \u201ccitizen data scientists\u201d \u2014 those who have the aptitude, curiosity, and creativity to explore data but lack the formal education and technical background of data scientists or computer programmers. In our experience, we\u2019ve found that there has been a relatively high barrier to entry to running Jupyter notebooks for non-technical users. Users or groups would first need to stand up their own Jupyter or JupyterHub instance, which required a background in Linux system administration, or at least familiarity with a command line. After that, they would need a version control system like GitHub to share and collaborate on code-based Jupyter notebooks. We felt that requiring the use of command-line git to save and check out code-based notebooks put Juypter beyond the reach of those who didn\u2019t have at least some software engineering background. We wanted to simplify the process of sharing and executing Jupyter notebooks so that users from a wide variety of skill sets and backgrounds could benefit from and collaborate on cutting-edge analytics written in Jupyter notebooks. By creating the web-based nbgallery, which acts as a visual middleman between the user and a remote git repo, we\u2019ve provided simplified access for our non-technical users to these code-based analytics.\n\nWhat differentiates nbgallery from other similar products or projects? We\u2019re certainly not the first to go down this path; however, our internal compute environment presented some unique challenges that required a slightly different approach. While there are some exciting projects that achieve many of our same overarching goals, the challenge came in attempting to integrate those products into our enterprise data security and compliance frameworks. As you can imagine, our organization deals with a lot of sensitive data and has a strict \u201cneed-to-know\u201d framework, meaning that our users should not be allowed to execute notebooks or access data for which they don\u2019t have sufficient clearance or authorization. Additionally we have strict restrictions on co-habitation of analytic input and output data, so nbgallery must prevent the sharing of a notebook\u2019s inputs and outputs. These requirements ruled out any public SAAS platforms and made it difficult to integrate any off-the-shelf product. The \u201cRun in Jupyter\u201d button connects to a separate Jupyter instance. Our internal version of nbgallery connects to our containerized compute environment, but you can customize this to point to a single notebook server. nbgallery also allows for a Jupyter execution environment that is independent from the notebook sharing platform. For example, the way we\u2019ve instrumented our user\u2019s Jupyter instances is to use an ephemeral (i.e. short-lived) personalized compute environment. This helps to ensure that while notebooks can be shared widely on the nbgallery server, they are executed in enclaves that maintain data security policy and protections. To prevent a notebook\u2019s output from leaving that enclave, all notebook output is stripped before being saved back to the nbgallery server. As an open source project from the government, what challenges did your project deal with during development? We built nbgallery with the intent to release it to the open source community. During its development we made sure it was possible for organizations to adapt it to their own needs, since we ourselves needed customized capabilities to address many of our unique requirements. Our solution was an extension framework for nbgallery that allows for customized plug-ins to support an organization\u2019s unique requirements. As one simple example, we developed and use an nbgallery extension that requires users to include control markings for each notebook, which can later be used for more fine-grained security access. We also support multiple authentication methods; our internal deployment of nbgallery integrates with our enterprise user authentication service, whereas the open source release available to the public includes support for more standard username/password and OAuth-based authentication. What features should the larger Jupyter community know about? Because of the ephemeral execution environment that I previously discussed, the time and speed it takes to install Jupyter was a very important factor for us. To address this, we developed a minimal Jupyter Docker image (<250MBs). The minimal image is based on Alpine Linux and offers a dozen language kernels, most of which are installed dynamically when the user tries to open a new notebook in that language.\n\nTo maintain such a small image, we couldn\u2019t include every possible library that an analytic author might need. However, since many notebooks do require external libraries, we\u2019ve created capabilities for both Python and Ruby to allow language and OS dependencies to be installed on the fly when a user runs the notebook. This means users do not have to know how to install packages from the command line in order to successfully execute code-based analytics found within nbgallery. While our Alpine Linux Jupyter Docker image contains all of the integration capability to interact with the nbgallery server, we have also developed a python package (called jupyter-nbgallery) which adds nbgallery integration into any existing Jupyter or JupyterHub servers. This enables groups or organizations who deploy and maintain their own Jupyter instances to still save notebooks to and run notebooks from a nbgallery server.\n\nAny features on the roadmap ahead? nbgallery has been under active development since November of 2015, and has been in regular use internally since April 2016. A big push for our efforts in 2017 is to improve the \u201cdiscoverability\u201d of notebooks in nbgallery. We see two areas of focus here \u2014 the first is in a recommendation system that can operate on code-based notebooks written in multiple languages. The second is a system to measure a notebooks \u201chealth\u201d which can identify when notebooks have gone \u201cstale\u201d and no longer run as expected. Combined, these efforts can help to improve the ability for a user to easily discover the most relevant notebook for their needs. You mentioned that you wanted to improve the \u201cdiscoverability\u201d of notebooks. How does the work on Altair dealing with code similarity fit into nbgallery? The internal recommendation capability with nbgallery is a big area of interest to us since ultimately we want to improve collaboration. We\u2019ve initially set up systems to recommend notebooks based on content similarity using TF-IDF as well as cosine similarity of user interactions. We contacted Lab41 a few months ago based on their initial research of recommender systems as part of the 2016 Hermes challenge. In particular, could approaches like doc2vec, py2vec, machine learning or deep learning algorithms provide a better way to analyze content similarity among a large corpus of code-based notebooks? We plan on integrating their solutions into the recommendation services used within nbgallery. As one example, the content-based approach will help with recommendations for newly created notebooks. What was the most significant lesson you learned during the development of nbgallery? The most important lesson we learned was how valuable it is to empower users of all backgrounds to create their own analytic solutions. While users without computer programming experience might initially balk at the thought of writing or executing code-based analytics, Jupyter\u2019s web-based Notebooks provide a powerful, yet approachable, analytic platform. And nbgallery makes it easier for non-technical users to mix and match pieces of code from other, well-vetted notebooks for their own purposes. Ultimately, by reducing the barriers to entry we allow users to employ popular and approachable languages like Python, Ruby and R to focus their creativity and innovation on solving pressing challenges of data-driven organizations. What was the one surprising feature request you learned from listening to your users? Notwithstanding what I just mentioned, we still found a significant amount of users who were successfully able to discover notebooks of interest in nbgallery but got stuck when trying to actually execute the code within the notebook itself. We\u2019d like to see how we can help improve the user-experience of running notebooks for beginner users who may be intimidated by the full suite of options within Jupyter \u2014 not to mention the sight of the raw code itself. Things like Jupyter Dashboards offer a lot of potential, and we look forward to further investigating that as well as other options. Our code is available on Github and our Docker images are available on DockerHub. We invite everyone to take a look and let us know what you think!"
    },
    {
        "url": "https://gab41.lab41.org/doc2vec-to-assess-semantic-similarity-in-source-code-667acb3e62d7",
        "title": "Doc2Vec to Assess Semantic Similarity in Source Code",
        "text": "How do you search for source code? A common answer is to enter a few keywords (ex: \u201cpython compare strings\u201d) into your favorite search engine and click the first match on Stack Overflow. If that Stack Overflow post wasn\u2019t exactly the code that you needed, you probably scan through the user comments for a link to a related issue. We love Stack Overflow too, but that won\u2019t work when you need to search within your organization\u2019s internal source code repositories.\n\nIn the beginning of 2017 we started Altair to explore whether Paragraph Vectors designed for semantic understanding and classification of documents could be applied to represent and assess the similarity of different Python source code scripts. Our primary focus was to enable semantically similar source code recommendations for algorithm and expert discovery. However, we believe that robust feature representation of source code could be applied to challenges in cyber security like determining authorship and automated vulnerability assessment.\n\nLab41 is a big fan of Word2Vec and we have had success using word embeddings for unique applications such as system security log files. If you are a regular reader of Lab41\u2019s blog, you might recall that my colleague Alex previously examined word embeddings for source code, which he called Python2Vec. The Python2Vec experiment involved training a Word2Vec model on almost 10,000 Python source code files. The trained model showed promising results when querying for nearest neighbors of example words like \u2018array\u2019 and \u2018range\u2019. We wanted to expand on Alex\u2019s work in two ways: 1) Leverage a lot more training data, and 2) Transition from word embeddings to document embeddings.\n\nA known limitation of Word2Vec is that it needs a lot of training data. NLP researcher Christopher Moody highlighted in a post that word vectorization for a highly specialized vocabulary requires training on hundreds of millions of words. Python source code is highly specialized so we decided to significantly expand the training set for Altair. We used Google BigQuery to download more than 1,000,000 Python scripts from GitHub as candidates for our training data. Google Cloud\u2019s platform (which includes BigQuery) offers a trial that allowed us to download a 30 GB dataset of Python scripts for free. Bazinga!\n\nWhile Word2Vec is great at vector representations of words, it wasn\u2019t designed to generate a single representation of multiple words found in a sentence, paragraph or a document. One well known approach is to look up the word vector for each word in the sentence and then compute the average (or sum) of all of the word vectors. Unfortunately this erodes much of the value that was obtained by training the Word2Vec model on your data. Our primary interest in Altair was to find a way to represent an entire Python source code script as a vector. That led us to experiment with Gensim\u2019s Doc2Vec python library, which is an implementation of Paragraph Vectors.\n\nThe Paragraph Vector and Word2Vec concepts were both designed by researchers at Google. Paragraph Vectors can be viewed as an extension of Word2Vec for learning document embeddings based on word embeddings. Similar nomenclature is used in both techniques and this is probably why Paragraph Vectors are more typically referred to as Doc2Vec or Paragraph2Vec.\n\nAt a high level, a Paragraph Vector is a new token that the authors explained as \u2018a memory that remembers what is missing from the current context \u2014 or the topic of the paragraph.\u2019 At a tactical level, Doc2Vec was designed to recognize that a sentence taken from a document that contains words about physics is more likely to use scientific words. This type of document-level context was not part of Word2Vec. Doc2Vec is built on Word2Vec and Doc2Vec maintains a matrix of paragraph vectors just like Word2Vec maintains a matrix of word vectors.\n\nDoc2Vec offers two approaches to leverage the Paragraph Vector, which are called Distributed Memory Model of Paragraph Vectors (PV-DM) and Distributed Bag of Words version of Paragraph Vector (PV-DBOW). They are very similar to how Word2Vec uses Continuous Bag of Words (CBOW) and Skip-gram, respectively.\n\nOur take on Distributed Memory Model of Paragraph Vectors (PV-DM) is that the paragraph id is inserted as another word in an ordered sequence of words. PV-DM attempts to predict a word in the ordered sequence based on the other surrounding words in the sentence and the context provided by the paragraph id. The input word vectors are averaged, summed or concatenated as part of the classification process.\n\nDistributed Bag of Words version of Paragraph Vector (PV-DBOW) approaches the problem from the opposite direction, similar to how Skip-gram works in Word2Vec. PV-DBOW takes a given Paragraph id and uses it to predict words in the window without any restriction on word order.\n\nWhile the original paragraph vector paper suggested that PV-DM worked well for most tasks, an empirical evaluation by researchers from IBM and the University of Melbourne found that PV-DBOW was superior to PV-DM despite PV-DBOW being a simpler model. The authors of the Doc2Vec library in Gensim also found that PV-DBOW outperformed PV-DM. (Spoiler alert: So did we)\n\nWe used training sets sizes of of 100,000 and 500,000 Python source code scripts taken from the 1,000,000+ total scripts that we downloaded from GitHub Archive. Our first step in preprocessing was to remove all comments/docstrings from the scripts. We normalized the remaining source code with standard NLP techniques, including removing and splitting on any character that wasn\u2019t a letter or a number.\n\nYou might be questioning why we removed all comments. Don\u2019t they provide valuable context too? We originally intended on treating the comments and the source code as two different Doc2Vec/Word2Vec language models. The idea was to obtain a vector representation of the comments from one model and a vector representation of the code from another model. If we concatenated the vectors, it might provide a richer contextual representation.\n\nThat sounded great in theory but the reality of comments in code on GitHub is more complicated. We confirmed that 1 out of every 4 Python scripts that we downloaded from GitHub Archive had no comments whatsoever. Do we have to assign a zero vector to every script without comments? A significant portion of the remaining code with comments contained long, slightly different licensing banners or commented out code that would require a dedicated effort to accurately identify and surgically remove. Even then, the majority of those scripts only contained a handful of \u201ctrue\u201d comments acting as tactical explanations about the next line of code (ex: \u201c#Initialize variables\u201d or \u201c#Catch deprecation error\u201d) .\n\nWe decided to only use source code in our Doc2Vec training while we were manually reviewing a sample Python script that contained no comments. While talking about it we realized that we could comprehend the code because of the context and meaning of the variable names and function names purposefully chosen by the author. As an example of the semantic value of function names and variable names, I bet you can follow this code snippet from Lab41\u2019s Pythia project.\n\nTraining a Doc2Vec model on 500,000 Python scripts over 20 epochs varied from 24 to 48 hours on a large multi-core (32+) server, depending on the parameters and data. Fortunately Gensim included multi-core support for training a Doc2Vec model.\n\nSimilar to what Alex did with Python2Vec, our first impulse was to review the underlying word vectors in the trained Doc2Vec models. It appeared that both PV-DBOW and PV-PM were learning from the Python scripts and forming similar word embeddings. The example below references the Python word \u2018open\u2019, which is used to read (\u201crb\u201d) and write (\u201cwb\u201d) to files.\n\nThe word \u2018poisson\u2019 was interesting as PV-DBOW returned several other probability density functions (ex: laplace, weibull) as nearby words. PV-PM returned several general statistical keywords (ex: uniform, variance) as nearby words but its similarity scores were much lower. This was our first subjective indication that PV-DBOW might be making better representations of words in Python source code than the PV-PM algorithm.\n\nWe suspect that the embedding differences between PV-DBOW and PV-PM are accentuated for rare words, as \u2018open\u2019 occurred 207k times and \u2018poisson\u2019 only appeared 2k times. This might be due to PV-PM\u2019s reliance on word order as a major feature in the model.\n\nHere\u2019s an example that really surprised us. When we asked one of our PV-DBOW models for the nearest words to the data visualization package \u2018plotly\u2019, the model returned several other visualization packages, including pylab, pyplot, seaborn, bokeh and ggplot! How did it learn that?\n\nFinally, everyone knows the Word2Vec vector math analogy example of \u201cking - man + woman = queen\u201d. Identifying similar vector math analogies in Python was a challenge but we had one success with \u201ctry - except + if = else\u201d.\n\nDoc2Vec and Word2Vec are unsupervised learning techniques and while they provided some interesting cherry-picked examples above, we wanted to apply a more rigorous test. To do this, we downloaded the free Meta Kaggle dataset that contains source code submissions from multiple authors as part of a series of Kaggle competitions. Our premise was that two different source code scripts written by two different authors to solve the same Kaggle competition problem should share semantic similarities. More specifically, Doc2Vec\u2019s vector representations of two source code scripts from the same competition should have a smaller cosine distance than a source code script from any other competition.\n\nWe found that the Python submissions in Meta Kaggle contained an incredibly high percentage (75%+) of exact duplicates and near duplicates. We used SequenceMatcher inside Python\u2019s difflib package with a threshold of 0.5 to filter out the nearly identical submissions. Our final testing set contained 907 code submissions for 25 Kaggle competition classes and each competition had at least 10 entries. We evaluated Doc2Vec against the final testing set and compared it with traditional NLP-based techniques: Bag of Words (BoW), Term Frequency Inverse Document Frequency (TFIDF), and Latent Dirichlet Allocation (LDA). All of the models in the charts below were trained on 500k Python scripts from the GitHub public dataset via Google BigQuery.\n\nIt\u2019s worth noting here that the PV-DBOW and PV-DM models performed equally well when trained on 500k Python scripts as shown above but we found that PV-DBOW slightly outperformed PV-DM when the models were trained on only 100k documents.\n\nThese Top 1 classification results gave us confidence that Doc2Vec was capable of generating similar representations for two Python scripts from the same competition and providing one similar source code recommendation. Most systems provide more than just a single recommendation. What if we examined the top 10 closest source code representations for a given submission?\n\nThe overall accuracy of the two Doc2Vec models hovered around 0.20 out of a maximum score of 1.0 (PV-DBOW was the best at 0.21) on the task of the top 10 closest source code representations for a given submission. While not a particularly impressive accuracy on its own, we noted the growing separation between Doc2Vec and the established NLP techniques as the classification difficulty increased. We also believe that Doc2Vec could be further optimized beyond our initial experiments.\n\nStill interested? Source code can be found in Lab41's Altair repository on GitHub."
    },
    {
        "url": "https://gab41.lab41.org/tldr-pre-commit-hooks-can-save-you-from-embarrassment-and-improve-workflow-739730278b76",
        "title": "TL;DR \u2014 pre-commit hooks can save you from embarrassment and improve workflow",
        "text": "Not everything is big data, machine or deep learning at Lab41. Sometimes I need to take a step back and look at how I\u2019m accomplishing my goals. I feel that I should evaluate how I do things more often. I have noticed my co-workers and I set large goals and short timelines. Sometimes rushed, I cut a review corner, and commit things too quickly.\n\nI\u2019m writing this as part of my journey of not committing internal IP addresses, private naming conventions, or using embarrassing words to a git repository. I have found, even under the best conditions, I will accidentally commit something I regret to a repo at least once a project.\n\nOn the bottom of my monitor I keep a tally of the repo resets that I have had to perform since starting at the lab. Even with the visual reminder I still manage to pollute the repo. I am baking compliance into the check-in process with hope that you will be able to improve your development environment in a similar way.\n\nGit can run custom scripts as a user transitions within their development workflow. There are local and server-side hooks for making decisions based on file contents and actions performed. If a hook test fails, then the transaction is aborted and the repo does not get updated.\n\nI would like to walk through my use of a pre-commit script I use to keep \u2018bad\u2019 words out of my commits. After reading an excellent summary and tutorial by Atlassian:[Git Hook Tutorial] I decided that implementing a local hook would suit my needs best. I could experiment in my local repo and not bother the other developers in my team.\n\nRecently most of my development is done in python, but I still work in other languages. I decided to install Yelp\u2019s pre-commit as it has a supportive community. Yelp\u2019s pre-commit is a framework for managing multi-language pre-commit hooks. The framework brokers running my custom hook along with community hooks, improving my code quality and readability.\n\nBefore you can take advantage of running the hooks, you need to install the package.\n\nThere are a slew of existing checks that can be immediately added and utilized. I looked through the listing and found several that I thought would make future dgrossman\u2019s life less miserable. (Future dgrossman\u2019s life is seldom made better by past or present dgrossman\u2019s decisions.)\n\nAfter picking the pre-existing modules I needed to make a file in the root of my repo:\n\nFinally, I was ready to perform the installation and verify the correctness of the file. Running in the repo started the installation.\n\nNow all the files I attempt to commit will need to pass before they can make it to the repo. Perfect time to add my hook for stopping bad words from making it to the repo.\n\nI used Yelp\u2019s guidelines in making the commit hook.\n\nYou will need to make a minimum of 4 files to make a python pre-commit hook.\n\nPre-commit hooks receive a listing of filenames for evaluation. How you evaluate the files is up to the individual author. The verboten_words repo serves as a good starting point, but the basic sketch of what to do for each file is:\n\nAfter writing up the program and testing it, I tagged the version v1.0.0 and then added it to my :\n\nI then re-run to install the new hook:\n\nMy module for not allowing bad words is now installed.\n\nNow my bad word file can be used to keep specific words out of my repo, or be extended to handle more advanced searches for regular expressions that are also deemed naughty.\n\nTo get a quick start on your own hook, clone/check out the verboten_words repo on GitHub."
    },
    {
        "url": "https://gab41.lab41.org/lab41-reading-group-swapout-learning-an-ensemble-of-deep-architectures-e67d2b822f8a",
        "title": "Lab41 Reading Group: Swapout: Learning an Ensemble of Deep Architectures",
        "text": "Next up for the reading group is a paper about a new stochastic training method written by Saurabh Singh, Derek Hoiem, and David Forsyth of the University of Illinois at Urbana\u2013Champaign. Their new training method is like dropout, stochastic depth, and ResNets but with its own special twist. I recommend picking up the paper after going through this post, it is very readable and includes an excellent section on performing inference with a stochastically trained network that I will only touch on.\n\nAs you may recall, dropout works by randomly setting individual neuron outputs in a network to zero, essentially dropping those neurons from training and hence forcing the network to use a variety of signals instead of over-training on one. Stochastic depth (covered in a previous post) is similar, but instead of dropping neurons it bypasses whole layers! We can think of these operations a little more mathematically, but first I\u2019ll have to define some notation.\n\nI\u2019ll use block to mean a set of layers in some specific configuration (for example, a convolution followed by a ReLU), and a unit to be one of the computational nodes within the block (basically a neuron). X will be the input from the previous block, and F(X) will be the output from a unit within the current block.\n\nUsing this notation then, we can think about ResNets as consisting of blocks where ever unit in the block always reports X + F(X). A standard, feed-forward layer can be viewed in this framework as well, with each unit always reporting F(X). The paper includes a figure, which I\u2019ve edited and included below, showing feed-forward and ResNets in this scheme:\n\nThings become more interesting when we start thinking about stochastic training methods in this manner. Dropout can be thought of as randomly selecting the output for each unit from the following set of possible outcomes: {Zero, F(X)}. Likewise, stochastic depth can be thought of as randomly selecting between the outcomes {X, F(X)} for each block, so that every unit in the block returns X or F(X) together. Both of these training methods are shown in the figure below, which is again has been modified from the paper:\n\nSo now that I\u2019ve laid the groundwork, what does swapout add? Well, add isn\u2019t really the right word, swapout combines! It randomly selects from the four possible outcomes mentioned above: feed-forward, ResNet, dropout, and stochastic depth. They do this by allowing each unit to randomly select from the following outcomes: {Zero, X, F(X), X + F(X)}. Therefore, swapout samples from every possible stochastic depth and ResNet architecture, both including and not include dropout!\n\nIn addition to swapout, the authors define a simpler version called skipforward. Skipforward only allows units to select from the outcomes {X, F(X)}, that is limiting the choice to only stochastic depth and feed-forward. Both of these architectures are shown in the figure below, which is again from the paper with modification:\n\nOne of the dilemmas when using stochastic training methods is: how do I use the network at inference time? When training the network is constantly mutating as units pick different ways of behaving, but at inference time that network needs to be roughly static so that the same input will always yield the same prediction. We can make the network static in two ways:\n\nAlthough it seems like deterministic inference should be faster (because it does not require running multiple networks) it has several drawbacks. The first drawback is that you can not actually calculate the true expectation value for a swapout network, only approximate it. The second is the the fact that batch normalization \u2014 one of the most powerful training methodologies, see our previous post for a summary \u2014 does not work with deterministic inference. The authors conclude (through testing) that stochastic inference works best.\n\nThe authors test swapout and skipforward networks against networks trained with stochastic depth, dropout, and various ResNet architectures. They conclude:\n\nOne final note on the paper: the way they define the various operations as random selections from a set of possible outcomes is, for me, a very intuitive way to think about them. I would love to see other papers use a similar framework for describing their network modifications!"
    },
    {
        "url": "https://gab41.lab41.org/850k-images-in-24-hours-automating-deep-learning-dataset-creation-60bdced04275",
        "title": "850k Images in 24 hours: Automating Deep Learning Dataset Creation",
        "text": "Normally this computer vision adventure would start with the protagonist scouring the internet to find dataset owners. These individuals have already gone through the trouble of amassing a large number of images, looked at each image, applied labels and/or tags for each image. This individual would have packaged things up for their own purposes and probably had the labeling work performed by indentured graduate students.\n\nIf you are lucky some small percentage of each corpus located relates to the feature/item that you yourself are attempting to amass.\n\nWash, rinse, repeat until you have amassed a corpus large enough to satisfy your personal data lust.\n\nI would like to bypass corpus searching, downloading, and triage by automagically making my own labeled data. I want to make a dataset and only spot check a small number of images.\n\nI am building a classifier and need a healthy number of images in a short amount of time. The classifier will be used in an upcoming project named Pelops. The classifier will take an image and determine the body type of a vehicle i.e. subcompact, compact, car, van, SUV, crossover, etc.\n\nI initially started thinking of sites with published APIs to call, like Edmonds, where you can specify the make and model of a vehicle. This approach would allow me to get images by make/model and I could use another site to ask for images by body type.\n\nSecondly I thought of hierarchical sites similar to Craigslist, where I could grab images and mine the posting for make and model. (Mining the posting was something I was not looking forward to.)\n\nIt became apparent that I would need to work across several sites to get the number of images I wanted. Each site would need an interface to the API or customized scraping code to bring in the images.\n\nIt was getting close to coding time; I needed to channel my inner lazy developer and think of ways of getting images on my terms. I definitely wanted to hand a list of attributes to something and have it return a series of images. A colleague and I thought that this is perfect occasion to abuse a search engine. I can supply color, make, and model; the search engine would mostly return images related to my query.\n\nRunning with this idea I found an article of the top 10 car colors, along with a GitHub repo where I could quickly extract around 1000 makes and models of vehicles.\n\nWriting code to query/scrape a search engine then download images from a search engine took less than a half hour. Running the script, creating the list of images to download took 5 hours (search engines get persnickety if scraped too quickly).\n\n(100 images * 1046 make/models * 10 colors ) would mean I would hopefully amass around a million images. Whether they would be useful images was something for future me to worry about.\n\nThe actual image downloading took over 10 hours. Some sites were unresponsive or slow to download.\n\n[NOTE: Some of the material you download may be copyrighted]\n\nEven though I asked for images, sometimes you don\u2019t always get what you want. I was not performing file type checking at download time. Spot checking showed I was sometimes downloading HTML instead of the image. The number of non-images was small compared to the number of images, though.\n\nI needed a program to detect if a file was indeed an image or not. The program was pretty speedy and cut through the million plus images in around 7 minutes.\n\nRemoving the not-images led to the next issue; some of the downloaded files were not images of cars. Not wanting to personally Mechanical Turk over 950k images, I turned to using a pre-trained computer vision model to help me out.\n\nI made use of Keras and a pre-trained neural network (resnet50) to determine if a car was in the image. If one of the following terms [\u2018car\u2019, \u2018truck\u2019, \u2018suv\u2019, \u2018ambulance\u2019, \u2026] was in the top 4 labels returned for the image I kept the image.\n\nRunning the images through a GPU assisted deep learning model took around 8 hours. The model evaluated approximately 2000 images/minute on a single Titan X.\n\nAfter the final pass, about 840k images were remaining. A cursory look through the data showed mostly cars.\n\nMy final lazy idea for supplementing image labels comes via even more search engine abuse. I wanted labels of car, SUV, truck and/or van for each vehicle. To see if a specific make model of a vehicle should have the label, simply search for it and count the results.\n\nAttempting to classify the Ford Mustang I would perform 4 searches:\n\nI would then capture the number of results returned by the search engine.\n\nYou could take the largest value as the label, or use the array as vector to describe the class. I bet the vector would be better because of cars like the Chevrolet El Camino."
    },
    {
        "url": "https://gab41.lab41.org/nips-2016-review-day-3-21c78586a0ec",
        "title": "NIPS 2016 Review, Day 3 \u2013",
        "text": "Good morning! It\u2019s the final day of the conference before workshops. There\u2019s a lot of stuff that\u2019s happened, and you can check out some of that in my reviews of Day 1 and Day 2. Today\u2019s agenda? Some talks and then the symposia. I\u2019m here with my coworkers Anna and Brad. We\u2019re sad that it\u2019s the last day, but excited to get the day started!\n\nThere were two invited speakers to kick off the day. The first one was on Brain and AI from IBM Watson\u2019s Irina Rish, and the second was a Stanford\u2019s Susan Holmes\u2019 take on reproducibility of research and modeling bacteria in the body.\n\nNeuron Imaging and Beyond (invited) \u2014 The brain talk was special interest to me because of some previous work I was slated to do with brain data. The idea was to find the best features to use for discrimination purposes. They started using deep learning, but a considerable amount of their work was in using programming techniques with regularization (LASSO, group LASSO, and other variants). Things that I learned? Adult brains still grow neurons! Meditation works differently for males and females. You can tell the difference between meth and ecstasy users. Ritalin apparently resembles cocaine.\n\nHuman Biome (invited)\u2014 NIPS has an interest in getting people from other fields to give talks. Today\u2019s was Susan Holmes\u2019. A primary reason for her talk was to see how biostatistics can help us with reproducibility. The biggest challenge is to \u201cmaintain and percolate uncertainty throughout the analyses.\u201d She emphasized sharing code, data models, and results. That\u2019s the reason why all her graduate students take longer to finish.\n\nMiscellaneous: Dr. Holmes has opinions about the term \u201cmetadata\u201d. She said, \"Metadata is something invented by the NSA. As far as I am concerned everything is just data. Metadata doesn't exist.\u201d Cool thing that I learned: you can tell pre-term birth based on bacteria. Not so cool thing: the U.S. is no better than 3rd world countries in number of pre-term births. A trivia answer, I\u2019m sure: we are more bacteria than regular cells.\n\nLearning to Criticize \u2014 Criticisms help diversify examples shown to users so they can better understand what category an item is in. So, say you want to decide whether or not strawberries are fruit. You\u2019d take a look at all the things that are fruits, but a lot of the time, you\u2019d ignore what isn\u2019t. That\u2019s a problem that humans generally have: they ignore criticisms. To remedy, they (Been Kim and al) first selects prototypes, generated from a distribution. Then, they select criticisms that represent points where p and q are different. They propose maximum mean discrepancy (MMD) as a measure to optimize for the difference between two distributions P and Q.\n\nThere were three symposia, all running concurrently. These panels were fairly useful as a review of what\u2019s been going on in machine learning and AI.\n\nWhat I\u2019ve learned about symposia is that they\u2019re good ways to learn what famous people are going to do next. And in most cases, they\u2019ve already done it, so you should look for them on arXiv. For example, Peter Xi Chen\u2019s got one on arXiv, perhaps based on stuff he\u2019s done with InfoGAN applied to Variational Autoencoders (I don\u2019t know, I haven\u2019t checked.) The WaveNet guys have applied the similar techniques that they applied to WaveNet to bytes information and video, both posted this year.\n\nA lot of the recent stuff are generative stuff: Representation building, InfoGAN, WaveNet. I found Peter Xi Chen to be very persuasive on InfoGAN. On this note, it\u2019s clear at the panel that Yann LeCun has fallen in love with Ian Goodfellow\u2026or at least the fact that he invented GANs.\n\nSchmidhuber presiding. By the way, if you ever need to address him, the pronunciation is, \u201cYou_again Shmidhoober.\u201d\n\nThe session is a walk through the history (history starting 3 years ago) of memory networks as it stands in the most recent work:\n\nNotable quote in this session when presenting a slide of related work, \u201cAnd this is stuff done within three years, because no one remembers anything from more than three years ago.\u201d\n\nOf note, OpenAI\u2019s pushing their environment/world for other people interested in true AI. They have got hundreds of fully integrated environments, and 1000+ without the reward structure in place. The purpose is to learn the fundamental nature of rules, i.e. given a distribution of environments how do you choose which RL algorithm to use. It\u2019s all in the framework of this idea of meta-learning, which pretty much means, you\u2019re operating in the OpenAI universe and reinforcement learning plays heavily.\n\nNando de Freitas also addressed the meta-neural networks talking about Neural Programmer-Interpreters, and how one algorithm and weights solved 3 very different problems. He finished with a quote that should be cross-stitched on a throw pillow, \u201cWe should apply the same rules to models as we do to algorithms, and test them in multiple environments.\u201d\n\nMarcus Hutter\u2019s asymptotically fastest solver of all well-defined problems and other universal program search methods. Basically, he\u2019s got this theory (and his proof is apparently one page) that you can solve everything fast. To do so, search all proofs to find a program q that your solution works within a time bound, spend a lot of time on analyzing the problem, and then compute q with a best current bound: n\u00b3+10\u00b9\u2070\u2070\u2070=n\u00b3 + O(1). Not sure where he got 10\u00b9\u2070\u2070\u2070 from, and I\u2019m even less clear what O(n\u00b3) has to do with. Closest thing I could find online was this.\n\nI guess the less crowded of the three. AI is here to stay, and the idea is to figure out how to improve/identify problems where they are already under regulations or legal control and where they need to be. Since the time conflicted, we were unable to attend this one.\n\nThanks everyone for tuning in! We\u2019re going to checkout the workshops tomorrow. Be sure to check out our Day 1 and Day 2 reviews. Signing off from Sunny Spain (where it\u2019s 65 degrees and lovely), sus amigos: Anna, Brad, and Karl!"
    },
    {
        "url": "https://gab41.lab41.org/nips-2016-review-day-1-6e504bcf1451",
        "title": "NIPS 2016 Review, Days 0 & 1 \u2013",
        "text": "Good morning, fellow machine learners. A few of us from Lab41 recently jumped the pond over to Barcelona, Spain, to see what machine learning and artificial intelligence stuffs we could glean from eager minds. We found former colleagues, former students, and more deep learning algorithms than the number of cat pictures on the internet.\n\nThe organizers came out before the keynote (Yann LeCun) to introduce us to NIPS 2016. They pulled together some statistics from the tags that submissions self-identified with. According to the picture below, the distribution of papers is heavy tailed, and the spread of topics makes for a rich problem set. That\u2019s a first order statement, since there seems to be high correlation between topics in a given paper. (I\u2019m sure large scale learning can be applied to computer vision, and they\u2019re using deep learning to do it.) Still, there\u2019s a wide variety of things you can see here.\n\nEver the scientists, the two organizers justified their choice on the program committee by maintaining that they want to grow the number submissions while decreasing bias and variance. They treated the problem with unknown ground truth of what the \u201cbest papers\u201d were,\n\nKeynote \u2014 Yann LeCun gave the keynote on \u201cPredictive Learning\u201d, an ambiguous title of a talk he must have presented a million times by now. It\u2019s the one where he makes an analogy to parts of a cake: unsupervised learning is the filling of the cake\u2026the big kahuna, supervised learning is frosting on the cake, and reinforcement learning is the cherry on top. It seems like he\u2019s been burned by this, and was apologetic, saying that he\u2019ll make it up (to Deep Mind, maybe?) because reinforcement learning, if aided by unsupervised approaches, is also a big chunk of where research should lie. He pretty much gave an overview of what he viewed as important topics. Among other big ideas, he said that Generative Adversarial Networks were the most important innovation in machine learning in recent times, and he credited Ian Goodfellow. High praise from one of the elders of Deep Learning.\n\nIntelligent Biosphere \u2014 Google Deep Mind\u2019s invited talk to kick off the first day on the Intelligent Biosphere. Drew Purves from Deep Mind is the speaker, and it\u2019s a refreshing insight into how AI can be used for social good. The premise is that AI can help Nature, but on a less intuitive note, Nature can help AI. On the former, if you think hard about it, you\u2019ll know that statistics and machine learning can aid policy makers on producing less waste in farming, agriculture, and other efficiency saving measures. Besides augmenting efficiency, breakthroughs can occur to make new innovations that can aid humanity. On the latter, he made the distinction between natural versus artificial and real versus simulated. The world is scale-less, cyclic on many levels, fuzzy, and just plain hard to work with. On the other hand, everything we train has been scoped. We can take cues to build better simulations of the real world.\n\nThere was a whole bunch of other stuff, but the idea is how to make sure machine learning algorithms applied to the real and natural, and so they were proud to introduce their real-world simulator. And\u2026their slides are, by far, the most stylish.\n\nOne fun note is that Purves pointed to two high school students getting a head start on things. These guys had started playing around with Tensorflow, and it\u2019s encouraging to see the future talent come. I was slightly disappointed that there wasn\u2019t any applause, but Purves moved on quickly and needed to wrap up.\n\nBest Paper Award: Value Iteration Networks\u2014 The best paper award went to Aviv Tamar, who talked about Value Iteration Networks, from Berkeley\u2019s Artificial Intelligence Research Laboratory. It was a new look at reinforcement learning. He wanted to build a neural network that can learn to plan a policy rather than follow a totally reactive policy. He also wanted it to be model-free, and what better way to do that than to use CNNs? His paper is on ArXiV and will appear in proceedings.\n\nSome big themes that I noticed, people were either using batch normalization or layer normalization, it has become somewhat of a mantra. I noticed at the posters, there was something called Weight Normalization too; I\u2019ll report on that tomorrow when it\u2019s in an oral.\n\nThe themes of the conference were:\n\nOn more specific themes, I happen to be working on an audio project currently, so maybe my thoughts are a bit skewed on which things are most interesting. If you have alternative views of what was going on in the conference, please do send me a note.\n\nWho knew that one dimensional signals would give us so much trouble. Time series and audio were very present at this year\u2019s NIPS, the hard problems, of course, being non-stationary signals.\n\nAapo Hyvarinen (Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA) was first in the unsupervised track with nonlinear ICA, where he asserted that independent components in nonlinear systems are difficult to obtain because the inverse problem (of a deep learning function) is impossible to solve. He proposes using Time-Contrastive Learning, which seems like another \u201csomething\u201d-2vec at first glance.\n\nOne talk, Using Fast Weights to Attend to the Recent Past, was on fast and slow weights for LSTMs. There is slow varying, longer term information and then there\u2019s fast weight rapid learning also decaying rapidly, storing specific temporary information. Solution? Add a layer to attend to the fast stuff.\n\nOther works dealt with compressing neural networks and providing approximate bounds on them (a la Supervised learning through the lens of compression). The concept was simple: you add an epsilon to your hypothesis, and say that you\u2019re compressing alright if you match your truth+ epsilon instead of just truth.\n\nPhased LSTMs (PLSTMs) \u2014 As if LSTMs aren\u2019t complicated enough, we\u2019d like to put more stuff in there. It seems like a sampler, but sampling at different phase. They built this to deal with really long sequences, and they sampled at regular frequencies (think Wavelets style: at low, medium, high frequencies) by putting a gates inside the LSTM.\n\nSRNNs \u2014 This one got some flak from Li Deng and others: use recurrent neural networks (RNNs) in combination with State Space Models (SSMs). RNN\u2019s are good at long-term dependencies. And SSMs are good at modeling uncertainty. So, let\u2019s put\u2019m together! Then, model non-stationary stuff.\n\nLots of GANs, here. Also, Ian Goodfellow gave a shout out to plug and play generative networks, which I saw at the posters. The idea was to synthesize images, but do it with a prior so that it doesn\u2019t look so creepy. Here are some of the images.\n\nOne of the practical papers on seeding k-means and improving upon k-means++ was Fast and Provably Good Seedings for k-Means by Oliver Bachem. The idea is to use MCMC to jump around the clusters in an efficient manner and seed the k-means with good clusters.\n\nHaving a discussion with another conference-goer, I learned that European conferences have a reputation for cutting registration right off. I contrasted with my experience at ICML in New York, where they booked an entirely different venue to supplement the overflow of conference participants. Up and downsides.\n\nI don\u2019t know if it\u2019s now commonplace, but WiFi is intermittent, which is frustrating. But on the positive side, the conference app, \u201cWhova\u201d, is neat. The forum is worth checking out. You should note that \u201cWhova\u201d actually doesn\u2019t need the internet, so this mitigates the fact that not everyone got hard-copy programs. The poster session was packed; it was clear that this was one of the largest conferences I\u2019ve been to in a while!\n\nI personally attended Nonstationary Time Series and Generative Adversarial Networks tutorials, where it was so crowded, people stood on the sides. On this point, I must apologize to the twenty people that I had to step over to get to my seat. I promise that I would have been just fine sitting on the sides, but twenty minutes after the lecture started, the security guard kicked everyone off from the edges, and made us all find seats. For good reason though, I think it\u2019s good that there\u2019s strict adherence to their fire codes in mind.\n\nIan Goodfellow delivered the GANs tutorial. So\u2026Las Fallas is a celebration in Spain in March, but there were some fireworks today. I\u2019m not going to gossip, but there was some contention about who came up with what first. Goodfellow shut it down pretty hard and quick.\n\nPlease let us know if there\u2019s anything that I missed, since I was severely jet lagged. More fun tomorrow! Please let us know what other fun things were at NIPS if you\u2019ve attended via e-mail (kni@iqt.org). Tune in tomorrow for the second blog post on NIPS 2016. It\u2019s a blast out here in Barcelona, and we love sharing what we\u2019re seeing!"
    },
    {
        "url": "https://gab41.lab41.org/nips-2016-review-day-2-daff1088135e",
        "title": "NIPS 2016 Review, Day 2 \u2013",
        "text": "Why good morning again, fellow machine learners. It\u2019s another day at NIPS, and what a marathon. The sessions ran from 9am to 9pm last night, and I was there for most of it! (Check out my NIPS 2016 Review, Day 1 for the low-down on yesterday\u2019s action.) Ok, let\u2019s get crackin\u2019.\n\nOverall, today\u2019s talks were better polished and more easy to follow, though that\u2019s partly because of jet-lag yesterday. You\u2019ll see a bit more coverage today over yesterday. The posters were still difficult to navigate, and so I just took note of which ones had the most people around them. (Generally, those were the ones with big company names behind the authors.) I\u2019m not going to go into detail with those here, mostly because I couldn\u2019t ever get close enough to snap a photo of the poster.\n\nAlso, on a more fun note, I talked with someone from Google\u2019s DeepMind. Apparently, they\u2019ve got some 150 people here. That\u2019s about half the group, with the other half being support staff!\n\nToday\u2019s first invited talk: machine learning in particle physics. The speaker is Kyle Cranmer, and he\u2019s from the Large Hadron Collider and did some work with the Higgs Boson. He now works on a project at CERN called Atlas. Their work has a lot of great innovations with PGM\u2019s, and shows what they\u2019ve been doing with likelihood free estimation.\n\nThe basic idea is that they\u2019ve got a lot of parameters in particle physics. That means your likelihood function is nuts. So, then you try to do inference on it. You then realize, that doing that is nuts. The example that he gave was in the following forward model (I guess, maybe looking for the Higgs Boson particle):\n\nThey\u2019ve had discussions with David Blei, and their machine learning contingent is quite strong; Cranmer\u2019s team has more machine learning staff than physicists. His modeling efforts have yielded a pretty generalizable chart shown below with CARL being his invention:\n\nOne of the more entertaining talks since there were so many videos. Here\u2019s a whole bunch of robots that they\u2019ve built.\n\nThey\u2019ve built a fairly robust humanoid robot (The Next Generation Atlas). From my coworker, Brad:\n\nAnother newer robot is called Spot. It was a crowd favorite: knocking it over, closing doors on it, hitting stuff it was trying to pick up, and throwing a banana on the ground for it to trip over. The engineers were the Lucy to it being Charlie Brown.\n\nIt\u2019s interesting to note that they are doing no learning whatsoever. At a conference where that play prominent, they were especially cognizant of that, and I think they\u2019ll be looking for AI experts in the near future.\n\nLearning by Poking \u2014 There was a similar talk in the afternoon session. This robot actually learns to manipulate objects by poking it. The experimental setup is this arm hanging over an object and you want it to move this object to another location.\n\nThe training setup: an initial image and a goal image. The robot then makes random pokes and will get rewarded if after it pokes the object, and the object gets closer to where it appears in the goal image. It then memorizes where it pokes the object and what happened to its orientation and position afterwards. They tried to make it harder by adding other random objects that are distractions to the the objective. In many cases, there are complex movements that are required to manipulate the object to get to the goal. Also, the objects that it pokes aren\u2019t your typical blocks; they\u2019re coffee cups (that roll) and other stuff that\u2019s randomly shaped.\n\nHere, we\u2019d like to point out that because it\u2019s second, I don\u2019t think it\u2019s less \u201cbest\u201d than the \u201cbest\u201d paper award. It\u2019s a very rigorous proof answering some questions on why non-convex optimization works well on training machine learning models, specifically for the matrix completion problem.\n\nI didn\u2019t know this, and it appears like it\u2019s recent work (2015), but there\u2019s been some stuff on building a conjectural unified theory, saying that all local minima are (approximately) global minima. This paper establishes this property for the matrix completion problem, implying that stochastic gradient descent (SGD) converges to a global minima. There\u2019s an initial sampling element, but with high probability under random initialization while using popular optimization techniques, the end minimum value stays consistent and solvable in polynomial time. The proof was a bit quickly glossed over, so it\u2019ll be worth looking at the paper.\n\nThere were two other talks relating to global and local minima. One was titled Without-Replacement Sampling for SGD, the other was Deep Learning Global Minima. Because the latter was in the same room, I opted to stay.\n\nDeep Learning without Poor Local Minima \u2014 Looks like global minima is the rage now (as well it should be). There were some strong statements in this work, but the idea is that while random initialization gives you all sorts of rando-weights, if you train with similar optimization functions, you\u2019re going to get similar performance.\n\nThe proof begins with the fact that there are apparently seven strong assumptions to ensure convexity (that is, local minima = global minima) under linear neural networks. He essentially shows that under assumptions 1\u20134, there are no bad local minima, which I guess there was a 1989 paper on. He then moves onto nonlinear neural networks (with ReLU activation), and with even fewer assumptions, asserts the same. I would encourage you to read this stack overflow for some background. I originally saw this on arXiv, and there\u2019s a good thread on Hacker News.\n\nFollowing the trend of neural network image and video synthesis, there\u2019s been some interesting stuff at this conference. At ICML, we saw Scott Reed\u2019s image from text work. Today, just a half year later, more of Scott Reed. Some other works without the need for GANs (congrats, Katie!) show some impressive predictive video.\n\nModeling Future Frames from an Image \u2014 Exactly what the title says. The demo was of a lady doing exercise, and it trains using the motion information from similar video sequences. They delivered it very well, and you can catch a lot of this at their website and on the video:\n\nGenerative Adversarial What-Where Networks \u2014 Another GAN from Scott Reed. He\u2019s at all the big conferences. It\u2019s similar to his Generating Interpretable Images talk. But now you can put these birds and stuff anywhere you want to. You just tell it some keypoints, and then off it goes. You can take a look at his code at: https://github.com/reedscot/nips2016. Conclusions: location conditioning is useful for image synthesis. It adds an additional layer of control to get more interpretability. Works well for birds, not so well for humans and faces.\n\nWeight Normalization \u2014 This is yet another normalization come optimization time of deep learning neural networks, though the results that he showed at the conference were more on accuracy rather than computational performance. Their argument is that batch normalization adds noise to your gradient updates. While noise is probably good when you\u2019re training images with CNNs because it adds a bit of regularization (e.g., it can take care of invariances and stuff that doesn\u2019t matter), it\u2019s not so useful when you want to do reinforcement learning. Instead of normalizing batches, he does normalization directly onto the weights, the contributions being: weight normalization + data dependent initialization. He showed this on reinforcement learning with DQN. Looks like the scores got better, sometimes 30% more. Their code is at https://github.com/openai/weightnorm. For keras, it\u2019s two lines of code.\n\nSupervised Word Mover\u2019s Distance \u2014 Killian Weinberger is co-author on this work that can do document comparisons. It\u2019s based on their ICML 2015 paper, titled From Word Embeddings To Document Distances, but this one is supervised. The idea is essentially to use Earth Mover\u2019s Distance on a Bag of Word Vectors. He calls it Word Mover\u2019s Distance, to which my colleagues and I laughed at the fact that Matt Kusner, the graduate student, had WMDs. It\u2019s a pretty interesting metric: you take the L2 distances of word vectors from one document and fill it into the second document. Their contribution is to learn a matrix that takes care of known similar words and apply it to this distance metric.\n\nThe Chinese Voting Process \u2014 Though the title is a play on the Chinese Restaurant Process, being from Korea, I would\u2019ve expected the author to know full well that the Chinese don\u2019t vote. His talk deals with bias in up-voting and helpful reviews in product reviews, Amazon stuffs, and stack overflow. There were a few good examples: apparently presentation bias (the tendency for people to conform to other\u2019s opinions) plays strong on stack overflow. In another example, he searched on Amazon shopping with a bold query of \u201cnips\u201d. Luckily, he came back with some chocolate cookies, but there are some interesting implications on how voting can be biased and reinforced due to position. His method takes care of this through a generative process called the Chinese Voting Process. This process essentially models (temporally) how people would vote based on the reviews already existing in the corpus. It\u2019s similar to the restaurant process, but the idea is that certain reviews build momentum and trickle up to the top, not by any merit of the review itself, but because of the biases inherent in behavior.\n\nWe\u2019ve undoubtedly missed something, I\u2019m sure! Be sure to get our take on Day 1 too. Please let us know what other fun things were at NIPS if you\u2019ve attended via e-mail (kni@iqt.org). Tune in tomorrow for the final blog post on NIPS 2016. It\u2019s a blast out here in Barcelona, and we love sharing what we\u2019re seeing!"
    },
    {
        "url": "https://gab41.lab41.org/sampled-backpropagation-27ac58d5c51c",
        "title": "Sampled Backpropagation \u2013",
        "text": "You might have heard of this thing called deep learning, and maybe even understand how computationally complex it is. This complexity comes about because you have linear algebraic operations with matrices that are really big. It\u2019s a familiar theme: matrix multiplications are the bottleneck. It\u2019s a major reason why performance in BLAS, LAPACK, and signal processing libraries have been squeezed through generations of electrical engineers and computer scientists. We\u2019d recently gotten even better at it with more specialized hardware like GPUs and TPUs.\n\nUpdating matrices with SGD is a core requirement for backpropagation in deep learning\u2026but, what if you didn\u2019t have to update all the columns of the matrix all the time? What if I told that you that in deep learning, your largest matrix update could be computationally reduced ten thousand fold and produce more accurate results in fewer iterations? What if, by doing this, you can train an image classifier to recognize millions of things in less time than it takes to train current algorithms to recognize thousands?\n\nWe recently stumbled onto the fact that you can do backpropagation, at least in the final layer, with drastically smaller matrix updates by sampling the labels. When you do this, not only will matrix updates be quicker per iteration of SGD, not only will it require fewer numbers of iterations, but you will even improve your accuracy and recognize a ton more things. In this post, we\u2019ll talk about how exactly this happens using a classic image annotation problem, and we demonstrate that it can work on the most unstructured and noisiest of data: photos in the wild from the internet.\n\nIt all starts with the fact that deep learning earns at least part of its keep with randomness: random initialization, random dropout, (hehe, random choices of architecture, random trying stuff out), etc. Closely related to randomness is the idea of sampling methods, where you\u2019re taking random points from data.\n\nIf you have trained image classifiers, you\u2019ve done it by randomly sampling your images and putting them in batches. Our contribution is the same idea but done to the max. Within each data sample, we\u2019re also going to sample the labels. If you take anything away from this blog post, it\u2019s that idea. Take a look at the animation below. We\u2019re first sampling our images, and then we\u2019re sampling the labels. As it turns out when you do that, you burn through your training much faster and more efficiently\u2026and interestingly enough, achieve higher accuracy.\n\nWe really only tried the sampling stuff on the final weight matrix, mostly because we\u2019re assuming the classification layer is the most computationally burdensome\u2026not an unfounded assumption. It\u2019s almost certainly the case if you\u2019re dealing with user generated content from the internet. These are photos that people have uploaded and tagged, and they can, and pretty much always do, tag them with whatever random word that comes to mind. You can imagine the number of possible things that supervised machine learning can classify using these tags\u2026in the millions in our case. Unfortunately, if we\u2019re using neural networks, that means that the last weight matrix is \u00d7 millions. That\u2019s a pretty big matrix\u2026and that\u2019s the reason our sampling method is essential.\n\nAh, open source multimedia; it\u2019s what 90% of the internet is made of. And while the internet is a dirty, noisy place, maybe there\u2019s something that machines can learn from it. One of the largest datasets that is composed of uncurated, open source multimedia is the YFCC 100M Dataset. The underlying reasoning for its initial release was its scale: lots of images, lots of metadata. A related issue, though, is its noisiness and overall difficulty. It\u2019s easy to understate this, so there\u2019s nothing like actually looking at the data. If you actually parse through the corpus, you\u2019ll start to notice individual metadata tags sometimes make sense but oftentimes don\u2019t. For example, take the below.\n\nYes, we\u2019ll see as metadata tags, which perfectly represent the picture. But the other stuff is enough to throw you off. I\u2019m guessing that this was an Austrian Tourist who has a Canon camera, and he went to Venice in April 2015. To get a machine to recognize that is difficult, but that\u2019s another story altogether. The YFCC chalenge has been to be able to decipher content from an image, and in that sense, the tags are noise.\n\nThat\u2019s actually a pretty good example of the tags. There are some that are just inexcusably bad.\n\nThis is literally the majority of the YFCC dataset; crap like this. It\u2019s bad, yes, but the hope is there is enough data to be able to overcome these issues. And, in fact, as you will see, there is\u2026surprisingly so. Again, it\u2019s the idea that the individual tag and image will likely not make any sense, but the corpus as a whole will produce a pretty reasonable classifier.\n\nDeep learning have mostly been trained with only thousands of labels on heavily curated, iconic, single-labeled words. Something like the YFCC dataset, though, has millions of labels. I ended up cutting it off at 400,000, but even that many words is difficult to accommodate during backpropagation.\n\nWhat is good at large vocabulary unstructured text are word embeddings. These have been called neural networks, but you should probably know that they\u2019re more wide than deep. One of the more well-known algorithms is word2vec, and in the subsections below, we\u2019ll tell you how we adapted its most prized contribution, sampling, to our problem.\n\nIn the past few years, not many papers have had more impact than Tomas Mikolov\u2019s word2vec. The major takeaway from word2vec is their use of negative sampling, through a broader idea of noise contrastive estimation. Here the negative in negative sampling just means that you\u2019re sampling from the distribution of unrelated words. The function as written in his paper is:\n\nHere, the v\u2019s with subscripts i and o are the input words and context words, respectively. To define an input word\u2019s meaning, you\u2019re using its context: its surrounding words (the first term). But you\u2019re also defining the input word by what it\u2019s not: things that aren\u2019t in its context. That\u2019s the second term, and the n~p(i) under the \ud835\udd3c is a fancy way of saying you\u2019re taking random samples over all words: i.e., negative sampling.\n\nThe first term pulls words together (maximizing correlation) while the second term pushes them away from negatively sampled words. It can be shown that if the sum in this term extends to the entire word vocabulary, the above equation is equivalent to a sampled cross-entropy function. (See this for a visual description and our ArXiV paper for a mathematical one.) Instead with sampling, you\u2019re approximating the distribution rather than empirically considering the entire word vocabulary.\n\nAgain, word2vec uses an input\u2019s surrounding words as context. In our case, the context of an image in the YFCC dataset is its tags in the metadata, and the negative samples are unrelated tags. Most entries into image classification competitions use categorical cross-entropy as the objective function, and since word2vec is essentially sampled cross-entropy, it makes sense to use the exact same idea.\n\nIf you take a look at the previous section\u2019s equation, all we\u2019re going to do is replace the input vector with a neural network hidden layer feature. Then, we do backpropagation through the rest of the deep network.\n\nHere, the g(x,{W}) is our neural network with the set of weights denoted by {W}, which we are optimizing, and v\u2019s with p and n subscripts are the context and unrelated tags, the positively and negatively sampled vectors. It might seem daunting at first, but we just replaced the input vector with some deep learning layers, and are now also sampling the labels because we have to do tensor algebra (the first expectation).\n\nSo, why negative sampling? With ordinary backpropagation, you\u2019re weighting all the unrelated stuff the same. By sampling, we\u2019re utilizing the actual probability distribution of the data itself. It\u2019s especially useful for internet data because the distribution actually matters in this case. On flickr at least, there are a whole host of tags that are meaningless and come up all the time. A lot of them come about because people tend to turn autotagging on so things like , , tend to turn up. We can all agree those are useless. (On a side note, I learned that was an artform of iphone photography.) These will only be pushed away from image content based on sampling properties.\n\nAnother thing we experimented with is positive sampling on the tags themselves. That is, you\u2019ll want to consider the probability of occurrence for frequently occurring words (like or ) over stuff like . The distribution of words in YFCC100M follows a zipf distribution, and there is a balance that we haven\u2019t struck just yet for positive sampling. We just used uniform sampling for now. Please let us know if you\u2019ve tried something that works better!\n\nYou might\u2019ve heard that GPUs are limited in memory. In all actuality, they\u2019ve gotten pretty beastly in this respect. Also consider that with NVLink, memory sharing could fit tons of data. Still, tags in UGC can number in the tens of millions, especially without constraints on language or spelling errors. Without doing an excessive amount of coding, and if you\u2019re a poor researcher with a GeForce 700 series card, you may need an alternative.\n\nMikolov\u2019s work is all done on multi-threaded CPUs with good reason. The motherboard simply has way more memory. Secondly, he deals with only wide neural networks, which means that optimizing a single layer in parallel may be just fine. It\u2019s analogous to HOG-Wild, where you\u2019re just randomly optimizing columns. The difference is that the chance of writing over a word vector that\u2019s being updated simultaneously is fairly low, since the vocabulary is really large.\n\nMy takeaway here was to use GPUs for deep learning. Use CPUs for wide learning. And in our case where we\u2019re training against internet photos and tags, we\u2019ve got to use both GPUs and CPUs. That\u2019s because we need deep learning to deal with the complexity of images and wide learning to deal with a really large vocabulary. Incidentally, it\u2019s also a gimmick to use the buzz term deep and wide neural network in Tensorflow.\n\nSo we trained on flickr data\u2026lots of it. And then, we did some example image retrieval by querying random words and seeing what we came up with. Mind you, the vocabulary in YFCC100M is ginormous and you can choose any word you want to search for. (There was a few exceptions when we presented at GTC; someone yelled out \u201cBrexit\u201d, and since our data scrape was in 2015, it wasn\u2019t in our vocabulary.)\n\nOur search terms are on top, and the images returned are on the bottom. The original tags are also included, and you can see that most of them make no sense. It\u2019s why it\u2019s surprising that our approach worked at all! Just goes to show you that there\u2019s enough signal in open source content that training deep learning is possible. You can see more of these examples in our paper and if you download our code. We\u2019ll provide models if you are interested.\n\nThanks for reading this far. We\u2019re really excited about the implications of this work. This work was conducted at Lab41/IQT, though initially started at Lawrence Livermore National Laboratory. It\u2019s been presented at CASIS, GTC, is on ArXiv and submitted to CVPR. Most of it was done in under six months, but it was enough to serve a demo. You can access all of the Tensorflow code with the corresponding Docker Containers at the Lab41 Github Page. For code we\u2019d written to load in features easier, checkout our CVPR submission. For simple scripts that demonstrate the concept, you can peruse our anything2vec code. If you have any trouble, feel free to reach out to me via e-mail, twitter, and my personal website."
    },
    {
        "url": "https://gab41.lab41.org/feature-engineering-is-just-easier-1928d935ed17",
        "title": "Feature engineering is just easier \u2013",
        "text": "The staggering proliferation of deep learning architectures in the past few years is evidence for the maxim that architecture engineering is the new feature engineering. It used to be that every new advance in machine learning relied on some clever feat of feature engineering \u2014 a tweak taking the raw data and exposing its characteristics in some way that a learning algorithm could exploit. Everyone had access to the same few learning algorithms, so feature engineering was the easiest \u2014 and often the only \u2014 way to differentiate yourself from the pack.\n\nThe pendulum has, however, swung sharply away from hand-guided feature engineering. Convolutional neural networks, to take the best-known example, can learn feature transformations for all sorts of computer vision tasks (among others), given only a dataset and an objective function. Turns out, convolutional neural networks are a good fit for image in part because their performance is \u201ctranslation-invariant\u201d \u2014 i.e., they do the same thing no matter where in the input they are looking. This property has stood them in good stead on problem domains outside of vision, as well, leading to some rather bullish declamations:\n\nSimilarly, recurrent neural network architectures have been used to do feature learning for NLP and tackle hard problems like machine translation. These architectures reflect the nature of the problem and elegantly exploit the structure of the data, and this is one reason they work so well. More and more, the \u201cright\u201d way to tackle many machine learning problems is to define a network architecture and objective function that conforms with the problem and the shape of the data.\n\nHere\u2019s the (not so) dirty secret: this kind of \u201carchitecture engineering\u201d is hard \u2014 and by hard I mean expensive. Finding the \u201cright\u201d network for the job, especially if one doesn\u2019t exist yet (but make sure to try ResNets first!) is not, strictly speaking, an engineering effort. Fiddling with deep learning architectures is still definitely in the realm of research \u2014 there are few established best practices, the risk of failure is high, and personnel with the necessary expertise are rare and costly. Deep learning slays the competition in object detection, image captioning, and machine translation, but small deformations of these more commonly researched problems can make duds of even the dearest of deep learning darlings. And doubly worth noting given all the deep learning hype is that manual feature engineering still provides an edge in image retrieval and tagging, among a plethora of other tasks.\n\nIn two of Lab41\u2019s recent endeavors, D*Script and Pythia, we pursued machine learning solutions to real-world problems \u2014 identifying writers of handwritten documents and detecting novel documents in large corpora, respectively. In both projects, some combination of \u201chand-engineered\u201d features rose to the top of the heap, despite a lot of experimentation with deep learning-based approaches. In D*Script, I bet we could have found more competitive solutions using deep learning if we had spent more time looking, or if we had hired a whole team of recent NYU PhDs. But that would have been prohibitively expensive. And when you add in constraints like \u201cnot much data available,\u201d it becomes harder to say whether deep learning will ever be able to do the job for you.\n\nFeature engineering, on the other hand, is cheaper than ever. As Anna pointed out in her post on experiment logging with Sacred, firing off thousands of experiments testing different feature configurations is nearly trivial. And though we aren\u2019t half as clever as the many Kaggle winners whose hair-raising feature engineering exploits would make any machine learning enthusiast\u2019s heart skip a beat, calculating a time lag feature or rolling mean takes on the order of minutes, and assessing its usefulness scarcely longer. Devising, training, and testing an end-to-end deep learning framework takes a bit more time than that.\n\nSo what deep learning has introduced isn\u2019t so much the death of hand-tuned features, but instead a richer continuum along the risk-reward axis between feature engineering and feature learning. Hand-tuned features combined with versatile, robust learners like XGBoost are a reasonably low-cost effort that can often yield satisfactory results \u2014 and if they don\u2019t, who cares? In the upper stratosphere of academic and industrial machine learning, deep learning has almost entirely taken over, but it\u2019s no accident that the field is dominated by a few large companies, and almost everyone involved has a PhD from one of a handful of programs. It\u2019s still an expert\u2019s game \u2014 and these days it does make more sense to have the experts spend their time designing sensible network architectures instead of chasing down the One True Feature.\n\nArchitecture engineering is getting cheaper, too. Though there\u2019s still a long way to go, efforts such as Keras have done a lot to make deep learning more accessible and tinkerable. And it is only going to get easier. In the meantime, hand-engineering features for your problem isn\u2019t necessarily some rearguard action, undertaken on behalf of a desperate ancien regime that doesn\u2019t know anything else. Sometimes, feature engineering is just the correct, economical choice."
    }
]