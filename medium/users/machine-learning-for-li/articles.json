[
    {
        "url": "https://medium.com/machine-learning-for-li/install-caffe-a198c400f1cb?source=---------0",
        "title": "Install caffe \u2013 Machine Learning for Li \u2013",
        "text": "If we need to install a certain version of caffe, we need to compile it.\n\nThere could have two ways to install caffe: conda or without conda\n\nNot sure if the correct solution but seemingly works:\n\n add the second line in the Makefile:"
    },
    {
        "url": "https://medium.com/machine-learning-for-li/how-to-use-tensorflow-on-tacc-supercomputer-827e7aaa9992?source=---------1",
        "title": "How to use TensorFlow on TACC supercomputer \u2013 Machine Learning for Li \u2013",
        "text": "Then use the following command to check the status\n\nUsing the command with the and options can provide an estimate of when a particular job will be scheduled:\n\nEven more extensive job information can be found using the \u201c \" command. The output shows quite a bit about the job: job dependencies, submission time, number of codes, location of the job script and the working directory, etc. See the man page for more details.\n\nThe command is used to remove pending and running jobs from the queue. Include a space-separated list of job IDs that you want to cancel on the command-line:\n\nExample job scripts are available online in . They include details for launching large jobs, running multiple executables with different MPI stacks, executing hybrid applications, and other operations.\n\n4. Copy Model and Data, you may copy the test dir to other places\n\n7. Run the program there, use the screen, which means the data is saved to the $WORK\n\n8. We can use command to check the submitted queue\n\nThen we try to connect on the working nodelist\n\nIf we use top on each of the working node, it showes the same PID running. All the tasks submitted from the terminal will be working under $WORK directory, and that is where the result is saved.\n\nThus, we can copy the result from $WORK to HOME that we can visualize the data with jupyter notebook easily from the visualization portal tool. It would be even nicer if the $WORK directory is visible to all the users on the jupyter notebook too.\n\nWe can run program in the $WORK directory from jupyter notebook\n\nIf we are running deep neural network that takes hours or days to finish, we can just close the visualization portal website but do not hit the log out. The job will keep running till it is done.\n\nData transfer from any Linux system can be accomplished using the utility to copy data to and from the login node. A file can be copied from your local system to the remote server by using the command:\n\nNotice I used the same port number (6006) that I was told to connect to by the tensorboard. Also note I used the compute node host name (c224\u2013202) in that command. Your node will likely have a different name. Use whatever node you land on. \n\n \n\n I then pointed my browser to login2.maverick.tacc.utexas.edu:6006 and was able to connect.\n\n2. A easier solution is to go to TACC Visualization Portal to start a VNC .\n\nThen go to the application menu to choose browser:\n\nAnd then navigate to the given address:"
    },
    {
        "url": "https://medium.com/machine-learning-for-li/restricted-boltzmann-machines-rbms-d355c4b5ebfa?source=---------2",
        "title": "Restricted Boltzmann Machines (RBMs) \u2013 Machine Learning for Li \u2013",
        "text": "RBMs have been used as generative models of many different types of data include labeled and unlabeled. In their conditional form they can be used to model high-dimensional temporal sequences such as video or motion capture data (Taylor et al., 2006) or speech (Mohamed and Hinton, 2010). Their most important use is as learning modules that are composed to form deep belief nets (Hinton et al., 2006a).\n\nIn recent years Restricted Boltzmann Machines has attracted growing attention in the computer vision community. Restricted Boltzmann Machine and its variants have been used to many computer vision applications including object recognition, facial expression generation, human motion generation and activity recognition. The increased popularity of Restricted Boltzmann Machines for computer vision is due partly to their excellent ability in feature extraction.\n\nRestricted Boltzmann Machines have been exploited in many computer vision applications. The following subsections summarize these applications including object recognition, human motion generation, facial expression generation.\n\nObject recognition is one of the fundamental challenges in computer vision. Its objective is efficiently detecting and classifying objects in an image or video sequence into generic categories such as \u201canimals\u201d, \u201cvehicles\u201d, \u201cflowers\u201d, etc. Object recognition in images and videos is very challenging due to high intra-class variety and viewpoint variants.\n\nBRBM employs binary hidden and visible units, which is applicable to quasi-binary images (e.g., handwritten digits). The extension of RBMs such as the GBRBM, the mcRBM and the ssRBM are more suits to the continuous data. The characterization of object recognition using RBM has been a recent focus in the computer vision community as summarized in Table 3.1. and more please refer the following papers."
    },
    {
        "url": "https://medium.com/machine-learning-for-li/how-to-calculate-the-number-of-parameters-in-cnns-5aa08d0edd55?source=---------3",
        "title": "How to calculate the number of parameters in CNNs? \u2013 Machine Learning for Li \u2013",
        "text": "This post comes from https://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networks.\n\nIf you refer to VGG Net with 16-layer (table 1, column D) then refers to the total number of parameters of this network, i.e including all convolutional layers, but also the fully connected ones.\n\nLooking at the 3rd convolutional stage composed of 3 x layers:\n\nThe convolution kernel is 3x3 for each of these layers. In terms of parameters this gives:\n\nAs explained above you have to do that for all layers, but also the fully-connected ones, and sum these values to obtain the final 138M number.\n\nIn particular for the fully-connected layers (fc):\n\n(x) see section 3.2 of the article: the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 \u00d7 7 conv. layer, the last two FC layers to 1 \u00d7 1 conv. layers).\n\nAs precised above the spatial resolution right before feeding the fully-connected layers is 7x7 pixels. This is because this VGG Net uses spatial padding before convolutions, as detailed within section 2.1 of the paper:\n\n[\u2026] the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3\u00d73 conv. layers.\n\nWith such a padding, and working with a 224x224 pixels input image, the resolution decreases as follow along the layers: 112x112, 56x56, 28x28, 14x14 and 7x7 after the last convolution/pooling stage which has 512 feature maps.\n\nThis gives a feature vector passed to with dimension: 512x7x7."
    },
    {
        "url": "https://medium.com/machine-learning-for-li/a-walk-through-of-cost-functions-4767dff78f7?source=---------4",
        "title": "A Walk-through of Cost Functions \u2013 Machine Learning for Li \u2013",
        "text": "This is one of the simplest and most effective cost functions that we can use. It can also be called the quadratic cost function or sum of squared errors.\n\nThe title pretty much spells out the equation for us:\n\nWe can see from this that first the difference between our estimate of y and the true value of y is taken and squared. This square isn\u2019t there for no reason, as it allows are result to be quadratic.\n\nYou may know that a quadratic function when plotted will always have a sort of \u2018u\u2019 shape making it convex, like so:\n\nThis shows us that in the future when we need to use something like gradient descent, we won\u2019t run into the major problem of getting stuck in a local optimum.\n\nWe then sum each of the results and find the average.\n\nLets say we have the following dataset and want to predict the following label \u2018happiness_scale\u2018:\n\nWe run the features through our neural network (the specifics are unimportant for now), and we get the following estimates of our labels:\n\nThese estimates look pretty wrong to me, but how wrong exactly? Lets use the mean squared error to tell us how wrong our neural network actually is:\n\nThis can guide us in our gradient descent process which will eventually reduce the cost function to its minimum.\n\nThis means that our neural network would be able to accurately predict the answers if we give it the same data and hopefully predict them if we give it data it hasn\u2019t seen before.\n\nGradient descent isn\u2019t something I want to go into too much detail about today in terms of the mathematics and how it\u2019s performed, but I will in the near future in a different post.\n\nAs mentioned above, if you want to learn more about gradient descent then I have provided some resources at the end of this article!\n\nThis cost function originally stems from information theory with the transfer of bits and how much bits have been lost in the process.\n\nWe can define cross entropy as the difference between two probability distributions p and q, where p is our true output and q is our estimate of this true output.\n\nThis difference is now applied to our neural networks, where it is extremely effective because of their strong usage of probability.\n\nWe can see above that p is compared to log-q(x) which will find the distance between the two.\n\nCross entropy will work best when the data is normalized (forced between 0 and 1) as this will represent it as a probability. This normalization property is common in most cost functions.\n\nWe should also note another common cost function used that is very similar to cross entropy, called KL Divergence. In fact, it\u2019s pretty much a mutated cross entropy, and can also be referred to as relative entropy:\n\nThe KL divergence will still measure the difference between probability distributions p and q.\n\nHowever, the difference to note is that in information theory it focuses on the extra number of bits needed to encode the data.\n\nThis means that when applied to our data, the KL divergence will never be less than 0. It is only equal to 0 if p = q. Also note that the KL divergence is not a distance, whereas the cross entropy is.\n\nThe function is called the hinge loss function. It is equal to 0 when . Its derivative is if and 0 if . It is not differentiable at . but we can still use gradient descent using any subderivative at t=1."
    }
]