[
    {
        "url": "https://medium.com/paperspace/tutorial-on-implementing-yolo-v3-from-scratch-in-pytorch-part-1-a0054d38ec78?source=---------0",
        "title": "Tutorial on implementing YOLO v3 from scratch in PyTorch: Part 1",
        "text": "Tutorial on implementing YOLO v3 from scratch in PyTorch: Part 1\n\nObject detection is a domain that has benefited immensely from the recent developments in deep learning. Recent years have seen people develop many algorithms for object detection, some of which include YOLO, SSD, Mask RCNN and RetinaNet.\n\nFor the past few months, I\u2019ve been working on improving object detection at a research lab. One of the biggest takeaways from this experience has been realizing that the best way to go about learning object detection is to implement the algorithms by yourself, from scratch. This is exactly what we\u2019ll do in this tutorial.\n\nWe will use PyTorch to implement an object detector based on YOLO v3, one of the faster object detection algorithms out there.\n\nThe code for this tutorial is designed to run on Python 3.5, and PyTorch 0.3. It can be found in it\u2019s entirety at this Github repo.\n\nThis tutorial is broken into 5 parts:\n\nI\u2019ve provided the link at the end of the post in case you fall short on any front.\n\nYOLO stands for You Only Look Once. It\u2019s an object detector that uses features learned by a deep convolutional neural network to detect an object. Before we get out hands dirty with code, we must understand how YOLO works.\n\nYOLO makes use of only convolutional layers, making it a fully convolutional network (FCN). It has 75 convolutional layers, with skip connections and upsampling layers. No form of pooling is used, and a convolutional layer with stride 2 is used to downsample the feature maps. This helps in preventing loss of low-level features often attributed to pooling.\n\nBeing a FCN, YOLO is invariant to the size of the input image. However, in practice, we might want to stick to a constant input size due to various problems that only show their heads when we are implementing the algorithm.\n\nA big one amongst these problems is that if we want to process our images in batches (images in batches gain be processed in parallel by the GPU, leading to speed boosts), we need to have all images of fixed height and width. This is needed to concatenate multiple images into a large batch (concatenating many PyTorch tensors into one)\n\nThe network downsamples the image by a factor called the stride of the network. For example, if the stride of the network is 32, then an input image of size 416 x 416 will yield an output of size 13 x 13. Generally, stride of any layer in the network is equal to the factor by which the output of the layer is smaller than the input image to the network.\n\nTypically, (as is the case for all object detectors) the features learned by the convolutional layers are passed onto a classifier/regressor which makes the detection prediction (coordinates of the bounding boxes, the class label.. etc).\n\nIn YOLO, the prediction is done by using a convolutional layer (Duh\u2026it\u2019s a fully convolutional network, remember?) with a kernel size of\n\nNow, the first thing to notice is our output is a feature map. Since we have used 1 x 1 convolutions, the size of the prediction map is exactly the size of the feature map before it. In YOLO v3 (and it\u2019s descendants), the way you interpret this prediction map is that each cell can predict a fixed number of bounding boxes.\n\nDepth-wise, we have (B x (5 + C)) entries in the feature map. B represents the number of bounding boxes each cell can predict. According to the paper, each of these B bounding boxes may specialize in detecting a certain kind of object. Each of the bounding boxes have 5 + C attributes, which describe the center coordinates, the dimensions, the objectness score and C class confidences for each bounding box. YOLO v3 predicts 3 bounding boxes for every cell.\n\nYou expect each cell of the feature map to predict an object through one of it\u2019s bounding boxes if the center of the object falls in the receptive field of that cell. (Receptive field is the region of the input image visible to the cell. Refer to the link on convolutional neural networks for further clarification).\n\nThis has to do with how YOLO is trained, where only one bounding box is responsible for detecting any given object. First, we must ascertain which of the cell this bounding box belongs to.\n\nTo do that, we divide the input image into a grid of dimensions equal to that of the final feature map.\n\nLet us consider an example below, where the input image is 416 x 416, and stride of the network is 32. As pointed earlier, the dimensions of the feature map will be 13 x 13. We then divide the input image into 13 x 13 cells.\n\nThen, the cell (on the input image) containing the center of the ground truth box of an object is chosen to be the one responsible for predicting the object. In the image, it is the cell which marked red, which contains the center of the ground truth box (marked yellow).\n\nNow, the red cell is the 7th cell in the 7th row on the grid. We now assign the 7th cell in the 7th row on the feature map (corresponding cell on the feature map) as the one responsible for detecting the dog.\n\nNow, this cell can predict three bounding boxes. Which one will be assigned to the dog\u2019s ground truth label? In order to understand that, we must get wrap out head around the concept of anchors.\n\nIt might make sense to predict the width and the height of the bounding box, but in practice, that leads to unstable gradients during training. Instead, most of the modern object detectors predict log-space transforms, or simply offsets to pre-defined default bounding boxes called anchors.\n\nThen, these transforms are applied to the anchor boxes to obtain the prediction. YOLO v3 has three anchors, which result in prediction of three bounding boxes per cell.\n\nComing back to our earlier question, the bounding box responsible for detecting the dog will be the one whose anchor has the highest IoU with the ground truth box.\n\nThe following formulae describe how the network output is transformed to obtain bounding box predictions.\n\nNotice we are running our center coordinates prediction through a sigmoid function. This forces the value of the output to be between 0 and 1. Why should this be the case? Bear with me.\n\nNormally, YOLO doesn\u2019t predict the absolute coordinates of the bounding box\u2019s center. It predicts offsets which are:\n\nFor example, consider the case of our dog image. If the prediction for center is (0.4, 0.7), then this means that the center lies at (6.4, 6.7) on the 13 x 13 feature map. (Since the top-left co-ordinates of the red cell are (6,6)).\n\nBut wait, what happens if the predicted x,y co-ordinates are greater than one, say (1.2, 0.7). This means center lies at (7.2, 6.7). Notice the center now lies in cell just right to our red cell, or the 8th cell in the 7th row. This breaks theory behind YOLO because if we postulate that the red box is responsible for predicting the dog, the center of the dog must lie in the red cell, and not in the one beside it.\n\nTherefore, to remedy this problem, the output is passed through a sigmoid function, which squashes the output in a range from 0 to 1, effectively keeping the center in the grid which is predicting.\n\nThe dimensions of the bounding box are predicted by applying a log-space transform to the output and then multiplying with an anchor.\n\nHow the detector output is transformed to give the final prediction. Image Credits. http://christopher5106.github.io/\n\nThe resultant predictions, bw and bh, are normalised by the height and width of the image. (Training labels are chosen this way). So, if the predictions bx and by for the box containing the dog are (0.3, 0.8), then the actual width and height on 13 x 13 feature map is (13 x 0.3, 13 x 0.8).\n\nObject score represents the probability that an object is contained inside a bounding box. It should be nearly 1 for the red and the neighboring grids, whereas almost 0 for, say, the grid at the corners.\n\nThe objectness score is also passed through a sigmoid, as it is to be interpreted as a probability.\n\nClass confidences represent the probabilities of the detected object belonging to a particular class (Dog, cat, banana, car etc). Before v3, YOLO used to softmax the class scores.\n\nHowever, that design choice has been dropped in v3, and authors have opted for using sigmoid instead. The reason is that Softmaxing class scores assume that the classes are mutually exclusive. In simple words, if an object belongs to one class, then it\u2019s guaranteed it cannot belong to another class. This is true for COCO database on which we will base our detector.\n\nHowever, this assumptions may not hold when we have classes like Women and Person. This is the reason that authors have steered clear of using a Softmax activation.\n\nYOLO v3 makes prediction across 3 different scales. The detection layer is used make detection at feature maps of three different sizes, having strides 32, 16, 8 respectively. This means, with an input of 416 x 416, we make detections on scales 13 x 13, 26 x 26 and 52 x 52.\n\nThe network downsamples the input image until the first detection layer, where a detection is made using feature maps of a layer with stride 32. Further, layers are upsampled by a factor of 2 and concatenated with feature maps of a previous layers having identical feature map sizes. Another detection is now made at layer with stride 16. The same upsampling procedure is repeated, and a final detection is made at the layer of stride 8.\n\nAt each scale, each cell predicts 3 bounding boxes using 3 anchors, making the total number of anchors used 9. (The anchors are different for different scales)\n\nThe authors report that this helps YOLO v3 get better at detecting small objects, a frequent complaint with the earlier versions of YOLO. Upsampling can help the network learn fine-grained features which are instrumental for detecting small objects.\n\nFor an image of size 416 x 416, YOLO predicts ((52 x 52) + (26 x 26) + 13 x 13)) x 3 = 10647 bounding boxes. However, in case of our image, there\u2019s only one object, a dog. How do we reduce the detections from 10647 to 1?\n\nThresholding by Object Confidence\n\nFirst, we filter boxes based on their objectness score. Generally, boxes having scores below a threshold are ignored.\n\nNon-maximum Suppression\n\nNMS intends to cure the problem of multiple detections of the same image. For example, all the 3 bounding boxes of the red grid cell may detect a box or the adjacent cells may detect the same object.\n\nIf you don\u2019t know about NMS, I\u2019ve provided a link to a website explaining the same.\n\nYOLO can only detect objects belonging to classes present in the dataset used to train the network. We will be using the official weight file for our detector. These weights have been obtained by training the network on COCO dataset, and therefore we can detect 80 object categories.\n\nThat\u2019s it for the first part. This post explains enough about the YOLO algorithm to enable you to implement the detector. However, if you want to dig deep into how YOLO works, how it\u2019s trained and how it performs compared to other detectors, you can read the original papers, the links of which I\u2019ve provided below.\n\nThat\u2019s it for this part. In the next part, we implement various layers required to put together the detector.\n\nAyoosh Kathuria is currently an intern at the Defense Research and Development Organization, India, where he is working on improving object detection in grainy videos. When he\u2019s not working, he\u2019s either sleeping or playing pink floyd on his guitar. You can find find connect with him on LinkedIn or look at more of what he does at GitHub"
    },
    {
        "url": "https://medium.com/paperspace/hands-on-with-the-google-tpuv2-9feb0c5ea952?source=---------1",
        "title": "Hands-on with the Google TPUv2 \u2013 Paperspace \u2013",
        "text": "Over the last year or so we have seen a lot of new developments in the AI chip space. Companies like Intel, Graphcore, Cerebras, Vathys, and more have been racing to develop newer hardware architectures that promise to take the general compute model of a GPU and build it specifically for the task of training and deploying deep learning models.\n\nOver the last few weeks we have been working to integrate the new TPUv2 into our suite of Machine learning and AI tools called Gradient\u00b0. Along the way we have learned a lot about this new hardware, and more importantly, what the future of hardware for Machine Learning looks like. Let\u2019s dive in.\n\nThe TPU is the first of these new chips to become publicly available and we are just starting to see the real-world performance benchmarks. The team at RiseML recently published a benchmark comparing the TPU to the NVIDIA P100 and V100 chipsets and the first results are exciting to say the least.\n\nThe TPUv2 differs from a traditional GPU in a few ways. Each chip actually combines 4 TPU chips each with 16GB of high-performance memory. The TPUv2 is capable of addressing up to 2,400GB/s memory bandwidth and like a GPU, it is designed to be connected to multiple TPU nodes for even faster parallel compute.\n\nThat said, there are a few key limitations worth noting:\n\nThere are some key differences in how machine learning tasks use storage when running on a GPU verses a TPU. For traditional GPU-backed training a GPU has to rely on a host process running on the CPU to pull data from a local filesystem or other storage source into the CPU\u2019s memory, and transfer that memory to the GPU for further processing. We rely on nvidia-docker containers to manage hardware dependencies and provide local storage access for GPUs. Additionally, underlying each Gradient GPU job is a shared persistent data store that is automatically mounted to , which can also be used as the main storage source for GPU-based training.\n\nIn deep learning tasks IO can quickly become a bottleneck and we have heavily optimized the Gradient storage architecture to be able to fully saturate a GPU training job with multiple tasks hitting a single data source.\n\nThe TPU storage access is quite different than a GPU. TPUs currently leverage the Distributed Tensorflow framework to pull and push dataflows for training sessions. This framework supports several different file system plugins, but Google has chosen Google Cloud Storage as the only supported TPU storage model currently.\n\nOn a TPU it is considered sub-optimal to run any code on the \u201ccompute node\u201d \u2014 ideally you get as much of your TensorFlow code as possible compiled to executable units using the XLA (accelerated linear algebra) architecture. This requires a slight rework of most existing code to use TPU-specific versions of some of the core Tensorflow algorithms. In turn, these TPU-specific routines require use of data sources and sinks in Google Cloud Storage to operate.\n\nFor Gradient access to TPUs we automatically provide a block of Google cloud storage to your machine learning jobs. In a TPU tensorflow job the input dataflow location is typically referred to as the and the output location is the . At the completion of the training job the TPU outputs are automatically loaded into a web-accessible directory for download and browsing.\n\nAnother major difference between TPUs and GPUs is that TPUs are provided as network addressable devices, as opposed GPUs which are typically locally attached hardware devices on a PCI bus. TPUs listen on an IP address and port for XLA instructions and operations to perform.\n\nIn Google\u2019s current TPU deployment model these network devices reside in a Google-defined TPU project and network separate from your personal or corporate Google cloud resources. The TPU projects and networks are specific to your account, but you do not have direct access to them through Google\u2019s console or APIs.\n\nTo make use of a TPU in this separate environment you must first establish various trust relationships between your network resources and the Google managed TPU project and network, including setting up VM instances with specific service account roles, and granting TPU access to Google cloud storage buckets. Gradient\u2019s job runner simplifies this by managing all of these relationships automatically, thus obviating the need to do a lot of complex manual setup and configuration.\n\nTPUs also have human readable names in Google\u2019s cloud, similar to DNS names. Google provides a basic name resolution library for translating a TPU name into a network address and port (actually a TPU gRPC URL i.e. ), so that your code does not have to hard-code this information.\n\nThere is a version of the library included with TensorFlow 1.6, but we have already seen some breaking changes to it appear in TensorFlow 1.7. We\u2019ve also found that the TPU name resolution library requires higher levels of permissions in order to access required Google Cloud APIs and function correctly.\n\nAs we have been working to integrate the TPU in to Gradient, we have attempted to smooth over these issues by pre-resolving the TPU names and providing a usable as an environment variable within the TensorFlow container. Gradient also provides some supplemental docker images that have the required TPU name resolution dependencies pre-installed for users who want to exercise those capabilities.\n\nThe TPU is weird in the sense that it requires a change in how you think about traditional deep learning tasks. Currently the TPU only supports TensorFlow code, and there are a few requirements for porting existing code to the new architecture.\n\nAt a high level, the official TPU documentation recommends using the new class which abstracts away some of the implementation details of running on a TPU. For most TensorFlow developers, this is a well-known best practice.\n\nYou can check out more here: https://www.tensorflow.org/programmers_guide/using_tpu\n\nWith Gradient, you can easily try out the TPU today (subject to demand and availability)!\n\nAfter you have created an account at www.paperspace.com/account/signup and installed our CLI you now train a model on a TPU in the cloud.\n\nFirst, clone the sample repo at https://github.com/Paperspace/tpu-test and in to the new directory.\n\nInitialize a project namespace using . Now with a single line of code you can submit the project to a TPU running on Gradient.\n\nIf you go to your console you will see the job and the logs that are generated:\n\nThat\u2019s it! You\u2019ve run a TPU job. Want try a more advanced model? Checkout our repo for training Resnet-50 on a TPU\n\nNote: The ResNet-50 example is potentially a long-running job, and can take up to 20 hours to complete.\n\nWe are entering a Golden Era of cloud machine learning. As a company focused on making production Machine Learning technology available to all developers, we are incredibly excited by the explosion of new hardware accelerators and what they might mean for the end user.\n\nWhile the GPU is not going anywhere, the TPU is the first real competitor and it points to a future where a developers will have their choice of hardware and software.\n\nTry a TPU today on Gradient, built for Machine Learning and AI developers."
    },
    {
        "url": "https://medium.com/paperspace/what-every-ml-ai-developer-should-know-about-onnx-efce36828ad6?source=---------2",
        "title": "What every ML/AI developer should know about ONNX \u2013 Paperspace \u2013",
        "text": "The end result of a trained deep learning algorithm is a model file that efficiently represents the relationship between input data and output predictions. A neural network is one of the most powerful ways to generate these predictive models but can be difficult to build in to production systems. Most often, these models exist in a data format such as a file or an HD5 file. Oftentimes you want these models to be portable so that you can deploy them in environments that might be different than where you initially trained the model.\n\nAt a high level, ONNX is designed to allow framework interoporability. There are many excellent machine learning libraries in various languages \u2014 PyTorch, TensorFlow, MXNet, and Caffe are just a few that have become very popular in recent years, but there are many others as well.\n\nThe idea is that you can train a model with one tool stack and then deploy it using another for inference and prediction. To ensure this interoperability you must export your model in the format which is serialized representation of the model in a protobuf file. Currently there is native support in ONNX for PyTorch, CNTK, MXNet, and Caffe2 but there are also converters for TensorFlow and CoreML.\n\nLet\u2019s imagine that you want to train a model to predict if a food item in your refrigerator is still good to eat. You decide to run a a bunch of photos of food that is at various stages past its expiration date and pass it in to a convolutional neural network (CNN) that looks at images of food and trains it to predict if the food is still edible.\n\nOnce you have trained your model, you then want to deploy it to a new iOS app so that anyone can use your pre-trained model to check their own food for safety. You initially trained your model using PyTorch but iOS expects to use CoreML to be used inside the app. ONNX is an intermediary representation of your model that lets you easily go from one environment to the next.\n\nUsing PyTorch you would normally export your model using Exporting to the ONNX interchange format is just one more line:\n\nUsing a tool like ONNX-CoreML, you can now easily turn your pre-trained model in to a file that you can import in to XCode and integrate seamlessly with your app. For a working example, checkout this excellent post by Stefano Attardi on building a ML-driven iOS app from start to finish.\n\nAs more and more deep learning frameworks emerge and workflows become more advanced, the need for portability is more important than ever. ONNX is a powerful and open standard for preventing framework lock-in and ensuring that you the models you develop will be usable in the long run."
    },
    {
        "url": "https://medium.com/paperspace/introducing-gradient-9f1c14f2c124?source=---------3",
        "title": "Introducing Gradient\u00b0 \u2013 Paperspace \u2013",
        "text": "When we started building Paperspace we knew that GPU\u2019s were going to unlock an entirely new universe of possibilites for what cloud computing would be able to do. Over the last couple years we have seen an explosion of software and tools built specifically for parallel compute, and with that a whole new set of workflows.\n\nIt is an understatement to say that the rise of deep learning and AI has opened up new frontiers for not only cloud computing, but for the entire world. New businesses are being created every day where modern deep learning is at the core of the business model, and for existing businesses in every industry, deep learning is becoming an increasingly important part of future strategy.\n\nA year ago we announced full support for machine learning, AI, and data science with the introduction of our powerful dedicated GPU instances and our Machine-Learning-in-a-Box template. Tens of thousands of users have used these tools to build things from genomics to cancer detection, to self-driving cars.\n\nToday we are excited to announce our biggest and most ambitious product to date \u2014 Gradient\u00b0.\n\nGradient\u00b0 is a suite of tools designed to accelerate cloud AI and machine learning. It includes a powerful job runner (that can even run on the new Google TPUs!), first-class support for containers and Jupyter notebooks, and a new set of language integrations.\n\nWith our emphasis on language integration, you can seamlessly run your existing python projects on the most advanced GPU infrastructure without having to ever leave your code.\n\nWe are excited to see what you build! Sign up today at www.paperspace.com/gradient\n\nRead what people are saying:\n\nTechCrunch: Paperspace goes serverless to simplify AI deployment in the cloud\n\nSiliconAngle: Paperspace launches Gradient, a new service for training AI models in the cloud\n\nChannelBuzz: Paperspace launches Gradient suite to bring GPU Compute and machine learning to developers\n\nCRN: Paperspace Introduces Toolkit For AI Developers Looking To Provision GPUs In The Cloud\n\nSDTimes: Gradient Launches To Fuel Next Generation Of Cloud AI"
    },
    {
        "url": "https://medium.com/paperspace/adversarial-autoencoders-with-pytorch-8375a04c4dac?source=---------4",
        "title": "Adversarial Autoencoders (with Pytorch) \u2013 Paperspace \u2013",
        "text": "Director of AI Research at Facebook, Professor Yann LeCunn repeatedly mentions this analogy at his talks. By unsupervised learning, he refers to the \u201cability of a machine to model the environment, predict possible futures and understand how the world works by observing it and acting in it.\u201d\n\nDeep generative models are one of the techniques that attempt to solve the problem of unsupervised learning in machine learning. In this framework, a machine learning system is required to discover hidden structure within unlabelled data. Deep generative models have many widespread applications, density estimation, image/audio denoising, compression, scene understanding, representation learning and semi-supervised classification amongst many others.\n\nVariational Autoencoders (VAEs) allow us to formalize this problem in the framework of probabilistic graphical models where we are maximizing a lower bound on the log likelihood of the data. In this post we will look at a recently developed architecture, Adversarial Autoencoders, which are inspired in VAEs, but give us more flexibility in how we map our data into a latent dimension (if this is not clear as of now, don\u2019t worry, we will revisit this idea along the post). One of the most interesting ideas about Adversarial Autoencoders is how to impose a prior distribution to the output of a neural network by using adversarial learning.\n\nIf you want to get your hands into the Pytorch code, feel free to visit the GitHub repo. Along the post we will cover some background on denoising autoencoders and Variational Autoencoders first to then jump to Adversarial Autoencoders, a Pytorch implementation, the training procedure followed and some experiments regarding disentanglement and semi-supervised learning using the MNIST dataset.\n\nThe simplest version of an autoencoder is one in which we train a network to reconstruct its input. In other words, we would like the network to somehow learn the identity function $f(x) = x$. For this problem not to be trivial, we impose the condition to the network to go through an intermediate layer (latent space) whose dimensionality is much lower than the dimensionality of the input. With this bottleneck condition, the network has to compress the input information. The network is therefore divided in two pieces, the encoder receives the input and creates a latent or hidden representation of it, and the decoder takes this intermediate representation and tries to reconstruct the input. The loss of an autoencoder is called reconstruction loss, and can be defined simply as the squared error between the input and generated samples:\n\nAnother widely used reconstruction loss for the case when the input is normalized to be in the range [0,1]^N is the cross-entropy loss.\n\nVariational autoencoders impose a second constraint on how to construct the hidden representation. Now the latent code has a prior distribution defined by design p(x). In other words, the encoder can not use the entire latent space freely but has to restrict the hidden codes produced to be likely under this prior distribution p(x). For instance, if the prior distribution on the latent code is a Gaussian distribution with mean 0 and standard deviation 1, then generating a latent code with value 1000 should be really unlikely.\n\nThis can be seen as a second type of regularization on the amount of information that can be stored in the latent code. The benefit of this relies on the fact that now we can use the system as a generative model. To create a new sample that comes from the data distribution p(x), we just have to sample from p(z) and run this sample through the decoder to reconstruct a new image. If this condition is not imposed, then the latent code can be distributed among the latent space freely and therefore is not possible to sample a valid latent code to produce an output in a straightforward manner.\n\nIn order to enforce this property a second term is added to the loss function in the form of a Kullback-Liebler (KL) divergence between the distribution created by the encoder and the prior distribution. Since VAE is based in a probabilistic interpretation, the reconstruction loss used is the cross-entropy loss mentioned earlier. Putting this together we have,\n\nWhere $q(z|x)$ is the encoder of our network and $p(z)$ is the prior distribution imposed on the latent code. Now this architecture can be jointly trained using backpropagation.\n\nOne of the main drawbacks of variational autoencoders is that the integral of the KL divergence term does not have a closed form analytical solution except for a handful of distributions. Furthermore, it is not straightforward to use discrete distributions for the latent code z. This is because backpropagation through discrete variables is generally not possible, making the model difficult to train efficiently. One approach to do this in the VAE setting was introduced here.\n\nAdversarial autoencoders avoid using the KL divergence altogether by using adversarial learning. In this architecture, a new network is trained to discriminatively predict whether a sample comes from the hidden code of the autoencoder or from the prior distribution p(z) determined by the user. The loss of the encoder is now composed by the reconstruction loss plus the loss given by the discriminator network.\n\nThe image shows schematically how AAEs work when we use a Gaussian prior for the latent code (although the approach is generic and can use any distribution). The top row is equivalent to an VAE. First a sample z is drawn according to the generator network q(z|x), that sample is then sent to the decoder which generates x\u2019 from z. The reconstruction loss is computed between x and x\u2019 and the gradient is backpropagated through p and q accordingly and its weights are updated.\n\nFigure 1. Basic architecture of an AAE. Top row is an autoencoder while the bottom row is an adversarial network which forces the output to the encoder to follow the distribution p(z).\n\nOn the adversarial regularization part the discriminator recieves z distributed as q(z|x) and z\u2019 sampled from the true prior p(z) and assigns a probability to each of coming from p(z). The loss incurred is backpropagated through the discriminator to update its weights. Then the process is repeated and the generator updates its parameters.\n\nWe can now use the loss incurred by the generator of the adversarial network (which is the encoder of the autoencoder) instead of a KL divergence for it to learn how to produce samples according to the distribution p(z). This modification allows us to use a broader set of distributions as priors for the latent code.\n\nThe loss of the discriminator is\n\nwhere m is the minibatch size, z is generated by the encoder and z\u2019 is a sample from the true prior.\n\nFor the adversarial generator we have\n\nBy looking at the equations and the plots you should convince yourself that the loss defined this way will enforce the discriminator to be able to recognize fake samples while will push the generator to fool the discriminator.\n\nBefore getting into the training procedure used for this model, we look at how to implement what we have up to now in Pytorch. For the encoder, decoder and discriminator networks we will use simple feed forward neural networks with three 1000 hidden state layers with ReLU nonlinear functions and dropout with probability 0.2.\n\nSome things to note from this definitions. First, since the output of the encoder has to follow a Gaussian distribution, we do not use any nonlinearities at its last layer. The output of the decoder has a sigmoid nonlinearity, this is because we are using the inputs normalized in a way in which their values are within 0 and 1. The output of the discriminator network is just one number between 0 and 1 representing the probability of the input coming from the true prior distribution.\n\nOnce the networks classes are defined, we create an instance of each one and define the optimizers to be used. In order to have independence in the optimization procedure for the encoder (which is as well the generator of the adversarial network) we define two optimizers for this part of the network as follows:\n\nThe training procedure for this architecture for each minibatch is performed as follows:\n\n1) Do a forward path through the encoder/decoder part, compute the reconstruction loss and update the parameteres of the encoder Q and decoder P networks.\n\n2) Create a latent representation z = Q(x) and take a sample z\u2019 from the prior p(z), run each one through the discriminator and compute the score assigned to each (D(z) and D(z\u2019)).\n\n3) Compute the loss in the discriminator as and backpropagate it through the discriminator network to update its weights. In code,\n\n4) Compute the loss of the generator network and update Q network accordingly.\n\nNow we attempt to visualize at how the AAE encodes images into a 2-D Gaussian latent representation with standard deviation 5. For this we first train the model with a 2-D hidden state. Then we generate uniform points on this latent space from (-10,-10) (upper left corner) to (10,10) (bottom right corner) and run them to through the decoder network.\n\nAn ideal intermediate representation of the data would be one that captures the underlying causal factors of variation that generated the observed data. Yoshua Bengio and his colleagues note in this paper that \u201cwe would like our representations to disentangle the factors of variation. Different explanatory factors of the data tend to change independently of each other in the input distribution\u201d. They also mention \u201cthat the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical\u201d.\n\nIn the case of MNIST data (which is a large dataset of handwritten digits), we can define two underlying causal factors, on the one hand, we have the number being generated, and on the other one the style or way of writing it.\n\nIn this part, we go one step further than the previous architecture and try to impose certain structure in the latent code z. Particularly we want the architecture to be able to separate the class information from the trace style in a fully supervised scenario. To do so, we extend the previous architecture to the one in the figure below. We split the latent dimension in two parts: the first one z is analogous as the one we had in the previous example; the second part of the hidden code is now a one-hot vector y indicating the identity of the number being fed to the autoencoder.\n\nIn this setting, the decoder uses the one-hot vector y and the hidden code z to reconstruct the original image. The encoder is left with the task of encoding the style information in z. In the image below we can see the result of training this architecture with 10,000 labeled MNIST samples. The figure shows the reconstructed images in which, for each row, the hidden code z is fixed to a particular value and the class label y ranges from 0 to 9. Effectively the style is maintained across the columns.\n\nAs our last experiment we look an alternative to obtain similar disentanglement results for the case in which we have only few samples of labeled information. We can modify the previous architecture so that the AAE produces a latent code composed by the concatenation of a vector y indicating the class or label (using a Softmax) and a continuous latent variable z (using a linear layer). Since we want the vector y to behave as a one-hot vector, we impose it to follow a Categorical distribution by using a second adversarial network with discriminator Dcat. The encoder now is q(z,y|x). The decoder uses both the class label and the continuous hidden code to reconstruct the image.\n\nThe unlabeled data contributes to the training procedure by improving the way the encoder creates the hidden code based on the reconstruction loss and improving the generator and discriminator networks for which no labeled information is needed.\n\nIt is worth noticing that now, not only we can generate images with fewer labeled information, but also we can classify the images for which we do not have labels by looking at the latent code y and picking the one with the highest value. With the current setting, the classification loss is about 3% using 100 labeled samples and 47,000 unlabeled ones.\n\nLastly, we do a short comparison in training time for this last algorithm in two different GPUs and CPU in Paperspace platform. Even though this architecture is not highly complicated and it is composed by few linear layers, the improvement in training time is enormous when making use of GPU acceleration. 500 epochs training time goes down from almost 4 hours in CPU to around 9 minutes using the Nvidia Quadro M4000 and further down to 6 minutes in the Nvidia Quadro P5000.\n\nTo try today on a cloud GPU, sign up here."
    }
]