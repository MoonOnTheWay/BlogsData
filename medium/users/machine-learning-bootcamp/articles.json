[
    {
        "url": "https://medium.com/machine-learning-bootcamp/curated-list-of-100-data-science-resources-for-practitioners-3c0510ed47a3?source=---------0",
        "title": "Curated List of 100+ Data Science resources for practitioners",
        "text": "Walking in dark might be fun sometimes but not when we have an agenda in mind. So, it is better to be hawk-eyed on the list of resources mentioned below and I shall keep updating it every now and then. Currently this list is jumbled up but I shall very soon segregate it into Categories, as it has various top-rated blogs/authors, ML/AI expert papers and links to terabytes of data repositories. Let\u2019s now embrace all that we have been gifted with till 2017:\n\nIf any of this helps you, please try to drop in a Thank You note to respective authors on their blogs, if feasible. And, a few claps out here from you can keep me boosted to regularly update this post. Have a wonderful new year ahead!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/demystifying-information-theory-e21f3af09455?source=---------1",
        "title": "Part 2: Information Theory | Statistics for Deep Learning",
        "text": "Information is an ordered symbol of sequences to interpret its meaning. Basic idea is quantification and communication of information in the form of an Entropy (discussed below).\n\nSelf-information: It is defined as a measure of information associated to a single value of discrete random variable. It is also called as the reciprocal of occurrence of an event. Let\u2019s better understand it with an example:\n\nMessage 1 \u2014 \u201cSun rises in the east.\u201d This is a highly certain event and has highest occurrence. So, the amount of information obtained in this case is zero or less.\n\nMessage 2 \u2014 \u201cIt is going to rain in the desert tomorrow\u201d. This is highly uncertain event and occurrence is low compared to the first message. So, this gives us more information compared to the previous one.\n\nSelf-information I is given by :\n\nwhere, X(i) has x1, x2, \u2026.. as a set of random events, P(X(i)) is Probability of occurrence of an event, & I (X(i)) is the amount of information obtained from the occurrences of above events.\n\nFrom above formula, we can conclude that the information content only depends on probability p(Xi) and not on actual value. Self-information is expressed as a unit of information in bits/shannons, nats or hartleys (depending on base of logarithm). This only deals with single outcome."
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/setting-up-gpu-powered-google-cloud-vm-deep-learning-7e67ec5e40da?source=---------2",
        "title": "Setting up GPU powered Google Cloud VM | Deep Learning",
        "text": "Most of us are not privileged enough to have a GPU powered machine to train our Deep Neural networks on. And this is where cloud hosted Virtual Machine (VM) gets into picture. Recently I came across an article that reflected on lack of resources for guiding beginners to access GPU powered Google cloud services. So my sole purpose in this blog is to demonstrate setting up a GPU powered VM on Google cloud platform & not training a model on top of it.\n\nThere couldn\u2019t have been a better day for drafting this post, as today Google has announced 36% price reduction on using NVIDIA\u2019s Tesla GPUs. Before we proceed, it is intrinsic to note that we shall be billed on per-second basis for this deployment. In U.S. regions, generally available Tesla K80 GPUs will cost $0.45 per hour while using the newer and more powerful beta Tesla P100 machines will cost $1.46 per hour.\n\nI will constantly keep adding screenshots for people who are not familiar with Google Cloud platform (GCP). The very first time we create our account in GCP, it allocates a credit of $300.00 for free trial and is valid for an year. Please be careful with your choices in the portal because the moment the meter goes beyond the free trial, your credit/debit card will start getting billed.\n\nAnother associated good news is that Google has also reduced the price for preemptible local SSDs by almost 40 percent. Enough of background & introduction, so now let us get started to feel the thrill. Our very first step would be to create a VM and your console should look like:\n\nOnce we click on \u2018Create\u2019, console will populate multiple hardware & networking parameters for our instance to choose from. This completely depends upon our requirement and will amount to our billing invoice so please do not get carried away while selecting hardware options. Please do remember to keep your CPU platform as Intel Haswell or later. Following screenshots showcase the options (can be extended) available:\n\nOnce the instance is ready, Internal and External IP Addresses shall be displayed for your use. Apart from your VM Console, these IP Addresses shall also be available in \u2018Networking\u2019 tab, like:\n\nAnd, we have a GPU powered Virtual Machine up and running. But since we are just experimenting with GPU-enabled machines, we shall set the scale tier to with a single NVIDIA Tesla K80 GPU. Our next step is to train our model that can be done using Cloud Machine Learning Engine. In case too many beginners still face issue in training a model on top of this VM, then please do keep me posted in Comments section and I will add that segment as well to this blog. Till then, enjoy machine learning!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/introduction-to-deep-learning-part-1-cbfda681c5ff?source=---------3",
        "title": "Introduction to Deep Learning \u2013 Deep Learning \u2013",
        "text": "Actual challenge to artificial intelligence was solving the tasks that are easy for people to perform but hard for people to describe formally. Solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs. The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones. Hence. this approach is termed as Deep Learning.\n\nA person\u2019s everyday life requires huge amount of knowledge about the world. Much of this knowledge is subjective and intuitive, and therefore difficult to articulate in a formal way. Computers need to capture this knowledge in order to behave in an intelligent way. One of the key challenges in artificial intelligence is how to get this informal knowledge into a computer. People struggle to devise formal rules with enough complexity to accurately describe the everyday world. The difficulties faced by systems relying on hard-coded knowledge suggest that AI systems need ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as Machine Learning.\n\nIntroduction of machine learning enabled computers to tackle problems involving knowledge of the real world and make decisions that appear subjective. The performance of simple machine learning algorithms depend heavily on the representation of data they are fed. This dependence on representations is a general phenomenon that appears throughout computer science and even daily life. Many artificial intelligence tasks can be solved by designing the right set of features to extract for that task, then providing these features to a simple machine learning algorithm, however, it is difficult to know what features should be extracted.\n\nOne solution to this problem is to use machine learning to discover not only the mapping from representation to output but also the representation itself. This approach is known as Representation Learning. Learned representations often result in much better performance than can be obtained with hand-designed representations. They also enable AI systems to rapidly adapt to new tasks, with minimal human intervention. A representation learning algorithm can discover a good set of features for a simple task in minutes, or for a complex task in hours to months.\n\nA brilliant example of a representation learning algorithm is an Autoencoder. An Autoencoder is the combination of an Encoder function, which converts input data into a different representation, and a Decoder function, which converts this new representation back into the original format. Autoencoders are trained to preserve as much information as possible when an input is run through the Encoder and then the Decoder, but they are also trained to make the new representation have various nice properties. Different kinds of Autoencoders aim to achieve different kinds of properties.\n\nWhen designing features or algorithms for learning features, our goal is to separate the factors of variation that explain the observed data. These factors indicate separate influencing sources & are not combined by multiplication. Either they are unobserved objects/forces in the physical world that affect observable quantities or constructs in human mind providing simplified explanations or inferred causes of the observed data. They are concepts or abstractions that help us make sense of the rich variability in the data.\n\nDeep learning solves this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations as it enables the computer to build complex concepts out of simpler concepts. Quintessential example of a deep learning model is the feed forward deep network, or multilayer perceptron (MLP). A multilayer perceptron is just a mathematical function formed by combining many simpler functions to map some set of input values to output values. Each application of a different mathematical function provides a new representation of the input.\n\nApart from learning right representation from data, another aspect is depth that enables computer to learn a multi-step program. Each layer of a representation is a state of the computer\u2019s memory after simultaneously executing another set of instructions & that empowers networks with greater depth to execute more instructions in sequence. Later instructions can refer back to the results of prior instructions, so all the information in a layer\u2019s activation don\u2019t necessarily encode factors of variation that explain the input. Representation also stores state information that helps to execute a program that can make sense of the input and keep model processing organized.\n\nDepth of a model can be viewed either based on number of sequential instructions (depth of Computational graph) OR based on correlation of concepts with each other (depth of Probabilistic modeling graph). Neither there is a single correct value for the depth of an architecture, nor is there a consensus about how much depth a model requires to qualify as \u2018deep\u2019. However, Deep Learning can be safely regarded as the study of models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does.\n\nEdit: Thanks to Jeff Clune who reminded me that I haven\u2019t credited original authors in here. This article is pretty much a subset of my learning from a recently (late 2017) published book Deep Learning (Adaptive Computation and Machine Learning series). This book has been one of the best investments that I have made recently and is highly recommended for beginner/professionals who wish to understand each and every concept of Deep Learning. It has been authored by few of the living legends in this domain, namely Yoshua Bengio, Ian Goodfellow and Aaron Courville; and published by MIT Press."
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/swish-activation-function-by-google-53e1ea86f820?source=---------4",
        "title": "Swish Activation Function by Google \u2013 Deep Learning \u2013",
        "text": "The choice of activation functions in Deep Neural Networks has a significant impact on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU), which is f(x)=max(0,x). Although various alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. So Google Brain Team has proposed a new activation function, named Swish, which is simply f(x) = x \u00b7 sigmoid(x). Their experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging data sets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNetA and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.\n\nWith ReLU, the consistent problem is that its derivative is 0 for half of the values of the input x in ramp Function, i.e. f(x)=max(0,x). As their parameter update algorithm, they have used Stochastic Gradient Descent and if the parameter itself is 0, then that parameter will never be updated as it just assigns the parameter back to itself, leading close to 40% Dead Neurons in the Neural network environment when \u03b8=\u03b8. Various substitutes like Leaky ReLU or SELU (Self-Normalizing Neural Networks) have unsuccessfully tried to devoid it of this issue but now there seems to be a revolution for good.\n\nSwish is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks applied to a variety of challenging domains such as Image classification and Machine translation. It is unbounded above and bounded below & it is the non-monotonic attribute that actually creates the difference. With self-gating, it requires just a scalar input whereas in multi-gating scenario, it would require multiple two-scalar input. It has been inspired by the use of Sigmoid function in LSTM (Hochreiter & Schmidhuber, 1997) and Highway networks (Srivastava et al., 2015) where \u2018self-gated\u2019 means that the gate is actually the \u2018sigmoid\u2019 of activation itself.\n\nWe can train deeper Swish networks than ReLU networks when using BatchNorm (Ioffe & Szegedy, 2015) despite having gradient squishing property. With MNIST data set, when Swish and ReLU are compared, both activation functions achieve similar performances up to 40 layers. However, Swish outperforms ReLU by a large margin in the range between 40 and 50 layers when optimization becomes difficult. In very deep networks, Swish achieves higher test accuracy than ReLU. In terms of batch size, the performance of both activation functions decrease as batch size increases, potentially due to sharp minima (Keskar et al., 2017). However, Swish outperforms ReLU on every batch size, suggesting that the performance difference between the two activation functions remains even when varying the batch size.\n\nObviously, the real potential can be adjudged only when we use it for ourselves and analyze the difference. I find it simpler to use Activation functions in a functional way by defining a fn that returns x * F.sigmoid(x). This article at this moment is just an overview. Please feel free to implement Swish your way and do share your experience. Thank You for your time and enjoy Machine Learning!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/sniper-vision-human-resource-management-simplified-dc17ea443f59?source=---------5",
        "title": "Sniper Vision | Human Resource Management Simplified",
        "text": "Human Resource professionals are the core of any business irrespective of how many times in a day we criticize them for their ineffectiveness with one or two of our tasks. Over the years managers have often had young talent approach them asking for mentoring because they want to work in Human Resources. These managers love to do that but only for right reasons and so has been Brian. So he has usually asked such questions: \u201cWhy do you want to work in HR?\u201d More often than not the answer goes something like this: \u201cI love working with people, developing them and helping them\u201d. To which he usually responds: \u201cIf that\u2019s what you want to do then you should work in operations or general management, not in HR.\u201d This is often a shocking response, but it\u2019s an honest one. The misconceptions that HR is a \u201cnice\u201d place to work because they work with people is pervasive, and often leads to the wrong kind of talent in the function.\n\nTo be fair, being nice is usually an expectation and requirement to be in HR. It\u2019s hard for most people to imagine their HR partners as not nice people. But I think this is where some young talent gets confused. They see \u201cnice\u201d HR colleagues and leaders, perceive that the role is all about helping people, and mistakenly assume that being a nice person is qualification enough for the function. However, \u201cnice\u201d is only a starting point \u2014 it is not nearly enough.\n\nRestructuring \u2014 Whenever there is an organizational restructure there are winners and losers. Dealing with the people that land on their feet is easy. But in any restructure there are those that lose their job, face demotions, or sometimes end up in a role that they don\u2019t like. These people deserve a respectful and \u201cfair\u201dprocess. Nice is just not enough. Recruiting \u2014 There are few things as enjoyable as telling somebody they got the job they were really hoping for. Unfortunately, for everybody that gets the job, there are many people who wanted it and didn\u2019t get it. It\u2019s not so fun to make those calls.\n\nCompensation is about paying people what the job is worth, not what they want. This often causes disagreement and friction. HR professionals must learn to explain facts and reality not only to employees at all levels, but also often to their managers who feel they should just be able to pay more. Sometimes HR\u2019s get to give great news in this regard, but more often they must find ways to keep integrity in the compensation structure. Talent management is about differentiating top talent and investing in them disproportionally. Delivering that news to the selected individuals can certainly be enjoyable. But for every top talent there are many who are not, and we often must explain why we have rewarded others differently.\n\nLearning & Development should be about giving people the training they need, not what they want. Labor/Employee relations is about ensuring they have a consistent and fair work environment, not to make everybody happy with their circumstances. Culture is about creating a great and/or effective working environment, not necessarily a nice environment. Great and nice aren\u2019t synonyms. It\u2019s not hard to see that the common perceptions that HR is an easy place to work, nice, or fun, are completely misguided. Of course, it can be fun. But when done well, it\u2019s difficult work. As Brian once mentioned:\n\nEmpathy is the Key \u2014 Actually what HR professionals really need is not niceness, but empathy. That is, understanding and taking into account how people feel. They must do the work, sometimes tough work, that their organizations need. Doing so with empathy, and helping other leaders have empathy, makes such a difference. As a function, they are often expected to give difficult news and feedback, or to help other leaders give such feedback. It\u2019s always better to give it in an empathetic way. Balance- HR professionals have to keep it all balanced if they want to maintain sanity. Balance in life is critical, otherwise it can become overwhelming and tempting to slide into nice for nice sake in order to avoid some of the tough work, which is not what organizations need. It\u2019s important to take a breath sometimes and keep it all in perspective. It is strategic and is also an art that must be practiced daily to be truly good at it. Helping and watching people grow is great but helping and watching the company grow through it\u2019s people is even more important and more fulfilling.\n\nSo, if you want to work in HR, please take a note of what\u2019s really required for success and make sure you are pursuing this career for the right reasons. If you already work in HR, keep perspective, and focus on what\u2019s most important. Have empathy, but do the right thing and don\u2019t be afraid to give the tough messages. Meanwhile from technology perspective, we are always there to help you with the best that we can to make your life little simpler. Hence, I have created this Predictive Machine Learning algorithm that almost anybody can use, provided you take care of couple of pointers. Entire code helps you predict if the employee can be a future attrition, or if you\u2019re paying him/her more than what the trend in the company has been so far. There are multiple other aspects that you can think of considered before formulating this algorithm in Python. The original set of data is right here for you to download and just keep adding your employee data in the same pattern after removing all existing data. And every time you want to predict behavior, just run the algorithm (don\u2019t worry! it has 97% accuracy). Please do take assistance of any software professional or post in comments for me to respond if you face issues in saving/running the file. A warm hug to all you amazing Human Resource professionals & entire code is available here. Please leave your claps as a token of appreciation if this works for you :)"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/demystifying-glasses-classifying-its-variants-2d92c4f022ad?source=---------6",
        "title": "Demystifying Glasses | Classifying its variants \u2013 Deep Learning \u2013",
        "text": "Your days have been rough and exhaustive if you haven\u2019t come across a piece of glass unless you have started staying in a forest or have become a monk somewhere on the Himalayas. All of us have importance of glass associated in different forms, be it Champagne flute, Dressing table, or probably an environment friendly Greenhouse. What else can I say when even Jon Snow trusts in a dragon glass for killing supernatural humanoid White Walkers.\n\nGlasses may be devised to meet almost any imaginable requirement. There are many different types of glass with different chemical and physical properties and each can be made by a suitable adjustment to chemical compositions. For many specialized applications in chemistry, pharmacy, the electrical and electronics industries, optics, construction and lighting industries, glass, or the comparatively new family of materials known as glass ceramics, may be the only practical material for an engineer to use. The main types of glass include: 1. Borosilicate Glass 2. Commercial Glass 3. Glass Fibre 4. Lead Glass.\n\nLet us now consider one such data set based on which we shall prepare our classification model. For you to reproduce any such model, you can pull the code from my GitHub or just follow as per the code snippets that I have attached below. Entire flow is in Python so you would require an IDLE like Anaconda Spyder, PyCharm, etc. The model below is based on K Nearest Neighbor algorithm along with Elbow method to choose the best of \u2018k\u2019 values.\n\nNow, we have a model ready to use on similar test data sets and easily classify the type of glass. To better understand the pattern of , we can refer to the image below:\n\nInstead of Elbow Method, we could have also used Principal Component Analysis (PCA) for relating to individual components with it\u2019s impact on Glass type and I will demonstrate that as well very soon in one of my other blog. So, if you\u2019re a businessman who wants to have a handy algorithm to segregate the information readily, you can simple use Python Flask to wrap it in a Restful API and use it in your software to get going. Any query/feedback would be highly appreciated in Comments section. Good Luck segregating!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/natural-calamity-classifying-forest-fire-damage-c4139acfc009?source=---------7",
        "title": "Natural Calamity | Classifying Forest Fire Damage \u2013 Deep Learning \u2013",
        "text": "Life is extremely precious, be it of humans, animals, birds or trees because all of us contribute to the existence of each-other. And one of the biggest threat to this existence is a natural calamity as it is pretty much unavoidable. Thousands of lives are lost and that certainly value much more than the property damage associated. One among them is a Forest Fire which destroys a forested area, and can be a great danger to people who live in forests, along with wildlife. Forest fires generally start by lightning, but also occasionally by human negligence or arson, and can burn thousands of square kilometers.\n\nThere is a separate segment of scientific study & research that is done to avoid such instances and to contribute from our end, we shall assess associated damage of one such occurrence. This data set was released by Paulo Cortez and Anibal Morais of Department of Information Systems, University of Minho, Portugal based on their findings of Meteorological Data in 2007. Portuguese experts confirmed an artificially intelligent model during 13th EPIA 2007 using Gaussian SVM with just 4 parameters to predict forest fire possibilities based on RMSE and MAD. This model works best for small fires but alternatively for forest fires affecting wider regions, we will try to approach differently.\n\nWe will set up a Dense Neural Network using TensorFlow Regressor to prepare our model to predict wide spread forest fires. Hence, we shall be considering multiple aspects of the available features to classify. Our network will composite of three layers associated with a hundred thousand steps to achieve better results, that we shall evaluate with RMSE of Scikit-Learn. By now, we all know that this specific model will be built using Python as our platform so no brownies for that. For convenience, I will add screenshots here along with my GitHub to access entire code for pulling across.\n\nAs expected, this shall fetch us Prediction values and we can even plot it against our original values to ensure that we\u2019re on the right path. Well, I will leave that part for you to explore with either Matplotlib, Seaborn, or any other library as per your choice. Please note that I have used TensorFlow which can be replaced by simpler Regression machine learning models as well. So without hesitation, if you would like to give it a shot, please go ahead. Good Luck predicting!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/the-wolf-of-wall-street-predicting-stock-prices-631bca4b4cbd?source=---------8",
        "title": "The Wolf of Wall Street | Predicting Stock Prices \u2013 Deep Learning \u2013",
        "text": "Back in 2013, we all wanted to be in the shoes of Leonardo DiCaprio with stacks of money around us. Matter of fact is that there are people around us who does melt that kind of money from stock market every other day while we just keep daydreaming about it. Certainly, JP Morgan or Goldmann Sachs are not going to share their trade secrets with us in public forums or do we expect Warren Buffet to disclose his Bitcoin investment strategy. We know that is not going to happen so how about we take inspiration from them & try to make our own strategy to understand and predict stock market variations.\n\nAll we need to have is an inquisitive mind to understand the analogies as I paste the code to duplicate. We require all three dragons of Daenerys Targaryen, which in our case are: 1. Installing Dependencies 2. Collecting relevant data 3. Write magic lines of code. Once done, we can relax with our coffee mug while I explain the patterns of the variations. Let us drill down into 2016 data of Apple Inc. and Microsoft with this simple set of code:\n\nThis will fetch us an insight of the returns that investors received on their venture in 2016. If we wish to visualize it for some other year, or span of years, we just need to alter \u2018start\u2019 and \u2018end\u2019 parameters in this piece of code. And then we can have a nice colorful plot of insights like this:\n\nWith that part of analysis been done, we still have an important task in hand as we need to create value with the insight that is available. Thinking of actions that can be taken, we can do: 1. Sentiment Analysis on company opinions 2. Past Stock Prices 3. Dividend 4. Sales Growth. Knowing the fact that changes in stock prices are not at random, good traders utilize predictive Machine Learning models as a tool before making an investment. To make all of us efficient with this, allow me to furnish an easy-to-use code that I have drafted again in Python that just needs to be cloned to get desirable insight before making an investment. But this time, to avoid complexity for beginners, I will use data only for one company and build three models on top of it, so lets ask Apple Inc. to \u201cShow me the money!\u201d\n\nWe already have our libraries imported so let us start by creating two empty list that would contain our data: and , followed by this two simple functions that I will define before calling them to get predicted values:\n\nis the magic keyword for us at the end to get 3 different types of plot for associated 2017 stock value predictions. Let us have a closer look at this graphical representation:\n\nWe can easily figure out looking at this graph that it is the Radio basis function (rbf) kernel that best fits our data so we have our key right here to obtain best of predictions. I don\u2019t disagree with the efficient market hypothesis that states Stock prices are unpredictable BUT again it is better to have effective Machine Learning model predictions than just investing randomly. Good Luck investing!\n\nFor entire code or any suggestion/feedback to it, please visit my GitHub."
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/why-do-you-wish-to-be-a-data-scientist-a8f8d862c203?source=---------9",
        "title": "Self Assessment | Why do we wish to be a Data Scientist?",
        "text": "All of us know and Google Trends have validated the fact that anyone even remotely associated with \u2018data\u2019 wants to be a \u2018Data Scientist\u2019. Be it professionals or high school students, this is the latest craze of technology world. There has been a huge surge in the number of people who have enrolled in one or the other eLearning portals like Udemy, Coursera, DataCamp, etc. Leaderships are ready to change their innovation strategy focusing more on churning of their used/unused data to get insights of every aspect of their present/future business model. Even small businesses like the restaurant in the city center downtown wants to strategically classify their guests and with all this buzz around, I literally needed to live on an isolated island to stay away from this. Allow me to begin with a thought-provoking analysis of aspiration agenda for this new generation aspirants by Eric Weber.\n\nPortraying myself as the protagonist in this aspiration scenario, I would like to start by saying that it all started couple of months back when I got fully convinced with the trending idea and decided to march to the same drum. Being a Data Analyst and also a DBA for sometime now, I realized that I have already been around petabytes of data till date, so the transition shouldn\u2019t be that difficult with my expertise in SQL, Excel and reporting tools like PowerBI, SAP BO, etc. Now my first assignment was to research what a Data Scientist does and what skill set do they require, where LinkedIn Jobs, Analytics Vidhya, KDNuggets, etc. proved to be a good resource. I started jotting down all the intrinsic skill sets and was very soon overwhelmed with the abundance of knowledge required to become a Data Scientist.\n\nSuddenly for next 2 days, I was swimming downstream before an article flashed in front of me \u2014 Data Scientist Skills & Salaries. Every coin has two sides, and we need to be extremely careful while choosing our\u2019s because I chose the wrong side, as the only thing I (unconsciously)focused on was the Data Scientist Salary, associated perks,recognition and a lavish life ahead. Tadaa! I was all set to begin my learning path (with gluttonous intentions). Evolving from a Finance background, I knew that it isn\u2019t going to be easy for me to learn all the tools required. Since I didn\u2019t know anyone around me with related skills, I chose DataCamp and Udemy for learning Python to begin with, and then moved to a detailed course on Data Science and Machine Learning to finish off what is required of me.\n\nMeanwhile, there was one good thing that I did, i.e. started connecting with Data Science experts & visionaries on LinkedIn. It was almost over a month now and I was struggling big time to cover all those topics included in the course along with my regular job. And there came a time when I quit studying for almost a week because I wasn\u2019t able to recollect most of what I had studied. This was extremely depressing for me because instead of coming closer to my king-size lifestyle and fancy perks, I was getting even more distant from it. I needed to transform myself so I took a week off from work and headed to my favorite getaway destination. For this entire week, I peacefully channelized all my energy into reassessing my adventure because I have never been a slacker. It is then that I realized that from the very start I chose wrong side of the coin.\n\nThe other side of the coin had a golden rule embedded in it \u2014 Interest leads to Passion and that feeds forward to knowledge, and then the rest automatically follows. Finally, I had a smile on my face and I kind of knew what needs to be done. I don\u2019t need to study to get a job because I already have it and am quite content with it. With reshuffled strategies, now my primary task was to decide what exactly am I passionate about, and luckily the answer remained to be quite on path \u2014 \u2018Divine power of Human Brain\u2019. Next self-assigned task was to understand my own brain & it\u2019s interest and that initiated with Machine Learning.\n\nAbruptly(for good), things automatically started changing for me. I finished that detailed course on Data Science with Python from Jose within next 3 weeks and I could remember almost everything from it. Additionally, I had been practicing heavily with data sets from online repositories. I have started taking little pride in my efforts, and these days it is all that matters to me. Actually Data Science isn\u2019t a topic for me to study anymore, it is a topic of interest that I have developed so this one course can\u2019t be the end because now I AM HUNGRY. Hence, I enrolled into extensive Machine Learning training from Kirill and Hadelin where they deeply cover every aspect of ML though they don\u2019t really get into core Mathematics (but do provide links to infamous research works by various other Data Science experts). With all the knowledge imparted by these amazingly talented instructors, it just requires little extra effort to understand the core mathematics and science behind every step. Jose was even kind enough to grant me access to his TensorFlow course free of cost.\n\nOn the other hand, LinkedIn has also been a great resource because experts like(to list a few) Daniel Tunkelang, Ben Taylor, Beau Walker, Carla Gentry, Brandon Rohrer, Eric Weber, Andy Kriebel, Andrew Trask and Matthew Mayo almost everyday share a lot of their knowledge, best approach and practical guidance which is immensely beneficent in segregating the best from the rest. There are many other resources freely available online for budding Machine Learning enthusiasts like me to learn from. Every individual (including experts) have a different perspective but going by whom I follow, I won\u2019t consider myself ready for the big game unless as Ben says \u201cHave you ever looked into Scikit-Learn behind-the-scene and tried to understand and then code it yourself\u2026.Have you made yourself uncomfortable?\u2026Have you LOVED Data Science enough?\u201d. Though when the going gets little tough with the overwhelming amount of knowledge, I prefer relaxing with what Brandon says \u201cBeing a Generalist is OK\u201d.\n\nSole intention is to make all these aspiring Data Scientists realize (with my own limited understanding) that we are on the right path but have we thought of \u2018Why are we on this path?\u2019. I realized it with my own mistakes that I cannot jump directly into the Data Science field because it has to be a natural progression.\n\nIn Data Science domain as well, there is a step-by-step progression that would lay the foundation for us. There is nothing more amazing than starting with a Data mining/analyst skill set, followed by Big Data, and by then we would automatically know what our actual strength is and which way can we head to achieve our goal. Research is the key to any problem in life and history has proved it every now & then, so exploit Stack Overflow, Quora, etc. or even better if we get a book (even an eBook) or Research papers on a subject and invest time in it.\n\nTime is Money so better to invest it wisely, instead of blindly following what others around us have been doing. Drill deep within ourselves and figure out our real passion. There are thousands of them taking the same online courses that we have been taking, so how are we different? Suppose, I am 29 and have a sibling who is 12, where my brother doesn\u2019t understand the implications of munching too many chocolates every day. Can I explain my inference model to him because in real world that is pretty much the case with stakeholders. So better we quickly assess which side of coin are we on and I wish good luck to everyone."
    }
]