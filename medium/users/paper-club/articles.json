[
    {
        "url": "https://medium.com/paper-club/analytical-bayesian-inference-with-conjugate-priors-4a1d75ca799b?source=---------0",
        "title": "Analytical Bayesian Inference with Conjugate Priors",
        "text": "I have been working a lot with pyro lately, trying to wrap my head around variational methods and statistical machine learning. Variational bayes(VB) methods are an excellent way to solve otherwise intractable problems and have revolutionized the field of statistical machine learning. Before VB methods, using bayesian inference required careful choices of distributions such that inference was analytically tractable. In order to build some intuition around VB, I\u2019ll walk throughs some of the analytically tractable approaches to bayesian inference. Full code for this post can be seen here\n\nThere are four main steps to creating a bayesian model.\n\nJohn just opened a bakery and needs our help making predictions around how many pies he\u2019ll sell in a day. A poisson distribution is a good starting point because it is a discrete probability distribution that expresses the probability of a fixed number of events occurring in a specific time interval. The poisson distribution takes a parameter \u03bb. The probability of getting a specific data point can be represented by fixing \u03bb in the equation:\n\nNow what\u2019s a good value for \u03bb? Well, a really nice property of the poisson distribution is that \u03bb represents the mean. John has a general idea that his mean will be around 4. Plugging this value in for \u03bb gives the following pmf.\n\nThe next step is to define the parameter of our model \u03bb, as a distribution.\n\nJohn\u2019s best guess is a mean of 4(\u03bc = 4), but he isn\u2019t very confident in his estimate so we\u2019ll reflect that confidence in our prior. We\u2019ll choose a gamma distribution to represent the mean, why that\u2019s a good choice will become apparent shortly! We want to reflect our prior belief that the actual mean is around 4, so we\u2019ll choose parameters for the gamma distribution, shape(\ud835\udc58) and scale(\ud835\udf03), such that the mean of our gamma distribution is 4.\n\nFor now we choose \ud835\udc58 = 4 and \ud835\udf03 = 1. Because \u03bc = \ud835\udc58\ud835\udf03 for the gamma distribution.\n\nNow the choice for \ud835\udc58 = 4 and \ud835\udf03 = 1 isn\u2019t entirely arbitrary. We want to choose values here that encode our uncertainty in our guess. If we are very uncertain, we\u2019ll choose values that make this distribution wider, reflecting this uncertainty.\n\nThe likelihood function for a specific observation is \ud835\udc43(x|\u03bb). For multiple observations, you multiply their likelihoods together, so the likelihood becomes:\n\nNow let\u2019s use bayes theorem to calculate our posterior.\n\nP(X) is a constant because the values of X are fixed to the observations. The equation reads probability of \u03bb given our observations X. We can ditch the P(X) denominator term temporarily just so long as we rescale the distribution at the end. We can now say that the left hand side is proportional to the right hand side\n\nPlugging in our equations, we get a posterior distribution for \u03bb:\n\nFor the sake of simplicity, let\u2019s say we observe 1 day where 6 pies are purchased. This equation simplifies to:\n\nensure we re-scale the distribution by some constant C so that it integrates to 1:\n\nand combine terms to get the posterior distribution:\n\nWe\u2019ve computed our posterior for \u03bb, but even more incredibly, we\u2019ve landed back on a Gamma distribution! Let\u2019s see exactly how this is the case. Remember the gamma distribution is defined as:\n\nAt the beginning of this post, we somewhat arbitrarily chose values for \ud835\udc58 and \ud835\udf03. Now, notice that a choice of \ud835\udc58 = 10 and \ud835\udf03 = 0.5 gives us our posterior!\n\nwhich just means that in our calculation of the posterior we choose C:\n\nNow this wasn\u2019t an accident, in fact it\u2019s precisely the reason that I chose a gamma distribution as the prior. A poissoin distribution\u2019s conjugate prior is the Gamma distribution, and whenever you choose a conjugate prior, the posterior will always be of the same form as the prior\n\nIf we step back for a moment we can see that by using a conjugate prior, we generate our posterior by updating the parameters of our prior \u2014 reflecting a new mean and confidence level, depending on the amount of observed data. As we observe more datapoints, \ud835\udc58 and \ud835\udf03 are updated in such a way as to shrink the width of our posterior, indicating an increased level of confidence in our distribution."
    },
    {
        "url": "https://medium.com/paper-club/must-read-clueless-individual-attempts-to-explain-basic-pymc3-model-9859d89d5468?source=---------1",
        "title": "MUST READ: Clueless Individual Attempts to Explain Basic PyMC3 Model.",
        "text": "For background on why all of a sudden there\u2019s Bayesian content on this blog, see here.\n\nN.B.: this isn\u2019t meant as an introduction to Bayesian methods or PyMC3. It was originally composed as a memo for myself at ~ Ch 4 of the book Bayesian Methods for Hackers, and thus makes assumptions about concepts and knowledge up to that point. I would highly recommend the book if any of this seems interesting :)\n\nThere have been many many (many many many) confusing parts of PyMC3 and Bayesian modeling in my short experience with these tools and concepts. Among the most challenging has been conceptualizing how observed data is added to the model, and relating this to where the MCMC (Markov Chain Monte Carlo; here\u2019s the best introduction I know of) algorithm hooks in to sample from the resulting posterior distribution.\n\nInitially, I got hung up on comparisons to mini-batch SGD (Stochastic Gradient Descent; if you\u2019re unfamiliar with this, maybe skim here for the basic idea) in deep neural networks. Since the majority of my exposure to machine learning has been through neural nets, this is the first example of a training process that my mind reached for. In mini-batch SGD, individual samples are fed one by one to the model, loss is calculated, and the error is backpropagated through the network as part of one sequential \"training step\".\n\nHowever, this mental model does not fit in a Bayesian context, where the construction of the item of interest and the algorithm to \u201cdiscover\u201d that item are distinct. Specifically, after the prior distribution is warped by the likelihood distribution to form the posterior, without any further action the posterior is completely opaque to us; with multiple dimensions, it becomes computationally intractable to determine a function for the posterior distribution. Contrast this to a DNN where the training process can be stopped or continued at any point and the network remains as \"whole\" as at any other point. No further steps need to be taken in order to receive output from the model.\n\nI brought this up as part of a larger discussion in our Slack group:\n\nJames Vanneman's second comment set off a lightbulb for me:\n\nSo: the posterior distribution exists after the observed data has been incorporated into the model. However, due to the curse of dimensionality this posterior is inaccessible analytically. In order to approximate the posterior and do anything useful with your model, you must perform MCMC.\n\nWith this in mind, I wanted to understand the PyMC3 API for representing these actions. Let's take a look at a function from Ch 4 of Bayesian Methods for Hackers. The context for this code is the task of generating a distribution for the \u201ctrue upvote ratio\u201d of a Reddit submission, based on the observed sample of up- and down-votes for that submission. From the text:\n\nI chose this example because it is more or less the most compact practical distillation of a PyMC3 analysis you can get to.\n\nWhile there are still lots of fuzzy areas, I think I have enough context to take a stab at an explanation here. There are four clear steps in this function, noted with comments in the code.\n\nAnd voila, you now have an approximation of your posterior! Easy\u00b9.\n\n\u00b9Disclaimer: actually not easy at all."
    },
    {
        "url": "https://medium.com/paper-club/where-have-we-been-75c229d42acc?source=---------2",
        "title": "Where Have We Been? \u2013 Paper Club \u2013",
        "text": "This blog may have been inactive since the turn of the calendar year, but rest assured Paper Club has not!\n\nAfter returning from the holidays we got together to chart a course for Paper Club in 2018 and beyond. The J\u2019s had a blast at NIPS in December and Jason Benn started his first professional machine learning gig at Sourceress. We were pleased with our respective and collective 2017's in AI/ML and wanted to keep the momentum going.\n\nWe looked at our goals and surveyed several areas of potential interest. There is so much activity in the domain right now that analysis paralysis and FOMO are almost unavoidable. We met on a boat (long story), waded through some of this, and eventually decided to explore one of our many shared curiosities: Bayesian neural nets. There was a lot of chatter about these architectures at NIPS, we had varying levels of personal interest in Bayesian statistics as a whole (mine was quite high), and we had all seen firsthand the main problem that Bayesian nets purport to solve: interpretable confidence levels in deep learning. And so, it seemed as good a place as any to continue learning together.\n\nBriefly and most basically, Bayesian neural nets are a class of neural net that:\n\nThese two properties allow for many extensions to the capabilities of neural nets. It is a comparatively greenfield architecture with exciting possibilities.\n\nWe quickly realized a third property as it applied to our attempts to learn Bayesian NNs: they are conceptually a bigger leap from vanilla neural nets than anything we\u2019d encountered before.\n\nWe began with Yarin Gal\u2019s PhD thesis. I\u2019m sure there is immense value in these 150-odd pages, and I think we will end up returning to it, but it was very far above the level of the January version of me (and others\u2026right guys?).\n\nI understood about 5% of the easiest chapters of the document. Even still, this was more than enough to pique my interest: active learning, deep Bayesian reinforcement learning, medical diagnostic confidence bounds, oh my! (See Chapter 5 for a relatively digestable summary of these applications and more) I understood the neural network-related concepts, but it was clear I\u2019d need more background in Bayesian analysis and probabilistic programming in order to grasp the entire thesis.\n\nThe next place I looked was Pyro\u2019s tutorials. Pyro is a new library from Uber for \u201cDeep Universal Probabilistic Programming\u201d. Since I\u2019d spent the past 9 months doing the \u201cdeep universal\u201d thing, I figured I might be able to pick up the \u201cprobabilistic programming\u201d part on the side as I worked through tutorials. After all, it comes second in the description so how hard can it really be?\n\nToo hard for me, as it turned out. Dropping down one more level of simplification, I arrived at Cam Davidson-Pilon\u2019s Bayesian Methods for Hackers book, which had previously been shared to the group by Jason Morrison. This book entirely focuses on Bayesian analysis and probabilistic programming, without getting into the deep learning applications. And it was just right.\n\nThe rest of the group seemed to share my appreciation for the writing style, engagingness, and overall fun of this book, so we decided to work through it together. And that\u2019s where we\u2019ve been! The material has been very interesting and, in my humble opinion, at the absolute perfect level of difficulty.\n\nSo here we are, with about two chapters left in that book. We have a loosely formed goal of taking what we\u2019ve learned and working on a project with Pyro or PyMC3. Hopefully we\u2019ll be sharing more of that and what we\u2019re up to once we get through the book. Cheers!"
    },
    {
        "url": "https://medium.com/paper-club/ai-index-thoughts-d745c0159205?source=---------3",
        "title": "AI Index Thoughts \u2013 Paper Club \u2013",
        "text": "Since Andrew Ng\u2019s letter to me was typed and not handwritten, I\u2019ve decided to publish my thoughts on the AI Index here instead of being included in the Expert Forum.\n\nFor my summary of key/favorite points see here, and to view the report itself go here. I\u2019ll reiterate that it only takes 15\u201330 minutes to skim through the bulk of the report, and I\u2019d highly encourage you do so!\n\nIn April 2017, a small group of software engineers that met each other taking computer science classes together decided to take the plunge into deep learning. That group eventually became Paper Club, and since then we\u2019ve gone through parts 1 and 2 of Fast AI, entered various Kaggle competitions, and tried to read one AI-related paper a week.\n\nBut seven months is barely enough to scratch the surface! I\u2019m still quite fresh in this domain. More than anything, I\u2019m excited \u2014 to continue my own path through this material, and to witness how AI will change the world over the coming years. The content of the AI Index raised and reassured this feeling quite a bit.\n\nWith that out of the way, here\u2019s a grab bag of my thoughts on AI index 2017.\n\nThis piece of the report echoes what has been a recurring theme in Paper Club: theory vs. practice. How much time should we dedicate to reading papers, vs. entering Kaggle competitions? Doing linear algebra exercises vs. training models for a side project? Should we read old fundamental papers or new cutting-edge papers?\n\nSince most of our members\u2019 respective programming backgrounds are in bootcamps and/or self-teaching, we have experience with the bad habits that can form fumbling high-level libraries without having a clue what\u2019s going on underneath the hood. However, we are also taking our own free time outside of work to learn this stuff, and would quickly lose interest if we were just going through worksheets of math problems.\n\nThis graph demonstrates a natural progression of ideas and interest in AI this century; starting with researchers, trickling down to students at universities, and now making its way into startups and industry.\n\nMy interpretation of this data is that now is a good time to lean towards practical application of the hard-won ideas produced by universities and big companies over the past few years. The growth trajectory for business interest has caught up (and may soon surpass) the interest in research/education as these powerful concepts are aimed towards solving real-world problems.\n\nAlthough humans are still better at more \u201cdifficult\u201d tasks like document/visual question answering, they are not improving at nearly the rate of neural nets.\n\nFor example, in the graph above plotting document question answering accuracy, we can see that just in the last two years the state-of-the-art AI has improved it\u2019s accuracy by almost a third, going from 60% to almost 80%. It\u2019s not hard to imagine these sections of the report singing a different tune in the next iteration or two of the AI Index.\n\nI find it somewhat ironic that a big recurring theme is difficulty finding relevant, trustworthy data. That just so happens to be the biggest problem facing the domain that the report is reporting on!\n\nWith a bit of extrapolation, the AI index was able to produce some trends around university enrollment in AI-related classes (pictured above). While this is a good start, it represents a very narrow demographic of potential deep learning lifers. I would love to see more thoughts and data around the accessibility of the industry to newcomers.\n\nThis is especially notable to Paper Club. When we first surveyed the landscape, all we saw was a ton of math and required computing power. This was quite intimidating (even to the guy with a math major!). One of the reasons we started with Jeremy Howard\u2019s fast.ai course (which is excellent, btw) is because of its promise to demystify deep learning. We couldn\u2019t be happier with that choice, as it gave us the confidence and tools to keep going and get into more complex material.\n\nImproving on and developing similar MOOCs or readily available books will widen reachable audience by a ton. Continued focus on the quality of open source tools will prop up the feeling that my single GPU and I can take on the world.\n\nCurrent media coverage of AI seems to be quite positive based on the data presented in the report. Given the built-in doomsday scenario with AI, how long will this last before naysayers get their chance? My experience has been that the media bounces back like a rubber band when its sentiment on a topic gets stretched too far in one direction. The general public can be wary of new technological developments, especially ones without clear explanations (which I would say currently describes neural networks). I hope new articles continue to trend positive, but would not be surprised if things took a turn for the worse.\n\nDiversity and inclusion in AI has a much greater impact than in almost any other industry because of the potential catastrophic consequences of machine bias in AI algorithms that are developed in a vacuum with a hivemind. This makes it even more worrying that we\u2019re unable to surface reliable diversity and inclusion data in the industry \u2014 I can\u2019t offer much right now, but I think it should be a priority and I sure hope this changes in the near future!\n\nThe Milestones timeline was a great visualization of the progress in AI, as AI has been able to beat humans at exponentially harder tasks in sub-linear time over the past few years. I would also like some kind of insight into what the next barriers to fall will be, and how close we are to breaking them.\n\nOne area of interest that I would like to see included in future reports is progress towards general AI (i.e. an AI that can \u201cthink\u201d, reason, and approach new tasks and problems like a human does).\n\nLike it or not, achieving general AI is a breakthrough that would instantly change the word. Expert opinions differ vastly on timelines and viability for general AI, and that\u2019s okay \u2014 I want to read about all sides of this discussion! If there\u2019s no substantive progress towards general AI, I want to hear about that as well.\n\nAI Index is part of the 100 Years Study on AI at Stanford, and as time goes on I think they\u2019ll have to turn their ears more and more towards general AI.\n\nI\u2019m very happy that the AI Index is a thing. My conclusion here is the same as it was for my summary: this is an exciting beginning to a fantastic project. I can\u2019t wait for AI Index 2018, and even more so AI Index 2038.\n\nP.S. There\u2019s a weird downwards dip in the graph of News Translation BLEU scores over time on page 29. If anyone know what\u2019s up with that I\u2019m curious."
    },
    {
        "url": "https://medium.com/paper-club/ai-index-index-ae0e3a8083d8?source=---------4",
        "title": "AI Index Index \u2013 Paper Club \u2013",
        "text": "Earlier this month a team made up of and sponsored by Stanford, MIT, OpenAI, Google, Microsoft, etc. released the AI Index, a first-of-its-kind deep-dive into the trends in the Artificial Intelligence community.\n\nIt was a much-needed, sometimes surprising, mostly reassuring look at the state of ML/AI in 2017. I\u2019m going to summarize some of my favorite parts and themes, but I would encourage you to read the report in its entirety here. The data itself runs for 40 or so pages of size 16 font with plenty of whitespace and lots of pictures (very refreshing after reading academic papers for months!); it is a casual half-hour skim.\n\nUnfortunately the committee\u2019s request for my inclusion in the Expert Forum got lost in the mail, so I\u2019ve been forced to publish my more subjective thoughts on Medium here.\n\nThis is an exciting beginning to a fantastic project. I can\u2019t wait for AI Index 2018, and even more so AI Index 2038.\n\nMy detailed thoughts on the AI index can be found here."
    },
    {
        "url": "https://medium.com/paper-club/from-research-to-practice-b2e2ecd292fb?source=---------5",
        "title": "From Research to Practice \u2013 Paper Club \u2013",
        "text": "Leslie Smith is a researcher at the US Naval Research Lab. The NRL isn\u2019t top of mind when it comes to deep learning research, but I have really enjoyed reading Smith\u2019s papers. I\u2019ve found them to contain just as many impressive contributions as folks at Google/Facebook/Baidu.\n\nThis paper covers recommended steps for using deep learning to solve problems in new or adjacent domains. It does not introduce a novel architecture or propose a mind-bending new training technique, but I think its content is just as important for the growth of deep learning and is often overlooked by researchers.\n\nBecause the format of this paper is a bit unique, I\u2019ll be covering it as a summary list of notes with some commentary interspersed rather than using our Paper Notes Template.\n\nAnd that\u2019s that! Just follow these 7 simple, straightforward, easy, clearly-defined, no-confusion-at-all, steps and you\u2019ll be on top of the world \ud83d\ude09"
    },
    {
        "url": "https://medium.com/paper-club/paper-review-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-4f25e8f2283a?source=---------6",
        "title": "Paper Review: Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
        "text": "This part of section 2 \u201cMotivation\u201d is just such a lucid and interesting analogy that I\u2019ll quote it here verbatim:\n\nPrimarily, the paper is looking to evaluate Dropout as a method of reducing overfitting and improving model generalization.\n\nDropout is a regularization technique \u2014 a family of techniques for reducing overfitting (thereby improving generalization) by making the decision boundary or fitted model smoother.\n\nThe most widely used implementation of Dropout is as a stochastic regularization technique, and that is the implementation primarily tested in this paper.\n\nIn addition, the authors find that there are additional improvements whereby neuron co-adaptation is reduced and feature (representational) sparsity is improved.\n\nA very solid paper. It gives easy-to-use practical recommendations for using dropout. It shows strong empirical results in favor of dropout. It shares an interesting motivating idea from a different field. It does lack a theoretical underpinning for dropout. It\u2019s possible that such an underpinning can be found from \u201cUncertainty In Deep Learning\u201d, Gal 2016 which ties \u201capproximate [Bayesian Neural Network] inference together with (\u2026) regularization techniques such as dropout.\u201d\n\nThe paper introduced me to a wide range of background material.\n\nAppendix A is titled \u201cA Practical Guide for Training Dropout Networks\u201d and makes a few recommendations:\n\nWhy do we not see dropout in modern CNN networks \u2014 e.g. fully convolutional residual networks with batch normalization?\n\n\u2014 in \"What happened to DropOut?\"\n\nPerhaps the structural correlation in CNNs (across spatial dimensions) and RNNs (across time) means that dropout actually hurts more than it helps, unless you take care to tie dropout weights to avoid this:\n\n(Blog post author note: See Gal and Ghahramani \u201cA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\u201d.) Continuing the same quote from above:\n\n\u2014 in \"What happened to DropOut?\"\n\nThere is very interesting research by Yarin Gal that builds on dropout to produce uncertainty measures (error bars) from neural networks. Very loosely, at prediction time you use Monte Carlo dropout (predict from many thinned networks), take the average of all the predictions as your prediction and the variance of all the predictions as the uncertainty.\n\nSome excerpts I really enjoyed from Gal\u2019s \u201cWhat My Deep Model Doesn\u2019t Know\u2026\u201d \u2014 absolutely worth a read. Gal\u2019s writing is approachable and lucid. Interactive figures in the webpages help reinforce the concepts.\n\nIt is appealing to make an analogy between dropout and ensembling. It seems that Monte Carlo dropout is truly ensembling, and the weight scaling approximation described is an approximation to this.\n\nFurther reading in \u201cAnalysis of dropout learning regarded as ensemble learning\u201d, Hara et al 2017\n\nI am keenly interested in reading more about Bayesian Deep Learning, perhaps starting with Yarin Gal\u2019s thesis \u201cUncertainty in Deep Learning\u201d, http://bayesiandeeplearning.org/ and/or http://www.gaussianprocess.org/gpml/chapters/\n\nWhat are the authors going to do to answer the specific question(s)?\n\nFor generalization, the authors compared networks with and without dropout on the following datasets, observing that \u201cdropout improved generalization performance on all data sets compared to neural networks that did not use dropout\u201d:\n\nA sparse feature is a feature that that has mostly zero values like a one-hot encoding of categorization or a TFIDF encoding. Think of them in contrast to dense features \u2014 some examples of which are image data, audio data, and word embedding vectors. Sparse features should be more interpretable, as individual neurons will be activated (or individual dimensions given a high value) which correspond to concepts. Think of this like NLP networks with a \u201csentiment neuron\u201d, or the output of VGG where each softmax dimension is a distinct class.\n\nThe authors produce a histogram of activation weights as well as a histogram of the mean activation weights across data samples.\n\nFor sparsity, indeed, we can see that dropout pushes the distribution of activations to skew heavily toward zeroes with very few units showing high activation. The mean activations are lowered from 2.0 to 0.7:\n\nHere is Table 9 from the paper with results:\n\nThe authors find that the weight scaling approximation method is a good approximation of the true model average:\n\nIt is interesting to me that the authors find that Bayesian Neural Networks yield beter performance than dropout networks at reducing overfitting in small dataset regimes, at the cost of additional computation. I was not previously aware of Bayesian Neural Networks and this does pique my interest in learning more:\n\nThe scaling approximation not as good as BNN, but for the one problem they examine (predicting the occurrence of alternative splicing based on RNA features), it\u2019s better than other approaches (early stopping, regression with PCA, SVM with PCA).\n\nI encountered some new terminology in this paper.\n\nI learned that the notation signifies the Normal distribution (aka the Gaussian distribution) with mean and standard deviation .\n\nI still don\u2019t quite grasp how this applies to dropout. Marginalization appears to be defined as removing a subset of variables (or dimensions) by summing over the distributions of the other variables.\n\nSee the Wikipedia page for marginal distribution and this Quora answer:\n\nFrom section 9 of the paper:\n\nThe authors go on to make an analogy to ridge regression \u2014 but ridge regression will regularize weights and will rarely select them out entirely. (Unlike lasso, which will more frequently choose coefficients of zero and thus select variables out entirely. See ISLR 6.2 \u201cThe Variable Selection Property of the Lasso\u201d) I don\u2019t then understand an intuitive relation between ridge regression and dropout. I don\u2019t quite grasp this section 9. The authors go on to say:\n\nI still would like to return to this and figure it out.\n\nThe paper devotes entire sections to examining dropout on RBMs, which I had heard about but never encountered before.\n\nI briefly read the DeepLearning4J article on RBMs and don\u2019t yet completely understand them. What I understand so far:\n\nRBMs can be trained in an unsupervised fashion to perform something called reconstruction, which is different from classification or regression, and which finds the joint probability distribution of the inputs and the outputs. In this case, a forward pass is performed from inputs to activations, and then a backward pass is performed from those activations, through the network weights, back to a \u201creconstruction\u201d output. The distribution of this reconstruction is compared to the distribution of the input with KL divergence. The weights are tuned to minimize this divergence, so the RBM learns to \u201creflect the structure of the input, which is encoded in the activations of the first hidden layer.\u201d\n\nConsider that matrix multiplication is defined for compatible dimensionality; e.g. we know how to multiply an AxB matrix by a second BxA matrix. Broadcasting defines a similar operation with looser constraints on the shape of the operands.\n\nRead more in Rachel Thomas\u2019 All the Linear Algebra You Need for AI\n\nQ: How does the breakup of coadaptation and increase in feature sparsity relate to Hinton\u2019s capsule networks? See forthcoming \u201cDynamic Routing between Capsules\u201d at NIPS 2017."
    },
    {
        "url": "https://medium.com/paper-club/draw-generating-small-images-by-adding-attention-to-variational-autoencoders-430ba241972b?source=---------7",
        "title": "Generating small images by adding attention to variational autoencoders",
        "text": "\u201cWhat problem is this entire field trying to solve?\u201d\n\nGenerative models may not seem like a useful technology in and of themselves, but building a data generating representation IS super useful. And meaningfully different from a representation that solves supervised problems: the former outputs an image and a label, and the latter outputs a label given an input image. Generative models are perhaps the key to learning from data that has never been seen before (like a car that can imagine car crash situations in order to avoid them), which means that maybe they\u2019re key to AGI. Images are a good target for generative modeling experiments because they\u2019re structurally complex and somewhat easier to debug (than text or sequences of decisions).\n\nWhat work has been done before in this field to answer the big question? What are the limitations of that work? What, according to the authors, needs to be done next?\n\nWell, DRAW is a twist on variational autoencoders. Past attempts at image generation involve one pass (Dayan et al., 1995; Hinton & Salakhutdinov, 2006, Larochelle & Murray, 2011), but one-shot approaches don\u2019t scale well to large images.\n\nIn response, a bunch of research suggesting that a series of partial glimpses are better for learning image structure rather than one high-level pass came out (Larochelle & Hinton, 2010, Denil et al., 2012; Tang et al., 2013, Ranzato, 2014; Zheng et al., 2014, Mnih et al., 2014; Ba et al., 2014, Sermanet et al., 2014) \u2014 DRAW draws (heh) on this by working iteratively, instead of in a single pass.\n\nOne other important distinction is that DRAW also incorporates a fully differentiable attention mechanism, which makes it resemble the selective read and write operations of Neural Turing Machines.\n\nWhat exactly are the authors trying to answer with their research? There may be multiple questions, or just one. Write them down. If it\u2019s the kind of research that tests one or more null hypotheses, identify it/them.\n\nCan we improve image generation tasks by taking variational autoencoders and adding a differentiable attention mechanism?\n\nWhat exactly did the authors do?\n\nWe didn\u2019t learn about variational autoencoders before reading this paper, so I\u2019ll build up an explanation of the net piece by piece.\n\nA standard autoencoder has two ends: an encoder RNN and a decoder RNN. They communicate via a \u201ccode\u201d, a series of numbers that encode all training images. One such valid code might be a one-hot encoding. In practice, autoencoder codes are less sparse and use continuous numbers to represent training images. For example, an image of a dog might be encoded by the encoder RNN as [4, 5, 1, 0, 1] (instead of [0, 1, 0, 0, \u2026]). These two RNNs are trained jointly by a loss function that computes the distance between the input image and the output image generated by the decoder. Because the code is lossy, the net will learn to compress like features into similar elements \u2014 in the above example, perhaps a basset hound would be encoded as [4.5, 5, 1, 0, 1] and a golden retriever as [5, 4.8, 1, 0, 1]. Dissimilar images will have very different codes \u2014 perhaps a cat would have [4, 3, 0, 0, 1] and a submarine would have [0, 0, 3, 5, 6].\n\nNow, the decoder is a convolutional net made of deconvolutional layers, and has learned to transform these codes into images. So you could pull a code out of thin air, say [0, 0, 1, 2, 3], and see what image the decoder generates, but if your decoder has no points of reference near that encoding, it\u2019ll probably generate garbage. To generate a similar enough code that your decoder will create interesting results you\u2019d have to pass every training image to your encoder net, collect those codes, compute a mean and standard deviation for all of your observations for every dimension of your code, and sample that multivariate probability distribution \u2014 but that\u2019s a lot of work. It\u2019s much easier to add a term to your loss function that penalizes the encoder net for generating an encoding based on its KL divergence from a unit Gaussian probability distribution. That way, your encoder will learn to output codes that approximately match a unit Gaussian distribution and your decoder will learn to decode them, so you\u2019ll be able to just generate your own codes all day long! No need to do all that analysis on the variation within your training set. This constraint is the only difference between variational autoencoders and regular autoencoders.\n\nOne: It learns attentional parameters that constrain which part of the canvas the net reads from and draws to. The decoder net emits them.\n\nTODO: insert Fig 3.\n\nand two: and it iteratively refines its output for several steps. This is achieved by letting the encoder read all of the decoder\u2019s previous outputs, and adding all of the decoder\u2019s outputs together into one final distribution.\n\nWhat do the authors think the results mean? Do you agree with them? Can you come up with any alternative way of interpreting them? Do the authors identify any weaknesses in their own study? Do you see any that the authors missed? (Don\u2019t assume they\u2019re infallible!) What do they propose to do as a next step? Do you agree with that?\n\nOn the MNIST generation with two digits task, the authors concluded that \u201cthe network typically generates one digit and then the other, suggesting an ability to recreate composite scenes from simple pieces.\u201d I\u2019m not so sure that I agree. If you watch the video of their generation results, the attention seems to meander across the screen, and when two digits are touching, the attention is more likely to wander into the second digit before finishing the first. How can we then conclude that the network has any idea that it understands the separation between the two?\n\nThe authors did not achieve great results on the CIFAR-10 dataset, and blamed these results on CIFAR-10\u2019s small size (50k images) and diverse makeup. I think that\u2019s fair.\n\nDrop any questions you have or would like to discuss here\n\nIs the data available? How much computation? Can the problem be scaled down? How much code development is necessary? How much work to turn this paper into a concrete and useful application? How much will we learn? How do we prove success? What are the results of success?\n\nVery. Because DRAW is made of RNNs on MNIST, I can train the net on my own laptop in ~10 minutes. Additionally, there are tons of existing implementations out there.\n\nDoes it match what the authors said in the paper? Does it fit with your interpretation of the paper?\n\nMore or less \u2014 it describes the architecture simply and states the results fairly.\n\nList any helpful references or citations that helped you understand the paper\n\nList and define any and all words you didn\u2019t previously know."
    },
    {
        "url": "https://medium.com/paper-club/language-modeling-survey-333077e43dd9?source=---------8",
        "title": "Language Modeling Survey \u2013 Paper Club \u2013",
        "text": "TL;DR Use a large, regularized LSTM language model with projection layers and Softmax approximation using importance sampling, trained on a large dataset, to beat state-of-the-art LM results.\n\nOverall impression: I am a fan of this paper. It was thorough, well-researched, and produced impressive results. It introduced a lot of new concepts to me, and did an above-average job of explaining them.\n\nWhat is the most effective way for computers to reason about human language?\n\nWhat work has been done before in this field to answer the big question? What are the limitations of that work? What, according to the authors, needs to be done next?\n\nAs a survey paper, the background is predictably detailed.\n\nLanguage Modeling is the process of assigning distributions over sentences in human language in an effort to accurately encode meaning and complexity. In simpler words, it\u2019s accurately predicting the probability of the next word given a sentence. It is a fundamental task of NLP, and powers many other areas such as speech recognition, machine translation, and text summarization.\n\nTraditional models rely heavily on n-gram features, but recently neural network approaches combined with n-gram features have been producing the most promising results. However, these models are often trained on smaller datasets than their n-gram counterparts. This produces misleading results and models that fall apart when applied to larger datasets.\n\nThe authors mainly build upon the work of LSTM RNNs for language modeling, due to their ability to model long-term dependencies. They also emphasize that they will train their model on the One Billion Word benchmark, which is considerably larger than the \u201cstandard\u201d PTB dataset; they make an analogy to ImageNet, which pushed computer vision forward by providing a much larger dataset to train models on.\n\nSecondarily, the authors recognize convolutional embedding models and softmax over large vocabularies as background for their work in this paper.\n\nConvolutional embedding models offer character-level insights by passing 1-d convolutions over input sequences and max pooling to extract high-signal features.\n\nOne problem with dealing with large vocabularies is the computational bottleneck of assigning probabilities over million-word vocabularies. The authors examine various solutions to this problem in their work, focusing on importance sampling and Noise Contrastive Estimation.\n\nThe first problem the authors address is the large scale softmax problem. They compare and contrast two approaches \u2014 Noise Contrastive Estimation and Importance Sampling (defined under \u201cWords I Don\u2019t Know\u201d).\n\nThey boil the difference between these two approaches into the following: NCE defines a binary classification between \u201ctrue\u201d and \u201cnoise\u201d words with log-loss, while IS defines a multiclass classification with cross-entropy loss.\n\nThe approach they use in their architecture is to use k noise samples and one data distribution sample (this part is from NCE) and train a multiclass loss over a multinomial random variable (this part is from IS).\n\nWith that out of the way, we dive into model architectures, outlined in the diagram above.\n\nColumn (b) represents the first proposed change to a basic RNN LM. The authors aim to further address the computational complexity of the softmax function by pre-computing word embeddings from the component character embeddings.\n\nThis approach initially caused the network to have trouble differentiating between words with similar spellings but different meanings; the authors added a corr_w correction clause to each word in order to correct this.\n\nA benefit of using this approach to word embeddings is the ability to score out-of-vocabulary words.\n\nEven with these improvements, the network is still practically slow. Another possible approach is utilizing char-LSTMs, which make predictions character by character. These are much more efficient but their performance is unacceptably worse. The authors propose combining word- and character-LSTMs by feeding the word-level hidden state into a char-LSTM.\n\nThis model scales independently of vocabulary size, but unfortunately accuracy is still subpar. These are difficult problems to solve!\n\nThe authors tried many different variations of RNN LM architectures using the techniques previously outlined, including CNN Softmax and char-LSTM, models with and without dropout, and models with and without a projection layer.\n\nThey ran all tests using TensorFlow on the 1B Word Benchmark (~ 0.8B words with a vocabulary ~ 800k). For word models, no preprocessing was done, and for character models begin- and end-of-word tokens are added to separate words.\n\nTheir hyperparameter choices were sane and, more importantly, consistent: Adagrad optimizer, learning rate of 0.2, batch size of 128, and weight clipping at 1.0.\n\nThey used 8192 samples from each step to approximate the softmax function. This reduces the factor from 800k, a 100x decrease.\n\nThese are the results the authors uncovered, with novel architectures below existing ones in each table.\n\nThe previous state of the art complexity on the 1B word benchmark was 51.3. The best single model in this paper beat it with a perplexity of 30.0, and an ensemble of models from this paper scored 23.7!\n\nThe authors\u2019 takeaways are as follows:\n\nI would say these results emphatically answer the specific questions. The best results the authors achieved are quite eye-opening to me.\n\nWhat do the authors think the results mean? Do you agree with them? Can you come up with any alternative way of interpreting them? Do the authors identify any weaknesses in their own study? Do you see any that the authors missed? (Don\u2019t assume they\u2019re infallible!) What do they propose to do as a next step? Do you agree with that?\n\nThe authors interpret these results to show that they\u2019ve effectively beaten the state of the art on the 1B Word Benchmark using the following techniques:\n\nThe authors also explored other approaches, such as char-CNN, in an effort to compare and contrast them.\n\nA key takeaway for the authors is the universal applicability of a larger dataset. They encourage further language modeling research to use larger corpuses of text to achieve better results.\n\nThis emphasis on dataset echoes what seems to me like the most pressing issue in deep learning right now: collecting data. For example, Andrew Ng\u2019s $150M AI fund has stated that the majority of their funding will go towards data.\n\nI am a big fan of this paper. I thought it was quite thorough and well-researched, and obviously it produced impressive results. It introduced a lot of new concepts to me, and did a reasonable job of explaining them.\n\nAs it stands, this paper would be most useful to use as an underlying model for the tasks that are built on top of LM, such as speech recognition, machine translation, and text summarization.\n\nThe authors released all of their training data and code, so there is a solid foundation to extend this research.\n\nIt seems like this paper could be useful for the text classification piece of the Personalized Medicine Kaggle challenge, but I\u2019m unsure exactly how at this point.\n\nFor the most part, the abstract matches what is presented in the paper. It calls out corpora sizes and long-term language structure as the two main focuses of the paper, but I feel like the former was emphasized more."
    },
    {
        "url": "https://medium.com/paper-club/remarques-sur-la-traduction-de-la-machine-neurale-en-apprenant-ensemble-%C3%A0-aligner-et-%C3%A0-traduire-cd23004cf207?source=---------9",
        "title": "Remarques sur la traduction de la machine neurale en apprenant ensemble \u00e0 aligner et \u00e0 traduire",
        "text": "Overall impression: this paper proposes a healthy new architecture for neural machine translation, and while it has some holes, the ideas presented are important and encouraging for this young field.\n\nHow can we teach machines how to translate between human languages at higher-than-human accuracy?\n\nBaseline state-of-the-art machine translation results are currently sourced from phrase-based systems which tune sub-components to produce sentence translations.\n\nRecently, neural machine translation approaches have been attempted. The most promising architecture is the encoder-decoder model, which includes an encoder RNN that reads the input sentence to produce a context c, and a decoder RNN that is trained to predict probabilities of the next word from c.\n\nThe biggest pitfall of encoder-decoder models is that they distill sentences of variable lengths into the same constant-length vector size, making it difficult to capture the information in longer sentences. The authors propose an approach that encodes each sentence into a set of more granular subvectors that can be chosen from, rather than one large vector. They call this \u201cjoint alignment and translation\u201d.\n\nThe authors propose a new model architecture for neural machine translation. It is based on the encoder-decoder model.\n\nUnlike previous approaches, the decoder produces probabilities based on distinct context vectors for each target word rather than one for the entire sentence.\n\nThe context is a weighted sum of annotations that are an encoded version of the input sequence. Each annotation is computed via an alignment model which scores how the inputs around a word\u2019s position match the outputs. This is referred to as the \u201cenergy\u201d of an annotation.\n\nThe energy of an annotation is a proxy for its relative importance from the previous state to the current state in deciding the prediction.\n\nThis implements an attention mechanism in the decoder, which decides which annotations are most important and carry the highest signal.\n\nThe encoder is a bidirectional RNN which calculates forwards and backwards hidden states for an input sequence. The annotation for a word is computed by concatenating the forward and backwards hidden states.\n\nThe authors built a model with this architecture for English->French translation, specifically on the ACL WMT \u201914 dataset. This set contains 348M words. They use a list of the 30,000 most frequently occurring words in each language for training.\n\nIn addition to the proposed model, they trained an RNN Encoder-Decoder for comparison. It had 1000 hidden units each in the encoder and decoder.\n\nThe models were trained on two corpuses of text: one with sentences up to 30 words long, and the other with sentences up to 50 words.\n\nThe above table shows the results of both models on the ACL WMT \u201914 dataset. It\u2019s especially notable that RNNsearch-50 outperforms Moses (state-of-the-art phrase-based translation system) on data that contains no unknown words.\n\nThese results also aid the authors\u2019 hypothesis that the flexibility of RNNsearch without having to use a fixed-length context vector would improve performance on longer sentences. RNNsearch-30 is even able outperform RNNencdec-50.\n\nThis visualization of the alignments between source and generated translations is helpful to perform qualitative analysis.\n\nThe benefits of soft alignment can be demonstrated in the translation of \u201cthe man\u201d to \u201cl\u2019homme\u201d (bottom right of figure 3(d)). Hard alignment would separate \u201cl\u2019 \u201d and \u201chomme\u201d completely, but in reality the two must be considered together as they carry an important and related signal.\n\nThe authors also performed a qualitative analysis for their hypothesis regarding long sentences. These examples demonstrate that RNNsearch is better than RNNencdec at retaining context and information from early on in long sentences.\n\nBased on the quantitative and qualitative tests they ran, the authors conclude that their proposed RNNsearch architecture correctly aligns each target word with the relevant source words, which generates better translations than the previous state-of-the-art.\n\nIt\u2019s very encouraging that the RNNsearch matched up well with phrase-based translation systems, especially since neural machine translation approaches had only been around for one year at the time of this paper\u2019s writing.\n\nThe authors believe that the next step is to improve RNNsearch\u2019s performance on unknown tokens, an area in which it still falls well short of phrase-based translation. I agree that this seems like an obvious next step.\n\nThe architecture proposed in this paper is definitely viable for a project. The authors are very thorough with the description of their process, and replicating the steps they took seems quite approachable.\n\nMachine translation work translates (\ud83d\ude09) almost directly to real-world applications; if you were able to verifiably beat the current state of the art, your architecture would simply replace the state-of-the-art applications.\n\nMy only potential cause for concern would be the size of the data (348M words), but my intuition is that commodity hardware could handle this load.\n\nThe abstract hits all of the key points in the paper concisely and accurately, and it fits my interpretation well.\n\nThis paper has a solid 1800 citations at the time I am reading it, just one short year after its release. This is quite promising!"
    },
    {
        "url": "https://medium.com/paper-club/paper-notes-template-1e4dcc631606",
        "title": "Paper Notes Template \u2013 Paper Club \u2013",
        "text": "Overall impression: fill this out last; it should be a distilled, accessible description of your high-level thoughts on the paper.\n\n\u201cWhat problem is this entire field trying to solve?\u201d\n\nWhat work has been done before in this field to answer the big question? What are the limitations of that work? What, according to the authors, needs to be done next?\n\nWhat exactly are the authors trying to answer with their research? There may be multiple questions, or just one. Write them down. If it\u2019s the kind of research that tests one or more null hypotheses, identify it/them.\n\nWhat are the authors going to do to answer the specific question(s)?\n\nWhat exactly did the authors do?\n\nSummarize the results for each experiment, each figure, and each table. Don\u2019t yet try to decide what the results mean, just write down what they are.\n\nThings to pay attention to in the results section:\n\nDo the results answer the SPECIFIC QUESTION(S)? What do you think they mean?\n\nWhat do the authors think the results mean? Do you agree with them? Can you come up with any alternative way of interpreting them? Do the authors identify any weaknesses in their own study? Do you see any that the authors missed? (Don\u2019t assume they\u2019re infallible!) What do they propose to do as a next step? Do you agree with that?\n\nDrop any questions you have or would like to discuss here\n\nIs the data available? How much computation? Can the problem be scaled down? How much code development is necessary? How much work to turn this paper into a concrete and useful application? How much will we learn? How do we prove success? What are the results of success?\n\nDoes it match what the authors said in the paper? Does it fit with your interpretation of the paper?\n\nWho are the (acknowledged or self-proclaimed) experts in this particular field? Do they have criticisms of the study that you haven\u2019t thought of, or do they generally support it?\n\nList any helpful references or citations that helped you understand the paper.\n\nList and define any and all words you didn\u2019t previously know."
    },
    {
        "url": "https://medium.com/paper-club/q-a-from-jeff-deans-lecture-for-yc-ai-4e0c2b7398ab",
        "title": "Q&A From Jeff Dean\u2019s Lecture for YC AI \u2013 Paper Club \u2013",
        "text": "We recently watched Jeff Dean\u2019s Lecture for YC AI for discussion (found here). As the head of Google Brain, he is in a uniquely credible position to provide an overview of the ML/AI landscape at Google and, by extension, the world. His slides did a great job of capturing the value and content of his talk, but I thought that some of his most insightful comments came from the Q&A portion. I\u2019ve roughly transcribed it belwo, for my own reference and hopefully others\u2019 as well.\n\nQ: Are the architectures that perform well from Learn2Learn models (introduced earlier as models that train other models) understandable by humans in a way that can be empirically analyzed?\n\nA: It depends on the problem; sometimes you only want the most accurate model, sometimes you want to be able to justify all of the model\u2019s decisions. Sometimes it\u2019s a balance; for example, when we looked at the results from the optimizer function Learn2Learn model we saw that the optimizer used a lot of the same subfunction e^(sign(gradient) * sign(momentum)). In other words, if the sign is the same as the direction you\u2019ve been going, speed up; if they\u2019re different, slow down. This is a good, helpful intuition.\n\n(follow-up) Q: Will the Learn2Learn paradigm eventually end up as a tool for humans to use, or as the universal way by which neural networks are designed?\n\nA: It could be either or both, but keep in mind that Learn2Learn can run 12,000 experiments in a weekend, and humans cannot.\n\n(follow-up) Q: Are there instances where there isn\u2019t enough human-labelled data to support 12,000 experiments?\n\nA: For one of our machine translation tasks, we trained using hundreds of GPUs for a week, and only got through 1/6th of the training data we had available. This varies by the problem.\n\n(follow-up) Q: Do you see a path in the near future to building powerful models from datasets that are much smaller than what Google has access to?\n\nA: Right now, when faced with a problem for which there is little data available, we reach for manual transfer learning. For example, for a specific image classification problem we would start with ImageNet and finetune from there. This is lame. Eventually, we want to get to a point where we have networks that can do 10,000 things, and when the 10,001st thing comes along the network is able to use what it already knows to solve the new problem with much less data. So, working on these more generalizable models is the ultimate way to solve the data efficiency problem.\n\nQ: What do your best engineers do when they\u2019re waiting for models to train?\n\nThey work on other experiments or write code or whiteboard things. At a higher level, this re-emphasizes the importance of reducing the feedback loop for our experiments, which helps mitigate this scenario altogether.\n\nQ: What would you attribute the gap in translation quality to between languages? Is it a data problem?\n\nA: Some language pairs are more natural to translate because they are part of the same family or character set. But ultimately, higher accuracy models will come from using all of the translation data we have available, powered by new massive amounts of specialized compute resources.\n\nQ: Can you brute-force the best neural network architecture for certain tasks more easily than others?\n\nA: We haven\u2019t trained neural networks on enough tasks to make conclusions about this yet. For supervised machine learning tasks, the problem here is compute; there are only a finite number of architectures you can attempt. Model-generating models do hold promise here. The architecture search that these models perform will improve over time as they see similar problems and, for example, learn which class of problems are best suited to convolutional architectures.\n\nQ: How often do you re-train and re-evaluate architectures for pre-trained Cloud ML products like Vision?\n\nA: It varies by domain. For example, the vision API is relatively stable but advertisement analysis changes a lot because the types of ads that are in circulation change constantly. Generally we try not to over-iterate on these production models since it causes issues with integrating into the existing product.\n\nQ: How much of cutting-edge neural network development is trial-and-error vs. deterministic decision making?\n\nA: It\u2019s still mostly empirical analysis, but over time researchers can develop an intuition for certain scenarios that helps a lot.\n\nQ: A few years ago it seemed like a large focus was on the interpretability of models, leading to research such as \u201cVisualizing and Understanding Convolutional Neural Networks\u201d. Does this new excitement about Learn2Learn models which produce black-box results run counter to that goal?\n\nA: You\u2019re right, and I skipped over some of the stuff related to this in order to shorten my talk. Interpretability is still very important, and we are still researching it. A prime example is in the health care field. If a model recommends that a patient receive a heart valve replacement, it needs to be able to point to a specific historical patient file in order to back that up.\n\nQ: Why did you get into machine learning in the first place six years ago?\n\nA: I started seeing more and more mentions of neural nets in paper abstracts. That, combined with my undergrad thesis that neural nets are the correct machine learning abstraction, and talking with Andrew Ng (professor at Stanford) while he was contracting at Google, led me to believe it would be a good field to invest in.\n\nQ: What are the biggest barriers right now to building neural nets that can reason?\n\nA: The problem, again, is that right now neural nets are trained to do only one thing. In order to reason, you need a network to be able to bring in knowledge from several different areas, such as math, science, and philosophy, to reach reasonable conclusions on what it\u2019s been tasked with.\n\nAlong with previous problem is the issue that the entire network is activated for every problem right now. Eventually we should aim for large networks with trillions of parameters that only activate 1% or 5% of their neurons for any given input. This is more in line with how the human brain functions.\n\nQ: How will network architecture research be affected by Learn2Learn models?\n\nA: They exist symbiotically; if a researcher discovers a novel new architecture, it can be added to the search space for a Learn2Learn model.\n\nQ: What are the coolest thing neural nets are being applied to right now?\n\nA: Health care. The ability of neural networks to ingest lots of data and make predictions is very well suited to this area, and potentially will have a huge societal impact.\n\nI\u2019m also very interested in the art generation we\u2019ve done, and pleased that the automatic image captioning work has produced great results earlier than I expected."
    },
    {
        "url": "https://medium.com/paper-club/fasttext-bc181f50a452",
        "title": "fastText \ud83c\udfc3 \u2013 Paper Club \u2013",
        "text": "Overall impression: This paper functioned well as a bite-sized introduction of a new architecture that delivered exactly what was promised: a bag of tricks for efficient text classification. It timidly reaches for larger conclusions, but stops short of anything concrete.\n\nHow can we improve the accuracy and speed with which computers extract meaning from text?\n\nTraditional text classification methods are centered around linear classifiers, which scale to large corpuses and approximate state-of-the-art results when tuned correctly. Recent approaches have been centered around neural networks, which achieve noticeable improvements in accuracy but are slower to train and test.\n\nThe authors will attempt to improve on the performance of basic linear classifiers with the key features of rank constraint and fast loss approximation.\n\nThe ngram features of the input are first looked up to find word representations, then averaged into hidden text representations, which go into a linear classifier and finally a softmax output.\n\nThe classifier trains on multiple CPUs with SGD and a linearly decaying learning rate.\n\nTwo nuances that are employed in this architecture are the hierarchical softmax function to improve performance with a large number of classes (training time goes from linear to logarithmic since the classes are arranged into a tree), and (when using n-grams) the hashtag trick to manage mappings of n-grams to local word order.\n\nThe authors constructed a model with the above architecture and evaluated it on sentiment analysis and tag prediction. They compare their results to various machine learning techniques, including several neural network architectures.\n\nThese are the results and training times for sentiment analysis tasks. The fastText model trains an order of magnitude faster than the neural network implementations, while achieving parity on accuracy.\n\nAnd these are the results for the tag prediction tasks. With the large data corpuses, these tasks demonstrate the scalability of fastText compared to the neural network. There is a significant speed-up, again without much compromise in accuracy.\n\nThe authors note that performance could be further improved using pre-trained word embeddings.\n\nThe results do answer the specific question, and they even go beyond to question the premise that a baseline necessarily will perform worse than the candidate(s).\n\nThe authors show that the fastText architecture provides comparable accuracy to neural networks while being orders of magnitude faster to train and test.\n\nThey conjecture that this demonstrates that text classification problems might not be the best domain for neural networks in practice, despite the theoretical demonstration of their higher representational power.\n\nI think this paper functioned well as a bite-sized introduction of a new architecture that delivered exactly what was promised: a bag of tricks for efficient text classification.\n\nThe fastText architecture could certainly be applied to the Personalized Medicine competition on Kaggle (https://www.kaggle.com/c/msk-redefining-cancer-treatment), but it loses a lot of the scalability benefits since the data corpus is quite manageable in this competition. I would more likely revisit this for future projects with huge datasets.\n\nThe authors released their code here and encouraged others to build on it; there\u2019s not much interest on my end, but there are plenty of extension points for this research.\n\nThe abstract does match the paper. To nitpick, I\u2019d say that the statistics presented in the abstract don\u2019t mean much out of context, but this context is available later in the paper."
    },
    {
        "url": "https://medium.com/paper-club/bag-of-tricks-for-efficient-text-classification-818bc47e90f",
        "title": "Bag of Tricks for Efficient Text Classification \u2013 Paper Club \u2013",
        "text": "Neural networks are typically a good choice for text classification problems in NLP. They tend to perform very well however they are slow to train to their usefulness on large datasets is limited. Linear classifiers can do very well on text classification problems if the right features are selected. However these classifiers are limited because linear classifiers(e.g. SVM) don\u2019t share parameters between features. When the output space is large, this prevents the classifiers from leveraging information from other parameters and generalizing well. Work that combines the two approaches, neural networks that train quickly on large datasets using linear connections has not yet been explored.\n\nThe model, fastText. The set of features x are made up of N ngram features in the sentence. The advantage of using ngrams is that you capture information about local word ordering.\n\nFor example, bag of words is ngram with N = 1 and completely disregards ordering. An ngram with N = 2 would take into account words that are adjacent to each other. Higher values of N are more computationally expensive but capture larger amounts of information about ordering.\n\nThe negative log likelihood is minimized with the following function\n\nThe output is passed through a hierarchical softmax classifier to improve runtime of the model. A typical softmax classifier has complexity O(kh) where k is number of classes and h is the dimensions of text representation. This makes sense when you consider the matrix needed to map the text representation to the classifier is of size k*h. A hierarchical softmax allows you to reduce this complexity to O(h*log2(k))\n\nThe authors follow the evaluation protocol of Zhang et al. (2015). They evaluate against 8 datasets and include four other architectures also tested against those datasets.\n\nThe results indicate that fastText is competitive, beating char-CNN, char-CRNN and not quite as good as the VDCNN\n\nngrams of 5 are used to achieve the results below.\n\nComparing training speeds shows that fastText trains significantly faster than the other models.\n\nThe authors used 100M images with titles, captions and tags. They trained their model by using the titles and captions to predict the tags. They have also released a script that builds the data they used so you can reproduce their results.\n\nThe authors compare against a different model for predicting tags called tagspace and show considerable improvements in accuracy when using bigrams and much faster performance\n\nWhat is data augmentation in the context of text classification? \u201cFor char-CNN, we show the best reported numbers without data augmentation.\u201d\n\nWhy can\u2019t word2vec features be averages together to create sentence representations? \u201cUnlike unsupervisedly trained word vectors from word2vec, our word features can be averaged together to form good sentence representations\u201d\n\nWhy don\u2019t linear classifiers share features among classes? \u201cHowever, linear classifiers do not share parameters among features and classes.\u201d\n\nWhat do they mean by \u201cfactorize the linear classifier into low rank matrices\u201d"
    },
    {
        "url": "https://medium.com/paper-club/grus-vs-lstms-e9d8e2484848",
        "title": "GRUs vs. LSTMs \u2013 Paper Club \u2013",
        "text": "Notes on Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n\nOverall impression: The authors seem to recognize that their study does not produce any novel ideas or breakthroughs (that\u2019s okay! Not every study needs to). They prove that gated units are superior to vanilla recurrent units, which is a hypothesis that had already been independently proven. They are unable to clearly distinguish between the performance of the two gated units they tested. This paper helped most to solidify my interpretation of the definitions of, similarities, and differences between GRUs and LSTMs, and also re-set my flawed assumption that GRUs are superior just because they were developed later.\n\nWhat techniques can be used to avoid the exploding and vanishing gradient problems in recurrent neural networks, thereby increasing their practicality and usefulness?\n\nRecurrent Neural Networks (RNNs) were proposed several decades ago as an architecture that handles variable-length sequential input by way of a recurrent, shared hidden state. However, they were mostly impractical due to the vanishing and exploding gradient problems during training until the introduction of the long-short term memory (LSTM) recurrent unit as a more complex activation function to confidently capture long-term dependencies.\n\nThe LSTM was followed by the Gated Recurrent Unit (GRU) and both have the same goal of tracking long-term dependencies effectively while mitigating the vanishing/exploding gradient problems. The LSTM does so via input, forget, and output gates; the input gate regulates how much of the new cell state to keep, the forget gate regulates how much of the existing memory to forget, and the output gate regulates how much of the cell state should be exposed to the next layers of the network. The GRU operates using a reset gate and an update gate. The reset gate sits between the previous activation and the next candidate activation to forget previous state, and the update gate decides how much of the candidate activation to use in updating the cell state.\n\nBoth LSTMs and GRUs have the ability to keep memory/state from previous activations rather than replacing the entire activation like a vanilla RNN, allowing them to remember features for a long time and allowing backpropagation to happen through multiple bounded nonlinearities, which reduces the likelihood of the vanishing gradient.\n\nLSTMs control the exposure of memory content (cell state) while GRUs expose the entire cell state to other units in the network. The LSTM unit has separate input and forget gates, while the GRU performs both of these operations together via its reset gate.\n\nThe authors will build an LSTM model, a GRU model, and a vanilla RNN model and compare their performances using a log-likelihood loss function over polyphonic music modelling and speech signal modelling datasets.\n\nThe authors built models for each of their three test units (LSTM, GRU, tanh) along the following criteria:\n\nThey tested their models across four music datasets and two speech datasets.\n\nFor the music datasets, all of the models performed relatively closely, with the GRU-RNN inching a bit ahead. For the speech datasets, the gated units well outperformed the tanh unit, with the GRU-RNN once again producing the best results both in terms of accuracy and training time. As this is the more challenging task, the authors derive more signal from it.\n\nHowever, the results are not enough to declare a winner between LSTMs and GRUs; this suggests that one or the other might be best suited to a given task based on the description of the task.\n\nThe authors seem to recognize that their study does not produce any novel ideas or breakthroughs (that\u2019s okay! Not every study needs to). They prove that gated units are superior to vanilla recurrent units, which is a hypothesis that had already been independently proven. They are unable to clearly distinguish between the performance of the two gated units they tested. This paper helped most to solidify my interpretation of the definitions of, similarities, and differences between GRUs and LSTMs, and also re-set my flawed assumption that GRUs are superior just because they were developed later.\n\nThey do not point out flaws in their own study.\n\nThey vaguely propose further testing and experimentation on these gated units, perhaps on different datasets or tasks, as a next step.\n\nThis paper is comparing two existing techniques/architectures rather than proposing any novel ideas, so unless one were inclined to continue this research it would not be directly applicable as a project.\n\nThe abstract does match what the authors said in the paper, and it does fit with my interpretation of the paper.\n\nIt is a bit unsatisfying for the conclusion to be that GRUs and LSTMs are \u201ccomparable\u201d, but I suppose that\u2019s better than manufacturing a reason for one to be superior to the other."
    },
    {
        "url": "https://medium.com/paper-club/cnns-for-text-classification-b45bde0bb254",
        "title": "CNNs for text classification \u2013 Paper Club \u2013",
        "text": "Paper Club\u2019s paper this week was Recurrent Convolutional Neural Networks for Text Classification. James Vanneman has already done an excellent job analyzing it, so I\u2019ll keep my own analysis short and instead provide a little context. Was this paper justified in its conclusions? What performance can we achieve on NLP tasks given a reasonable CNN baseline, and how much value does the \u201cRecurrent\u201d part of an RCNN add?\n\nText classification tasks generally involve classifying a sentence (i.e., into one of 5 sentiments, or into one of 6 question types, etc) and require scanning across the sentence and building up a representation of its constituent words. As a word\u2019s meaning is heavily influenced by its surroundings, it\u2019s important to scan across the sentence with an aperture that considers multiple words at a time. Consider the word \u201cwindow\u201d. You would probably assume that it was referring to a transparent feature of a wall, no? You might even come to the same conclusion if you saw 1 word of context to either side: \u201csmall window sizes,\u201d though perhaps you\u2019d be less confident. Expanding the context to 3 words on either side: \u201cclaim that small window sizes lose information\u201d, and now you realize that \u201cwindow\u201d has a completely different meaning than you had originally surmised. We call the nearby phrase \u201close information\u201d discriminative because it caused us to change our understanding of this usage of \u201cwindow\u201d.\n\nOf the approaches for capturing this context (as presented by the RCNN paper), all have drawbacks:\n\n1. recursive neural nets have O(n\u00b2) complexity, making them unsuitable for large tasks\n\n2. recurrent neural nets are biased, in that later words are more dominant than earlier words, and\n\n3. convolutional neural nets that scan over long segments of text at a time (\u201cfixed window kernels with large window sizes\u201d) could work, but these take a long time to train.\n\nSo the authors propose that we capture the benefits of both recurrent NNs with convolutional NNs by putting a max pooling layer on top of a bidirectional recurrent NN.\n\nSkipping right to the results of their experiments:\n\nFor which my main takeaway was shock that a plain CNN did as well as it did (2nd row from the bottom), as did ordinary NLP benchmarks (the top 4 rows).\n\nEven crazier, this CNN benchmark is from a 2011 paper by Collobert et. all, released 1 year before AlexNet won ImageNet and convinced everyone that neural nets were worth researching. I skimmed the paper \u2014 it includes quaint anachronisms like explaining the basics of neural nets: \u201cThe features computed by the deep layers of the network are automatically trained by backpropagation to be relevant to the task\u201d, which tells you a thing about the research community\u2019s knowledge of NNs in 2011, and later refers to \u201cmultilayer neural networks\u201d almost apologetically as \u201ctwenty year old technology\u201d. In all it seems hardly fair to use this CNN for text classification as a baseline \u2014 I think they should have compared their RCNN to an actual peer CNN. I did find such a paper \u2014 I discuss it below, but first, a tangent about the Collobert paper!\n\nThe Collobert paper is a 47-page beast and still genuinely impressive, especially for the time it was published. It was written at a time when neural networks were fairly unknown and NLP consisted of more manual feature engineering; the authors spend the second half of the paper enumerating the results of combining various old-school NLP techniques with their neural net and often obtain improvements in classification accuracy. Here are summaries of what they tried, as best as I could understand their piles of jargon and acronyms:\n\nHowever, while the authors explored the additional benefit of these techniques on top of their primitive CNN, the paper is titled \u201cNatural Language Processing (Almost) From Scratch\u201d, and emphasized how much they could achieve without manual language feature engineering. So the baseline they establish does not include the use of the above techniques.\n\nAnyway, I wish that the authors of the RCNN paper had mentioned that their results were already eclipsed by another, similar approach to CNNs, that was more up-to-date than the Collobert paper. Here is the conclusion of Convolutional Neural Networks for Sentence Classification, published in 2014 (a year before the RCNN paper) and now cited by 1010 other authors:\n\nWhich shows a better result on the Stanford Sentiment Treebank task (48.0) than the RCNN (47.21), the only NLP task these two papers had in common, despite the RCNN research being released a year later.\n\nIn all, I tend to agree with the very humble conclusion of the CNN for Sentence Classification paper:\n\n\u201cDespite little tuning of hyperparameters, a simple CNN with one layer of convolution performs remarkably well. Our results add to the well-established evidence that unsupervised pre-training of word vectors is an important ingredient in deep learning for NLP.\u201d\n\nWhich is surprising to me, given how amazing RNNs seem given the classic Karpathy blog post (wish I could see how many times that\u2019s been cited)."
    },
    {
        "url": "https://medium.com/paper-club/recurrent-convolutional-neural-networks-for-text-classification-107020765e52",
        "title": "Recurrent Convolutional Neural Networks for Text Classification",
        "text": "The development of word embeddings has allowed neural networks to make large advances in NLP related tasks. Embeddings are superior to previous features used in text processing such as Bag of Words. Recursive Neural Networks capture information about sentences in trees but are inefficient to construct O(n\u00b2). Recurrent Neural networks capture contextual information by maintaining a state of all previous inputs. The problem with RNNs is that they\u2019re a biased and favor more recent inputs\n\nCNNs can learn important words or phrases through selection via a max pooling layer. However processing text is difficult with CNNs because learning an optimal kernel size is challenging.\n\nThe core of the model is creating a word representation(y\u00b9) that consists of the left side context, word embedding, and right side context.\n\nThe Left context is constructed from a forward RNN, and the right context is structure from a reverse RNN.\n\ny\u00b2 represents the result of the word representation passed through a standard neural network layer\u2014weight matrix multiplication plus a bias term passed through a tanh activation function.\n\nThe max pooling operation takes the most important feature from each word representation. The final layer is computer with W*y\u00b3 + b and passed through a softmax activation function for classification.\n\nThe training target of the network attempts to maximize the log likelihood of a given class. The weights of the network are initialized from a uniform distribution where the maximum value is the square root of the fan-in.\n\nThe skip-gram model is used to pre-compute the word embeddings\n\nThe model was tested against several well known datasets and compared against frequently used models as well as current state of the art approaches.\n\nBoth Neural Network approaches outperform clearly outperform the baselines. The authors claim that Convolutional(CNN and RCNN) approaches are better than RecursiveNN approach because the RecursiveNN\u2019s ability to predict the correct sentiment relies on a properly constructed tree to represent the sentiment. Since tree construction is O(n\u00b2) performance is a main limiting factor. The authors follow on to say that training time is 3\u20135 hours for RecursiveNN and several minutes for their RCNN.\n\nOn all four datasets, the RCNN approach was superior to the CNN approach. To test this further, the authors tried various kernel sizes on the CNN.\n\nThe authors say that small window sizes lose out on picking up long distance patterns, while larger window sizes suffer from data sparsity. It seems that regardless of kernel size, the RCNN approach is superior.\n\nIn Both the 20New and Fudan datasets, the authors achieve state of the art results.\n\nGiven the standardized datasets this could definitely be reproduced as a project. I think the interesting part about doing this as a project would be to use the 2014 paper\u2019s CNN as a comparison against the proposed RCNN and see if their results hold."
    },
    {
        "url": "https://medium.com/paper-club/hierarchical-multiscale-recurrent-neural-networks-9e614e4fb04",
        "title": "Hierarchical Multiscale Recurrent Neural Networks \u2013 Paper Club \u2013",
        "text": "This week we chose to review a relatively recent paper on RNNs, https://arxiv.org/abs/1609.01704.\n\nPrevious work has been done that shows models self organizing in a hierarchical structure as well as RNNs whose layers are stacked in reverse order of update frequency. Stacking the layers in reverse order of update frequency lets the model build up higher and higher levels of abstraction, word -> sentence -> paragraph -> chapter. These are known as multiscale networks because the layers represent different levels of abstraction. Self organization prevents the need to manually describe each layers role and allows the network to learn the relevant roles by itself. Other models have been build such that the layer hierarchy is hardcoded, meaning that layers update at a fixed interval that is a function of the layer number. This has the clear disadvantage in that the network doesn\u2019t learn a hierarchy and thus it is likely not to be optimal. Despite the recent successes of RNNs, previous work is lacking because RNNs have not yet been shown to self organize and identify hierarchical structures when the data is unbounded.\n\nIn order to understand the novel architecture introduced in this paper, let\u2019s do a quick review of the LSTM architecture.\n\nAt every iteration of input to the LSTM it does the following:\n\nAn important point here is that these updates occur at every step. The act of \n\n\u201cforgetting\u201d with the forget gate is nuanced \u2014 the model forgets certain parts of the previous cell state but not necessarily all of it.\n\nAn RNN can also be viewed in its unrolled form:\n\nThe architecture in this paper introduces an HM(hierarchical multiscale)-LSTM that looks like this in the unrolled form:\n\nThey key distinction here is that the dotted lines represent updates that don\u2019t happen. The model has an additional parameter that it passes between each layer called a binary boundary detector(we\u2019ll call this Z). This simply passes a value of 0 or 1 to the layer above. Let\u2019s talk about these updates in the context of the second layer in the picture above(not drawn). The first layer looks for words, and the second layer is detecting phrases. This is the HM part of the title in the paper. Hierarchy because of the stacked layers and multiscale because phrases take a bigger picture view than words.\n\nThe Flush operation here is much different than a typical RNN because it completely erases all information obtained in this cell\n\nMore formally, these operations are defined by the following formulas.\n\nAn interesting thing to note is that the binary boundary detector is either 0 or 1 and as is therefor non-differentiable. In order to compute the gradient during back propagation, the authors used a method called straight-through estimator (Hinton 2012) which basically replaces the step-wise function with a differentiable one.\n\nPerformance of the model was measured against writing samples because a clear hierarchy exists\u2014 words, phrases etc. Against the Text8 dataset, data obtained from wikipedia that only has letters and spaces, the model achieves state of the art performances:\n\nOn the Hutter Prize wikipedia dataset, the model ties previous NN state of the art results and on Penn Treeback comes close but does not beat current state of the art (1.24 BPC vs 1.23 BPC).\n\nVisualizing the results we can see that the first layer typically fires on word boundaries.\n\nOn the Penn Treebank data we can see the first layer capturing prefixes (\u201ctele\u201d) and the second layer capturing larger sentences.\n\nThe model was also tested against handwriting sequence generation and performed better than a standard LSTM."
    },
    {
        "url": "https://medium.com/paper-club/multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding-6f71bc7d0566",
        "title": "A clever and powerful way to combine multimodal feature representations",
        "text": "First of all, here\u2019s a link to the paper. I chose to review this one because it has achieved state-of-the-art results on visual question answering and visual grounding tasks, which are just plain cool as far as AI datasets go.\n\nHow do we best identify salient areas of and answer questions about images?\n\nWe can create good representations of images using convolutional nets and good representations of phrases and sentences using RNNs, so it makes sense that a \u201cvisual question answering\u201d (VQA) task or a \u201cvisual grounding\u201d task should involve combining these representations somehow. Typical approaches use vector concatenation, element-wise vector summing, or element-wise vector multiplication to combine these representations. The authors suspect these approaches are too simplistic to fully capture the relationships between images and text (makes sense), but are reluctant to use bilinear pooling (AKA a full outer product of vectors \u2014 see definitions of terms at the bottom of this article), because the number of resulting parameters is too high to be practical (in this case, our image and text vectors are of length 2048, so the resulting matrix would have 2048\u00b2 elements, and we\u2019d need to fully connect that matrix to 3000 classes, resulting in ~12.5 billion learnable parameters). The authors here apply an existing technique for capturing the discriminating abilities of bilinear pooling with only a a few thousand parameters (16k) while preserving backpropability using a recently invented technique called multimodal compact bilinear pooling (MCB).\n\nCan we improve the state of the art in visual question answering and visual grounding tasks with this clever approach to bilinear pooling?\n\nThe authors developed a model that uses MCB at three different points where visual and textual representations need to be combined:\n\n1. to predict spatial attention in the image\n\n2. to predict an answer to the question\n\n3. to relate the encoded multiple-choice answers to the combined question+image space (this only applies to the multiple choice problem).\n\nThey also use attention maps and additional training data, which feels a bit like sacrificing experimental purity to achieve state-of-the-art results.\n\n> Like, is it a vector of dimensionality 2048, or a vector with 2048 elements, where each element has 3000 dimensions? Is that a 2048-dimensional vector, a 3000-dimensional vector, or a 6,144,000-dimensional vector? Why don\u2019t we call this thing a matrix instead of a vector??Here\u2019s the architecture breakdown:\n\nVisual question answering: Inputs are an image and a question, and the output is one of 3k classes.\n\nThe image representations are trained using the 152-layer ResNet model, pretrained on ImageNet. They threw out the final 1,000-way fully connected layer, used L2 norm on the second-to-last layer\u2019s output (the \u201cpool5\u201d layer); this results in a 2048-dimensional vector.\n\nThe questions are tokenized into words, one-hot encoded, and passed through a learned embedding layer followed by a 2-layer LSTM, each outputting a 1024-dimensional vector, which are then concatenated to form a 2048-dimensional vector.\n\nThe two pieces are passed through a 16,000-dimensional MCB and fully connected to the 3,000 top answers.\n\nAttention and visual grounding: inputs are a phrase and an image, and the output is a set of bounding boxes. I need to read one or two of these papers to understand this bit:\n\n1. Xu et al., 2015 \u201cShow, attend and tell: Neural image caption generation with visual attention\u201d\n\n2. Xu and Saenko, 2016: \u201cAsk, attend and answer: Exploring question-guided spa- tial attention for visual question answering\u201d\n\n3. Yang et al., 2015: \u201cStacked attention networks for image question answering\u201d\n\nBut I\u2019ll take a stab at explaining the result anyway: it appears that there are \u201cspatial grid locations\u201d in certain layers of the conv net; the technique relies on merging these visual representations with the language representation and predicting \u201cattention weight\u201d. From the paper: \u201cPredicting attention maps\u2026 allows the model to effectively learn how to attend to salient locations based on both the visual and language representations.\u201d\n\nMultiple choice answering: answers are encoded with a word embedding and LSTM layers, with the LSTM weights shared across all the answers. These are then merged with MCB and fully connected to N classes, where N is the number of multiple choice answers.\n\nVisual question answering: significant improvement on the previous year\u2019s state-of-the-art (7.9%), moderate improvement over this year\u2019s 2nd place competing model (<1%), but most of this improvement is due to incorporating attentional maps (and presumably some is due to the additional training data). Thankfully, they omitted parts of the model so we can see that MCB accounts for about 3% improvement in the results.\n\nVisual grounding: moderate improvement in the state of the art: slightly under 1% improvement on each of the two VG datasets.\n\nThe authors were trying to win the competition; thankfully, they also broke down their results by technique. Seems like MCB is worth considering in pretty much any model that involves combining similarly dimensional vectors \u2014 it\u2019s an improvement over the standard techniques of concatenation, addition, or multiplication because it captures more information about the relationship between the multimodal representations. As an added benefit, because it\u2019s based on element-wise multiplication, any number of similarly-sized vectors can be added efficiently.\n\nIs the data available? Yes.\n\nHow much computation? Not so much \u2014 they rely on pre-trained ImageNet weights for their convnet, and presumably used pretrained word embeddings as well.\n\nCan the problem be scaled down? Yes, you could test the viability of alternate approaches using subsets of the Visual7W/Visual Genome/VQA datasets.\n\nHow much code development? Their code is available on Github! Nice!\n\nHow much work to turn this paper into a concrete and useful application? Well, if we\u2019re specifically talking about improving the state of the art for the tasks discussed in the paper:\n\nBut the larger principle \u2014 that MCB is a superior approach to combining different types of information if you buy their methodology) is useful for any mixed-modality task.\n\nThe trickiest part about all this is that MCB requires that vectors are the same length. But that might be impractical \u2014 imagine a self-driving plane, which combines visual data from a video camera on the wings with less complex instrument \u2014 perhaps a wind sensor on the nose. Preferably you would be able to combine these representations efficiently (i.e., without using an outer product) but without requiring that your wind sensor output a representation as highly dimensional as your video camera. I understand why the authors chose to test combining vectors with MCB on a VQA task \u2014 language and imagery are similarly complex and so warrant output vectors of similar dimensionality \u2014 it\u2019s a perfect application. A technique for efficiently combining vectors of uneven dimensionality would be a good point of research.\n\nI didn\u2019t find many reviews of this paper, but I did find reviews of a later one criticized for being highly similar to this one, and the reviewer\u2019s comments are relevant enough to be worth mentioning here:"
    }
]