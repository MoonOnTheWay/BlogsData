[
    {
        "url": "https://hackernoon.com/reinforcement-learning-and-supervised-learning-a-brief-comparison-1b6d68c45ffa?source=user_profile---------1----------------",
        "title": "Reinforcement Learning and Supervised Learning: A brief comparison",
        "text": "Most beginners in Machine Learning start with learning Supervised Learning techniques such as classification and regression. However, one of the most important paradigms in Machine Learning is Reinforcement Learning (RL) which is able to tackle many challenging tasks. One example is the game of Go which has been played by a RL agent that managed to beat the world\u2019s best players.\n\nMany have heard about RL but don\u2019t actually know what makes it different from Supervised Learning. They get confused by the two paradigms and why they co-exist. This post is intended to clarify the differences and introduce how Deep Learning fits into the picture.\n\nLet\u2019s start off with the most important question: Why should we care about RL?\n\nSupervised Learning can address a lot of interesting problems, from classifying images to translating text. Now let\u2019s look at problems like playing games or teaching a robot limb to grab objects. Why can\u2019t we do this properly with Supervised Learning?\n\nConsider the case of playing Go. Suppose we had a data set that contained the history of all Go games played by humans. Then we could use as input X the game state and as output labels Y the optimal moves that are taken for that state. In theory that sounds nice but in practice a few issues arise.\n\n1. Data sets like this don\u2019t exist for all domains we care about\n\n2. Creating such as a data set might be expensive and unfeasible\n\n3. The approach learns to imitate a human expert instead of actually learning the best possible strategy\n\nRL comes to the rescue here. Intuitively, RL attempts to learn actions by trial and error. We learn the optimal strategy by sampling actions and then observing which one leads to our desired outcome. In contrast to the supervised approach, we learn this optimal action not from a label but from a time-delayed label called a reward. This scalar value tells us whether the outcome of whatever we did was good or bad. Hence, the goal of RL is to take actions in order to maximize reward.\n\nMathematically, a RL problem can be seen as a Markov Decision Process. This process is memoryless, so everything we care about we know through the current state. The RL setup can be visualized like this:\n\nThere is an agent in an environment that takes actions and in turn receives rewards. Let\u2019s briefly review the supervised learning task to clarify the difference.\n\nIn Supervised Learning, given a bunch of input data X and labels Y we are learning a function f: X \u2192 Y that maps X (e.g. images) to Y (e.g. class label). The function will be able to predict Y from novel input data with a certain accuracy if the training process converged.\n\nNow let\u2019s move on to the RL setup, which is defined by the 5-tuple (S,A,P,R,\ud835\udefe). We are given a set of states S and a set of actions A. P is the state transition probability. The reward is a value that tells us how good we did in terms of the goal we want to optimize towards. It is given by a reward function R: S\u00d7A \u2192 R. We will come to \ud835\udefe in a bit.\n\nThe task is to learn a function \u03c0: S \u2192 A that maps from states to actions. This function is called the policy function. The objective is now to find an optimal policy that maximizes the expected sum of rewards. This is also called the control problem.\n\nThe game of Go can be modeled with this approach in the following way:\n\nActions: Where the player put its piece down\n\nReward: 1 if the player wins at the end the game, 0 otherwise\n\nThe problem with the rewards in RL is that we don\u2019t know which action had the deciding effect on the outcome. Was it the move we made three actions before or the current one? We call this the credit assignment problem.\n\nTo deal with this problem, the discount factor \ud835\udefe \u2208 (0, 1] is introduced to calculate the optimal policy \u03c0*. Our optimization problem is maximizing the expected sum of discounted rewards. Thus, the optimal policy can be found by calculating the result of this equation:\n\nIntuitively, we\u2019re blaming each action assuming that its effects have exponentially decaying impact into the future.\n\nTo learn the optimal policy, there are different approaches such as policy gradient and Q-Learning. While policy gradient tries to learn the policy directly, Q-Learning is learning a function of state-action pairs. I will delay a detailed explanation of these algorithms to a future post.\n\nIn Supervised Learning, we use Deep Learning because it is unfeasible to manually engineer features for unstructured data such as images or text. In RL, we use deep learning largely for the same reason. With neural networks, RL problems can be tackled without need for much domain knowledge.\n\nTo exemplify this, consider the game of Pong. In traditional learning, we need to extract features from the game positions to gain meaningful information. Using neural networks we can feed the raw game pixels into the algorithm and let it create high-level non-linear representations of the data.\n\nFor doing this, we construct a policy network that is trained end-to-end, meaning that we input our game states and out comes a probability distribution over possible actions we can take.\n\nIf we consider the example of Pong, the action is either going UP or DOWN. This is an example setup from learning how to play Pong:\n\nAt first glance this might look the same way as a typical supervised learning setup, for example for image classification. Remind yourself, however, that we don\u2019t have labels for each game state given and thus we can\u2019t just train this network in the same easy fashion.\n\nI hope this post allowed you to gain a better intuition about the difference between Supervised Learning and RL. Both approaches have their rightful place and there are many success stories. In the future, I\u2019ll explain in more depth how RL systems are trained. For anyone that wants to learn more, I\u2019ve attached some resources that I personally found useful."
    },
    {
        "url": "https://medium.com/@sebwagner/should-we-be-afraid-of-artificial-intelligence-28a3e8fd96ab?source=user_profile---------2----------------",
        "title": "Should we be afraid of artificial intelligence? \u2013 Sebastian Wagner \u2013",
        "text": "Should we be afraid of artificial intelligence?\n\nArtificial intelligence (AI) systems have not only led to improvements in many areas of research such as medicine, autonomous driving and text processing but also steadily entered our daily lives, for example through voice assistants such as Siri. Although many of us have a latent feeling that AI is somehow threatening, we don\u2019t often base this feeling on technical facts. In this post, I want to lay out, in layman\u2019s terms, what AI\u2019s risks are and why our future depends on understanding them.\n\nAI is everywhere, and often dressed in sheep\u2019s clothes. The Snapchat filter that adds dog\u2019s ears to your selfie? That\u2019s AI, right there on your smartphone, and called face detection. Whether it\u2019s social media or medical imaging, most of us, if we like it or not, have grown dependent on AI systems. The main drivers of recent progress in AI technology are decades of exponential growth in computing power, availability of large data sets which are used to train learning systems, advances in the implementation of learning algorithms and increasing investment from industry.\n\nHas it had a largely positive net effect? I would argue yes. So far the advantages have largely outgrown the disadvantages. We should be happy that a lot of smart people are doing amazing things with these systems. But even though potential risks of AI might seem unclear and somehow far away, I will in the next sections explain why they are in fact very imminent and require much more thought and resources than we allocate to it at the moment.\n\nThe AI community makes an important distinction between the concept of narrow AIs and general AIs, also called Artificial General Intelligence.\n\nAI systems that we know today such as the ones deployed in self-driving cars or in your smartphone are all instances of narrow AIs. The term \u201cnarrow\u201d refers here to the ability to only perform a specific task that the AI was designed for. So for instance, a narrow AI for self-driving cars can steer your car over the highway but fails at investing your stocks. But what would happen if systems become capable of all tasks that humans can perform? As a loose definition of general AI we can say they are\n\nMost experts believe that such a technology is at least a few decades away. This can mean that it comes in a lifetime or two, depending if one is bullish about mid-term technological progress. This is a very small timeframe in the large scale of things.\n\nThere are strong reasons to believe that a system which reaches our level of cognitive ability does not stop becoming better. If we were able to improve intelligent systems, we can employ AI agents that work on new AI technology. At some point these agents will probably come up with an AI system that is superhuman, so is at least as good as humans in all tasks and better in some.\n\nImagine having a few Albert Einsteins in silicon. Such an armada of intelligent agents will probably produce fast progress in a lot of fields, including medicine, physics and genetics. We should be happy, right?\n\nThe problem we ignored so far is that a general AI will probably do its own moral reasoning. As Oxford philosopher Nick Bostrom argues, a superintelligence will likely even surpass us in moral thinking. These moral motivations should be encoded by the creators a priori, because a superintelligence might be so powerful that it can\u2019t be stopped by humans. So as a society, we have to think about which values we want to have encoded in these systems and if we are willing to accept them after a point of no return.\n\nOne of the main points that politicians and media outlets mention about AI is its potential to make redundant a large number of jobs. Why is this a problem? One might argue that the destruction of jobs will necessarily lead to the creation of jobs in other areas, potentially higher up the value chain and requiring more abstract thoughts that machines are not (yet) capable of. Even if we suppose that this argument is true, it will still lead to a period of societal transformation with large potential for political and economical instability, where resentments among the population might grow and populist parties gain even more traction.\n\nTo quote U.S. Treasury Secretary Larry Summers:\n\nSome thinkers go even so far as to predict a time where most tasks will probably be solved by a general AI and we as human can largely just enjoy the fruits. As I see it, this is largely a question of timeline. Before we have created a superhuman intelligence, we probably have to do most of the work ourselves but many jobs will be replaced by very focused, specialized systems. Then, when we have created a superhuman system, most of the jobs will be taken over by AI except the ones that really require human-to-human connections.\n\nIn any case, like the industrial revolution, AI will reshape the relationship between capital and labor in the world economy. It is possible that an edge in technology of a country is more important in the future than population size for international power. Governments and organizations have to prepare for this next industrial revolution and it won\u2019t be easy.\n\nOne of the main drivers of current advances in AI systems are artificial neural networks. Those are loosely inspired by what we know how the human brain brain works. More specifically, they are a family of algorithms that can \u2014 with little manual engineering \u2014 learn how to solve a variety of problems. Even though we are able to optimize them with some human intuition, it would be overconfident to say that we really can understand how they come up with their predictions.\n\nThere is a whole area of research which deals with interpreting the decision boundaries of neural networks. This shows that they are largely a black box that \u2018magically\u2019 transforms input to output. In the end, we are just happy that they work so well.\n\nSimpler algorithms can somehow be interpreted and are somehow intuitive to our understanding of making decisions. If we bet the future of the field on neural networks and reinforcement learning algorithms however, we must place great importance on ensuring the safety of these systems.\n\nOkay, let\u2019s assume we have created a benign AI that we\u2019ve aligned with our good human values. Nothing can go wrong, right? I wouldn\u2019t be so sure about that. In fact, alarming messages are coming from AI researchers who explain that current algorithms are prone to many risks which arise from technical design issues.\n\nAs these authors from Google Brain and Stanford point out in their paper:\n\nBesides the issues specific to constructing learning agents, there are all the issues arising from insecure IT systems that we have seen increasingly in the last decade. Let\u2019s say, there is a swarm of autonomous armed drones flying around for military use and someone finds weaknesses in the algorithms. Could we potentially end up with an army of drones that was benign before but can be turned against their creators? Before we \u201csolve\u201d IT security, we can\u2019t be confident in our ability to control intelligent systems fully.\n\nWe have touched upon several aspects of potentially dangerous AI systems and the question remains: What do we do about it? Should we be paralyzed by our fear and just stop developing AI systems altogether?\n\nIn my point of view, we shouldn\u2019t let ourselves be forced into panic. AI will probably make a huge positive impact on us individually and society at large. The transformation towards more health and life quality is going to be significant.\n\nNevertheless, we should put time and money into ensuring that AI systems of the future will ultimately benefit us. The Machine Learning community, which I\u2019m part of, has to work really hard to make sure our algorithms do what we want them to do and are aligned with human values.\n\nIf you\u2019re interested in AI and Machine Learning, stay tuned! More content to come soon on my Medium blog. Thanks to all the people who helped with proof-reading my drafts."
    }
]