[
    {
        "url": "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc?source=user_profile---------1----------------",
        "title": "Logistic Regression \u2014 Detailed Overview \u2013",
        "text": "Logistic Regression was used in the biological sciences in early twentieth century. It was then used in many social science applications. Logistic Regression is used when the dependent variable(target) is categorical.\n\nConsider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.\n\nFrom this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.\n\nIf \u2018Z\u2019 goes to infinity, Y(predicted) will become 1 and if \u2018Z\u2019 goes to negative infinity, Y(predicted) will become 0.\n\nThe output from the hypothesis is the estimated probability. This is used to infer how confident can predicted value be actual value when given an input X. Consider the below example,\n\nBased on the x1 value, let\u2019s say we obtained the estimated probability to be 0.8. This tells that there is 80% chance that an email will be spam.\n\nMathematically this can be written as,\n\nThis justifies the name \u2018logistic regression\u2019. Data is fit into linear regression model, which then be acted upon by a logistic function predicting the target categorical dependent variable.\n\nThe categorical response has only two 2 possible outcomes. Example: Spam or Not\n\nThree or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n\nThree or more categories with ordering. Example: Movie rating from 1 to 5\n\nTo predict which class a data belongs, a threshold can be set. Based upon this threshold, the obtained estimated probability is classified into classes.\n\nSay, if predicted_value \u2265 0.5, then classify email as spam else as not spam.\n\nDecision boundary can be linear or non-linear. Polynomial order can be increased to get complex decision boundary.\n\nWhy cost function which has been used for linear can not be used for logistic?\n\nLinear regression uses mean squared error as its cost function. If this is used for logistic regression, then it will be a non-convex function of parameters (theta). Gradient descent will converge into global minimum only if the function is convex.\n\nThis negative function is because when we train, we need to maximize the probability by minimizing loss function. Decreasing the cost will increase the maximum likelihood assuming that samples are drawn from an identically independent distribution.\n\nTrain and test accuracy of the system is 100 %\n\nThis implementation is for binary logistic regression. For data with more than 2 classes, softmax regression has to be used."
    },
    {
        "url": "https://medium.com/@saishruthi.tn/gradient-descent-algorithms-cefa1945a774?source=user_profile---------2----------------",
        "title": "Gradient Descent Algorithms \u2013 Saishruthi Swaminathan \u2013",
        "text": "Gradient descent algorithm is one of the most popular optimization algorithms for finding optimal parameters for the model. Goal is to find the parameter which minimize the cost function.\n\nLocal gradient of the error function with respect to the parameter is measured and it goes in the direction of decreasing gradient. Cost function should decrease after every iteration. When the decrease in cost function is less than 10^ (-3), it indicates that minimum point is approaching or very near.\n\nLearning rate is the one of the important hyper-parameters that controls how big the steps are during gradient descent. Smaller the learning rate, algorithm takes smaller steps in the process of reaching global minimum. If the learning rate is large, it may overshoot minimum and fail to converge. At times, it may even diverge.\n\nIf there are more than one feature, then feature scaling will help gradient descent to converge quickly. Two feature scaling methods can be applied.\n\n1. Normalization \u2014 This is used to adjust the differences among attributes in terms of frequency of occurrence, mean, range and variance. Replace x(i) with x(i) \u2014 Mean(x).\n\n2. Standardization \u2014 This is done by subtracting the mean and dividing by standard deviation.\n\nDerivative of error function with respect to the parameters\n\nThree types of gradient descent algorithms are discussed below\n\nAs we approach minimum, gradient descent will take only small steps. There is no need to decrease learning rate over time.\n\nCost function with respect to number of iteration\n\nCost function go up and down. There is only gradual decrease in the cost function on an average.\n\nThis can help in coming out of local minimum and chances of finding global minimum is not as perfect as batch gradient method. But by decreasing learning rate over the time can help reaching the perfect minimum point.\n\nProcess of changing the learning rate at every step is called as simulated annealing. The function which determines the learning rate is called as learning schedule.\n\nAs discussed, cost function goes up and down for every iteration\n\nGradient descent is performed on small random sets. It is Less erratic. It can get quite closer to minimum as they reduce variance of parameter update. This leads to more stable updates."
    },
    {
        "url": "https://medium.com/@saishruthi.tn/is-r-sqaure-value-always-between-0-to-1-36a8d17807d1?source=user_profile---------3----------------",
        "title": "Is R-Square value always between 0 to 1? \u2013 Saishruthi Swaminathan \u2013",
        "text": "R-square value gives the measure of how much variance is explained by model. For a given set of points, the default regression line with minimum sum of square is the horizontal line that passes through the mean. This horizontal regression line denotes that there is no information that can be obtained from the data. If a model can not be designed better, at least it can get into the mean value yielding the minimum square error.\n\nR-Square value can be defined using three other errors terms.\n\nIt is the summation (for all the data points) of square of difference between the actual and the predicted value.\n\nIt is the summation (all data points) of square of difference between actual output and average value \u2018Y(bar)\u2019\n\nIt is the summation (for all the data points) of square of difference between the predicted and the average value \u2018Y(bar)\u2019.\n\nAdding and Subtracting (predicted_value) from TSS, we get\n\nThese two terms are ESS and RSS and the equation becomes,\n\nDividing both the sides by \u2018TSS\u2019\n\nThe formula for \u2018R-Square\u2019 is,\n\nOn examining the equation 1 and 2, it can be observed that when regression line is plotted with intercept, equation 2 can be replaced by (ESS/TSS). From this equation, it can be inferred that R2 can have maximum value of \u20181\u2019. But minimum value can below 0 and its explanation is given below. Reiterating the points,\n\nBut, when we do not specify intercept the below term will not be equal to zero.\n\nThis can also be understood as, value of R2 may end up being negative if the regression line is made to pass through a point forcefully. This will lead to forcefully making regression line to pass through the origin (no intercept) giving an error higher than the error produced by the horizontal line. This will happen if the data is far away from the origin.\n\nWhen the above term is not equal to 0, then R2 can become negative (either of the terms become negative). This tells that the horizontal line is better than the obtained regression line.\n\nTherefore, range of R2 can range from (-infinity to 1) not (0 to 1) or (-1 to 1)"
    },
    {
        "url": "https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86?source=user_profile---------4----------------",
        "title": "Linear Regression \u2014 Detailed View \u2013",
        "text": "Linear regression is used for finding linear relationship between target and one or more predictors. There are two types of linear regression- Simple and Multiple.\n\nSimple linear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and other is response or dependent variable. It looks for statistical relationship but not deterministic relationship. Relationship between two variables is said to be deterministic if one variable can be accurately expressed by the other. For example, using temperature in degree Celsius it is possible to accurately predict Fahrenheit. Statistical relationship is not accurate in determining relationship between two variables. For example, relationship between height and weight.\n\nThe core idea is to obtain a line that best fits the data. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line.\n\nWe have a dataset which contains information about relationship between \u2018number of hours studied\u2019 and \u2018marks obtained\u2019. Many students have been observed and their hours of study and grade are recorded. This will be our training data. Goal is to design a model that can predict marks if given the number of hours studied. Using the training data, a regression line is obtained which will give minimum error. This linear equation is then used for any new data. That is, if we give number of hours studied by a student as an input, our model should predict their mark with minimum error.\n\nThe values b0 and b1 must be chosen so that they minimize the error. If sum of squared error is taken as a metric to evaluate the model, then goal to obtain a line that best reduces the error.\n\nIf we don\u2019t square the error, then positive and negative point will cancel out each other.\n\nFor model with one predictor,\n\nApart from above equation co-efficient of the model can also be calculated from normal equation.\n\nTheta contains co-efficient of all predictors including constant term \u2018b0\u2019. Normal equation performs computation by taking inverse of input matrix. Complexity of the computation will increase as the number of features increase. It gets very slow when number of features grow large.\n\nBelow is the python implementation of the equation.\n\nComplexity of the normal equation makes it difficult to use, this is where gradient descent method comes into picture. Partial derivative of the cost function with respect to the parameter can give optimal co-efficient value.\n\nRandomness and unpredictability are the two main components of a regression model.\n\nDeterministic part is covered by the predictor variable in the model. Stochastic part reveals the fact that the expected and observed value is unpredictable. There will always be some information that are missed to cover. This information can be obtained from the residual information.\n\nLet\u2019s explain the concept of residue through an example. Consider, we have a dataset which predicts sales of juice when given a temperature of place. Value predicted from regression equation will always have some difference with the actual value. Sales will not match exactly with the true output value. This difference is called as residue.\n\nResidual plot helps in analyzing the model using the values of residues. It is plotted between predicted values and residue. Their values are standardized. The distance of the point from 0 specifies how bad the prediction was for that value. If the value is positive, then the prediction is low. If the value is negative, then the prediction is high. 0 value indicates prefect prediction. Detecting residual pattern can improve the model.\n\nNon-random pattern of the residual plot indicates that the model is,\n\nThis value ranges from 0 to 1. Value \u20181\u2019 indicates predictor perfectly accounts for all the variation in Y. Value \u20180\u2019 indicates that predictor \u2018x\u2019 accounts for no variation in \u2018y\u2019.\n\nThis gives information about how far estimated regression line is from the horizontal \u2018no relationship\u2019 line (average of actual output).\n\nHow much the target value varies around the regression line (predicted value).\n\nThis tells how much the data point move around the mean.\n\nIs the range of R-Square always between 0 to 1?\n\nValue of R2 may end up being negative if the regression line is made to pass through a point forcefully. This will lead to forcefully making regression line to pass through the origin (no intercept) giving an error higher than the error produced by the horizontal line. This will happen if the data is far away from the origin.\n\nThis is related to value of \u2018r-squared\u2019 which can be observed from the notation itself. It ranges from -1 to 1.\n\nIf the value of b1 is negative, then \u2018r\u2019 is negative whereas if the value of \u2018b1\u2019 is positive then, \u2018r\u2019 is positive. It is unitless.\n\nNull hypothesis is the initial claim that researcher specify using previous research or knowledge.\n\nLow P-value: Rejects null hypothesis indicating that the predictor value is related to the response\n\nHigh P-value: Changes in predictor are not associated with change in target"
    },
    {
        "url": "https://medium.com/@saishruthi.tn/support-vector-machine-using-numpy-846f83f4183d?source=user_profile---------5----------------",
        "title": "Support Vector Machine \u2014 Using Numpy \u2013 Saishruthi Swaminathan \u2013",
        "text": "Support Vector Machine is used for finding an optimal hyperplane that maximizes margin between classes. SVM\u2019s are most commonly used for classification problem. They can also be used for regression, outlier detection and clustering. SVM works great for a small data sets.\n\nThere are two classes in the below example. One is denoted by \u2018- \u2018and other by \u2018+\u2019. Both the classes are plotted on a 2D graph and separated using randomly guessed hyperplane. Using SVM, a decision boundary can be drawn that can best separates two classes. This decision boundary is called as a hyperplane. Hyperplane is a linear decision surface. For 2-dimensional space, hyperplane will be (2\u20131) 1 dimension. Similarly, for a three-dimensional space, hyperplane will be (3\u20131) 2-dimensional.\n\nGiven two or more labelled classes of data, a discriminative classifier can be created using support vector machines. Decision boundary is drawn by maximizing the margin(space) between the line(hyperplane) and classes. Points that are closest to the decision boundary are called support. They are called so because they support the creation of the hyperplane. If these points move, the decision boundary will move as well. Maximizing the space and placing a hyperplane between classes can greatly increases the chance of new point falling into its correct class category.\n\nNot all the time data points are linearly separable due to which the line which separates classes cannot be linear. Kernel mapping can be used in this case so that input space is mapped to a feature space making the hyperplane linear. Sometimes, support vector classifiers instead of finding the maximum margin it will allow some observations to be on the wrong side. This is called as soft margin as they allow few points to be on the wrong side.\n\nGoal is to find the function that best represents the relationship between the variable. Weights are updated through optimization technique and gradient descent is the one used here. Optimization is nothing but minimizing the loss or an error function.\n\nHinge loss is a very popular loss function for SVM\n\nObjective is to minimize the following function\n\nHere, first term is the regularizer and second term is the hinge loss calculated for all the data points.\n\nGradient descent is done by taking partial derivative of both terms.\n\nGiven a test input, its class is predicted. Negative sign indicates that the point is below the hyperplane and positive sign indicates that the point lies above the plane.\n\nThanks to siraj for an awesome tutorial"
    },
    {
        "url": "https://medium.com/@saishruthi.tn/math-behind-gradient-descent-4d66eb96d68d?source=user_profile---------6----------------",
        "title": "Math behind Gradient Descent \u2013 Saishruthi Swaminathan \u2013",
        "text": "There are many ways to frame the learning process. Easiest way is to use labelled data. For a given input, it is necessary to learn how these are mapped to the output. To measure the performance, an error function is defined.\n\nGiven an input, prediction is made. This prediction is then compared with the actual output and their difference will give the accuracy of the model. Parameters used for the predictions must be updated during every iteration. Parameters are changed until the system learns mapping and gives significant decrease in the error value.\n\nUpdating parameter is called optimization. Here is an example which has x-y pairs, where \u2018x\u2019 represents the distance a person travels in bike and \u2018y\u2019 represents the number of calories that they have lost.\n\nBelow is the graphical representation of the input and output.\n\nTask is to find a function that can effectively map the input to output. Since, the data points are scattered it is not possible to represent it using a straight-line. Best solution is to draw a line that will best fit and pass through as many as data points as possible.\n\nOptimum value for \u2018b\u2019 and \u2018m\u2019 must be found so that the obtained line can best fit as many as data points possible and also output obtained from these values will be nearly equal to the actual value.\n\nSumming up all the errors can give final error value for the function. Initially \u2018b\u2019 and \u2018m\u2019 are assigned 0.\n\nOutput error value and line when slope and intercept are set as 0\n\nIterating through different values of slope and intercept can yield different error values. Out of all values, there will be one point where error value will be minimum and parameters corresponding to this value will yield the optimal solution.\n\nNow, how to descend in this graph to reach the minimum point. Finding the slope at a given point can do this. This slope will point in a direction that can lead to the minimum of the graph.\n\nIn calculus, this is called derivative of a function. Partial derivative with respect to both \u2018b\u2019 and \u2018m\u2019 are calculated.\n\n\u2018b\u2019 and \u2018m\u2019 are 0, error value is 5565.11\n\nObtained line using the above parameter is,"
    },
    {
        "url": "https://medium.com/@saishruthi.tn/handling-sparse-matrix-concept-behind-compressed-sparse-row-csr-matrix-4fe6abe58a7a?source=user_profile---------7----------------",
        "title": "Handling Sparse matrix \u2014 Concept behind Compressed Sparse Row (CSR) matrix",
        "text": "There are two types of matrices that are common \u2014 Dense and Sparse\n\nSparse matrix is the one which has most of the elements as zeros as opposed to dense which has most of the elements as non-zeros. Provided with large matrix, it is common that most of the elements are zeros. Therefore, it makes sense to use only non-zero values to perform operations as zero times anything will always give zero.\n\nScipy offers variety of sparse matrices functions that store only non-zero elements. By doing so, memory required for data storage can be minimized. Machine learning process often requires data frame to be in memory. It breaks down the data frame for fitting into RAM. By compressing, data can easily fit in RAM. Performing operations using only non-zero values of the sparse matrix can greatly increase execution speed of the algorithm.\n\nCompressed Sparse Row(CSR) algorithm is one of the types of provided by Scipy. Below is how it works.\n\nThis is not a short sentence\n\nIt is not raining. Is it?\n\nAssigning numbers to the words. If words are repeating, assign the same numbers. From this step, we know how many words are present in the entire document.\n\nIndexing starts with 0. First word is \u2018short\u2019 and it gets index as \u20180\u2019 likewise every unique word will be indexed. Word \u2018Short\u2019 occurs twice because of which it gets same index value \u20180\u2019 every time it occurs in the document.\n\nFor each line in the document, vector representation is created. Count number of unique indexes. In this case, we have 8 indexes ranging from 0 to 7, so each document(line) is represented using 8 values with each value represents number of times a particular word corresponding to the index occurs.\n\nHere, value 1 in first position of first vector indicates that word with index \u20180\u2019 which is \u2018short\u2019 occurs 1 time in the document. Value 2 in 4th position of 3rd vector indicates that the word with index \u20183\u2019 which is \u2018is\u2019 occurs two times in that document.\n\nBelow is the sparse matrix representation of each document. It removes all the zero value and store only the non-zero values.\n\nFirst document has values only in first two positions, so only they are represented. (0 1) indicates 0th position has value as 1 and (1 1) indicates 1st position has value 1. In (0 1) of <0 1, 1 1>, 0 corresponds to index whereas 1 corresponds to number of times particular index occurs in the document(value)\n\nCSR matrix representation requires three arrays \u2014 Index, value and pointer.\n\nIndex obtained for each document from the above step is combined as one. Size of the index is the number of non-zeros present in the document.\n\nValue array contains value corresponding to each index obtained from each document. Size of the index is the number of non-zeros present in the document.\n\nNow, that we have combined index and value for all lines present in the document. Pointer helps in identifying beginning and end of each document(line) in the complete text document in index and value array.\n\n0 indicates start of document1(line1) and it ends two positions after the start. From third position, document2(line2) starts and it ends at 8th position. From 9th position, document3(line3) starts and it ends in 12th position. This applies to both index and value arrays. Pointer helps in understanding index and value arrays. Given an index and pointer array, it is possible to tell beginning and end of each document.\n\nPassing these arrays will give CSR matrix and this is how csr_matrix function in scipy works.\n\nThis is how CSR matrix is created when handling text documents."
    },
    {
        "url": "https://medium.com/@saishruthi.tn/data-mining-introduction-data-preprocessing-5080be604f96?source=user_profile---------8----------------",
        "title": "Data Mining Introduction \u2014 Data Preprocessing \u2013 Saishruthi Swaminathan \u2013",
        "text": "Data preprocessing is crucial in any data mining process as they directly impact success rate of the project. This reduces complexity of the data under analysis as data in real world is unclean.\n\nData is said to be unclean if it is missing attribute, attribute values, contain noise or outliers and duplicate or wrong data. Presence of any of these will degrade quality of the results.\n\nHere are few important data pre-processing techniques that can be performed before getting into algorithm selection.\n\nThis combines two or more attributes into a single attribute. Purpose of aggregation can be\n\nThe probability of selecting any item is same. Sampling can be done in two ways one is with replacement and other is without replacement.\n\nData is split into several partitions and then random samples are drawn from each partition.\n\nData sparsity increases as the dimensionality increases which makes operations like clustering, outlier detection less meaningful as they greatly depend on density and distance between points.\n\nPurpose of dimensionality reduction is to,\n\nNew features can be created which can best capture important and relevant information effectively than the provided attributes. Three general feature creation methodologies are"
    },
    {
        "url": "https://medium.com/@saishruthi.tn/data-mining-a-gentle-introduction-78efac19834e?source=user_profile---------9----------------",
        "title": "Data Mining \u2014 A Gentle Introduction \u2013 Saishruthi Swaminathan \u2013",
        "text": "Data mining is the process of handling large data sets and identify interesting as well as meaningful patterns and relationships between them to solve real world problems. This helps business organizations to make accurate and on-time business decisions leading to tremendous increase in profit. This has become an integral part of every business as they help organization gain an edge over others.\n\nData mining tasks are basically divided into two methods\n\nDescriptive methods are used to find human-interpretable patterns that best describes the data. Clustering, pattern mining etc. come under descriptive methods.\n\nPredictive methods are used to predict values of other variables. Recommender systems, Time-series analysis etc. come under predictive methods\n\nData is the collection of data objects and their attributes. An attribute is the property of an object and collection of such attributes define an object.\n\nAttribute can be divided to four major types and they are Nominal, Ordinal, Interval and Ratio. These types depend on the following properties\n\nThis has the property of being distinct. They neither have inherent order nor have sequence.\n\nThis attribute has the property of both distinctness and order. It is not possible to infer meaningful differences between them.\n\nThis attribute has the property of distinctness, order and the differences between the values are meaningful.\n\nThis attribute has the property of distinctness, order, meaningful differences along with meaningful ratios."
    }
]