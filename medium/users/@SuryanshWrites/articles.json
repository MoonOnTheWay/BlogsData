[
    {
        "url": "https://towardsdatascience.com/nns-aynk-c34efe37f15a?source=user_profile---------1----------------",
        "title": "Neural Networks: All YOU Need to Know \u2013",
        "text": "Since, I do not want to bore you with a lot of history about NNs, I will only be going over their history, very briefly. Here\u2019s a Wiki article on the topic if you want more in-depth knowledge on their history. This section is majorly based off of the wiki article.\n\nIt all started when Warren McCulloch and Walter Pitts created the first model of an NN in 1943. Their model was purely based on mathematics and algorithms and couldn\u2019t be tested due to the lack of computational resources.\n\nLater on, in 1958, Frank Rosenblatt created the first ever model that could do pattern recognition. This would change it all. The Perceptron. However, he only gave the notation and the model. The actual model still could not be tested. There were relatively minor researches done before this.\n\nThe first NNs that could be tested and had many layers were published by Alexey Ivakhnenko and Lapa in 1965.\n\nAfter these, the research on NNs stagnated due to high feasibility of Machine Learning models. This was done by Marvin Minsky and Seymour Papert in 1969.\n\nThis stagnation however, was relatively short-termed as 6 years later in 1975 Paul Werbos came up with Back-propagation, which solved the XOR problem and in general made NN learning more efficient.\n\nMax-pooling was later introduced in 1992 which helped with 3D object recognition as it helped with least shift invariance and tolerance to deformation.\n\nBetween 2009 and 2012, Recurrent NNs and Deep Feed Forward NNs created by J\u00fcrgen Schmidhuber\u2019s research group went on to win 8 international competitions in pattern recognition and machine learning.\n\nIn 2011, Deep NNs started incorporating convolutional layers with max-pooling layers whose output was then passed to several fully connected layers which were followed by an output layer. These are called Convolutional Neural Networks.\n\nThere have been some more researches done after these but these are the main topics one should know about.\n\nA good way to think of an NN is as a composite function. You give it some input and it gives you some output.\n\nThere are 3 parts that make up the architecture of a basic NN. These are:\n\nAll of the things mentioned above are what you need to construct the bare bones architecture of an NN.\n\nYou can think of these as the building blocks/bricks of a building. Depending on how you want the building to function, you will arrange the bricks and vice versa. The cement can be thought of as the weights. No matter how strong your weights are, if you don\u2019t have a good amount of bricks for the problem at hand, the building will crumble to the ground. However, you can just get the building to function with minimal accuracy(using the least amount of bricks) and then, progressively build upon that architecture to solve a problem.\n\nI will talk more about the weights, biases, and units in later section. Those sections might be short but the sections are there to emphasize their importance.\n\nBeing the least important out of the three parts of an NNs architectures, these are functions which contain weights and biases in them and wait for the data to come them. After the data arrives, they, perform some computations and then use an activation function to restrict the data to a range(mostly).\n\nThink of these units as a box containing the weights and the biases. The box is open from 2 ends. One end receives data, the other end outputs the modified data. The data then starts to come into the box, the box then multiplies the weights with the data and then adds a bias to the multiplied data. This is a single unit which can also be thought of as a function. This function is similar to this, which is the function template for a straight line:\n\nImagine having multiple of these. Having more than 2 of these could promote non-linearity in an NN. Since now, you will be computing multiple outputs for the same data-point(input). These outputs then get sent to another unit as well which then computes the final output of the NN.\n\nIf all of this flew past you then, keep reading and you should be able to understand more.\n\nBeing the most important part of an NN, these(and the biases) are the numbers the NN has to learn in order to generalize to a problem. That is all you need to know at this point.\n\nThese numbers represent what the NN \u201cthinks\u201d it should add after multiplying the weights with the data. Of course, these are always wrong but the NN then learns the optimal biases as well.\n\nThese are the values which you have to manually set. If you think of an NN as a machine, the nobs that change the behavior of the machine would be the hyper-parameters of the NN.\n\nYou can read another one of my articles here(Genetic Algorithms + Neural Networks = Best of Both Worlds) to find out how to make your computer learn the \u201coptimal\u201d hyper-parameters for an NN.\n\nThese are also known as mapping functions. They take some input on the x-axis and output a value in a restricted range(mostly). They are used to convert large outputs from the units into a smaller value, most of the times. Your choice of an activation function can drastically improve or hinder the performance of your NN. You can choose different activation functions for different units if you like.\n\nHere are some common activation functions:\n\nThese are what help an NN gain complexity in any problem. Increasing layers(with units) can increase the non-linearity of the output of an NN.\n\nEach layer contains some amount of Units. The amount in most cases is entirely up to the creator. However, having too many layers for a simple task can unnecessarily increase its complexity and in most cases decrease its accuracy. The opposite also holds true.\n\nThere are 2 layers which every NN has. Those are the input and output layers. Any layer in between those is called a hidden layer. The NN in the picture shown below contains an input layer(with 8 units), an output layer(with 4 units) and 3 hidden layers with each containing 9 units.\n\nAn NN with 2 or more hidden layers with each layer containing a large amount of units is called a Deep Neural Network which has spawned a new field of learning called Deep Learning. The NN shown in the picture is one such example.\n\nThe most common way to teach an NN to generalize to a problem is to use Gradient Descent. Since I have already written an elaborate article on this topic which you can read to fully understand GD(Gradient Descent), I will not be explaining GD in this article. Here\u2019s the GD article: Gradient Descent: All YOU Need to Know.\n\nCoupled with GD another common way to teach an NN is to use Back-Propagation. Using this, the error at the output layer of the NN is propagated backwards using the chain rule from calculus. This for a beginner can be very challenging to understand without a good grasp on calculus so don\u2019t get overwhelmed by it. Click here to view an article that really helped me when I was struggling with Back-Propagation. It took me over a day and a half to figure out what was going on when the errors were being propagated backwards.\n\nThere are many different caveats in training an NN. However, going over them in an article meant for beginners would be highly tedious and unnecessarily overwhelming for the beginners.\n\nTo explain how everything is managed in a project, I have created a JupyterNotebook containing a small NN which learns the XOR logic gate. Click here to view the notebook.\n\nAfter viewing and understand what is happening in the notebook, you should have a general idea of how a basic NN is constructed.\n\nThe training data in the NN created in the notebook is arranged in a matrix. This is how data is generally arranged in. The dimensions of the matrices shown in different projects might vary.\n\nUsually with large amounts of data, the data gets split into 2 categories: the training data(60%) and the test data(40%). The NN then trains on the training data and then tests its accuracy on the test data.\n\nIf you still can\u2019t understand what\u2019s going on, I recommend looking at the links to resources provided below."
    },
    {
        "url": "https://towardsdatascience.com/gas-and-nns-6a41f1e8146d?source=user_profile---------2----------------",
        "title": "Genetic Algorithms + Neural Networks = Best of Both Worlds",
        "text": "Genetic Algorithms were very popular before NNs. Since, NNs required a lot of data, and GAs didn\u2019t. GAs were used mostly to simulate environments and behaviors of entities in a population. They were mostly used to learn the path to a problem which we knew the answer to.\n\nGAs are still used today but Machine Learning(ML) has mostly taken over.\n\nIf this still doesn\u2019t sink in, then I\u2019m sure Daniel Shiffman\u2019s playlist of GAs will help. It helped me learn how GAs work and Shiffman\u2019s videos are really good in general. Although I do recommend speeding them up.\n\nNNs have helped us solve so many problems. But there\u2019s a huge problem that they still have. Hyperparameters! These are the only values that can not be learned\u2026 Until now.\n\nWe can use GAs to learn the best hyper-parameters for a NN! This is absolutely awesome!\n\nNow, we don\u2019t have to worry about \u201cknowing the right hyperparameters\u201d since, they can be learned using a GA. Also, we can use this to learn the parameter\u2019s(weights) of a NN as well.\n\nSince, in a GA, the entities learn the optimum genome for the specified problem, here, the genome of each NN will be its set of hyper-parameters.\n\nTo solve the hyper-parameter problem, we need to do the following:\n\nPerforming all the steps above, at the end of the latest generation, your algorithm will have found the population containing a NN with the optimum hyper-parameters. It will be the fittest NN of all in the population. The picture shown below attempts to explain the process.\n\nThis being a good solution to learning your hyper-parameters, it does come with its own problems. The 2 most prominent being the problem of computational resources and time.\n\nTraining many NNs simultaneously or one by one, several times requires a lot of time and computational resources. This limits the implementation of this solution to only the people who are willing to put in the money and buy a lot of processing power.\n\nThis is also the reason why it\u2019s widely used by large companies.\n\nA little less than a year ago, OpenAIs Dota 2 bot beat a pro Dota 2 player(Article and YouTube video). What took the player years to learn and master, took the bot only a few weeks. In a video I saw on YouTube, an OpenAI Engineer explained how they trained the bot.\n\nThey used a GA to train their bot. Since, they had the processing power necessary, they were able to run multiple instances of the Dota 2 simultaneously with each having an instance of the bot playing the game. It took them 2 weeks to teach the bot with this process.\n\nJust imagine how much processing power that must have required.\n\nIn my opinion, GAs are good to teach a NN but they will not be my first choice. Instead, I will try to look for better ways to learn the hyper-parameters of a NN. If there are any, that is.\n\nHowever, if in the future I get access to a lot of processing power, I will be sure to try this method out."
    },
    {
        "url": "https://hackernoon.com/gradient-descent-aynk-7cbe95a778da?source=user_profile---------3----------------",
        "title": "Gradient Descent: All You Need to Know \u2013",
        "text": "Gradient Descent requires a cost function(there are many types of cost functions). We need this cost function because we want to minimize it. Minimizing any function means finding the deepest valley in that function. Keep in mind that, the cost function is used to monitor the error in predictions of an ML model. So minimizing this, basically means getting to the lowest error value possible or increasing the accuracy of the model. In short, We increase the accuracy by iterating over a training data set while tweaking the parameters(the weights and biases) of our model.\n\nSo, the whole point of GD is to minimize the cost function.\n\nThe meat of the algorithm is the process of getting to the lowest error value. Analogically this can be seen as, walking down into a valley, trying to find gold(the lowest error value). While we\u2019re at this, I\u2019m sure you\u2019ve wondered how we would find the deepest valley in a function with many valleys, if you can only see the valleys around you? I won\u2019t be going over the ways to solve that problem as that is beyond of this post(meant for beginners). However, just know that there are ways to work around that problem.\n\nMoving forward, to find the lowest error(deepest valley) in the cost function(with respect to one weight), we need to tweak the parameters of the model. How much do we tweak them though? Enter Calculus. Using calculus, we know that the slope of a function is the derivative of the function with respect to a value. This slope always points to the nearest valley!\n\nHere(in the picture), we can see the graph of the cost function(named \u201cError\u201d with symbol \u201cJ\u201d) against just one weight. Now if we calculate the slope(let\u2019s call this dJ/dw) of the cost function with respect to this one weight, we get the direction we need to move towards, in order to reach the local minima(nearest deepest valley). For now, let\u2019s just imagine our model having just one weight.\n\nNow that we have found the direction we need to nudge the weight, we need to find how much to nudge the weight. Here, we use the Learning Rate. The Learning Rate is called a hyper-parameter. A hyper-parameter is a value required by your model which we really have very little idea about. These values can be learned mostly by trial and error. There is no, one-fits-all for hyper-parameters. This Learning Rate can be thought of as a, \u201cstep in the right direction,\u201d where the direction comes from dJ/dw.\n\nThis was the cost function plotted against just one weight. In a real model, we do all the above, for all the weights, while iterating over all the training examples. In even a relatively small ML model, you will have more than just 1 or 2 weights. This makes things way harder to visualize, since now, your graph will be of dimensions which our brains can\u2019t even imagine.\n\nGoing back to the point I made earlier when I said, \u201cHonestly, GD(Gradient Descent) doesn\u2019t inherently involve a lot of math(I\u2019ll explain this later).\u201d Well, it\u2019s about time.\n\nWith a cost function, GD also requires a gradient which is dJ/dw(the derivative of the cost function with respect to a single weight, done for all the weights). This dJ/dw depends on your choice of the cost function. There are many types of cost functions(as written above as well). The most common is the Mean-Squared Error cost function.\n\nThe derivative of this with respect to any weight is(this formula shows the gradient computation for linear regression):\n\nThis is all the math in GD. Looking at this, you can tell that inherently, GD doesn\u2019t involve a lot of math. The only math it involves out of the box is multiplication and division which we will get to. This means, that your choice of a cost function, will affect your calculation of the gradient of each weight.\n\nEverything we talked about above, is all text book. You can open any book on GD and it will explain something similar to what I wrote above. Even the formulas for the gradients for each cost function can be found online without knowing how to derive them yourself.\n\nThe problem for most models however, arises with the learning rate. Let\u2019s look at the update expression for each weight(j ranges from 0 to the amount of weight and Theta-j is the jth weight in a weight vector, k ranges from 0 to the amount biases where B-k is the kth bias in a bias vector). Here, alpha is the learning rate. From this, we can tell that, we\u2019re computing dJ/dTheta-j(the gradient of weight Theta-j) and then we\u2019re taking a step of size alpha in that direction. Hence, we\u2019re moving down the gradient. To update the bias, replace Theta-j with B-k.\n\nIf this step size, alpha, is too large, we will overshoot the minimum, that is, we won\u2019t even be able land at the minimum. If alpha is too small, we will take too many iterations to get to the minimum. So, alpha needs to be just right. This confuses many people and honestly, it confused me for a while as well.\n\nI will be writing a whole post regarding the learning rate alpha in the future.\n\nWell, that\u2019s it. That\u2019s all there is to GD. Let\u2019s summarize everything in pseudo-code:\n\nThis was just one GD iteration.\n\nRepeat this process from start to finish for some number of iterations. Which means for 1 iteration of GD, you iterate over all the training examples, compute the gradients, then update the weights and biases. You then do this for some number of GD iterations.\n\nThere are 3 variations of GD:\n\n3. Batch \u2014 GD: This is what we just discussed in the above sections. Looping over every training example, the vanilla(basic) GD.\n\nHere\u2019s a picture comparing the 3 getting to the local minima:\n\nIn essence, using Batch GD, this is what your training block of code would look like(in Python).\n\nIf this still seems a little confusing, here\u2019s a little Neural Network I made which learns to predict the result of performing XOR on 2 inputs."
    },
    {
        "url": "https://medium.com/@SuryanshWrites/dont-know-these-machine-learning-resources-you-re-missing-out-1e97fa9da67c?source=user_profile---------4----------------",
        "title": "DON\u2019T know these Machine Learning Resources? You\u2019re missing out!",
        "text": "Machine Learning mostly requires the fundamental understanding of Linear Algebra, Statistics and Probability. While you can learn how to use all the advanced libraries to accomplish your ML tasks, once something breaks you won\u2019t be able to fix it. Even worse, you won\u2019t be able to understand any new studies being done in the field, since to understand them, you will need a somewhat deep understanding of mathematics. Also, you won\u2019t be able to conduct your own studies or play around with mathematical ML concepts.\n\nHere is list of links which I recommend in descending order of importance:\n\nThis being a very huge topic, it is somewhat hard to find good resources which explain the content properly. Hopefully the following help.\n\nAlthough this might not seem like a lot, but trust me, once you start getting into the links and really start utilizing the content to the fullest, you will understand how much content there is in just these links."
    }
]