[
    {
        "url": "https://towardsdatascience.com/redefining-immersive-gaming-with-autoencoders-powered-audio-visual-cloning-7fe9765c3547?source=user_profile---------1----------------",
        "title": "Redefining Immersive Gaming with Autoencoders powered audio-visual cloning",
        "text": "An important aspect of enjoying computer games is the feeling of being a part of the game and its story-line. The feeling of immersion is very important to stimulate emotions, and the more you feel these emotions, the more fun you have playing the game. While current games offer some level of immersion with excellent visuals and sound effects, it\u2019s still not the same as you yourself being present in that game environment. This is where I believe adding an audio-visual clone of yourself in the game as a playable character will take immersive gaming to the next level. Once you form a connection with this clone of yours in the game, it opens up the possibilities for the story writers to stimulate all kinds of emotions in you leading to a much better gaming experience.\n\nThe idea of putting yourself in a game is not new. Many games offer extensive character customization that let you control the looks and the behavior of the character, but the technology is very limited in terms of how close it can make the character resemble you. However, with deep learning, it is possible to improve on both audio and video aspects of this character customization. With this idea in mind, I set out to create a concept to see how well current Deep Learning techniques can help us achieve this. I tried to create my audio and visual clone in the game of FIFA 17 with the help of the wonderful autoencoder networks.\n\nIn my previous article/video, I showed how visual cloning is possible using encoder-decoder networks and how freakishly good they are in generating photo-realistic faces. I won\u2019t be going into this part again, so feel free to check out the aforementioned article for more details. I have used the same technique to generate my face in the subsequent results.\n\nSequence-to-sequence learning with Deep Neural Networks has proven to be very successful with tasks like text-to-speech conversion and machine translation. Thus, I wanted to explore the possibility of using such techniques for creating my voice given any text in written format. In my search for creating my audio clone, I came across Lyrebird."
    },
    {
        "url": "https://towardsdatascience.com/using-deep-learning-to-improve-fifa-18-graphics-529ec44ea37e?source=user_profile---------2----------------",
        "title": "Using Deep Learning to improve FIFA 18 graphics \u2013",
        "text": "The deepfakes algorithm involves training of deep neural networks called autoencoders. These networks are used for unsupervised learning and have an encoder that can encode an input to a compact representation called the \u201cencoding\u201d, and a decoder that can use this encoding to reconstruct the original input. This architecture forces the network to learn the underlying distribution of the input rather than simply parroting back the input. For images as our input, we use a convolutional net as our encoder and a deconvolutional net as our decoder. This architecture is trained to minimize the reconstruction error for unsupervised learning.\n\nFor our case, we train two autoencoder networks simultaneously. One network learns to recreate face of Ronaldo from FIFA 18 graphics. The other network learns to recreate the face from actual pictures of Ronaldo. In deepfakes, both networks share the same encoder but are trained with different decoders. Thus, we now have two networks that have learnt how Ronaldo looks like in the game and in real life.\n\nWhen training using a pre-trained model on other faces, the total loss goes down from around 0.06 to 0.02 within 4 hours on a GTX 1070. In my case, I continued training on top of the original CageNet model that has been trained to generate Nicolas Cage\u2019s face.\n\nNow comes the fun part. The algorithm is able to swap faces by adopting a clever trick. The second autoencoder network is actually fed with the input of the first one. This way, the shared encoder is able to get the encoding from FIFA face, but the decoder reconstructs the real face using this encoding. Voila, this setup just converted the face from FIFA to the actual face of Ronaldo.\n\nThe GIF below shows a quick preview of results from running this algorithm on faces of other players. I think the improvement is astonishing, but maybe I am biased, so you be the judge.\n\nMore such results in video format can be found on my YouTube channel, with the video embedded below. Please subscribe to my channel if you like the video."
    },
    {
        "url": "https://towardsdatascience.com/building-a-deep-neural-network-to-play-fifa-18-dce54d45e675?source=user_profile---------3----------------",
        "title": "Building a Deep Neural Network to play FIFA 18 \u2013",
        "text": "The underlying mechanism to build such a bot needs to work without having access to any of the game\u2019s internal code. Good thing then that the premise of this bot says we do not want to look at any such in-game information. A simple screenshot of the game window is all that is needed to feed into the bot\u2019s game engine. It processes this visual information and outputs the action it wants to take which gets communicated to the game using a key-press simulation. Rinse and repeat.\n\nExploring this would require a game where it is possible to collect such data of humans playing the game ahead of developing the game itself. FIFA is one such game that let me explore this. Being able to play the game and record my in-game actions and decisions allowed me to train an end-to-end Deep Learning based bot without having to hard-code a single rule of the game.\n\nA.I. bots in gaming are usually built by hand-coding a bunch of rules that impart game-intelligence. For the most part, this approach does a fairly good job of making the bot imitate human-like behavior. However, for most games it is still easy to tell apart a bot from an actual human playing. If we want to make these bots behave more human-like, would it help to not build them using hand-coded rules? What if we simply let the bot figure out the game by making it learn from looking at how humans play?\n\nNow that we have a framework in place to feed input to the bot and to let its output control the game, we come to the interesting part: learning game intelligence. This is done in two steps by (1) using convolution neural network for understanding the screenshot image and (2) using long short term memory networks to decide appropriate action based on the understanding of the image.\n\nCNNs are well known for their ability to detect objects in an image with high accuracy. Add to that fast GPUs and intelligent network architectures and we have a CNN model that can run in real time.\n\nFor making our bot understand the image it is given as input, I use an extremely light weight and fast CNN called MobileNet. The feature map extracted from this network represents a high level understanding of the image, like where the players and other objects of interest are located on the screen. This feature map is then used with a Single-Shot Multi-Box to detect the players on the pitch along with the ball and the goal.\n\nNow that we have an understanding of the image, we could go ahead and decide what move we want to make. However, we don\u2019t want to look at just one frame and take action. We\u2019d rather look at a short sequence of these images. This is where LSTMs come into picture as they are well known for being able to model temporal sequences in data. Consecutive frames are used as time steps in our sequence, and a feature map is extracted for each frame using the CNN model. These are then fed into two LSTM networks simultaneously.\n\nThe first LSTM performs the task of learning what movement the player needs to make. Thus, it\u2019s a multi-class classification model. The second LSTM gets the same input and has to decide what action to take out of cross, through, pass and shoot: another multi-class classification model. The outputs from these two classification problems are then converted to key presses to control the actions in the game.\n\nThese networks have been trained on data collected by manually playing the game and recording the input image and the target key press. One of the few instances where gathering labelled data does not feel like a chore!\n\nI don\u2019t know what accuracy measure to use in order to judge the bot\u2019s performance, other than to let it just go out there and play the game. Based on only 400 minutes of training, the bot has learned to make runs towards the opponent\u2019s goal, make forward passes and take shots when it detects the goal. In the beginner mode of FIFA 18, it has already scored 4 goals in about 6 games, 1 more than Paul Pogba has in the 17/18 season as of time of writing.\n\nVideo clips of the bot playing against the inbuilt bot can be found on my YouTube channel, with the video embedded below.\n\nMy initial impressions on this approach of building game bots are certainly positive. With limited training, the bot has already picked up on basic rules of the game: making movements towards the goal and putting the ball in the back of the net. I believe it can get very close to human level performance with many more hours of training data, something that would be easy for the game developer to collect. Moreover, extending the model training to learn from real world footage of matches played would enable the game developers to make the bot\u2019s behavior much more natural and realistic. Now if only anyone from EA sports was reading this\u2026"
    }
]