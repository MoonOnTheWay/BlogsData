[
    {
        "url": "https://towardsdatascience.com/how-to-build-a-gesture-controlled-web-based-game-using-tensorflow-object-detection-api-587fb7e0f907?source=user_profile---------1----------------",
        "title": "How to build a Gesture Controlled Web based Game using Tensorflow Object Detection Api",
        "text": "Control the game paddle by waving your hand in from of a web cam.\n\nWith the TensorFlow object detection api, we have seen examples where models are trained to detect custom objects in images (e.g. detecting hands, toys, racoons, mac n cheese). Naturally, an interesting next step is to explore how these models can be deployed in real world use cases \u2014 for example, interaction design.\n\nIn this post, I cover a basic body-as-input interaction example where real time results from a hand tracking model (web cam stream as input) is mapped to the controls of a web-based game (Skyfall). The system demonstrates how the integration of a fairly accurate, light weight hand detection model can be used to track player hands and enable realtime body-as-input interactions.\n\nWant to try it out? Project code is available on Github.\n\nUsing parts of the human body as input has the benefit of being always available as the user is not required to carry any secondary device. Importantly, appropriating parts of the human body for gesture based interaction has been shown to improve user experience [2] and overall engagement [1]. While the idea of body as input is not entirely new, existing approaches which leverage computer vision, wearables and sensors (kinect, wii, [5]) etc sometimes suffer from accuracy challenges, are not always portable and can be challenging to integrate with 3rd party software. Advances in light-weight deep neural networks (DNNs), specifically models for object detection (see [3]) and key point extraction (see [4]) hold promise in addressing these issues and furthering the goal of always available (body as) input. These models allow us track the human body with good accuracy using 2D images and with the benefit of easy integration with a range of applications and devices (desktop, web, mobile). While tracking from 2D images does not give us much depth information, it is still surprisingly valuable in building interactions as shown in the Skyfall game example.\n\nSkyfall is a simple web based game created using planck.js \u2014 a 2D physics engine. The play mechanism for SkyFall is simple. 3 types of balls fall from the top of the screen in random order \u2014 white balls (worth 10 points), green balls (worth 10 points) and red balls (worth -10 points). Players earn points by moving a paddle to catch the good balls (white and green balls) and avoid bad balls (red balls). In the example below, the player can control the paddle by moving the mouse or by touch (drag) on a mobile device.\n\nThis is a pretty simple and fun game. However, we can make it even more engaging by allowing the user control the paddle using their body (hand). The goal is to accomplish this by detecting hand position using web cam video stream\u2014 no additional sensors or wearables.\n\nTo add gesture interaction, we replace the mouse controls above with a system that maps the movement of the players hand to the game paddle position. In the current implementation, a python application (app.py), detects the player\u2019s hand using the TensorFlow object detection api, and streams hand coordinates to the game interface \u2014 a web application served using FLASK \u2014 over websockets.\n\nIn a previous post, I covered how to build a real-time hand detector using the Tensorflow Object detection api. Please see the blog post to learn more on how the hand tracking model is built. For any errors or issues related to loading the hand model, please see the hand tracking Github repo and issues. This example follows a similar approach where a multi threaded python app reads web cam video feed and outputs bounding boxes for each hand detected.\n\nNote that hand detection is done on a frame-by-frame basis and the system does not automatically track hand across frames. However, this type of inter-frame tracking is useful as it can enable multiple user interaction where we need to track a hand across frames (think a bunch of friends waving their hands or some other common object, each controlling their own paddle). To this end, the current implementation includes naive euclidean distance based tracking where hands seen in similar positions across frames are assigned the same id.\n\nOnce each hand in the frame is detected (and a tracking id assigned), the hand coordinates are then sent to a web socket server which sends it out to connected clients.\n\nThe game interface connects to the web socket server and listens for hand detection data. Each detected hand is used to generate a paddle, and the coordinate of the hand in the video frame is used to relatively position the paddle on the game screen.\n\nThere are several limitations with the current implementation \u2014 so contributions, pull requests are most welcome!\n\nHand Detector Improvement\n\nThis entails collecting additional training data and leveraging data augmentation strategies to improve the hand detector. This is important as the entire interaction (and user experience) depends on accurate and robust hand tracking (false positives, false negatives make for bad UX).\n\nThe current implementation uses a simple euclidean based metric to track hands across frames (hand in current frame is identified based on its distance from hands in previous frames). With several overlapping hands, things can get complicated\u2014 a more robust tracking algorithm is required. Perhaps integrating a fast tracking algorithm from OpenCV or other sources \u2026\n\nTensorflowjs implementation \n\nConduct some experiments with a TensorFlowjs implementation that allows the entire interaction to be prototyped completely in the browser!!! Tensorflowjs brings so many benefits \u2014 easy deployment (no python or Tensorflow installation), no websocket servers and clients, easy reproducibility, more potential users \u2026\n\nThere are existing projects that apply machine learning models in designing interactions. A common example is the use of LSTMs to autocomplete text and create fast email replies (see the Smart Reply paper by Google), and the more recent experiments where image classification models are used as game controls (see Teachable Machines and other Tensorflowjs demos by Google Pair). This work builds on these trends by exploring the use of object detection models in creating interactions.\n\nAs AI algorithms continue to mature (accuracy, speed), there is potential to leverage these advances in building better interactions. This could be generative interfaces that tailor content to the user or predict their intended interaction, models that enable new types of vision-based interactions, conversational UI etc. Across these areas, it is increasingly important to study the mechanics of such interactions, rigorously test these interactions and and create design patterns that inform the use of AI models as first class citizens in interaction design.\n\nGot feedback, comments, want to collaborate? Feel free to reach out \u2014 twitter, linkedin.\n\nNote: An earlier version of Skyfall was submitted in January 2018 to the NVIDIA Jetson Challenge.\n\n[1] Shafer, D. M., Carbonara, C. P., and Popova, L. 2011. Spatial presence and perceived reality as predictors of motion-based video game enjoyment. Presence: Teleoperators and Virtual Environments 20(6) 591\u2013619.\n\n[2] Birk, M., and Mandryk, R. L. 2013. Control your game-self: effects of controller type on enjoyment, motivation, and personality in game. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems \u2014 CHI \u201913 685\u2013694.\n\n[3] Huang, J., Rathod, V., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z., Song, Y., Guadarrama, S., and Murphy, K. 2017. Speed/accuracy trade-offs for modern convolutional object detectors. CVPR\n\n[5] Harrison, C., Tan, D., and Morris, D. 2010. Skinput: appropriating the body as an input surface. Proceedings of the 28th international conference on Human factors in computing systems \u2014 CHI \u201910 453."
    },
    {
        "url": "https://towardsdatascience.com/a-review-of-nvidia-gtc-2018-conference-new-gpus-deep-learning-acceleration-data-augmentation-d6d4f638bcda?source=user_profile---------2----------------",
        "title": "A Review of NVIDIA GTC 2018 Conference \u2014 New GPUs, Deep Learning Acceleration, Data Augmentation\u2026",
        "text": "A selection of the technical talks I attended are detailed below.\n\nI found this talk to be interesting as it provided practical advice on ways to satiate the data hungry demands of supervised deep learning. The speaker begins with the premise that human annotation of data is an expensive process and proposes four approaches they use in their workflow to address this.\n\nWeb scraping: Approaches to efficiently scrape labelled data from websites and social networks. \n\nWeakly Supervised Methods: Given a small dataset labelled by experts, we we can learn labelling strategies and apply this in labelling larger datasets.\n\nData Transformations: We can augment datasets by generating additional examples using simple linear transformations \u2014 e.g cropping, shifting, color casting, lens distortion, vignetting, random backgrounds etc. Example of a library for this sort of transformation is imageAug.\n\nData Synthesis: We can generate texturized CAD models as training data, we can add specific features to data such as adding glasses to facial images, and altogether synthesizing new images using GANs.\n\nMore can be found on the presenter\u2019s slides here.\n\nResearchers from NVIDIA demonstrated some early work detecting drowsiness in drivers. The authors train a scaled down VGG16 model, and augment their training dataset using synthetic data generated from 3D face models. For classification they rely on predicted eye pitch angle over a time period.\n\nGenerative Design, Autodesk: In this talk, the presenter discussed some interesting ways in which evolutionary algorithms were used in generating CAD designs. Given a design challenge, the goal is usually to balance the cost (of materials) and performance. To this end, they have experimented with evolutionary algorithms that generate design candidates, while optimizing on parameters such cost/performance/manufacturing method etc and use automated stress tests (FEA analysis) as part of feedback. A specific example was given where an evolutionary algorithm came up with a high performance (and unusual looking) part of a motorbike.\n\nA.I. Disrupting the Future of Content Creation for Games \u2014 Eric Risser, Artomatix\n\nThis talk focused on how AI accelerated workflows can be applied to aspects of the media industry (e.g. movies, video games). The presenter reveals that the video game industry spends 61% of its budget on artistic content generation \u2014 main character as well as background. Much of this efforts include a manual workflow. Creative or generative AI offers opportunities to improve this, across areas such as Texture Synthesis, Material Enhancement, Hybridization and Style Transfer. This includes methods that enable artists paint with structure, example based workflow (scanning real world objects and improving with AI) and photogrammetry. AI can also help with recycling old content e.g. up-res video. An industry use case was given with IKEA being able to easily scan 3D models of products which were then used in websites (studies showed having 3D models led to 20% higher sales on websites). See more details on the presenters company blog.\n\nGrowing Generative Models \u2014 Samuli Laine et Al\n\nResearchers from NVIDIA presented some interested work on how to generate high resolutions images using Generative Adversarial Networks (GANs). Their approach addresses a known problem with GANs (mode collapse), speeding up and stabilizing the training process. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, and adding new layers that model increasingly fine details as training progresses. They highlight the potential of this work in generating assets for games and films and conditioning the GAN to determine output (e.g. male or female faces). More details can be found in their paper.\n\nAnother interesting talk looked at how ML can be used to address some issues in security \u2014 detecting domain generation algorithms. Domain Generation Algorithms DGAs, are algorithms seen in various families of malware that are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control servers. They are used by hackers to communicate and exfiltrate data from networks, designed to circumvent traditional cyber defenses and have been extremely successful. They cite a recent paper \u201cInline DGA Detection with Deep Networks\u201d."
    },
    {
        "url": "https://towardsdatascience.com/data2vis-automatic-generation-of-data-visualizations-using-sequence-to-sequence-recurrent-neural-5da8e9d3e43e?source=user_profile---------3----------------",
        "title": "Data2Vis: Automatic Generation of Data Visualizations Using Sequence-to-Sequence Recurrent Neural\u2026",
        "text": "We formulate data visualization as a sequence to sequence translation problem. TLDR; We train a model that can take in a dataset as input and generate a plausible visualization as output. We have an early paper draft describing the work on arxiv, short video, and demo. Feedback, discussion (@vykthur, @serravis)and and comments welcome! Demo web app where a user can paste data and get a (quirky) visualization. This work is done jointly with a colleague (Cagatay Demiralp) and started from a conversation we had after a paper discussion meeting. We had read some papers where various forms of generative and sequence models were used to create a wide range of stuff \u2014 from generating images (GANs), music, source code, image captions to generating questions and answers about images (VQA) etc. Despite the quirks that can sometimes be associated with these models (one eyed cats, music that ultimately lacks that natural feel etc), they all demonstrate a promise of value when trained and deployed at scale. And so we were curious about the possibility applying advances in demonstrated within these deep learning models to the task of creating visualizations. This is valuable as visualization authoring can be a time consuming process (selecting what tool to use, which fields to visualize, which transforms to apply and which interactions to support). If we can get a model to do some of these things, we felt that could improve the visualization authoring process. The first challenge was related to finding ways to formulate the problem such that it becomes amenable to deep learning. How can we teach a model to understand and then generate a visualization? How do we curate our training dataset? Do we assemble nice images of visualizations + data pairs and train on those? Luckily, similar problems related to improving visualization authoring have been considered by researchers in the past. One line of interesting work is around efforts to create declarative languages that succinctly describe visualization and provide the \u201cright\u201d trade off between expressivity and speed in authoring visualizations. With these languages or grammars, you write some compact text specification and some engine does the hardwork of turning that text into actual visualizations. A good example of such a declarative language is Vega-Lite created by the awesome researchers over at University of Washington Interactive Data Lab. For our experiments, we represent visualizations using Vega-Lite Grammar specifications (JSON).\n\nWhat we did If we then think of visualizations as text specifications, the problem is simplified to generating a bunch of these specifications, given some input data. And so our next task was to explore models that learn the space of a visualization specification format and perhaps can \u201challucinate\u201d some new visualizations? We began our initial experiments following notes from Karpathy\u2019s blog \u2014 we generated visualization specifications (data + spec), concatenated them and attempted to train an RNN that would generate something plausible. From this first experimentations, we learned quite a bit about how well character RNNs (LSTMs) perform with respect to learning the structure of data contained within nested structures like a Vega-Lite specification. We found that while character based RNNs work well for the problem (compared to word-based RNNs), their limitations in modeling long sequences and memory requirement for long term sequences made them challenging to train (unintelligible tokens after a few characters, limited learning progress even after thousands of steps, millions of parameters without preprocessing). Also, character RNNs did not provide a concrete mapping between our input and output pairs for training. These challenges motivated us to think of other ways to formulate the problem \u2014approaches that learn the structure of our output space, and generates new data given some input. Essentially translating from source data specification to target visualization specification. Sequence to sequence models [1,2,3] are capable of doing just that \u2014 they take in some input (e.g. text in one language) and generate an output (e.g. same text in a different language). They have been applied with wide success to problems such as language translation, text summarization, image captioning. Variations of sequence to sequence models that utilize encoder decoder architectures have also been explored for translating between formal programming languages, learning domain specific programs and general program synthesis. We train a sequence to sequence model based on the work of Britz et al 2017. Training data consists of a source token (a single row from a data set) and a target token (a valid Vegalite visualization specification) pair. First we assembled a training dataset (source and target pairs) based on examples of valid Vega-Lite visualizations. A total of 215k pair samples based on 11 distinct datasets and 4300 Vega-Lite examples were used. Please see the paper for more details on how the data is sampled and pre-processed. We train a sequence to sequence model using the architecture and sample code provided by Britz et al 2017. The model is an encoder-decoder architecture with attention mechanism (please see the Britz paper for more details on the architecture).\n\nWe also perform some simple normalization on the dataset to simplify training. We replace string and numeric field names using a short notation \u2014 \u201cstr\u201d and \u201cnum\u201d in the source sequence (dataset). Next, a similar backward transformation (post processing) is replicated in the target sequence to maintain consistency in field names. These transformations help scaffold the learning process by reducing the vocabulary size, and prevents the LSTM from learning field names (learning field names in the training set is not useful to our generation problem). In turn we are able to reduce the overall source and target sequence length, reduce training time and reduce the number of hidden layers which the model needs to converge.\n\nTo evaluate the model, we use the Rdataset repository (cleaned and converted to a valid JSON format) which was not included in our training. The range of valid univariate and multivariate visualizations produced suggests the model captures aspects of the visualization generation process. As training progresses, the model incrementally learns the vocabulary and syntax for valid Vega-Lite specifications, learning to use quotes, brackets, symbols and keywords. The model also appears to have learned to use the right type of variable specifications in the Vega-Lite grammar (e.g. it correctly assigns a string type for text fields and a quantitative for numeric fields). Qualitative results also suggest the use of appropriate transformations (bins, aggregate) on appropriate fields (e.g. means are performed on numeric fields). The model also learns about common data selection patterns that occur within visualizations and their combination with other variables to create bivariate plots. For example, as experts create visualizations, it is common to group data by geography (country, state, sex), characteristics of individuals (citizenship status, marital status, sex) etc. Early results suggests that our model begins to learn these patterns and apply them in its generation of visualizations. For example, it learns to subset data using common ordinal fields such as responses (yes/no), sex (male/female) etc and plots these values against other fields. Finally, in all cases, the model generates a perfectly valid JSON file and valid Vega-Lite specification with some minor failure cases.\n\nAs observed with many deep learning models, there are promising results and also quirks or failure cases (e.g. 5 legged dogs, 3 eyed cats etc). In our case, the model sometimes generates specifications using non-existent (phantom variables) or applies transformations that are invalid during runtime leading to empty plots. See the figure above (plots marked in orange). Our intuition is that these challenges can be addressed by expanding our relatively small dataset. There a few reasons why we think automated generation of visualizations is interesting: Providing users who have little or no programming experience with the ability to rapidly create expressive data visualizations empowers them and brings data visualization into their personal workflow. For experts, models like Data2Vis also hold potential to \u201cseed\u201d the data visualization process, reducing the amount of time spent specifying visualization language syntax and supporting iteration over visualization possibilities. This motivation resonates with other research efforts that explore deep learning approaches for source code generation and translation (UCBerkeley, Microsoft Research etc). In theory, as we assemble more diverse and complex training examples (and a better architecture perhaps), DataVis should learn more complex visualization strategies, increasing its utility. This holds promise to create a model that may one day achieve human-level (or superhuman level) expertise in data visualization much like we have observed in other domains such as image recognition, gaming, medical imaging etc. Existing approaches to visualization generation tend to be based on rules and heuristics that generate univariate summary plots or plots based on combinations of rules. In such situations, Jeff Dean identifies opportunities to improve such systems using machine learning (ML) and encourages the \u201clearn\u201d and \u201cmeta learn\u201d everything approach. He also highlights challenges with heuristics \u2014 they don\u2019t adapt to patterns of usage and may fail to take valuable context into consideration. We also agree that ML and AI approaches offer opportunity to outperform rule-based methods by learning from existing training examples. This project is still very much work-in-progress - there are limitations and future work we hope to address. In addition to collecting more data and running experiments to improve Data2Vis, some future work includes: Data2Vis is currently implemented as a sequence-to-sequence translation model and outputs a single visualization spec for a given dataset. Now, wont it be awesome to train a model that generates multiple valid visualizations for a given dataset? We think it would. Perhaps training models that can map input data to multiple different visualization specification languages (e.g. Vega-Lite, ggplot2, D3 etc.). Or translating visualizations from one language to others. This could help make visualizations more accessible by enabling visualization specification reuse across languages, platforms and systems. How about Text2Vis? - models that generate visualizations using natural language text in addition to input data. Think a person exploring data in front of a large screen and asking for visualizations based on specific fields, transforms or interactions? This article discusses some ideas and early results around automatic generation of visualizations using sequence to sequence models. Hopefully, this work serves as a baseline for future work in automated generation of visualizations using deep learning approaches. Feel free to read the paper for more details, and share your thoughts too (@vykthur, @serravis)! We hope to share code and trained models sometime soon. This work was enabled by the contributions of many individuals. Thanks to the authors of the Vega-Lite,Voyager library and for sharing example data [4] used for our experiments. Many thanks to the authors of the TensorFlow Seq2seq model implementation and the TensorFlow library team \u2014 their work enabled us to learn about sequence models and rapidly prototype our experiments will little previous experience. [1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. (sep 2014). [2] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks"
    },
    {
        "url": "https://towardsdatascience.com/a-hci-researchers-summary-on-aaai-2018-conference-6496d6328aa4?source=user_profile---------4----------------",
        "title": "A HCI Researcher\u2019s Summary on AAAI 2018 Conference!",
        "text": "Summary of talks from AAAI18 that cover topics in Computer Vision, Machine Learning/Deep Learning(Catastrophic Forgetting), Learning Representations, Knowledge Graphs and Applied AI in general.\n\nThis post document contains notes on sessions I attended at the just concluded Artificial Intelligence Conference (AAAI 2018, New Orleans Louisiana). The selection of talks where based on my interests from a HCI and applied AI perspective. These include Human aspects of AI, Vision, Machine Learning/Deep Learning(Catastrophic Forgetting), Learning Representations, Knowledge Graphs and Applied AI in general. I welcome feedback, corrections (typos!) and discussions - please get in touch (@vykthur). For those interested in an additional overview of AAAI, David Abel has also written a detailed summary of AAAI 2018 and covers some sessions I did not attend. \n\nNote: technical talks were between 15\u201320 minutes and many do not have the papers/slides publicly available; TLDR \u2014 some of my notes may lack details.\n\nThis tutorial provided an overview of the process involved with creating a knowledge graph using data scraped from websites and was presented by researchers from the information science institute at USC.\n\nExample Tools for Knowledge Graph Construction. \n\nThe final part of the tutorial covered demonstrations on some tools which the authors have developed at their lab for knowledge graph generation \u2014 Karma and DIG. They also show an interesting use case where they build a knowledge graph to help with tacking human trafficking.\n\nThis talk was presented by the AAAI president (Subbarao Kambhampati) and provided some perspective on human-aware AI (HAAI) \u2014 motivations, research challenges and open issues.\n\nThis talk (presented by Michael S. Ryoo from university of Indiana), introduces a neural network approach to recognize activity in low resolution images \u2014 as low as 16 * 12 pixels!!\n\nThis paper provides an approach to tackling an extension of catastrophic forgetting (which they call domain expansion) in neural networks.\n\nMotivation.\n\nThe domain expansion problem as the problem of creating a network that works well both on an old domain and a new domain even after it is trained in a supervised way using only the data from the new domain without accessing the data from the old domain. They identify two challenges for domain expansion.\n\nThis paper presents three novel techniques step by step to efficiently utilize different levels of features to improve human pose estimation.\n\nSome work done by researchers from Microsoft Research China in creating an end-to-end model that does both dehazing and a downstream operation like object detection."
    },
    {
        "url": "https://towardsdatascience.com/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce?source=user_profile---------5----------------",
        "title": "How to Build a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow",
        "text": "How to Build a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow This post documents steps and scripts used to train a hand detector using Tensorflow (Object Detection API). I was interested mainly in detecting hands on a table. And in real time. Hopefully this post demonstrates how neural networks can be applied to the problem of tracking hands (egocentric and other views) with good results. All of the code (including the frozen model) are now available on Github. And here is the detector in action. As with any DNN/CNN based task, the most expensive (and riskiest) part of the process has to do with finding or creating the right (annotated) dataset. I experimented first with the Oxford Hands Dataset (the results were not good). I then tried the Egohands Dataset which was a much better fit to my requirements (egocentric view, high quality images, hand annotations). 11 FPS using a 640 * 480 image run while visualizing results (image above) Above numbers are based on tests using a macbook pro CPU (i7, 2.5GHz, 16GB). There are several existing approaches to tracking hands in the computer vision domain. Incidentally, many of these approaches are rule based (e.g. extracting background based on texture and boundary features, distinguishing between hands and background using color histograms and HOG classifiers, etc) making them not very robust. For example, these algorithms might get confused if the background is unusual or where sharp changes in lighting conditions cause sharp changes in skin color or the tracked object becomes occluded. (see here for a review paper on hand pose estimation from the HCI perspective). Detection on live video from a webcam. There were some misses when motion was fast and hands were from an unlikely egocentric viewpoint. With sufficiently large datasets, neural networks provide opportunity to train models that perform well and address challenges of existing object tracking/detection algorithms \u2014 varied/poor lighting, diverse viewpoints and even occlusion. The main drawbacks to usage for real-time tracking/detection is that they can be complex, are relatively slow compared to tracking-only algorithms and it can be quite expensive to assemble a good dataset. But things are changing with advances in fast neural networks. Furthermore, this entire area of work has been made more approachable by deep learning frameworks (such as the tensorflow object detection api) that simplify the process of training a model for custom object detection. More importantly, the advent of fast neural network models like ssd, faster r-cnn, rfcn (see here ) etc make neural networks an attractive candidate for real-time detection (and tracking) applications. There are multiple applications for robust hand tracking like this across HCI areas (as an input device etc.). If you are not interested in the process of training the detector, you can skip straight to the section on applying the model to detect hands. Training a model is a multi-stage process (assembling dataset, cleaning, splitting into training/test partitions and generating an inference graph). While I lightly touch on the details of these parts, there are a few other tutorials which cover training a custom object detector using the tensorflow object detection api in more detail (see here and here). I recommend you walk through those if interested in training a custom detector from scratch. The hand detector model is built using data from the Egohands Dataset dataset. This dataset works well for several reasons. It contains high quality, pixel level annotations (>15000 ground truth labels) where hands are located across 4800 images. All images are captured from an egocentric view (Google glass) across 48 different environments (indoor, outdoor) and activities (playing cards, chess, jenga, solving puzzles etc). If you will be using the Egohands dataset, you can cite them as follows: Bambach, Sven, et al. \u201cLending a hand: Detecting hands and recognizing activities in complex egocentric interactions.\u201d Proceedings of the IEEE International Conference on Computer Vision. 2015.\n\nThe Egohands dataset (zip file with labelled data) contains 48 folders of locations where video data was collected (100 images per folder). Some initial work needs to be done to the Egohands dataset to transform it into the format (tfrecord) which Tensorflow needs to train a model. The Github repo contains egohands_dataset_clean.py a script that will help you generate these csv files. Renames all files to include their directory names to ensure each filename is unique Reads in `polygons.mat` for each folder, generates bounding boxes and visualizes them to ensure correctness. Once the script is done running, you should have an images folder containing two folders - train, and test. Each of these folders should also contain a csv label document each - `train_labels.csv`, `test_labels.csv` that can be used to generate `tfrecords`. Next: convert your dataset + csv files to tfrecords. Please use the guide provided by Harrison from pythonprogramming on how to generate tfrecords given your label csv files and your images. The guide also covers how to start the training process if training locally. If training in the cloud using a service like GCP, see the guide here. Note: While the egohands dataset provides four separate labels for hands (own left, own right, other left, and other right), for my purpose, I am only interested in the general `hand` class and label all training data as `hand`. You can modify the train script to generate `tfrecords` that support 4 labels. Now that the dataset has been assembled, the next task is to train a model based on this. With neural networks, it is possible to use a process called transfer learning to shorten the amount of time needed to train the entire model. This means we can take an existing model (that has been trained well on a related domain (here image classification) and retrain its final layer(s) to detect hands for us. Sweet!. Given that neural networks sometimes have thousands or millions of parameters that can take weeks or months to train, transfer learning helps shorten training time to possibly hours. Tensorflow does offer a few models (in the tensorflow model zoo) and I chose to use the `ssd_mobilenet_v1_coco` model as my start point given it is currently (one of) the fastest models (see the research paper on SSD here). The training process can be done locally on your CPU machine which may take a while, or better on a (cloud) GPU machine (which is what I did). For reference, training on my macbook pro (tensorflow compiled from source to take advantage of the mac\u2019s cpu architecture) the maximum speed I got was 5 seconds per step as opposed to the ~0.5 seconds per step I got with a GPU. For reference it would take about 12 days to run 200k steps on my mac (i7, 2.5GHz, 16GB) compared to ~5hrs on a GPU. As the training process progresses, the expectation is that total loss (errors) gets reduced to its possible minimum (about a value of 1 or lower). By observing the tensorboard graphs for total loss(see image below), it should be possible to get an idea of when the training process is complete (total loss does not decrease with further iterations/steps). I ran my training job for 200k steps (took about 5 hours) and stopped at a total Loss (errors) value of 2.575.(In retrospect, I could have stopped the training at about 50k steps and gotten a similar total loss value). With tensorflow, you can also run an evaluation concurrently that assesses your model to see how well it performs on the test data. A commonly used metric for performance is mean average precision (mAP) which is single number used to summarize the area under the precision-recall curve. mAP is a measure of how well the model generates a bounding box that has at least a 50% overlap with the ground truth bounding box in our test dataset. For the hand detector trained here, the mAP value was 0.9686@0.5IOU. mAP values range from 0\u20131, the higher the better.\n\nOnce training is completed, the trained inference graph (`frozen_inference_graph.pb`) is then exported (see the earlier referenced guides for how to do this) and saved in the `hand_inference_graph` folder. Now its time to do some interesting detection. Using the Detector to Detect/Track hands If you have not done this yet, please follow the guide on installing Tensorflow and the Tensorflow object detection api. This will walk you through setting up the tensorflow framework, cloning the tensorflow object detection repo and installing it. The general steps to detect hands are as follows: Load the `frozen_inference_graph.pb` trained on the hands dataset as well as the corresponding label map. Read in your input image (this may be captured from a live video stream, a video file or an image). On GitHub, the provided repo contains two scripts that tie all these steps together. detect_multi_threaded.py : A threaded implementation for reading camera video input detection and detecting. Takes a set of command line flags to set parameters such as ` \u2014 display` (visualize detections), image parameters ` \u2014 width` and ` \u2014 height`, video ` \u2014 source` (0 for camera) etc. detect_single_threaded.py : Same as above, but single threaded. This script works for video files by setting the video source parameter videe ` \u2014 source` (path to a video file). Most importantly, the repo contains a frozen_inference_graph.pb that contains a trained model based on SSD which you can easily import to your tensorflow applications to detect hands. A few things that led to noticeable performance increases. Threading: Turns out that reading images from a webcam is a heavy I/O event and if run on the main application thread, can slow down the program. I implemented some good ideas from Adrian Rosebuck on parallelizing image capture across multiple worker threads. This mostly led to an FPS increase of about 5 points. For those new to OpenCV, the `cv2.read()` method return images in [BGR format]. Ensure you convert to RGB before detection (accuracy will be much reduced if you don't). Keeping your input image small will increase fps without any significant accuracy drop.(I used about 320 x 240 compared to the default1280 x 720 which my webcam provides). Performance can also be increased by a clever combination of tracking algorithms with the already decent detection and this is something I am still experimenting with. Have ideas for optimizing , please share! Performance on random images with hands show some limitations of the detector. Note: The detector does reflect some limitations associated with the training set. This includes non-egocentric viewpoints, very noisy backgrounds (e.g in a sea of hands) and sometimes skin tone. There is opportunity to improve these with additional data. One way to make things more interesting is to integrate our new knowledge of where \u201chands\u201d are with other detectors trained to recognize other objects. Unfortunately, while our hand detector can in fact detect hands, it cannot detect other objects (a factor or how it is trained). To create a detector that classifies multiple different objects would mean a long involved process of assembling datasets for each class and a lengthy training process. Given the above, a potential alternate strategy is to explore structures that allow us efficiently interleave output form multiple pretrained models for various object classes and have them detect multiple objects on a single image. An example of this is with my primary use case where I am interested in understanding the position of objects on a table with respect to hands on same table. I am currently doing some work on a threaded application that loads multiple detectors and outputs bounding boxes on a single image. More on this soon. This work also served as an intense weekend crash course for me to learn Python and Tensorflow. It would be impossible without the Egohands Dataset, many thanks to the authors! The tensorflow custom object detection guides by Harrison from pythonprogramming and Dat Tran were immensely helpful to this learning process. And ofcourse, many thanks to the Tensorflow authors! Its a great framework! If you\u2019d like to cite this tutorial, use the below. Victor Dibia, Real-time Hand-Detection using Neural Networks (SSD) on Tensorflow, (2017), GitHub repository, https://github.com/victordibia/handtracking\n\n\n\n@misc{Dibia2017,\n\n author = {Victor, Dibia},\n\n title = {Real-time Hand Tracking Using SSD on Tensorflow },\n\n year = {2017},\n\n publisher = {GitHub},\n\n journal = {GitHub repository},\n\n howpublished = {\\url{https://github.com/victordibia/handtracking}},\n\n commit = {b523a27393ea1ee34f31451fad656849915c8f42}\n\n} If you would like to discuss this in more detail, feel free to reach out on Twitter, Github or Linkedin. Bambach, S., Lee, S., Crandall, D. J., and Yu, C. 2015. \u201cLending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions,\u201d in ICCV, pp. 1949\u20131957 (available at https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Bambach_Lending_A_Hand_ICCV_2015_paper.html). Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., and Berg, A. C. 2016. \u201cSSD: Single shot multibox detector,\u201d in European conference on computer vision (Vol. 9905 LNCS), Springer Cham, pp. 21\u201337 (doi: 10.1007/978-3-319-46448-0_2). Betancourt, A., Morerio, P., Regazzoni, C. S., and Rauterberg, M. 2015. \u201cThe Evolution of First Person Vision Methods: A Survey,\u201d IEEE Transactions on Circuits and Systems for Video Technology (25:5), pp. 744\u2013760 (doi: 10.1109/TCSVT.2015.2409731)"
    }
]