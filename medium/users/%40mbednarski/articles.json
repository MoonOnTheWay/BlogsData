[
    {
        "url": "https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb?source=user_profile---------1----------------",
        "title": "Implementing word2vec in PyTorch (skip-gram model) \u2013",
        "text": "You probably have heard about word2vec embedding. But do you really understand how it works? I though I do. But I have not, until implemented it.\n\nThis is why I\u2019m creating this guide.\n\nI assume you know more-less what word2vec is.\n\nIn order to be able to track every single step i\u2019m using following nano corpus:\n\nVery first step is word2vec to create the vocabulary. It has to be built at the beginning, as extending it is not supported.\n\nVocabulary is basically a list of unique words with assigned indices.\n\nCorpus is very simple and short. In real implementation we would have to perform case normalization, removing some punctuation etc, but for simplicity let\u2019s use this nice and clean data. Anyway, we have to tokenize it:\n\nThis will give us a list of tokens:\n\nWe iterate over tokens in corpus, and generate list of unique words(tokens). Next we create two dictionaries for mapping between word and index.\n\nWhich gives us:\n\nWe can now generate pairs , . Let\u2019s assume context window to be symmetric and equal to 2.\n\nIt gives us list of pairs , indices:\n\nWhich can be easily translated to words:\n\nNow, we are going through details from very first equation to working implementation.\n\nFor skip-gram we are interested in predicting context, given center word and some parametrization. This is our probability distribution for single pair.\n\nNow, we want to maximize it trough all word/context pairs.\n\nAs we are interested in predicting context given center word, we want to maximize P(context|center) for each , pair. As probability sums up to 1 \u2014 we are implicitly making P(context|center) close to 0 for all non-existing , pairs. By multiplying those probabilities we make this function close to 1 if our model is good and close to 0 if it is bad. Of course we are pursuing a good one \u2014 so there is max operator at the beginning.\n\nThis expression is not very suitable to compute. That\u2019s why we are going to perform some very common transformations.\n\nRecall that neural nets are about minimizing loss function. We could simply multiply P by minus one , but applying log gives us better computational properties. This does not change position of function extrema (as log is a strictly monotonic function). So expression is changed to:\n\nNext step is to replace products with sums. We can do it because:\n\nAnd after dividing by number of pars (T) we have our final loss term:\n\nGreat, but how we do define P(context|center)? For now, let\u2019s assume that reach word has actually two vectors. One if is present as center word (v), and second one if context(u). Given that definition for P looks following:\n\nLet me break it down to smaller parts. See following structure:\n\nThis is just a softmax function. Now closer look at nominator\n\nBoth u and v are vectors. This expression is just scalar product of given , pair. Bigger as they are more similar to each other.\n\nWe\u2019re iterating over all words in vocabulary.\n\nAnd computing \u201csimilarity\u201d for given center word and each word in vocabulary treated as context word.\n\nFor each existing center, context pair in corpus we\u2019re computing their \u201csimilarity score\u201d. And divide it by sum of each theoretically possible context \u2014 to know whether score is relatively high or low. As softmax is guaranteed to take a value between 0 and 1 it defines a valid probability distribution.\n\nNeural net implementing this concept consists of three layers: input, hidden and output.\n\nInput layer is just the center word encoded in one-hot manner. It dimensions are\n\nHidden layer makes our v vectors. Therefore it has to have neurons. To compute it value we have to define weight matrix. Of course its has to be There is no activation function \u2014 just plain matrix multiplication.\n\nWhat\u2019s important \u2014 each column of stores v vector for single word. Why? Because x is one-hot and if you multiply one-hot vector by matrix, result is same as selecting select single column from it. Try on your own using a piece of paper ;)\n\nLast layer must have neurons \u2014 because it generates probabilities for each word. Therefore, is in terms of shape.\n\nOn top on that we have to use softmax layer. PyTorch provides optimized version of this, combined with \u2014 because regular softmax is not really numerically stable:\n\nThis is equivalent to compute softmax and after that applying log.\n\nNow we can compute loss. As usual PyTorch provides everything we need:\n\nThe computes negative-log-likelihood on logsoftmax. is context word \u2014 we want to make this as high as possible \u2014 because pair is from training data \u2014 so the are indeed center, context.\n\nAs we fished forward pass, now it\u2019s time to perform backward pass. Simply:\n\nFor optimization SDG is used. It is so simple, that it was faster to write it by hand instead of creating optimizer object:\n\nLast step is to zero gradients to make next pass clear:\n\nTime to compile it into training loop. It can look like:\n\nOne potentially tricky thing is definition. We do not create one-hot explicitly \u2014 does it by itself.\n\nOk, we have trained the network. One, last thing is to extract vectors for words. It is possible in three ways:\n\nTry to think on your own when to use which one ;)\n\nI\u2019m working on online interactive demonstration on this. It should be available soon. Stay tuned ;)\n\nYou can download code from here."
    },
    {
        "url": "https://towardsdatascience.com/structure-and-automated-workflow-for-a-machine-learning-project-part-2-b5b420625102?source=user_profile---------2----------------",
        "title": "Structure and automated workflow for a machine learning project \u2014 part 2",
        "text": "Now there is time for magic:\n\nWe know that the 4th column contains the label that we will want to predict. As iris is famous dataset we also know names of features in columns [0,3] \u2014 they are: sepal length, sepal width, petal length and petal width . But for purpose of exercise let\u2019s assume we do not know them and give generic names:\n\nLater on, we want to actually load and take a look at the data:\n\nIn the next cell we import required libraries and store root directory \u2014 this is very useful.\n\nThis loads watermark extension and prints details about running platform. It can be useful for other people, especially when they get have difficulties with reproducing your results. Output from my machine:\n\nThis 00 is very important \u2014 It indicates the order of execution. The very first cell of the notebook contains:\n\nFor exploration, I personally prefer to use jupyter notebooks. Let\u2019s create one in directory notebooks/00-initial-exploration.ipynb\n\nFirst of all, we need to add some additional packages. Here is a complete list for today.\n\nHi! I hope that previous part was useful. Today we will cover some other dimensions. We will start with preprocessing the data and perform a simple exploratory analysis. Then I will show you how to create automated tests. Lastly, we will train a model. Please keep in mind that I describe this as a process, not showing just final results ;) In case you missed it, first part is here .\n\nWoah! By using seaborn we are able to plot pair distribution between all variables. This is very useful to visually examine the data. And impressive when used for the first time ;)\n\nSo far we performed two basic steps\n\nNotebooks are great for experimenting but rather poor to execute the process. Now, we will move code to . Also, we will expand our pipeline with two new items:\n\nWe could argue if we really need to save back data just with added column names. Probably not, but in general our first preprocessing round can be more complicated.\n\nDo we really need read_raw_data method? It\u2019s only one line of code. Yes, we need. Will your colleague remember all parameters for non-standard csv file? And please remember that reading data can be far more complicated and including things like PCA. In I make a copy of a frame. This is because I like functional-like style where parameters are immutable.\n\n \n\nIn order to avoid enumerating all features explicitly there is a function for this.\n\n \n\nAnother one thing that\u2019s worth to mention is this excel file. It will be not used anywhere, however, I found it very useful to export Excel files in order to manually review them when needed. But this is completely optional (given that, it uses an optional parameter as well).\n\nTo make it complete create file with content:\n\nCode for plotting is much simpler:\n\nI am setting matplotib backend to because I found that using default one (QtAgg) on my system causes improper rendering.\n\nWe also have to create file just as in part one.\n\nLast time I explained what means. To recap, this is first prerequisite ( in this example). Accordingly, means the first target. So, above rule is equivalent to:\n\nWhy the hell should I care about and ?\n\nYou might wonder why I\u2019m using this crazy symbols instead of just pasting filename twice. The answer is: because make can use generic rules.\n\n \n\nImagine following rule:\n\nThis will compile every file into corresponding using . We will use this syntax later.\n\nNow we can update notebook! This will give us something like:\n\nEverything is fine except one thing. I changed my mind and decided to use Kernel Density Estimation on the plot. After modifying to , I re-run notebook and nothing happens. This is because jupyter does not reloaded my module. To fix this, add\n\nAt the beginning of the notebook. It will cause reloading modules when executing code.\n\nAutomated testing can provide a lot of value. Designing tests for machine learning project is a topic for separate article, so here I will present only very basics.\n\nNow, we only will test our assumptions about shape of data. Create file\n\nIt contains two simple tests. First, checks if raw data has assumed dimensions. Second one checks if there are 4 features and 1 label. Actual testing is done by using built-in statement. To run test session open terminal in project root and type:\n\nOutcome should look like:\n\nWe also should include testing into our workflow:\n\nDo not forget to mark it as .PHONY as it does not create any files\n\nFinally let\u2019s create a model. It can be implemented using various libraries \u2014 sklearn, theano, xgboost etc so it would be nice to have a common interface. .\n\nIt is very simple. I just want to mention save and load methods. They are here in order to provide a convenient way to persist trained models. Each model should define it by itself as it is strongly dependent.\n\nIn our pipeline, we would also need model training.\n\nThis is very simple. Just load preprocessed data and save trained model into a file. Of course we have to ensure trained models directory existence.\n\nWhole dependency graph now looks as follows:\n\nWe did quite a lot today. In next, last part I will show you how to deploy your model. We will create a simple web service and make it online using docker. If you want to read about something, let me know in comments.\n\nSee you soon :)"
    },
    {
        "url": "https://towardsdatascience.com/structure-and-automated-workflow-for-a-machine-learning-project-2fa30d661c1e?source=user_profile---------3----------------",
        "title": "Structure and automated workflow for a machine learning project \u2014 part 1",
        "text": "You start with a brand new idea for the machine learning project. First of all you download the dataset. Then perform some kind of preprocessing \u2014 possibly multi step because task is sophisticated. You create a bunch of models, some of them perform better while other ones worse. Oh shit, I overwrite my best model. No problem \u2014 will train it again. Wait. What were the parameters? Oh, got it. I\u2019m mostly done \u2014 just generate some nice plots and that\u2019s all. Maybe should I refactor a little? Nah, everything is fine, moreover \u2014 I could break something as it\u2019s quite fragile.\n\nA few months later you want to back, to it and maybe slightly modify to work with new data. Piece of cake. Git clone mycoolproject and\u2026 your mind went blank. What am I supposed to execute? In which order? What the hell System cannot find path specified? And from where data did previously come? Shit, I can\u2019t reproduce previous results :c Why?\n\nLooks similar? I bet you experienced similar scenario, especially if you are not much experienced in machine learning. And if you are not working in models that will be eventually deployed \u2014 so you would be kind of \u201cforced\u201d to do things the right way. On the other hand, you do not want to spend a lot of time on structuring your project. Here I will present you simple approach.\n\nBut before we start I made a few assumptions about You ;)\n\nOK, so what are benefits of presented approach?\n\nAt the beginning, after cloning new repo from GitHub we have following structure:\n\nLet\u2019s start with creating reproducible environment. Create file called We can start with following content:\n\nAs the project will grow up we will keep this file up to date. But now let\u2019s create an actual env:\n\nShit, I forgot to include jupyter package. No worries. Just update with jupyter and run\n\nAnd let\u2019s check if it works\n\nFirst step is to download the data. But we will not do it by copying from Downloads directory . Instead: we can create python script in\n\nAnd try to run it\n\nAaaaand it does not work. Because directory isn\u2019t there. Before we create it, hold on for a second. Here is great place to stop and think for a while. Firstly: our code should be robust to non-existing directories. Next, we should not create any directory by hand. If your colleague would be forced to create directory by hand every hour on error during saving results, he won\u2019t be happy. Also, calling always when we refer a directory might not be the best idea.\n\nWhat is more, downloading data is very first step in our workflow \u2014 and should be repeatable.\n\nSo let\u2019s start fixing this issues. At the beginning create directory structure for data. There would be three of them:\n\nWe said, that we need a way to enforce existing of this directories And it\u2019s simple way of doing this:\n\nGit does not store empty directories. By creating an empty file we can enforce directory persistence. By default, data should not be stored in git repository \u2014 so that we need to adjust :\n\nBut this makes data directories invisible for git. So It won\u2019t create them on clone. To do so, we have to force add\n\nWe should see something like:\n\nIf so, we can\n\nSince now, I will not commit to git explicitly \u2014 you can do it whenever you want.\n\nOK, we solved issue with directories. Let\u2019s move on to workflow. Downloading is very first step and should be written correctly. But what does it mean? Parameters should be possible to pass by command line arguments. For that we will use click library. Add to\n\nScript can look like this:\n\nWhooah, so far we definitely not simplifying things.\n\nTo explain: we define command with two parameters and click will handle this for us. If we type:\n\nWe will see:\n\nLooks nice. For actual downloading we have to call:\n\nNow we are ready to automate workflow. We will use \u2026 GNU Make. No. I\u2019m not joking. This tool suits very well for our needs. Also writing Makefiles is not as scary as you might think. Interesting fact: in practice this is very portable solution between Linux, Mac and Windows.\n\nOur first workflow will consist of three steps.\n\nAll depends on download so we have a graph of dependencies\n\nOf course, this graph will grow up in the future. The nice thing about make is, that it will handle most of the complexity by itself.\n\nThis is quite obvious \u2014 remove all downloaded data. Let\u2019s check it\n\nAnd is gone\n\nNext target would be download\n\nBefore the colon is target name \u2014 usually file that would be created (I will back to this soon). After colon \u2014 dependencies for the task. As we can see downloading is first step in our workflow so does not have any prerequisites. There is only one command in order to \u201cbuild\u201d iris.csv \u2014 calling download script. This crazy means \u201cfirst target in a rule\u201d. We have only one target (data/raw/iris.csv) so it will be inserted here. We can make a test drive:\n\nAnd indeed, file has been downloaded:\n\nLet\u2019s try to execute it one more time:\n\nMake knows that target already exists and there is no need to rebuild it.\n\nThere is only one piece left \u2014 . We define it as follows:\n\nThis means: \u201cIn order to build all you need to have data/raw/iris.csv\u201d\n\nLet\u2019s see it in action:\n\nJust three more things to add. When you type just make it will execute first target. So it is reasonable to make all first target in file . Secondly, you can use variables \u2014 for example to store URL\n\nFinally, targets that do not create files, should be marked as\n\nIf you want know why, try creating files with names either or ;)\n\nHere is the final Makefile:\n\nThis is the end of first part. In next ones I will show you how to further structure machine learning project and how to extend whole pipeline.\n\nIf you want to hear about something specific feel free to leave a comment."
    }
]