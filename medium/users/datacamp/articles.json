[
    {
        "url": "https://medium.com/datacamp/making-sense-of-design-systems-ffd38c6151d3?source=---------0",
        "title": "Making Sense of Design Systems \u2013 DataCamp \u2013",
        "text": "At DataCamp our mission is to democratize data science education for everyone, and with over 2 million students worldwide we\u2019re making rapid progress. Operating at this growth rate requires a lot of effort from everyone within the company and introduces a lot of exciting new challenges.\n\nFor the design team it means that we\u2019re shifting from page-by-page design to a more scalable way of designing. Reusing existing components, reducing development time, improving usability, and increasing accessibility are becoming more and more important.\n\nThis article covers how we are starting to make sense of a design system.\n\nA design system brings together all the components that are needed to distribute and reproduce a design solution. It\u2019s not something new or exclusive to digital product design \u2014 a lot of examples can be found throughout history.\n\nFor example, the first Chinese empire had the problem that archers couldn\u2019t use arrows from their comrades. Each arrow was specifically designed for a certain bow, so other arrows simply wouldn\u2019t fit. The first Chinese emperor, Ying Zheng, standardized the way arrows were designed and it had a huge impact on their military conquests.\n\nBack in the 50\u2019s, each station of the NYC Transit System had its own signage. It caused a lot of confusion because every single station had their own way of navigating. As a result, a lot of people were hesitant to use the transit system because they didn\u2019t understand how it worked. Massimo Vignelli and Bob Noorda created the NYC Transit Authority \u2014 Graphics Standards Manual, which documented how signs should be made and how they are supposed to be placed within stations. Over time this system has remained more or less the same and it\u2019s still actively being used today.\n\nWhen every page is designed in isolation, a lot of components have duplicates or similar alternatives. It makes a digital product hard to understand by users and hard to maintain by engineering. At DataCamp, we discovered over 20 different buttons throughout our product, each with their own quirks (different hover states, inconsistent disabled states, etc). It\u2019s hard to keep track of all these components when a product is growing rapidly and it becomes near impossible to maintain. That\u2019s where a design system kicks in.\n\nFor a digital product, a design system describes the user interface components that can be used to create an interface that solves a problem and will result in a cohesive experience for the user.\n\nWhen a design system is executed well it will result in a cascading effect that \u2026\n\nCreating, maintaining and enforcing a design system requires a lot of discipline and a continuous effort for everyone that\u2019s part of the creation process. It\u2019s not just design; engineering and product management are also important stakeholders. They all should be able to ask tough questions like: \u201cWhy are we building this?\u201d, \u201cCan we reuse this component somewhere else?\u201d, \u201cDo we already have something similar?\u201d, \u201cDoes this custom component add enough benefit opposed to the effort we put in?\u201d.\n\nGoal: Make sure everyone knows the existence and status of a component.\n\nWhen I started at DataCamp there was a Sketch file that contained colors, icons, fonts and some basic components. It was far from complete but it was a good starting point. Front-end engineering also had a collection of components they used across projects.\n\nTogether we created an inventory in Airtable based on the components that are used in production. This gave the design team an idea of what already existed and prevented us from unknowingly creating new components. It also gave us an insight on how many similar components we had and how bad the situation really was.\n\nWhen we started comparing components we noticed Design and Engineering were not speaking the same language. For example the \u2018primary color\u2019 that was defined by design was the \u2018secondary color\u2019 in code. Things like this can lead to miscommunication when teams start to scale and causes confusion and frustration. We made sure that the same lingo is being used across the different teams.\n\nWorking together as a design team introduces its own set of challenges. We used to store designs in Dropbox but when our team started to scale, the risks of losing work and the effort to keep track of changes increased dramatically.\n\nOur design team was really open for a workflow change and we started using Abstract. It enabled us to properly version our designs, have a transparent workflow, and have more confidence while designing.\n\nWe started documenting our process and design system using Slite. If we have to explain something more than once it should be documented and shared with the team. This avoids a lot of communication overhead and prevents information from getting lost.\n\nIt\u2019s important that design reviews happen in both directions. Designers should be able to review front-end, but front-end should also be able to review design. Designers have this urge to create something cool and new. Engineers should be able to ask tough questions like: \u201cWhy are we building something custom?\u201d, \u201cCan\u2019t we reuse this component?\u201d, \u201cCan we make this component better?\u201d, \u201cIs this component worth the extra development effort?\u201d. It\u2019s about justifying decisions and setting expectations to everyone involved.\n\nWe just started putting together our design system and we\u2019re still trying to make sense of all the duplicate components we found throughout our product. There\u2019s still a long road ahead but with every component we touch we\u2019re creating a better, more consistent, and easier to learn product for our customers.\n\nWe\u2019re curious how other organizations make sense of a design system. Send me an email if you\u2019d like to share your experience.\n\nAt DataCamp we\u2019re always looking for talented people to join our team. Interested? Check our open positions."
    },
    {
        "url": "https://medium.com/datacamp/duolingo-style-learning-for-data-science-datacamp-for-mobile-3861d1bc02df?source=---------1",
        "title": "Duolingo-style Learning for Data Science: DataCamp for Mobile",
        "text": "Data science is the future, but learning it takes a lot of time. Time you don\u2019t have? Wrong. With DataCamp for Mobile, you master data science while on the train or on the bus (or on the toilet, as we\u2019ll explain). You don\u2019t have to be a math genius either. DataCamp\u2019s Learning by Doing methodology makes learning it intuitive for everyone.\n\nAt DataCamp, we\u2019re dedicated to bringing you the best possible learning experience for data science. At the core, there are our 100+ intuitive web courses teaching the fundamentals of R, Python, and SQL as well as more advanced topics like Machine Learning and Deep Learning. Furthermore, to attain true proficiency, our Practice Mode lets you train what you\u2019ve learned before, and with Projects you get hands-on experience solving real-world problems.\n\nHowever, a big problem for many of our users is finding the time to learn and practice. That\u2019s why we wanted to make a version of DataCamp that better fits a busy schedule. But how? The answer is simple: better use the time that is otherwise lost. Did you know you spend 20 hours per year on the toilet? That\u2019s more time than you need to complete our Python Programming skill track!\n\nDataCamp for Mobile (available in the Apple App Store and Google Play Store) offers courses on Python, R and SQL that help you master the practical skill set to do data science. Taught in a Duolingo-style way, you\u2019ll still learn the same concepts as you would in our web courses. It also contains a practice mode to train what you learned before.\n\nCourses are divided into 5 minute lessons that teach you one concept at a time. Each lesson gently introduces data science concepts without making you read tons of text. Instead, all material is presented as an exercise. Starting from zero, each exercise builds on the next to break down even the most complex data science concepts in understandable pieces. This learning by doing methodology is the core of the DataCamp experience and the foundation of its success.\n\nOur users seem to like it so far: the app was downloaded almost 100.000 times in only a few weeks, and the average app store rating is 4.5/5.\n\nThere are two main challenges when teaching on mobile: the small screen size and the limited attention span of mobile users compared to web users. That\u2019s why there is only one way to make the mobile app successful: creating a learning experience that is as engaging as a game.\n\nEvery 5-minute lesson is divided into a set of exercises and is designed to only take a couple of minutes. Here are a couple of examples of the exercises you can expect:"
    },
    {
        "url": "https://medium.com/datacamp/datacamp-the-plan-7ff04346fd6b?source=---------2",
        "title": "DataCamp: The plan \u2013 DataCamp \u2013",
        "text": "In a previous post, I gave some context on why we started DataCamp at a macro level. This post describes the key steps in building out DataCamp, i.e. the plan.\n\nFirst of all, let\u2019s fast forward to what we\u2019re trying to achieve here in the long run: \u201cDataCamp\u2019s mission is to help individuals and companies at every step of their journey to become data fluent by building the smartest data science education platform out there.\u201d .\n\nTo be more specific, the end goal is to:\n\nAt a high level, there\u2019s 5 steps to reach that end goal:\n\nStep 1: Build the best learning platform for data science \n\nStep 2: Attract and retain students\n\nStep 3: Attract and retain the best instructors\n\nStep 4: Help companies to retrain their workforce\n\nStep 5: Expand our offering\n\nIn what follows, we take a deeper dive into the steps of the plan. All of these steps are required to reach the end goal. They are sequential in the sense that we want to get to a decent level for every step, before we move to the next step. Focus is crucial as explained in step 5. That said, it\u2019s extremely important to understand that we need to keep executing on the first steps as we move to new ones.\n\nStep 1: Build an engaging and effective platform to learn data science \n\nWe\u2019ve built 3 different (sub)products that help students learn data science in an engaging and effective way:\n\nInteractive courses: Our focus on data science enables us to build interactive exercise interfaces that work really well for learning data science. Students solve challenges with R/Python/.. in the comfort of their browser. Furthermore, we\u2019ve created custom technology to give personalized and instant feedback to students when they are solving challenges. We want our students to spend at least 70% of the time with their hands on the keyboard, actively solving challenges and receiving instant and personalized feedback. Not only does this lead to higher engagement (many of our courses have completion rates of over 50%), we believe it is a more effective way of learning for most students.\n\nPractice mode: Our interactive courses are great to teach students new skills. The courses are not very helpful to build fluency and retention of knowledge. In contrast, for practice mode, we encourage students to practice regularly as spaced repetition leads to better knowledge retention. Furthermore, the somewhat simplified bite-sized challenges help students to build fluency. Practice mode is a fairly new part of our product: the goal is to become really good at understanding someone\u2019s skill level and offer the right amount of practice on the right topics at the right time.\n\nFurthermore, we\u2019re investing heavily in a mobile app that allows our students to learn new topics and practice data fluency on the go.\n\nProjects: The previous 2 elements of our learning product help to: (i) gain understanding of a topic, (ii) become fluent in applying it and (iii) have a high probability of retaining that knowledge. There\u2019s still something missing though. Our students want to be able to make better decisions based on data and gain insight from data. They want to tackle real data science projects. In the next few weeks, we\u2019ll launch a new part of the product: projects. Projects help students transition into the real world of data science. The goal here is to bring the learning experience as close as possible to what they will be doing in actual data science projects.\n\nBy the end of 2017, we expect these 3 core components of the learning experience to reach a decent level of maturity from a product perspective. In addition to further improving them, we want to invest in:\n\nWhen we built the first version of the platform, we faced the chicken or egg problem: If you don\u2019t have students, no one is interested in building courses, if you don\u2019t have courses you certainly won\u2019t attract any students.\n\nWe did two things to overcome this challenge. First, we created a course for absolute beginners and made it available for free. As people started linking to that course, it started ranking in top position for various \u201clearn R\u201d related search terms. We intend to keep releasing free courses on topics that matter to beginners. Second, we built complements to existing courses on Coursera to improve their lab/exercise components, and set up a similar collaboration with Microsoft for their edX courses (on R, Python and SQL). These collaborations helped to get the word out on DataCamp initially.\n\nDataCamp now has over 1.7 million students. We\u2019re ready to welcome millions more in 2018. A few of our current initiatives to welcome new students:\n\nWe love offering free services to our students. That said, we also want to build a large and sustainable business. This is the best way to ensure that we can keep innovating and providing value to our students. Our business model is simple: to get access to the entire content library students pay an affordable fee of $29/month or $300/year. This is either on par or much lower than alternative options. We are committed to maintaining a low price point as we believe in the importance of data fluency for a large group in society. There\u2019s room for improvement here though: ideally, we would have a price point adjusted to the GDP per capita in a country for example (we have subscribers from most countries in the world).\n\nAs we\u2019re getting better at attracting students, our focus should be shifting more towards student retention. Supporting students in their journey certainly helps to improve retention. Unfortunately, due to our rapid growth, we\u2019ve not always been able to provide excellent support. By the end of 2017, we\u2019ll have a dedicated customer support and a customer success team. Not surprisingly, our data shows that product improvement and the addition of more content also has a positive effect on student retention.\n\nStep 3: Attract and retain the best instructors\n\nAs mentioned above, we believe creating a network of expert instructors to teach our data science courses. Instructors build DataCamp courses for various reasons. The three main reasons being:\n\nMaking sure our instructors get value from creating content on DataCamp is essential for the long term success of the platform. Therefore, we have a content team focused on (i) finding the best people in the world to join that network, (ii) supporting instructors to teach according to best practices and (iii) providing practical support and insightful feedback to help them create and maintain the best data science courses out there.\n\nAs there\u2019s a learning curve to creating DataCamp content, we want instructors to create multiple courses. The following external instructors already created multiple courses on the platform: Garrett Grolemund, Dhavide Aruliah, Charlotte Wickham, Rick Scavetta, Ben Baumer, Justin Bois, David Robinson, Jason Myers, Daniel Kaplan.\n\nIn addition to forging a network of instructors, we\u2019re designing a scalable process for the creation of high-quality content. Technology is key in this context. We have invested heavily in technology that makes it easy to create interactive courses on DataCamp. That said, there\u2019s still a lot of room for improvement on DataCamp\u2019s authoring environment. It\u2019s a core part of our strategy to make it very easy and scalable to create content for DataCamp\u2019s platform (see this post as well). With this network of instructors and efficient authoring technology, we believe it\u2019s realistic to have about 1,000 content modules (=courses/projects/practice modes) by 2019.\n\nIt doesn\u2019t stop at authoring great content though. Students have completed over 70 million exercises on DataCamp. We can learn a lot from that data, and our content team and instructors can and do use that data to improve the content. We have an unfair advantage here as a company, since a large percentage of DataCamp employees and instructors actually know how to work with data well.\n\nStep 4: Help companies to retrain their workforce\n\nMost people learn data related skills in a professional context. Therefore, the end game is to offer scalable training solutions to companies. It helps increase DataCamp\u2019s impact and provides a more stable revenue stream.\n\nWith already over 1.7 million registered students, we can rely on engaged professional students to help bring DataCamp into their organization. Often, we start by training smaller groups of people (20\u201350). Their enthusiasm then spreads within the organization and other groups sign up for a DataCamp for business license. Ultimately, we want to help the entire organization become more data literate through enterprise licenses. If you\u2019re curious how DataCamp is helping companies, have a look at the case studies on our business page.\n\nAs we move towards helping organizations retrain their workforce, we\u2019re also expanding our product offering to cater to the needs of larger organizations. In the next months (years), we\u2019ll continue to launch product features focused at helping organizations such as:\n\nMany students, employees & investors have asked me some version of \u201cWhy is DataCamp not doing XYZ\u201d? With XYZ being, anything from mentorship, to recruiting, to certification, etc. The real question is often not why not but when is it appropriate.\n\nIn the first years of building DataCamp, we deliberately want to remain hyper focused on building a learning resource with the best instructors. We believe that creating a great learning resource for data science is not only what\u2019s most missing in the market right now. It\u2019s also the most defensible asset. The defensibility follows from: (i) the fact that it takes a lot of time, effort and the right team to create such a resource (content+product), and (ii) that it\u2019s an asset that captures people and companies at the start of their journey towards data fluency.\n\nWhen we feel we master the first four steps, we want to expand our offering (at the time of this writing, DataCamp is gradually shifting focus to step 4). The importance of certification for skill level was discussed in a previous blog post, and seems the most likely candidate for initial expansion of our offering. We know our individual subscribers and corporate clients want mentorship, help in their job search, etc., so we won\u2019t stop there. DataCamp wants to help as they progress the journey towards more data fluency."
    },
    {
        "url": "https://medium.com/datacamp/context-on-why-we-started-datacamp-d4df02fcc87c?source=---------3",
        "title": "Context on why we started DataCamp \u2013 DataCamp \u2013",
        "text": "DataCamp\u2019s mission is to help individuals and companies at every step of their journey to become data fluent by building the smartest data science education platform out there.\n\nThis post aims to provide more context on why we started DataCamp at a macro level. In the first section of the post, we\u2019ll discuss the data science (talent) market. The second part covers the core pillars that support our vision for the future of data science education. Finally, we tackle the importance of certification.\n\nUnderstanding how to analyze data enables people and companies to make better decisions, create better & new services and products, etc. We believe the impact data analysis and data science will have over the next 20 years, will be as large as the impact software engineering and computer science have had in the last 20 years. Software is eating the world, but data science and AI are going to have software engineering for supper!\n\nWhile that\u2019s very exciting, there\u2019s a problem: Harvard Business Review and a McKinsey report first shined the spotlight on the data scientist shortage. Today, companies in every single industry are indeed struggling to find data scientists or people with a decent level of data literacy/fluency. Numbers on the (expected) shortage of data scientists range from hundreds of thousands to millions in the more recent research reports on this topic.\n\nWhile you can debate the accuracy of these predictions, one thing is clear: traditional educational institutions can\u2019t train enough people in the next decades to resolve this gap. Furthermore, a large part of the world does not have access to high-quality data science education at traditional education institutions because it\u2019s often (very) expensive.\n\nConsequently, the power of data science can only be harnessed if we find ways to retrain the existing workforce affordably. On the one hand, you should expect a large group of individuals to up-skill themselves for career advancement (and career shifts to a lesser extent), on the other hand, expect companies to retrain significant portions of their workforce. Note as well that the group of people learning for career advancement is larger than people learning because they want to change careers. This observation is not new. lynda.com and Pluralsight have proven that a focus on continuing education is a winning strategy in the creative and developer space respectively.\n\nFinally, it\u2019s important to realize that the skill gap exists at multiple levels: It ranges from people working with data on a weekly basis who run into the limits of the tools they currently use (often Microsoft Excel), to data analysts and data scientists, to managers and consultants. All of these groups have different needs, but for all these groups, there\u2019s a shortage of people with the right level of data literacy/fluency.\n\nBecause of the above, we believe the education market for data literacy is huge and probably larger than the market for teaching basic coding skills. Two data points seem to confirm the validity of this belief:\n\nOpen-source software development has completely transformed the web development community and is now transforming the applied statistics and data science community. Whereas SAS and SPSS dominated the applied statistics world not that long ago, open-source technologies like R and Python seem to have pretty much taken over. By the way, DataCamp has also open-sourced some courses and technology.\n\nIt\u2019s difficult to argue with the growth of open-source software as the foundational layer of the future of data science. This is the main reason all of our data science training is focused on open-source at the moment. Furthermore, its very nature makes it easier to build interactive educational tools on top of it.\n\nA focus on open-source also allows us to give back to the open-source data science communities. We provide scalable revenue streams to open-source contributors who build DataCamp courses through our royalty model.\n\nWhile open-source will remain a large part of our curriculum in the years to come, we ultimately want to offer training on every relevant tool and methodology to our students. Expect us to start building training for proprietary tools as well in the future.\n\nA vertical focus and technology enables the best learning experience\n\nHistorically, educational institutions covered a wide range of topics (let\u2019s call this a horizontal focus). For example, the oldest university in Europe teaches topics ranging from history, economics, psychology, statistics, etc. The horizontal focus made sense at the time because of the combination of (i) economies of scale (around buildings, student administration, attracting the smartest professors, brand value for certification, etc.) and (ii) a constrained geographical reach given high communication and transportation costs. Horizontal focus was the dominant education model for centuries, which is why we\u2019re still used to thinking about educational institutions as being horizontally focused.\n\nCosts of transportation & communication and the fixed costs of the educational models have dropped significantly in recent decades, thanks to technological advancement. So\u2026 how does that impact educational landscape now and in the future?"
    },
    {
        "url": "https://medium.com/datacamp/what-world-war-2-can-teach-you-about-product-management-3053138f8be8?source=---------4",
        "title": "What World War 2 Can Teach You About Product Management",
        "text": "Leading product at a fast growing company (DataCamp) and being interested in history, I often find that the stories of World War 2 provide me with useful insights for my daily problems. Today, I wanted to focus on how the Western Allies\u2019 better product management led to their victory.\n\nAt the outbreak of WW2, Nazi Germany seemed to have the winning hand: a rising empire, military victories at Blitzkrieg speed, superior weaponry, and Europe at its knees. Five years later, they had nothing. Germany was a crippled country with no resources left.\n\nShipping and Logistics Are Everything \u2014 How Germany\u2019s Scope Creep and Inability to Scale Accelerated the Allied Victory\n\nDuring World War 2 Nazi Germany propagandized the \u201cWunderwaffe\u201d or \u201cMiracle Weapon\u201d that could tip the scales of war to the benefit of Germany.\n\nBelieving more advanced technology would yield a higher return on investment (a thesis that probably sounds familiar if you hang out in today\u2019s start-up and scale-up scene), Germany started developing advanced weaponry, ranging from the first rocket powered airplane to aircraft carriers and nuclear bombs.\n\nHowever, they did not ship. And in the occasions they did, it took a lot more time than expected.\n\nThe first rocket-propelled plane (the Messerschmitt Me 163) came out at a time that the Allies already had total dominance of the airspace, and the aircraft carriers and nuclear bombs never even made it \u201cto production\u201d. A similar fate was reserved for the Horten Ho 229, one of the first stealth bombers ever to be developed.\n\nIn product managerial lexicon, Nazi Germany had \u201cScope Creep\u201d: projects that grow into infinite complexity, that undergo iteration over iteration, that eat up resources, to eventually die or launch too late at a time when your competitors have already divided the pie.\n\nIf, as a product manager, you don\u2019t ship your product or product feature quickly enough, it does not matter. The other side will win."
    },
    {
        "url": "https://medium.com/datacamp/technical-vision-blog-part-3-cabde9c7a041?source=---------5",
        "title": "Technical Vision Blog \u2014 Part 3 \u2013 DataCamp \u2013",
        "text": "As our engineering team started growing, so did our number of microservices. Initially we started off with 3, now we have closer to 30. We believe in giving our engineers ownership over their services. This means that they have a high level of freedom on how to structure their application, which programming languages to use, which API to expose, etc.\n\nIn this part we will discuss how we plan on creating more consistency between services, without limiting the ownership of our engineers\n\nWe have grown rapidly in the last few years. As a result, we have taken some technical debt when wanting to launch new services ASAP. Some of our stack is running on Heroku (mostly lower traffic services), others on Amazon EC2 Container Service (ECS) (direct or by using Amazon Elastic Beanstalk). This seems fine, however when auditing the services we found some irregularities:\n\nAs a result some engineers need to adapt their applications to Heroku, others need to write Dockerfiles, some need to learn how to configure ECS, etc.\n\nWe have chosen to enforce consistency by moving all microservices to our own cluster:\n\nThe infrastructure team will become responsible for maintaining and improving the cluster. Their responsibility however, ends at the container level. All containers run in an orchestrator, so in a way they could affect each other. For example if one container starts dumping GBs of data on disk, it could affect disk performance or even cause an outage of the node.\n\nAt Google, applications need to be engineered so they are compatible with the Borg cluster. For example a service like the Multiplexer (responsible for starting up other R / Python / SQL containers) will not be able to run on our Kubernetes cluster out of the box.\n\nThis means that when there is a new service that should be deployed on the cluster, the engineers should sync with the infrastructure team, explain the high level functionality, expected load, expected CPU / RAM / Storage requirements, how downtime will affect other services, etc. Given part 1 on how to scale communication, this info is ideally part of the Github Wiki of the service.\n\nGiven that the choice has a big impact on recruiting, making cross-team pull-requests, speed for people to switch teams, etc. We decided to limit our stack for web applications to Ruby on Rails, NodeJS, React and Python.\n\nDoes this mean we will never use Go? No. However this does imply that if there is a team that would like to start using Go for a new service, it\u2019s not something they should decide by themselves. Hence the idea is to start a broader discussion with the engineering team, to see if we are going to expand our stack to include Go.\n\nEnforcing rules or limitations is never an easy task, however we hope that what we just described will increase productivity for all engineers in the long run, without limiting ownership."
    },
    {
        "url": "https://medium.com/datacamp/how-to-scale-datacamps-engineering-group-f319a833bcd4?source=---------6",
        "title": "How to scale DataCamp\u2019s engineering group? \u2013 DataCamp \u2013",
        "text": "This post focuses on how to scale the people building the tech behind DataCamp, not on how to scale the actual technology (for the latter, I refer to our CTO\u2019s technical vision). Scaling the engineering group in a smart way is essential for the company. The goal of this post is to lay the foundation for a structure that will allow DataCamp\u2019s engineering group to double or triple in size in the next 12 months.\n\nThe simple fact that I\u2019m writing this is a testament to the fact that we\u2019re missing a key position on the DataCamp management team. We won\u2019t be able to scale the team engineering team unless we have an end responsible for the people in the engineering group.\n\nOn the one hand, we want to grow the engineering group in a highly decentralized fashion with fairly independent teams, who can use different technologies, development practices, etc. (similar to spotify). On the other hand, we\u2019ll have quite a few team leads and product managers over time, which will create a decent amount of chaos. Chaos is good and intended as innovation needs freedom and chaos. The remainder of this post focuses on how we can control that level of chaos and freedom. We need just enough people and process management to avoid total anarchy while maintaining innovation.\n\nLet\u2019s start with a few examples of what\u2019s currently not going well:\n\nTo be specific, the key problem is that there\u2019s no clear end responsible for:\n\nTo sum all of that up: \n\nWhile the technical vision is set by the CTO, someone else ultimately needs to own execution of the technical vision and hold people accountable to execute the technical vision. As the engineering team grows, ensuring the execution of the tech vision will become less straightforward, while it\u2019s obvious with just a few engineers. And\u2026\n\nWe\u2019ll be growing teams and adding teams in the next few months. We\u2019re moving to a situation with at least the following teams by our next company gathering in October:\n\nGiven our focus on decentralization, ownership and microservices, we mistakenly assumed that close to zero management of engineering team leads and coordination between team leads was OK. The examples above & future growth plans indicate that we\u2019ve reached the stage where this is absolutely no longer true. The role of the CTO at DataCamp has changed recently and I have moved away from regular check ins with engineering team leads some time ago, essentially leaving people & project management of the engineering group to be owned by no-one.\n\nBy consequence, we now miss a position typically referred to as: VP engineering. Most companies create this role much earlier, our focus on ownership and decentralization has allowed us to postpone filling this role until now.\n\nFor more context, please read:\n\nBoth the CTO as well as the VP engineering should be part of the senior management team. To make this very tangible: In senior management meetings, the CTO will answer questions and bring up concerns related to the technology (e.g. technical vision around microservices, what technologies should we use, what\u2019s the feasibility of certain new projects, etc.). The VP engineering should be able to answer questions related to people and process of the engineering group (e.g. who\u2019s ready to become a team lead, what\u2019s the status of projects across teams, why have certain aspects of the tech vision not been implemented yet, how can we improve communication with product managers, etc.).\n\nWe believe scaling a decentralized engineering team will work best if we ensure that:\n\nWe\u2019re very excited to welcome Filip Schouwenaars in this new role. He\u2019ll be transitioning into the new role in the next few weeks. At a high level, we envision his key responsibilities in the next 6\u201312 months to be the following:"
    },
    {
        "url": "https://medium.com/datacamp/start-with-why-bf0a07ab0ce4?source=---------7",
        "title": "Start with why \u2013 DataCamp \u2013",
        "text": "Some context first on DataCamp\n\nAt the start of 2016, DataCamp\u2019s team consisted of 8 people. Less than 18 months later, we\u2019ve grown to 45 people. DataCamp\u2019s subscriber numbers and courses have grown exponentially as well in that period. In the spirit of transparency, since the start of 2016:\n\nWhile that\u2019s incredibly exciting, it has forced us to rethink both internal as well as external communication flows. We hope to fix some of the internal communication challenges we\u2019ve recently experienced by regularly explaining why DataCamp is doing certain things, by sharing mistakes, challenges, future goals, etc. In addition to improving internal communication, by being transparent, I hope other entrepreneurs may learn from our past and future mistakes in building a SAAS (education) company.\n\nThe initial posts will cover the following topics (among other things):"
    },
    {
        "url": "https://medium.com/datacamp/technical-vision-part-2-e3eb53dbaf5d?source=---------8",
        "title": "Technical Vision \u2014 Part 2 \u2013 DataCamp \u2013",
        "text": "At DataCamp we have more than doubled our engineering team in the last 12 months. The first part of this series goes in detail on which technical changes are required for us to scale our internal communication. This part goes more in depth on how we plan to change our technical architecture.\n\nAs mentioned in part 1, we want engineers to take ownership over their services. This requires a significant amount of granularity in our architecture. (We don\u2019t like having two teams both owning a service)\n\nThe fact that we have a \u201cmain app\u201d, shows that we have too much logic in one place. This application is responsible for DataCamp for Groups, integrations with other learning platforms, user authentication, course info, tracking user progress, etc. This has some implications:\n\nAdditionally, some services like the Projector app (responsible for video playback on DataCamp) are highly coupled to the main app. We want to have a clear separation of concerns between our services.\n\nOur flow for extracting the campus-backend (an API that will manage all data related to taking courses) from the main app would look something like this:\n\nIn part 1, we talked about the benefits of looking at our internal services as if they were open source, with a focus on improving communication among teams. An additional benefit that we did not discuss, is that this mindset imposes questions like \u201cIf we were to open source this service, would anyone use it?\u201d, \u201cHow coupled is this library to our specific problem?\u201d, \u201cHow hard would it be for other people to integrate with it?\u201d, etc.\n\nAssuming that other teams will interact with the newly created service, as if it were to be an open source library, automatically puts you in a mindset of high decoupling.\n\nThe next part will go over how we want to enforce some consistency among our services, without limiting ownership in our engineering teams."
    },
    {
        "url": "https://medium.com/datacamp/technical-vision-part-1-5f016c163340?source=---------9",
        "title": "Technical Vision \u2014 Part 1 \u2013 DataCamp \u2013",
        "text": "What has worked really well for us so far, is giving engineers ownership over their applications. It means that they become responsible for creating technical specs for new features, manage the planning of their team, do the implementation, make sure everything is working as expected when they deploy. We don\u2019t have a QA team, which means that our engineers are responsible for the stability of what they push to production.\n\nThis level of decentralization implies that we have a flat engineering organisation. Our engineering teams consist of a team lead and 0\u20135 additional engineers. Each team is responsible for a clearly defined set of features of our stack. These features are driven by multiple microservices, owned by the team.\n\nThis blog is divided into three parts:\n\nGiven the number of teams we have, communication is becoming increasingly more important. Looking at some of the questions new and existing engineers ask:\n\nWhat we get is something like this:\n\nWithout a structured process, every team has to communicate with all other teams for information. This means that if we have n teams, the number of inter-team communication flows become n*(n-1)/2. For example, 6 teams gives us 15 flows. This becomes hard to scale.\n\nLet\u2019s take a step back and look at what communication flows seem to work well. At DataCamp we use some of the most popular open-source libraries and frameworks like React, React Native, Express, Koa, Ruby on Rails, etc. What\u2019s striking, is that we are able to use them, without needing to have contact with the maintainers.\n\nWhy is this? Imagine trying to use React, when there is no documentation. You probably wouldn\u2019t consider using this in production. Let\u2019s keep this in mind and try to come up with a list of criteria that are needed to scale a library or service:\n\nIn order for us to scale communication between engineering teams, the goal should be, that when an engineer wants to use an internal service or library, all they need to do, is read the documentation.\n\nThe next part will detail how we plan on scaling our technical architecture."
    },
    {
        "url": "https://medium.com/datacamp/cto-role-and-responsibilities-8b5d83733b29",
        "title": "CTO \u2014 Role and responsibilities \u2013 DataCamp \u2013",
        "text": "The role of a CTO changes when the company grows. In the first months of this year, I have spent most of my time in the Growth Engineering (GE) team and have been heading up the infrastructure team. Now that we have a Product manager and a team lead for the GE team, and someone onboarded on the infrastructure side. I\u2019d like to evaluate where I could add the most value to the company.\n\nThis quora answer is surprisingly applicable to our history and current situation. To my understanding, we are currently in:\n\nAnd in about 6\u201318 months, we\u2019ll move towards\u2026\n\nThe question now becomes: Which responsibilities remain with the CTO, which get added and which are removed?\n\nI envision taking on the following 4 responsibilities, listed below in order of importance, and with a rough estimate for how much time I expect these to take:\n\nAs the engineering team grows, and more microservices are created, it becomes increasingly more difficult to understand how they work together. The engineering subteams (Growth, Learn, Teach engineering, etc.) are all responsible for multiple microservices, that need to interact with services from other teams. Someone needs to make sure that these microservices are interacting in a way that represents the vision of all PMs and teams:\n\nIn order to be able to make good architectural decisions, it\u2019s important that the CTO has a good understanding of the latest technologies. The best way of learning is by doing ;-). The goal is to investigate the viability of new projects / features / products that could have a huge impact on our business. The deliverable is an MVP, combined with a plan on how the MVP can be turned into a full application.\n\nThe biggest challenge we have faced so far is hiring. Yet, we have not managed to come up with a scalable solution. Right now, it takes us months to hire 10 really good software engineers. That sets a bottleneck on our growth as a team. How can we make this scale?\n\nMost really great developers don\u2019t go to job fairs (often), and already have a job they love.\n\nGreat developers, to my understanding, want to stay up-to-speed with what\u2019s going on in the space and are intrinsically motivated to learn about new things. The best places for this are meetups and conferences.\n\nWhy do people want to work for Heroku, Netflix, Zalando, Google, Facebook? Their products aren\u2019t that different from their competitors, yet they seem to attract a huge amount of applicants. I believe that one of the reasons is due to the perception that they are working on super cool problems. Just reading Netflix\u2019s tech blog, or Facebook\u2019s engineering blog makes you want to apply for a job.\n\nThe ultimate goal is to have a self-reinforcing system where:\n\nWe hire great developers working on cool projects -> We can talk about these at conferences / write about them on blogs -> Attract more great developers and improve the ones working for us -> Close the loop. If we provide the support so that our engineers can become authorities in their domain (rails, react, etc), everyone wins.\n\nGiving awesome talks at conferences is not only fun, we\u2019ll attract the interest of devs attending. Even if they are not looking for a job right away, when the time comes that they are looking for alternatives, DataCamp has to be the among the first names that come to mind. A more scalable alternative to conferences/meetups are webinars and podcasts (check e.g. this list of podcasts).\n\nI believe that the only way this can be done is by creating an engineering culture that promotes working on open-source projects, speaking at conferences and meetups, and writing about technical challenges we overcame. In my opinion this can only work if I focus a lot more time and effort on technical evangelism.\n\nI think the focus for the CTO should be on the Technical evangelism when it comes to recruiting longer term. Making sure there are as many great applicants as possible applying for our positions is something that works at scale. In the next 6\u201312 months, it also makes sense to keep the CTO as one of the final checks on technical ability in the recruiting funnel."
    },
    {
        "url": "https://medium.com/datacamp/pyspark-cheat-sheet-spark-dataframes-in-python-2a4a1fcb6d29",
        "title": "PySpark Cheat Sheet: Spark DataFrames in Python \u2013 DataCamp \u2013",
        "text": "You\u2019ll probably already know about Apache Spark, the fast, general and open-source engine for big data processing; It has built-in modules for streaming, SQL, machine learning and graph processing. Spark allows you to speed analytic applications up to 100 times faster compared to other technologies on the market today. Interfacing Spark with Python is easy with PySpark: this Spark Python API exposes the Spark programming model to Python.\n\nThe PySpark Basics cheat sheet already showed you how to work with the most basic building blocks, RDDs.\n\nNow, it\u2019s time to tackle the Spark SQL module, which is meant for structured data processing, and the DataFrame API, which is not only available in Python, but also in Scala, Java, and R. If you want to know more about the differences between RDDs, DataFrames, and DataSets, consider taking a look at Apache Spark in Python: Beginner\u2019s Guide.\n\nWithout further ado, here\u2019s the cheat sheet:\n\nClick here to download the cheat sheet.\n\nThis PySpark SQL cheat sheet covers the basics of working with the Apache Spark DataFrames in Python: from initializing the SparkSession to creating DataFrames, inspecting the data, handling duplicate values, querying, adding, updating or removing columns, grouping, filtering or sorting data. You\u2019ll also see that this cheat sheet also on how to runSQL Queries programmatically, how to save your data to parquet and JSON files, and how to stop your SparkSession.\n\nMake sure to check out our other Python cheat sheets for data science, which cover topics such as Python basics, Numpy, Pandas, Pandas Data Wrangling and much more!"
    },
    {
        "url": "https://medium.com/datacamp/python-for-finance-algorithmic-trading-60fdfb9bb20d",
        "title": "Python For Finance: Algorithmic Trading \u2013 DataCamp \u2013",
        "text": "Technology has become an asset in finance: financial institutions are now evolving to technology companies rather than just staying occupied with just the financial aspect: besides the fact that technology brings about innovation the speeds and can help to gain a competitive advantage, the speed and frequency of financial transactions, together with the large data volumes, makes that financial institutions\u2019 attention for technology has increased over the years and that technology has indeed become a main enabler in finance.\n\nAmong the hottest programming languages for finance, you\u2019ll find R and Python, alongside languages such as C++, C# and Java. In this tutorial, you\u2019ll learn how to get started with Python for finance. The tutorial will cover the following:\n\nDownload the Jupyter notebook of this tutorial here.\n\nBefore you go into trading strategies, it\u2019s a good idea to get the hang of the basics first. This first part of the tutorial will focus on explaining the Python basics that you need to get started. This does not mean, however, that you\u2019ll start completely from zero: you should have at least done DataCamp\u2019s free Intro to Python for Data Science course, in which you learned how to work with Python lists, packages and NumPy. Additionally, it is desired to already know the basics of Pandas, the well-known Python data manipulation package, but this is no requirement. If you do want to already get into Pandas before starting this tutorial, consider taking DataCamp\u2019s Pandas Foundations course.\n\nWhen a company wants to grow and undertake new projects or expand, it can issue stocks to raise capital. A stock represents a share in the ownership of a company and is issued in return for money. Stocks are bought and sold: buyers and sellers trade existing, previously issued shares. The price at which stocks are sold can move independent of the company\u2019s success: the prices instead reflect supply and demand. This means that, whenever a stock is considered as \u2018desirable\u2019, due to a success, popularity, \u2026 the stock price will go up.\n\nNote that stocks are not exactly the same as bonds, which is when companies raise money through borrowing, either as a loan from a bank or by issuing debt.\n\nAs you just read, buying and selling or trading is essential when you\u2019re talking about stocks, but certainly not limited to it: trading is the act of buying or selling an asset, which could be financial security, like stock, a bond or a tangible product, such as gold or oil.\n\nStock trading is then the process of the cash that is paid for the stocks is converted into a share in the ownership of a company, which can be converted back to cash by selling, and this all hopefully with a profit. Now, to achieve a profitable return, you either go long or short in markets: you either buy shares thinking that the stock price will go up to sell at a higher price in the future, or you sell your stock, expecting that you can buy it back at a lower price and realize a profit. When you follow a fixed plan to go long or short in markets, you have a trading strategy.\n\nDeveloping a trading strategy is something that goes through a couple of phases, just like when you, for example, build machine learning models: you formulate a strategy and specify it in a form that you can test on your computer, you do some preliminary testing or backtesting, you optimize your strategy and lastly, you evaluate the performance and robustness of your strategy.\n\nTrading strategies are usually verified by backtesting: you reconstruct, with historical data, trades that would have occurred in the past using the rules that are defined with the strategy that you have developed. This way, you can get an idea of the effectiveness of your strategy and you can use it as a starting point to optimize and improve your strategy before applying it to real markets. Of course, this all relies heavily on the underlying theory or belief that any strategy that has worked out well in the past will likely also work out well in the future, and, that any strategy that has performed poorly in the past will likely also do badly in the future.\n\nA time series is a sequence of numerical data points taken at successive equally spaced points in time. In investing, a time series tracks the movement of the chosen data points, such as the stock price, over a specified period of time with data points recorded at regular intervals. If you\u2019re still in doubt about what this would exactly look like, take a look at the following example:\n\nYou see that the dates are placed on the x-axis, while the price is featured on the y-axis. The \u201csuccessive equally spaced points in time\u201d in this case means that the days that are featured on the x-axis are 14 days apart: note the difference between 3/7/2005 and the next point, 3/31/2005, and 4/5/2005 and 4/19/2005.\n\nHowever, what you\u2019ll often see when you\u2019re working with stock data is not just two columns, that contain period and price observations, but most of the times, you\u2019ll have five columns that contain observations of the period and the opening, high, low and closing prices of that period. This means that, if your period is set at a daily level, the observations for that day will give you an idea of the opening and closing price for that day and the extreme high and low price movement for a particular stock during that day.\n\nFor now, you have a basic idea of the basic concepts that you need to know to go through this tutorial. These concepts will come back soon enough and you\u2019ll learn more about them later on in this tutorial.\n\nGetting your workspace ready to go is an easy job: you basically just make sure you have Python and an Integrated Development Environment (IDE) running on your system. However, there are some ways in which you can get started that are maybe a little easier when you\u2019re just starting out.\n\nTake for instance Anaconda, a high performance distribution of Python and R and includes over 100 of the most popular Python, R and Scala packages for data science. Additionally, installing Anaconda will give you access to over 720 packages that can easily be installed with conda, our renowned package, dependency and environment manager, that is included in Anaconda. And, besides all that, you\u2019ll get the Jupyter Notebook and Spyder IDE with it.\n\nThat sounds like a good deal, right?\n\nYou can install Anaconda from here and don\u2019t forget to check out how to set up your Jupyter Notebook in DataCamp\u2019s Jupyter Notebook Tutorial: The Definitive Guide.\n\nOf course, Anaconda is not your only option: you can also check out the Canopy Python distribution (which doesn\u2019t come free), or try out the Quant Platform.\n\nThe latter offers you a couple additional advantages over using, for example, Jupyter or the Spyder IDE, since it provides you everything you need specifically to do financial analytics in your browser! With the Quant Platform, you\u2019ll gain access to GUI-based Financial Engineering, interactive and Python-based financial analytics and your own Python-based analytics library. What\u2019s more, you\u2019ll also have access to a forum where you can discuss solutions or questions with peers!\n\nWhen you\u2019re using Python for finance, you\u2019ll often find yourself using the data manipulation package, Pandas. But also other packages such as NumPy, SciPy, Matplotlib,\u2026 will pass by once you start digging deeper.\n\nFor now, let\u2019s just focus on Pandas and using it to analyze time series data. This section will explain how you can import data, explore and manipulate it with Pandas. On top of all of that, you\u2019ll learn how you can perform common financial analyses on the data that you imported.\n\nThe package allows for reading in data from sources such as Google, Yahoo! Finance, World Bank,\u2026 If you want to have an updated list of the data sources that are made available with this function, go to the documentation. For this tutorial, you will use the package to read in data from Yahoo! Finance.\n\nNote that the Yahoo API endpoint has recently changed and that, if you want to already start working with the library on your own, you\u2019ll need to install a temporary fix until the patch has been merged into the master brach to start pulling in data from Yahoo! Finance with . Make sure to read up on the issue here before you start on your own!\n\nNo worries, though, for this tutorial, the data has been loaded in for you so that you don\u2019t face any issues while learning about finance in Python with Pandas.\n\nIt\u2019s wise to consider though that, even though offers a lot of options to pull in data into Python, it isn\u2019t the only package that you can use to pull in financial data: you can also make use of libraries such as Quandl, for example, to get data from Google Finance:\n\nFor more information on how you can use Quandl to get financial data directly into Python, go to this page.\n\nLastly, if you\u2019ve already been working in finance for a while, you\u2019ll probably know that you most often use Excel also to manipulate your data. In such cases, you should know that you can integrate Python with Excel.\n\nCheck out DataCamp\u2019s Python Excel Tutorial: The Definitive Guide for more information.\n\nThe first thing that you want to do when you finally have the data in your workspace is getting your hands dirty. However, now that you\u2019re working with time series data, this might not seem as straightforward, since your index now contains DateTime values.\n\nNo worries, though! Let\u2019s start step-by-step and explore the data first with some functions that you\u2019ll might already know if you have some prior programming experience with R or if you\u2019ve already worked with Pandas.\n\nEither way, you\u2019ll see it\u2019s very easy!\n\nAs you saw in the code chunk above, you have used to import data into your workspace. The resulting object is a DataFrame, which is a 2-dimensional labeled data structure with columns of potentially different types. Now, one of the first things that you probably do when you have a regular DataFrame on your hands, is running the and functions to take a peek at the first and the last rows of your DataFrame. Luckily, this doesn\u2019t change when you\u2019re working with time series data!\n\nTip: also make sure to use the function to get some useful summary statistics about your data.\n\nAs you have seen in the introduction, this data clearly contains the four columns with the opening and closing price per day and the extreme high and low price movements for the Apple stock for each day. Additionally, you also get two extra columns: and .\n\nThe former column is used to register the number of shares that got traded during a single day. The latter, on the other hand, is the adjusted closing price: it\u2019s the closing price of the day that has been slightly adjusted to include any actions that occurred at any time before the next day\u2019s open. You can use this column to examine historical returns or when you\u2019re performing a detailed analysis on historical returns.\n\nNote how the index or row labels contain dates, and how your columns or column labels contain numerical values.\n\nTip: if you now would like to save this data to a csv file with the function from and that you can use the function to read the data back into Python. This is extremely handy in cases where, for example, the Yahoo API endpoint has changed and you don\u2019t have access to your data any longer :)\n\nNow that you have briefly inspected the first lines of your data and have taken a look at some summary statistics, it\u2019s time to go a little bit deeper.\n\nOne way to do this is by inspecting the index and the columns and by selecting, for example, the last ten rows of a certain column. The latter is called subsetting because you take a small subset of your data. The result of the subsetting is a Series, which is a one-dimensional labeled array that is capable of holding any type.\n\nRemember that the DataFrame structure was a two-dimensional labeled array with columns that potentially hold different types of data.\n\nCheck all of this out in the exercise below. First, use the and attributes to take a look at the index and columns of your data. Next, subset the column by only selecting the last 10 observations of the DataFrame. Make use of the square brackets to isolate the last ten values. You might already know this way of subsetting from other programming languages, such as R. To conclude, assign the latter to a variable and then check what type is by using the function. You can find the exercise here.\n\nThe square brackets can be nice to subset your data, but they are maybe not the most idiomatic way to do things with Pandas. That\u2019s why you should also take a look at the and functions: you use the former for label-based indexing and the latter for positional indexing.\n\nIn practice, this means that you can pass the label of the row labels, such as and , to the function, while you pass integers such as and to the function.\n\nComplete the exercise in the original article to understand how both and work.\n\nTip: if you look closely at the results of the subsetting, you\u2019ll notice that there are certain days missing in the data; If you look more closely at the pattern, you\u2019ll see that it\u2019s usually two or three days that are missing; These days are usually weekend days or public holidays and aren\u2019t part of your data. This is nothing to worry about: it\u2019s completely normal and you don\u2019t have to fill in these missing days.\n\nBesides indexing, you might also want to explore some other techniques to get to know your data a little bit better. You never know what else will show up. Let\u2019s try to sample some 20 rows from the data set and then let\u2019s resample the data so that is now at the monthly level instead of daily. You can make use of the and functions to do this.\n\nThe function is often used because it provides elaborate control and more flexibility on the frequency conversion of your times series: besides specifying new time intervals yourself and specifying how you want to handle missing data, you also have the option to indicate how you want to resample your data, as you can see in the code example above. This stands in clear contract to the method, where you only have the first two options.\n\nTip: try this out for yourself in the IPython console of the above DataCamp Light chunk. Pass in to see what happens!\n\nLastly, before you take your data exploration to the next level and start with visualizing your data and performing some common financial analyses on your data, you might already start to calculate the differences between the opening and closing prices per day. You can easily perform this arithmetic operation with the help of Pandas; Just substract the values in the column of your data from the values of the column of that same data. Or, in other words, subtract from . You storethe result in a new column of the DataFrame called and then you delete it again with the help of\n\nTip: make sure to comment out the last line of code so that the new column of your DataFrame doesn\u2019t get removed and you can check the results of your arithmetic operation!\n\nOf course, knowing the gains in absolute terms might already help you to get an idea of whether you\u2019re making a good investment, but as a quant, you might be more interested in a more relative means of measuring your stock\u2019s value, like how much the value of a certain stock has gone up or gone down. A way to do this is by calculating the daily percentage change.\n\nThis is good to know for now, but don\u2019t worry about it just yet; You\u2019ll go deeper into this in a bit!\n\nThis section introduced you to some ways to first explore your data before you start performing some prior analyses. However, you can still go a lot further in this; Consider taking our Python Exploratory Data Analysis if you want to know more.\n\nNext to exploring your data by means of , , indexing, \u2026 You might also want to visualize your time series data. Thanks to Pandas\u2019 plotting integration with Matplotlib, this task becomes easy; Just use the function and pass the relevant arguments to it. Additionally, you can also add the argument to indicate that the plot should also have a grid in the background.\n\nIf you run the code in the original article, you\u2019ll come to the following plot:\n\nIf you want to know more about Matplotlib and how to get started with it, check out DataCamp\u2019s Intermediate Python for Data Science course.\n\nNow that you have an idea of your data, what time series data is about and how you can use to quickly explore your data, it\u2019s time to dive deeper into some of the common financial analyses that you can do so that you can actually start working towards developing a trading strategy.\n\nIn the rest of this section, you\u2019ll learn more about the returns, moving windows, volatility calculation and Ordinary Least-Squares Regression (OLS).\n\nYou can read more and practice these common financial analyses in the original article.\n\nNow that you have done some primary analyses to your data, it\u2019s time to formulate your first trading strategy; But before you go into all of this, why not first get to know some of the most common trading strategies? After a short introduction, you\u2019ll undoubtedly move on more easily your trading strategy.\n\nFrom the introduction, you\u2019ll still remember that a trading strategy is a fixed plan to go long or short in markets, but much more information you didn\u2019t really get yet; In general, there are two common trading strategies: the momentum strategy and the reversion strategy.\n\nFirstly, the momentum strategy is also called divergence or trend trading. When you follow this strategy, you do so because you believe the movement of a quantity will continue in its current direction. Stated differently, you believe that stocks have momentum, or upward or downward trends, that you can detect and exploit.\n\nSome examples of this strategy are the moving average crossover, the dual moving average crossover and turtle trading:\n\nSecondly, the reversion strategy, which is also known as convergence or cycle trading. This strategy departs from the belief that the movement of a quantity will eventually reverse. This might seem a little bit abstract, but will not be so any more when you take the example. Take a look at the mean reversion strategy, where you actually believe that stocks return to their mean and that you can exploit when it deviates from that mean.\n\nThat already sounds a whole lot more practical, right?\n\nAnother example of this strategy, besides the mean reversion strategy, is the pairs trading mean-reversion, which is similar to the mean reversion strategy. Whereas the mean reversion strategy basically stated that stocks return to their mean, the pairs trading strategy extends this and states that if two stocks can be identified that have a relatively high correlation, the change in the difference in price between the two stocks can be used to signal trading events if one of the two moves out of correlation with the other. That means that if the correlation between two stocks has decreased, the stock with the higher price can be considered to be in a short position. It should be sold because the higher-priced stock will return to the mean. The lower-priced stock, on the other hand, will be in a long position becaue the price will rise as the correlation will return to normal.\n\nBesides these two most frequent strategies, there are also other ones that you might come across once in a while, such as the forecasting strategy, which attempts to predict the direction or value of stock, in this case, in subsequent future time periods based on certain historical factors. There\u2019s also the High Frequency Trading (HFT) strategy, which exploits the sub-millisecond market microstructure.\n\nThat\u2019s all music for the future for now; Let\u2019s focus on developing your first trading strategy for now!\n\nAs you read above, you\u2019ll start with the \u201chello world\u201d of quantitative trading: the moving average crossover. The strategy that you\u2019ll be developing is simple: you create two separate Simple Moving Averages (SMA) of a time series with differing lookback periods, let\u2019s say, 40 days and 100 days. If the short moving average exceeds the long moving average then you go long, if the long moving average exceeds the short moving average then you exit.\n\nRemember that when you go long, you think that the stock price will go up and will sell at a higher price in the future (= buy signal); When you go short, you sell your stock, expecting that you can buy it back at a lower price and realize a profit (= sell signal).\n\nThis simple strategy might seem quite complex when you\u2019re just starting out, but let\u2019s take this step by step:\n\nSee the code here.\n\nThis wasn\u2019t too hard, was it? Print out the DataFrame and inspect the results. Important to grasp here is what the and the columns mean in this DataFrame. You\u2019ll see that it will become very important when you move on!\n\nWhen you have taken the time to understand the results of your trading strategy, quickly plot all of this (the short and long moving averages, together with the buy and sell signals) with Matplotlib:\n\nPS. You can find the code for this plot here.\n\nThe result is pretty cool, isn\u2019t it?\n\nNow that you\u2019ve got your trading strategy at hand, it\u2019s a good idea to also backtest it and calculate its performance. But right before you go deeper into this, you might want to know just a little bit more about the pitfalls of backtesting, what components are needed in a backtester and what Python tools you can use to backtest your simple algorithm.\n\nIf, however, you\u2019re already well up to date, you can simply move on to the implementation of your backtester!\n\nBacktesting is, besides just \u201ctesting a trading strategy\u201d, testing the strategy on relevant historical data to make sure that it\u2019s an actual viable strategy before you start making moves. With backtesting, a trader can simulate and analyze the risk and profitability of trading with a specific strategy over a period of time. However, when you\u2019re backtesting, it\u2019s a good idea to keep in mind that there are some pitfalls, which might not be obvious to you when you\u2019re just starting out.\n\nFor example, there are external events, such as market regime shifts, which are regulatory changes or macroeconomic events, which definitely influence your backtesting. Also liquidity constraints, such as the ban of short sales, could affect your backtesting heavily.\n\nNext, there are pitfalls which you might introduce yourself when you, for example, overfit a model (optimization bias), when you ignore strategy rules because you think it\u2019s better like that (interference), or when you accidentally introduce information into past data (lookahead bias).\n\nThese are just a few pitfalls that you need to take into account mainly after this tutorial, when you go and make your own strategies and backtest them.\n\nBesides the pitfalls, it\u2019s good to know that your backtester usually consists of some four essential components, which should usually present in every backtester. As such, a backtester consists of the following:\n\nBesides these four components, there are many more that you can add to your backtester, depending on the complexity. You can definitely go a lot further than just these four components. However, for this beginner tutorial, you\u2019ll just focus on getting these basic components to work in your code.\n\nAs you read above, a simple backtester consists of a strategy, a data handler, a portfolio and an execution handler. You have already implemented a strategy above, and you also have access to a data handler, which is the or the Pandas library that you use to get your saved data from Excel into Python. The components that are still left to implement are the execution handler and the portfolio.\n\nHowever, since you\u2019re just starting out, you\u2019ll not focus on implementing an execution handler just yet. Instead, you\u2019ll see below how you can get started on creating a portfolio which can generate orders and manages the profit and loss:\n\nAs a last exercise for your backtest, visualize the portfolio value or over the years with the help of Matplotlib and the results of your backtest:\n\nNote that, for this tutorial, the Pandas code for the backtester as well as the trading strategy has been composed in such a way that you can easily walk through it in an interactive way. In a real-life application, you might opt for a more object-oriented design with classes, which contain all the logic. You can find an example of the same moving average crossover strategy, with object-oriented design, here or check out this presentation.\n\nYou have seen now how you can implement a backtester with the Python\u2019s popular data manipulation package Pandas. However, you can also see that it\u2019s easy to make mistakes and that this might not be the most fail-safe option to use every time: you need to build most of the components from scratch, even though you already leverage Pandas to get your results.\n\nThat\u2019s why it\u2019s common to use a backtesting platform, such as Quantopian, for your backtesters. Quantopian is a free, community-centered, hosted platform for building and executing trading strategies. It\u2019s powered by , a Python library for algorithmic trading. You can use the library locally, but for the purpose of this beginner tutorial, you\u2019ll use Quantopian to write and backtest your algorithm. Before you can do this, though, make sure that you first sign up and log in.\n\nNext, you can get started pretty easily. Click \u201cNew Algorithm\u201d to start writing up your trading algorithm or select one of the examples that has already been coded up for you to get a better feeling of what you\u2019re exactly dealing with :)\n\nLet\u2019s start simple and make a new algorithm, but still following our simple example of the moving average crossover, which is the standard example that you find in the zipline Quickstart guide. It so happens that this example is very similar to the simple trading strategy that you implemented in the previous section. You see, though, that the structure in the code chunk below and in the screenshot above is somewhat different than what you have seen up until now in this tutorial, namely, you have two definitions that you start working from, namely and\n\nThe first function is called when the program is started and performs one-time startup logic. As an argument, the function takes a , which is used to store the state during a backtest or live trading and can be referenced in different parts of the algorithm, as you can see in the code below; You see that comes back, among others, in the definition of the first moving average window. You see that you assign the result of the lookup of a security (stock in this case) by its symbol, ( in this case) to .\n\nThe function is called once per minute during simulation or live-trading to decide what orders, if any, should be placed each minute. The function requires and as input: the is the same as the one that you read about just now, while the data is an object that stores several API functions, such as to retrieve the most recent value of a given field(s) for a given asset(s) or to get trailing windows of historical pricing or volume data. These API functions don\u2019t come back in the code below and are not in the scope of this tutorial.\n\nNote That the code that you type into the Quantopian console will only work on the platform itself and not in your local Jupyter Notebook, for example!\n\nYou\u2019ll see that the object allows you to retrieve the , which is the forward-filled, returning last known price, if there is one. If there is none, an value will be returned.\n\nAnother object that you see in the code chunk above is the , which stores important information about\u2026. Your portfolio. As you can see in the piece of code , this object is stored in the context and is then also accessible in the core functions that has to offer to you as a user. Note that the that you just read about, store Position objects and include information such as the number of shares and price paid as values. Additionally, you also see that the also has a property to retrieve the current amount of cash in your portfolio and that the object also has an property to explore the whole number of shares in a certain position.\n\nThe places an order to adjust a position to a target number of shares. If there is no existing position in the asset, an order is placed for the full target number. If there is a position in the asset, an order is placed for the difference between the target number of shares or contracts and the number currently held. Placing a negative target order will result in a short position equal to the negative number specified.\n\nTip: if you have any more questions about the functions or objects, make sure to check the Quantopian Help page, which contains more information about all (and much more) that you have briefly seen in this tutorial.\n\nWhen you have created your strategy with the and functions (or copy-pasted the above code) into the console on the left-hand side of your interface, just press the \u201cBuild Algorithm\u201d button to build the code and run a backtest. If you press the \u201cRun Full Backtest\u201d button, a full backtest is run, which is basically the same as the one that you run when you build the algorithm, but you\u2019ll be able to see a lot more in detail. The backtesting, whether \u2018simple\u2019 or full, can take a while; Make sure to keep an eye out on the progress bar on top of the page!\n\nYou can find more information on how to get started with Quantopian here.\n\nNote that Quantopian is an easy way to get started with zipline, but that you can always move on to using the library locally in, for example, your Jupyter notebook.\n\nYou have successfully made a simple trading algorithm and performed backtests via Pandas, Zipline and Quantopian. It\u2019s fair to say that you\u2019ve been introduced to trading with Python. However, when you have coded up the trading strategy and backtested it, your work doesn\u2019t stop yet; You might want to improve your strategy. There are one or more algorithms may be used to improve the model on a continuous basis, such as KMeans, k-Nearest Neighbors (KNN), Classification or Regression Trees and the Genetic Algorithm. This will be the topic of a future DataCamp tutorial.\n\nApart from the other algorithms you can use, you saw that you can improve your strategy by working with multi-symbol portfolios. Just incorporating one company or symbol into your strategy often doesn\u2019t really say much. You\u2019ll also see this coming back in the evaluation of your moving average crossover strategy. Other things that you can add or do differently is using a risk management framework or use event-driven backtesting to help mitigate the lookahead bias that you read about earlier. There are still many other ways in which you could improve your strategy, but for now, this is a good basis to start from!\n\nImproving your strategy doesn\u2019t mean that you\u2019re finished just yet! You can easily use Pandas to calculate some metrics to further judge your simple trading strategy. In this section, you\u2019ll learn about the Sharpe ratio, the maximum drawdown and the Compound Annual Growth Rate (CAGR).\n\nRead and practice more here.\n\nBesides these two metrics, there are also many other that you could consider, such as the distribution of returns, trade-level metrics, \u2026\n\nWell done, you\u2019ve made it through this Python Finance introduction tutorial! You\u2019ve covered a lot of ground, but there\u2019s still so much more for you to discover!\n\nCheck out Yves Hilpisch\u2019s Python For Finance book, which is a great book for those who already have gathered some background into Finance, but not so much in Python. \u201cMastering Pandas for Data Science\u201d by Michael Heydt is also recommended for those who want to get started with Finance in Python! Also make sure to check out Quantstart\u2019s articles for guided tutorials on algorithmic trading and this complete series on Python programming for finance.\n\nIf you\u2019re more interested in continuing your journey into finance with R, consider taking Datacamp\u2019s Quantitative Analyst with R track. And in the meantime, keep posted for our second post on starting finance with Python and check out the Jupyter notebook of this tutorial."
    },
    {
        "url": "https://medium.com/datacamp/jupyter-and-r-markdown-notebooks-with-r-5182e16f9188",
        "title": "Jupyter And R Markdown: Notebooks With R \u2013 DataCamp \u2013",
        "text": "When want to get started working on data science problems, you first might want to consider setting up an interactive environment to work and share your code for a project with others.\n\nOr let\u2019s say you just want to communicate about the workflow and your analysis\u2019 results.\n\nYou\u2019ll already see me coming: notebooks are perfect for both situations. In these two cases, you want to combine plain text with rich text elements such as graphics, calculations, etc.\n\nToday\u2019s blog post will focus on the two notebooks that are popular with R users, namely, the Jupyter Notebook and the R Markdown Notebook. You\u2019ll discover how to use these notebooks, how they compare to one another and what other alternatives exist.\n\nContrary to what you might think, Jupyter doesn\u2019t limit you to working solely with Python: the notebook application is language agnostic, which means that you can also work with other languages.\n\nThere are two general ways to get started on using R with Jupyter: by using a kernel or by setting up an R environment that has all the essential tools to get started on doing data science.\n\nAs described above, the first way to run R is by using a kernel. If you want to have a complete list of all the available kernels in Jupyter, go here.\n\nTo work with R, you\u2019ll need to load the IRKernel and activate it to get started on working with R in the notebook environment.\n\nFirst, you\u2019ll need to install some packages. Make sure that you don\u2019t do this in your RStudio console, but in a regular R terminal, otherwise you\u2019ll get an error like this:\n\nThis command will prompt you to type in a number to select a CRAN mirror to install the necessary packages. Enter a number and the installation will continue.\n\nThen, you still need to make the R kernel visible for Jupyter:\n\nNow open up the notebook application with . You'll see R appearing in the list of kernels when you create a new notebook.\n\nThe second option to quickly work with R is to install the R essentials in your current environment:\n\nThese \u201cessentials\u201d include the packages dplyr, shiny, ggplot2, tidyr, caret, and nnet. If you don\u2019t want to install the essentials in your current environment, you can use the following command to create a new environment just for the R essentials:\n\nNow open up the notebook application to start working with R.\n\nYou might wonder what you need to do if you want to install additional packages to elaborate your data science project. After all, these packages might be enough to get you started, but you might need other tools.\n\nWell, you can either build a Conda R package by running, for example:\n\nOr you can install the package from inside of R via or (to install packages from GitHub). You just have to make sure to add the new package to the correct R library used by Jupyter:\n\nIf you want to know more about kernels or about running R in a Docker environment, check out this page.\n\nA huge advantage of working with notebooks is that they provide you with an interactive environment. That interactivity comes mainly from the so-called \u201cmagic commands\u201d.\n\nThese commands allow you to switch from Python to command line instructions or to write code in another language such as R, Julia, Scala, \u2026\n\nIf you want more details about magic commands, on how to set up a notebook, where to download the application, how you can run the notebook application (via Docker, pip install or with the Anaconda distribution) or other details, check out our Definitive Guide or the original article.\n\nUp until recently, Jupyter seems to have been a popular solution for R users, next to notebooks such as Apache Zeppelin or Beaker.\n\nAlso, other alternatives to report results of data analyses, such as R Markdown, Knitr or Sweave, have been hugely popular in the R community.\n\nHowever, this might change with the recent release of the R or R Markdown Notebook by RStudio.\n\nYou see it: the context of the R Markdown Notebook is complex, and it\u2019s worth looking into the history of reproducible research in R to understand what drove the creation and development of this notebook. Ultimately, you will also realize that this notebook is different from others.\n\nIf you want to know more about the history of R Notebooks, check out the original article.\n\nR Markdown is probably one of the most popular options in the R community to report on data analyses. It\u2019s no surprise whatsoever that it is still a core component in the R Markdown Notebook.\n\nAnd there are some things that R Markdown and notebooks share, such as the delivering of a reproducible workflow, the weaving of code, output, and text together in a single document, supporting interactive widgets and outputting to multiple formats. However, they differ in their emphases: R Markdown focuses on reproducible batch execution, plain text representation, version control, production output and offers the same editor and tools that you use for R scripts.\n\nOn the other hand, the traditional computational notebooks focus on outputting inline with code, caching the output across sessions, sharing code and outputting in a single file. Notebooks have an emphasis on an interactive execution model. They don\u2019t use a plain text representation, but a structured data representation, such as JSON.\n\nThat all explains the purpose of RStudio\u2019s notebook application: it combines all the advantages of R Markdown with the good things that computational notebooks have to offer.\n\nThat\u2019s why R Markdown is a core component of the R Markdown Notebook: RStudio defines its notebook as \u201can R Markdown document with chunks that can be executed independently and interactively, with output visible immediately beneath the input\u201d.\n\nIf you\u2019ve ever worked with Jupyter or any other computational notebook, you\u2019ll see that the workflow is very similar. One thing that might seem very different is the fact that now you\u2019re not working with code cells anymore by default: you\u2019re rather working with a sort of text editor in which you indicate your code chunks with R Markdown.\n\nThe first requirement to use the notebook is that you have the newest version of RStudio available on your PC. Since notebooks are a new feature of RStudio, they are only available in version 1.0 or higher of RStudio. So, it\u2019s important to check if you have a correct version installed.\n\nIf you don\u2019t have version 1.0 or higher of RStudio, you can download the latest version here.\n\nThen, to make a new notebook, you go to File tab, select\u201dNew File\u201d, and you\u2019ll see the option to create a new R Markdown Notebook. If RStudio prompts you to update some packages, just accept the offer and eventually a new file will appear.\n\nTip: double-check whether you\u2019re working with a notebook by looking at the top of your document. The output should be html_notebook.\n\nYou\u2019ll see that the default text that appears in the document is in R Markdown. R Markdown should feel pretty familiar to you, but if you\u2019re not yet quite proficient, you can always check out our Reporting With R Markdown course or go through the material that is provided by RStudio.\n\nNote that you can always use the gear icon to adjust the notebook\u2019s working space: you have the option to expand, collapse, and remove the output of your code, to change the preview options and to modify the output options.\n\nThis last option can come in handy if you want to change the syntax highlighting, apply another theme, adjust the default width and height of the figures appearing in your output, etc.\n\nFrom there onwards, you can start inserting code chunks and text!\n\nYou can add code chunks in two ways: through the keyboard shortcut Ctrl + Alt + I or Cmd + Option + I, or with the insert button that you find in the toolbar.\n\nWhat\u2019s great about working with these R Markdown notebooks is the fact that you can follow up on the execution of your code chunks, thanks to the little green bar that appears on the left when you\u2019re executing large code chunks or multiple code chunks at once. Also, note that there\u2019s a progress bar on the bottom.\n\nYou can see the green progress bar appearing in the gif below:\n\nTalking about code execution: there are multiple ways in which you can execute your R code chunks.\n\nYou can run a code chunk or run the next chunk, run all code chunks below and above; but you can also choose to restart R and run all chunks or to restart and to clear the output.\n\nNote that when you execute the notebook\u2019s code, you will also see the output appearing on your console! That might be a rather big difference for those who usually work with other computational notebooks such as Jupyter.\n\nIf there are any errors while the notebook\u2019s code chunks are being executed, the execution will stop, and there will appear a red bar alongside the code piece that produces the error.\n\nYou can suppress the halt of the execution by adding in the chunk options, just like this:\n\nNote that the error will still appear, but that the notebook\u2019s code execution won\u2019t be halted!\n\nJust like with Jupyter, you can also work interactively with your R Markdown notebooks. It works a bit differently from Jupyter, as there are no real magic commands; To work with other languages, you need to add separate Bash, Stan, Python, SQL or Rcpp chunks to the notebook.\n\nThese options might seem quite limited to you, but it\u2019s compensated in the ease with which you can easily add these types of code chunks with the toolbar\u2019s insert button.\n\nAlso working with these code chunks is easy: you can see an example of SQL chunks in this document, published by J.J Allaire. For Bash commands, you just type the command. There\u2019s no need extra characters such as \u2018 \u2019 to signal that you're working in Bash, like you would do when you would work with Jupyter.\n\nBefore you render the final version of a notebook, you might want to preview what you have been doing. There\u2019s a handy feature that allows you to do this: you\u2019ll find it in your toolbar.\n\nClick on the \u201cpreview\u201d button and the provisional version of your document will pop up on the right-hand side, in the \u201cViewer\u201d tab.\n\nBy adding some lines to the first section on top of the notebook, you can adjust your output options, like this:\n\nTo see where you can get those distributions, you can just try to knit, and the console output will give you the sites where you can download the necessary packages.\n\nNote that this is just one of the many options that you have to export a notebook: there\u2019s also the possibility to render GitHub documents, word documents, beamer presentation, etc. These are the output options that you already had with regular R Markdown files. You can find more info here.\n\nBesides the general coding practices that you should keep in mind, such as documenting your code and applying a consistent naming scheme, code grouping and name length, you can also use the following tips to make a notebook awesome for others to use and read.\n\nBesides the differences between the Jupyter and R Markdown notebooks that you have already read above, there are some more things.\n\nThere are four aspects that you will find interesting to consider: notebook sharing, code execution, version control, and project management.\n\nThe source code for an R Markdown notebook is an file. But when you save a notebook, an file is created alongside it. This HTML file is an associated file that includes a copy of the R Markdown source code and the generated output.\n\nThat means that you need no special viewer to see the file, while you might need it to view notebooks that were made with the Jupyter application, which are simple JSON documents, or other computational notebooks that have structured format outputs. You can publish your R Markdown notebook on any web server, GitHub or as an email attachment.\n\nThere also are APIs to render and parse R Markdown notebooks: this gives other frontend tools the ability to create notebook authoring modes for R Markdown. Or the APIs can be used to create conversion utilities to and from different notebook formats.\n\nTo share the notebooks you make in the Jupyter application, you can export the notebooks as slideshows, blogs, dashboards, etc. You can find more information in this tutorial. However, there are also the default options to generate Python scripts, HTML files, Markdown files, PDF files or reStructured Text files.\n\nR Markdown Notebooks have options to run a code chunk or run the next chunk, run all code chunks below and above; In addition to these options, you can also choose to restart R and run all chunks or to restart and to clear the output.\n\nThese options are interesting when you\u2019re working with R because the R Markdown Notebook allows all R code pieces to share the same environment. However, this can prove to be a huge disadvantage if you\u2019re working with non-R code pieces, as these don\u2019t share environments.\n\nAll in all, these code execution options add a considerable amount of flexibility for the users who have been struggling with the code execution options that Jupyter offers, even though if these are not too much different: in the Jupyter application, you have the option to run a single cell, to run cells and to run all cells. You can also choose to clear the current or all outputs. The code environment is shared between code cells.\n\nIf you want to know more about version control and project management options in Jupyter and R Notebook, go to the original article.\n\nApart from the notebooks that you can use as interactive data science environments which make it easy for you to share your code with colleagues, peers, and friends, there are also other alternatives to consider.\n\nBecause sometimes you don\u2019t need a notebook, but a dashboard, an interactive learning platform or a book, for example.\n\nYou have already read about options such as Sweave and Knitr in the second section. Some other options that are out there, are:"
    },
    {
        "url": "https://medium.com/datacamp/jupyter-notebook-tutorial-the-definitive-guide-660c7e651ecd",
        "title": "Jupyter Notebook Tutorial: The Definitive Guide \u2013 DataCamp \u2013",
        "text": "In this case, \u201cnotebook\u201d or \u201cnotebook documents\u201d denote documents that contain both code and rich text elements, such as figures, links, equations, \u2026 Because of the mix of code and text elements, these documents are the ideal place to bring together an analysis description and its results as well as they can be executed perform the data analysis in real time.\n\nThese documents are produced by the Jupyter Notebook App.\n\nWe\u2019ll talk about this in a bit.\n\nFor now, you should just know that \u201cJupyter\u201d is a loose acronym meaning Julia, Python, and R. These programming languages were the first target languages of the Jupyter application, but nowadays, the notebook technology also supports many other languages.\n\nAnd there you have it: the Jupyter Notebook.\n\nAs you just saw, the main components of the whole environment are, on the one hand, the notebooks themselves and the application. On the other hand, you also have a notebook kernel and a notebook dashboard.\n\nLet\u2019s look at these components in more detail.\n\nAs a server-client application, the Jupyter Notebook App allows you to edit and run your notebooks via a web browser. The application can be executed on a PC without Internet access or it can be installed on a remote server, where you can access it through the Internet.\n\nIts two main components are the kernels and a dashboard.\n\nA kernel is a program that runs and introspects the user\u2019s code. The Jupyter Notebook App has a kernel for Python code, but there are also kernels available for other programming languages.\n\nThe dashboard of the application not only shows you the notebook documents that you have made and can reopen but can also be used to manage the kernels: you can which ones are running and shut them down if necessary.\n\nTo fully understand what the Jupyter Notebook is and what functionality it has to offer you need to know how it originated.\n\nLet\u2019s back up briefly to the late 1980s. Guido Van Rossum begins to work on Python at the National Research Institute for Mathematics and Computer Science in the Netherlands.\n\nWait, maybe that\u2019s too far.\n\nLet\u2019s go to late 2001, twenty years later. Fernando P\u00e9rez starts developing IPython.\n\nIn 2005, both Robert Kern and Fernando P\u00e9rez attempted building a notebook system. Unfortunately, the prototype had never become fully usable.\n\nFast forward two years: the IPython team had kept on working, and in 2007, they formulated another attempt at implementing a notebook-type system. By October 2010, there was a prototype of a web notebook and in the summer of 2011, this prototype was incorporated and it was released with 0.12 on December 21, 2011. In subsequent years, the team got awards, such as the Advancement of Free Software for Fernando P\u00e9rez on 23 of March 2013 and the Jolt Productivity Award, and funding from the Alfred P. Sloan Foundations, among others.\n\nLastly, in 2014, Project Jupyter started as a spin-off project from IPython. IPython is now the name of the Python backend, which is also known as the kernel. Recently, the next generation of Jupyter Notebooks has been introduced to the community. It\u2019s called JupyterLab. Read more about it here.\n\nAfter all this, you might wonder where this idea of notebooks originated or how it came about to the creators. Go here to find out more.\n\nOne of the requirements here is Python, either Python 3.3 or greater or Python 2.7. The general recommendation is that you use the Anaconda distribution to install both Python and the notebook application.\n\nThe advantage of Anaconda is that you have access to over 720 packages that can easily be installed with Anaconda\u2019s conda, a package, dependency, and environment manager. You can download and follow the instructions for the installation of Anaconda here.\n\nIs something not clear? You can always read up on the Jupyter installation instructions here.\n\nIf you don\u2019t want to install Anaconda, you just have to make sure that you have the latest version of pip. If you have installed Python, you will normally already have it.\n\nWhat you do need to do is upgrading pip and once you have pip, you can get started on installing Jupyter.\n\nGo to the original article for the commands to install Jupyter via pip.\n\nDocker is an excellent platform to run software in containers. These containers are self-contained and isolated processes.\n\nThis sounds a bit like a virtual machine, right?\n\nNot really. Go here to read an explanation on why they are different, complete with a fantastic house metaphor.\n\nYou can easily get started with Docker: turn to the original article to get started with Jupyter on Docker.\n\nNow that you know what you\u2019ll be working with and you have installed it, it\u2019s time to get started for real!\n\nRun the following command to open up the application:\n\nThen you\u2019ll see the application opening in the web browser on the following address: http://localhost:8888.\n\nFor a complete overview of all the components of the Jupyter Notebook, complete with gifs, go to the original article.\n\nIf you want to start on your notebook, go back to the main menu and click the \u201cPython 3\u201d option in the \u201cNotebook\u201d category.\n\nYou will immediately see the notebook name, a menu bar, a toolbar and an empty code cell.\n\nYou can immediately start with importing the necessary libraries for your code. This is one of the best practices that we will discuss in more detail later on.\n\nAfter, you can add, remove or edit the cells according to your needs. And don\u2019t forget to insert explanatory text or titles and subtitles to clarify your code! That\u2019s what makes a notebook a notebook in the end.\n\nFor more tips, go here.\n\nAre you not sure what a whole notebook looks like? Hop over to the last section to discover the best ones out there!\n\nUp until now, working with notebooks has been quite straightforward.\n\nBut what if you don\u2019t just want to use Python 3 or 2? What if you want to change between the two?\n\nLuckily, the kernels can solve this problem for you! You can easily create a new conda environment to use different notebook kernels.\n\nThen you restart the application and the two kernels should be available to you. Very important: don\u2019t forget to (de)activate the kernel you (don\u2019t) need. Go to the original article to see how this works and how you can manually register your kernels.\n\nAs the explanation of the kernels in the first section already suggested, you can also run other languages besides Python in your notebook!\n\nIf you want to use R with Jupyter Notebooks but without running it inside a Docker container, you can run the following command to install the R essentials in your current environment. These \u201cessentials\u201d include the packages , , , , and . If you don't want to install the essentials in your current environment, you can use the following command to create a new environment just for the R essentials.\n\nNext, open up the notebook application to start working with R with the usual command.\n\nIf you want to know about the commands to execute or extra tips to run R successfully in your Jupyter Notebook, go here.\n\nIf you now want to install additional R packages to elaborate your data science project, you can either build a Conda R package or you can install the package from inside of R via or (from GitHub). You just have to make sure to add new package to the correct R library used by Jupyter.\n\nNote that you can also install the IRKernel, a kernel for R, to work with R in your notebook. You can follow the installation instructions here.\n\nNote that you also have kernels to run languages such as Julia, SAS, \u2026 in your notebook. Go here for a complete list of the kernels that are available. This list also contains links to the respective pages that have installation instructions to get you started.\n\nIf you want to get the most out of this, you should consider learning about the so-called \u201cmagic commands\u201d. Also, consider adding even more interactivity to your notebook so that it becomes an interactive dashboard to others should be one of your considerations!\n\nThere are some predefined \u2018magic functions\u2019 that will make your work a lot more interactive.\n\nTo see which magic commands you have available in your interpreter, you can simply run the following:\n\nAnd you\u2019ll see a whole bunch of them appearing. You\u2019ll probably see some magics commands that you\u2019ll grasp, such as , or , but others will be less straightforward.\n\nIf you\u2019re looking for more information on the magics commands or on functions, you can always use the\n\nNote that there is a difference between using and . To know more about this and other useful magic commands that you can use, go here.\n\nYou can also use magics to mix languages in your notebook without setting up extra kernels: there is to run R code, SQL for RDBMS or Relational Database Management System access and for interactive work with ,... But there is so much more here!\n\nThe magic commands already do a lot to make your workflow with notebooks agreeable, but you can also take additional steps to make your notebook an interactive place for others by adding widgets to it!\n\nThis example was taken from a wonderful tutorial on building interactive dashboards in Jupyter, which you can find on this page.\n\nIn practice, you might want to share your notebooks with colleagues or friends to show them what you have been up to or as a data science portfolio for future employers. However, the notebook documents are JSON documents that contain text, source code, rich media output, and metadata. Each segment of the document is stored in a cell.\n\nIdeally, you don\u2019t want to go around and share JSON files.\n\nThat\u2019s why you want to find and use other ways to share your notebook documents with others.\n\nWhen you create a notebook, you will see a button in the menu bar that says \u201cFile\u201d. When you click this, you see that Jupyter gives you the option to download your notebook as an HTML, PDF, Markdown or reStructuredText, or a Python script or a Notebook file.\n\nYou can use the command to convert your notebook document file to another static format, such as HTML, PDF, LaTex, Markdown, reStructuredText, ... But don't forget to import first if you don't have it yet!\n\nThen, you can give in something like the following command to convert your notebooks:\n\nWith , you can make sure that you can calculate an entire notebook non-interactively, saving it in place or to a variety of other formats. The fact that you can do this makes notebooks a powerful tool for ETL and for reporting. For reporting, you just make sure to schedule a run of the notebook every so many days, weeks or months; For an ETL pipeline, you can make use of the magic commands in your notebook in combination with some type of scheduling.\n\nBesides these options, you could also consider the following options.\n\nThis all is very interesting when you\u2019re working alone on a data science project. But most times, you\u2019re not alone. You might have some friends look at your code or you\u2019ll need your colleagues to contribute to your notebook.\n\nHow should you actually use these notebooks in practice when you\u2019re working in a team?\n\nThe following tips will help you to effectively and efficiently use notebooks on your data science project.\n\nUsing these notebooks doesn\u2019t mean that you don\u2019t need to follow the coding practices that you would usually apply.\n\nYou probably already know the drill, but these principles include the following:\n\nIn addition to these general best practices for programming, you could also consider the following tips to make your notebooks the best source for other users to learn:\n\nJonathan Whitmore wrote in his article some practices for using notebooks for data science and specifically addresses the fact that working with the notebook on data science problems in a team can prove to be quite a challenge.\n\nThat is why Jonathan suggests some best practices:\n\nThis section is meant to give you a short list with some of the best notebooks that are out there so that you can get started on learning from these examples.\n\nYou will find that many people regularly compose and have composed lists with interesting notebooks. Don\u2019t miss this gallery of interesting IPython notebooks or this KD Nuggets article."
    }
]