[
    {
        "url": "https://medium.com/the-artificial-impostor/pytorch-0-4-0-release-1-0-preview-86ca50441b0b?source=user_profile---------1----------------",
        "title": "PyTorch 0.4.0 Release & 1.0 Preview \u2013 The Artificial Impostor \u2013",
        "text": "This is an experimental series in which I briefly introduce the interesting data science stuffs I read, watched, or listened to during the week. Please give this post some claps if you\u2019d like this series to be continued.\n\nI\u2019ve been busy with other stuffs this week, so this issue will only cover the new Pytorch 0.4.0 and the roadmap to the production ready 1.0 version.\n\nA perhaps incomplete list of important changes with a brief summary for each one of them:\n\nThe code samples at the end of the migration guide are a good way to check if you\u2019ve understood the above changes correctly.\n\nSimilarly, a maybe incomplete list of new features:\n\nProbably one of the most important takeaways:\n\nBasically Facebook is merging Caffe2 and PyTorch to provide both a framework that works for both research and production settings, as hinted earlier in April:\n\nSo the gist of the solution is adding a just-in-time (JIT) compiler to export your model to run on a Caffe2-based C++-only runtime. This compiler has two modes:\n\nThe naming is still subject to change. The 1.0 version is expected to be released this summer."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/tpu-listing-embeddings-pyception-1b3b014c1012?source=user_profile---------2----------------",
        "title": "TPU, Listing Embeddings, Pyception \u2013 The Artificial Impostor \u2013",
        "text": "This is an experimental series in which I briefly introduce the interesting data science stuffs I read, watched, or listened to during the week. Please give this post some claps if you\u2019d like this series to be continued.\n\nIt\u2019s been a while since Google made TPU available on their cloud platform in beta in February. I\u2019m curious if there are already some people sharing their experience using TPU on the Internet. So I did some Googling\u2026\n\nThe article above then led me to\u2026\n\nSo it seems the released TPU (TPUv2) is a bit more cost effective then . However, the fact that TPU supports only mixed precision training may become an issue sometimes.\n\nThe downside is that there are a lot of hoops to jump through to be able to use TPU, according to the Paperspace blog post. And some of them are quite intimidating. Also, only Tensorflow supports TPU so far.\n\nThis new post by RiseML showed that TPU might be even more cost-effective than we thought, and the top-1 accuracy (on the validation set) is bit better coming from TPU than from GPU.\n\nThis post by Airbnb describes how they embeds every listing on their platform to improve similar listing recommendations and later real-time search personalization. Its is well-written and easy to read. The model evaluation parts are particularly interesting. The methodology should be applicable to other similarity problems, too.\n\n(I\u2019m not affiliated with the channel.) I\u2019ve found the links posted in this channel relevant and informative:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/weekly-reading-list-1-9d436aa94b6d?source=user_profile---------3----------------",
        "title": "Chryons, Racist Machines, \u201cGender Gap\u201d, Getting Values from ML",
        "text": "This is an experimental series in which I briefly introduce the interesting data science stuffs I read, watched, or listened to during the week. Please give this post some claps if you\u2019d like this series to be continued.\n\nThis visualization uses data from the Third Eye Project (which captures chyrons via OCR). Chyrons between August 25, 2017, and January 21, 2018 were processed and transformed answer three questions:\n\nIt\u2019s really impressive. I especially appreciate how they have written the \u201cData and methodology\u201d section.\n\nSlides from a talk given by Renee M. P. Teate, creator of Becoming a Data Scientist blog and podcast, give an overview of the bias that could be hidden inside machine learning algorithms.\n\nThe presenter then discussed typical types of machine learning models (regression, classification, and clustering) and how things can become problematic. I find the \u201cCrime Forecasting Using Spatio-Temporal Pattern with Ensemble Learning\u201d example very intriguing for its subtleness.\n\nBasically, predictive model development involves a lot of decision making, and all these decision are made by human, thus the potential bias injection. The presenter gave quite a few examples of this kind of flawed process, including:\n\nSo a machine CAN indeed be racist or sexist. What can we do about it? I recommend reading the slides for answers (page 86\u201390). They are definitely worth your time.\n\nAn excellent book on this topic \u2014 Weapons of Math Destruction:\n\n\uff08The three key ingredients of weapons of math destruction: 1.Opacity 2. Scale 3.Damage.\uff09\n\n(For some reason I wasn\u2019t able to embed the link) A great piece from the Washington Post on the huge gaps of male and female populations in India and China.Not exactly a data science article, but I just love its interactive charts. Here are some sneak peeks:\n\nThis article argues that the main problem why machine learning haven\u2019t made as much impact in the business world as we\u2019d anticipated is not the model under-performing, but the difficulty of deployment.\n\nIt describes an \u201cAI project manager\u201d they built to predict red flags of ongoing projects. And they found the biggest requirements for this product were:\n\nFinally they propose a new machine learning paradigm, with key steps described in this paper, and supported by open-source tools."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/visualizing-air-quality-data-2ec16268711e?source=user_profile---------4----------------",
        "title": "Visualizing Air Quality Data \u2013 The Artificial Impostor \u2013",
        "text": "I wrote a post on visualizing PM 2.5 concentration before. With KDD Cup 2018(KDD Cup of Fresh Air) coming up, I figure it is a good time to write an update version.\n\nThis will be a miscellaneous collection of visualization, external resources, and random thoughts(I wouldn\u2019t call them analysis), based on the KDD Cup dataset. Although the competition rule did not seem to explicitly forbid public sharing of its dataset, I\u2019d prefer to play it safe and ask you to download the dataset from its website if you want to run the code locally.\n\nThe dataset includes hourly observations from 35 stations in Beijing and 13 stations in London (and also some stations in London that are not included in the prediction set) spanning from Jan. 2017 to Mar. 2018. New data from Apr. is available via an official API.\n\nTake stations in Beijing as an example:\n\nI added some jitter using , so we can see the density of missing points more clearly. There are quite some missing data points across the board, with some station missing out an entire segment(like zhiwuyuan station in the last row).\n\nIt can be helpful to also include valid data points in the plot:\n\nThis is to make sure all data points are accounted for. Through an earlier version of this plot I was able to find that some segments like early Jul. 2017 and May. 2017 are missing for all stations. Therefore we have to put the timestamps in ourselves (this is important especially if you want to do sequence modeling later):\n\nThe missing data can make forecasting difficult because you don\u2019t have a reliable source of history. Ways to alleviate this problem includes cross-referencing data points from different stations, but it won\u2019t help when the entire segment is missing, in which case yearly correlation can not be used either.\n\nThis package dygraphs for R is extremely easy to use and create beautiful interactive charts. A huge chunk of the above code is dividing the chart into 6 regions according to the US EPA AQI standard. I was not able to follow the color designation exactly because the background shading must be light enough so the foreground is visible.\n\nThe center line is the mean concentration of the day, the shading covers 5 to 95 percentile of the day. The maximum 95 percentile is clipped to 500 as there are some outliers (way more than 500 ug/m\u00b3).\n\nI probably should have explicitly stated the unit of concentration in the chart.\n\nThis part is largely inspired by this repo (TheMoods/AirChina) to use the Python package folium to visualize geographical information.\n\nI\u2019m sure we can combine several snapshots and make an animation in leaflet.js (folium is a Python wrapper of leaflet), but I couldn\u2019t find a easy way to do it in folium. This brings us to the next section.\n\nThe background map is not essential in the animation (you only need to see it once), so why not just ditch it? Based on this idea, I used gganimate to visualize the dispersion of PM 2.5 particles:\n\nThe mechanism of gganimate is really simple. It creates one PNG file per frame, and combine them together to make a GIF file or an MP4 file using ffmpeg (the commented out part).\n\nWe can see some regional patterns going on in the video. So taking information from nearby stations can be helpful when forecasting.\n\nBe aware that the low frame rates might make some video player not able to play the resulting MP4 files properly. One of the solution is to tell ffmepg to artificially increase the frame rate, but I find this solution too slow and the larger size of the output unacceptable. Just find a video player that supports lower frame rates (obviously Youtube supports them at least).\n\nThe above is a very good overview on how to forecast PM 2.5 concentration, along with an awesome visualization. It tells us that one of the most important factors is the wind, whose forecast is part of the bigger weather forecasting problem. So this competition might actually boil down to an weather forecasting problem.\n\nIn The Signal and The Noise, Nate Silver dedicated an entire chapter to weather forecasting, referring it as one of the rare success stories of forecasting. Indeed, the forecasting and visualization of weather is widely available and recognized. And the forecasting of pollutants based on weather is also well-studied.\n\nAFAIK, most of these forecasts are using the large-scale simulation (ensemble forecasting) technique. It seems to me the main challenge of this KDD Cup is to match that technique with much less data (we don\u2019t have pollutant readings for nearby areas) and much less computing resources (no supercomputer cheese). We are allowed to use public external data, though. So you can grab the forecasts of wind speeds and directions, and based you forecast on them. But the model will be hard to validate because we don\u2019t have the forecast data in the past. All in all, it seems to be a very challenging problem to be tackled.\n\nPlease refer to the following Github repo for code used in this post:\n\nAnd here are some rendered pages:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/tensorflow-fashion-mnist-with-dataset-api-cce1e3cc8cd4?source=user_profile---------5----------------",
        "title": "[Tensorflow] Fashion-MNIST with Dataset API \u2013 The Artificial Impostor \u2013",
        "text": "Fashion-MNIST intends to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It addresses the problem of MNIST being too easy for modern neural networks, along with some other issues.\n\nWe\u2019re going to continue using the models from Part 2(GRU) and Part 3(TCN), but replace MNIST with Fashion-MNIST using the Dataset API.\n\nPreviously we were looping through the MNIST data batches via class method provided by and feed the data to the graph via parameter in . By switching to the Dataset API, we get:\n\nThe two fundamental abstractions of the Dataset API are:\n\nThey are roughly analogous to and in PyTorch. Though batching, shuffling, parallelism configuration are done in DataLoader in PyTorch instead of in Dataset.\n\nThese are the main resources I used when researching for this post:\n\nInterestingly, in \u201cHow to use Dataset in Tensorflow\u201d, the author did not cover Feedable Iterator because he did not think it is useful. However, I found it is quite useful in our situation where we need to evaluate the validation set once every few hundred steps. So this post could be used to fill in the missing part of that post.\n\nNow comes the real deal. As always, the code is hosted on Google Colab:\n\nWe use the CSV files from Kaggle Dataset. To download it to the Google Colab environment, I used to download from a Google Cloud Storage bucket I created (you have to create your own to run). If you don\u2019t have Google Cloud access, I suggest uploading from your local filesystem.\n\nBecause this is a small dataset, we can safely read everything into memory:\n\nAnd choose 10,000 images randomly as the validation set (note that the test set also has 10,000 images):\n\nFirst of all, we group all dataset-related definition into one scope:\n\nSo they are displayed nicely as one block in Tensorboard:\n\nNext we use to define configurable batch sizes (or you can use fixed batch sizes as in the comment):\n\nThen we directly create the dataset from the Pandas data frames (the back-end Numpy arrays, to be precise):\n\nAgain, this is because this dataset is very small. For medium-size datasets, you might want to use to create datasets with Numpy arrays. For bigger datasets, you\u2019ll have to use or .\n\nThe training set is shuffled randomly at each iteration/step. Set the buffer size to be larger or equal to the size of the dataset to make sure it is completely shuffled.\n\nWe use to make the dataset repeat indefinitely. We\u2019ll control how many iterations/steps we need outside of the graph. To make it iterate only for N epochs, use and use to detect the depletion of data.\n\nWe use function to transform the imported tensors. The first transformation to reshape the feature from (batch_size, length) to (batch_size, length, 1). The second transformation performs (one-hot encoding) on the target labels. Note that both transformation were Tensorflow functions (starts with ), as recommended by the official documentation. If you want to do transformation that depends on third-party libraries (e.g. OpenCV), you need to use to wrap the call.\n\nIt is pretty much the same as in the documentation:\n\nTo use this iterator in a session, we need to initialize the base iterators first. This also set the batch sizes for each iterator:\n\nThen tell Tensorflow which iterator you want to use when training or testing:\n\nThis final step connect the dataset to the rest of the graph:\n\nAnd we\u2019re done! The model is ready to be trained.\n\nHere\u2019s a trick to track a metric in both training and validating stages in one plot \u2014 Creating two instances that write to two different sub-folders:\n\nAnd use the same metric name for both writers:\n\n(I keep an exponential moving average of the training loss in the graph. It is a leftover from my experiments with Estimator API. Spoiler: I don\u2019t like that API.) The latter part shows you how to add values outside of the graph to the Tensorboard.\n\nThen you\u2019ll have both curve in one plot:\n\nWe can also compare curves from different runs. For example, we can see that permuted sequential Fashion-MNIST is harder from the following plot:\n\nSample results(accuracies) of the CudnnGRU models taken from the notebook:\n\nSample results(accuracies) of the TCN models taken from the notebook:\n\nGenerally TCN still performs better than GRU. But bear in mind that these models are not really tuned, so there might be some rooms for improvement. As suggested by the submitted benchmarks in the project README, adding dropouts to the GRU is likely to help with the accuracy. You can also explore more benchmarks with scikit-learn models here:\n\nThank you very much for reading. This is the last part of this series and the end of my Tensorflow crash course. There are still some missing pieces of the puzzle, e.g. higher level training APIs other than Keras. I played with Estimator and Experiment APIs a bit and found them really restricting. I\u2019d rather write my own training process. For more layer abstractions and data manipulation helpers, TensorLayer seems to be a good Tensorflow medium-level library that is quite popular. I recommend you to quickly browse through their official examples to see if it fits your needs."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7?source=user_profile---------6----------------",
        "title": "[Tensorflow] Implementing Temporal Convolutional Networks",
        "text": "In this post, we\u2019ll learn how to write models with customized building blocks by implementing TCNs using tf.layers APIs.\n\nThe authors released the source code in PyTorch, which is well-written and easy to incorporate into your own projects. You can skip all the Tensorflow parts below and use their implementation instead if you just want to use TCNs with PyTorch.\n\nThe term \u201c Temporal Convolutional Networks \u201d (TCNs) is a vague term that could represent a wide range of network architectures. In this post it is pointed specifically to one family of architectures proposed in the paper An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling :\n\nThe most important component of TCNs is dilated causal convolution. \u201cCausal\u201d simply means a filter at time step t can only see inputs that are no later than t. Dilated convolution is well explained in this blog post. The point of using dilated convolution is to achieve larger receptive field with fewer parameters and fewer layers. (I also mentioned dilated causal convolution in the writeup of the Instacart competition).\n\nA residual block stacks two dilated causal convolution layers together, and the results from the final convolution are added back to the inputs to obtain the outputs of the block. If the width(number of channels) of the inputs and the width(number of filters) of the second dilated causal convolution layers differs, we\u2019ll have to apply an 1D convolution to the inputs before the adding the convolution outputs to match the widths.\n\nWhat TCNs do is simply stacking a number of residual blocks together to get the receptive field that we desire. If the receptive field is larger or equal to the maximum length of any sequences, the results of a TCN will be semantically equivalent to the results of a RNN.\n\nIt\u2019s important to know how to calculate the receptive field because you\u2019ll need it to determine how many layers of residual blocks you need in the model.\n\nHere we denote the number of previous time steps(history) the ith a dilated causal convolution layer can see as .\n\nFor layer 0 (a imagined convolution as the base case), F(0) = 1, as a causal convolution can always see its current time steps it\u2019s at.\n\nFor layer 1, F(1) = F(0) + 2 * [kernel_size(n)-1] * dilation(n). It can see what the previous layer can see plus the position of the last kernel minus the position of the first. We can verify this using Figure 1 \u2014 F(1) = 1 + (3\u20131) * 1 = 3.\n\nYou should be able to see the pattern now. Generally, F(n) = F(n-1) + [kernel_size(n)-1] * dilation(n), where n means we\u2019re at the nth dilated causal convolution layer since the input layer. Since every residual block has two identical dilated causal convolutions (same kernel sizes and dilations), we could simplifies the formula to F\u2019(n) = F\u2019(n-1) + 2 * [kernel_size(n)-1] * dilation(n), but n now means we are at the nth residual block.\n\nIf the kernel size is fixed, and the dilation of each residual block increases exponentially by 2, i.e. dilation(n) = 2^(n-1), we can expand the formula as F\u2019(n) = 1 + 2 * (kernel_size-1) * (1 + 2 + 2\u00b2 + \u2026 + 2^(n-1)) = . Verify using Figure 1c \u2014 1+2*(3\u20131)*(2\u00b9-1)=5. You could verify the result with more residual blocks yourself.\n\nSo there it is, with a fixed kernel size and exponentially increasing dilations, TCN with n residual blocks will have a receptive field of \n\n at the final block. It most likely won\u2019t match your maximum sequence length exactly, so you\u2019ll have to decide to add one more block to make it larger than the maximum length, or sacrifice some of the older history.\n\nAs before, the notebook with the source code use in the post is uploaded to Google Colab:\n\nWe\u2019re going to use the module to provide high-level abstraction for the implemented TCNs. The base layer class is the foundation of all other layers in the module. The official documentation recommends descendants to this class implements the following three methods:\n\nWhen in doubt, try to read the source code of a built-in layer and imitate what it does in those methods.\n\nIt\u2019s quite simple to implement this since already supports dilation through the parameter. What we need to do is to pad the start of the sequence with (kernel_size-1) * dilation zeros ourselves, and pass (basically means no padding) to the parent . The padding will make the first output element only able to see the first input element (and the padding zeros).\n\nBecause of the restriction from other layers, only support channels_last data format, i.e. input shape is always (batch_size, length, channels). It use to pad the input tensor. Most of the lines are just capturing the initialization parameters of .\n\nBesides dilated causal convolution, we still need weight normalization, dropout, and the optional 1x1 Conv to complete the residual block.\n\nI did not find an easy way to implement weight normalization in Tensorflow, so I replaced it with (layer normalization). They won\u2019t be the same, but should have similar effects in stabilizing training. The layer normalization implementation basically assumes the channels are located at the last dimension of the input tensor, so the whole stack needs to use channels_last data format.\n\nIn the dropout section, we randomly drop out some of the channels across all time steps (a.k.a spatial dropout). layer has a parameter that does exactly that. By setting the to (batch_size, 1, channels), we select some channels for each example and set the dropout mask. Then the mask is broadcast to all time steps. (Check the notebook for a simple example.)\n\nIn the following implementation is set to (1, 1, channels) to allow dynamic batch sizes. This will slow down convergence. If you want dynamic batch sizes with different masks for each example, you\u2019ll have to override the method to generate dynamically.\n\nFinally, the 1x1 convolution can easily be achieved with a layer (it creates a projection at the last dimension):\n\nThe naming of the class follows the PyTorch implementation. Two dropout layers was created instead of one (same applies to layer normalization) simply to make Tensorboard create a cleaner graph visualization:\n\nAll that is left to do is to stack residual blocks together and create dilations exponentially:\n\nNote we can name each block manually with the parameter, which will be shown in the Tensorboard:\n\nI haven\u2019t figured out how to properly write unit tests against Tensorflow layers, but it should be a hard requirement if you want to use this implementation on real-world datasets.\n\nSome differences comparing to the previous RNN models:\n\nWe set kernel size to be 8 and number of stacked blocks to be 6, so the receptive field will be 1 + 2 * (8\u20131) * (2\u2076-1) = 883, a bit larger than the maximum sequence length 784.\n\nYou can see in the notebook that a TCN with ~ 36K parameters converged faster and had better test accuracy than RNN from the previous notebook.\n\nWe\u2019ve been using the test set in the training process to pick the final model, which is a very bad practice. It makes the results from the two notebooks so far somewhat unreliable. In the next and probably the final part of this series, we\u2019ll learn how to import the Fashion-MNIST dataset and create a proper validation set to evaluate our models."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5?source=user_profile---------7----------------",
        "title": "[Tensorflow] Building RNN Models to Solve Sequential MNIST",
        "text": "In this post, we\u2019re going to lay some groundwork for the custom model which will be covered in the next post by familiarizing ourselves with using RNN models in Tensorflow to deal with the sequential MNIST problem. The basic framework of the code used in this post is based on the following two notebooks:\n\nI\u2019ve put the source code for this post in a notebook hosted on Google Colaboratory, which kindly provides a free GPU runtime for the public to use \uff08I kept getting disconnected to the runtime when running the notebook. So some of the model training was not completed. You can copy the notebook and run it yourself.\uff09:\n\nThe notebook should have done most of the talking. The following sections of this post will discuss some parts of the notebook in more detail, and also provide some additional information that was left out in the notebook.\n\nEvery example from the MNIST dataset is a 28x28 image. We are going to apply recurrent neural network on it in two ways:\n\nThe pixel-by-pixel case is a lot harder because a decent model has to keep a very long-term memory.\n\nWe\u2019re going to build four models (two models for each case):\n\nWe\u2019re jumping directly to the second model, which is different from the first model in the following ways:\n\nI\u2019m going to discuss some of them in the following sections.\n\nThis Tensorflow LSTM benchmark is very comprehensive:\n\nTensorflow has a nice wrapper that does variational dropout for you:\n\nThat\u2019s probably the main reason why you sometimes want to use LSTMBlockCell instead of CudnnLSTM. For sequential MNIST the problem of overfitting is relatively low, so we did not use any dropouts in the notebook.\n\nI feel the difference between and is somewhat vague in the documentation. These two discussion threads (stackoverflow and github) cleared things up a bit for me. The main difference seems to be that supports dynamic maximum sequence length in batch level, while doesn\u2019t. From what I\u2019ve read, there seems to be little reason not to always use .\n\nYou simply supply the whole batch of input data as a tensor to instead of slicing them into a list of tensor (sequences). This is easier to write and read than :\n\nIn the first model, you have to define the weight and the bias for the linear (output) layer manually:\n\nAnd calculate the output logits by doing a matrix multiplication and an addition:\n\nAlbeit very good for educational purpose, you probably don\u2019t want to do it every time you need a linear layer. The abstraction provided by provides similar experience to layer in PyTorch:\n\nYou can also use the shortcut function like I just did with :\n\nRMSProp speeds up the convergence, and gradient clipping helps dealing with the exploding gradient problem of RNNs.\n\nThe row-by-row only involves 28 time steps, and is fairly easy to solve with a wide range of hyper-parameters (initialization methods, number of hidden units, learning rate, etc.). The pixel-by-pixel MNIST with 784 time steps is a lot harder to crack. Unfortunately I could not find a set of hyper-parameters for a LSTM model that could guarantee converge. Instead, I\u2019ve found GRU models much easier to tune and succeed to reach 90%+ test accuracy in multiple cases.\n\nPyTorch uses CuDNN implementations of RNNs by default, and that\u2019s what makes it faster. We could also utilize those implementations in Tensorflow via :\n\nRNN classes from the module doesn\u2019t have a parameter, so the input shape is always (length, batch_size, channels). Moreover, if you want to get the most speed, let run through the whole sequence in a single command (as the code above did) instead of feeding it step-by-step. It seems to work similarly to , meaning the maximum length is allow to differ between batches.\n\nGrouping variables and operations using tf.variable_scope brought us this modularized graph in Tensorboard:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-1-5f0ebb253ad4?source=user_profile---------8----------------",
        "title": "[Tensorflow] Core Concepts and Common Confusions \u2013 The Artificial Impostor \u2013",
        "text": "This post will be written from my personal perspective, so the readers should already have some basic idea about what deep learning is, and preferably are familiar with PyTorch.\n\nI\u2019ve been considering picking up Tensorflow for a while, and have finally decided to do it. The main appeal of learning Tensorflow includes (compared to using PyTorch exclusively. Keras as a high-level library is not really comparable.):\n\nTensorflow started with only the kernel and low-level APIs, but has accumulated a lot of modules and become a behemoth. Official documentation recommends using Estimators and Datasets, but I personally chose to start from Layers APIs and low-level APIs to have the kind of access similar to ones in PyTorch, and work my way up to Estimators and Datasets. There will be more posts coming up on these topics.\n\nI find the complexity of its stack one of Tensorflow\u2019s main turn-offs. The vast of amount of modules to choose from can also overwhelms the beginners. It\u2019ll become a lot easier when you finally find a set of modules you can work with most comfortably.\n\nThis is the one of the most important concept for those who come from PyTorch. Pytorch create a dynamic computational graph on the fly to do automatic differentiation. Tensorflow, on the other hand, requires you to define the graph first. I\n\nFor Tensorflow, it\u2019s like building a systems of pipes first(a graph), pumping water into it and receiving the processed water in the other end (session.run). Pytorch allows you to pump water into the system while you are building it, so it\u2019s easier to find any sub-units that malfunctioned (debugging).\n\nThere are some upsides to static graphs, despite the obvious downsides. Because it is static, Tensorflow can infer some parameters like input sizes for you when compiling the graph. The trained model will also be more portable to other platforms.\n\nUsually you only need one graph. Tensorflow implicitly defines a default graph for you, but I prefer to explicitly define it and group all graph definition in a context:\n\nThis method is basically the whole point of creating a session. It starts the data flow in the graph (pumps water into a piping system). The two most important parameters are fetches(outputs) and feeds(inputs).\n\nBy passing a list of nodes (can be operations, tensors, or variables) as fetches, you tell Session.run you want the data to flow to the given nodes. Session.run will close off all the subgraphs that are not required to reach those nodes, hence saves execution time.\n\nBy passing a map from values to tensors as a dictionary of feeds, you tell Session.run to fill the tensors with the given values when running the graph. This is how you input information to the graph. (You can also use variables instead of tensors in feeds, although it\u2019s not very common.)\n\nIn the following example from the official tutorial, y is passed as the sole element of fetches, and values to placeholder x is passed in a dictionary:\n\nIn PyTorch, a variable is part of the automatic differentiation module and a wrapper around a tensor. It represents a node in the computation graph, and stores its parent in the graph and optionally its gradients. So basically all tensors in the graph are variables.\n\nA variable in Tensorflow is also a wrapper around a tensor, but has a different meaning. A variable contains a tensor that is persistent and changeable across different Session.runs. So they are usually the ones that are updated in back-propagations (e.g. the weights of a model), and also any other states we want to keep between different runs. Also, all variables need to be initialized (usually through an operation) before they can be used.\n\nVariables does not persist after a session being closed. You have to remember saving those variables before closing the session. (The official documentation mentioned modifications to variables are visible across multiple sessions, but that seems only apply to concurrent session running on multiple workers.)\n\nTo save variables/weights in Tensorflow usually involves serializing all variable into a file. While PyTorch relies on the state_dict method to extract Parameters (a subclass of Variable) and persistent buffers to be serialized.\n\nSaving models in Tensorflow involves defining a Saver in the graph definition and invoking the save method in a session:\n\nThere\u2019s also a SavedModel class that not only saves variables, but also the graph and the metadata of the graph for you. It is useful when you want to export your model.\n\nPyTorch names the parameters in a quite Pythonic way:\n\nTensorflow, however, uses namespaces to organize tensors/variables and operations. Tensorboard group operations according namespaces they belong to, and generate a nice visual representation of the graph for you:\n\nAn example of tensor name is . is the actual name of the tensor. The suffix is the endpoint used to give the tensors returned from an operation unique identifiers, i.e. is the first tensor, is the second and so on.\n\nSome medium or high level APIs like tf.layers will handles some of the scope naming for you. But I\u2019ve found it not smart enough sometime, so I had to do it manually. And it brings me to the question \u2014 which one to use, or ?\n\nThis stackoverflow answer gave a brilliant explanation to the differences between these two, as illustrated in the following graph:\n\nTurns out there is only one difference \u2014 affects , while doesn\u2019t. also has a parameter , which allow you to reuse the same variable (with the same name in the same namespace) in different part of the code without having to pass a reference to that variable around.\n\nHere\u2019s an example of mixing and from the official documentation:\n\nIMO, usually you\u2019d want to use unless there is a need to put operations and variables in different levels of namespaces.\n\nThat\u2019s pretty much it! In this post we have covered 4 topics that I\u2019ve found most confusing to beginners:\n\nNow we can go on and write some actual Tensorflow code. In the next two or three posts I\u2019ll share some of the exercises I set for myself, using low-to-medium level APIs. After that maybe I\u2019ll do a piece on how to integrate higher level APIs like Estimators and Datasets, so we can comfortably move between different levels of APIs to suit our requirements."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/review-kaggle-toxic-comment-classification-challenge-part-1-934447339309?source=user_profile---------9----------------",
        "title": "[Review] Kaggle Toxic Comment Classification Challenge \u2014 Part 1",
        "text": "Public kernel blends performed well in this competition (i.e. did not over-fit the public leaderboard too much). I expected it to overfit, but still selected one final submission that used the best public blend of blends to play it safe. Fortunately it paid off and gave me a 0.0001 boost in AUC on private leaderboard:\n\nI tried a few ideas after building up my PyTorch pipeline but did not find any innovative approach that looks promising. Text normalization is the only strategy I had found to give solid improvements, but it is very time consuming. The final result (105th place~3%) was quite fitting IMO given the time I spent on this competition(not a lot).\n\nToxic comment classification challenge features a multi-label text classification problem with a highly imbalanced dataset. The test set used originally was revealed to be already public on the Internet, so a new dataset was released mid-competition, and the evaluation metric was c hanged from Log Loss to AUC .\n\nIn this post, I\u2019ll review some of the techniques used and shared by top competitors. I do not have enough time to test every one of them myself. Part 2 of this series will be implementing a top 5 solution by my own, if I ever find time to do it.\n\nI tired to attribute techniques to all appropriate sources, but I\u2019m sure I\u2019ve missed some sources here and there. Not all techniques are covered because of the vast amount of contents shared by generous Kagglers. I might come back and edit this list in the near future.\n\nI\u2019ve already tried these two techniques and trained a couple of models for each.\n\nHead-tail truncating (keeping 250 tokens at head, 50 tokens at tail) helped only a bit for bi-GRU, but not for QRNN. It basically had no effect on my final ensemble.\n\nFor pseudo-labelling(PL), I used the test-set predictions from my best ensemble as suggested in [1], and they improved the final ensemble a little (see table 1). I\u2019d assume that adding more model trained with PL will further boost the final AUC. However, the problem of this approach is the leakage it produces. The ensemble model had seen the the all the validation data, and that information leaked into its test set predictions. So the local CV will be distorted and not comparable to those trained without PL. Nonetheless, this technique does create the best single model, so it\u2019ll be quite useful for production deployment.\n\nI think the more conservative way of doing PL is to repeat the train-predict-train(with PL) process, so the model is trained twice for every fold. But that\u2019ll definitely takes more time."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/march-madness-predictions-using-pymc3-e64574497f47?source=user_profile---------10----------------",
        "title": "March Madness Predictions using PyMC3 \u2013 The Artificial Impostor \u2013",
        "text": "This post describes my journey from exploring the model from Predicting March Madness Winners with Bayesian Statistics in PYMC3! by Barnes Analytics to developing a much simpler linear model.\n\nTaken from the official documentation:\n\nPyMC3 is Python-native, so I personally find it easier to use Stan. It is based on Theano, whose development has unfortunately stopped. The work to replace Theano seems to be ongoing. There have also been some new competitors closing in, e.g. Edward(based on Tensorflow) and Pyro(based on PyTorch).\n\nAs mentioned in the beginning of the post, this model is heavily based on the post by Barnes Analytics. The model seems to originate from the work of Baio and Blangiardo (in predicting footbal/soccer results), and implemented by Daniel Weitzenfeld. There is also an example in the official PyMC3 documentation that uses the same model to predict Rugby results.\n\nThe model decompose everything that influences the results of a game into five factors:\n\nAnd it use a Possion regression (belongs to the generalized linear regression family) to model the relationship between these factors. It can be easily formalized by the following formulae from Daniel Weitzenfeld\u2019s post:\n\nI\u2019m not going to spend time explaining how to do data preprocessing here. Please check the Barnes Analytics post or this Kaggle Kernel. I\u2019ve made some changes to the Barnes Analytics code. Besides some small code tweaks, there are two bigger changes:\n\nOriginally the model can only fit data from only one season (in the following case, the 2017 season):\n\n(The operations starts with \u201ctt.\u201d are Theano tensor operations. ) Note the model implements a sum-to-zero constraint by subtracting the global mean. Otherwise we can shift all offensive power by z , all defensive power by -z and still get the same model.\n\nThe simplest way to fit more than one season in one run is to assign separate offensive and defensive powers to each team for each season, and assume there are no correlations between seasons (You can also simply loop through each season and fit one model per season under this assumption.):\n\nIf we want to model the correlations between seasons, one way to do it is to base the score/power of one team in this season on the last season. For example, we can make a team\u2019s score in season 2 the sum of its score in season 1 and a white noise. There\u2019s a GaussianRandomWalk class in PyMC3 that does exactly this. However, I somehow could not make GaussianRandomWalk to work with multiple series (teams). So instead I manually defined scores season by season and linked them together. The results from this model are not better than assuming independence between seasons, so I\u2019ll skip the code for this one.\n\nThe code inside the loop takes a set of samples from the posterior distributions, calculates the corresponding thetas, sample from the resulting Poisson distributions, and see if the home team has a higher score than the away team. The code is repeated sample_size times and the winning ratio of the home team is our predicted winning probability.\n\nThe code can be vectorized to utilize the optimized code of Numpy and Theano:\n\nThe vectorized function also returns the 5, 50, and 95 percentiles of the simulated home_score and away_score for later model diagnostics.\n\nThe speedup (tested on a single season model, but should make no difference anyway):\n\nIn fact, if we take the percentile calculation away, the vectorized function can achieve a 1000x speedup (~2.6ms) instead of a 100x speedup.\n\nTake this Kaggle kernel/notebook as an example. The model was fitted to the results of regular season matches:\n\nNow we use the tournaments in March to test our predictions:\n\n(Correction: it\u2019s really a 90% confidence interval instead of 95% in the screenshot) Only around 85% of the time the actual score fell into the 90% confidence interval, so the model is far from ideal.\n\nAs for accuracy, overall accuracy is 67.54% (the percentage where the model predicted the winner correctly), and for 2017 season the accuracy is 67.16%. The overall and 2017 logloss is 0.5388 and 0.5654, respectively.\n\nOne of my favorite things to do is to take a problem and try to find the simplest model that can solve the problem to a reasonable degree. For this prediction problem, I want to assign only one score per team, use the difference of the scores between two teams to predict the difference of their final points in a game. So there are down to only three factors influencing the results of a match:\n\nThe relationship between the difference of the final points and the factors is entirely linear, i.e. . I know, it seems to incredibly over-simplify things, but does it?\n\n(Although the differences of the final score are discrete, using a Gaussian distribution to approximate it should not be a big problem) Extremely simple code. There are two random variables set as the standard deviations, the (latent) variables for (season, team), and one random variable for the home court advantage.\n\nTake this Kaggle kernel/notebook as an example:\n\nBecause this is a linear model, the results are very interpretable. Home team get an advantage of around 3.4 points in average, which is not a lot, but already requires two field goals to counter. The standard deviation of team scores is around 8.6, and the one of point differences is around 10.35. They are both quite large, which means the uncertainty of the predictions are high.\n\nAs for accuracy, overall accuracy is 69.78% (the percentage where the model predicted the winner correctly), and for 2017 season the accuracy is 70.15%. The overall and 2017 logloss is 0.5007 and 0.5349, respectively.\n\nSurprisingly, the second model performs better than the first model. In additional to the 2%+ improvement in accuracy, the better logloss values also indicate a better quality in probability predictions.\n\nAdmittedly, 70% accuracy is still nothing to be proud of, but we can build more sophisticated models based on it by adding more features or data. For example, we can add an adjustment term to account for the fact that some teams play better against better teams, while only mediocre against equal or worse teams. It\u2019s also common sense that basket ball cannot be truly linear. Even if team A beats team B by 10 points and team B beats team C by 10 points, team A beating team C by 20 points is definitely not a sure thing (It\u2019d probably often be less than that). How to better model that dynamics is for you to find out.\n\nThe first model (offensive and defensive decomposition):\n\nThe second model (single score per team):\n\nThis visualization tool written by Mark McClure is really good. The quality of your prediction is shown in colors, and hovering over a match will show the probability you predicted:\n\nYou have to follow Kaggle\u2019s submission format, though. Check it here and here."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/note-talent-vs-luck-the-role-of-randomness-in-success-and-failure-edc97896f0c2?source=user_profile---------11----------------",
        "title": "Talent vs Luck: the role of randomness in success and failure",
        "text": "There are some discussions on the Internet about this paper which captured my attention recently:\n\nIt turns out there are already at least two write-up by well-known media outlets:\n\nThere are also two Hacker News threads on this topic: [1], [2].\n\nBasically the authors, by running simulations based on a model, argued in the paper that luck may play a bigger role in an individual\u2019s success than we usually think. The key observation is that while talents are normally distributed (which arguably is a big assumption), the wealth distribution in the real world typically follows a power law. The authors claimed that their model successfully explains this gap between the two distributions, and then provided some directions to \u201cimprove meritocracy\u201d based on some simulation results.\n\nI\u2019ll admit that this kind of arguments is pretty much what I\u2019d like to hear. They usually lead to solutions like \u201cLet\u2019s tax the rich more to give the unlucky ones more chances to success, since the rich probably don\u2019t deserve most of their wealth anyway\u201d, etc., etc. But honestly, as a layman, I\u2019m not very convinced by this paper. Evidences provided by this paper to support the model seems flimsy to me. The only connection between the real-world and the model is that the results from simulation follows the 80:20 rule (and also roughly a power law). Even if we ignore the lack of evidence, a model that fits the data well does not necessarily generalize well. In my opinion, the paper need to try harder on justifying the model.\n\nThe paper chose to leave out some model details, but similar results is not very hard to reproduce. I tried to simplify the model by removing the square world, and the results are still roughly in line with the ones in the paper, as briefly explained below.\n\nThe model proposed in the paper randomly put people and \u201cevents\u201d in a square world. These \u201cevents\u201d move randomly at each time step. When an event intercepts the position of a person, the capital of the person can be doubled or halved depending on the type of the event and the person\u2019s talent.\n\nThe paper did not specify the coordinate system, and how a event would act if if it moves beyond the boundary of the square world. So the actual probabilities at any time step of an event intercepting a person is unknown.\n\n20180316 Update: For those who are interested in trying the original model in a square world, the paper stated they used NetLogo to conduct the agent-based experiments.\n\nI removed the square world and simply assigned a global probability of event happening at any time step. This gives an absolute level playing field for every person, and it\u2019s not so in a square world. Once the people and events are placed at time zero in a square world, there are gonna be some regions where the number of \u201clucky\u201d events is higher than the \u201cunlucky\u201d one, and people in this region of the world will tend to be a bit luckier than the others (because of the nature of random walks). It helps model some inequality of opportunities.\n\nWe\u2019ll run the simulation in R. First set up the parameters:\n\nLet\u2019s see the result of a single run:\n\nIn this run, the top 20% rich hold 80.19711% of the wealth. The 80:20 rule is not automatically true. I had to tune (probability of an event occurring) to match it. Turns out quite robustly produces results that follows this rule. That means a person will in average encounter 6 events in his working life.\n\n10.3% of people did better than when they started (having more than 10 units of capital at the end). For those with talent > 0.7, 13.89% did better. So talent still plays a role, but not much.\n\nIn this simplified model, the talented people have a larger chance of becoming insanely rich than in the square world. The simulation run shown above is one of the more unfortunate ones (for the talented). The reason is because in the simplified model, every people get the exactly same probability of good event, so there are fewer chances where an ordinarily talented get incredibly lucky.\n\nAs you can see from the plot, the capital distribution definitely not emulate the real-world wealth distribution every well. This is because of the artificial \u201cdouble/half\u201d capital fluctuation rule. If we further complicate the model to make it match the real-world data better, whether the underlying dynamic will be the same still remains to be seen.\n\n20180403 Update: This post did a very impressive job reviewing the paper and playing with more models based on the original one. I recommend anyone who find my post interesting to read it:"
    },
    {
        "url": "https://medium.com/@ceshine/project-wikiquote-navigator-4a7a27f3e9aa?source=user_profile---------12----------------",
        "title": "[Project] Wikiquote Navigator \u2013 CeShine Lee \u2013",
        "text": "Recently I\u2019ve decided to put more energy into some pet projects of mine. They are relatively low-effort and low-risk, but useful to my daily life. And I hope the small sense of achievement from them will alleviate my (suspected) chronic depression.\n\nWikiquote Navigator (or Smarter Wikiquote) is one of them. Famous quotes can be inspiring, as they are wisdoms of prominent members of the human race; and they can sometimes be comforting, as they shows the best of us can also be vulnerable or even stupid at times. But it\u2019s hard to find the quote that is most suitable to your mood or ideas you want to explore. \u201cRandom quote\u201d features are often too random, and tag-based solution usually involves too many quotes to browse through. This project aims to be a librarian to a quotation database(Wikiquote) who can guide you in your search.\n\nCurrently only one feature is implemented \u2014 input a piece of text and it will find you the top 5 quotes that are most similar to the text.\n\nI chose to implement a Telegram bot first. Telegram bots provide a cross-platform interactive user interface, and requires very little amount of programming to get started. As an example, here\u2019s one of my bots that automatically notify me the latest air quality readings:\n\nObviously you\u2019ll need a Telegram account to be able to use it, and that will limit your potential user base. But let\u2019s not forget Telegram already has more than 100 million monthly active users. Anyway, since this project is in a very early stage, I\u2019m not too concerned.\n\nA customized python-wikiquote Python package is used to download quotes from all \u201cPeople\u201d pages. All \u201cdisputed\u201d and \u201cmis-attributed\u201d quotes are removed. However, currently some pages are mishandled so there are some dirty data. Many of the dirty entries can be removed by restricting the minimum length of a quote.\n\nIt\u2019s currently on a very low-end VPS, so it probably won\u2019t be able to handle more than 2 queries per second\u2026\n\nTweets from @realDonaldTrump have always been a great source of fun:\n\nA quote from Trump himself came up second! Let\u2019s see what it is:\n\nYou can also type in some actual quotes from the database:\n\nThe bot successfully identifies the quote as one of Albert Einstein\u2019s.\n\nThe model I used (which will be discussed later) can identified around 90% of the quote correctly as the first returned item, but ones from Einstein are particularly hard:\n\nGenerally the model works surprisingly well! The returned quotes are usually in the same theme as the query. It can capture the sentiment (bitter, joyful, etc.) and sometimes even the sarcasm.\n\nThe back-end system is divided into two parts:\n\nThe Doc2Vec implementation from gensim is used. For an overview of the Doc2Vec algorithm, please check:\n\nThere are two variants of the algorithm \u2014 distributed memory and distributed bag of words:\n\nThe distributed bag of words version (PV-DBOW) doesn\u2019t require training of word vectors and thus less memory. So I chose to use it exclusively. From my experiment it is roughly on par with the distributed memory version for this dataset.\n\nDoc2Vect, like Word2Vect, is an unsupervised algorithm, which makes evaluation tricky. As suggested by the gensim tutorial, I counted how many times a quote can be correctly returned as the top 1 similar result by the model. Higher count implies better understanding of the underlying topics (could also means over-fitting). Inspection of selected examples can be useful, too.\n\nHierarchical softmax (hs=1 in gensim) is used instead of negative sampling (hs=0 and negative={5\u201320} in gensim). The \u201cexact matching\u201d evaluation shows better scores for hierarchical softmax, but negative sampling could yield some more interesting results. More exploration is needed on this front. For details about hierarchical softmax and negative sampling, see:\n\nThis post probably has been too lengthy already for a simple project at a very early stage, so I\u2019ll end here. Thank you for reading. If you\u2019re interested to see more features or analysis, please consider subscribe to the following email list:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/analyzing-tweets-with-r-92ff2ef990c6?source=user_profile---------13----------------",
        "title": "Analyzing Tweets with R \u2013 The Artificial Impostor \u2013",
        "text": "NLP(Natural-language processing) is hard, partly because human is hard to understand. We need good tools to help us analyze texts. Even if the texts are eventually fed into a black box model, doing exploratory analysis is very likely to help you get a better model.\n\nI\u2019ve heard great things about a R package tidytext and recently decided to give it a try. The package authors also wrote a book about it and kindly released it online:\n\nAs the name suggests, tidytext aims to provide text processing capability in the tidyverse ecosystem. I was more familiar with data.table and its way of data manipulation, and the way tidyverse handles data had always seemed tedious to me. But after working through the book, I\u2019ve found the syntax of tidyverse very elegant and intuitive. I love it! All you need is some good examples to help you learn the ropes.\n\nChapter 7 of the book provides a case study comparing tweet archives of the two authors. Since twitter only allows downloading the user\u2019s own archive, it is hard for a reader without friends (i.e. me) to follow. So I found a way to download tweets of public figures and I\u2019d like to share with you how to do it. This post also presents an example comparing tweets from Donald Trump and Barack Obama. The work flow is exactly the same as in the book.\n\nWarning: The content of this post may seem very elementary to professionals.\n\nFirst of all, follow the instruction of this article to obtain your own API key and access token, and install package:\n\nYou need these four variables:\n\nThe main access point for this post is . It downloads at most 3200 recent tweets of a public twitter user. The default parameter seems to remove a lot of false positives, so we\u2019ll instead do it manually later.\n\nNow we have tweets from @realDonaldTrump, @BarackObama and @POTUS44 as List objects. We\u2019ll now convert them to data frames:\n\nNow we plot the time distribution of the tweets:\n\nYou could remove to have compare the absolute amount of tweets instead of relative:\n\n(The lack of activity of @realDonaldTrump is from the 3200-tweet restriction) We can see that as a president, Donald Trump tweets a lot more than Barack Obama did.\n\nFrom this point we\u2019ll enter the world of tidyverse:\n\n(It\u2019s worth noting that the pattern used in are for matching separators, not tokens.) Now we have the data in one-token-per-row tidy text format. We can use it to do some counting:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/review-kaggle-corporaci%C3%B3n-favorita-grocery-sales-forecasting-part-ii-680cca7f9bc5?source=user_profile---------14----------------",
        "title": "[Review] Kaggle Corporaci\u00f3n Favorita Grocery Sales Forecasting \u2014 Part II",
        "text": "The following table contains the results which were obtained from the \u201cLate Submission\u201d feature of Kaggle. The numbers in the model columns are the ensemble weights and the actual number of trained models involved (in the parenthesis). The \u201ccomp\u201d under in v12 LGB column means the predictions from the v12 LGB models (3-model average) were used only for (store, item) combinations with no sales in recent 56 days. The local CV scores are not comparable between v13 and v14, and does not include the scores for (store, item) combination covered by v12 LGB models.\n\nThe local CV scores of v13 models seems to be consistent with leaderboard scores, so I\u2019d say my validation is good enough. (The CV scores of v14 are only for reference. They should not be relied upon.)\n\nThe final row represents the ensemble I\u2019d mostly likely to have picked in the competition, to hedge the risk of v14 blind training being a huge mistake. Its private score of .513 means I probably wouldn\u2019t get top 3 results even if I was given more time and didn\u2019t place that bad bet. So there\u2019s that.\n\nThe training time of my models became too long for v13 and v14 because of the 56-day filter, so I only trained 2 models with different seeds for each hyper-parameter setting . I actually included dozens of models and around 10 types of model variations in the final submission. We might be able to get even lower score if we use stronger bagging and include more variations (some of them I\u2019ll describe in the following sections), but I am not interested in using more computing resources to find out.\n\nMany of the the features and model structures were inspired by:\n\nThere are three types of features in my models:\n\nInteger features 4 to 14 are converted to vectors by entity embedding.\n\nThe alignment of year 1 and year 2 is tricky. To predict the sales at time t, we want the sales at time t-1, all other year 2 features at time t, and year 1 features at time t-364. So every year 2 sales features are shifted 1 day to the left. It\u2019s important to remember that when calculating correlation coefficients and shift the year 2 sales series back.\n\nThe derived features are calculated on the fly in a Dataset method. The other two types of features are written to numpy memmap files on the disk and will be read by the Dataset instance when needed (this saves a tremendous amount of memory).\n\nNot all features are always used. Sometimes some features are dropped when training a model to increase overall variations. A dropout of 0.25 is also applied along the embedding dimension.\n\nThe float series are normalized by subtracting their (series-wise) means, and then divided by their corresponding constant numbers, which are the standard deviations of all the residuals (after subtracting means) for that type of series, e.g. year 2 (store, item) sales, in the training data. The normalized values are clipped by (-3, 3) to reduce the influence of outliers.\n\nFor example, this is the distribution of the log-transformed float series 1:\n\nAfter normalization, it becomes:\n\nFor model structure 3 and 4, LSTM, GRU, SRU[3], QRNN[4] are all available. I\u2019ve found SRU and QRNN can obtain quite good validation loss with less training time, so they are heavily used when doing feature selection. The training time reduction is not as much as the papers reported, though. It might has something to do with my code not optimized enough.\n\nThe model 3(a) with scheduled sampling actually has the best local CV scores, but it takes very long to train, and decay schedule is very hard to tune. Hence I did not include it in the post-competition models.\n\nAs mentioned in Part I, I use ReduceLROnPlateau to schedule learning rate. For model structure 1 and 4(CNN) Adam optimizer is used. RMSProp optimizer is used for the rest.\n\nI tried Yellowfin after reading its paper[7] and thought it was really promising. However its behavior was really strange. I had to do a lot of hand-tuning to make it almost on par with RMSProp, so it was eventually dropped. Recently I tried the official PTB example with both its PyTorch and Tensorflow implementation, and found that the Yellowfin optimizer still underperformed compared to Adam. Not sure what the problem was (I used Python 2.7, Tensorflow 1.1 and PyTorch 0.2.0 as specified in the READMEs.).\n\nI found out that the same seed does not generated the same PyTorch model mid-competition, and spent quite some time trying to find out why. I became pretty sure that the non-deterministic behavior came from customized weight calculation for perishables. (I explicitly create a weight vector and multiply it with the loss vector before doing back-propagation, and keep the weight vector and the product vector to track the learning curve.) Tensorflow community has this discussion about the non-deterministic mean and sum reduction. I think PyTorch should have the similar problem. It really make sense because parallel summation needs to ensure the exact same workload split and reduce order to guarantee the same result (the problem of float point precision).\n\nI used the almost the same model structure on other dataset without non-uniform sample weight and the trained PyTorch models were perfectly reproducible.\n\nThere are some really bizarre series that are almost impossible to predict, for example:\n\nThe reason of the abrupt might be items out of supply or being removed from the shelf. We can only guess.\n\nI have not started preparing the code that is ready to be published yet\u2026.\n\nThere are some experimental code blocks that need to be removed. The model ensemble script currently involves hand-picking models, which should be automated instead. There are some works to be done.\n\nI\u2019ll update this post with a link to the Github repo, or write a Part III for that. We\u2019ll see then.\n\nUpdate on 2017-02-11: The incomplete solution on Github:"
    },
    {
        "url": "https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496?source=user_profile---------15----------------",
        "title": "Use torchtext to Load NLP Datasets \u2014 Part II \u2013",
        "text": "In Part I we\u2019ve discussed how to load text dataset from csv files, tokenize the texts, and put them into tensors via torchtext. Now we\u2019re going to address two issues in that solution (still using the Toxic Comment dataset):\n\nFirst of all, is unfortunately not directly serializable. We start the search of alternatives from the observation that in the method, read the files into a list of Examples:\n\nThe next observation is that , the superclass of , accepts a parameter (a list of Examples). So it becomes clear now what we need is to serialize examples from instances and create instances upon request. The bonus point is to serialize the Field instance as well.\n\nTo be more specific, the following is a general workflow:\n\nThe first two returned variables are the essential components for rebuilding the datasets. You can refit a Field instance if you want, but it\u2019ll be faster if you don\u2019t. Simply insert as one of the fields when initializing a dataset.\n\nSince now we create dataset instances from a list of Examples instead of CSV files, life is much easier. We can split the list of Examples in whatever ways we want and create dataset instances for each split. For classification tasks, I\u2019d usually prefer stratified K-Fold validation. But because the Toxic Comment dataset is multi-label, it\u2019s harder to do stratification. We\u2019ll use simple K-Fold validation in following sections.\n\nPlease refer to the end of the post for the complete code. Here are some comparisons between the new solution and the previous one in Part I:\n\nThe following is an example of a script training 5 models under a 5-Fold validation scheme:\n\nIn case you\u2019ve forgot, here\u2019s an example of features and target extraction:\n\nIt depends on the power of your CPU and the read speed of your disk. In my computer takes 6+ minutes the first time it was called, and around 1 minute after that."
    },
    {
        "url": "https://towardsdatascience.com/review-kaggle-corporaci%C3%B3n-favorita-grocery-sales-forecasting-part-i-9330b7350713?source=user_profile---------16----------------",
        "title": "[Review] Kaggle Corporaci\u00f3n Favorita Grocery Sales Forecasting \u2014 Part I",
        "text": "The challenge of the competition is to predict the unit sales for each item in each store for each day in the period from 2017/08/16 to 2017/08/31. There are 54 stores located at 22 different cities in 16 states of Ecuador. There are 4400 unique items from 33 families and 337 classes. The evaluation metric is Normalized Weighted Root Mean Squared Logarithmic Error (NWRMSLE):\n\nDeciding evaluation metric is actually the most important part in real world scenarios. You need to make sure it aligns with your business goal. Unfortunately, Kaggle usually don\u2019t share much information on how the decision was made, possibly because of the trade secrets involved. It\u2019s really a very complicated problem with many trade-offs to make, and we could write an entire independent post on that. The discussion forum of this competition has many good insights on whether this metric is appropriate. Go there if you\u2019re interested.\n\nThere are two major problems:\n\nThere are of course other aspects of this dataset that can be improved, e.g. providing the types and intensities of the promotions, the shelf positions of items in stores, but they are mostly icing on the cake. The two problems presented above actually hinders the real-world application of this dataset.\n\nA lot of people had tried to restore the onpromotion information, as we\u2019d learned after the competition was over. My teammate also came up a clever way to restore the information, based on the insight that the stores often had very similar promotion schedule for certain items. So he developed an algorithm that finds subgroups of stores and dates that are (1) always or (2) mostly have the same promotion schedule if we ignore entries having unknown onpromotion, and guesses the unknown based on that pattern. It does not involve any leaderboard probing, but there\u2019s no other way to validate its effect other than using the public leaderboard.\n\nI felt uncomfortable how dramatic the distribution of the predictions had changed from original models to models trained with restored onpromotion from both patterns (1) and (2), so I decided to train my models restored onpromotion from only patterns (1) for 2017 and 2016 data. I left 2015 and 2014 data as is because they contains a lot more NA in onpromotion. And I mixed the models trained with restored onpromotion with ones trained with all unknown onpromotion filled with 0 (50/50 ratio). (That\u2019s why I found out what went wrong very quickly after the competition finished. I just removed the problematic 50% of the ensemble and voil\u00e0 the private score improved.)\n\nThis final 50/50 setup gave me 0.001~0.002 boosts in public score but 0.002+ decreases in private score. My teammate went all-in and seemed to get even larger decreases. That\u2019s the peril of validating using public score. We got comfortable with this very risky bet and did not do enough to hedge the bet. Even the 50/50 ratio was set in a completely arbitrary and subjective way since we have no way to verify it except for the public score.\n\nWe also used all our spare submission slots to probe the leaderboard about the new items. We want to know from the public score which stores had non-zero sales of those new items in the next 5 days (the public split). My teammate built the models for predicting those new items. Unfortunately in the end we are better off predicting zeros for all new items. I\u2019m not sure if removing restored onpromotion can help, but the score differences were less than 0.001 anyway.\n\nThe data from year 2014 to 2017 were used to train my model. I only included selected months for each year so I can avoid modeling the effect of 2016 earthquake and also too much disk serialization. I would use more data if I have 32+ GB RAM in my computer.\n\nI mainly used three different validation periods for this competition:\n\nFor all the DNN models, I used (1) the last 56 days (2) roughly the same 56 days in the previous year and (3) the 16 days after that 56 days in the previous year to predict the next 16 days. To save time, I used various filters to reduce the size of the dataset:\n\nI predicted zero for all the discarded (store, item) combinations. I had planned to explore removing those filters but kept postponing it. My teammate reminded me of it in the last week of the competition, and I checked the validation prediction to see if the models can do better than predicting zeros for those (store, item) with no sales recently. The models actually did significantly better! I was going to leave those simple gains in score on the table!\n\nI did not have much time then, so I quickly trained a set of models using 2017/07/26 validation with the filter removed, and also picked up previously trained models using 2016/09/07 and 56 days filter because it\u2019s the only trained setup with 56 days filer available at the time. The (store, item) discarded by some models will be predicted solely by the models that did not in the final ensemble. This explains the weird model setup we\u2019re about to see in the next section.\n\nI won\u2019t go into model details in the post. They will be covered in the later part(s). I used 4 versions of the settings in my final ensemble:\n\nThe DNN models and GBM models are averaged in log scale using 13:3 weight ratio. (Ratio obtained from cross-validation.)\n\nHere\u2019s how my ensemble would perform without the restored onpromotion:\n\nmeans the predictions from that setting are only used for those (store, item) combonations that are discarded by all other settings. Note that all the models were trained before the end of the competition, and the internal ensemble weights remains the same except the weights for all models trained with restored onpromotion (they are set to zero). I only changed the way how models from different settings are mixed together.\n\nThese are the ideal settings I\u2019d use if I had the time:\n\nWe can remove v12_lgb by taking out 56-day filters in v13 and v14. It might actually do a little bit better. But I think it does not worth the increase of training time.\n\nI\u2019m training some models according to these settings and we\u2019ll see how they perform in the next post.\n\nIt\u2019s worth mentioning that the score distribution in the private leaderboard is more dense than I expected. I expected more variation because of the higher uncertainty in the later days. A possible explanation is we all did a bad job predicting sales in the later days, so we ended up in the same ballpark.\n\nThat\u2019s the problem of this kind of time-split competition. In the real world, we\u2019d probably care more about the prediction for the first 5 days than for the later 11 days, as we can adjust our prediction again 5 days later. How this competition was set up implied we only cared about the later 11 days, which would only be reasonable if the sales data takes 5 days to be ready to use. I think the better way to do this is a two-stage setup like Web Traffic Time Series Forecasting and Zillow\u2019s Home Value Prediction. Release a train dataset solely for the private leaderboard.\n\nThis Part I did not cover what most people care about \u2014 model structures, features, hyper-parameters, ensemble technique, etc. Instead it focused on what I found more important in the data science process \u2014 analyzing and formulating dataset, and described my thought process leading to where I was in the end.\n\nIn fact, you can achieve top 1 spot with a LGBM model with some amount of feature engineering. If you just want to see a top level solution, you could just check out that kernel. Personally I\u2019m satisfied with a working DNN framework that can be used in later projects and requires little feature engineering, even though it may be outperformed by well-crafted GBM models.\n\nI\u2019ll share the scores from model trained with the ideal settings, and maybe describe my models a bit in the next part. Eventually I hope I can find time to extract a cleaner and simpler version of my code and open-source it on Github."
    },
    {
        "url": "https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84?source=user_profile---------17----------------",
        "title": "Use torchtext to Load NLP Datasets \u2014 Part I \u2013",
        "text": "The first thing that needs to be addressed is the documentation. There are basically no concise code examples outside the project README. The next best thing is the unit tests inside the test folder. You have to figure out where things are and put them together on your own.\n\nYou could build documentation from the docstrings with sphinx (we use torchtext 0.2.1 in this post):\n\nThe HTML pages will be in text/docs/build/html folder.\n\nI personally find it easier to read the source code directly.\n\nAs in the previous post, I put the raw data in data folder, and everything that was derived from it in cache folder. This prepare_csv function exists mainly for two reasons:\n\nis for making sure we have the same split every time.\n\nAs mentioned, the tokenization scheme is the same as in the previous post:\n\nThis function returns a list of tokens for one comment. The very long comments are trimmed to MAX_CHARS characters, otherwise NLP.tokenizer can take a long time to return.\n\nThere are two main components in this function: and . The variable specifies the preprocessing pipeline for column and is shared among , , datasets so they are using the same vocabulary. Some details:\n\nThe other very handy feature is , which build the vocabulary so we can convert tokens/words into integer numbers later, and can optionally load pretrained word vectors for you ( will be the loaded vectors aligned with the current vocabulary). Check the available pretrained vectors here. sets the maximum vocabulary size, and sets the minimum times a word has to appear in the corpus to be included.\n\nThe fields , , , , , are binary variables. We\u2019ll need to combine them together later as the target for the model.\n\ndoes basically the same as , but it reads multiple files at the same time. The test dataset is loaded in a separate call because it does not have target columns. We have to specify fields in the exact order in the csv files, and we cannot skip any of the columns. Therefore we have to specify explicitly to skip the first column.\n\nThis should be quite straight-forward. It\u2019ll return something we can iterate through and will stop when the entire dataset has been read. We don\u2019t use the advanced features yet, and because we are using a fixed length, we probably wouldn\u2019t need to anyway.\n\nHere is a simple example usage (for one epoch):\n\nAnd to use the loaded pretrained vectors (assuming your word embedding is located at and train dataset is loaded as ):\n\nThat\u2019s it! We pretty much has what we need to start building and training models.\n\n20180207 Update: I noticed does not have a nor parameter. It actually depends on built-in module to manage randomness. So you need to do one of following to get difference batches between epochs:\n\nThere is a significant problem in the approach presented above. It\u2019s really slow. The entire dataset loading process takes around seven minutes on my computer, while the actual model training takes around ten minutes. We should serialize tokenized sequences and maybe also the vocabularies to make it faster. I plan to write down how to do it in the next post once I figure it out.\n\nBesides serialization, many things can be improved, too. For example, a more flexible train/validation splitting scheme can help a lot. Advanced sorting mechanism and packed sequences probably also worth exploring."
    },
    {
        "url": "https://towardsdatascience.com/learning-note-starspace-for-multi-label-text-classification-81de0e8fca53?source=user_profile---------18----------------",
        "title": "[Learning Note] StarSpace For Multi-label Text Classification",
        "text": "StarSpace is an ambitious model that attempts to solve a wide range of entity-embedding-related problems. It has been created and open-sourced by Facebook AI Research(FAIR). I haven\u2019t read the details of the model in the paper yet, but it\u2019s easy to postulate that it expands on FAIR\u2019s previous text embedding library fastText. StarSpace intends to be a straight-forward and efficient strong baseline, that is, the first model you\u2019d train for a new dataset or a new problem.\n\nIn this post, I\u2019ll write down how I (tried to) get StarSpace to work with the dataset from Kaggle\u2019s Toxic Comment Classification Challenge. This dataset represents a multi-label text classification problem, i.e., more than one labels can be assigned to a single comment. As fastText does not exactly support multi-label classification, I thought StarSpace might be a good alternative.\n\nI\u2019ve found two main problems of StarSpace in its current state that make it not ready as a baseline for this dataset:\n\nIn addition, the documentation of StarSpace is not very comprehensive. It\u2019s reason why this post was written in the first place.\n\nStarspace is written in C++, and you have to build it yourself. It\u2019s quite straight-forward in my Linux Mint 18.3 environment. Just clone the Git repo and run :\n\nCopy the compiled executable to somewhere in your PATH (personally I use )\n\nIn other environment, you might need to jump through additional hoops to meet the requirements. Please check the link for further instructions.\n\nIt goes without saying that you need to download the dataset to your computer first. I usually put the dataset inside sub-folder under the project root, and put everything that derives from the raw dataset inside sub-folder. We\u2019ll use the same setting below.\n\nThe next step is to clean and tokenize the comments. Here I use the English tokenizer from spacy, remove newline characters and some other punctuations, and trim the comments to 20,000 characters. (Cleaning schemes are often dataset-dependent. There are definitely better scheme for this dataset than the simplistic one presented in this post. You could filter out stop words, for instance.)\n\nAfter the comments are tokenized, we combine these tokens with their associated labels, and save them in a format that StarSpace can recognize (Here we use fastText format).\n\nWe use 25% of the train dataset as validation. The results are saved to and respectively. Note because I haven\u2019t found a way to extract predictions easily, the test dataset is not processed here.\n\nThe fastText format is pretty simple. Put a space between consecutive tokens, one comment/instance per line, and labels at the end. Labels are encoded numerically, and a dummy label is created for comments that are not toxic (StarSpace does not accept empty list of labels).\n\nAs an example, this is the first comment in the train dataset:\n\nAfter processing, it becomes:\n\nTo train a model, run this command in the command line:\n\nThis model uses unigram and bigram, requires a token to appear at least 10 times to be consider, and use 4 threads. More parameters can be found via .\n\nStarSpace save the model to , and the word embedding and label embedding vectors to . You can analyze or visualize those embedding vectors for further insights.\n\nTo evaluate the model with the validation dataset, run this command:\n\nBesides command line outputs, it\u2019ll also write the case-by-case evaluation to . An example:\n\nNote that most of the comments in the dataset are not toxic (__label__6), so I\u2019m really cherry-picking examples here.\n\nThe meaning of the values assigned to each label is unclear. I guess the answer can be found in the the paper.\n\nIt\u2019s unfortunate that StarSpace is not ready to serve as a baseline for the toxic comment dataset yet. This post keeps track of the steps needed to get as far as we can for future reference. Hopefully StarSpace will continue to be developed and be truly ready for production. We\u2019ll come back for more then."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/sgd-implementation-in-pytorch-4115bcb9f02c?source=user_profile---------19----------------",
        "title": "SGD implementation in PyTorch \u2013 The Artificial Impostor \u2013",
        "text": "PyTorch documentation has a note section for optimizer that says:\n\nThis is the formula the paper used (g being the gradient; v the velocity; p the parameters/weights; rho the momentum coefficient; lr the learning rate):\n\nAnd this is the formula PyTorch used:\n\nThe only difference is where the learning rate is applied. In the paper the learning rate is applied when calculating new velocity, and in PyTorch the learning rate is applied when calculating new parameters/weights.\n\nIt may or may not have observable impacts on train and validation loss, but being aware of the difference can help guide the tuning schedule toward the right direction.\n\nFor example, if we\u2019re using schedule that reduce the learning rate once the validation score plateaus. If we misunderstood the PyTroch SGD implementation to be the one in the paper, we\u2019d expect gradients to have much less influence in later velocity updates; in other words, we\u2019d expect the momentum to increase. But in reality the momentum did not change. Instead, we were just getting smaller changes in parameters in each iteration.\n\nIf we substitute in the PyTorch formula, we get and . If we set we\u2019d get the same formula in the paper. So what PyTorch does is actually adjusting the momentum coefficient relative to the learning rate, so momentum stays invariant to changes in learning rate.\n\nIt\u2019s not hard to modify the SGD implementation in PyTorch and make it consistent with the paper (If that\u2019s what you want).\n\nIf we take a look at the source code, we\u2019d find it quite easy to read:\n\nLine 23 and 25 get the gradients. Line 30, 33, and 35 update the velocity. Line 39 updates the parameters.\n\nSo if we just tweak line 19 and apply the learning rate directly on the gradients (line 27) we\u2019d have implemented the formula in the paper:\n\nThe small difference in implementation might not be a big deal, but can cause you some confusion when tuning if you have not understood it correctly. Moreover, tuning algorithms that are based on the alternative formula may not work as expected in PyTorch. For example, in YellowFin paper[2] this is used, where learning rate and momentum coefficient are decoupled:\n\nYou\u2019ll need to be careful when implementing those algorithms in PyTorch. Otherwise a huge amount of time is likely to be wasted on debugging."
    },
    {
        "url": "https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66?source=user_profile---------20----------------",
        "title": "Understanding Bidirectional RNN in PyTorch \u2013",
        "text": "Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. The input sequence is fed in normal time order for one network, and in reverse time order for another. The outputs of the two networks are usually concatenated at each time step, though there are other options, e.g. summation.\n\nThis structure allows the networks to have both backward and forward information about the sequence at every time step. The concept seems easy enough. But when it comes to actually implementing a neural network which utilizes bidirectional structure, confusion arises\u2026\n\nThe first confusion is about the way to forward the outputs of a bidirectional RNN to a dense neural network. For normal RNNs we could just forward the outputs at the last time step, and the following picture I found via Google shows similar technique on a bidirectional RNN.\n\nBut wait\u2026 if we pick the output at the last time step, the reverse RNN will have only seen the last input (x_3 in the picture). It\u2019ll hardly provide any predictive power.\n\nThe second confusion is about the returned hidden states. In seq2seq models, we\u2019ll want hidden states from the encoder to initialize the hidden states of the decoder. Intuitively, if we can only choose hidden states at one time step(as in PyTorch), we\u2019d want the one at which the RNN just consumed the last input in the sequence. But if the hidden states of time step n (the last one) are returned, as before, we\u2019ll have the hidden states of the reversed RNN with only one step of inputs seen.\n\nKeras provides a wrapper for bidirectional RNNs. If you take a look at line 292 in wrappers.py:\n\nYou\u2019d find that by default the outputs of the reversed RNN is ordered backward as time step (n\u20261). Keras will reverse it when is true (it\u2019s false by default). So if we\u2019re taking one time step output, Keras will take the one at time step n for normal RNN and the one at time step 1 for reverse RNN. This pretty much confirms that figure 2 shows flawed structure.\n\nWith the first confusion sorted out. We are now interested in how to use bidirectional RNNs correctly in PyTorch:\n\nThe above notebook answered the two confusions we had (assuming is false):\n\n(Side note) The output shape of GRU in PyTorch when is false:\n\nThe LSTM\u2019s one is similar, but return an additional cell state variable shaped the same as h_n."
    },
    {
        "url": "https://becominghuman.ai/feature-importance-measures-for-tree-models-part-ii-20c9ff4329b?source=user_profile---------21----------------",
        "title": "Feature Importance Measures for Tree Models \u2014 Part II",
        "text": "In part II we\u2019re going to apply the algorithms introduced in part I and explore the features in the Mushroom Classification dataset. Honestly, it might not be the best dataset to demonstrate feature importance measures, as we\u2019ll see in the following sections. But it\u2019s the dataset I was able to find given the limited time that is interesting (I want to know how to determine if a mushroom is edible) and also has a large number of categorical features (another topic I\u2019m digging recently).\n\nBecause Permutation Importance is not supported in scikit-learn, this case study uses R to measure importances and make visualization. The exact distribution and version of R is Microsoft R Open 3.4.0. The Reproducible R Toolkit( ) is used to install all external packages from the snapshot of CRAN on 2017\u201310\u201326.\n\nThe package nicely supports categorical variables (factors) via parameter. If you\u2019re not familiar with how to deal with categorical variables, I recommend this post by Laurae to start with:\n\nThe official documentation of the parameter:\n\nHere we\u2019re going to use the option \u2018order\u2019 as recommended. For other two options, \u2018ignore\u2019 will be presented with XGBoost measures later, and \u2018partition\u2019 is omitted since the computation costs is too high for factors with even slightly larger cardinality (2^n possible splits, where n is the cardinality). For each value, the model \uff08250 trees) is re-fitted 20 times to capture the random effects. The top 10 important features are presented in the chart:\n\nFirst observation is that parameter has dramatic influence on the calculated importances. The random forest algorithm randomly select features in each possible split point to evaluate, and higher means the most important feature( in this case) is more likely to be in that set of selected features. Therefore higher can be used to do more aggressive feature selection, and lower can help you find out the predictive power of each feature.\n\nSecond observation is that Permutation Importances are more uniform over the variables than Gini Importances. This is consistent with the example given in the Element of Statistical Learning (see part I).\n\nWe can also removes the most important feature(s) from the training data to get a clearer picture of the predictive power of less important features:\n\nUnlike , XGBoost doesn\u2019t have built-in support for categorical variables. We can one-hot encode or encode numerically (a.k.a. label encoding) ourselves. Numeric encoding ordered by mean target rate as in is more involved and weren\u2019t implemented here.\n\nFirst we take a look at results from one-hot encoding scheme (max_depth=6, eta=1, lambda=0, subsample=.8, colsample.bytree=.8, repeats 20 times):\n\nOne-hot encoding is usually not recommended for gradient boosting trees because it can easily lead to over-fitting. But in this case it\u2019s not a problem judging from the validation score(100% accuracy).\n\nSplit-based importance can be misleading as stated in part I. As you can see from the chart, the feature level with the most predictive power ( ) only ranked second.\n\nComparing with Permutation Importances from random forest with one-hot encoding ( needs to be higher in one-hot scheme, otherwise would have under-fitted easily):\n\nYou can see that importance measures from XGBoost has much higher variances.\n\nNow we turn to ad-hoc numeric encoding scheme (max_depth=3, eta=1, lambda=0, subsample=.8, colsample.bytree=.8, repeats 20 times):\n\nBecause of the ad-hoc(hence non-informative) numeric encoding, brings much less gains to the model. Also the variance is larger.\n\nComparing with Permutation Importances from random forest with the same encoding scheme:\n\nInterestingly, the rankings are very different in the above charts. It\u2019s possibly from the significant under-fitting of (validation accuracy drops from ~99% to ~90%)\n\nThis dataset is not suitable to demonstrate Boruta because every features are significantly better than noise, and all features are confirmed at the same time. It worth noting that Boruta uses internally and setting influence the results. There are three internal importance measures to choose from: Gini Importance, Permutation Importance, Normalized Permutation Importance. I haven\u2019t figure out how Normalized Permutation Importance (Z-score) is computed, but it seems not preserving the ranking in Permutation Importance (moreover, if using it for this dataset, Boruta will reject one of the features).\n\nWe can use the results so far to guide data analysis. For example, we can start with examining the most important feature :\n\nIt shows that if a mushroom smells, the we can confidently determine whether it\u2019s edible or not. The only samples that needs further features is those without odor.\n\nNow we check the combination of and the other two most important features:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3?source=user_profile---------22----------------",
        "title": "Feature Importance Measures for Tree Models \u2014 Part I",
        "text": "This post is inspired by a Kaggle kernel and its discussions [1]. I\u2019d like to do a brief review of common algorithms to measure feature importance with tree-based models. We can interpret the results to check intuition(no surprisingly important features), do feature selection, and guide the direction of feature engineering.\n\nHere\u2019s the list of measures we\u2019re going to cover with their associated models:\n\nNote that measure 2 and 3 are theoretically applicable to all tree-based models.\n\nAccording to [1], MDI counts the times a feature is used to split a node, weighted by the number of samples it splits:\n\nHowever, Gilles Louppe gave a different version in [4]. Instead of counting splits, the actual decrease in node impurity is summed and averaged across all trees. (weighted by the number of samples it splits).\n\nIn R package , the implementation seems to be consistent with what Gilles Louppe described [5] (another popular package, , also seems to be doing the same) [6]:\n\nThis is IMO most interesting measure, because it is based on experiments on out-of-bag(OOB) samples, via destroying the predictive power of a feature without changing its marginal distribution. Because scikit-learn doesn\u2019t implement this measure, people who only use Python may not even know it exists.\n\nHere\u2019s permutation importance described in the Element of Statistical Learning:\n\nFor other tree models without bagging mechanism (hence no OOB), we can create a separate validation set (apart from the test set) and use it to evaluate the decrease in accuracy.\n\nThis algorithm gave me an impression that it should be model-agnostic (can be applied on any classifier/regressors), but I\u2019ve not seen literatures discussing its theoretical and empirical implications on other models. The idea to use it on neural networks was briefly mentioned on the Internet. And the same source claimed the algorithm works well on SVM models [8].\n\nBoruta is the name of an R package that implements a novel feature selection algorithm. It randomly permutes variables like Permutation Importance does, but performs on all variables at the same time and concatenates the shuffled features with the original ones. The concatenated result is used to fit the model.\n\nDaniel Homola, who also wrote the Python version of Boruta(BorutaPy), gave an wonderful overview of the Boruta algorithm in his blog post [7]:\n\nThe shuffled features (a.k.a. shadow features) are basically noises with identical marginal distribution w.r.t the original feature. We count the times a variable performs better than the \u201cbest\u201d noise and calculate the confidence towards it being better than noise (the p-value) or not. Features which are confidently better are marked \u201cconfirmed\u201d, and those which are confidently on par with noises are marked \u201crejected\u201d. Then we remove those marked features and repeat the process until all features are marked or a certain number of iteration is reached.\n\nAlthough Boruta is a feature selection algorithm, we can use the order of confirmation/rejection as a way to rank the importance of features.\n\nFor Kagglers, this part should be familiar due to the extreme popularity of XGBoost and LightGBM. Both packages implement more of the same measures (XGBoost has one more):\n\nFirst measure is split-based and is very similar with the one given by [1] for Gini Importance. But it doesn\u2019t take the number of samples into account.\n\nThe second measure is gain-based. It\u2019s basically the same as the Gini Importance implemented in R packages and in scikit-learn with Gini impurity replaced by the objective used by the gradient boosting model.\n\nThe final measure, implemented exclusively in XGBoost, is counting the number of samples affected by the splits based on a feature.\n\nThe default measure of both XGBoost and LightGBM is the split-based one. I think this measure will be problematic if there are one or two feature with strong signals and a few features with weak signals. The model will exploit the strong features in the first few trees and use the rest of the features to improve on the residuals. The strong features will look not as important as they actually are. While setting lower learning rate and early stopping should alleviate the problem, also checking gain-based measure may be a good idea.\n\nNote that these measures are purely calculated using training data, so there\u2019s a chance that a split creates no improvement on the objective in the holdout set. This problem is more severe than in the random forest since gradient boosting models are more prone to over-fitting. It\u2019s also one of the reason why I think Permutation Importance is worth exploring.\n\nAs usual, I will demonstrate some results of these measures on actual datasets in the next part."
    },
    {
        "url": "https://towardsdatascience.com/note-statistical-inference-the-big-picture-b1c1c4099cc7?source=user_profile---------23----------------",
        "title": "[Note] Statistical Inference: The Big Picture \u2013",
        "text": "This post contains my notes from this paper and the associated comments:\n\nThis is a high-level paper, as you can tell by the title. It tries to pull us from the frequentist / Bayesian quagmire to the more important aspect of the field of statistics, that is, the mismatch of the model and the real data. This reminds me of the famous quote from George Box:\n\nIMO, this is a very good read for every statistician or data scientist. (You might need at least some basic statistical background to read it.) Some part of the paper might need some time for you to digest, so it is worth reviewing every once in a while.\n\nIn this paper, Kass describe \u201cthe dominant contemporary philosophy of statistics\u201d and call it \u201cstatistical pragmatism\u201d\uff0c in an attempt to \u201dplace in the center of our logical framework the match or mismatch of theoretical assumptions with the real world of data.\u201c instead of teaching student to \u201drecite correctly the long-run interpretation of confidence intervals\u201c\uff08focusing on the frequentist / Bayesian dichotomy.\uff09\n\nKass thinks the standard \u201cbig picture\u201d story as depicted in Figure 3 above is \u201cnot a good general description of statistical inference\u201d, and proposes a new perspective as in Figure 1(\u201cmy claim is that Figure 1 is more accurate\u201d)\n\nThe concept Figure 3 is trying to get across:\n\nWhile Figure 1 is trying to remind us:\n\nTo put more emphasize on the \u201chypothetical\u201d part, Fishers\u2019 \u201cpopulation mean\u201d (as opposed to sample mean) can be replaced by \u201ctheoretical mean\u201d.\n\nComparing the use of probability to describe variation and to express knowledge:\n\n\u201cLet us assume the data are normally distributed\u201d is a shorthand for assuming \u201cthe variability of the data is adequately consistent with variability that would occur in a random sample.\u201d\n\n\u201cLeap of faith\u201d \u2014 crossing the real-theoretical bridge, in the absence of an explicit chance mechanism, \u201cmakes statistical reasoning possible in the vast majority of applications.\u201d\n\nIn modern Bayesian approach, confidence intervals / posterior intervals are \u201csummaries of inference about parameters conditional on an assumed model\u201d. Hypothesis testing /model checking is the \u201cthe process of comparing observed data to replications under the model if it were true\u201d."
    },
    {
        "url": "https://towardsdatascience.com/building-a-translation-system-in-minutes-d82a154f603e?source=user_profile---------24----------------",
        "title": "Building a Translation System In Minutes \u2013",
        "text": "Sequence-to-sequence(seq2seq)[1] is a versatile structure and capable of many things (language translation, text summarization[2], video captioning[3], etc.). For a short introduction to seq2seq, here are some good posts: [4][5].\n\nSean Robertson\u2019s tutorial notebook[6] and Jeremy Howard\u2019s lectures [6][7] are great starting points to get a firm grasp on the technical details of seq2seq. However, I\u2019d try to avoid implementing all these details myself when dealing with real-world problems. It\u2019s usually not a good idea to reinvent the wheel, especially when you\u2019re very new to this field. I\u2019ve found that OpenNMT project is very active, has good documentation, and can be used out-of-the-box:\n\nThere also are some more general frameworks (for example, [8]), but may need some customization to make it work on your specific problem.\n\nThere are two official versions of OpenNMT:\n\nWe\u2019re going to use the PyTorch version in the following sections. We will walk you through the steps needed to create a very basic translation system with a medium-sized dataset.\n\nClone the OpenNMT-py git repository on Github into a local folder:\n\nYou might want to fork the repository on Github if you\u2019re planning to customize or extend it later. Also it is suggested in the README:\n\nHere we\u2019re going to use the dataset from AI Challenger \u2014 English-Chinese Machine Translation competition. It is a dataset with 10 million English-Chinese sentence pairs. The English copora are conversational English extracted from English learning websites and movie subtitles. From my understanding, most of the translation are submitted by enthusiasts, not necessarily professionals. The translated Chinese sentences are checked by human annotators.\n\nDownloading the dataset requires account sign-up and possibly ID verification (can\u2019t remember whether the latter is mandatory). If that\u2019s a problem for you, you can try datasets from WMT17.\n\nThere are some problems to the AI Challenger dataset: 1. The quality of the translation is not consistent. 2. Because many of sentences are from movie subtitles, the translation are often context-dependent (related to the previous or the next sentence). However, there are no context information available in the dataset.\n\nLet\u2019s see how the out-of-the-box model perform on this dataset. Because of memory restriction, I down-sampled the dataset to 1 million sentences.\n\n\uff08We\u2019ll assume that you put the dataset into folder challenger under the OpenNMT root directory.\uff09\n\nThe validation and test dataset comes in XML format. We need to convert it to plain text files where a line consists of a single sentence. A simple way to do that is using BeautifulSoup. Here\u2019s a sample chunk of code:\n\nThe input sentence must be tokenized with tokens space-separated.\n\nFor English, there are a few tokenizers to choose from. One example is :\n\nIt turns \u201cIt\u2019s a neat one \u2014 two. Walker to Burton.\u201d into \u201cIt \u2018s a neat one \u2014 two . Walker to Burton .\u201d.\n\nFor Chinese, we use the simplest character-level tokenization, that is, treat each character as a token:\n\nIt turns \u201c\u6211\u5c31\u4e00\u592924\u5c0f\u65f6\u90fd\u5f97\u5728\u5979\u773c\u76ae\u5b50\u5e95\u4e0b\u3002\u201d into \u201c\u6211 \u5c31 \u4e00 \u5929 2 4 \u5c0f \u65f6 \u90fd \u5f97 \u5728 \u5979 \u773c \u76ae \u5b50 \u5e95 \u4e0b \u3002\u201d. (Note because the token are space-separated, we need a special token \u201c<s>\u201d to represent the space characters.)\n\nSimply run the following command in the root directory:\n\nThe preprocessing script will go through the dataset, keep track of token frequencies, and construct a vocabulary list. I ran into memory problem here and had to down-sample the training dataset to 1 million rows, but I think the raw dataset should fit into 16GB memory with some optimization.\n\nIt\u2019ll use your first GPU to train a model. The default model structure is:\n\nThe vocabulary size of source and target corpora is 50,002 and 6,370, respectively. The source vocabulary is obviously truncated to 50,000. The target vocabulary is relatively small because there are not that many common Chinese characters.\n\nReplace with your own model. The model naming should be obvious: this is a model after 14 epochs of training, with 58.79 accuracy and 7.51 perplexity on validation set.\n\nYou can also calculate BLEU score with the following:\n\nNow you have a working translation system!\n\nIf you want to submit the translation to AI Challenger, you need to reverse Step 4 and then Step 3. Again, they should be quite simple to implement.\n\nEnglish: You knew it in your heart you haven\u2019t washed your hair\n\nChinese(pred): \u4f60\u5fc3\u91cc\u6e05\u695a\u4f60\u6ca1\u6d17\u5934\u53d1\n\nChinese(gold): \u4f60\u5fc3\u91cc\u77e5\u9053\u4f60\u538b\u6839\u5c31\u6ca1\u6d17\u8fc7\u5934\n\nEnglish: I never dreamed that one of my own would be going off to a University, but here I stand,\n\nChinese(pred): \u6211\u4ece\u6765\u6ca1\u68a6\u5230\u8fc7\u6211\u7684\u4e00\u4e2a\u4eba\u4f1a\u53bb\u5927\u5b66\uff0c\u4f46\u662f\u6211\u7ad9\u5728\u8fd9\u91cc\uff0c\n\nChinese(gold): \u6211\u4ece\u6ca1\u60f3\u8fc7\u6211\u7684\u5b69\u5b50\u4f1a\u4e0a\u5927\u5b66\uff0c\u4f46\u6211\u7ad9\u5728\u8fd9\uff0c\n\nEnglish: We just don\u2019t have time to waste on the wrong man.\n\nChinese(pred): \u6211\u4eec\u53ea\u662f\u6ca1\u65f6\u95f4\u6d6a\u8d39\u4eba\u3002\n\nChinese(gold): \u5982\u679c\u627e\u9519\u4e86\u4eba\u6211\u4eec\u53ef\u73a9\u4e0d\u8d77\u3002\n\nThe above three examples are, from top to bottom, semantically correct, partially correct, and entirely incomprehensible. After examining a few examples, I found most of the machine translated sentences were partially correct, and there were surprising amount of semantically correct ones. Not a bad result, considering how little effort we\u2019ve had put in so far.\n\nIf you submit the result you should get around .22 BLEU. The current top BLEU score is .33, so there\u2019s a lot of rooms for improvement. You can check out in the root folder for more built-in model parameters. Or dive deep into the codebase to figure out how things work and where might be improved.\n\nThe other paths include applying word segmentation on Chinese sentences, adding named entity recognition, using pronunciation dictionary[10] to guess translation to unseen English names, etc."
    },
    {
        "url": "https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-3-1b161d030cd4?source=user_profile---------25----------------",
        "title": "[Learning Note] Dropout in Recurrent Networks \u2014 Part 3",
        "text": "This dataset is used in the paper [1] to evaluate different models. There are 10,620 training samples, and 2,655 test/validation samples. It is a regression problem. Given a sequence of words, the model need to predict what is the score the user gave to the film. The scores are normalized to make them zero-centered with unit variance. In the following discussion, \u201craw MSE\u201d will correspond to the mean squared error to the normalized scores, \u201cRMSE\u201d will be the root mean squared error to the original scores (the paper use RMSE).\n\nBatch size is set to 128 with sequence length 200 (padded), the same as in the paper. The number of hidden units remained the same in all model. The only difference is how the dropout is applied.\n\nThe dropout probability used in paper appears mostly to be 0.5. It might have something to do with th slow convergence speed as you can see in the figure above. The variational LSTM requires hundreds of epochs to outperform the other two model. I did\u2019nt have much time to train so many epochs, so the dropout probability was tuned down to boost speed while allowing some amounts of over-fitting. For the exact probabilities, please refer to the spreadsheet at the end of this section.\n\nNo embedding dropout for all three models.\n\nThe chart \u201c(MC \u2014 Approx) Histogram\u201d is a histogram of the raw MSE of MC dropout minus the one of standard dropout approximation for each sample. Note in both variational and naive dropout LSTM models, MC dropout generally produces lower raw MSE.\n\nNaive dropout seems to be the best performer, and does not tend to over-fit over time.\n\nFor weight dropped LSTM, MC dropout under-performs standard dropout approximation. It\u2019s also the case for variational LSTM without recurrent dropout.\n\nIn this case, \u201cno dropout\u201d model over-fitted severely. Comparing with the results from Keras, we might be able to attribute the steep climb in RMSE to the absence of weight decay. However, \u201cnaive dropout\u201d model in PyTorch also had a slow upward trend in RMSE. \u201cVariational weight dropped\u201d model seems to have some under-fitting issues.\n\nBelow is the detail records of the experiments and their results:"
    },
    {
        "url": "https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8?source=user_profile---------26----------------",
        "title": "[Learning Note] Dropout in Recurrent Networks \u2014 Part 2",
        "text": "Before going into the experiments, I\u2019d like to examine the implementations in detail for better understanding and future reference. Because of space limitation, I used to omit the lines that aren\u2019t essential to the current discussion.\n\nHere I use Keras that comes with Tensorflow 1.3.0.\n\nThe implementation mainly resides in class. We start with class method. It is invoked for every batch in method to provide dropout masks. \uff08The input dropout and recurrent dropout rates have been stored as instance attributes in .\uff09\n\nThe inputs are arranged in the form of (samples, time (padded with zeros), input_dim). The above code block creates input masks with shape (samples, input_dim), and then randomly sets elements to zero. So new masks are sampled for every sequence/sample, consistent with what was described in paper [1].\n\nNote four different masks are created, corresponds to the four gates in LSTM. Only untied-weights LSTM supports this setting. (More details below).\n\nSimilarly, the above creates four recurrent masks with shape (samples, hidden_units).\n\nNext we turn to method, which is executed for each time step sequentially:\n\nKeras has 3 implementations of LSTM, with implementation 0 as default:\n\nThe implementation 2 corresponds to tied-weights LSTM. And the above code block implements dropout just like in this formula [1]:\n\nNote how it just take the first mask and discard the rest (three masks). That is because this formulation requires the RNN dropout be shared for all gates.\n\nIt appears implementation 0 and 1 differs in the way how input dropout is applied. In implementation 0 the transformed inputs are precomputed outside method, while in implementation 1 the inputs are dropped out and transformed inside .\n\n\uff08Note how each gate use its own dropout mask, and how transformed inputs and hidden states are combined for each gate.\uff09\n\nThat's it. The implementation doesn\u2019t have any surprises, so you can use and parameters with confidence. The only thing you need to consider is probably whether to use implementation 2 instead of 0 to speed things up."
    },
    {
        "url": "https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307?source=user_profile---------27----------------",
        "title": "[Learning Note] Dropout in Recurrent Networks \u2014 Part 1",
        "text": "Before Gal and Ghahramani [6], new dropout masks are created for each time step. Empirical results have led many to believe that noise added to recurrent layers (connections between RNN units) will be amplified for long sequences, and drown the signal [7]. Consequently, it was concluded that dropout should be used with only the inputs and outputs of the RNN. (See the left part of figure below)\n\nThe variant Gal and Ghahramani [6] proposed is to use the same dropout mask at each time step for both inputs, outputs, and recurrent layers. And it is equivalent to a form of variational inference in recurrent neural networks, which will be derived in the following sections.\n\n(BNN) Given weight matrices Wi and bias vectors bi for layer i, we often place standard matrix Gaussian prior distributions over the weight matrices, p(Wi) = N(0, I) and often assume a point estimate for the bias vectors for simplicity.\n\nThe posterior over the weights given our observables X, Y is p(\u03c9|X, Y), and is generally intractable. For VI, we need to minimize the KL divergence between the approximating distribution q(\u03c9) and the full posterior:\n\nThe above formulation is called evidence lower bound (ELBO). The first term(expected log likelihood) measures how well samples from approximate posterior q(\u03c9) explain data x (reconstruction cost). The second term(prior KL) ensures explanation of doesn\u2019t deviate too far from our prior beliefs (penalizes complexity). [4]\n\nNow we turn to simple RNN models. Derivations for LSTM and GRU follows similarly. Given input sequence x = [x_1, \u2026, x_T] of length T, a simple RNN is formed by repeated application of a function f_h. This generates a hidden state h_t for time step t:\n\nThis RNN can be viewed as a probabilistic model by regarding \u03c9 = {W_h,U_h,b_h,W_y,b_y} as random variables (following normal prior distributions)\n\nApproximate it with Monte Carlo integration with a single sample:\n\nPlug it back to ELBO and we get:\n\nNote we sample a new \u03c9 for each sequence, but each symbol in the sequence is passed through the same function with the same \u03c9.\n\nWe factorize the approximation distribution over the weight matrices and their rows in \u03c9. For every weight matrix row w_k the approximating distribution is:\n\nwith m_k variational parameter (row vector), p given in advance (the dropout probability), and small \u03c3\u00b2. Optimizing the loss function using gradient descent will update m_k, which correspond to a row of the weight matrix in the traditional settings.\n\nThe prior KL (q(\u03c9)||p(\u03c9)) can be approximated as L2 regularization over variational parameter m_k.\n\nPredictions can be made via MC dropout:\n\nLSTM is defined using four gates: \u201cinput\u201d, \u201cforget\u201d, \u201coutput\u201d\uff0c and \u201cinput modulation\u201d:\n\nThis parameterization is call untied-weights LSTM. The alternative parameterization is called tied-weights LSTM:\n\nThe approximation distributions q(\u03c9) are different for these two parameterization. For variational untied-weights LSTM one could use different dropout masks for different gates, because q(\u03c9) is placed over matrices instead of inputs. This leads to slower forward-pass but slight better results than variational tied-weights LSTM:\n\nIn comparison, Zaremba et al.[7] use different dropout masks in every time step, and does not use dropout on hidden states:\n\nGal and Ghahramani [6] also propose a new way to regularize word embedding, in addition to apply dropout on inputs. They suggest dropout be used on word type, instead of individual words. That is, randomly setting rows of the embedding matrix to zero.\n\nSo, for the sequence \u201cthe dog and the cat\u201d, the dropout might yield \u201c \u2014 dog and \u2014 cat\u201d or \u201cthe \u2014 and the cat\u201d, but never \u201c \u2014 dog and the cat\u201d.\n\nThis technique can be interpreted as encouraging the model not to depend on single words for its output.\n\nRecently proposed [8], weight-dropped LSTM apply dropout to recurrent hidden-to-hidden weight matrices (U_i, U_f, U_g, U_o), in hope to prevent over-fitting on the recurrent connection.\n\nFrom my understanding, weight-dropped LSTM is essentially further factorizing the approximation distribution over the the elements of each row (untied-weight LSTM). In addition, \u03c9 was sampled for each mini-batch instead of for each sequence. It also comes with some performance bonus. As the dropout operation is applied once to the weight matrices, before the forward and backward pass, the impact on training speed and any standard (and optimized) black box RNN implementation can be used.\n\nFor variational dropout, Keras has already implemented it in its LSTM layer Use parameter dropout for input dropout (W matrices). Use parameter recurrent_dropout for hidden state dropout (U matrices). The dropout seems to be in untied-weights settings. PyTorch does not natively support variational dropout, but you can implement it yourself by manually iterating through time steps, or borrow code from AWD-LSTM Language Model (WeightDrop with variational=True). It seems \u03c9 was sampled for each mini-batch in these implementations, probably for simplicity.\n\nFor word embeddings dropout, Keras seems to once have dropout parameter in its embedding layer, but it has been removed for some reason. I\u2019m not aware other native way to do it in Keras so far. For PyTorch, once again you can borrow code from AWD-LSTM Language Model (see embed_regularize.py).\n\nThat\u2019s it for the theoretical part. I\u2019ve also done some preliminary empirical experiments to evaluate the effect of different dropout variants, but I\u2019ll need to find some time to organize them to be more coherent and create some visualization."
    },
    {
        "url": "https://medium.com/machine-learning-world/deblur-photos-using-generic-pix2pix-6f8774f9701e?source=user_profile---------28----------------",
        "title": "Deblur Photos Using Generic Pix2Pix \u2013 Machine Learning World \u2013",
        "text": "Last week my partner came across a problem at work. There were some poorly shot photos that were quite blurry and needed to be repaired. Unsharp masking didn\u2019t work well, along with a few free reparing softwares. The problem was solved by manually recreate important parts of the photo using Photoshop. But I couldn\u2019t help but wonder if deblurring can be done via some generic deep learning algorithms.\n\nI started with some super resolution algorithms, but soon realized that there are some differences. De-blurring, in essence, is trying to reverse convolution on an image (blind decovolution). Super-resolution, on the other hand, is trying to reverse the down-sampling on an image. Therefore I found pix2pix model should be more adequate for this task (as paired mappings between blurry photos and clear photos)[1]\n\nThe code is based on pix2pix implementation by mrzhu-cool on Github, with the following modifications.\n\nThe MIRFLICKR-25k dataset is used (in hope of generalizing better with real-life photos). The first 20k photos is used in training. Scaling and random cropping is applied. For the last 5k photos, around 2k are used as validation/development set, and the rest is reserved as test set (not used yet).\n\nThe artificial blurring is created by applying an uniform 3x3 filter and an Gaussian 5x5 filter (there\u2019s a lot of rooms to be improved):\n\nThe code released on Github. It has been tested on ceshine/cuda-pytorch:0.2.0 Docker image. Please check the accompanying Dockerfile for details."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/docker-nvidia-gpu-nvidia-docker-808b23e1657?source=user_profile---------29----------------",
        "title": "Docker + NVIDIA GPU = nvidia-docker \u2013 The Artificial Impostor \u2013",
        "text": "Update on 2018-02-10: nvidia-docker 2.0 has been released and 1.0 has been deprecated. Check the wiki for more info.\n\nDocker is a wonderful technology. In my opinion, every reasonably Python(what I am most familiar with) project that requires teamwork should have at least one Dockerfile, preferably with application-level configuration files(e.g. Docker Compose files). Life before Docker was full of the infamous \u201cIt works on my machine\u201c problems. I get to experience that nightmare again recently. When I asked someone from another team what packages are needed to run their code, he handed my a dump of output from his machine\u2026\n\nDocker virtualizes CPU natively. CPU resource should automatically available to you inside the container. You can even allocate CPU resource with parameters (e.g. ). Not so easy for GPU. GPU usually requires specialized(often proprietary) drivers to run inside the container.\n\nFor NVIDIA GPUs, one the early solutions is to fully install the driver inside the container. The problem of this solution is that the driver version inside the container must exactly match the driver version on the host machine. That means whenever I upgrade the driver on the host machine, I must rebuild every Docker images that uses GPU (not to mention it\u2019s not really straightforward to install driver inside containers). I gave up that solution very quickly and settled with miniconda to manage my deep learning packages. It caused some regression, since previously I had mostly switched from virtualenvwrapper to Docker containers for managing Python development environment.\n\nNVIDIA has been developing another solution since late 2015. Recently I\u2019ve noticed open-sourced deep learning implementations are starting to come with docker images, and a PaaS provider seems to build the entire service around GPU-enabled docker images. It seems to me that the new solution has become production-ready, so it\u2019s a good time to give it a try."
    },
    {
        "url": "https://medium.com/@ceshine/python-debugging-pitfall-mixed-use-of-property-and-getattr-f89e0ede13f1?source=user_profile---------30----------------",
        "title": "Beware of Mixed use of Property and __getattr__ \u2013 CeShine Lee \u2013",
        "text": "Recently I\u2019ve been trying to apply Weight Dropped LSTM/RNN to a time series prediction project. The author of the paper released the source code, but it\u2019s written for PyTorch 0.1.12_2. If you try to run it with PyTorch 0.2.0, you\u2019d get the following error:\n\nBecause the time series project has used some features in PyTorch 0.2.0, and I\u2019d really rather not spend time making it backward-compatible, some investigation has been conducted to find a way to make the code work with 0.2.0.\n\nThe exception is raised from the class WeightDrop when tying to load the model parameters into CUDA. After some probing, I found the problem originates from this part of the code:\n\nwill be missing from the object right after the operation. I am puzzled for a while why deleting an element from an OrderedDict would mask a class property. It just doesn\u2019t make sense.\n\nFortunately I was able to find the answer on Google. In this Github issue (Usage of both @property and __getattr__ can lead to obscure and hard to debug errors) , skrivanos wrote:\n\nIf we overwrite the LSTM class to print the exception raised:\n\nNow the solution is clear. We just need to update in . Namely, add this line inside the loop (sorry for the weird formatting):\n\nThis effectively renames the weight matrix that has been applied dropout.\n\nPlease be advised this might not be the best way to do Pytorch 0.1.12 to 0.2.0 migration, but only a hacky way to make it work. It also serves as an example to demonstrate how mixing @property and __getattr__ can lead to confusing error messages that inexperience developers like me can get derailed and waste lots of time. I guess the lesson is to be doubtful of the error messages, and continue to explore other possibilities when the literal interpretation of the messages is leading you nowhere.\n\nAs this issue points out, the previously proposed solution is broken on GPU. It is due to the way PyTorch 0.2.0 flatten RNN weights. The proper solution is to stop PyTorch from flattening by killing the function in :\n\nwhere is a dummy method that returns nothing. (Note you no longer need to as proposed previously."
    },
    {
        "url": "https://towardsdatascience.com/version-control-for-jupyter-notebook-3e6cef13392d?source=user_profile---------31----------------",
        "title": "Version Control for Jupyter Notebook \u2013",
        "text": "Jupyter Notebook Best Practices for Data Science by Jonathan Whitmore has been incredibly helpful and I\u2019ve been strongly encouraging team members to adopt at least a subset of them, specifically the post-save hook and the notebook naming convention.\n\nThe idea is to automatically save a copy of the notebook in both Python script and HTML format whenever the notebook is saved.\n\nI used to clear all output in the notebook before committing it to the version control to make change history cleaner, but that creates two problems:\n\nAll in all, the most direct way to solve these problems is to just include the outputs of the notebook in the version control. (Though I\u2019d recommend restart the kernel and re-run the whole notebook sequentially before committing.)\n\nConvert the notebook to a Python script and a HTML file. Use the Python script to do code diff in the review process. HTML file works best as an attachment in cross-team communication emails. (Personally I\u2019d only add HTML files to version control only if there are active participants of the project who don\u2019t really use Jupyter.)\n\nThe following gist is basically the direct copy from Jupyter Notebook Best Practices for Data Science. It\u2019s tested against the latest Jupyter and put here for quick reference:\n\nThere are two ways to use the above script:\n\nJupyter notebook, if used properly, can be a very high leverage tool that boosts the efficiency of team collaboration. Please check out Jupyter Notebook Best Practices for Data Science for more."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/kaggle-instacart-competition-b7177c3324dd?source=user_profile---------32----------------",
        "title": "[Review] Kaggle Instacart Competition \u2013 The Artificial Impostor \u2013",
        "text": "Now we come back to what brought me to this competition \u2014 figuring out how to learn the features automatically from neural networks, without having to hand-craft features myself.\n\nSean Vasquez did a fantastic job with his solution which relies purely on learnt features. The code was written superbly. However, it requires 64GB of RAM to run and I have only 16GB\u2026 Therefore I spent some time modifying my code to be closer to Sean\u2019s and you can find it on Github (WIP) :\n\nAs mentioned in the last section, the data is split by users and save as a single pickle file (basket.preprocessing.prepare_users):\n\nUSER_CHUNK is set to 1000. Opening data/users/ in the file manager (accidentally) might freeze the GUI if instead we store the files in the same folder. (Modern file systems support hundreds of thousands of files in the same folder, but some of the tools are not comfortable with that.)\n\nThe features are dynamically assembled and split into batches with a fixed size (basket.models.rnn_product.data_loader). The InstacartDataLoader class is based on PyTorch\u2019s DataLoader. The custom dataset of PyTorch is not applicable because we want to sample by user and have non-standard return values. One of the disadvantage is that rows from the same user is highly likely to be in the same batch, but I think this is a necessary trade-off.\n\nThe model combines 2-layer LSTM and 3-layer causal CNN with dilated convolution. Sean used 1-layer LSTM and 6-layer CNN with dilated convolution, but I find this structure more effective in my setting (only considers products appeared in the last 10 orders). In fact, I didn\u2019t find causal CNN provide any noticeable gain in performance. This part of the training (basket.models.rnn_product.model) should run smoothly with no less than 8GB of RAM.\n\nBTW, I found spotlight a really interesting project and it helps me understand how causal CNN (which is new to me) can be implemented in PyTorch:\n\nThe state of the final fully connected layer is extracted and feed into a LightGBM model. Currently I have only implemented one neural network model, and the performance of LightGBM model isn\u2019t much different from directly apply sigmoid function on the last layer. Perhaps LightGBM will perform much better with states from multiple models. But I\u2019ll have to reduce the size of the last layers or the states won\u2019t fit into memory (16GB RAM can handle ~ 80 features).\n\nThere\u2019s a lot more to be done. The current result is far from competitive. I might need to implement more ideas from Sean\u2019s solution. And Colin Morris also provided some interesting insights:\n\nHowever, I\u2019m a bit burnt out on this one and need to take a break. Hopefully I\u2019ll come back later and push this solution to at least around 0.4050 private score.\n\nI\u2019ve added a Bernoulli mixture model for product-level features. It\u2019s basically a LSTM + FC model structure, but uses the last FC layer to simultaneously create multiple predictions and weights to these predictions (each use half of the layer output). And the final prediction is a weighted sum of these predictions.\n\nI\u2019ve tried to combine it with the previous model and feed it to GBM meta model, but there\u2019s no significant improvement to the public nor the private score. Maybe I should try add some tabular features to the meta model."
    },
    {
        "url": "https://towardsdatascience.com/pytorch-implementation-of-perceptual-losses-for-real-time-style-transfer-8d608e2e9902?source=user_profile---------33----------------",
        "title": "Pytorch Implementation of Perceptual Losses for Real-Time Style Transfer",
        "text": "In this post I\u2019ll briefly go through my experience of coding and training real-time style transfer models in Pytorch. The work is heavily based on Abhishek Kadian\u2019s implementation, which works perfectly Fine. I\u2019ve made some modification both for fun and to be more familiar with Pytorch.\n\nThe model uses the method described in Perceptual Losses for Real-Time Style Transfer and Super-Resolution along with Instance Normalization. (Super-Resolution is not implemented)\n\nThree major parts I\u2019ve added to the implementation:\n\nFirst we need to take a quick look at the model structure. The main contribution of the paper is proposing that feeding forward the generated image to a pre-trained image classification model and extract the output from some intermediate layers to calculate losses would produce similar results of Gatys et al but with significantly less computational resources. So the first part of the structure is a \u201cImage Transform Net\u201d which generate new image from the input image. And the second part is simply a \u201cLoss Network\u201d, which is the feeding forward part.The weight of the loss network is fixed and will not be updated during training.\n\nAbhishek\u2019s implementation uses a traditional VGG model with BGR channel order and [-103.939, -116.779, -123.680] offsets to center channel means (it seems to also be what the paper used). The official pytorch pre-trained models use a unified format:\n\nHere\u2019s the code to extract outputs from the official pre-trained model:\n\nThere\u2019s no batch normalization in VGG model unless explicitly specified. So the values of activation is on a significant different scale from the previous implementation. Generally you\u2019d need to scale up the style loss (gram matrix) because most activation is less than 1, and taking dot products would make it smaller.\n\nThis helps a lot when tuning content weight against style weight ratio. You can stop training and re-tune the parameters during training, instead of having to wait 4 hours for the training to complete.\n\nThe paper mentioned this in the Experiment section, but it seems Abhishek didn\u2019t implement it:\n\nIt is quite easy to implement:\n\nPytorch autograd will handle backward propagation for you. In practice I haven\u2019t found out how to tune the regularization weight properly. So far the weights I used seemed not making much difference in the output images.\n\nThe model is trained using Microsoft COCO dataset. The image is resized to 256 x 256, and network is trained around 2 epochs with a batch size of 4 (the same as the paper). The training time using a GTX1070 is around 4 to 4.5 hours, on par with what the paper reports. Based on my rough experiment, a lot of computation time is used in normalizing the input images. The training might be faster if we use the original VGG model (not tested). The content weight v.s. style ratio is usually set to 1 : 10e3 ~ 10e5 after some manual tuning.\n\nBecause the network is fully convolutional, you could feed much bigger or smaller images than 256 x 256 to the network in test time. I write some script to transform animated GIFs and videos using scikit-video and ffmpeg for fun:\n\nOriginal video credit: Backpacking Around New Zealand. The processing speed is around 4 frames per second. I didn\u2019t align the frame rate, so the time in the following videos is slower than the original one.\n\nThe code is located at this Github repo. Since this is basically a personal fun project, the documentation is nonexistent and the main part of the code resides in two Jupyter notebooks: style-transfer.ipynb and Video.ipynb. Sorry about that."
    },
    {
        "url": "https://towardsdatascience.com/learning-note-single-shot-multibox-detector-with-pytorch-part-3-f0711caa65ad?source=user_profile---------34----------------",
        "title": "[Learning Note] Single Shot MultiBox Detector with Pytorch \u2014 Part 3",
        "text": "Every deep learning / neural network needs a differentiable objective function to learn from. After pairing ground truths and default boxes, and marking the remaining default boxes as background, we\u2019re ready to formulate the objective function of SSD:\n\nThere are two parts of this objective function:\n\nThis is a simple softmax loss function between the actual label and the predicted label. is 1 when there is a matching between the i-th default box and the j-th ground-truth of category p. Note there is a special category corresponding to background boxes (no ground truth is matched). The background boxes are treated as negative, and as we\u2019ll see later, are down-sampled to avoid an highly imbalance training dataset.\n\nThe localization loss is calculated only on positive boxes (ones with a matched ground truth). It calculates the difference between the correct and predicted offsets to center point coordinates, and the correct and predicted scales to the widths and heights. And smooth the absolute differences.\n\nThe differences of offsets and scales are normalized according to the widths and heights of the default boxes, and the scales are log-scaled before taking differences.\n\nIt\u2019s quite long, so let\u2019s break it down:\n\ncorrespond to the batch size. parameter is passed to to let know which row to write.\n\n(Setting requires_grad=False indicates that we do not need to compute gradients with respect to these variables during the backward pass.)[reference]\n\nRemember class label 0 correspond to background (negative box), we can use to find positive boxes. is expanded to (num, num_priors, 4) to be used to select the positive boxes. What does is to flatten the tensor from (num, num_priors, 4) to (num * num_priors, 4). comes from .\n\nFor confidence loss SSD use a technique called hard negative mining, that is, select the most difficult negative boxes (they have higher confidence loss) so negative to positive ratio is at most 3:1.\n\ncomes from layers/box_utils.py. It computes the denominator part of . computes the numerator part, where only the predicted probability to the true label matters.\n\nThe code use two sort to find the rank of each box. Firstly get the sorted index, then get the sorted index of sorted index as the rank. The is clamped by , which seems weird. The actual number of negative seems more reasonable.\n\nThis is part should be pretty straight forward. Collect the predictions and true labels, and then pass to cross_entropy function to get the overall loss (not averaged yet).\n\nFinally, average both losses and return them. Now we have everything we need to train the network.\n\nAfter training the network, it\u2019s time to put our detector to use. One particular problem from SSD design is that we can match multiple default boxes to a single ground truth box if the threshold is passed. Therefore when predicting we might predict multiple highly-overlapped boxes around an object. This is normally not the desired output of a object detection algorithm. We need to do a bit of post-processing.\n\nThe technique SSD use is called non-maximum suppression (nms). The basic idea is to iteratively add most confident boxes to the final output. If a candidate box highly overlaps (has a Jaccard overlap higher than 0.45) any box of the same class from the final output, the box is ignored. It also caps the total predicted boxes at 200 per image.\n\nThe implementation (nms function) is located at layers/box_utils.py. It has some new things like Tensor.numel and torch.index_select with out parameter. But you should be quite familiar with the work flow by now, so I won\u2019t analyze the code in detail here.\n\nThat\u2019s it! Thank you very much for read through the entire series. I haven\u2019t written such long learning note / tutorial for a while, and it feels great to do it again!\n\nThe purpose of this series is really to force myself to drill into the messy details and understand what\u2019s going on. I\u2019d be glad if it helps you in any way. Also please feel free to let me know if I got something wrong or missed something important."
    },
    {
        "url": "https://towardsdatascience.com/learning-note-single-shot-multibox-detector-with-pytorch-part-2-dd96bdf4f434?source=user_profile---------35----------------",
        "title": "[Learning Note] Single Shot MultiBox Detector with Pytorch \u2014 Part 2",
        "text": "In the previous post we discussed the network structure and the prediction scheme of SSD. Now we move on to combine default boxes and the ground truth, so the quality of the prediction can be determined (and be improved via training).\n\nParameters of default boxes for each feature map are pre-calculated and hard-coded in data/config.py:\n\nThe actual mapping happens in layers/functions/prior_box.py (P.S. default boxes are called prior boxed in the implementation):\n\nTake the first feature map (38x38) as an example. . and range from 0.5 to 37.5. So the center point coordinates cx and cy translate to (0.0133, 0.0133), (0.0133, 0.04) \u2026, (1, 0.9733), (1, 1). Note the code normalize the coordinates to (0, 1), and remember that most of the feature maps are zero padded (the outermost cells are always zero). You can verify yourself that the outermost center points in the second last (not padded) feature map is a bit away from 0 and 1. And the last feature map has only one center point located exactly at(0.5, 0.5).\n\nNow we have the center points of every default boxes. Next we want to calculate the widths and heights. There are six default box layouts:\n\nFor feature maps with 4 default boxes, only the first four layouts are used. The areas of rectangles are the same as the small square. This is different from the figure above, where the area seems to be the same as the large square.\n\nThe s_k\u2019s come from the following formula, with exception of the first feature map:\n\nThe default boxes are in fact designed empirically, as stated in the paper:\n\nSo you may modify prior_box.py freely to suit your needs.\n\nThis is called \u201cmatching strategy in the paper\u201d. The idea is really simple \u2014 Any pair of ground truth box and default box is considered a match if the their Jaccard overlap is larger than a threshold(0.5). In (hopefully) plain English, it\u2019s a match if the overlap area is larger than half of the area both of the boxes covered.\n\nThe relevant code is located at layers/box_utils.py:\n\nThe author calculates the intersection in a clever way. By expanding the tensors, we now are able to calculate the intersections of every combination of box_a(ground truth) and box_b(default boxes) in one run without any for loop.\n\nHere the author use the same trick to calculate the area of every boxes in one run and then get the union.\n\nRemember what we get from prior_box.py is in (cx, cy, w, h) format? Here we use to convert it into (xmin, ymin, xmax, ymax) format. The code is not posted to save space (find it here).\n\nThis part of the code might be the most confusing:\n\nThe tensor best_prior_idx contains the index of the best matched default box for each ground truth box. So what the first line of code does is to make sure every ground truth box has a least one default box that passed the threshold.\n\nThe for loop propagates the changes from the first line back to the tensor best_truth_idx, which contains the index of the best matched ground truth box for each default box. The effect of this loop is forcing the prior box give up the original best matching ground truth when there exists another ground truth that need it more (otherwise no default box for that ground truth).\n\nNote that we match each default box to exactly one ground truth, and assign a special label/class zero for all default boxes with maximum Jaccard overlap less than the threshold (thus background).\n\nThere is an function which transform the matched ground truth and default box pair into a format the loss function understands. The loss function will be discussed in the next post.\n\nWe discussed how to map default box to actual coordinates and how to match the ground truth boxes and default boxes. It took longer than I expected, so there\u2019ll be a part 3 discussing objective function and finally how to predict/detect in the test phase."
    },
    {
        "url": "https://towardsdatascience.com/learning-note-single-shot-multibox-detector-with-pytorch-part-1-38185e84bd79?source=user_profile---------36----------------",
        "title": "[Learning Note] Single Shot MultiBox Detector with Pytorch \u2014 Part 1",
        "text": "Admittedly, I have some trouble understanding some ideas in the paper. After reading the implementation and scratching my head for a while, I think I figured out at least some parts of them. So the following is my notes on some confusing concept after my first and second pass of reading.\n\nRecently I\u2019m trying to pick up Pytorch as well as some object detection deep learning algorithms. So to kill two birds with one stone, I decided to read the Single Shot MultiBox Detector paper along with one of the Pytorch implementation written by Max deGroot .\n\nFirstly, Single Shot MultiBox Detector (SSD) uses VGG-16 structure pre-trained on the ILSVRC CLS-LOC dataset, and add some extra convolution layers. The relevant code is located at ssd.py:\n\n\u2018M\u2019 means max pooling with kernel size 2 and stride 2. \u2018C\u2019 means the same max pooling but with , which does not appear in the original structure. My understanding is that deals with cases where input height or width is not divisible by 2, so there will be some cell coming from 1x2, 2x1, 1x1 max pooling in the output. Not sure why it\u2019s there, but shouldn\u2019t make much difference.\n\n\u2018S\u2019 means a and convolution layer, the number of filters comes next in the list (for example, the first \u2018S\u2019 has 512 filters).\n\nNote it adds a conv6 (1024 3x3 convolution filters with dilation=6 and padding=6) and a conv7 (1024 1x1 convolution filters) layer to the original VGG structure.\n\nThe construction of extra layers use a rotating 3x3 and 1x1 kernel size with optional \u2018S\u2019 flag indicated and as we already mentioned.\n\nWe\u2019ve covered the network structure. Now it\u2019s time to move on to actually predict/detect the class and location of the objects.\n\nA key concept of SSD is taking intermediate layers in the neural network as feature map. It then run 3x3 convolution filters on the feature map to classify and predict the offset to the default boxes (prior boxes in Python code). Each position has 4 or 6 corresponding default boxes. Naturally, default boxes in the lower layers are smaller because lower layers captures more fine details of the input images.\n\nFor each default box we predict:\n\nSSD use two layers from the VGG model \u2014 Conv4_3 and Conv7/FC7, which correspond to layer index 24 and -2 (that is, before relu activation). This way of getting layers is a bit shaky. If we decided to use use in VGG construction, the multibox construction would get the wrong layer. The same should go with extra layers as well, but in fact has not even been implemented yet in .\n\nFor extra layers, we use the second layer as feature map in every two layers. One weird part is that because final layer Conv11_2 has shape (256, 1, 1), so 3x3 convolution isn\u2019t really necessary. I guess it just for the simplicity of code structure.\n\nNote we should have + 4 (x, y, w, h) outputs per default box.\n\nTry to verify the number of default boxes in SSD300 (the one implemented).\n\nNote that this calculation includes default boxes from padded cells, which will always be zeros, thereby essentially useless boxes.\n\nAdditional exercise: calculate the number of valid default boxes in SSD300.\n\nWe still haven\u2019t discuss how to map those default boxes back to actual locations in the input images, how do we pick correct default boxes that matches the ground truth, and how to construct loss function to train the network. They will be addressed in the next post."
    },
    {
        "url": "https://medium.com/@ceshine/beijing-pm-2-5-concentration-chart-e03f41ca68b5?source=user_profile---------37----------------",
        "title": "Beijing PM 2.5 Concentration Chart \u2013 CeShine Lee \u2013",
        "text": "The data is downloaded from U.S. department of State. I think the observation point is located at the U.S. embassy in Beijing.\n\nI used Rmarkdown and dygraphs to make the chart. Because of the restrictions of Medium editor, the actual chart is published on RPubs. Please use the link if you want to access the interactive plot and source code.\n\nThe air quality in Beijing is far from ideal, but most locals seem not to care at all. Wearing mask in heavily polluted days attracts unwanted attention and some people actually ask you to remove the mask. Making this kind of data exploration and visualization is my way of telling myself that I\u2019m not insane\u2026"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/learning-notes-from-planet-competition-b764e2e458f5?source=user_profile---------38----------------",
        "title": "Learning Notes from Planet Competition \u2013 The Artificial Impostor \u2013",
        "text": "Full name of the competition: Planet: Understanding the Amazon from Space \u2014 Use satellite data to track the human footprint in the Amazon rain forest\n\nI started participating in this competition pretty late, essentially not until the last week. As a result I gave up using tiff images (instead using the pre-processed jpeg images), and focused on practicing building a image classification pipeline.\n\nIt was quite frustrating that my result keeps falling short of beating the best public scripts (I did get on par, and even a little better with the scripts in the end). The biggest problem I cannot figure out is when I use higher resolution (256x256 and 224x224) images, the increase in training time does not accompany with a increase in accuracy. In the end, my best submission comes from a model based on 128x128 images. It\u2019s weird because the pre-trained model I used (VGG16 and Resnet50) should work better with 224x224 images. My guess is that I didn\u2019t manage the learning rate well. I\u2019ll update the post if later I somehow figured it out.\n\nThe two highlights of the learning experiences are:\n\nI plan to update this post periodically until I finished reviewing this competition.\n\nUpdate on 2018/01/23: I\u2019ve found the cause of under-performance a while ago. It was a stupid bug which led to only a small portion of the parameters were updated at each step. Using JPEG files is actually enough to achieve top-level performance. I had also done some more experiments on this dataset and planed to write a independent post on the results, but I never find the time to write it."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/cxx11-is-not-defined-problem-in-mro-3-4-e51f1d27da15?source=user_profile---------39----------------",
        "title": "\u201cCXX11 is not defined\u201d problem in MRO 3.4 \u2013 The Artificial Impostor \u2013",
        "text": "Recently I tried to set up the R development environment on a fresh Linux system, every thing went well until I hit this error installing xgboost:\n\nAfter some Googling I found this Github issue. It appears to be a MRO-specific problem, and mjmg provided a working solution.\n\nThe gist is, replacing every \u201cCXX1X\u201d with \u201cCXX11\u201d in R compiler configuration files (System wide: Per-user: ) and fill in some values:"
    }
]