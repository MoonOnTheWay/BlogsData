[
    {
        "url": "https://medium.com/jungle-book/missing-data-filling-with-unsupervised-learning-b448964030d?source=---------0",
        "title": "Missing Data Filling with Unsupervised Learning \u2013 Jungle Book \u2013",
        "text": "We started by removing from the train and test sets all the features that have missing values. Using the remaining features, we fit the clustering method to the train set and predict the clusters for each sample in both sets. With the removed columns we calculate the mean (or mode if categorical) of each feature after being grouped by each cluster label. So now we have the means of the uncompleted features for each cluster.\n\nIt has 26 features with missing data and some also with a large ratio of NaNs (>90%).\n\nData set with 858 samples with 32 features and 4 target variables (different medical test binary outputs) which were converted to one target variable by taking the mode.\n\nIt has 93 columns with missing data, with some having a large ratio of NaNs (>90%).\n\nAfter merging the training data with data from the Russia\u2019s macro economy and financial sector, one gets 30471 samples with 389 features, one of them being the price to predict (regression problem).\n\nIt is relatively small, with 20560 samples and 7 features, one of them being the occupancy variable (binary classification problem).\n\nThis is a time-series data set with no missing values, therefore missing value imputation was performed artificially.\n\nAnd from gmm-mml . This package was based on the solution present in Unsupervised learning of finite mixture models . It suggests a solution which integrates estimation and model selection in a single algorithm.\n\nThe clusters produced by the K-Means algorithm are usually called \u201chard\u201d, since a sample either is or is not a member of a particular cluster. This variance of the algorithm is called \u201csoft\u201d or \u201cfuzzy\u201d, where samples have a degree of membership to each cluster. The update of the cluster centroids is done based on those memberships.\n\nMany of the most famous UL algorithms, such as Hierarchical clustering, K-Means, Gaussian mixture models or Hidden Markov models, will arrive at different answers for the same problem, and in my modest opinion there is no better or more correct way of finding structure generally (Really? The No Free Lunch Theorem again?).\n\nSo if not even humans would reach a consensus, about the structure of this data how can one teach a computer on how to make some sense of it? The problem here is that there is no universal definition of what a cluster is, or more generally what \u201cstructure\u201d is. One can look into a certain aspect of their daily lives and find it structured or not, though this will change according to the environment or the person involved.\n\nWavy hi: This one is harder. There is clearly structure here, but how do I teach a computer to extract it? To make you understand this problem better, imagine if I gather 1000 humans and ask them how many clusters they see in this picture. Probably the most common answer would be 2, though answers like 3, 4 or even 1 would emerge!\n\nBlobs : This is an easy one. Anyone who sees this image perceives that is composed of three distinct families (clusters). If you are fairly familiar with statistics, you probably suspect that three underlying Gaussian distributions originated it. When a new sample arrives, one would infer to which of the families it belongs, just by checking where it gets placed.\n\nThere\u2019s much untapped potential in Unsupervised Learning (UL). It is the the art of inferring a function to describe hidden structure from \u201cunlabeled\u201d data. But what does finding structure in data means in the first place? Lets consider the following two examples:\n\nYou probably clicked this post because you\u2019ve got some data set filled with NaNs and filling them with the mean values unsettles you or just because you want to learn more about it. In this post I will compare the most commonly used methods for missing data imputation, with others that take advantage of some clustering techniques.\n\nBy \u201cnormal filling\u201d I mean that each sample is imputed the mean/mode of the cluster it belongs to.\n\nThe weighted method uses the \u201cdegrees of belonging\u201d of a sample to each cluster. For instance in the GMM the degree are the likelihoods of a sample belonging to each cluster, and in the K-Means approach they are based on the distance between the sample and the centroids of the clusters.\n\nLittle feature engineering was done to the data sets besides normalization.\n\nFor the time-series ones, the time stamp was ordered and converted to the number of seconds, for the Occupancy Detection, and the number of days, for the Housing Market, that occurred since the first sample.\n\nAfter the imputation method is performed it is scored in the test set using XGBoost. Negative Log Loss and the Mean Squared Log Error were chosen as score metrics.\n\nInitially the \u201celbow\u201d or \u201cknee\u201d approach was considered. This approach consists in plotting the scores of the clustering methods for a range of number of clusters and look for the elbow.\n\nFor instance, in the previous plot the elbow is in the range of 8 to 12. The drawback is that this method needs human intervention for the choice of the elbow and an automatic way is desired. The efforts made to automatically choose the elbow were not successful therefore a new approach was considered.\n\nBy using cross validation a relatively effective though computational expensive method was obtained. How does it work? First pick a classifier, then for a range of number of centroids do the unsupervised imputation and evaluate it with a K-Fold cross validation using that classifier. Finally pick the number of centroids that performed better in the cross validation.\n\nIn the bar plots the red line sets the score obtained by mean imputation for easier comparison.\n\nAs previously said this data set has no missing data so artificial imputation was performed.\n\nThe features and samples for which the missing data is going to be imputed need to be chosen. The features are specifically chosen and for the samples a probability is set. So if the probability is 0.5 there is a 50% chance that a sample is discarded. Since the samples chosen to impute NaN values vary each time, we averaged the scores of three different runs and in the end the scores are averaged.\n\nFor this data set an idea was tested to study how to do clustering imputation on data sets so big that they do not fit in memory. Instead of using the whole data set for the imputation computations one should get randomly a number of samples capable of fitting in memory (for this test I used 5000).\n\nBy getting a random number of samples of the whole data set, time structure can still be slightly kept. The bigger the number of samples the more accurate it will be.\n\nBased on the results, filling methods built on clustering tend to perform better than the usual methods.\n\nFor the occupancy detection data set the best performing algorithm was the one implemented in the gmm-mml package, while for the housing market and cervical cancer data sets was the K-Means. Though in the housing market data set the gmm-mml one was not implemented, the reason why was the difficulty calculating the co-variance matrices with a large amount of features compared with the number of samples.\n\nWhen increasing the amount of missing data in the occupancy detection data set, it can be noticed that in the overall, the unsupervised based filling methods tend to perform better when compared to the mean filling. Thus, in features with high percentage of missing values, finding structure in data to help with the filling represents an advantage.\n\nOne can notice that as the number of features with missing data increases from 2 to 4 in the occupancy detection data set, and the number of features used for clustering decreases, the unsupervised based filling methods tend to perform a bit better when compared with the mean filling, which is counter-intuitive and can be a characteristic of this particular data set and the chosen features.\n\nAlso, as the percentage of missing data increases, the scores distantiate more from the baseline score, which is not surprising.\n\nBetween the three implementations of K-Means the vanilla one performs better then the others. It is also the one that does less computations per iteration, being then the best choice.\n\nMethods based on GMM tend to outperform the K-Means ones, which makes sense being K-Means an heuristic of GMM that works with euclidean distances. Euclidean distances are a good metric for low dimensions, but loose meaning for high dimensional spaces. For more info consult this link. GMM is based on the likelihood of samples belonging to the PDFs which is a better metric in higher dimensions.\n\nAlthough there is no obvious winner for which clustering based imputation algorithm should be chosen on top of the others, my advice is to try one based on GMM.\n\nFor the right number of mixtures is better to use the cross validation technique, though if it is too computationally expensive metrics like the akaike information criterion (aic) and bayesian information criterion (bic) can be used where one tests for a range of number of mixtures and chooses the one that minimizes the criterion.\n\nThere are different ways of calculating co-variance matrices. Here are the two most relevant:\n\nHaving a high number of features in a data set, one may not have not enough samples to correctly calculate the co-variance matrices for the GMM, or it can just take too much time when in \u201cfull\u201d mode, therefore the literature advises the use of diagonal co-variance, which gives a good compromise between quality and model size.\n\nIf the data set is too big to fit in memory, one should get a random sample of the train set to perform the clustering.\n\nMean imputation is not far behind the clustering based imputation methods in performance, therefore it is a method still to be considered.\n\nA new way of arranging the data set could be explored. Instead of discarding the features with missing data, one could fill them with the mean and mode values and then perform clustering on this modified data set. After each sample has been labeled, filling can be performed.\n\nMore criteria to get the correct number of clusters or mixtures can be studied. Methods like NEC and ICL are described has good options in this book (chapter 6).\n\nThere are still many unsupervised techniques to be studied and tested (for instance, Hierarchical Clustering with different types of distance metrics), tough a generally better one most probably does not exist, being each case one of its own (no free lunch theorem)."
    },
    {
        "url": "https://medium.com/jungle-book/automatic-feature-extraction-with-t-sne-62826ce09268?source=---------1",
        "title": "Automatic feature extraction with t-SNE \u2013 Jungle Book \u2013",
        "text": "A common problem data scientists face nowadays is dealing with very high-dimensional data (lots of features). Most of the algorithms for classification and prediction that work for low-dimensional datasets don\u2019t work as well when you have hundreds of features, a problem commonly referred as the curse of dimensionality. To solve this, we can reduce the amount of dimensions by feature selection or feature extraction, usually handpicking the most relevant features or using Principal Component Analysis (PCA) before feeding the data into our favorite model. Even though PCA is amazing in most scenarios, it still is a linear model, which might not be powerful enough to apply to some datasets.\n\nIn this article, we\u2019ll explore one non-linear alternative to PCA.\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE) [Original paper] is a very popular and state of the art dimensionality reduction technique that is usually used to map high dimensional data to 2 or 3 dimensions in order to visualize it. It does so by computing affinities between points, and trying to maintain these affinities in the new, low-dimensional space. A nice post to learn with examples and code about t-SNE can be found here.\n\nHere I denote X as a matrix containing the dataset, where each row is a sample and Y the matrix containing the representations in low-dimensional space (which we want to find). Similarity between 2 points in high-dimensional space is given by:\n\nThe affinity metric we use is a symmetric version of this equation, so that the affinity of A to B is the same as B to A, which is given by:\n\nFor the low dimensional space, t-SNE uses a student-t distribution instead, as it has an easier and cheaper to compute gradient, giving us the formula for affinities in low dimensional space (for d dimensions):\n\nSo now that we have the affinities for each pair of points, and we want to keep the low-dimensional ones as close as possible to the original ones, we need to have a loss function that measures the distances between those similarities. Since we defined the similarities as probabilities, a common loss function to use between probability distributions is the Kullback-Liebler divergence, defined as\n\nAnd our goal is to find Y (the low-dimensional embeddings) that minimize this loss function, for which we can use for example Stochastic Gradient Descent (SGD).\n\nLet\u2019s go over the algorithm step by step, illustrated by Figure 1.\n\nAfter stopping the algorithm, we end up with our Y as the embeddings of the whole dataset.\n\nThere\u2019s also a small step we are missing here to solve the problem of computing \u03c3 in Equation (1). Usually a perplexity level is provided by the user which allows us to compute this parameter, but I won\u2019t go into detail in this post. You can get a better insight about this parameter in this post which explains very well what it is and how to use it.\n\nThere are, however, some drawbacks related to t-SNE:\n\nThe main reason this work was done was to address these drawbacks.\n\nThe t-SNE algorithm finds those embeddings (Y) for all the dataset that was used, but what if we want to find the embedding of a new point after we have trained the model? Well, you can\u2019t with a vanilla t-SNE. In other words, originally t-SNE was not a parametric model, but the author created a model a few years later that bypasses this limitation. (See here)\n\nA way to create a parametric t-SNE is using a feed-forward neural network to map a high-dimensional point (x) to its low-dimensional embedding (y), and optimizing the t-SNE loss function with respect to the neural networks weights. An implementation in Keras can be found here.\n\nLet\u2019s go over the algorithm step by step, illustrated by Figure 2.\n\nAfter training, we end up with a neural network that takes as input a high-dimensional point (x) and outputs its embedding (y), which we can use on data that t-SNE has never seen before!\n\nA big limitation of t-SNE is the poor scalability of the matrices P and Q with the size of the dataset. If, for example, we have 1 million samples (which occupies some MB of memory) and we want to run t-SNE on it, we\u2019d end up with a matrix with 10\u00b9\u00b2 entries, which would occupy roughly 1 TB of memory!\n\nThere is an approximation called barnes-hut t-SNE (not parametric though) released by the same author. This version computes the (approximated) nearest neighbors for each point and then only computes the pairwise affinities for the nearest neighbors instead of the whole dataset and assumes the rest are zero. You no longer need a N\u00b2 matrix in memory (if you use a sparse representation), but you still need the whole dataset in memory as the algorithm doesn\u2019t work in batches.\n\nInstead we\u2019ll use a simple way around the problem which feels more natural with neural networks.\n\nWhat we\u2019ll do is to feed the neural network in batches and to simply compute P and Q (Eq. 2 and 3) only within each batch, that is, we\u2019ll compute the pairwise distances between points that are in the same batch. That reduces the complexity immensely and allows us to run t-SNE on datasets of virtually any size.\n\nTo demonstrate the power of this scalable t-SNE, we\u2019ll use a big dataset (> 1 million samples) from kaggle, the NYC taxi dataset.\n\nThe goal of this kaggle competition is to predict the trip duration of a taxi, given its pickup location, date, time of day, vendor and number of passengers. We have extracted some features from the data, such as day of week and month.\n\nLet\u2019s run t-SNE (removing the target column) and see what we find out.\n\nWe can clearly see 3 big clusters in the data, and 2 smaller ones in the right with significantly longer trip duration. What could they be? Let\u2019s color them by different features and see if we can find the structure uncovered by t-SNE."
    },
    {
        "url": "https://medium.com/jungle-book/towards-data-set-augmentation-with-gans-9dd64e9628e6?source=---------2",
        "title": "Towards data set augmentation with GANs \u2013 Jungle Book \u2013",
        "text": "Generative Adversarial Networks (GANs) have taken over the machine learning community by storm. Their elegant theoretical foundations and the great results that are continuously improved upon in the computer vision domain make them one of the most active topics of research in Machine Learning in recent years. In fact, Yann Lecun, director of Facebook AI Research said in 2016 that GANs, \u201cand the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion\u201d. To get an idea of how much this topic is being explored right now, visit this great blog post.\n\nAlthough they have been shown to work wonderfully as generative models for images, including pictures of faces and bedrooms, GANs haven\u2019t really been tested extensively on data sets such as the ones a factory would provide you, containing a large amount of measurements from sensors in a production line, for example. Such data sets may even contain time series information that our machine learning models must leverage to make predictions on future events \u2014 something which doesn\u2019t happen for static data such as pictures. Applying a generative model to these kinds of data can be useful, for example, if our predictive models need an even larger number of samples to train on to improve its generalization. Also, if we come up with a model that can generate high-quality synthetic data, then it surely must have learned the original data\u2019s underlying structure. And if it has, we can use that representation as a new feature set for our predictive models to exploit!\n\nIn this post I will describe some of the GAN architectures that may be useful for data set augmentation, both sample- and feature-wise. Let\u2019s start with the basic GAN.\n\nA GAN is a model made up of two entities: a generator and a discriminator. Here we will consider them both to always be parametrized neural networks: G and D, respectively. The discriminator\u2019s parameters are optimized to maximize the probability of correctly distinguishing real from fake data (from the generator) while the generator\u2019s goal is to maximize the probability of the discriminator failing to classify its fake samples as fake.\n\nThe generator network produces samples by taking an input vector z, sampled from what is called a latent distribution, and transforming it by applying the G function defined by the network, yielding G(z). The discriminator network receives alternating G(z) and x, a real data sample and outputs a probability of that input being real.\n\nWith proper hyperparameter tuning and enough training iterations, the generator and the discriminator will converge jointly (performing parameter updates via a gradient descent method) to a point where the distribution which described the fake data is the same as the distribution from which real data is sampled.\n\nIn the rest of this post the GANs workings will be illustrated on the MNIST dataset for generating new digits, or encoding the original ones on a latent space. We will also look at how they can be used on categorical and time series data.\n\nTo start off, here\u2019s a bunch of samples generated by a simple GAN whose neural networks are Multilayer Perceptrons (MLPs), trained on the MNIST dataset:\n\nAlthough the GAN as we\u2019ve seen it works, in practice it has some drawbacks whose solutions have been object of intensive research since the original 2014 paper by Ian Goodfellow et al. The major drawbacks have to do with the training of the GAN, which has become quite infamous for being extremely difficult: first, training a GAN is highly hyperparameter-dependent. Second, and most importantly, the loss functions (both the generator\u2019s and discriminator\u2019s) are not informative: while the generated samples may start to closely resemble the true data \u2014 approximating significantly its distribution \u2014 this behavior can\u2019t be indexed to a trend of the losses in general. This means that we can\u2019t just run a hyperparameter optimizer such as skopt using the losses and must instead iteratively tune them manually, which is a shame.\n\nThe other drawbacks of this GAN architecture have to do with functionality. The way it\u2019s shown in Figure 1 and using the original cross-entropy loss, we can\u2019t:\n\nGenerating categorical data is a particularly difficult problem for GANs. Ian Goodfellow explained it in a very intuitive way in this reddit post:\n\nThe key idea is that of the impossibility of the generator to go \u201call the way\u201d from one entity (eg, \u201cpenguin\u201d) to another (eg, \u201costrich\u201d). Because the space in between has 0 probability of occurring, the discriminator can easily tell that samples in that space are not real, and hence it can not be fooled by the generator.\n\nTo solve the issues associated with the original GAN, several other training approaches and architectures have been developed. In the following paragraphs, a brief description of each is presented. The goal of these descriptions is to get a sense of how these methods could be applied to structured data such as the one you would find in a Kaggle competition.\n\nPreviously, we\u2019ve set up a GAN to generate random digits that look like the ones from the MNIST data set. But what if we want to generate specific digits? To be able to tell our generator to generate any digit we want by command, only a very small change in training is needed. For each iteration, the generator takes as input not only z but also a one-hot encoded vector indicating the digit. The discriminator input consists then of not only the real or fake sample but also the same label vector.\n\nProceeding the same way as before but with this slight change of inputs, the Conditional GAN (CGAN) learns to generate samples conditioned on the label it takes as input.\n\nLet\u2019s then generate one sample of each digit! When sampling from the latent space, we also input a one-hot encoded vector indicating the class we want. Doing this for all 10 classes of digits yields the results in Figure 4:\n\nThe Wasserstein GAN (WGAN) is one of the most popular GANs and consists of an objective change which results in training stability, interpretability (correlation of the losses with sample quality) and the ability of generating categorical data. The key aspect is that the generator\u2019s objective is to approximate the true data distribution, and for that the choice of distance measure between distributions is important, as that is the objective to minimize. The WGAN chooses the Wasserstein (or Earth-Mover) distance \u2014 or rather, an approximation of it \u2014 as it can be shown that it converges for sets of distributions for which the Kullback-Leibler and Jensen-Shannon divergences don\u2019t. If you are interested in the theory, read the original paper or this excellent summary.\n\nImplementation-wise, the implications of approximating the Wasserstein distance can be summarized as follows:\n\nThe authors of the WGAN paper show that a GAN trained in this way exhibits training stability and interpretability, but only later was it proven that using the Wasserstein distance also provides the GAN with the ability of generating categorical data (i.e., not continuous-valued data like images or even integer-coded data like 1 for Sunday, 2 for Monday and so on). While if the original GAN was trained on this kind of data, the discriminator\u2019s loss would remain low throughout iterations while the generator\u2019s wouldn\u2019t stop increasing, training a WGAN on categorical data is done the same way as on continuous-valued data.\n\nAll one needs to do is this (see Figure 5 for an example): for each categorical variable in a data set, have a corresponding softmax output in the generator network of dimensionality equal to the number of possible discrete values. Instead of one-hot encoding the softmax output and using that as input to the discriminator, use the raw softmax output as if it was a set of continuous-valued variables. In this way, training will converge! At test time, to generate fake categorical data, just one-hot encode the generator\u2019s discrete outputs and there you go!\n\nAs an example of training a WGAN with gradient penalty on a data set with categorical values, in Figure 6 you can see the beautiful stable, converging loss functions you get when training it on the Sberbank Russian Housing Market data set from a Kaggle competition, which contains both continuous and categorical variables.\n\nOf course, we may also combine the WGAN with the CGAN to train the WGAN in supervised fashion to generate samples conditioned on class labels!\n\nNote: a further improvement on the Wasserstein GAN is the Cramer GAN, which aims at providing even better-quality samples and improved training stability. Inspecting its possibility of generating categorical data is a topic for future research.\n\nAlthough the WGAN seems to solve quite a lot of our problems, it doesn\u2019t allow the access to latent space representations of the data. Finding these representations may be useful not only for controlling what data to generate by moving continuously in the latent space, but also for feature extraction.\n\nThe Bidirectional GAN (BiGAN) is an attempt at solving this issue. It works by learning not only a generative network but also, at the same time, an encoder network E which maps the data to the generator\u2019s latent space. This is done also in an adversarial setting using only one discriminator for both the generating and encoding tasks. The authors of the BiGAN show that, in the limit, the pair G and E yield an autoencoder: encoding a data sample via E and decoding it via G yields the original sample.\n\nWe saw earlier that the CGAN allows the conditioning of the generator to generate samples according to their labels. But would it be possible to learn to distinguish digits in a fully unsupervised manner by simply forcing a categorical structure in the GAN\u2019s latent space? What about setting also a continuous code space that we can access to describe continuous semantic variations in data samples (in the case of MNIST, things like digit width or tilt)?\n\nThe answer to both questions is yes. Better than that: we can do both things simultaneously! Truth is, we can impose any set of code space distributions that we find useful and train the GAN to encode meaningful traits in those distributions. Each code would learn to contain a different semantic trait of the data, resulting in effective information disentanglement.\n\nThe GAN that allows such a thing is the InfoGAN. Simply put, the InfoGAN tries to maximize the mutual information between the generator\u2019s input code space and an inference net\u2019s output. The inference net can be set to simply be an output layer on the discriminator network, sharing all the other parameters, meaning it\u2019s computationally free. Once trained, the InfoGAN\u2019s discriminator inference output layer can be used for feature extraction or, if the code space contains label information, for classification!\n\nCreating an InfoGAN with two code spaces \u2014 one continuous of dimension 2 and one discrete of dimension 10 \u2014 we can generate data conditioned on the discrete code to generate specific digits, and on the continuous code to generate specifically-styled digits, as seen in Figure 9. Note that no labels were harmed in this entirely unsupervised learning scheme \u2014 imposing a categorical distribution in the latent space suffices to make the model learn to encode label information in that distribution!\n\nThe Adversarial Autoencoder (AAE) is where autoencoders meet GANs. In this model, two objectives are optimized: the first, the minimization of the reconstruction error of the data x through the encoder and decoder networks, P and Q, respectively. The second training criterion is the enforcement of a prior distribution on the code P(x), via adversarial training where the generator corresponds to P. So while P and Q are optimized to minimize the distance between x and Q(z), where z is the code space vector of the autoencoder, P and D are optimized as a GAN to force the code space P(x) to match a pre-defined structure. This can be seen as a regularization on the autoencoder, forcing it to learn a meaningful, structured and cohesive code space (as opposed to fractured \u2014 see page 76 of these lecture notes by Geoffrey Hinton) that allows for effective feature extraction or dimensionality reduction. Also, because a known prior is imposed on the code vector, sampling from such prior and passing the samples through Q, the decoder network, configures a generative modelling scheme!\n\nLet us then impose a 2D Gaussian distribution with standard deviation 5 on the code space via adversarial training on the autoencoder. Sampling from neighboring points in this space, a continuous variation of the generated digits is observed!\n\nAnother thing we can do is train the AAE with labels to force the disentanglement of label and digit style information. This way, by fixing the desired label, variations in the imposed continuous latent space will result in different styles of the same digit. For digit number eight for example:\n\nClearly, there is a meaningful relationship between neighboring points! This property may come in handy when generating samples for our data set augmentation problem.\n\nOften, real-world structured data consists of time series. This is data in which each sample has some dependence on the previous sample. For this type of data, using Recurrent Neural Network (RNN)-based models are often chosen for their intrinsic ability of modelling it. Leveraging these neural networks in our GAN models could, in principle, result in higher-quality samples and features!\n\nLet us then replace the MLPs we used before in our GANs by RNNs as proposed here. In particular, let\u2019s make those RNNs Long Short-Term Memory (LSTM) units (we really are dealing with the buzziest of the buzz words of Deep Learning \u2014 oops, I did it again) and try what I very originally called the Waves dataset. This dataset contains 1-D sinusoidal and sawtooth signals of different offsets, frequencies and amplitudes, all with the same number of time steps. From the RNN\u2019s point of view, each sample consists of one wave with 30 time steps.\n\nLet us then run our CGAN with both generator and discriminator networks as LSTM-based neural networks, turning it into an RCGAN that will be trained to learn to generate sinusoidal and sawtooth waves on demand:\n\nAfter training, we may also inspect how variations in the latent space yield a continuous variation in the characteristics of the generated samples. In particular, if we impose a 2D normally distributed latent space and fix the class label to sinusoidal waves we get the samples shown in Figure 14. There, a clear continuous variation between low and high frequencies and amplitudes is observed, meaning that the RCGAN learned a meaningful latent space!\n\nWhile using RNNs in GANs is useful for real-valued sequential data generation, it still doesn\u2019t work for discrete sequences, and using the Wasserstein distance using RNNs is not yet a clear option (enforcing the Lipschitz constraint on an RNN is a topic for further research). Some ideas to note that aim at solving this issue are SeqGAN and more recently the ARAE .\n\nWe have seen that aside from all the fuss being generated (get it?) around GANs\u2019 ability to generate really cool pictures, some architectures may also be useful for more general machine learning problems containing continuous and discrete-valued data. This post served as an introduction to that idea and was not intended to be a hard comparison between multi-purpose generative models, but it does prove that such a study involving GANs is bound to be done.\n\nNote: the work shown here was developed in its entirety during a summer internship I took at jungle.ai."
    }
]