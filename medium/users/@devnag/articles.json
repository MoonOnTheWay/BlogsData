[
    {
        "url": "https://medium.com/@devnag/what-separates-us-from-ai-part-4-priors-if-youre-not-cheating-you-re-not-trying-9a7396e4ed5c?source=user_profile---------1----------------",
        "title": "What separates us from AI, Part 4: Priors (if you\u2019re not cheating, you\u2019re not trying)",
        "text": "Let\u2019s quickly review what we\u2019ve been through.\n\nMy assertion in this post is that our ability to \u2018beat\u2019 machine learning in terms of data-efficiency is entirely the result of our ability to encode priors at multiple levels of this hierarchy. In other words, we\u2019re better at taking the test because \u2014 in some deeper sense \u2014 we\u2019ve already seen the answers.\n\nThe role of meta-modeling? Building the higher rungs on the ladder gives us a broader canvas on which to paint our priors. Priors at the lower levels, by themselves, just aren\u2019t enough.\n\nSo what\u2019s a prior? In mathematical terms, it\u2019s just a \u2018starting\u2019 probability distribution that precedes the processing of a given dataset, something of an initial guess (though you can get into trouble trying to anthropomorphize an equation, as I am doing here). Priors are preferences for some regions of a space over another. As you might have guessed, there are priors at every levels of the hierarchy, from likely datasets to likely functions and operators. Priors at the higher levels are needed to constrain the search process at lower levels, helping to make that search more efficient.\n\nIndeed, Lin and Tegmark argue convincingly that the widespread success of deep learning is because it happens to approximate the \u201cpriors of the universe\u201d, the particular Hamiltonians which generate the physical data we see and experience all around us.\n\nInterestingly, the kind of data that\u2019s especially troublesome for machine learning is human-generated data \u2014 human speech, human language, human behavioral prediction. In other words, data that was constrained by our priors in the first place! It\u2019s pretty intuitive that we would have the biggest advantage over machines here.\n\nThere are at least two ways in which humans receive priors \u2014 evolution and cultural. We either have priors because the ones hard-coded into our genetic lineage thrived & survived, or because we were directly gifted them by others during our lives (in education, media, interpretive dance, whatever). Of course, we then go on to develop lots of other models through experience, which are posteriors at the time but become priors for future data.\n\nThe idea of evolutionary priors is not at all new. Noam Chomsky famously suggested the Language Acquisition Device (LAD) to explain the poverty of stimulus problem \u2014 why humans children were so efficient with linguistic ramp-up given that languages are underdetermined by the available data (in other words, efficient search!). This LAD is itself a proposed prior. Interpreted one way, it\u2019s at least a prior on F0, over the distribution of valid sentences \u2014 though it might also be a prior on F1 or F2, depending on your perspective.\n\nIndeed, \u201cNature vs. Nurture\u201d might be considered the battle of evolutionary priors vs. cultural priors.\n\nThis is pretty much the only way we can beat machines. We can\u2019t match them on the volume of data perceived. We can\u2019t beat them on the speed of data processed. We can\u2019t generate, search, or evaluate models as quickly as they can. We certainly can\u2019t benefit from advances in materials science and electrical engineering to upgrade our thinking velocity year after year. The one advantage we have, and that seems to explain the performance gap where it exists, is our set of priors.\n\nIt\u2019s also the one thing we have been extremely bad at encoding into machine learning software.\n\nThis is the missing explanation for why the \u2018universal\u2019 approaches (AIXI-mc, AIXI-tl, etc) don\u2019t work as well as advertised \u2014 because they\u2019re made to be optimal across all possible data sets (weighed by an idealized information-theoretic prior), not tuned and optimized for the special subset of actual data sets we see in reality. F\u03a9 is itself a human prior, guiding us towards higher-level operators that are more promising than they should be, given the unthinkable dimensionality of the search space.\n\nWe\u2019re not consciously aware of our priors \u2014 sometimes we act on the basis of them without knowing it, and sometimes we tell stories and rationalize over them, masking them forever.\n\nIn that sense, I\u2019m sympathetic to Gary Marcus when he states that we need \u201ctop-down models\u201d to complement the bottom-up approach that has seemingly hit a wall. But as I\u2019ve tried to argue in this series of posts, the problem isn\u2019t that deep learning systems don\u2019t have higher-level models. The problem is that they don\u2019t know which models to use.\n\nThat\u2019s up to us."
    },
    {
        "url": "https://medium.com/@devnag/what-separates-us-from-ai-part-3-get-meta-recursive-automation-not-automated-recursion-8005e089ed89?source=user_profile---------2----------------",
        "title": "What separates us from AI, Part 3: Get Meta (recursive automation, not automated recursion)",
        "text": "See Part 1 and Part 2 here.\n\nIn the first post of this series, I hinted that something (at level F4) was creating those F3 meta-learners. What is it?\n\nIt\u2019s us! We are F4. In fact, we\u2019re also F3, and F2, and F1. We\u2019re a versatile lot \u2014 we can play any of these roles.\n\nWhen we do arithmetic in our heads or recognize our friends in photos, we\u2019re playing the role of F1. When we create an Excel spreadsheet or a python program that does some simple math, we\u2019re playing the role of F2, creating a software F1. And when we implement an architecture and optimization routine for some model in Tensorflow or Pytorch, we\u2019re playing the role of F3, creating a software F2.\n\nThe machine learning hierarchy was built by us, bottom-up, slowly replacing and automating a role that we ourselves were playing. We first started with F0 data, and implemented F1 \u2018functions\u2019 by writing software that took input and generated output.\n\nThe bulk of machine learning was taking this one level higher \u2014 instead of us writing the F1 functions by hand, we wrote F2 learners by hand and generated F1 functions through some optimization process. Then, in a few cases, instead of writing F2 learners by hand, we implemented F3 meta-learners by hand that acted on F2 learners, which would eventually generate F1 functions. In that final step, we\u2019re playing the role of F4, birthing F3.\n\nThis didn\u2019t happen overnight, and in no sense is that process \u2018complete\u2019. We still write F1 functions by hand, along with the higher operators. But this is one of key differences between humans and modern ML/AI \u2014 we\u2019re capable of going higher and automating ourselves out of a role.\n\nWe\u2019re capable, in other words, of meta-modeling, of going from F0 to F1, then F1 to F2, then F2 to F3, and so on. Let\u2019s give this meta-operator a special name\u2026F\u03a9.\n\nF\u03a9 actually isn\u2019t a horizontal level of the hierarchy at all, like the others, but an operator that sits vertically within it. And the practice of machine learning is us implementing the F\u03a9 operator over and over again (whether we go beyond the top layer, or search more broadly in an existing layer from the layer below it) to find interesting/useful generators.\n\nAnd as far we we know, F\u03a9 doesn\u2019t exist in software, only within ourselves. There\u2019s no software that churns out interesting ensemble methods\u2026as far as we know. But human researchers, as a group, do that all day long.\n\nSo what\u2019s the significance? What could F\u03a9 have to do with efficient search?"
    },
    {
        "url": "https://medium.com/@devnag/what-separates-us-from-ai-part-2-efficient-search-is-the-real-goal-of-ai-ml-721dce83b26b?source=user_profile---------3----------------",
        "title": "What separates us from AI, Part 2: Efficient Search is the Real Goal of AI/ML",
        "text": "In Part 1, we noted in passing that the F1 models are the \u2018final output\u2019 of most machine learning endeavors (however you get there). These are the tightly-guarded jewels that we spend days/weeks/months burning out our GPUs to create. F2 and F3 are means to that particular end.\n\nBut here\u2019s the thing. We already know how to generate every possible F1 model there is.\n\nAll F1 models are computable functions \u2014 whether it\u2019s a simple decision tree or a convolutional neural network with gigabytes of parameters. Which means we can actually define all of the potential models in a nice, ordered (countably infinite) list.\n\nSo the problem is not that we can\u2019t generate the right F1 model. And the problem isn\u2019t that we can\u2019t find them, theoretically \u2014 the list can be ordered lexicographically, so we can just go up the list and we will reach the right model, eventually.\n\nThe problem is that we can\u2019t find the right model efficiently.\n\nThere have been many attempts at universal approaches to this problem, essentially F2 learners that will attempt to find the \u2018best\u2019 F1 model, given whatever criterion, efficiently given an F1 dataset:\n\nThe thing is \u2014 all of these approaches have been implemented in software, and tried on real data, and none of them have lived up to their theoretical promise. They just don\u2019t work that well on realistically-sized data sets. In particular, they don\u2019t seem to learn as well as human beings on a variety of tasks, even when they have orders of magnitude more data than humans could ever see.\n\nThere\u2019s the rub. There\u2019s no fundamental law of math or physics that blocks us from learning a language fluently by processing a few hundred million words \u2014 we humans are the existence proof that it\u2019s possible, even commonplace. Yet no software has ever been able to do that.\n\nSo what\u2019s the difference? What do humans do that AI/ML doesn\u2019t? Why are we more efficient on certain data sets?"
    },
    {
        "url": "https://medium.com/@devnag/what-separates-us-from-ai-part-1-the-periodic-table-of-machine-learning-508b237624c3?source=user_profile---------4----------------",
        "title": "What separates us from AI, Part 1: The Periodic Table of Machine Learning",
        "text": "AI is one of those fields that see-saws back and forth in the public consciousness (sometimes wired, sometimes tired) yet seems to reach both higher highs and lower lows. We\u2019re currently in the middle of a very high high, indeed, with the resulting backlash that signifies something large and threatening (at least memetically) to, er, lash back against.\n\nBut, for all of the drama and hand-wringing about Skynet inventing its own language or machines beating humans at increasingly complicated games, there\u2019s a lack of perspective on what the recent advances really mean and what larger picture they fit into.\n\nIn this series of posts, I want to convince you of four things:\n\nWe\u2019ll denote the hierarchy of operators, or higher-order functions, by \u2018F\u2019 for each level of functions. F0 is the bottom level, F1 is the next one up, and so forth. Let\u2019s get to it!\n\nF0: In the beginning, there were data types. Floating-point, integers, characters, and all the tensors that combined them. We didn\u2019t discover them; rather, we invented them in our software language design. Let\u2019s call the set of all such primitive types F0, denoting the bottom-most layer of this hierarchy. Every vector, every text string, and every number is represented in this class.\n\nF1: Now, let\u2019s go up one level. Define F1 as the set of all functions that act on F0 (for input or output). Maybe an element of F1 maps integers to integers, or infinite series of characters to floating-point values, or whatever. In the following sections, I\u2019ll be using the notation \u201cC: [A \u21e8 B]\u201d to indicate that C is a function which maps elements of space A to elements of space B; or in the diagram above, to indicate that C is the arrow that starts at A and ends at B. I\u2019ll also append a lowercase letter to describe subclasses of each level \u2014 for example, \u201cF1d\u201d is a particular subclass within F1.\n\nSo, what are some interesting subclasses of F1?\n\nNote that I\u2019m putting data and models in the same F1 class! In other words, they both map F0 types to other F0 types; they only differ in how complete those mappings are. This should make some intuitive sense \u2014 there\u2019s a sense in which y = x*x (a model) and [(1,1),(2,4),(3,9)] (a data set) live in the same world; the former is just more complete than the latter. It\u2019s defined on every real x, while the latter is only defined on a tiny subset of the natural numbers. An F1 dataset is a plausible sampling from an F1 function, hence their living in the same plane.\n\nNote that I haven\u2019t said anything about where these elements of F1 come from. In the case of F1 data sets, they come from \u2018the world\u2019 in some fashion, while F1 models and data transformations are usually the optimized output of some algorithm. So let\u2019s go up a level and talk about those algorithms directly.\n\nF2: F2 is the set of all operators that act on F1 functions (or lower, meaning F0 \u222a F1 here). They typically take some F1 as input and return a different F1 as output. And just like F1, this set includes a bunch of interesting subclasses:\n\nNote that iterative optimization itself (whether gradient descent or anything else) is typically an F2 operator, but with a somewhat different domain. It\u2019s actually a function of two arguments; a starting F1m model and an F1d dataset, resulting in a different F1m model. In other words, optimization maps [(F1m,F1d) \u21e8 F1m] \u2014 and if you curry out the initial F1m argument as an architectural choice, you get a standard F2l learner, which maps the remaining F1d dataset to the final F1m model.\n\nF0, F1, F2\u2026how far we can we take this?\n\nF3: F3 \u2014 operators which act on F2 or lower levels \u2014 is where things really explode outwards. There are a dizzying number of different signatures here. Let\u2019s go through just a few examples:\n\nWhew. What\u2019s at level F4 \u2014 in other words, what generates these F3 meta-learners? I\u2019ll get to that later (cliffhanger!). The core point here is that pretty much everything in modern machine learning fits into this hierarchy. Which implies two things:\n\nIndeed, if we define nested sets from these layers (M0 = F0, M1 = {F0 \u222a F1}, M2 = {F0 \u222a F1 \u222a F2}, \u2026) then we get a sequence of model languages, each with more power than the previous one. M1 only allows us to talk about data types, data sets, and functions; M2 allows us to talk about those as well as learners and higher data/function transforms, and so forth. Imagine this as roughly mapping to an increasingly expressive sequence of human languages where you can first only use nouns; then use nouns and adjectives; then nouns, adjectives, and verbs; then nouns, adjectives, verbs, and adverbs; and so on.\n\nSo this hierarchy of operators (which is indeed a category, dear reader) seems to cover actual ML techniques pretty well. But is this structure \u201cuniversal\u201d in any sense? What\u2019s the bigger problem we\u2019re trying to solve?"
    },
    {
        "url": "https://medium.com/@devnag/sex-death-and-anonymity-50dd783df498?source=user_profile---------5----------------",
        "title": "Sex, Death, and Anonymity \u2013 Dev Nag \u2013",
        "text": "The price of sex, it turns out, is death.\n\nFour billion years ago, the earliest life forms were born immortal and capable of reproducing alone, without any mating partners. Asexual bacteria, if given infinite food and space, could theoretically continue to reproduce and live forever; they would \u201ccontinue clonal expansion through [their] progeny indefinitely\u2026.we would likely never find a single dead cell in such a culture.\u201d [1]. Asexual bacteria do not gradually get older, become creaky, or die of natural causes; they do not senesce. When these organisms had the planet to themselves, the end of life was either predation or an accident.\n\nTwo billion years later, eukaryotes evolved when one bacteria was subsumed into and subjugated by another, resulting in a more metabolically complex organism. About a billion years after that, during the Stenian period, sexual reproduction between two cells arose for the first time [2]; for these lucky eukaryotes, their offspring would be genetically different from them (and from their mating partner), allowing their species to evolve and adapt to changing environments faster than their more chaste competition could. With sexual reproduction came the first multicellular life, organisms capable of differentiating their cells into specialized functions, allowing them to scale up in both size and capability. All plant and animal life today is descended from these sexual pioneers.\n\nA germ cell is the cell of an organism that is, or eventually becomes, a mating cell (such as a sperm or an egg). In sexually-reproducing, multicellular life, with their tightly scripted division of labor, there was for the first time a break between germ cells (for which all single-celled life functionally qualifies) and non-germ cells, which are known as somatic cells (from the Greek \u03c3\u03c9\u03bc\u03b1, meaning \u2018body\u2019). In single- celled life, the cell is always a germ cell [3], whether sexual or asexual. In multi-cellular life, such as ourselves, there is a clear partition between the crown jewels of genetic information (the germ cells) and the support system of somatic cells which ferries the germ cells around, feeds them, finds them a suitable mate, and defends them.\n\nAlmost everything that you think makes you you is composed of somatic cells \u2014 your brain, your heart, your face, your hands. The only germ cells in your body are the 50,000 egg cells that every woman is born with (most of which die off; only 400 will eventually be ovulated) and the millions of spermatogonia that every man is born with (which eventually differentiate into individual sperm cells).\n\nIn sexually reproducing organisms, germ cells have the chance to pass their DNA forward a generation, masking mutation damage and enjoying novel enhancements. That DNA, rejuvenated, becomes part of the tree of life. But what shall happen to the somatic cells? Over time, their DNA has been continually compromised by damage, and will never be enhanced or protected through mating, leaving them especially risky (through increased susceptibility to cancer) with no real return. To make way for their freshly-generated germ-line descendants, and through a poorly-understood process involving chromosomal telomeres, they are destined to die.\n\nDeath was never the natural partner of life, but of sex. Once we were able to create truly novel life, through genetically distinct children, we sacrificed our own.\n\nSomething very similar is true for ideas. Cats and dogs, for example, do not expect to have different ideas than their parents. They don\u2019t expect to move the culture of their species forward. The same impulses that propelled a dog a thousand years ago propel it today \u2014 whatever ideas it has that motivate its behavior have been around for almost as long as the species. But once ideas were able to mate and recombine and evolve, as they do for us, they also became mortal. When an idea can be improved, its original form is no longer necessary. Evolution implies obsolescence \u2014 on Quora or anywhere else.\n\n\u201cYour children are not your children.\n\n They are the sons and daughters of Life\u2019s longing for itself. They come through you but not from you,\n\n And though they are with you yet they belong not to you.\n\nYou may give them your love but not your thoughts, For they have their own thoughts.\n\n You may house their bodies but not their souls,\n\n For their souls dwell in the house of tomorrow, which you cannot visit, not even in your dreams.\u201d\n\nChildren, paradoxically, teach us first to grasp tighter, and then to let go, as we never have before. They teach you both closeness and distance like no romantic relationship can. At one level, children represent our most intense form of self-love, and so we naturally imagine that we have far more control and influence over them than we really do. When you watch a child discover a new part of their world, and share in their delight, you realize with a twinge that they (in all hopeful likelihood) will see and experience the world that is post-you.\n\nWhen you look at them, you see a flash of your own immortality; yet when they return your gaze, they see mortality \u2014 a part of the world that will cease to be, even as they move onward. They are your scouts into the house of tomorrow; yet they will have no one to report back to.\n\nYour words, likewise, take on a life beyond your uttering them. In a very deep sense, they come through you but not from you. We are each the product of an incredibly rich and extended culture, one that has enjoyed thousands of years of externalized, compounded knowledge (through pictures and writing and every other form of media). That culture shapes the very way in which you experience it. In interpreting culture, you have applied your own peculiar set of filters and distortions and cognitive biases, implicitly, unknowingly; as Ruth Benedict said, \u201cwe do not see the lens through which we look.\u201d Your own words will also be taken and interpreted in myriad different ways, if they find an audience at all.\n\nIndeed, if you have never been shocked at how badly your written or verbal communication has been misinterpreted, then you have never said anything interesting. At the frontier, we are all poking at a membrane that we don\u2019t fully understand. And the heart of Quora is almost entirely at that frontier. It is a crackling, ferocious place, one ripe with conflict and passion. It is an extended game of Telephone, one where a crowd builds on each idea with mating and mutation playing their unappreciated roles. Just as every generation believes that they discovered sex anew, we all believe that we discovered Quora and the ideas that play continually on its surface.\n\nThe Quora lifecycle has become so stereotyped as to border on hilarity. A new user shows up, gets hooked, and posts ceaselessly for weeks with bright-eyed ebullience and an almost child-like deference to more senior users and their frequently gruff ways. The timestamps of their posts may as well be a random number generator, and it\u2019s not clear whether they have found a way to transcend sleep. They happily complain of addiction as if it were a badge of honor. If they blog externally, they will trumpet their finding to the world in revelatory tones (neatly implying that they are either the tipping point or a prophet).\n\nThen, the initial passion slows to a trickle. Their posting volume declines, and when they do post, their answers take on a drastically less idealistic tone. They start to nitpick at small issues \u2014 the way that comments are laid out, or the way that the feed updates in real-time. Briefly, they act out, asking strangely provocative questions that reflect a passive-aggressive testing of the community\u2019s mores. With time, they start to ask leading meta-questions like \u201cWhat\u2019s wrong with Quora?\u201d and \u201cWhen did Quora jump the shark?\u201d that reflect their own inner storms. Unable to imagine a Quora before them, or after them, they reveal their intrinsic and mistaken belief in their own immortality; their experience of Quora must be the canonical experience of Quora. With a literary flourish, they vociferously announce that they are done with Quora, and are leaving for good.\n\nTwo months later, they are back \u2014 without even the courtesy of sheepishness. As in any off-and-on relationship, the current moment fills the whole universe.\n\nYou are not your college, job, bank account, car, house, or clothes. And you are not your Quora answers.\n\nWhen I was a child, I was fascinated with the philosophical problem of solipsism, for the way it cleaved the logic of skepticism from my complacent certainty in emotional intuition. I know that I\u2019m conscious\u2026 but how do I really know that anyone else is? How could I prove it? Sure, people seemed conscious, but all I saw was the outside, and inferred that their subjective \u2018inside\u2019 matched mine in some way. That process of social induction is instinctive, and did not start with humans.\n\nBut we over-apply it on Quora at our own peril. Just because you would have to be an incredibly callous and unfeeling person to write a given set of words does not imply that anyone who writes them is the same way; perhaps they grew up in a family with a more confrontational style than yours, or were punished for being indirect and passive. And just because you would have to be brilliant to write something does not make the other person so; they may simply be parroting something they read elsewhere. We are higher-dimensional than any written language can allow, more unknowable than we can imagine.\n\nSeeing that gap between the writer and the written gives you the freedom to let your own words go. No answer I have ever written on Quora, or anywhere else, has really exhausted my passion for or interest in or contributions to the subject at hand.\n\nFurthermore, your attachment to your answer \u2014 the notion that your words are Settled Doctrine and Uniquely Yours \u2014 not only stagnates the idea pool, but actually makes you less satisfied with your creation. When your answer has no ancestors, and no descendants, it has become (by definition) a dead end. Facing mortality (of our culture or of ourselves), and accepting our role in a process that extends before and beyond us, actually makes us freer. You can trade one kind of immortality for another; it is a sad bargain for some, a liberating one for others.\n\nSometimes materialism arises from a physical insecurity, the feeling that one must amass buffers and barriers against a hostile and fickle world. But much of the time, it is the manifestation of a desire for social validation, for status, for recognition.\n\nOccasionally, a new Quora user will start dropping names or hint at wealth or make some other reference to their perceived superiority, as if their answer were an oversized codpiece. They will mention details from a lunch date with Bill Gates or smugly reveal their \u2018mastery of women\u2019, or some such nonsense. The Quora community is simultaneously fascinated with and repulsed by this sort of validation-seeking behavior, for it confirms their secret fears about how the world really works (that it is mostly run by pompous bullshit artists) while conveniently giving them a concrete, accessible target.\n\nThey needn\u2019t try so hard. As the writer David Foster Wallace said in 2005, with his typically casual eloquence:\n\n\u201cThe compelling reason for maybe choosing some sort of god or spiritual-type thing to worship\u2026is that pretty much anything else you worship will eat you alive. If you worship money and things, if they are where you tap real meaning in life, then you will never have enough, never feel you have enough. Worship your body and beauty and sexual allure and you will always feel ugly. And when time and age start showing, you will die a million deaths before they finally grieve you. Worship power, you will end up feeling weak and afraid, and you will need ever more power over others to numb you to your own fear. Worship your intellect, being seen as smart, you will end up feeling stupid, a fraud, always on the verge of being found out.\u201d\n\nHe spoke from experience; three years later he would hang himself, an unfinished manuscript just beneath his dangling feet, feeling that he had never lived up to his own potential and was shamefully undeserving of adulation. Seeking validation causes a sort of emptiness; having it and feeling unworthy seemingly does as well. Both are a form of validation-worship, and both will hollow you out in the end.\n\nI watch you, watching me, watching you, ad infinitum. I read your writing to me knowing that I will read it, and so forth. With each loop a layer of expectation and distortion is superimposed on both the writing and the reading process. We write what we are expected to write, narrow our voices to what is presumed; likewise, our audience comes to expect a certain line of reasoning, certain stylistic motifs, and is confused and dismayed when we stray. We even take the reactions of our friends as near-gospel in reacting to a new user\u2019s answers \u2014 should they be cheered or ostracized? Whose side are we expected to be on? A single grudge can be amplified by the tribal herd, and pity those who get in the way.\n\nThe writer who is too acutely aware of their identity is like the parent who is standing up, yelling proudly about the recital pianist on stage, obliviously drowning out their own creation.\n\nA common characteristic of many long-term relationships (romantic, platonic, professional) is a spiraling into a smaller and smaller set of acceptable behaviors. The micro-reactions of each day become evolutionary feedback, both reinforcement and punishment, and entrain the two people into a constrained, stagnating dynamic. Novelty becomes routine becomes reflex. We think we understand the Other completely even as we consider ourselves unendingly complicated. It takes effort to consciously reverse this process, to escape from predetermined roles, to question our social and emotional assumptions.\n\nThe same thing can happen between a writer and a reader when both are known to each other. Almost everyone I have talked to on Quora has eventually found a specialization of some sort \u2014 a social role they play in the ecosystem, or a niche style of writing, or a specific domain that they \u2018own\u2019, for better or for worse. They are expected to present a particular persona, or be an expert in certain topics; but those expectations eventually become a straitjacket, limiting their experience of us and our experience of them.\n\nWriting anonymously short-circuits instinct and confounds expectation. It thrusts both writer and reader back into the honeymoon phase of your relationship, when every act was an adorable discovery and not a violation of implicit norms.\n\nAnonymity seems like a fundamental questioning of identity itself. What makes you you? Is it your continuity of self-awareness? Well, no, it can\u2019t be \u2014 that continuity is broken every time you fall asleep. When you wake up, your only connection to the previous you is your memories, which might well be implanted, as in so many science fiction movies.\n\nIs it your behavior? Perhaps you act in a strongly consistent, clearly distinctive manner? Perhaps, but the evidence is not encouraging. From Stanley Milgram\u2019s electric shock experiments to the now-famous Stanford Prison Experiment, individuals seems to act strongly according to their assigned roles, even though no one actually expects themselves to do so going into the experiment. Indeed, we judge others harshly for doing so, for claiming that they were merely \u2018following orders\u2019. The contrast is so powerful and repeatable that it has been given the kingly title of Fundamental Attribution Error \u2014 we over-attribute dispositional (personality) explanations for another person\u2019s behavior, while underestimating situational explanations.\n\nIt may sound as if I am nudging you towards some sort of psycho-emotional nihilism. But in fact, I mean to do the exact opposite.\n\nIn our greatest moments of personal meaning \u2014 whether making love, admiring a stunning landscape, or being fully immersed in an artistic experience \u2014 we are connected to something beyond our understanding. We feel something approaching infinite empathy, patience, and understanding. And notably, we lose track of our boundaries, of where we end and the magnificence before us begins. Ego is simply not large enough to hold such abundance.\n\nAll of the attachments above \u2014 the illusion of somatic immortality; the connection to material objects; the unfulfillable desire for validation; the infinite recursion of social modeling; and the self-oriented notion of a completely independent personality \u2014 are forms of separation, of artificial boundaries. Even as they wall us off from the reality of the world, they wall us off from ourselves, from our true voice, and from our greatest (and yes, most unique) contributions.\n\nWith anonymity, we are free to discover and evolve our writing voice, let it fall on less-biased ears, avoid the status traps of jealousy or superiority, and let the community take ownership of our words and our ideas and our gifts. We are free to connect. We are free to create something larger, ultimately, than ourselves."
    },
    {
        "url": "https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f?source=user_profile---------6----------------",
        "title": "Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)",
        "text": "In 2014, Ian Goodfellow and his colleagues at the University of Montreal published a stunning paper introducing the world to GANs, or generative adversarial networks. Through an innovative combination of computational graphs and game theory they showed that, given enough modeling power, two models fighting against each other would be able to co-train through plain old backpropagation.\n\nThe models play two distinct (literally, adversarial) roles. Given some real data set R, G is the generator, trying to create fake data that looks just like the genuine data, while D is the discriminator, getting data from either the real set or G and labeling the difference. Goodfellow\u2019s metaphor (and a fine one it is) was that G was like a team of forgers trying to match real paintings with their output, while D was the team of detectives trying to tell the difference. (Except that in this case, the forgers G never get to see the original data \u2014 only the judgments of D. They\u2019re like blind forgers.)\n\nIn the ideal case, both D and G would get better over time until G had essentially become a \u201cmaster forger\u201d of the genuine article and D was at a loss, \u201cunable to differentiate between the two distributions.\u201d\n\nIn practice, what Goodfellow had shown was that G would be able to perform a form of unsupervised learning on the original dataset, finding some way of representing that data in a (possibly) much lower-dimensional manner. And as Yann LeCun famously stated, unsupervised learning is the \u201ccake\u201d of true AI."
    },
    {
        "url": "https://medium.com/@devnag/short-story-how-does-a-penniless-entrepreneur-attract-gold-diggers-bb048a80f986?source=user_profile---------7----------------",
        "title": "Short Story: How does a penniless entrepreneur attract gold diggers?",
        "text": "There is a special kind of disappointment that grips a woman who has married up and realized, sadly, that there is always a higher up. Is he worth a hundred million? She reads several stories every year about yet another twenty-something, often in her very own neighborhood, who has already outdone her thirty- or forty-year old mate, and in a seemingly shorter period of time. Did she imagine that her husband would start one blockbuster after another, that he was a Jim Clark or a Steve Jobs? When he presses, coming home later and later to try and repeat his earlier feats, stressed and tired and blaming his co-founders and realizing that he will soon be the high school quarterback who tells war stories from behind the counter at 7\u201311, she looks at him and realizes that her initial impressions of his godlike omnipotence were just a projection \u2014 and that regardless of their bank wealth, they will not be joining the Buffetts and the Gates at their next bridge game, will not be giggling with Bill and Melinda at a junket in Africa, will not be leading the morning panels at Davos and skiing in the crisp afternoons.\n\nAnd that is when the penniless entrepreneur strikes.\n\nYou see, he reminds her of everything she once fell in love with. No one really cares about money \u2014 all the same emotions have existed for hundreds of thousands of years, across eras, across societies, across species. Money is just a means to an end \u2014 to the emotional high of status, or the safe womb of security. For our heroine, years of absolute security have dulled her of her need. The emotional high of status wears on her as she and her husband always look higher, always dream larger, and fall short time and time again. When you worship money or status, you will never have enough. There is always more. A status insecurity meter that evolved in tribes of 50 people is forever pegged in a connected world of 6 billion.\n\nShe looks \u2014 really looks \u2014 at her husband and realizes that outside of his consumer marketing instincts and technical wizardry and (increasingly infrequent) write-ups in TechCrunch and VentureBeat and his nouveau confidence, he\u2019s quite boring. He was one of the guys she scrunched her nose at in high school. And, like the high-status but none-too-bright boys she dated junior year, he has already peaked, and is now in steady decline.\n\nThe penniless entrepreneur has no mountain to fall from. He is all bright, shiny potential, fearlessly excited about the future, not imprisoned by the past. She falls into his future with him \u2014 a future without blemish, without the realities of those damn Khoslas down the street and their private island, without the harsh comments in the TechCrunch stories comparing her husband to a washed-up celebrity. She reads those comments with rubber-necking, guilty fascination\u2026and in her heart, she cannot help but agree.\n\nThe penniless entrepreneur wins a tiny seed investment. A mere $25k, from a family friend. This is such a trivial amount to her, a rounding error, less than her annual clothing budget. Yet\u2026.his delight is palpable, infectious. Her husband does not get this excited by $25 million, or by much of anything, anymore. She has awoken. She swells with pride and glee and, on some level, realizes that the thrill was never in the reward, but in the hunt.\n\nAnd she does not realize, until it is far too late, that she herself has become the hunt."
    },
    {
        "url": "https://medium.com/@devnag/tough-customers-make-the-best-fans-because-information-theoretic-shannon-entropy-8d3cf6030b36?source=user_profile---------8----------------",
        "title": "Tough customers make the best fans because\u2026information-theoretic Shannon entropy",
        "text": "tl; dr: If you can win over the customers with the highest standards, you\u2019ll have an evangelist for life. Here\u2019s why\n\nOne of the things I didn\u2019t understand about enterprise (business) customers until I started Wavefront was the interplay between early reactions to a demo and long-term satisfaction/evangelism.\n\nWhen we demo Wavefront for customers, we get one of two reactions\u2014 either they love it from the first moment, enjoying the ride up to and through the heart of the \u201cgood stuff\u201d; or, we are constantly pushed with one hard-nosed question after another, open expressions of doubt, sneers about differentiation, etc.\n\nIn other words, some days you get the elevator, and some days you get the shaft.\n\nHere was my naive expectation \u2014 the \u201ctough\u201d questioners would be the ones who would never really buy into Wavefront and would be internal obstacles, even if their teams purchased and started using our product, while the initially \u201chappy\u201d prospects would be our best long-term fans.\n\nInstead, I found exactly the opposite.\n\n\u201cHappy\u201d prospects were satisfied over the long-term, as expected. But the initially \u201ctough\u201d prospects became, counterintuitively, our best evangelists \u2014 the ones who mentioned us glowingly at their conference speeches, the ones who told their old co-workers and other friends in the industry about us, the ones who volunteered to do case studies and interviews with us, the ones who stopped by our booth at trade shows to convert whatever skeptics wandered by.\n\nWhy is this? Well, it has to do with information theory."
    },
    {
        "url": "https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264?source=user_profile---------9----------------",
        "title": "Pointer Networks in TensorFlow (with sample code) \u2013 Dev Nag \u2013",
        "text": "The SQuAD task is a nice step on the way to linguistic AI; you basically get a medium-length text passage, a variety of questions about that passage, and then text answers. The catch \u2014 and what makes this task more \u2018feasible\u2019 than full-fledged Q&A \u2014 is that the answer has to be a contiguous sequence of letters or words in the original passage. In other words, the answer to any of the questions has to be a set of two pointers : one pointer to the start of the \u2018answer range\u2019 and one pointer to the end (say, from character 47 to character 65, or word 14 through word 17).\n\nI recently read a paper that described a new state-of-the-art result on the Stanford Question Answering Dataset (SQuAD) . The performance (F1 of 70%) was impressive, but especially interesting was an architectural capability that I hadn\u2019t seen when it came out a year earlier in the literature \u2014 the ability to compute variable-length probability-distributions with a fixed architecture, over arbitrary inputs .\n\ntl; dr: Deep learning networks can be applied to variable-length targets, meaning you can index into arbitrary text, time series, or any sequence of data your selfish little heart desires\n\nObviously, not all questions can be answered in such a way, so this is something of an \u201ceasy subset\u201d of all possible, relevant questions (\u201ceasy\u201d being relative, of course). But if you actually peruse the dataset over a chilled glass of Pinot Grigio, it\u2019s fascinating just how many meaningful questions fit into this merest haiku of a linguistic task.\n\nNow, let\u2019s describe the naive way that you might try to attack this problem. Typically, your output layer will be a vector that\u2019s either a distributed representation (maybe a so-called \u2018sentence vector\u2019 or \u2018thought vector\u2019) or a one-hot representation (representing a probability distribution over the set of vector slots/dimensions).\n\nYou have to pick an output size. You have to train that final matrix with some fixed output dimensions (whether 10 or 10,000) and that\u2019s the size of the output vector you\u2019re going to get.\n\nHow do we apply this to SQuAD? What if one passage is 300 words long and another is 3,000 words long? Maybe you should pick the longest length passage you\u2019ll accept (say, 10,000) and just hope that 1.) you won\u2019t be wasting too much computation time training shorter passages on a giant architecture and that 2.) learning will actually transfer well across different passage lengths inside this fixed-length box. Unfortunately, this approach is just as fragile as it sounds.\n\nThe state-of-the-art SQuAD paper described above (Wang & Jiang, 2016) used some common designs (heavy use of bidirectional LSTMs, alignment matrices as used in translation tasks, etc.) but also mentions a technique called Pointer Networks that is, indeed, that better way.\n\nHere\u2019s the basic idea, architecturally: we\u2019ll train a fixed-size network but map it over variable-length input to get variable-length output (pointers).\n\nTo do this, we start with the sequence-to-sequence design pattern explained here, folding the input sequence (whatever the length) into a fixed-size hidden state. Then, we\u2019ll unfold that hidden state into a series of soft \u2018pointers\u2019 \u2014 probability distributions over the input sequence. In the SQUAD example above, as well as our coming example, there are two pointers (start and end), so we unfold the hidden state twice \u2014 the first time, to get the \u2018start pointer\u2019 probability distribution over all the inputs, and the second time, to get the \u2018end pointer\u2019 probability distribution over all the inputs. (If we had a different problem that required one or three or fourteen pointers, we would have to unfold the hidden state one or three or fourteen times.)\n\nHere\u2019s a schematic of the whole pipeline:"
    },
    {
        "url": "https://medium.com/@devnag/seq2seq-the-clown-car-of-deep-learning-f88e1204dac3?source=user_profile---------10----------------",
        "title": "seq2seq: the clown car of deep learning \u2013 Dev Nag \u2013",
        "text": "Let\u2019s start by diving into one of the problems that led to seq2seq \u2014 automated translation. Suppose you want to translate words from one language (say, English) to another language (say, German). You can\u2019t just map word-token to word-token, obviously; some tokens disappear in the translation, others appear from nowhere, some are highly context-dependent on the tokens around them, and some tokens converge or diverge like my personal favorite, \u201cRechtsschutzversicherungsgesellschaften\u201d (or, in English, \u201cinsurance companies that provide legal protection\u201d). Easy examples like this drive home the point that translation just isn\u2019t a token-level function.\n\nNow, a professional literature translator would say that translation isn\u2019t a sentence-level function, either (it\u2019s worth noting that both Nabakov and Borges translated the works of others, and considered it an act of literary creativity in its own right)\u2014but at least this simplification gets closer to the task.\n\nNow that we\u2019ve decided on sentence-level translation as the task, we have to ask ourselves \u2014 how can we possibly model that with deep learning? Whatever architecture you pick (feedforward vs. recurrent, deep vs. shallow, etc), you still have to pick the size of the input layer, the size of the output layer, and all the machinery within. But sentences are all sorts of different lengths! Do you build different networks for each length (say, one network that translates all of the 12-word sentences in English into 8-word sentences in German, and so on)? That seems absurd, and it is. Don\u2019t do this."
    },
    {
        "url": "https://medium.com/@devnag/machine-learning-representation-evaluation-optimization-fc7b26b38fdb?source=user_profile---------11----------------",
        "title": "Machine Learning = Representation + Evaluation + Optimization",
        "text": "tl; dr: You can think of machine learning algorithms as the combination of landscape, preference, and strategy\n\nPedro Domingos, a CS professor at the University of Washington (pronounced \u201cYou-Dub\u201d by everyone who grew up in the Pacific Northwest and \u201cIs that near the White House?\u201d by everyone else), published a brief and immensely readable paper in 2012 that helpfully decomposed machine learning into three components: Representation, Evaluation, and Optimization.\n\nRepresentation is basically the space of allowed models (the hypothesis space), but also takes into account the fact that we are expressing models in some formal language that may encode some models more easily than others (even within that possible set). This is akin to the landscape of possible models, the playing field allowed by a given representation. For example, 3-layer feedforward neural networks (or computational graphs) form one type of representation, while support vector machines with RBF kernels form another.\n\nEvaluation is essentially how you judge or prefer one model vs. another; it\u2019s what you might have seen as a utility function, loss function, scoring function, or fitness function in other contexts. Think of this as the height of the landscape for each given model, with lower areas being more preferable/desirable than higher areas (without loss of generality). Mean squared error (of a model\u2019s output vs. the data output) or likelihood (the estimated probability of a model given the observed data) are examples of different evaluation functions that will imply somewhat different heights at each point on a single landscape.\n\nOptimization, finally, is how you search the space of represented models to obtain better evaluations. This is the way you expect to traverse the landscape to find the promised land of ideal models; the strategy of getting to where you want to go. Stochastic gradient descent and genetic algorithms are two (very) different ways of optimizing a model class. Note that once you provide a trained model, it\u2019s quite possible that you may no longer be able to recover exactly how it was optimized. Just because I can see you standing in the pit doesn\u2019t mean I know how you got there."
    },
    {
        "url": "https://medium.com/@devnag/a-simple-design-pattern-for-recurrent-deep-learning-in-tensorflow-37aba4e2fd6b?source=user_profile---------12----------------",
        "title": "A simple design pattern for recurrent deep learning in TensorFlow",
        "text": "tl; dr: You can hide/encapsulate the state of arbitrary recurrent networks with a single page of code In an ideal world, every deep learning paper proposing a new architecture would link to a readily-accessible Github repository with implemented code. In reality, you often have to hand-code the translated equations yourself, make a bunch of assumptions, and do a lot of debugging before you get something that may or may not be related to the authors\u2019 intent. This process is especially fraught when dealing with recurrent architectures (aka \u201crecurrent neural networks\u201d): computational graphs which are DGs (directed graphs) but not DAGs (directed acyclic graphs). Recurrent architectures are especially good at modeling/generating sequential data \u2014 language, music, video, even video games \u2014 anything where you care about the order of data rather than just pure input/output mapping. However, because we can\u2019t directly train directed graphs with directed cycles (whew!), we have to implement and train graphs that are transformations of the original graph (going from \u201ccyclic\u201d to \u201cunrolled\u201d versions) and then use Backpropagation through time (BPTT) on these shadow models. In essence, we\u2019re mapping connections across time to connections across space:\n\nNow, if you\u2019re just using vanilla LSTM/GRU, there are off-the-shelf components that you can duct-tape together easily. That\u2019s not the problem. The hard part is taking a new recurrent architecture and trying to code the novel graph while also handling all the unrolled state tricks without introducing new bugs. For example, suppose you found yourself perusing Alex Graves\u2019 bodice-ripping classic from 2013, \u201cGenerating Sequences with Recurrent Neural Networks\u201d, and wanted to implement his LSTM. Everywhere you see t-1 as a subscript is yet another place (and yes, Virginia, there are 7 in that little brick of symbols) that you need to worry about recurrent state: initializing it, retrieving it from the past, and saving it for the future. If you look at TensorFlow tutorials, you\u2019ll see a lot of code dedicated to worrying about packing and unpacking the various recurrent states. There\u2019s much room for error here, and a cruel and unusual intermingling of architecture and logistics. It becomes half-declarative, half-procedural\u2026and all-fugly. But with just a tiny amount of bookkeeping code, you can make it so much easier, and almost live the dream (!) of typing in recurrent equations and getting working TensorFlow code out the other side. We can even steal TensorFlow\u2019s idiom for get-or-create scoped variables. Let\u2019s briefly look at the relevant stanza:\n\nHere, the bp variable is a BPTT object that\u2019s responsible for all recurrent state. There are two interesting method calls here. bp.get_past_variable() handles both initialization from a random constant and retrieval of past state (t-1), and bp.name_variable() saves the current state for future suitors. Look how close this code is to the raw mathematical equations \u2014 I\u2019ve left out the shape definitions for the weight matrices and bias vectors for clarity, but for the most part it\u2019s a 1-to-1 mapping: easy to write and easy to read. The only mental overhead is retrieving the recurrent variable(immediately before usage) and saving it (inline with usage), all in local context. There\u2019s no reference to this state anywhere else in the graph-building code. In fact, the rest of the code bears a striking resemblance to non-recurrent TensorFlow. Then, to generate the shadow (unrolled) model, we just call on the bp object to generate the sequence of connected graphs with a single line: This sequence of graphs has placeholders at the right places (where those inline constants will come back to make a dramatic cameo) and are stitched together at every bp.get_past_variable() call. During training (or inference), there are three places where all this recurrent state must be brought back into play. First, we have to send the working state into the feed dictionary (either the initialized constants defined above, or the working state from a previous training loop), and insert the training data into the unrolled placeholders. Second, we have to define the state variables we want returned from the session.run() method. Third, we have to take the post-session.run() results and extract out the state for the future.\n\nThe BPTT object takes care of that bookkeeping as well. Note that we\u2019re also passing a flag (bp.DEEP) in many of the calls here, during the training phase. This is because another common design pattern of recurrent networks is that you first train the unrolled/deep network but then infer using the cyclic/shallow network (with the same, shared post-training parameters). When we infer, we use the bp.SHALLOW flag which has a different set of placeholder variables and thus needs to manage a different state pipeline. There\u2019s also a convenience method (copy_state_forward()) to copy the final unrolled/deep state (recurrent variables) into the cyclic/shallow network before starting inference. Inference idiom \u2014 almost identical to the training phase, but feeding one frame of data at a time Recurrent deep learning in TensorFlow can be \u2014 if not easy \u2014 a little bit easier. Want to check out the code + sample usage? Say no more."
    }
]