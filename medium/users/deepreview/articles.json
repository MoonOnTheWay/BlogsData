[
    {
        "url": "https://medium.com/deepreview/review-of-identity-mappings-in-deep-residual-networks-ad6533452f33?source=---------0",
        "title": "Review of Identity Mappings in Deep Residual Networks",
        "text": "Note: To have a clear understanding of the contents of this review, you need to first read our review of Deep Residual Networks for Image Recognition\n\nDeep residual learning by Kaiming et al. provides a solid framework to optimize deep neural networks through the use of shortcut connections that has eased the training of such deep networks, effectively solving the vanishing gradient problem. However, despite the improved information flow enabled by shortcut connections in residual networks, training of networks with as many as 1000 layers has been demonstrated to result in reduced accuracy. This indicates that the vanishing gradient problem resurfaces with ultra-deep networks. In this paper, the authors investigate the nature of residual networks and the impact of information flow. To combat the vanishing gradient problem in ultra-deep networks, the authors devised a new residual unit that allows information to flow unimpeded through the entire network. With this modification, they demonstrate that training of layers as deep as 1000 results in further increase in accuracy.\n\nIn this review, we comprehensively analyse the nature of residual networks based on the observations of the original authors, we concisely explicate why the new residual units work better and we conclude by implementing the second version of Resnet, henceforth referred to as ResnetV2.\n\nResidual networks are formulated as the equation y = f(x) + x. Where x is the input to the residual unit, and f(x) is a number of layers of convolution \u2014 bn \u2014 relu, usually two of them or three in the case of bottleneck layers.\n\nHowever, due to the application of relu after the addition, the result of the addition denoted as y is not returned directly, rather the output of the residual unit, denoted as Xl + 1 is the result of applying relu on the y.\n\nFor clarity, we shall henceforth refer to all inputs to a layer L as\n\nand the output as\n\nsince this would serve as the input to the next layer L + 1.\n\nIn light of this; the residual unit is represented by the equation\n\nIn the equation above, the original input is added to the output of the layers without modification, however, the input can be passed through another function before being added with the output. Denoting such a function as H , the residual unit becomes.\n\nAlso, the relu function applied to the layer outputs can be any other function. Hence, we shall substitute G for relu in the equation above. Hence, our residual unit becomes.\n\nIn the first version of resnet, H is an identity function, since we don\u2019t pass the original input through any other function before addition, except when we perform downsampling, in that case, we appy a 1 x 1 Conv and batch normalization on the input, hence H becomes a 1 x 1 Conv followed by batch normalization.\n\nHowever, G is never an identity function, it is always the relu function. The function G leads to loss of information about the original state of the image, in this case, the input to the next layer would no longer contain exactly the original data, rather it would have been modified by the relu function denoted as G . To ensure unimpeded information flow through the network, both H and G must be identity functions.\n\nGiven G and H as identity functions, the equation for a residual block becomes.\n\nThis can be simplied as\n\nThis ensures that the original information is not altered and can flow unimpeded layer to layer throughout the entire network. Consider the equations below.\n\nEqn 6 can be written as\n\nThis can be simplified as\n\nHence, for any layer L + N, the equation for the network is represented as\n\nConsequently, gradients at every single layer could be computed with the original input taking into consideration. Given the above equation, when G and H are identity functions, information would always flow unimpeded and gradients would never vanish no matter how deep we go.\n\nTo satisfy the equation for identity connection residual connections, the authors designed new residual units with pre-activation. This entails, rather than put batch normalization and relu after convolution, they put them before the convolution. This can be seen below\n\nThe identity resnet module above satisfies the conditions for the equations we earlier stated. Since the result of the addition is directly passed into the next layer, the input is preserved, this is also of no consequence since batch normalization and relu always comes before the convolution.\n\nSo far, we have explained the theoretical advantages of identity connections in deep residual networks. These theoretical justifications also result in better performance, improved stability and increased accuracy with ultra-deep networks.\n\nBelow are results on CIFAR 100\n\nNotice in the above that the 164-layers pre-activation Resnet outperforms the original Resnet by 0.93%. The impact of identity connections is far more pronounced with 1001-layers version. While the accuracy of the 1001-layers original Resnet is 2.66% lesser than the 164-layer version, the 1001-layers pre-activation Resnet outperforms it\u2019s 164-layers version by 1.45%. This clearly indicates that with identity connections, a deeper network is guaranteed to always perform better than a shall network given same network width.\n\nThe authors further experimented with various gating mechanisms applied to either the original input, the function output or both, however, all of them paled in comparison to the identity residual units.\n\nKey to the success of deep networks including residual networks is the use of batch normalization, which is known to have regularization effects similar to dropout. Althought, the original Resnet used batch normalization, it is applied before the addition. The addition causes the output to become un-normalized, hence, the input to the next layer is not normalized properly. Using pre-activation Resnets, the input is always normalized at the very beginning of the identity unit. Consequently, convolutions always receive properly normalized inputs.\n\nThe structure of Pre-activation resnet is exactly the same as the original resnet, with a few exceptions. First the identity resnet module is structured as below.\n\nAs you can see, batch normalization and relu comes before the convolution, also, we do not apply relu after addition. And we do not apply batch normalization when we downsample the residual.\n\nThe resnet block is still exactly the same as in v1. See code below.\n\nFinally, the complete model is almost the same except with a few changes. First, since the modules all start with bn and relu, we do not need to apply batch normalization and relu after the very first convolution. Second difference is that since the output of the modules is always the addition of outputs of convolutions, we need to apply batch normalization and relu on the output of the last layer just before GlobalAveragePooling2D.\n\nHaving said that, you would gain clearer understanding of the differences by studying the code below.\n\nDeep residual networks works well due to the flow of information from the very first layer to the last layer of the network. By formulating residual functions as identity mappings, information is able to flow unimpeded throughout the entire network. This allows any layer to be represented as a function of the original input. Using pre-activation resnets by placing batch normalization and relu before the convolution, the output of the addition becomes the output of the layer, this achieves the identity effect we desire. Consequently, the authors are able to train residual networks as deep as 1001 layers with increasing accuracy.\n\nTHE COMPLETE MODEL AND TRAINING CODE FOR CIFAR10 IS AVAILABLE FROM https://github.com/johnolafenwa/deepreview/tree/master/vision\n\nThis post is a part of the Deep Review project by AI Commons by Moses Olafenwa and John Olafenwa. Visit https://commons.specpal.science to learn more about our mission to advance and democratize Artificial Intelligence."
    },
    {
        "url": "https://medium.com/deepreview/review-of-deep-residual-learning-for-image-recognition-a92955acf3aa?source=---------1",
        "title": "Review of Deep Residual Learning for Image Recognition",
        "text": "Deep Learning is based on the idea of stacking many layers of neurons together. Such neurons include fully connected layers where every output in the previous layer is connected to every node in the next layer, locally connected convolution layers with kernels that act as feature detectors and recurrent layers such as Gated Recurrent Units (GRU) and Long-Short Term Memory (LSTM) cells. Over the past years, deep learning researchers have successfully optimized the performance of neural networks by stacking more layers. With the growing availability of high performance GPUs as well as larger datasets. This technique has proven very effective. However, deeper networks are more difficult to train, not because of their computational cost, but due to difficulty of propagating gradients through so many layers. Neural networks train by computing the derivatives of parameters with respect to the training loss function. However, with so many deep layers, derivatives start to diminish, this is known as the gradient vanishing problem. Normalization techniques such as the highly effective batch normalization helped to greatly alleviate the problem of vanishing gradients by normalizing the activations of every single layer to have zero mean and unit variance with respect to the statistics obtained per batch during training. However, the problem still resurfaces as layers go very deep.\n\nIn this paper, the authors comprehensively analyse the cause and effects of vanishing gradients and devised an effective solution to enable the training of ultra-deep networks.\n\nThe authors also present a new family of image recognition networks that formed the basis of their submission to the ImageNet2015 and COCO 2015 challenges. On all benchmarks, their network outperforms all state of the art models.\n\nIn this review, we examine the key points in the original paper and prior related work, we comprehensively analyse the structure of their network and we conclude with an open source implementation of the original models.\n\nKey to the performance of artificial neural networks is the depth of the network. Neural networks act as both feature extractors and classifiers at the same time. The ability of neural networks to act as automatic feature extractors greatly increases their ability to generalize to new problems, for example, to classify unseen images. With manual feature engineering, it is extremely hard in domains such as image recognition to hand-craft the features necessary for classification of new unseen images. Shallow networks are fully capable of automatic feature extraction, however, due to the low depth of representation, they cannot extract fine grained features that would ultimately allow the model to generalize properly. Deeper models on the other hand are able to extract low-level, mid-level and high-level features. Hence, they act as more excellent feature extractors than shallow models, this has enabled them to perform much better at classification that shallow models. This is evidenced by the fact that all leading models in various domains of deep learning, exploit the concept of depth.\n\nHowever, a key problem with great depth is loss of information as networks go very deep. A simple intuition behind this is to consider decision making as a function of the history of events. Events happen in sequence, with past events influencing future events, consider in this light; a decision maker that can only see the last past event. Such a decision maker makes decisions on the grand assumption that the past event already encodes all we need to know about all the previous events. For very short history of events, this assumption can hold fine because there is often a strong correlation between closely successive events, however, when the history is long, at each time step, information about how the past affects the future is lost gradually, as we go deeper into the future, eventually, we become very short-sighted, relying only on the consequence of past time steps without putting actual past events into consideration when making decisions.\n\nThe depth problem becomes exposed as layers go very deep. The authors trained a 20-layer network and a 56-layer network on the CIFAR10 dataset. Surprisingly, the 20-layer network outperformed the 56-layer network. Thus, it became clear that simply stacking more layers is not sufficient to optimize deep neural networks. The 56-layer network also had higher training error than the 20-layer network, this clearly indicates that it is not an overfitting problem, hence, well known regularization techniques like dropout cannot be applied to solve the problem.\n\nTo solve the gradient vanishing problem associated with ultra-deep networks, the authors introduced residual connections into the network. Residual connections are simply connections between a layer and layers after the next.\n\nThis idea is clearly illustrated in the diagram below:\n\nIn the diagram above, the plain network simply sends information over from one layer to the next, information about the past state of the image is highly limited and all activations must be based on the new features, the residual connections on the other hand takes the future map from layer t and adds it to the output of layer t + 2.\n\nThis is equivalent to learning the residual function y = f(x) + x\n\nIn direct feedforward networks without residual connections, a layer T only relies on data at layer T \u2014 1 with layer T -1 encoding the consequence of all the previous layers, on the other hand, residual connections look farther into the past, putting into consideration information from layer T \u2014 2.\n\nThis very simple but powerful idea enabled the authors to train over a 100 layers network with increasing accuracy.\n\nIt is noteworthy that while the authors originally considered residual connections as being important for depth, future work has proven that residual networks can improve the performance of both shallow and deep neural networks. This agrees with our illustration of residual functions as improving accuracy by providing sufficient data about the original state of the data.\n\nAdding features from previous time steps has been used in various tasks involving multi-layer fully connected networks has well as convolutional neural networks. Most notable of these are Highway networks proposed by Srivastava et al. Highway networks feature residual connections however, unlike resnet, their residual connections are gated. Hence, information flow from the past is determined by how much of the data the gating mechanism allows to pass through. This idea was primarily inspired by gating mechanisms in LSTMs.\n\nWhile residual networks have the form\n\nNote that in the equation for highway networks, the sigmoid function takes the general form 1/ (1 +e^-x), the sigmoid function always outputs values in the range of 0\u20131, the parameters W and b are learned weights and bias which controls the output of the sigmoid function. A non-residual network can be viewed as a special case of highway networks with the output of the sigmoid gate as 1.\n\nWhen the output of the sigmoid gate is 0, a highway network becomes an identity function\n\nHighway networks enabled information flow from the past but due to the gating function, the flow of information can still be impeded. Hence, a highway network with 19 layers performed better than a highway network with 32 layers.\n\nResnets have a very homogenous structure, they are similar in construction to VGG by Simonyan et al. They are made up of many layers of residual modules, which are in turn grouped into residual blocks.\n\nResnet modules are of two variants; the first is made up of two layers of 3 x 3 convolutions, the other which is more popular is called a bottleneck layer, because it is composed of a 1 x 1 convolution that reduces the number of channels by a factor of 4, followed by a 3 x 3 convolution and finally a 1 x 1 convolution that expands the layers back to C. The motivation for the bottleneck block is to reduce the computational cost of the network, since 1 x 1 convolutions are 9 times less expensive than a 3 x 3 convolution, they are used to minimize the number of channels that comes into the 3 x 3 convolution.\n\nThe bottleneck module can constructed in keras as below:\n\nTo have a clear understanding of the above code, consider this picture we carefully drew.\n\nIn the diagram above, an input x comes into the module, C represents the number of output channels, we pass the input into a 1 x 1 conv with channels equal C / 4, followed by batch normalization and relu. This setting is repeated for the 3 x 3 conv, finally, we pass the output of the 3 x 3 conv through a 1 x 1 conv with C channels, followed by only batch normalization.\n\nAlso, in the beginning we let the residual equal to the input, but if we are to pool, which often involves doubling the number of channels, then the residual is the result of a strided 1 x 1 conv with channels equal to C. If this is not done, the dimensions of the residual and the output would not match.\n\nFinally, we add the residual with the output of the last 1 x 1 Conv \u2014 BN layer. We then apply relu on the result of the addition.\n\nA resnet block is a stack of the bottleneck layer.\n\nNote in the above, we set pool = True only in the first layer in each block, this is clearly defined in the code above.\n\nThe above code is highly modular to allow scaling to thousands of layers. Here only 50, 101 and 152 layers are supported, if other values are supplied, a value error is raised, having said that, the code can be modified to scale to ultra-deep networks.\n\nResnet is made of of 4 blocks. The number of layers and filters for each block is determined by the block_layers dictionary.\n\nThe block_layers defines a dictionary that maps the total number of layers to the number of modules per block. For the 50-layer network, there are 3 modules in the first block, 4 modules in the second, 6 modules in the third and 3 in the fourth.\n\nNote that given that each module consists of 3 convolutional layers, if you compute the total number of layers for each configuration, you would realize it is equal to num_layers \u2014 2. Hence, for 50 layers we would have (3 + 4 + 6 + 3) * 3 = 48.\n\nFor the 101 layers, we would have 99 layers while for the 152 layers network we would have 150 layers.\n\nThe reason for this is, there is a convolution layer before the first block, and a fully connected layer that maps the feature maps to the class predictions at the end. This two layers are added to the number of layers in the blocks to make 50, 101 and 152 layers depending on the desired number of layers.\n\nThe block_filters dictionary determines filters for each block\n\nFor all the different configurations, the first block has 256 filters for all modules in each block, 512 for the second block, 1024 for the third and 2048 for the fourth.\n\nFinally, GlobalAveragePooling2D is applied on the output feature maps, this is simply a standard AveragePooling with pool size equal to the width and height of the feature map, hence, the result would be 1 x 1 x Filters, since each filter would become a 1 x 1 feature map.\n\nThe output of this is passed into a fully connected layer with softmax.\n\nThe depth of neural networks is absolutely important to obtaining better performance. When neural network layers are very deep, they exhibit higher training and validation loss due to information loss leading to vanishing gradients. To guarantee that deeper networks would always yield better accuracy than shallow networks, the authors of Resnet proposed the use of residual blocks, bringing in information from the past to compensate for information loss. This technique enables the training of very deep networks resulting in state of the art accuracy on standard benchmarks.\n\nTo reduce computation cost due to very deep layers, the authors use bottleneck layers with 1 x 1 convolutions that reduce the number of feature maps of the input.\n\nTHE COMPLETE MODEL AND TRAINING CODE FOR CIFAR10 IS AVAILABLE FROM https://github.com/johnolafenwa/deepreview/tree/master/vision\n\nThis post is a part of the Deep Review project by AI Commons by Moses Olafenwa and John Olafenwa. Visit https://commons.specpal.science to learn more about our mission to advance and democratize Artificial Intelligence."
    }
]