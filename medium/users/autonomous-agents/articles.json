[
    {
        "url": "https://medium.com/autonomous-agents/multi-gpu-training-of-large-sparse-matrix-on-wide-neuralnetwork-cac7afc52ffe?source=---------0",
        "title": "Multi-GPU training of Large, Sparse-Matrix on Wide #NeuralNetwork",
        "text": "Multi-GPU training of neural network on TensorFlow (v0.12 as of this blog) is a pain. It is a pain if you get off the beaten path that is. I found out a bit early that the SparseTensors in the contrib.learn package does not play well on GPUs. This is a tale of re-writing a separate utility on Keras/TensorFlow that plays well for very large sparse matrix on multiple GPUs.\n\nThe specific piece of code that did not work for me is as follows:\n\nNote that a large portion of input data which is loaded onto SparseTensor being not available on GPU defies the purpose of a multi GPU setup. Hence the issue is raised.\n\nAfter several attempts to help fix the SparseTensor in the TensorFlow library, for the lack of time, I resorted to write a sparse tensor utility myself for large sparse matrix. When I say large, I have tested the utility on 1.5 billion samples in input data, with about 20+ categorical features where some of the features have about 100K values per feature. Have tested this on 3 Tesla-X GPUs, and thoroughly satisfied with the results.\n\nThere are mainly 2 parts to this code.\n\nThe objective of the sparse hash is 2 fold as shown in the code section\n\nI use a non-cryptographic hash to speed up on large payloads. xxHash is a extremely fast hash function with throughputs of 5.4GB/s or above.\n\nHave used a reduction of SQRT of the maximum length of total possible feature values in the sample space per feature. This is a good thumb-rule that reduces hash collision as I have observed over years. I need 4 such buckets per hash in the 4-hash-tuple received from the four_bit_split_hash procedure.\n\nthe 4-bit-split (or 4 hot encoding) is a good compressor for large sparse datasets while keeping hash collision in mind. I could not see a single collision for the buckets on 4-bit-split.\n\nThis is a simple routine that converts the hash to sparse-matrix. I use the row-wise csr_matrix from scipy.sparse module to do the trick. You can easily replace this with column-wise if your downstream operations are sharded differently. I presume most Neural Network data is row-wise sharded on key-codes per sample space (Unless you are resorting to Model Parallelism)\n\nAlso the hstack procedure is used to concatenate the splits in the hash which are converted to it\u2019s own sparse array. Just for simplicity, I have also added a one-hot function.\n\nAt runtime, you have to convert the sparse matrix to a dense matrix to feed it to the Keras model (unfortunately, Keras does not yet support sparse feature as I understand). You can do so using the following procedure.\n\nNote that I use the toarray() function on the sparse matrix to convert this to dense.\n\nThe multi-GPU parallelization can be done in 3 different ways.\n\nLine #24 is where I am loading the exact same replica of the model across multiple GPU\n\nLine #27 is a Keras Lambda layer that allows the mix of outputs as defined in the prediction_mean function (line #15). I am just taking the mean of the outputs for now. This can be converted to a gaussian mix as explained in the mixture of experts in the past post titled : Committee of Intelligent Machines.\n\nOn Line #34, we split the batch to multiple slices to fit to as many GPUs as suggested.\n\nLine #60 merges all the outputs from different GPU into a single output post the final layer of the model.\n\nHere is a sample template to use the utility. Its incomplete for brevity\u2026\n\nOn Line #62 in the figure, creates a feature dictionary with all the features and feature_length (to be used by the hashing and sparse functions). The feature length is the maximum sample space per feature.\n\nLine #71, obtain the model by passing the feature dictionary along with type of parallelization needed (Defaults to Tower)\n\nLine #73, obtain the hash lookup for all uniques\n\nLine #75, get the sparse matrix and convert it to dense matrix with categorical encoding.\n\nLine #77, train the model with the dense matrix.\n\nNote that the same model architecture should be used if you are planning to save_weights and load_weights of the trained model. Also, needles to state, change in feature lengths shall change the model architecture by changing the input size drastically.\n\nHope this helps reduce your workloads and improve model throughputs. Specifically for my training, I could reduce 48hr model training workloads to nearly 10\u201312 hours. I shall take that any-day !\n\nNow, set those GPUs on fire people ! figuratively\u2026"
    },
    {
        "url": "https://medium.com/autonomous-agents/how-to-tame-the-valley-hessian-free-hacks-for-optimizing-large-neuralnetworks-5044c50f4b55?source=---------1",
        "title": "How to tame the valley \u2014 Hessian-free hacks for optimizing large #NeuralNetworks",
        "text": "Let\u2019s say you have the gift of flight (or you are riding a chopper). You are also a Spy (like in James Bond movies). You are given the topography of a long narrow valley as shown in the image and you are given a rendezvous point to meet a potential aide who has intelligence that is helpful for your objective. The only information you have about the rendezvous point is as follows:\n\nHow do you go about finding the lowest co-ordinate point? More so, how do you intend to find it in a stipulated time period?\n\nWell, for complex Neural Networks which has very large parameters, the error surface of the Neural Network is very similar to the long narrow valley of sorts. Finding a \u201cminima\u201d in the valley can be quite tricky when you have such pathological curvatures in your topography.\n\nIn the past posts, we used Gradient Descent algorithms while Back-propagating that helped us minimize the errors. You can find the techniques in the post titled \u201cBackpropagation \u2014 How Neural Networks Learn Complex Behaviors\u201d\n\nThere is nothing fundamentally wrong with a Gradient Descent algorithm [or Stochastic Gradient Descent (SGD) to be precise]. In fact we have proved that it is quite efficient for some of the Feed Forward examples we have used in the past. The problem of SGD arises when we have \u201cDeep\u201d Neural Networks which has more than one hidden layer. Especially when the Network is fairly large.\n\nHere are some illustrations of a non-monotonic error surface of a Deep Neural Network to get an idea.\n\nNote that there are many minima and maxima in the illustration. Let us quickly look at the weight update process in SGD\n\nThe problem with using SGD for the illustrations is as follows:\n\nWe need a better method to work with large or Deep Neural Networks.\n\nSGD is a first order optimization problem. First order methods are methods that have linear local curves. In that we assume that we can apply linear approximations to solve equations. Some examples of first-order methods are as follows:\n\nThere are methods called the second-order methods which considers the convexity or curvature of the equation and does quadratic approximations. Quadratic approximations is an extension of linear approximations but provide an additional variable to deal with which helps create a quadratic surface to deal with a point on the error surface.\n\nThe key difference between the first-order and second-order approximations is that, while the linear approximation provides a \u201cplane\u201d that is tangential to a point on a error surface, the second-order approximation provides a quadratic surface that hugs the curvature of the error surface.\n\nThe advantage of a second-order method is that, it shall not ignore the curvature of the error surface. Because of the fact that the curvature is being considered, second-order methods are considered to have better step-wise performance.\n\nThe following are some second-order methods\n\nLet\u2019s take a look at Newton\u2019s method which is a base method and is bit more intuitive compared to others.\n\nNewton\u2019s Method, also called Newton-Raphson Method is an iterative method approximation technique on the roots of a real valued function. This is one of the base method\u2019s used in any second-order convex optimization problems to approximate functions.\n\nLet\u2019s first look at Newton\u2019s method using first-derivate of a function.\n\nLet\u2019s say we have a function f(x) = 0, and we have some initial solution x_0 which we believe is sub-optimal. Then, Newton\u2019s method suggest us to do the following\n\nReally that simple. The caveat is that the method does not tell you when to stop so we add a 5th step as follows:\n\n5. If x_n (the current value of x) is equal to or lesser than a threshold then we stop.\n\nHere is the image that depicts the above:\n\nHere is an animation that shows the same:\n\nHere is the math for a function which is a first degree polynomial with one-dimension.\n\nNow, we can work on Newton approximation for a second degree polynomial (second-order optimizations) function with one-dimension (before we get to multiple dimensions). A second degree polynomial is quadratic in nature and would need a second-order derivative to work with. To work on the second-derivative of a function, let\u2019s use the Taylor approximation as follows:\n\nSuppose that we are working on a second degree polynomial with multiple dimensions, then we work with the same Newton\u2019s approach as we found above but replace the first-derivatives with a gradient and the second-derivatives with a Hessian as follows:\n\nA Hessian Matrix is square matrix of second-order partial derivatives of a scalar, which describes the local curvature of a multi-variable function.\n\nSpecifically in case of a Neural Network, the Hessian is a square matrix with the number of rows and columns equal to the total number of parameters in the Neural Network.\n\nThe Hessian for Neural Network looks as follows:\n\nNow, the second-order optimization using the Newton\u2019s method of iteratively finding the optimal \u2018x\u2019 is a clever hack for optimizing the error surface because, unlike SGD where you fit a plane at the point x_0 and then determine the step-wise jump, in second-order optimization, we find a tightly fitting quadratic curve at x_0 and directly find the minima of the curvature. This is supremely efficient and fast.\n\nBut !!! Empirically though, can you now imagine computing a Hessian for a network with millions of parameter? Of course it gets very in-efficient as the amount of storage and computation required to calculate the Hessian is of quadratic order as well. So, though in theory, this is awesome, in practice it sucks.\n\nWe need a hack for the hack ! And the answer seems to lie in Conjugate Gradients.\n\nActually, there are several quadratic approximation methods for a convex function. But Conjugate Gradient Method works quite well for a symmetric matrix, which are positive-definite. In fact, Conjugate Gradients are meant to work with very-large, sparse systems.\n\nNote that a Hessian is symmetric around the diagonal, the parameters of a Neural Network are typically sparse, and the Hessian of a Neural Network is positive-definite (Meaning, it only has positive Eigen Values). Boy, are we in luck?\n\nThe easiest way to explain the Conjugate Gradient (CG) is as follows:\n\nYou can check most of the hairy-math around arriving at a CG equation by the paper cited above. I shall directly jump to the section of the algorithm of the conjugate gradient:\n\nFor solving a equation Ax=b, we can use the following algorithm:\n\nGiven that we know how to compute the Conjugate Gradient, let\u2019s look at the Hessian Free optimization technique.\n\nNow that we have understood the CG algorithm, let\u2019s look at the final clever hack that allows us to be free from the Hessian.\n\nHere we need to find the best delta_x and then move to x+delta_x and keep iterating until converge. In other words, the steps involved in Hessian-free optimization is as follows:\n\nThe crucial insight: Note that unlike in the Newton\u2019s method where a Hessian is needed to compute x_n+1, in Hessian-free algorithm we do not need the Hessian to compute x_n+1. Instead we are using the Conjugate Gradient.\n\nClever Hack: Since the Hessian is used along with a vector x_n, we just need an approximation of the Hessian along with the vector and we do NOT need the exact Hessian. The approximation of Hessian with a Vector is far faster than computing the Hessian itself. Check the following reasoning.\n\nTake a look at the Hessian again:\n\nHere, the i\u2019th row contains partial derivates of the form\n\nWhere \u2018i\u2019 is the row index and \u2018j\u2019 is the column index. Hence the dot product of a Hessian matrix and any vector:\n\nThe above gives the directional derivative of \u2018e\u2019 with respect to \u2018w\u2019 in the direction \u2018v\u2019.\n\nUsing finite differences, we can then optimize the above as following:\n\nWith this insight, we can completely skip the computation of a Hessian and just focus on the approximation of the Hessian to a vector multiplication, which tremendously reduces the computation and storage capacity.\n\nTo understand the impact of the optimization technique, check the following illustration.\n\nNote that with this approach, instead of bouncing off the side of the mountains like in SGD, you can actually move along the slope of the valley before you can find a minima in the curvature. This is quite effective for very large Neural Networks or Deep Neural Networks with million of parameters.\n\nApparently, It\u2019s not easy to be a Spy\u2026"
    },
    {
        "url": "https://medium.com/autonomous-agents/bayesian-regularization-for-neuralnetworks-2f2d34f03adc?source=---------2",
        "title": "Bayesian Regularization for #NeuralNetworks \u2013 Autonomous Agents \u2014 #AI \u2013",
        "text": "Bayes\u2019s Theorem fundamentally is based on the concept of \u201cvalidity of Beliefs\u201d. Reverend Thomas Bayes was a Presbyterian minster and a Mathematician who pondered much about developing the proof of existence of God. He came up with the Theorem in 18th century (which was later refined by Pierre-Simmon Laplace) to fix or establish the validity of \u2018existing\u2019 or \u2018previous\u2019 beliefs in the face of best available \u2018new\u2019 evidence. Think of it as a equation to correct prior beliefs based on new evidence.\n\nOne of the popular example used to explain Bayes\u2019s Theorem is to detect if a patient has a certain disease or not.\n\nThe key inferences in the Theorem is a follows:\n\nEvent: An event is a fact. The patient truly having a disease is an event. Also, truly NOT having the disease is also an event.\n\nTest: A test is a mechanism to detect if a patient has the disease (or a test devised to prove that a patient does not have the disease. Note that they are not the same tests)\n\nSubject: A patient is a subject who may or may not have the disease. A test needs to be devised for the subject to detect the presence of disease or devise a test to prove that the disease does not exist.\n\nTest Reliability: A test devised to detect the disease may not be 100% reliable. The test may not possibly detect the disease all the time. When the detection fails to recognize the disease in a subject who truly has the disease, we call them false negatives. Also the test on the subject who truly does not have the disease may show that the subject does have the disease. This is called false positives.\n\nTest Probability: This the probability of a test to detect the event (disease) given a subject (patient). This does not account the Test Reliability.\n\nEvent Probability (Posterior Probability): This is the \u201ccorrected\u201d test probability to detect the event given a subject by considering the reliability of the devised test.\n\nBelief (Prior Probability): A belief, also called a prior probability (or prior in short) is the subjective assumption that disease exits in a patient (based on symptoms or other subjective observations) prior to conducting the test. This is the most important concept in Bayes\u2019s Theorem. You need to start with the priors (or Beliefs) before you make corrections to that belief.\n\nThe following is the equation which shall accommodate the stated concepts.\n\nSo let\u2019s assign the values for each probabilities.\n\nThe posterior probability can be calculated based on Bayes\u2019s Theorem as follows:\n\nSo the posterior probability of the person truly having the disease, given that the test result is positive is only 19% !! Note the stark difference in the corrected probability even if the test results are 90% accurate ? Why do you think, this is the case?\n\nThe answer lies in the \u2018priors\u2019. Note that the \u201cbelief\u201d that only 5% of the population may have a disease, is the strong reason for a 19% posterior probability. It\u2019s easy to prove. Change your prior beliefs (all else being equal) from 5% to let\u2019s say a 30%. Then you shall get the following results.\n\nNote that the posterior probability for the same test with a higher prior jumped significantly to 65%.\n\nHence, while all evidence and tests being equal, Bayes\u2019s theorem is strongly influenced by priors. If you start with a very low prior, even in the face of strong evidence the posterior probability will be closer to the prior (lower).\n\nA prior is not something you randomly make up. It should be based on observations even if subjective. There should be some emphasis on why someone holds on to a belief before assigning a percentage."
    },
    {
        "url": "https://medium.com/autonomous-agents/mathematical-foundation-for-noise-bias-and-variance-in-neuralnetworks-4f79ee801850?source=---------3",
        "title": "Mathematical foundation for Noise, Bias and Variance in #NeuralNetworks",
        "text": "Neural Nets are quite powerful models in machine learning which are used for learning the behavior of high dimensional data. Typically, data is not always present in its purest form, the signal. Most of the data that is made available for machines during training or runtime prediction do have some amount of noise. During training, the Neural Nets can get highly sensitive to the noise in the input and can start overfitting the learning to the noise in the input.\n\nIn the previous post titled \u201cAlgorithms to Improve NeuralNetwork Accuracy\u201d, we learnt about the overfitting problem in Neural Nets and how to break it using L1/L2 regularizers, Weight penalties decay and constraints. Also in the post title \u201cCommittee of Intelligent Machines\u201d , we learnt how to stack different models to improve accuracy and generalize the prediction better.\n\nIn this post, I would like to introduce the fundamental math on understanding Noise, Bias and Variance during Neural Net training and also use Noise as a regularizer for generalizing the Neural Nets.\n\nIt\u2019s important to understand noise in machine learning because this is a fundamental underlying concept that is present in all datasets.\n\nLet\u2019s start with simple understanding of Noise. Noise is a distortion in data, that is unwanted by the perceiver of data. Noise is anything that is spurious and extraneous to the original data, that is not intended to be present in the first place, but was introduced due to faulty capturing process.\n\nNoise gets into data in many ways:\n\nGiven this, we need to always account for noise during machine learning.\n\nNow, let\u2019s say we have some data that we are using to train our Neural Net models. Let\u2019s say that the input vectors {x1, x2,\u2026 xn} has outcomes {y1, y2\u2026yn} associated with it. Then the way to think about Noise is as follows:\n\nWhere f(x) is some underlying function on the independent variable x (input features) and y is the outcome.\n\nHere, epsilon is the noise in the data that has a functional relation with the input. Lets also assume that epsilon has a zero-mean and a standard-variance.\n\nThe objective of the Neural Nets is to learn the underlying function f(x) to predict the outcomes. As you may guess, since there is a noisy functional relation \u2018epsilon\u2019 between the underlying function f(x) and the outcome, the probability of the Neural Nets to learn about the noise is high.\n\nThe trick of the trade is that, if the Neural Nets can learn about the noise \u2018epsilon\u2019 as a separate functional variant, then we can claim that the Neural Nets are balanced. Typically, Neural Nets models are unable to differentiate noise from the input and lands up learning about noise as part of the input function, f(x+epsilon). This is when the Neural Nets have overfitted. We also call this state as high-variance state. The opposite of a high-variance state is the high-bias state, where the Neural Nets are unable to come up with any learning at all (as in, the Neural Net is not able to find any relation between the input vector and the outcomes). Let\u2019s study this further\u2026\n\nLet\u2019s mathematically decompose the input function to understand the concept of Bias and Variance, given that there is noise in the signal.\n\nLet\u2019s say we train the Neural Nets to find a underlying function f_cap(x) that tries to approximate the underlying function f(x) while noise \u2018epsilon\u2019 is present. Now, we can say that the Neural Network has learnt well if the mean squared error between the expected outcome \u2018y\u2019 and the output of the learnt function f_cap(x) is minimal or near to zero. Note that it should be minimal not only for the training set, but also for any new data that is provided during validation.\n\nLet\u2019s define the concept of bias and variance now:\n\nBias: As we said, bias is an inability of the Neural Nets to learn any co-relation between the input vector and the output state. Mathematically it can be expressed as follows:\n\nI am using the < > notation to denote the expected-value here. If the expected value of the difference between the learnt-function and the underlying function is high, then we state that the learnt function is highly-biased.\n\nVariance: Variance is the sensitivity of the Neural Nets to small changes in the inputs. In other words, any minor noise in the input gets picked up by the learning functions of the model and tries to overfit the noise as if they are signal. This causes overfitting and produces poor accuracy during validation. Mathematically it can be expressed as follows:\n\nWe can also generalize the expected value of a variance function for any given random variable X as follows:\n\nWe said that the mean squared error between the outcome \u2018y\u2019 and the learnt-function f_cap(x) should be minimal or near zero. In other words, we said:\n\nIn order to decompose the mean-squared error to its Bias and Variance parts, lets work on the equation as follows:\n\nAnd hence, the above equation provides the relation between Variance and Bias for a mean-squared-error between the learnt-function and the signal.\n\nSo, whenever we say that our Neural Net models are overfitting, we are stating that there is a high variance in the learnt-function.\n\nOne of the ways to reduce overfitting is to actually add more noise while training Neural Nets!!\n\nIf a Neural Net is displaying high variance, it is possibly because it is overfitting \u201cf(x) + epsilon\u201d of the training-set nearly well. So when a validation-set during test is provided, the model does not know how to predict accurately.\n\nOne of the ways to break this is actually to add noise into f(x) so that the model is not able to co-relate the input vector {x1\u2026xn} tightly to its output class {y1\u2026.yn}. In other words, we are trying to reduce variance of the system by breaking its ability to tightly fit f_cap(x) to \u201cf(x) + epsilon\u201d.\n\nNote: For people who have used ensemble methods, this is NOT the same as bootstrapping in ensemble methods where multiple subsets of data is created from the same data-set using noise. Instead this concept is similar to Additive White Gaussian Noise, or AWGN.\n\nA controlled noise like Gaussian is good to modulate any noise that exists in the input data.\n\nHere, \u2018N\u2019 represents the Gaussian noise with zero-mean and Gaussian variance.\n\nMathematically, the effect can be understood as follows:\n\nNote that the sigma-square turns out to be Gaussian penalty on the cost function, and is very similar to a L2-penalty which you learnt from the post titled \u201cAlgorithms to improve Neural Network Accuracy\u201d."
    },
    {
        "url": "https://medium.com/autonomous-agents/committee-of-intelligent-machines-unity-in-diversity-of-neuralnetworks-8a6c494f089c?source=---------4",
        "title": "Committee of Intelligent Machines \u2014 Unity in Diversity of #NeuralNetworks",
        "text": "Have you noticed that the best fitness functions that most creatures adopt for survival is to work in collectives? School of fishes, Hive of bees, the nest of ants, horde of wildebeests or flock of birds all have something in common. They co-operate to survive. This is an example of Unity in Uniformity (when the same species act as a collective to form a fitness function)\n\nWhat is even more perplexing about nature is the ecological inter-dependence of different species, collectively surviving to see a better day. This fitness function is a sum of averages of sorts which enables a different form of collective strength. It\u2019s called Unity in Diversity. In essence, this signifies the \u2018unity without uniformity and diversity without fragmentation\u2019 when it comes to ecological fitness at any given time.\n\nHow do we apply this learning to AI? Specifically Neural Networks?\n\nIn the previous posts, we discovered how Neural Networks learn, and also observed that the models can begin overfitting the data. We learnt that the L1/L2 weight penalties, early stopping and weight constraints provide a mechanism to improve prediction accuracy while reducing overfitting.\n\nIn this post, we shall learn about better generalization functions based on how nature works, which is, Unity, or collaborative prediction. This is the first post in series and will establish the fundamentals of \u2018Committee of Machines\u2019 model.\n\nOne of the ways to reduce overfitting and improve prediction accuracy is to have a collection of Neural Networks (similar to nature) predicting the target value. The idea is to average the prediction errors in order to get a better prediction.\n\nAs shown in illustration, if each model predicts an output \u2018y\u2019 with an error distance d1 from the target (due to the absolute prediction value, or the weight of the ball, as a analogy), then it\u2019s possible to combine the variance in the distance to arrive at a better prediction.\n\nJust like in nature,\n\nThe idea behind combining various models is to reduce the variance in resulting prediction. We know that \u2018variance\u2019 is a function of the squared error distance from the mean prediction. The reason we look at squared error distance is because, we want predictions that are farther away from the target to produce a higher standard deviation and predictions which are closer to produce a lower standard deviation.\n\nSquaring moves the value of prediction errors into the realm of positive numbers (to the right of the target).\n\nThe illustration gives the idea that when you have more than one model trained on the same dataset used to predict the outcomes, the squared error distance shall produce, on average, a better output than any individual output.\n\nYou might wonder, why we are not retaining the model that is closer to the target and throwing the rest of the models away? Note that the same model can produce different errors in prediction for different inputs. The model may produce better results for some inputs more accurately than others. Similarly, you shall have other models producing different predictions for the same input. The idea is to get, on average, good predictions for all inputs.\n\nThe prediction vector for all inputs for a given model may be pointing to different directions in the resultant vector sub-space. The combination of all the directions of the prediction vector across all models achieve equilibrium by combining the average predictions as illustrated:\n\nTo find the average prediction, we apply the variance function as follows:\n\nThe above equation can be used as a error function to train the models. This concept is called co-operative error optimization.\n\nNote that we are comparing the average of all predictions with the target value to improve accuracy of prediction. This helps reduce variance and improve model accuracy.\n\nBut, this has a bad side-effect though. As the models co-operate to improve the average prediction, the equation lands up overfitting higher on average as well. This is because, we are taking the combined squared error over \u2018all models collectively\u2019. So this is not a good outcome for reducing overfitting.\n\nInstead, there is a better approach to improve generalization while improving accuracy. The concept is called a \u201cMixture of Experts\u201d.\n\nA better approach for improving prediction accuracy while improving model generalization is based on the concept of specialization. Instead of comparing the average of all predictors with a target value as a cost function, Hinton et al, proposed a better model where each predictor is compared individually and separately with the target value and average-out the \u2018probability\u2019 of selecting a expert based on the \u2018class\u2019 of input.\n\nOne of the ways to classify an input before prediction, is to use the internal representation of the input vector and cluster them based on \u2018un-supervised' learning. There are several techniques to cluster the input data as follows:\n\nOnce a class of the input is obtained, we can keep track of the probability of accuracy of a model participating in the \u2018Committee of Machines\u2019 and come up with a better cost function that can be used to train a Neural Net that is participative in the Committee.\n\nThe setup for a Committee of Machines looks as follows:\n\nHere, P1, P2, P3 are the probabilities assigned to the input class. The number of probabilities are equal to the number of experts. An expert can be any machine learning model that can predict the target value, given the input feature vector. Subsequently, y1, y2, y3 are the resultant prediction.\n\nThe manger is used for allocating the input vector based on its class to different specialized expert (a machine learning model) and is done by the softmax gating network. A softmax gating network can be a Neural Network which simply takes the raw inputs and spits out \u2019N\u2019 probabilities based on the \u2019N\u2019 experts we have. Note that N is just the number of experts and is not co-related to the experts in any other way.\n\nThe softmax function is a normalized exponential function that squashes a k-dimensional vector to a vector with real values between {0 and 1} which adds upto 1, and hence is a good gating function to identify the internal representation of the input vector and come up with a probability distribution over N.\n\nThe base loss function is as follows:\n\nNow, if you need a signal to train each expert, we have to differentiate the error w.r.t the output of each expert as follows:\n\nIf you need a signal to train the softmax gating network (which is supervised learning Neural Network), we have to differentiate the error w.r.t the output of the softmax gating network as follows:\n\nIn fact, Hinton et al > here, proposes a better way of coming up with the probability distribution for predicting the target value. They state that, we can think of the prediction made by each expert as a Gaussian distribution around their outputs with unit-variance. In that case, they propose:\n\nThen, the probability of a target can be thought of:\n\nIn fact, drop-outs as regularizers works on a similar concept as Committee of Machines. Instead of using different network for predicting the target value, it is proposed that for each training example:"
    },
    {
        "url": "https://medium.com/autonomous-agents/part-2-error-analysis-the-wild-west-algorithms-to-improve-neuralnetwork-accuracy-6121569e66a5?source=---------5",
        "title": "Part-2: Error Analysis \u2014 The Wild West. Algorithms to Improve #NeuralNetwork Accuracy.",
        "text": "Wyatt Earp was the most famous lawman in the Wild West who is glorified beyond means for his abilities as a fearless gunman. He may not have been the quickest draw in the west, but was the most deadliest of his times. It has been stated that he often used to quote:\n\nNeural Net training is a bit like the wild west. The errors are quite lawless and unhinged. They can behave erratically without rules, rhyme or reason. The best way to arrest and stabilize a model is to get a hang on accuracy first as against trying to focus on training speed.\n\nIn the previous post \u201cPart-1: Error Analysis\u2026\u201d we learnt about different components of error and the respective error scoring methods. In this post, we are going to learn on how to improve upon these errors. Specifically, we are going to focus on accuracy.\n\nWe learnt that accuracy is a proximity of the predicted values to its true values. The equation for accuracy is:\n\nAs per the equation, the way to increase accuracy is to increase the true-positives and true-negatives over the total population.\n\nNote that the sum of total population is a given. As in, you do not have much wiggle room to reduce the total population by only training your Neural Nets on \u2018Relevant\u2019 items (or the positive set of a feature). Especially in a multivariate binary classification, the number of not-relevant items shall always be higher than the number of relevant items.\n\nIn other words, In the wine dataset, if you have 3 mutually exclusive class into which a wine can be classified, then the positive set shall always be 1 and the negative set shall always be 2 per wine (One of three classes is positive, 2 of 3 classes are negative). The negative set only increase as the number of classes increase for mutually exclusive multivariate binary classification.\n\nSo the only way to increase accuracy is to increase the true-positives and the true-negatives.\n\nIn the previous post, we analyzed the accuracy measure during validation (on the validation-set, not during training), after the network was fully trained. This is not a great place for error analysis if we do not know what was the accuracy of the network on the training set.\n\nLet me alter the wine tasting example a bit to walk you through the new observations.\n\nFirst I shall reduce the number of epochs to 25 and 1 iteration per epoch as follows:\n\nSecond, I shall comment out a important section of the code where I was using a regularizer (which I shall explain shortly) and momentum-value as follows:\n\nNow, I shall use a dip-stick to evaluate the wine tasting Neural Net model on 2 things.\n\nThe code for the changes is as follows:\n\nWhat I am doing here is print two different evaluation measures, one measure shall display the accuracy of the model on the data it was recently trained.\n\nThe other shall display the accuracy for a validation data-set that was NOT used to train the model. The idea of prediction is to predict good results on the NEW set of data which was NOT foreseen during training. (What good is a model otherwise?)\n\nThe results are as follows:\n\nNotice that the accuracy of the model to predict the data that was seen during training is far higher (0.9304) than its ability to predict data for not foreseen data (0.6508) !! Now, you are wondering, what the heck !! Right?\n\nIt\u2019s simple. The network is \u201cMemorizing\u201d the data from the training set here. So whenever you run a sample prediction on the training set, the network performs well (from memory) but it is not able to accurately predict new dataset from the validation set.\n\nTo understand, overfitting, let\u2019s look at the following depiction:\n\nThe first graph depicts a \u201cLinear Regression\u201d or a function that is asked to learn the total weights of all the points in the subspace and draw a best fit \u201cline\u201d. Of-course, we know that linear regression models are not great when it comes to multivariate binary classification problems. It\u2019s only used as a reference graph for explanation. So this graph is useless beyond showing us a hyperplane division of the data cluster (don\u2019t worry about hyperplanes for now).\n\nThe third graph depicts a polynomial function, that fits all the data points perfectly. This curve has memorized the underlying data-points as against learning anything about the underlying function of the subspace.\n\nThis curve is nearly useless in predicting where to plot a new data-point for a new input (x axis) in the feature subspace. Does the curve extend upwards? Should it drop back down? Should it plateau? Nothing. Zilch. This curve is overfitted.\n\nThe second graph is more interesting and intuitive. It seems to have learnt the underlying function of the subspace as against trying to memorize where the data-points are ! So given a feature input (on x-axis), we can intuitively state that the next data point as x-progresses can be plotted in the direction (upwards) as the function climbs. This is very useful for prediction.\n\nThe best possible way to identify overfitting in your Neural Net models is to plot a graph of accuracy for your training-set and validation-set data over number of iterations for the model.\n\nIf the validation set is not able to catchup with the training-set as illustrated, then you have a overfitted model.\n\nThe technique to break overfitting or memorization of the network is called \u201cgeneralization\u201d\n\nThere are umpteen number of ways to improve generalization of the Neural Nets. While I had provided a high level overview of generalization in the previous post titled \u201cIs optimizing your Neural Net a dark art\u201d which provides some key techniques, I shall focus on Weight Penalties, Early Stopping and using Weight Constraints as a generalizer.\n\nOne of the techniques used to generalize a Neural Net is to regularize. Regularization is function introduced to the loss function of the Neural Net. We add a Regularization term R(f) to the loss function to prevent the coefficients to fit perfectly.\n\nWe can decay the weights either by using a L1 or L2 regularization term added to the loss function. While an L1 regularization decays the absolute value of the weight, the L2 regularizer decays the squared weights.\n\nHere, we have added a weight penalty on the absolute value of the weight to the cost function as illustrated. This can be broken down as follows:\n\nUsing L1 weight penalty, when cost is zero, the weight can get to zero. This way many weights can get to zero and introduce sparsity in the network. This limits from having many large weights. L1 regularization helps in regularizing the network from perfectly fitting the feature vector. Instead, the network learns the feature vector more generally.\n\nAs noticed, the L2 regularizer penalizes the squared weights. The idea here is to keep the weights small enough, but not to let it slip to a zero. This keeps the network dense (unlike L1 weights which introduces sparsity).\n\nIn L2 regularization, when cost is zero, the weight gets a very small value. The beauty of the L2 regularizer is that it smoothens the output by changing the outputs much more slowly as input grows. The main difference between a L1 and L2 regularizer is as follows:\n\nNotice, in the code, I have used a L2 regularizer with the lambda of 1e-4 as follows:\n\nHere is the output on the validation set after using L2 regularization in the cost function:\n\nHence, proved\u2026 (It helps to keep the lambda between 1e-2 to 1e-6).\n\nAnother technique is to stop as early as possible during training before the network starts memorizing the features. This keeps the network semi-trained and hopefully general as against memorization. To understand this, let\u2019s take a look at the error curves below\n\nThe y-axis is the prediction error, and the x-axis is the number of iterations. One of the ways to regularize a network is to \u201cvisualize\u201d the prediction errors on one of the error measures (Either accuracy, recall or the overall performance measure, the F1 score) and stop the training of a network at a particular iteration or a epoch when the error scores starts to degrade.\n\nThe visualization is a cumbersome method, instead, you can use a model measure tracker which can keep track of the validation error w.r.t the training error on every iteration (mini-batch) or epoch and compare the measure with the previous iteration. If the error continues to improve then you continue with the iteration. As soon as the error starts to degrade, you can terminate the learning.\n\nSince I am using DL4J in the examples, It is prudent to point to the DL4J documentation which has a nice write on Early Stopping > here\n\nThe other technique is to use a weight constraint on the weights as against using a weight decay in the cost functions.\n\nThe constraint can be set on how large the weight is allowed to go. Usually, the best weight constraint is to clip the weights to the length on the vector of the incoming weights. The equations is as follows:"
    },
    {
        "url": "https://medium.com/autonomous-agents/part-1-error-analysis-how-not-to-kill-your-puppy-with-neuralnetwork-66a766b6b406?source=---------6",
        "title": "Part-1: Error Analysis \u2014 How not to kill your Puppy with #NeuralNetwork.",
        "text": "Do you remember getting your first puppy (or kitten or baby) which started responding to your commands. Do you remember that the initial responses were just some random reactions? You tell the puppy to \u201csit\u201d and she is all curious, shaking her butt vigorously in the hope that we will play with her! You say \u201croll-over\u201d and she rotates in the same place chasing her own tail endlessly? (and, you face palm saying \u201cI said *roll*, not rotate\u201d)\n\nWell, welcome to training the Neural Networks.\n\nJust like puppies, you cannot cut open a Neural Network to see what is happening inside that black-box to fix it. If you land up visualizing the innards of the Neural Nets, you shall just see some nodes and connections and some numbers that are associated with it. There is no code, rules, labels, documentation or perceptible schematics that shall allow you to program it.\n\nIn this post, I intend to get a little more deeper in \u2018training ourselves\u2019, the humans, to better observe the behavior of the Neural Network, its error rates, its overall conditioning and change our training techniques to get a desired response from the Neural Network.\n\nBut in Part-1, lets focus on understanding the Error scores.\n\nBefore we start training the Machines, let\u2019s lay out some golden rules (or principles) for us humans first.\n\nWhy these rules and disciplines?\n\nTake a print out of the rules and stick this on your cubicle actually. Ok, enough of analogies, let\u2019s learn about the nature of errors that the Neural Nets commit.\n\nThere are many types of errors. Let\u2019s start with the simplest. Notice that in the wine tasting post, here, we saw the following output :\n\nLet\u2019s delve a bit deeper into understanding what the scores are.\n\nAs a recap, the wine tasting example had 3 outputs or classes into which the wine was classified. Given a set of 13 input features, the network choose the most probable cultivar from which the wine came from. The way the network choose a cultivar, was by turning ON or OFF a specific output neuron which was assigned to a cultivar.\n\nThe ON/OFF is considered as a binary state, and the process is called binary classification in statistics. The ON/OFF, 1/0, TRUE/FALSE is a predicted probability state based on some input, activities and threshold as we understood.\n\nIn the wine dataset, we had 178 different wines that was created by 3 different cultivars. We set aside 65% of that dataset which is about 115 wines for training the model. During training, we supervised the outputs to see if the correct cultivar is predicted, and if not, we backpropagated the error to fix the knowledge weights on the network to predict the correct weight on the next iteration. There, we used ESS (Sum of Squared Error) to keep the cost function in check during training. We spent 600 iterations to train the puppy.\n\nAfter full training was complete, we ran the remaining 35% (without backpropagation or any other network optimizations) to validate if the model is predicting correctly. Now, just like ESS was the cost function during training, we need some error mechanism for the validation phase as well. Accuracy, Precision, Recall and F1-scores are just that.\n\nLets first understand some base statistical errors which are components of the scores.\n\nFalse-positives (or Type-1 error): A false-positive is a error when your model wrongly identifies a cultivar as positive for a wine. In other words, if wine-1 is from cultivar-A, then the positive set for wine-1 shall be {cultivar-A} and negative set for wine-1 shall be {cultivar-B, cultivar-C}. When the model is asked to predict the cultivar for wine-1 and the model predicts a class from the negative set (either cultivar-B or C) then we state that we have a false-positive error.\n\nFalse-negative (or Type-2 error): When a model rejects a true positive as a negative, then its a false-negative or type-2 error. In the wine example, if cultivar-A for wine-1 (from the positive set) was not chosen (not predicted correctly), then the rejection of cultivar-A from the output is called a false-rejection or a false-negative.\n\nThe opposite of False-positive and False-negative, is True-positive (A correct cultivar for a wine is predicted) and True-negative (The in-correct cultivars for the wine is rejected).\n\nUsing these base statistics, lets understand the error scores in the validation set now. We have 13 wines from cultivar-A, 26 wines from cultivar-B, and 24 wines from cultivar-C (from the above illustration).\n\nLet\u2019s take a hypothetical scenario of the 13 wine predictions which came from cultivar-A as follows:\n\nWhile we must have 13 {1,0,0} predictions (because all the 13 wines came from cultivar-A), let\u2019s say we have the above predictions instead.\n\nThen, we have the following types of base measures identified:\n\nNow comes the scores.\n\nPrecision is a measure that shows, among all the items that are selected how many are relevant. It is also called the Positive Predictive Value or PPV.\n\nIn our scenario we have 13 predicted positives (Since in this example, we shall get a prediction for every input) and only 8 of them are true-positive. hence our precision score should be\n\nRecall is a measure which portrays among all the possible relevant items that should have been selected, how many are really selected. It is also called True Positive Rate or TPR sometimes also called sensitivity.\n\nIn our case, the relevant items is also 13 (This is because, we have a mutually-exclusive multivariate, and we are predicated a value for every input). So our recall score is as follows:\n\nAccuracy is a measure of proximity of predicted values to its true values. Statistically stated, its the distance between the true value as a reference to the mean of the probability density of the predicted value. The equation of Accuracy is as follows:\n\nThe total population is the sum of all the grid\u2019s that needs to be predicted for each wine, which in our case is 13 relevant classes for cultivar-A and 26 not relevant classes for cultivar-B + cultivar-C. Hence:\n\nHere is a illustration to show the difference between accuracy and precision\n\nPrecision is a measure of repeatability or reproducibility of the prediction. If the model is reproducing similar response to similar inputs each time you predict a response for your input, then its precise (it may not be a accurate prediction where the prediction is close to its true value). Here is another illustration for accuracy and precision :\n\nThe F1-score, F-score or the F-measure is a harmonic mean of precision and recall. It is a true measure of the overall performance of the model which considers both precision and recall. You can consider it as a weighted average of precision and recall.\n\nHence in our wine scenario it shall be:\n\nWell, the value of precision, recall and F1-score for a multivariate class, which are \u201cmutually exclusive\u201d and a \u201cprecisely-rounded\u201d where a prediction is available for every input shall all be equal.\n\nBut, in a Neural Network, the value shall not be \u201cprecisely-rounded\u201d to a set of {1,0,0} or {0,1,0} or {0,0,1} in a softmax activation. Instead the value is a probability as shown\n\nHere, the sum of the k-dimensional set adds upto 1 (as the result of the quashing function of the softmax) hence there shall be variance in precision, recall and f1-scores when you run the model. Technically speaking you may see results as following:\n\nNote in the above result, there are 2 wrong predictions denoted by the output \u201cExamples labeled as 1 classified by model as 0: 2 times\u201d\n\nWe should strive for the overall performance of the system to get better (F1-score).\n\nThere are cases or business needs where a high accuracy is demanded while they can be relaxed with precision. In such cases, you strive to improve the \u201crecall\u201d of the system.\n\nAn example can be, a medical diagnostic system which wants all people who have diseases to be predicted correctly by the system that they do indeed have a disease. Maybe, such a medical facility can tolerate false-positives (model predicting that a person has a disease while he does not) because, a drug administered to a normal person who does not have a disease is not as harmful versus not administering a drug to a person who truly had a disease, but the model predicting he does not, may turn out critical.\n\nHence, the choice of which score is more important for predictions is driven by the business case as against trying to improve the model in vacuum.\n\nThis post has established the base scores for analyzing the error on your validation test after the model is trained. In the next posts, we shall see how to improve upon the errors and other numerical conditioning of the Neural Network.\n\nRemember, its not the puppy you train\u2026"
    },
    {
        "url": "https://medium.com/autonomous-agents/how-to-train-your-neuralnetwork-for-wine-tasting-1b49e0adff3a?source=---------7",
        "title": "How to train your #NeuralNetwork for Wine tasting? \u2013 Autonomous Agents \u2014 #AI \u2013",
        "text": "Wine tasting is a fine art which enables classification of Wine. When it comes to classification of wine, the practice is quite varied based on region of origin and time. It is one of the most tasteful traditions which is also protected by law of its own in certain regions. The classification varies based on vintage, sweetness, appellation , vinification styles, varietal or blend.\n\nIs it possible to teach something about the classification of different variety of wines to Neural Networks? Well, I intend to do exactly that and get hands on with code as well.\n\nIn order to train a Neural Network on how to classify wine, we need to provide it a set of data labels that describes a variety of attributes that has real values for the attributes of wine. Since the training shall be supervised, we need to ensure the wine \u201cclasses\u201d are also part of the training data so that we \u201cteach\u201d the Neural Nets about the attributes.\n\nI shall use the UC Irvine Machine Learning repository, which has a wine dataset having 13 attributes based off a chemical analysis done for different wine in the Italy region. The wines are from 3 different cultivars and hence has distinct chemical compositions that helps identify which cultivar is the wine from.\n\nThe link to the wine data set is > here\n\nThe characteristics of the data set is as follows (reproduced from the UCI link)\n\nThe names of the attributes are as follows:\n\nThe data values, which are in a comma separated format for these attributes are available in the UCI data folder > here\n\nA sample of the data shall look like this:\n\nNote that the first value describes the \u201cclass\u201d of the data. In other words, it is the ID of the cultivar. For the sake of this exercise, we don\u2019t need the exact name or locality of the cultivar. We are fine with the data, as long as the value for the next 13 attributes are made available.\n\nNow, lets design the Neural Network, that shall taste the wine and classifiy the wine to the correct cultivar.\n\nWe need to determine the type of network, the number of input, hidden and output neurons and type of activation functions to use.\n\nWe shall use a standard Multilayer Feedforward Neural Network since this is good enough for classification tasks.\n\nFrom the dataset, we know that there are 13 attributes, so we shall use 13 input neurons for the network.\n\nAlso, the total number of cultivars are 3, so we shall use 3 output neurons for the network to point to. The output shall be mutually exclusive (Only one output neuron shall be activated, denoting the correct cultivar for the input value)\n\nWhy are we using 3 separate output neurons as against using 1 output neuron to learn the 3 different IDs? Well, because the IDs themselves don\u2019t have any real meaning. The value 1, 2, 3 can be replaced with 22, 23, 24 to represent the cultivar anytime and the dataset still holds. This is the characteristic of a \u201cMultivariate\u201d dataset.\n\nNow, how many hidden neurons are needed? Let\u2019s use our learnings from the previous post \u201cis Optimizing your Neural Network a Dark-Art?\u201d which provides us a thumb rule as follows:\n\nAs per the stated thumb rule we get:\n\nThe equation is self explanatory. I am using an \u2018alpha\u2019 of 2 since the inputs samples are low. Lets round up the hidden neurons needed as 6.\n\nBefore we design the activities of the network, its prudent to learn the characteristics of the input data. Without really understanding the data, there is no way you can design the activities of the Neural Net.\n\nNote: All the values for input and output in a Neural Network MUST always be numerical.\n\nSome of the characteristics of the input data can be:\n\nGiven this, its important to pre-process the input data. In the wine dataset, notice that you have real values ranging from 0.1 for some features and above 1000 for others. While all of this data is not correlated in its own, it is important in machine-learning to scale down the data to a meaningful number.\n\nSpecifically for Neural Network, the transfer potential is a inner-product of the input data to the weights. Since all input neurons is connected to every single hidden neuron, the number of fan-ins, and the value of the input neuron can get very large quickly. Even if you are not using a dot-product for your transfer potential function but decide to use some Radial Basis Function, the distance measure shall be very large (Euclidean distance) for the activation functions.\n\nAlso, if one of the features land up having a very broad range of value (Lets say a particular attribute has a value ranging from 10 to 10,000), then the rest of the distance of other variables will be governed by the particular feature (either Euclidian in case of a RBF or the vector length in case of a dot-product).\n\nIts important to scale down the range of all the features so that each feature contributes proportionally to the final distance (to the vector length or some other metric)\n\nAlso, note that in Neural Networks, since we use Gradient Descent to reduce the empirical error, scaling down the range helps the network converge faster.\n\nSo, what are some of the techniques for Feature Scaling?\n\nRescaling the feature : Rescaling the feature is a technique that sets the feature to a range between {-1, 1} based on the maximum value and the minimum value of the range within the feature. The Rescaling function is as follows:\n\nZero-mean, unit-variance : You can also standardize your feature using a standard-score by determining the mean of the distribution within the range of possible values and the standard deviation. This is typically the most popular method used in other machine learning techniques such as Logistic-Regression or Support Vector Machines where margins for decision-boundaries has to be calculated. The equation for Zero-mean, unit variance is as follows:\n\nUnit Length : Another technique is to reduce the feature range to a unit length (as against a unit-variance). Here, we normalize the length of the feature-range-vector to unit length 1. In other words, the range of features add up to 1. To do so, we divide each component value within the feature-range-vector by its Euclidean distance. The equation is as follows:\n\nHere is a snapshot of the maximum and minimum values of the range in our features :\n\nIn the raw data, we can notice that the Proline feature has large values above 1000, while we have other features which are in fractions (Hue, Malic Acid, Total Phenols). This is quite a variance. Also the range (variance) within Proline is ranging from 278 to 1680. This seems true for Magnesium and Color intensity as well.\n\nGiven this, its prudent that we choose Zero-mean, unit-variance to normalize the raw data to given us a standard-score to work with the data.\n\nNow that we have got the type and size of network, we need to determine the activation functions for the network. We looked at activation functions in the previous post titled \u201cMathematical Foundation for Activation Functions\u201d\n\nFrom the data, we understand that the data is multivariate classification with 3 classes. Also there are about 178 samples (this is not huge) and 13 input features (not big again).\n\nDesign of the Output Activities: For multivariate classification, we can choose the output layer activation function as softmax activation.\n\nWhy Softmax? : The output classifiers are multivariate and mutually exclusive in our example. In other words, the wine shall be classified into one and ONLY one class without a overlap.\n\nWe would need a mechanism to choose the right class with the highest degree of probability.\n\nIn other words, given a set of 13 input values, we need to determine the possibility (or probability) of the inputs to belong to either class 1, class 2, or class 3.\n\nIn other words, we are stating that the output is mutually exclusive and collectively exhaustive. Hence :\n\nHere A or B or C represents the cultivars.\n\nThe softmax function is a quashing function, which takes a k-dimensional vector of any arbitrary real-values and converts that to a k-dimensional vector of real values in the range {0,1} which sums totally to 1.\n\nThe softmax function is given as follows:\n\nThe softmax equation can be used in any probabilistic multiclass classification including multinomial regression.\n\nThe input activities has a choice of being a sigmoid, hyperbolic tan or a ReLU.\n\nThe sigmoid or the hyperbolic tanh has the tendency to quash the output of the hidden activity to a range between {0,1} or {-1,1} respectively. While this is useful, given that we are using a zero-mean-unit variance as a normalizer for the inputs and also given that we are using a softmax (as explained) for the output activity, it does not makes sense to restrict the hidden activity to a range. Instead, we may want to amplify the feature-differences in the input feature vector when its above a zero threshold so that the softmax gets to work with a linear scale when the input value is above zero.\n\nA non-linear function, which becomes linear post zero-threshold is your Rectified Linear units. The function of the ReLU is quite easy to differentiate and the activity by itself is quite light as follows:\n\nHence, we shall choose ReLU for the hidden activities.\n\nWe shall keep the Stochastic Gradient Descent (standard) for the empirical error reduction as explained > here\n\nWe shall also use the Negative Log Likelihood as our Cost function as explained > here\n\nWe shall use a momentum (Also called the Nestrov Momentum) for dampening the velocity of learning as explained (momentum value; \u201calpha\u201d) > here\n\nSo the backpropagation equation along with a learning rate and the momentum value shall look like this:\n\nGiven that we have considered the pre-processing, network type, size and optimization procedures, its time to get to syntax.\n\nThe full code for the classifier can be found here > WineClassfier.java\n\nThe code sets up a network as follows:\n\nThere are 13 input neurons ranging from {I1\u2026.I13}, 6 hidden neurons {H1\u2026H6}, and 3 output neurons {O1.. O3}. While, this is not the best picture to show the entirety of the architecture, you get the idea.\n\nThis section of code sets up the optimization constants\n\nThis section of code loads the training data\n\nI have changed the training data from the UCI site to replace the class in the first col that was tagged as 1, 2, and 3 for the wine cultivars to 0, 1, and 2. The meaning of the class will not change by changing the ID. DL4J seems to need the classes to start from zero as against 1. The file I am using is made available here > wine.data\n\nAlso, one important thing you must note in the code is that I am splitting the wine.data file into 2 sections (as shown in line : 55).\n\nI am using 65% of the file to train the Neural Network, while the remaining as test data for validation.\n\nAlso you can notice that in Line 60 to 63, I have normalized the raw data to a zero-mean, unit variance.\n\nThe following section of code sets up the Neural Network architecture\n\nI have used, Stochastic Gradient Descent for optimization, a learning rate Epsilon of 5% on the error derivative and a momentum value alpha of 10% on the previous error delta to be used by the backpropagation algorithm.\n\nThe hidden neuron gets a ReLU activation and the output neurons get a softmax activation. The cost function is a negative log likelihood.\n\nThe final sections of the code, sets up the training to run for 30 epochs with 20 iteration each (found through trial and error) as the minimum set of iterations needed to teach the Neural Network how to classify wine data.\n\nWhen your code finally runs, you should read an output as follows:\n\nThe final output actually uses the 35% of the wine.data file that was set aside to test the Neural Network. Notice that there were about 13 class zero data, 26 class 1 data and 24 class 2 data that was used.\n\nGood news is that every single test data that was used has been converged to a positive result.\n\nIt states, \u201cexamples labeled as 1 classified by model as 1: 26 times\u201d which means that what should be classified as class-1 input is actually classified correctly as class-1.\n\nHoorah ! We have trained the Neural Network with a tasteful tradition in just 600 iterations !! You guys did well so far.\n\nYou can change the optimization parameters and activities to play around with the Network Model to see differing results.\n\nDo shoot comments on any of my errata or questions you may have. Keep hacking\u2026"
    },
    {
        "url": "https://medium.com/autonomous-agents/how-to-teach-logic-to-your-neuralnetworks-116215c71a49?source=---------8",
        "title": "How to teach logic to your #NeuralNetworks ? \u2013 Autonomous Agents \u2014 #AI \u2013",
        "text": "Logic gates are the fundamental building blocks of electronics. They pretty much make up the complex architecture of all computing systems today. They help in addition, choice, negation and combination thereof to form complex circuits. In this post, I shall walk you through couple of basic logic gates to demonstrate how Neural Networks learn on their own without us having to manually code the if-then-else logic.\n\nLet\u2019s get started with the \u201cHello World\u201d of Neural Networks, which is the XOR gate. A XOR gate is a exclusive OR gate with two inputs A and B and an output. It\u2019s called an exclusive OR because, the gate returns a TRUE state as an output if and ONLY if one of the input states is true.\n\nWe know what are the inputs and what is the output expected. Given that we know what is the output expected, this becomes a \u201cSupervised Learning\u201d exercise for the Neural Networks.\n\nThere are several Neural Network frameworks out there. The most notable are the following:\n\nTensorFlow: TensorFlow is a open-source library for numerical computations by Google. The framework is quite generic and flexible to model any type of data flow graphs and computation. The current version has a full stack support for Python programmers (though there is a C++ API that is not as comprehensive).\n\nTheano: Another Python based open source framework for large scale machine learning and numerical computing. Focuses heavily on efficient large scale multi-dimensional array computation, using NumPy and GPU.\n\nKeras: Keras is a library written on top of TensorFlow and Theano, which claims to be a minimalist and modular library built for fast experimentation.\n\nTorch7: Torch is also a BSD open-source licensed framework for machine learning and neural networks. The original authors claim to have developed Torch from ground up with GPU (Hardware efficiency in mind). It is based on the LuaJIT scripting language.\n\nCaffe: A deep-learning framework focusing on model architecture configurability and speed. In fact they claim to be the fastest convnet architecture to date with a 1ms/image processing and 4ms/image learning speed. C++ is the preferred language.\n\nDL4J: Deeplearning4J is a Java based open-source framework which is gaining recent popularity among the Java crowd due to its integrated support for Hadoop and Spark. Many existing enterprises are heavily Java and Hadoop shop and hence DL4J seems to be a easy transition for enterprise. Also (like Redhat), they also have their commercial org, SkyMind which trains, distributes and support all your production class deployments.\n\nHere is a comparison sheet, for what it\u2019s worth that has other frameworks as well: Deep Learning Framework Comparison\n\nWhile I work on TensorFlow and have dabbled around on Caffe, I shall use DL4J for the posts for the following reasons:\n\nBefore you begin, you can setup DL4J by following instructions here.\n\nYou can find the full code for XOR Gate Example here > XORExample.java\n\nLet\u2019s walk through the code now.\n\nI have setup the truth table in the following section of code :\n\nThe input variable states that I have 4 records of 2 cols each.\n\nThe output variable (denoted as labels) states that I have 4 records of 1 col each.\n\nIn the line of code > input.putScalar(new int[] { 0, 0 }, 0); I am stating that for the {0,0} position load a value 0. In other words, in the 0'th row index and 0'th col index, load value 0.\n\nSimilarly: input.putScalar(new int[] { 0, 1 }, 0); states that load a value zero in 0'th row and 1'st col.\n\nThis shall load the following truth table into a Dataset ds.\n\nI intend to use a network with 2 input neurons, 4 hidden neurons and 1 output neuron for learning the XOR gate. The architecture shall look as follows:\n\nHere {I1, I2} are input neurons, {H1.. H4} are hidden neurons and O is a single output neuron which should learn to output either zero or 1 based on the input values. B is a bias neuron whose value shall be zero.\n\nThe hidden neurons are setup to have a sigmoidal activation function\n\nThe output neuron shall have a hardtanh activation function as follows:\n\nhardtanh has a harder threshold than tanh (and good for XOR) as follows: (Activation functions are explained > here)\n\nThe following set of code configures your Neural Network architecture.\n\nYou set up the layers and activation function as illustrated above. Key things to note from above are as follows:\n\nProbability is used to describe the future outcome given a fixed set of parameters. In other words, given a coin, and given a fixed parameter \u201chead\u201d, what is the probability of it being an outcome? there is a probability of 1 out of 2 outcomes. (When the coin is flipped, a \u201ctail\u201d maybe observed as the future event though)\n\nLikelihood is used after this data is found (the coin is already flipped and an outcome is visible), In other words, Likelihood is used to define the function of a parameter for a given outcome. If a coin was already flipped and an outcome is observed, which let\u2019s say is \u201ctail\u201d, then we ask, what was the underlying statistical process or function that caused this \u201ctail\u201d. In short, we ask, what is the likelihood that a \u201ctail\u201d occurred among all other possibilities. Likelihood can be defined as follow:\n\nLog-Likelihood (as per wiki states), for many applications, the natural logarithm of the likelihood function, called the log-likelihood, is more convenient to work with. Because the logarithm is a monotonically increasing function, the logarithm of a function achieves its maximum value at the same points as the function itself, and hence the log-likelihood can be used in place of the likelihood.\n\nThe key is that they are monotonically increasing and achieves a maximum value at the same points.\n\nTo find a best fit weight vector, when we are using negative log-likelihood as the cost function, we are trying to minimize the error which is the same as, maximizing the log-probability density of the expected output. (Note that Likelihood is pretty much the opposite of the probability, in colloquial sense)\n\nThe following line of code initializes the Network\n\nAnd in the following lines of code, we are checking to see what was the output when the network was not trained, the actual training of the network itself, and checking the output again after training.\n\nnet.fit(ds); is the line which trains the Neural Network model\n\nThe output of this code should look as follows:\n\nThis states that there are 12 free parameters in layer zero. This is because, we have 2 input neurons and 1 bias neurons connecting into 4 hidden neurons. That is 3 * 4.\n\nAlso, we have 4 hidden neuron + 1 additional bias neuron in the hidden layer connecting to 1 output neuron. So layer one shows 5 free parameters.\n\nFree parameters are nothing but connections for which weights needs to be found. Here, we have to find the correct weights for 17 free parameters.\n\nWe can notice that the output values of the truth table before we train is displayed as [0.64, 0.82, 0.85,0.99]. In other words, it can be interpreted as follows: (clearly this is random and the Neural Network is just in a initial state with no learning)\n\nSince we are using a ScoreIteratorListener which outputs the scores to the console, we can see the scores at batches of 100 (we set this value) once the training starts.\n\nAt the end of the training we notice another set of output [0.02, 1.00, 1.00, -0.02], which can be interpreted as follows:\n\nNotice that the output neuron has converged to the expected values just within 200 iterations of training for a network as small as having 17 free parameters.\n\nThis is the power of a Neural Network, where we can now just send different truth tables for training the exact architecture of the same Neural Network and it can pretty much learn different logic. Of course you need to tune the parameters to ensure the network is learning correctly through trial and error.\n\nHere is the code for AND Gate > ANDExample.java\n\nNotice that I changed the output activation function to sigmoid and the number of iterations to 350 for the AND gate to learn correctly. The output of the AND gate looks as follows:\n\nWhich can be interpreted as follows:\n\nCan you now code the behavior of the rest of the logic gates and post the link to your code as comments here?\n\nAlso shoot me questions if you missed out on understanding the concepts."
    },
    {
        "url": "https://medium.com/autonomous-agents/is-optimizing-your-ann-a-dark-art-79dda77d103?source=---------9",
        "title": "Is Optimizing your Neural Network a Dark Art ? \u2013 Autonomous Agents \u2014 #AI \u2013",
        "text": "Learning is a procedure where we are searching for the best-fit value for the knowledge weights of the ANN in order to minimize the global error. To do so, we found a mechanism to update the weights of the ANN using error derivates and gradient descent as follows: (note that this is a basic equation)\n\nWe established that the rate of change of error with respect to the hidden knowledge weights, is a function of the output of the activities of the network.\n\nThis is not as straight forward as it seems. There are several other optimization decisions that needs to be made before the learning can begin and/or is optimal. Questions such as:\n\nLet\u2019s break down each of this and see what techniques are available.\n\nWhile there are innumerable types of ANN, they can be broadly classified into the following:\n\nThis is the simplest type of ANN. Information moves in a single direction, from input layer to hidden layer to output layer. Typically they are single layered (not always) and good enough to encapsulate behavior to classify textual data. Activation functions are sigmoidal in nature and uses simple backpropagation for learning. You can use FFNN extremely well in large scale classification systems with noisy data.\n\nCNNs are a type of FFNN where information flows in single direction. CNNs are modeled after the visual cortex in animals. CNNs are multi-layered. Each layer of CNN has portions of neurons which process a focused portions of the image (which can overlap). They use techniques like tilting the image for further layers to process. This creates many different copies of the same image in different positions, which increases visual accuracy. This technique is also called replicated features apporach. CNNs are heavily used in computer vision and visual pattern recognition domains.\n\nUnlike FFNN, a RNN is bi-directional. Which means, information can flow back into neurons after activations. RNNs are memory based models (similar to any Linear Dynamical Models or Hidden Markov Models). They are quite powerful as they not only allow memory of past states, but also allow non-linear, dynamic, temporal behavior that can update the states. While RNNs are powerful, they are quite unstable relative to simple FFNNs. RNNs are used in applications where speech translations, signal processing, motor control, Natural Language Interface (NLI/NLP), or text prediction is needed.\n\nThere are inumerable variations of the above types of ANNs to custom fit specific applications. Also, the nature of activation functions used, transfer potential functions used, training and cost functions used produce different types of networks.\n\nThe size of the network is a function of the input and output expected. There are three main factors of input and output of an ANN.\n\nIf you want to train the ANN to recognize the picture of a cat, you have to determine the size of each cat picture (which should be a constant), and the total number of pictures used for training.\n\nThe size of the cat picture can be defined as the x and y pixel co-ordinates of the cat. Let\u2019s say we use a cat picture of size 200 * 200. Then we have 40K pixels that we are working with. 40k shall be the total number of input neurons needed that shall represent the state of every pixel. (The state is always represented as a real-valued number which can be the RGB value of the pixel)\n\nThe number of samples in the training set can be 500 cat pictures.\n\nNow, we need to determine what is the total number of classifications the cat should be categorized into. Do we just want a true/false output to state if the input picture is a cat or not? Or, do we want the output to be a specific breed of cat to distinguish between a Siamese cat and a Abyssinian cat?\n\nLet\u2019s say, I want a single output of true/false (0 or 1) for now. Hence the total number of output neuron shall be 1.\n\nNow that we have the input size and the output size, we can use a thumb rule as follows to determine the size of a simple FFNN.\n\nWe have the following equation:\n\nYou can use the scaling factor for tuning the network based on the complexity of the input domain.\n\nThere is no magic number to figure out the number of hidden units needed. Also note that the hidden units can be laid out in multiple layers (The input layer and output layer shall always be a single layer).\n\nTo choose the optimal size of the network, you can use couple of different approaches.\n\nComplex behaviors need very large networks nonetheless. Today, mid-2016, there is a practical hardware limit in how large a network can get. Google is meant to have systems with several billion connections.\n\nTrivia: Did you know the number of connections in a human brain can run to 100 trillion connections, connecting 100 billion neurons in such a compact space? That is 1000 times the number of stars in our galaxy. And, it runs on relatively lower energy compared to the artificial hardware behemoths by consuming somewhere between 300 to 1500 calories every day. A true marvel we humans are. The AI systems is quite far from getting such uber-powered hardware anytime soon.\n\nWe understood that training is a process that adjusts the weight of the connections in hidden neurons to a number optimal to exhibit behaviors that is expected of it.\n\nCan all weights be zero to start with? How do these weights start? If all weights starts with zero, then the transfer potential shall always be zero (from the following equation) and shall be useless.\n\nWhat if everything start with one? In this case, if hidden units have exactly the same incoming and outgoing weights, then they shall land up getting exactly the same gradient. Then, all units will have exactly the same gradient. Which is quite useless as they can never learn different features of the input.\n\nThen should we just start assigning a +1 to every connection starting from the first connection? as in, the first connection gets a weight 1, the second connection gets a weight 2, the third a weight 3 etc..? This also has potential problems. Note that every unit has several incoming connections and hence, the transfer potential can quickly become a very large number. Even small changes to the weight gradients in this case will over-shoot learning.\n\nSo, what did we learn? We said that, the weights cannot be the same number nor symmetrical. Also the weights should not be a large number.\n\nSo what should the weight initialization be? The weights should be a non-zero, random (non-symmetrical), small fraction (positive or negative) to get the network started. Typically, all ANNs are initialized this way.\n\nIt is observed that if 50% of the network contains a negative weight and 50% positive weight randomly with non zero real-value in range -1 to 1, then its a good place to start. If you take a weight histogram of 1000 random real-valued weights in range -1 to 1, you get to see a plot similar to the following:\n\nWhile, this is how the network starts with its weight, is this how the network ends up after full training? is there a pattern in how these number form after learning? do they shape-up in a particular way specific to the input? do they cluster in a particular manner?\n\nOver innumerable number of observations, it is found that, in-general, a fully trained Neural Net does NOT have their weights distributed equally (as shown in the start of the network). Instead, it is observed that a large number of weight is tightly clustered around the zero-mean while the rest of the numbers are spread out (This pattern can deem different based on the transfer potential and activation functions used. This pattern is true for most sigmoidal activations though).\n\nThe following illustration shows the histogram from a gaussian function which shows that by controlling the standard deviation, we can cluster the weights more closely to zero-mean during initializtion.\n\nSo, instead of spreading the weight range, randomly-equally, between the range -1 to 1, there are techniques to initialize them in a way that the network starts with a tight clustering around zero-mean. The reason to do this, is to improve the efficiency of the network to identify different features of the network from get-go, which helps in better training.\n\nHere are some sample weight initialization techniques: (To retain the sanity of reading-time of this post, I have purposefully kept the hints to high level. Will explain the techniques and merits in detail in future posts..)\n\nFor sigmoidal activation functions, we noticed that the curve thresholds at zero-mean and starts moving from a output value of 0.5 towards 1, as shown in following illustration (assuming that the weight is 1) :\n\nA better way to interpret the above illustration, is to understand that the activation function outputs a value 0.5 when the input value x is zero.\n\nWhat if we wanted to bias the network in such a way that the activation function outputs 0.5 when the value of x is 5? In other words, is there a way to bias the network to threshold up for input value 5?\n\nTo do so, you have to introduce a bias term to the \u201ctransfer potential\u201d as follows:\n\nThe parameter \u2018b\u2019 stands for bias. So if we need the network to threshold at around input value of 5, then we can introduce a bias value of -5 (Since this is sigmoidal, its intuitive)\n\nIn other words, the network would like as shown:\n\nNow the output of the sigmoidal activation function would look as illustrated:\n\nNotice that with a negative bias, we have effectively shifted the sigmoid to the right. Similarly, if you want to shift the sigmoid to the left, then you can introduce a positive bias as follows:\n\nIn the above illustration, notice how the curve switches over and thresholds for x = -5.\n\nIn effect you are biasing the activities of the neurons to behave in a particular manner. Here is a collective illustration of all curves plotted on the same graph to emphasize the bias.\n\nSince training a ANN is a resource intensive activity, one of the questions that comes to mind is, do we have to update the weights everytime for a single pass of the input? If we have a 1000 pictures of cat:\n\nAnother question that plagues the optimization is how much of the error derivative should we use to update the weights? This is a important question. If we keep applying the full derivative to the weights as shown\n\nthen, the full error derivates applied as-is might be introducing large step-jumps. The network may not converge quickly and the error may randomly oscillate and in worst cases many not converge at all, as illustrated:\n\nInstead, we can control the scale of learning and the velocity of change.\n\nThere are two specific parameters that are used to do so:\n\nThe new updated equation looks as follows:\n\nThere are other techniques for better backpropagation as well as follows:\n\nANNs are very powerful models to train high-dimensional data. ANN performs well over other statistical machine learning models because of its ability to work with high-dimensional, noisy data.\n\nDue to this, there are some pitfalls to these models that trains on high-dimensional data. one of the major pitfall is called overfitting.\n\nTo understand overfitting, in the case of the 1000 cats picture being used for training, if most of the cats were brown in color and only had a front profile of the cat, then the system pretty much learns to only recognize front-profile-of-brown-cats as cats and may provide low confidence values for other cat pictures. The reason this happens is because, the model is almost \u201cmemorizing\u201d the pictures as against \u201clearning\u201d the features of the cat. Now if you add-in ambient noise into the pictures like trees in the background or window sills, this gets to memorize only cats on trees as cats etc..\n\nTo reduce overfitting, its important to regularize the learning in such a way that the model can in-general recognize all cat pictures. This is also called generalization. The following are different techniques to regularize the learning\n\nTo conclude, we learnt about different issues during optimizing an ANN and some techniques to avoid the issues. While these are only indicative issues and base techniques, the entire work of getting results from your ANN and AI systems relies in your ability to tune every aspect of the system to work optimally. There are tons of techniques to do so.\n\nIn effect, tuning such complex systems is part science, part art, part black-magic and part luck and need a whole lot of patience (and resource) for large systems. Happy Tuning."
    },
    {
        "url": "https://medium.com/autonomous-agents/backpropagation-how-neural-networks-learn-complex-behaviors-9572ac161670",
        "title": "Backpropagation \u2014 How Neural Networks Learn Complex Behaviors",
        "text": "Learning is the most important ability and attribute of a Intelligent System. A system which acquires knowledge by experience, trial-and-error or through coaching, exhibits early traces of intelligence. This post explains how ANNs learn.\n\nIn the previous post, \u2018Layman\u2019s Intro to AI\u2019, we explored a simple analogy of how a Artificial Neural Network or ANN gains to understand the \u2018knowledge weight\u2019 of a Cat (or what we termed as the \u2018catiness\u2019).\n\nWe said, the best fit arithmetic analogy of a Neural Network is in the following equation (which is a oversimplified lie btw):\n\nWhere, \u2018E\u2019 is the error which should tend to zero\n\n\u2018w\u2019 is the knowledge weight that the network needs to learn (about the Catiness of a Cat)\n\nand \u2018y\u2019 is the output expected (Which in our case was the classification \u201cCat\u201d)\n\nThe \u2018*\u2019 operator is a function called the Activation Function, which was introduced in the post titled \u201cMathematical foundation for Activation Functions\u201d. This post will now look at the \u2018minus\u2019 operator (as an analogy again) which encapsulates the Loss (or Cost)and Learning Function of Neural Network.\n\nIn the simplified equation E=(x*w)-y, we said we do not know what the weight \u2018w\u2019 needs to be. Instead we started taking guess works at \u2018w\u2019 to reduce the error E to zero. Any value for \u2018w\u2019 that best fits the equation and reduces the error to zero will be considered as \u2018knowledge\u2019.\n\nHow do we now fit this analogy to a real ANN?\n\nLet\u2019s say, the network in the above illustration uses a Logistic Sigmoid as the activation function as shown below\n\nThe logit \u2018\ud835\udec9\u2019 in the above activation function is the transfer potential function as shown below\n\nSo the \u2018*\u2019 operator is a Activation Function as follows:\n\nHere y\u2019 (y-prime) is the output of the activation function.\n\nSo, the arithmetic analogy of E=(x*w)-y can be replaced with E=y\u2019-y\n\nWhy do I use logistic sigmoid in this example?\n\nHere is an illustration of the logistic sigmoid from wolframalpha. Here, I have plotted the logit between -10 and 10. Notice how the curve thresholds at 0.5 and switches over. This is quite intuitive.\n\nSo far, so good. But, how do we calculate the error? Enter the Loss Function\u2026\n\nTraining on ANN happens in iterations. Let\u2019s consider, the iteration as a single forward pass of input vector to the hidden units to the output of the neural network.\n\nWhenever we see a Error deviation in this single iteration, it\u2019s considered as a local error.\n\nThe equation E=y\u2019-y is actually a true representation of a local error which is a standard linear error, where y\u2019 is the output from the activation function and y is the actual expected output.\n\nTraining can also be done in batches. For example, if we have 500 pictures of cats used for training, we can set a batch size of 250. Which means we shall send 250 pictures of cat, one after another and capture all local errors for each picture and aggregate that to a global error. Here, we are running 2-iterations of batch size 250. The global error is calculated for each iteration.\n\nThe global errors can be aggregated using any of the following techniques (But, not limited to).\n\nNow, we know what the error is. How do we tell ANN that what it produced was not the answer expected? (This is the concept of supervised learning where we know what to expect). Enter Backpropagation\u2026\n\nBackpropagation is a powerful training tool used by most ANNs to learn the knowledge weights of the hidden units. By now, we have the activation function and we have the loss function. What we do not know is how to change the hidden units, or particulary the knowledge weights \u2018w\u2019 of the hidden unit in such a way that the error reduces to zero.\n\nIn a multi-layered ANN, every unit of neuron affects many output units in the next layer. Let\u2019s consider the illustration below.\n\nHere, each hidden activity in the layer \u2018i\u2019 can affect many outputs in the layer \u2018j\u2019 and hence shall have many different errors. It is prudent to combine these errors. The idea behind combining the different errors is to compute the rate of change of error for all the units at the same time for every iteration.\n\nThe rate of change is nothing but the error derivative of the units. (At this point, I would advise you brush up your calculus a bit)\n\nThe error derivative we are planning to find is a Gradient Descent function. In other words, we are trying to find a way to reduce the error to zero in every step of the iteration (and hence a \u201cdescent\u201d in gradient as a function).\n\nIn order to truly understand backpropagation, we need to understand 2 simple truths,\n\nIn the above illustration, we have:\n\nIn order to compute the rate of change of error E with respect to weight w(ij), we must compute in the following order."
    },
    {
        "url": "https://medium.com/autonomous-agents/mathematical-foundation-for-activation-functions-in-artificial-neural-networks-a51c9dd7c089",
        "title": "Mathematical foundation for Activation Functions in Artificial Neural Networks",
        "text": "The foundation of Artificial Neural Net or ANN is based on copying and simplifying the structure of the brain. Like the brain, ANN is made of multiple nodes called the neurons which are all connected to each other in varying proportions, similar to synapses.\n\n[If you are just beginning, I have explained the basic premise of how ANNs work at a very high level in the previous post]\n\nThe connection between the neurons have a weight to represent the strength of the connection. The weights models the synapses in the real brain which links the neurons.\n\nPositive weights are used to excite other neurons in the network and negative weights are used to inhibit other neurons.\n\nAs the illustration shows, A simple Feed Forward Neural Network (FFNN) has a layer of input neurons, a layer of hidden neurons and a layer of output neurons. It is called Feed Forward due to the direction of the arrows where the connection and weights represents only one way from input to hidden to output layer.\n\nThe structure of ANN can be broken down to it\u2019s Architecture (or topology of how the network is structured), the Activities (Or how one neuron responds to another to produce complex behavior) and the Learning Rule (or how the weights of the connections changes over time, w.r.t input, output and error).\n\nThere can be many hidden layers in the Architecture of ANN which makes it deep. This is also called Deep Neural Network and is the premise of all things DeepLearning.\n\nThe most important aspect of ANN lies in its Activities also called it\u2019s Activation Function. The objective of an Activation Function is to introduce non-linearity into the network. Note that only non-linear activation-functions are used in ANN. Without non-linearity, a Neural Net is useless to produce complex behavior. The output of a linear activation function is also linear, which is not qualitatively helpful. A linear activation function dampens the effects of a deep network topology which reduces the whole network to a single layer (even if the topology has deep architecture)\n\nThe basic idea of connectionism is to use simple neuron units which interconnect with each other and produce complex behavior. Without non-linearity, you shall not be able to achieve this complexity.\n\nConsider the above illustration in which, there are many input neurons { x1, x2.. xn } which are all connected to different hidden units {y1, y2,\u2026 yn }. Each input neuron, is connected to every hidden unit, and the connection between each input unit to a hidden unit has a connection weight Wij where \u2018i\u2019 is the input unit and \u2018j\u2019 is the hidden unit.\n\nLet\u2019s zoom in and expand the relationship a bit further to understand how the activation function is applied.\n\nThe above illustration provides a view of a single hidden unit, which is getting its inputs from multiple input units. You can notice that there are 3 specific functions introduced.\n\nThe transfer potential can be a simple summation function which is a sum of inner dot products of the input to the weights of the connection.\n\nTypically, the transfer potential is mostly a inner dot product as illustrated above, but it can be anything. For example, it can be a Radial Basis Function (RBF) like a Gaussian. It can also be Multiquadratics or Inverse-Multiquadratics function.\n\nThe activation function should be any differentiable, non-linear function. It needs to be differentiable so that the learning functions can find an error gradient (will explain in later posts), and it has to be non-linear to gain complex behavior from the neural net.\n\nTypically the activation functions used is a logistic sigmoid as follows:\n\nwhere, theta is the \u201clogit\u201d which is equal to the transfer potential function as follows:\n\nThe overall network with multiple layers feeding-forward the non-linearity to encapsulate complex behavior from simpler neural units looks as illustrated:\n\nThe idea to model the network this way is a direct reflection of the activities of the brain where neurons communicate with each other by firing or activating each other through its action potential. The activation function simulates the \u201cspike train\u201d of the brain\u2019s action potential. This is quite mesmerizing, intuitive and simple to understand.\n\nActivation functions are NOT limited to a logistic sigmoid. They can be any of the following:\n\nAs each layer feeds to the next, the final output activates one or several of the output neurons to denote the end of the \u201cspike train\u201d. The output neurons that are activated then proposes the final answer to a pattern recognition problem, classification problem, anomaly detection problem etc.. Typically during training the output is checked for accuracy and the error is converted to some delta to the weights and fed back.\n\nThe learning of the error is the cost function of the network and feeding back is called backpropagation. Will introduce these concepts in next set of posts.\n\nIn conclusion, the choice of which activation function to use is completely dependent on the network architecture, the topological structure of the input feature vector, learning functions, cost functions and how the learning is optimized in the neural network."
    },
    {
        "url": "https://medium.com/autonomous-agents/emergence-of-the-artificial-neural-networks-among-other-models-cd93f6c97532",
        "title": "Emergence of the Artificial Neural Networks among other Models",
        "text": "Among all models of AI systems, Artificial Neural Networks (ANNs for short) is gaining prominence in learning and codifying complex behaviors. ANNs are modeled after the substrate structure of the brain, in the hope of getting closer to do things that the brain does well. Tasks such as speech and vision heuristics are done fairly effectively by the brain and hence ANNs should also do well with such tasks. In this logic, ANNs should also fall short in places where Brains are fairly weak, such as precise calculation of complex numbers.\n\nBefore we get to ANNs, there are broadly about 5 differing branches through which AI can be modeled. (If you read the book \u201cThe Master Algorithm\u201d by Pedro Domingos, he calls them as the 5 tribes.)\n\nDue to its connectionist approach where emergent processes for complex behavior is codified through interconnection of simpler units, ANNs are quite simple to implement. Also the main tool of ANN is the back-propogation, which is based on propagating information back into the interconnected simpler units to model correct behavior. Back-Propagation is quite intuitive to implement.\n\nAnother important reason why ANNs are prominent is that, ANNs can accommodate techniques and theories from all other models quite well into the Connectionist model. A full Bayesian model can be used for interpreting weight decays. Markov Chain Monte Carlo Methods can be used for better sampling methods. Genetic Algorithms can be used for efficient pre-training. Evolutionary Programming can be used in clever Activation Functions. Support Vector Machines is a variation on Neural Net model which can be easily codified as ANNs.\n\nGiven this, you get to hear a lot about ANNs in the form of Deep Neural Networks, Convolutional Neural Networks, Recurrent Neural Networks etc.\n\nIts important to conclude that usage of a model is mainly based on the complexity of the problems, topology of information within the domain and accuracy of the solution expected. While ANNs does Pattern Recognition, Classification or Anomaly Detection tasks quite well, you may not want to use them for Regression based solutions."
    },
    {
        "url": "https://medium.com/autonomous-agents/laymans-intro-to-ai-and-neural-networks-ce074457d85a",
        "title": "Layman\u2019s Intro to #AI and Neural Networks \u2013 Autonomous Agents \u2014 #AI \u2013",
        "text": "Simply put, any algorithm that has the ability to learn on its own, given a set of data, without having to program the rules of the domain explicitly, falls under the ambit of Machine Learning. This is different from Data Analytics or Expert systems where, rules, logic, propositions or activities has to be manually coded by an expert programmer.\n\nSystems which has ability to learn on its own and progress towards a pre-defined goal, without much of human intervention can be broadly termed as Intelligent Systems. The quality of intelligence can range from an amoeba, algae, ant, armadillo all the way to chimps, humans or beyond.\n\nAs an example, systems which interact with humans in natural language cannot be built by coding the rules and conversational logic of human language. The rule base cannot accommodate the nuances and vastness of conversations that is possible. Or, systems that view a picture of a bird and catalog them into different categories cannot be programmed for all possible variations, quality of image, features of the bird, angle of photography, lighting, shadows, noise in the image etc..\n\nAI or Artificial Intelligence is a specialized branch of Machine Learning and you can notice people use this interchangeably sometimes. Machine Learning is a broader set of algorithms which may not necessarily be considered as AI. AI can be broadly grouped into Artificial Narrow Intelligence (ANI), Artificial General Intelligence (AGI) and Artificial Super Intelligence (ASI).\n\nThe current (advanced) crop of algorithms you constantly hear in news and articles are all in the early stages of Artificial Narrow Intelligence or ANI. ANI is also termed as weak or lite AI. The objective of ANIs is to take a very narrow goal and improve the quality of results by accelerated, autonomous learning in order to improve accuracy while reducing costs. Goals can be to drive a car, play chess or Go, find an anomaly in trading, optimize the search relevance etc. While ANIs have out-competed humans in most areas, it is limited by its domain specificity (AlphaGO cannot drive a car, even if you want to teach it).\n\nArtificial General Intelligence or AGI (Strong AI or the holy grail) is the ability of a system (or a collection of systems) to perform most activities that a Human can perform. We are talking about common sense activities which involves all senses of perception (vision, speech, hearing, touch..), understanding and judgement that any normal human is blessed with. Common sense such as understanding that something is \u2018vulgar\u2019 within the context of what is commonly shared by other humans in a given culture, locality or region (or, universality of what vulgar means). Ability to understand the good versus the bad in a ethical and moral way as an \u2018ideal\u2019 human should understand.\n\nArtificial Super Intelligence or ASI (Singularity, fairy dust and beyond), is the ability of the machines to have acquired senses and sentience beyond human capabilities. It is hard to comprehend the magical powers these machines shall acquire and the possible outcomes given such powers.\n\nIMO, when you read AI articles for year 2016 and 2017, you should equate that in your mind to ANI.\n\nThere has been strong camps around AI where one camp fears the rise of the machines or another camp embraces the progress. The fear is mainly around AGI and ASI, where it is hard to predict the outcome of what the machines will do, once they come to full senses. There are questions around survival of the human species upon a untoward accident (or glitch). The proponents, though are cautiously optimistic that the progress of technology is inevitable, independent of whether we \u2018will\u2019 it or not. Either ways, both the camps are putting in efforts to ensure that from ground-up, we are consciously encoding (or biasing) the machines with highest order of morality and ethics so as to avert \u2018evil\u2019 tendencies of the machine upon full senses.\n\nSeveral luminaries like Stephen Hawking, Bill Gates, Elon Musk has been warning us on the rise of the intelligent machines and urging all of us to work on safe-guards, starting right now. To know more, you can follow the work by OpenAI (an initiative to help build safe AI) here : Concrete AI Safety Problems.\n\nWhile, AGI and ASI is probably decades away, given the acceleration of technological systems, it is hard to predict the probability of arrival of AGI by end of this year (lucky accidents happen all the time).\n\nThat said, lite ANI is already quite pervasive, in your phones, at your service providers (telco), when you search for something, and probably in your homes if you have SmartHome kits installed.\n\nAn AI system needs Data. Large quantities of Data. And it needs training. Broadly, learning-systems can be classified to do the following activities:\n\nAlso these activities can be performed using the following learning techniques:\n\nTo understand how learning systems train, we need to glimpse into how kids learn. The very first time a toddler comes across a cat, we say, \u201cthat\u2019s a cat\u201d. every time the toddler sees a cat, we continue to say that it\u2019s a cat, until the toddler learns to recognize any cat (not necessarily a specific cat).\n\nWe have to train the AI systems in a similar fashion.\n\nLet\u2019s say I would like to build a model which recognizes the picture of cats. Let\u2019s also say that a picture of a cat is fed to the model as a pixel of X and Y coordinates. In other words a 100x200 pixel size picture of a cat. Let\u2019s say I have many different pictures of cats, say about a 1000 pictures.\n\nDuring training, we set aside 500 pictures of cats to train the system to recognize how a cat looks like. In other words, we send the input signal of cat\u2019s picture, and we also tell the model that we are sending the picture of a cat. (This is the idea behind supervised learning, where we tell the system what ouput is desired, and supervise the behavior of the systems to see if it can produce the output.)\n\nThe equation may look something like follows:\n\ny -> Output which in our case is \u201ccat\u201d\n\nw -> Is the \u201cknowledge weight\u201d that needs to be acquired or learnt on the cattiness of an image that makes it a cat.\n\nSo Output \u2018y\u2019 = (Some input picture \u2018x\u2019) and (some knowledge weight \u2018w\u2019 about the cattiness or composition of a cat)\n\nWe need to learn what \u2018w\u2019 is, as we don\u2019t know what makes a cat. Without AI, you have to encode a lot of rules into \u2018w\u2019 about all different variations of cat, how it looks from the side, from above, under different lighting conditions, different size of cats, colors of cat, cat pictures in different actions etc.. This is close to impossible\u2026\n\nWith AI modeling, the equation y= x * w can be considered as follows:\n\nLet\u2019s assume \u201c*\u201d as an operator is a known function that can be applied on x and w.\n\nif \u2018y\u2019 as an output is known during training (which is \u201ccat\u201d in our case), and \u2018x\u2019 as a input is given (which is the pixels of a cat image) then, can we learn about the cattiness \u2018w\u2019 through training?\n\nThe problem is, the \u2018*\u2019 operator is not a straight forward multiplication in the equation y = x * w. Hence there is no inverse for that operation that can be coded.\n\nInstead, the system can be built as follows: If \u201c*\u201d is a known function, then can we change the equation to the following ?\n\nE = (x * w) \u2014 y where E is a error, and we can find another function let\u2019s say equivalent to \u2018minus\u2019 that can be used.\n\nIn other words, (as a mathematical equivalent),\n\nif y = 20, x = 5, and w = is unknown, in the equation 20 = 5 * w, what we are trying to find out is the value for \u2018w\u2019.\n\n20 = 5 * w ; (what is the value of \u2018w\u2019?)\n\nSince w = 20 / 5 is not a possible operation as explained, can we arrive at:\n\nE = (5 * w) \u2014 20 where E tends to zero ? In other words, can we substitute the variable \u2018w\u2019 with different values in such a way that the Error \u2018E\u2019 is close to zero?\n\nThis is exactly how Neural Networks work. A Neural Net is a collection of nodes (called neurons) which are layered as input nodes, hidden nodes and output nodes. The input nodes can be the pixels from the cat image, the hidden nodes shall learn the \u2018knowledge weights\u2019 of the cattiness of a cat, and then one of the output node is selected based on the type of the cat.\n\nIn our model, the input pixels shall be fed through many input nodes {x1, x2, x3.. xn}, there shall be a output node \u2018y\u2019 for a particular type of cat and there are many hidden nodes with a weight \u2018w\u2019 assigned to the connection between the input node \u2018x\u2019 and the hidden node.\n\nSo in our model of equation E = (5 * w) \u2014 20,\n\nwe can take a first random guess at \u2018w\u2019 as 10, then we get\n\nSince E = 30 and is clearly greater than zero, we have to take a second guess.\n\nBy now, we have clear information that w = 10 resulted in 30 and w = -8 resulted in -60. So the value of w should lie between 10 and -8. We can repeat this exercise by reducing value from \u2018w\u2019 backwards or increasing \u2018w\u2019 from -8 forward until you arrive at 4.\n\nThe above is a over simplified example of the process. But the premise of how a neural network learns that the cattiness is a value 4 is a good enough analogy.\n\nThe \u2018*\u2019 operator in a neural network is called the Summation Function. The minus operator is called the Gradient Descent in order to minimize the error towards a minima. Each hidden node additionally contains a activation function which encodes the overall equation.\n\nBy now, you may have got the basic premise on how the neural networks or AI possibly works. The beauty of this techniques is that, as long as we can convert the inputs to a number, we can always learn the \u201cknowledge weight\u201d (another number) which shall automatically codify the knowledge without having to program any rules. The flip side, we will never know why the knowledge weight is a 4 for cattiness (as a analogy again). As long as the systems find the knowledge weights (which happens to be in fractions in real systems) that can recognize an input, we are good.\n\nRemember, that we set aside 500 pics of cats for training among the 1000 pics? The other 500 is used as a validation set after training to see if the model can now recognize the remaining 500 pics as cat after training. If there are large errors in validation, then the model is continuously tuned to perform until the error is contained. Once the model performs within a error range, it can be taken to production.\n\nAs stated, ANI has just started gaining prominence. I presume it shall take a decade before AGI can be achieved. But there is a whole host of research pending in ANI to automate many tasks around Sales, Journalism, IT Support, Law, Security, Medicine, Games, Operating Vehicles, Retail, eCommerce, Packaging, Manufacturing, Banking, Trading, Education, Governance etc\u2026 The world is waiting to get automated through such learning systems called ANI and the possibilities are endless before we even move towards AGI."
    }
]