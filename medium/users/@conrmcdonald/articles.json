[
    {
        "url": "https://towardsdatascience.com/up-and-running-with-pytorch-minibatching-dataloading-and-model-building-7c3fdacaca40?source=user_profile---------1----------------",
        "title": "Up and running with PyTorch \u2014 minibatching, dataloading and model building",
        "text": "I have now experimented with several deep learning frameworks \u2014 TensorFlow, Keras, MxNet \u2014 but, PyTorch has recently become my tool of choice. This isn\u2019t because I think it is objectively better than other frameworks, but more that it feels pythonic, intuitive, and better suited to my style of learning and experimenting.\n\nThis post provides a tour around PyTorch with a specific focus on building a simple neural network (multilayer perceptron) to separate (i.e. classify) two classes in some toy data. My goal is to introduce some of PyTorch\u2019s basic building blocks, whilst also highlighting how deep learning can be used to learn non-linear functions. All of the code for this post is this github repo. This is more of a practical post, if you are looking for a tour of the inner workings of PyTorch I strongly recommend this post.\n\nTo follow along make sure you have PyTorch installed on your machine. Note that I am using version 0.3.1.post2 The next version of pytorch (0.4) will introduce some breaking changes (read about them here). Also worth keeping an eye out for the release of PyTorch 1.0, which aims to be \u201cproduction ready\u201d \u2014 I\u2019m very excited for this!.\n\nThe learning task for this post will be a binary classification problem \u2014 classifying points in half moon shapes. This is a simple task for a deep learning model, but it serves to highlight their ability to learn complex, non-linear functions.\n\nFor example, if we use a logistic regression to classify this data look what happens:\n\nDespite applying a softmax transformation to the predicted outputs (squeezing predicted output logits to sum to 1), the logistic regression is linear in its parameters and, therefore, struggles to learn non-linear relationships. We could use a more advanced ML model for this task, such as a random forest, but then we wouldn\u2019t have an excuse to play around with a neural network!\n\nBefore building the model, we will first create a custom data pre-processor and loader. In this example, the transformer will simply transform X and y from numpy arrays to torch tensors. We will then use the dataloader class to handle how data is passed through the model.\n\nIn this instance we will set-up a mini-batch routine. This means that during each epoch the model will train on small subsets (batches) of the data \u2014 that is, it will update its weights with respect to the loss associated with each mini-batch. This is generally a better approach than training on the full dataset each epoch. It is also advisable to use smaller batches \u2014 though this is a hyper parameter so do experiment!\n\nThe standard approach to defining a deep learning model with PyTorch is to encapsulate the network in a class. I quite like this approach because it ensures that all layers, data, and methods are accessible from a single object. The purpose of the class is to define the architecture of the network and to manage the forward pass of the data through it.\n\nThe typical approach is to define layers as variables. In this case we define a single layer network. The nn.Linear function requires input and output size. In the first layer input size is the number the features in the input data which in our contrived example is two, out features is the number of neurons the hidden layer.\n\nThe input to the output layer is the number of neurons in the previous layer and the output is the number of targets in the data \u2014 in this case two.\n\nWe then define a class method to manage the flow of data through the network. Here we call the layers on the data and also use apply the activation (from torch.nn.functional) on the hidden layer. Finally, we apply a sigmoid transformation on the output to squeeze the predicted values to sum to one.\n\nNext we need to define how the model learns. First we instantiate a model object from the class, we\u2019ll call this . Next we define the cost function \u2013 in this case binary cross entropy \u2013 see my previous post on log loss for more information. Finally we define our optimiser, . The optimiser will be optimising parameters of our model, therefore, the argument is simply the model parameters.\n\nNow we are ready to train the model. We will do this for 50 epochs. I have commented the code block to make it clear what is happening at each stage. We set up a for loop to iterate over the data (epochs) and with each epoch we loop over the mini batches of X and y stored in , which we defined previously. During each of these loops we make the input and target torch (note this step will not be necessary in the next release of pytorch because and \u200b will be merged) and then specify the forward and backward pass.\n\nIn the forward pass we use to model to predict y given X, calculate the loss (and accuracy). To so so we just pass the data through the model. In the backward bass we backproprogate the loss through the model and updates the weights according to the learning rate.\n\nWe can see that the loss decreases rapidly (the volatility can be explained by the mini-batches), which means our model is working \u2014 awesome! You can also see the non-linear decision regions learned by the model.\n\nAlso, check out the decision regions learned by the model:\n\nIt is pretty clear that the neural network can learn the non-linear nature of the data!\n\nThe purpose of this post was to show how to get up and running defining neural networks with pytorch. The model defined here is very simple, but the intention is to highlight the building blocks. I also didn\u2019\u2019t run a testing loop so we have no idea about the models test performance. In my next post I\u2019ll define a more complex pytorch model so stay tuned!"
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef?source=user_profile---------2----------------",
        "title": "Machine learning fundamentals (II): Neural networks",
        "text": "In my previous post I outlined how machine learning works by demonstrating the central role that cost functions and gradient descent play in the learning process. This post builds on these concepts by exploring how neural networks and deep learning work. This post is light on explanation and heavy on code. The reason for this is that I cannot think of any way to elucidate the internal workings of a neural network more clearly that the incredible videos put together by three blue one brown \u2014 see the full playlist here.\n\nThese videos show how neural networks can be fed raw data \u2014 such as images of digits \u2014 and can output labels for these images with amazing accuracy. The videos highlight the underlying mathematics of neural networks in a very accessible way, meaning even those without a heavy math background can begin to understand what goes on underneath the hood in deep learning.\n\nThis post is intended as a \u201ccode-along\u201d supplement to these videos (full Tensorflow and Keras scripts are available at the end of the post). The purpose is to demonstrate how a neural network can be defined and executed in Tensorflow such that it can identify digits such as those shown above.\n\nTensorFlow (for those who do not know) is Google\u2019s deep learning library and while it is quite low-level (I typically use the higher-level Keras library for my deep learning projects), I think it is a great way to learn. This is simply because, although it does incredible amounts of magical things behind the scenes, it requires you (yes you!) to explicitly define the architecture of the NN. In doing so you\u2019ll gain a better understanding of how these networks work.\n\nNeural networks are mathematical and computational abstractions of biological processes that take place in the brain. Specifically, they loosely mimic the \u201cfiring\u201d of interconnected neutrons in response to stimuli \u2014 such as new incoming information. I don\u2019t find biological analogies particularly helpful for understanding neural networks, so I won\u2019t continue down this path.\n\nNeural networks work by computing weighted summations of input vectors that are then passed through non-linear activation functions, thereby creating a mapping from input to output via a non-linear transformation layer. The weights (represented by neutrons) in the transformation, or hidden, layer are iteratively adjusted such that they come to represent relationships in the data that map inputs to outputs.\n\nIn the first step we define the architecture of our network. We will create a four layer network comprised of one input layer, two hidden layers and one output layer. Note how the output from one layer is the input to the next. This model is quite simple as far as neural nets go, it is comprised of dense or fully connected layers, but is still quite powerful.\n\nThe input layer \u2014 also sometimes referred to as the visible layer \u2014 is the layer of the model that represents the data in its raw form. For example, for the digit classification task the visible layer is represented by numbers corresponding to pixel values.\n\nIn TensorFlow (all code is below) we need to create a placeholder variable to represent this input data, we will also create a placeholder variable for the correct label corresponding to each input. This effectively sets up the training data \u2014 the X values and y labels we will use to train the neural network.\n\nThe hidden layer(s) enable the neural network to create new representations of the input data that the model uses to learn complex and abstract relationships between the data and the labels. Each hidden layer is comprised of neurons, each representing a scalar value. This scalar value is used to compute a weighted sum of the input plus a bias (essentially y1 ~ wX + b) \u2014 creating a linear (or more specifically an affine) transformation.\n\nIn Tensorflow you have to explicitly define the variables for the weights and bias that comprise this layer. We do so by wrapping them in the tf.Variable function \u2014 these are wrapped as variables because the parameters will update as the model learns the weights and bias that best represent relationships in the data. We instantiate the weights with random values with very low variance, and fill the bias variable with zeros. We then define the matrix multiplication that takes place in the layer.\n\nThis transformation is then passed through an activation function, (here I am using ReLU or rectified linear units) to make make the output of the linear transformation non-linear. This allows the neural net to model complex non-linear relationships between input and output \u2014 check out Siraj Raval\u2019s excellent video explainer on activation functions here.\n\nThe output layer is the final layer in the model and, in this case, is size ten, one node for each label. We apply a softmax activation to this layer so that it outputs values between 0 and 1 across the final layer nodes \u2014 representing probabilities across the labels.\n\nNow that the neural net architecture is defined, we set the cost function and optimiser. For this task I use categorical cross entropy. I also define an accuracy measure that can be used to evaluate the model\u2019s performance. Finally, I set the optimiser as stochastic gradient descent and call its minimise method once it is instantiated.\n\nFinally, the model can be run \u2014 here 1000 iterations are run. On each iteration a mini-batch of the data is fed to the model, it makes predictions, computes the loss and through backpropogation, updates the weights and repeats the process.\n\n**from three blue one brown\u2019s \u201cBut, what is a neural network?\u201d video**\n\nThis simple(ish) model gets to around 95.5% accuracy on the test set, which isn\u2019t too bad, but could be a lot better. In the plots below you can see the accuracy and cost for each iteration of the model, one thing that clearly stands out is the discrepancy between the performance on the train set and performance on the test set.\n\nThis is indicative of overfitting \u2014 that is, the model is learning the training data too well, which is limiting its generalisability. We can handle overfitting using regularisation methods, which will be the subject of my next post.\n\nThank you for reading \ud83d\ude42\n\nP.S The full Tensorflow script can be found here and the same model defined in Keras here."
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220?source=user_profile---------3----------------",
        "title": "Machine learning fundamentals (I): Cost functions and gradient descent",
        "text": "In this post I\u2019ll use a simple linear regression model to explain two machine learning (ML) fundamentals; (1) cost functions and; (2) gradient descent. The linear regression isn\u2019t the most powerful model in the ML tool kit, but due to its familiarity and interpretability, it is still in widespread use in research and industry. Simply, linear regression is used to estimate linear relationships between continuous or/and categorical data and a continuous output variable \u2014 you can see an example of this in a previous post of mine https://conorsdatablog.wordpress.com/2017/09/02/a-quick-and-tidy-data-analysis/.\n\nAs I go through this post, I\u2019ll use X and y to refer to variables. If you prefer something more concrete (as I often do), you can imagine that y is sales, X is advertising spend and we want to estimate how advertising spend impacts sales. Visually, I\u2019ll show how a linear regression learns the best line to fit through this data:\n\nOne question that people often have when getting started in ML is:\n\n\u201cWhat does the machine (i.e. the statistical model) actually learn?\u201d\n\nThis will vary from model to model, but in simple terms the model learns a function f such that f(X) maps to y. Put differently, the model learns how to take X (i.e. features, or, more traditionally, independent variable(s)) in order to predict y (the target, response or more traditionally the dependent variable).\n\nIn the case of the simple linear regression (y ~ b0 + b1 * X where X is one column/variable) the model \u201clearns\u201d (read: estimates) two parameters;\n\nThe bias is the level of y when X is 0 (i.e. the value of sales when advertising spend is 0) and the slope is the rate of predicted increase or decrease in y for each unit increase in X (i.e. how much do sales increase per pound spent on advertising). Both parameters are scalars (single values).\n\nOnce the model learns these parameters they can be used to compute estimated values of y given new values of X. In other words, you can use these learned parameters to predict values of y when you don\u2019t know what y is \u2014 hey presto, a predictive model!\n\nThere are several ways to learn the parameters of a LR model, I will focus on the approach that best illustrates statistical learning; minimising a cost function.\n\nRemember that in ML, the focus is on learning from data. This is perhaps better illustrated using a simple analogy. As children we typically learn what is \u201cright\u201d or \u201cgood\u201d behaviour by being told NOT to do things or being punished for having done something we shouldn\u2019t. For example, you can imagine a four year-old sitting by a fire to keep warm, but not knowing the danger of fire, she puts her finger into it and gets burned. The next time she sits by the fire, she doesn\u2019t get burned, but she sits too close, gets too hot and has to move away. The third time she sits by the fire she finds the distance that keeps her warm without exposing her to any danger. In other words, through experience and feedback (getting burned, then getting too hot) the kid learns the optimal distance to sit from the fire. The heat from the fire in this example acts as a cost function.\n\nIn ML, cost functions are used to estimate how badly models are performing. Put simply, a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This cost function (you may also see this referred to as loss or error.) can be estimated by iteratively running the model to compare estimated predictions against \u201cground truth\u201d \u2014 the known values of y.\n\nThe objective of a ML model, therefore, is to find parameters, weights or a structure that minimises the cost function.\n\nNow that we know that models learn by minimizing a cost function, you may naturally wonder how the cost function is minimized \u2014 enter gradient descent. Gradient descent is an efficient optimization algorithm that attempts to find a local or global minima of a function.\n\nGradient descent enables a model to learn the gradient or direction that the model should take in order to reduce errors (differences between actual y and predicted y). Direction in the simple linear regression example refers to how the model parameters b0 and b1 should be tweaked or corrected to further reduce the cost function. As the model iterates, it gradually converges towards a minimum where further tweaks to the parameters produce little or zero changes in the loss \u2014 also referred to as convergence.\n\nThis process is integral (no calculus pun intended!) to the ML process, because it greatly expedites the learning process \u2014 you can think of it as a means of receiving corrective feedback on how to improve upon your previous performance. The alternative to the gradient descent process would be brute forcing a potentially infinite combination of parameters until the set that minimizes the cost are identified. For obvious reasons this isn\u2019t really feasible. Gradient descent, therefore, enables the learning process to make corrective updates to the learned estimates that move the model toward an optimal combination of parameters.\n\nTo observe learning in a linear regression, I will set the parameters b0 and b1 and will use a model to learn these parameters from the data. In other words, we know the ground truth of the relationship between X and y and can observe the model learning this relationship through iterative correction of the parameters in response to a cost (note: the code below is written in R).\n\nHere I define the bias and slope (equal to 4 and 3.5 respectively). I also add a column of ones to X (for the purposes of enabling matrix multiplication). I also add some Gaussian noise to y to mask the true parameters \u2014 i.e. create errors that are purely random. Now we have a dataframe with two variables, X and y, that appear to have a positive linear trend (as X increases values of y increase).\n\nNext I define the learning rate \u2014 this controls the size of the steps taken by each gradient. If this is too big, the model might miss the local minimum of the function. If it too small, the model will take a long time to converge (copy the code and try this out for yourself!). Theta stores the parameters b0 and b1, which are initialized with random values. n_iterations controls how many times the model will iterate and update values. Finally, I create some placeholders to catch the values of b0, b1 and the sum of squared errors (SSE) upon each iteration of the model (creating these placeholders avoids iteratively growing a vector, which is very inefficient).\n\nThe SSE in this case is the cost function. It is simply the sum of the squared differences between predicted y and actual y (i.e. the residuals)\n\nNow, we run the loop. On each iteration the model will predict y given the values in theta, calculate the residuals, and then apply gradient descent to estimate corrective gradients, then will update the values of theta using these gradients \u2014 this process is repeated 100 times. When the loop is finished, I create a dataframe to store the learned parameters and loss per iteration.\n\nWhen the iterations have completed we can plot the lines than the model estimated.\n\nThe first thing to notice is the thick red line. This is the line estimated from the initial values of b0 and b1. You can see that this doesn\u2019t fit the data points well at all and because of this it is has the highest error (SSE). However, you can see the lines gradually moving toward the data points until a line of best fit (the thick blue line) is identified. In other words, upon each iteration the model has learned better values for b0 and b1 until it finds the values that minimize the cost function. The final values that the model learns for b0 and b1 are 3.96 and 3.51 respectively \u2014 so very close the parameters 4 and 3.5 that we set!\n\nVoilla! Our machine! it has learned!!\n\nWe can also visualize the decrease in the SSE across iterations of the model. This takes a steep decline in the early iterations before converging and stabilizing.\n\nWe can now use the learned values of b0 and b1 stored in theta to predict values y for new values of X.\n\nThis post presents a very simple way of understanding machine learning. It goes without saying that there is a lot more to ML, but gaining an initial intuition for the fundamentals of what is going on \u201cunderneath the hood\u201d can go a long way toward improving your understanding of more complex models."
    },
    {
        "url": "https://towardsdatascience.com/word-vectors-for-non-nlp-data-and-research-people-8d689c692353?source=user_profile---------4----------------",
        "title": "Word vectors for non-NLP data and research people \u2013",
        "text": "Word vectors represent a significant leap forward in advancing our ability to analyse relationships across words, sentences and documents. In doing so, they advance technology by providing machines much more information about words than has previously been possible using traditional representations of words. It is word vectors that make technologies such as speech recognition and machine translation possible. There are many excellent explanations of word vectors, but in this one I want to make the concept accessible to data and research people who aren\u2019t very familiar with natural language processing (NLP) \u2014 for a primer on foundational NLP concepts check out my post here: https://dataflume.wordpress.com/2017/03/17/intro-nlp-python-spacy/.\n\nWord vectors are simply vectors of numbers that represent the meaning of a word. For now, that\u2019s not very clear but, we\u2019ll come back to it in a bit. It is useful, first of all to consider why word vectors are considered such a leap forward from traditional representations of words.\n\nTraditional approaches to NLP, such as one-hot encoding and bag-of-words models (i.e. using dummy variables to represent the presence or absence of a word in an observation (e.g. a sentence)), whilst useful for some machine learning (ML) tasks, do not capture information about a word\u2019s meaning or context. This means that potential relationships, such as contextual closeness, are not captured across collections of words. For example, a one-hot encoding cannot capture simple relationships, such as determining that the words \u201cdog\u201d and \u201ccat\u201d both refer to animals that are often discussed in the context of household pets. Such encodings often provide sufficient baselines for simple NLP tasks (for example, email spam classifiers), but lack the sophistication for more complex tasks such as translation and speech recognition. In essence, traditional approaches to NLP, such as one-hot encodings, do not capture syntactic (structure) and semantic (meaning) relationships across collections of words and, therefore, represent language in a very naive way.\n\nIn contrast, word vectors represent words as multidimensional continuous floating point numbers where semantically similar words are mapped to proximate points in geometric space. In simpler terms, a word vector is a row of real valued numbers (as opposed to dummy numbers) where each point captures a dimension of the word\u2019s meaning and where semantically similar words have similar vectors. This means that words such as wheel and engine should have similar word vectors to the word car (because of the similarity of their meanings), whereas the word banana should be quite distant. Put differently, words that are used in a similar context will be mapped to a proximate vector space (we will get to how these word vectors are created below). The beauty of representing words as vectors is that they lend themselves to mathematical operators. For example, we can add and subtract vectors \u2014 the canonical example here is showing that by using word vectors we can determine that:\n\nIn other words, we can subtract one meaning from the word vector for king (i.e. maleness), add another meaning (femaleness), and show that this new word vector (king \u2014 man + woman) maps most closely to the word vector for queen.\n\nThe numbers in the word vector represent the word\u2019s distributed weight across dimensions. In a simplified sense each dimension represents a meaning and the word\u2019s numerical weight on that dimension captures the closeness of its association with and to that meaning. Thus, the semantics of the word are embedded across the dimensions of the vector.\n\nIn the figure we are imagining that each dimension captures a clearly defined meaning. For example, if you imagine that the first dimension represents the meaning or concept of \u201canimal\u201d, then each word\u2019s weight on that dimension represents how closely it relates to that concept.\n\nThis is quite a large simplification of word vectors as the dimensions do not hold such clearly defined meanings, but it is a useful and intuitive way to wrap your head around concept of word vector dimensions.\n\nWe can use the Python NLP library spaCy (check out my introduction here: https://dataflume.wordpress.com/2017/03/17/intro-nlp-python-spacy/, it has also been recently ported to R https://github.com/kbenoit/spacyr) to quickly access some pre-trained 300 dimensional word vectors. We create a list of words, apply spaCy\u2019s parser, extract the vector for each word, stack them together and then extract two-principal components for visualisation purposes.\n\nHere we simply extract vectors for different animals and words that might be used to describe some of them. As mentioned in the beginning word vectors are amazingly powerful because they allow us (and machines) to identify similarities across different words by representing them in a continuous vector space. You can see here how the vectors for animals like \u201clion\u201d, \u201ctiger\u201d, \u201ccheetah\u201d and \u201celephant\u201d are very close together. This is likely because they are often discussed in similar contexts, for example these animals are big, wild and potentially dangerous \u2014 indeed, the descriptive word \u201cwild\u201d maps quite closely to this group of animals.\n\nSimilar words are mapped together in the vector space. Notice how close cat and dog are to pet, how clustered elephant, lion and tiger are, and how descriptive words also cluster together.\n\nWhat is also interesting here is how closely the words \u201cwild\u201d, \u201czoo\u201d and \u201cdomesticated\u201d map to one another. It makes sense given that they are words that are frequently used to describe animals, but highlights the amazing power of word vectors!\n\nAn excellent question at this point is where do these dimensions and weights come from?! There are two common ways through which word vectors are generated:\n\n*Note: below I describe a high-level word2vec approach to generating word vectors, but a good overview of the count / co-occurence approach can be found here (https://medium.com/ai-society/jkljlj-7d6e699895c4).\n\nBoth approaches to generating word vectors build on Firth\u2019s (1957) distributional hypothesis which states:\n\nPut differently, words that share similar contexts tend to have similar meanings. The context of a word in a practical sense refers to its surrounding word(s) and word vectors are (typically) generated by predicting the probability of a context given a word. Put differently, the weights that comprise a word vector are learned by making predictions on the probability that other words are contextually close to a given word. This is akin to attempting to fill in the blanks around some given input word. For example, given the input sequence, \u201cThe fluffy dog barked as it chased a cat\u201d, the two-window (two-words preceding and proceeding the focal word) context for the words \u201cdog\u201d and \u201cbarked\u201d would look like:\n\nI don\u2019t wish to delve into the mathematical details how neural networks learn word embeddings too much, as people much more qualified to do so have explicated this already. In particular these posts have been helpful to me when trying to understand how word vectors are learned:\n\nIt is useful, however, to touch on the workings of the word2vec model given its popularity and usefulness. A word2vec model is simply a neural network with a single hidden layer that is designed to reconstruct the context of words by estimating the probability that a word is \u201cclose\u201d to another word given as input.\n\nThe model is trained on word, context pairings for every word in the corpus, i.e.:\n\nNote that this is technically a supervised learning process, but you do not need labelled data \u2014 the labels (the targets / dependent variables) are generated from the words that form the context of a focal word. Thus, using the window function the model learns the context in which words are used. In this simple example the model will learn that fluffy and barked are used in the context (as defined by the window length) of the word dog.\n\nOne of the fascinating things about word vectors created by word2vec models is that they are the side effects of a predictive task, not its output. In other words, a word vector is not predicted, (it is context probabilities that are predicted), the word vector is a learned representation of the input that is used on the predictive task \u2014 i.e. predicting a word given a context. The word vector is the model\u2019s attempt to learn a good numerical representation of the word in order to minimize the loss (error) of it\u2019s predictions. As the model iterates, it adjusts its neurons\u2019 weights in an attempt to minimize the error of it\u2019s predictions and in doing so, it gradually refines its representation of the word. In doing so, the word\u2019s \u201cmeaning\u201d becomes embedded in the weight learned by each neuron in the hidden layer of the network.\n\nA word2vec model, therefore, accepts as input a single word (represented as a one-hot encoding amongst all words in the corpus) and the model attempts to predict the probability that a randomly chosen word in the corpus is at a nearby position to the input word. This means that for every input word there are n output probabilities, where n is equal to the total size of the corpus. The magic here is that the training process includes only the word\u2019s context, not all words in the corpus. This means in our simple example above, given the word \u201cdog\u201d as input, \u201cbarked\u201d will have a higher probability estimate than \u201ccat\u201d because it is closer in context \u2014 i.e. it is learned in the training process. Put differently, the model attempts to predict the probability that other words in the corpus belong to the context of the input word. Therefore, given the sentence above (\u201cThe fluffy dog barked as it chased a cat\u201d) as input a run of the model would look like this:\n\nNote: This conceptual NN is a close friend of the diagram in Chris McCormick\u2019s blog post linked to above\n\nThe value in going through this process is to extract the weights that have been learned by the neurons of the model\u2019s hidden layer. It is these weights that form the word vector, i.e. if you have a 300 neuron hidden layer you will create a 300-dimension word vector for each word in the corpus. The output of this process, therefore, is a word-vector mapping of size n-input words * n-hidden layer neurons.\n\nThank you for reading, hope you learned something new :)"
    },
    {
        "url": "https://towardsdatascience.com/a-short-introduction-to-nlp-in-python-with-spacy-d0aa819af3ad?source=user_profile---------5----------------",
        "title": "A short introduction to NLP in Python with spaCy \u2013",
        "text": "Natural Language Processing (NLP) is one of the most interesting sub-fields of data science, and data scientists are increasingly expected to be able to whip up solutions that involve the exploitation of unstructured text data. Despite this, many applied data scientists (both from STEM and social science backgrounds) lack NLP experience.\n\nIn this post I explore some fundamental NLP concepts and show how they can be implemented using the increasingly popular spaCy package in Python. This post is for the absolute NLP beginner, but knowledge of Python is assumed.\n\nspaCy is a relatively new package for \u201cIndustrial strength NLP in Python\u201d developed by Matt Honnibal at Explosion AI. It is designed with the applied data scientist in mind, meaning it does not weigh the user down with decisions over what esoteric algorithms to use for common tasks and it\u2019s fast. Incredibly fast (it\u2019s implemented in Cython). If you are familiar with the Python data science stack, spaCy is your for NLP \u2013 it\u2019s reasonably low-level, but very intuitive and performant.\n\nSo, what can it do?\n\nspacy provides a one-stop-shop for tasks commonly used in any NLP project, including:\n\nI\u2019ll provide a high level overview of some of these features and show how to access them using spaCy.\n\nFirst, we load spaCy\u2019s pipeline, which by convention is stored in a variable named . declaring this variable will take a couple of seconds as spaCy loads its models and data to it up-front to save time later. In effect, this gets some heavy lifting out of the way early, so that the cost is not incurred upon each application of the parser to your data. Note that here I am using the English language model, but there is also a fully featured German model, with tokenisation (discussed below) implemented across several languages.\n\nWe invoke nlp on the sample text to create a object. The object is now a vessel for NLP tasks on the text itself, slices of the text ( objects) and elements ( objects) of the text. It is worth noting that and objects actually hold no data. Instead they contain pointers to data contained in the object and are evaluated lazily (i.e. upon request). Much of spaCy\u2019s core functionality is accessed through the methods on (n=33), (n=29) and (n=78) objects.\n\nTokenisation is a foundational step in many NLP tasks. Tokenising text is the process of splitting a piece of text into words, symbols, punctuation, spaces and other elements, thereby creating \u201ctokens\u201d. A naive way to do this is to simply split the string on white space:\n\nOn the surface, this looks fine. But, note that it disregards the punctuation and, it does not split the verb and adverb (\u201cwas\u201d, \u201cn\u2019t\u201d). Put differently, it is naive, it fails to recognise elements of the text that help us (and a machine) to understand its structure and meaning. Let\u2019s see how SpaCy handles this:\n\nHere we access the each token\u2019s method, which returns a string representation of the token rather than a SpaCy token object, this might not always be desirable, but worth noting. SpaCy recognises punctuation and is able to split these punctuation tokens from word tokens. Many of SpaCy\u2019s token method offer both string and integer representations of processed text \u2013 methods with an underscore suffix return strings, methods without an underscore suffix return integers. For example:\n\nHere, we return the SpaCy token, the string representation of the token and the integer representation of the token in a list of tuples.\n\nIf you want to avoid returning tokens that are punctuation or white space, SpaCy provides convienence methods for this (as well as many other common text cleaning tasks \u2014 for example, to remove stop words you can call the method.\n\nA related task to tokenisation is lemmatisation. Lemmatisation is the process of reducing a word to its base form, its mother word if you like. Different uses of a word often have the same root meaning. For example, all essentially refer to the same thing. It is often desirable to standardise words with similar meaning to their base form. With SpaCy we can access each word\u2019s base form with a token\u2019s method:\n\nWhy is this useful? An immediate use case is in machine learning, specifically text classification. Lemmatising the text prior to, for example, creating a \u201cbag-of-words\u201d avoids word duplication and, therefore, allows for the model to build a clearer picture of patterns of word usage across multiple documents.\n\nPart-of-speech tagging is the process of assigning grammatical properties (e.g. noun, verb, adverb, adjective etc.) to words. Words that share the same POS tag tend to follow a similar syntactic structure and are useful in rule-based processes.\n\nFor example, in a given description of an event we may wish to determine who owns what. By exploiting possessives, we can do this (providing the text is grammatically sound!). SpaCy uses the popular Penn Treebank POS tags, see https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html. With SpaCy you can access coarse and fine-grained POS tags with the and methods, respectively. Here, I access the fine grained POS tag:\n\nWe can see that the \u201c \u201d tokens are labelled as . We can exploit this tag to extract the owner and the thing that they own:\n\nThis returns a list of owner-possession tuples. If you want to be super Pythonic about it, you can do this in a list comprehenion (which, I think is preferable!):\n\nHere we are using each token\u2019s method which returns a token\u2019s neighbouring tokens.\n\nEntity recognition is the process of classifying named entities found in a text into pre-defined categories, such as persons, places, organizations, dates, etc. spaCy uses a statistical model to classify a broad range of entities, including persons, events, works-of-art and nationalities / religions (see the documentation for the full list https://spacy.io/docs/usage/entity-recognition).\n\nFor example, let\u2019s take the first two sentences from Barack Obama\u2019s wikipedia entry. We will parse this text, then access the identified entities using the object\u2019s method. With this method called on the we can access additional methods, specifically and :\n\nYou can see the entities that the model has identified and how accurate they are (in this instance). PERSON is self explanatory, NORP is natianalities or religuos groups, GPE identifies locations (cities, countries, etc.), DATE recognises a specific date or date-range and ORDINAL identifies a word or number representing some type of order.\n\nWhile we are on the topic of methods, it is worth mentioning spaCy\u2019s sentence identifier. It is not uncommon in NLP tasks to want to split a document into sentences. It is simple to do this with SpaCy by accessing a method:"
    }
]