[
    {
        "url": "https://medium.com/@ManishChablani/using-soft-attention-saliency-maps-for-vision-neural-nets-prediction-interpretation-7e41be4f2429?source=user_profile---------1----------------",
        "title": "Using soft attention saliency maps for vision neural nets prediction interpretation",
        "text": "While reading the nature article on predicting cardio vascular risk factors by training deep neural nets on retinal images. One thing stood out about interpreting the features built by neural nets using soft attention saliency maps.\n\nAssumption here is that the saliency map generated by simpler network would be similar to the features used by more complex/bigger network for generating predictions. I am little uncomfortable with this assumption, none the less its a good tool to validate the features extracted by neural networks and can catch other issues in your training like data leakage, etc."
    },
    {
        "url": "https://medium.com/@ManishChablani/bayesian-inference-68510b7468fc?source=user_profile---------2----------------",
        "title": "Bayesian Inference \u2013 Manish Chablani \u2013",
        "text": "Bayesian inference is an extremely powerful set of tools for modeling any random variable, such as the value of a regression parameter, a demographic statistic, a business KPI.\n\nThis approach to modeling uncertainty is particularly useful when:\n\nThis procedure effectively updates our initial beliefs about a proposition with some observation, yielding a final measure of the plausibility of rain, given the evidence.\n\nThis procedure is the basis for Bayesian inference, where our initial beliefs are represented by the prior distribution p(rain), and our final beliefs are represented by the posterior distribution p(rain | wet). The denominator simply asks, \u201cWhat is the total plausibility of the evidence?\u201d, whereby we have to consider all assumptions to ensure that the posterior is a proper probability distribution.\n\nBayesians are uncertain about what is true (the value of a KPI, a regression coefficient, etc.), and use data as evidence that certain facts are more likely than others. Prior distributions reflect our beliefs before seeing any data, and posterior distributions reflect our beliefs after we have considered all the evidence. To unpack what that means and how to leverage these concepts for actual analysis, let\u2019s consider the example of evaluating new marketing campaigns.\n\nInference refers to how you learn parameters of your model. A model is separate from how you train it, especially in the Bayesian world.\n\nConsider deep learning: you can train a network using Adam, RMSProp or a number of other optimizers. However, they tend to be rather similar to each other, all being variants of Stochastic Gradient Descent. In contrast, Bayesian methods of inference differ from each other more profoundly.\n\nThe two most important methods are Monte Carlo sampling and variational inference. Sampling is a gold standard, but slow. The excerpt from The Master Algorithm has more on MCMC.\n\nVariational inference is a method designed explicitly to trade some accuracy for speed. It\u2019s drawback is that it\u2019s model-specific, but there\u2019s light at the end of the tunnel \u2014 see the section on software below and Variational Inference: A Review for Statisticians.\n\nIn the spectrum of Bayesian methods, there are two main flavours. Let\u2019s call the first statistical modelling and the second probabilistic machine learning. The latter contains the so-called nonparametric approaches.\n\nModelling happens when data is scarce and precious and hard to obtain, for example in social sciences and other settings where it is difficult to conduct a large-scale controlled experiment. Imagine a statistician meticulously constructing and tweaking a model using what little data he has. In this setting you spare no effort to make the best use of available input.\n\nAlso, with small data it is important to quantify uncertainty and that\u2019s precisely what Bayesian approach is good at.\n\nBayesian methods \u2014 specifically MCMC \u2014 are usually computationally costly. This again goes hand-in-hand with small data.\n\nTo get a taste, consider examples for the Data Analysis Using Regression Analysis and Multilevel/Hierarchical Models book. That\u2019s a whole book on linear models. They start with a bang: a linear model with no predictors, then go through a number of linear models with one predictor, two predictors, six predictors, up to eleven.\n\nThis labor-intensive mode goes against a current trend in machine learning to use data for a computer to learn automatically from it.\n\nLet\u2019s try replacing \u201cBayesian\u201d with \u201cprobabilistic\u201d. From this perspective, it doesn\u2019t differ as much from other methods. As far as classification goes, most classifiers are able to output probabilistic predictions. Even SVMs, which are sort of an antithesis of Bayesian.\n\nBy the way, these probabilities are only statements of belief from a classifier. Whether they correspond to real probabilities is another matter completely and it\u2019s called calibration.\n\nLatent Dirichlet Allocation is a method that one throws data at and allows it to sort things out (as opposed to manual modelling). It\u2019s similar to matrix factorization models, especially non-negative MF. You start with a matrix where rows are documents, columns are words and each element is a count of a given word in a given document. LDA \u201cfactorizes\u201d this matrix of size n x d into two matrices, documents/topics (n x k) and topics/words (k x d).\n\nThe difference from factorization is that you can\u2019t multiply those two matrices to get the original, but since the appropriate rows/columns sum to one, you can \u201cgenerate\u201d a document. To get the first word, one samples a topic, then a word from this topic (the second matrix). Repeat this for a number of words you want. Notice that this is a bag-of-words representation, not a proper sequence of words.\n\nThe above is an example of a generative model, meaning that one can sample, or generate examples, from it. Compare with classifiers, which usually model to discriminate between classes based on x. A generative model is concerned with joint distribution of y and x, . It\u2019s more difficult to estimate that distribution, but it allows sampling and of course one can get from .\n\nWhile there\u2019s no exact definition, the name means that the number of parameters in a model can grow as more data become available. This is similar to Support Vector Machines, for example, where the algorithm chooses support vectors from the training points. Nonparametrics include Hierarchical Dirichlet Process version of LDA, where the number of topics chooses itself automatically, and Gaussian Processes.\n\nGaussian processes are somewhat similar to Support Vector Machines \u2014 both use kernels and have similar scalability (which has been vastly improved throughout the years by using approximations). A natural formulation for GP is regression, with classification as an afterthought. For SVM it\u2019s the other way around.\n\nAnother difference is that GP are probabilistic from the ground up (providing error bars), while SVM are not. You can observe this in regression. Most \u201cnormal\u201d methods only provide point estimates. Bayesian counterparts, like Gaussian processes, also output uncertainty estimates.\n\nCredit: Yarin Gal\u2019s Heteroscedastic dropout uncertainty and What my deep model doesn\u2019t know\n\nUnfortunately, it\u2019s not the end of the story. Even a sophisticated method like GP normally operates on an assumption of homoscedasticity, that is, uniform noise levels. In reality, noise might differ across input space (be heteroscedastic) \u2014 see the image below.\n\nA relatively popular application of Gaussian Processes is hyperparameter optimization for machine learning algorithms. The data is small, both in dimensionality \u2014 usually only a few parameters to tweak, and in the number of examples. Each example represents one run of the target algorithm, which might take hours or days. Therefore we\u2019d like to get to the good stuff with as few examples as possible.\n\nMost of the research on GP seems to happen in Europe. English have done some interesting work on making GP easier to use, culminating in the automated statistician, a project led by Zoubin Ghahramani.\n\nWatch the first 10 minutes of this video for an accessible intro to Gaussian Processes.\n\nAssume that we run an ecommerce platform for clothing and in order to bring people to our site, we deploy several digital marketing campaigns. These campaigns feature various ad images and captions, and are presented on a number of social networking websites. We want to present the ads that are the most successful. For the sake of simplicity, we can assume that the most successful campaign is the one that results in the highest click-through rate: the ads that are most likely to be clicked if shown.\n\nWe introduce a new campaign called \u201cfacebook-yellow-dress,\u201d a campaign presented to Facebook users featuring a yellow dress. The ad has been presented to 10 users so far, and 7 of the users have clicked on it. We would like to estimate the probability that the next user will click on the ad.\n\nBy encoding a click as a success and a non-click as a failure, we\u2019re estimating the probability \u03b8 that a given user will click on the ad. Naturally, we are going to use the campaign\u2019s historical record as evidence. Because we are considering unordered draws of an event that can be either 0 or 1, we can infer the probability \u03b8 by considering the campaign\u2019s history as a sample from a binomial distribution, with probability of success \u03b8. Traditional approaches of inference consider multiple values of \u03b8 and pick the value that is most aligned with the data. This is known as maximum likelihood, because we\u2019re evaluating how likely our data is under various assumptions and choosing the best assumption as true. More formally:\n\nargmax\u03b8p(X |\u03b8), where X is the data we\u2019ve observed.\n\nHere, p(X |\u03b8) is our likelihood function; if we fix the parameter \u03b8, what is the probability of observing the data we\u2019ve seen? Let\u2019s look at the likelihood of various values of \u03b8 given the data we have for facebook-yellow-dress:\n\nOf the 10 people we showed the new ad to, 7 of them clicked on it. So naturally, our likelihood function is telling us that the most likely value of theta is 0.7. However, some of our analysts are skeptical. The performance of this campaign seems extremely high given how our other campaigns have done historically. Let\u2019s overlay this likelihood function with the distribution of click-through rates from our previous 100 campaigns\n\nClearly, the maximum likelihood method is giving us a value that is outside what we would normally see. Perhaps our analysts are right to be skeptical; as the campaign continues to run, its click-through rate could decrease. Alternatively, this campaign could be truly outperforming all previous campaigns. We can\u2019t be sure. Ideally, we would rely on other campaigns\u2019 history if we had no data from our new campaign. And as we got more and more data, we would allow the new campaign data to speak for itself.\n\nThis skepticism corresponds to prior probability in Bayesian inference. Before considering any data at all, we believe that certain values of \u03b8 are more likely than others, given what we know about marketing campaigns. We believe, for instance, that p(\u03b8 = 0.2)>p(\u03b8 = 0.5), since none of our previous campaigns have had click-through rates remotely close to 0.5. We express our prior beliefs of \u03b8 with p(\u03b8). Using historical campaigns to assess p(\u03b8) is our choice as a researcher. Generally, prior distributions can be chosen with many goals in mind:\n\nFor our example, because we have related data and limited data on the new campaign, we will use an informative, empirical prior. We will choose a beta distribution for our prior for \u03b8. The beta distribution is a 2 parameter (\u03b1, \u03b2) distribution that is often used as a prior for the \u03b8 parameter of the binomial distribution. Because we want to use our previous campaigns as the basis for our prior beliefs, we will determine \u03b1 and \u03b2 by fitting a beta distribution to our historical click-through rates. Below, we fit the beta distribution and compare the estimated prior distribution with previous click-through rates to ensure the two are properly aligned:\n\nWe find that the best values of \u03b1 and \u03b2 are 11.5 and 48.5, respectively. The beta distribution with these parameters does a good job capturing the click-through rates from our previous campaigns, so we will use it as our prior. We will now update our prior beliefs with the data from the facebook-yellow-dress campaign to form our posterior distribution.\n\nAfter considering the 10 impressions of data we have for the facebook-yellow-dress campaign, the posterior distribution of \u03b8 gives us plausibility of any click-through rate from 0 to 1.\n\nThe effect of our data, or our evidence, is provided by the likelihood function, p(X|\u03b8). What we are ultimately interested in is the plausibility of all proposed values of \u03b8 given our data or our posterior distribution p(\u03b8|X). From the earlier section introducing Bayes\u2019 Theorem, our posterior distribution is given by the product of our likelihood function and our prior distribution:\n\nSince p(X) is a constant, as it does not depend on \u03b8, we can think of the posterior distribution as:\n\nWe\u2019ll now demonstrate how to estimate p(\u03b8|X) using PyMC.\n\nUsually, the true posterior must be approximated with numerical methods. To see why, let\u2019s return to the definition of the posterior distribution:\n\nThe denominator p(X) is the total probability of observing our data under all possible values of \u03b8. A more descriptive representation of this quantity is given by:\n\nWhich sums the probability of X over all values of \u03b8. This integral usually does not have a closed-form solution, so we need an approximation. One method of approximating our posterior is by using Markov Chain Monte Carlo (MCMC), which generates samples in a way that mimics the unknown distribution. We begin at a particular value, and \u201cpropose\u201d another value as a sample according to a stochastic process. We may reject the sample if the proposed value seems unlikely and propose another. If we accept the proposal, we move to the new value and propose another.\n\nPyMC is a python package for building arbitrary probability models and obtaining samples from the posterior distributions of unknown variables given the model. In our example, we\u2019ll use MCMC to obtain the samples.\n\nThe prototypical PyMC program has two components:\n\nLet\u2019s now obtain samples from the posterior. We select our prior as a Beta(11.5,48.5). Let\u2019s see how observing 7 clicks from 10 impressions updates our beliefs:\n\nNow that we have a full distribution for the probability of various values of \u03b8, we can take the mean of the distribution as our most plausible value for \u03b8, which is about 0.27.\n\nThe data has caused us to believe that the true click-through rate is higher than we originally thought, but far lower than the 0.7 click-through rate observed so far from the facebook-yellow-dress campaign. Why is this the case? Note how wide our likelihood function is; it\u2019s telling us that there is a wide range of values of \u03b8 under which our data is likely. If the range of values under which the data were plausible were narrower, then our posterior would have shifted further. See what happens to the posterior if we observed a 0.7 click-through rate from 10, 100, 1,000, and 10,000 impressions:"
    },
    {
        "url": "https://medium.com/@ManishChablani/artistic-style-transfer-paper-4887d79d4a0?source=user_profile---------3----------------",
        "title": "Artistic Style transfer paper \u2013 Manish Chablani \u2013",
        "text": "The results presented in the main text were generated on the basis of the VGG-Network. We used the feature space provided by the 16 convolutional and 5 pooling layers of the 19 layer VGGNetwork. We do not use any of the fully connected layers. For image synthesis we found that replacing the max-pooling operation by average pooling improves the gradient flow and one obtains slightly more appealing results.\n\nGenerally each layer in the network defines a non-linear filter bank whose complexity increases with the position of the layer in the network. Hence a given input image ~x is encoded in each layer of the CNN by the filter responses to that image.\n\nSo let ~p and ~x be the original image and the image that is generated and P l and F l their respective feature representation in layer l. We then define the squared-error loss between the two feature representations\n\nThus we can change the initially random image ~x until it generates the same response in a certain layer of the CNN as the original image ~p.\n\nOn top of the CNN responses in each layer of the network we built a style representation that computes the correlations between the different filter responses, where the expectation is taken over the spatial extend of the input image. These feature correlations are given by the Gram matrix Gl \u2208 R Nl\u00d7Nl , where Gl ij is the inner product between the vectorised feature map i and j in layer l:\n\nTo generate a texture that matches the style of a given image (Fig 1, style reconstructions), we use gradient descent from a white noise image to find another image that matches the style representation of the original image. This is done by minimising the mean-squared distance between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated."
    },
    {
        "url": "https://towardsdatascience.com/densenet-2810936aeebb?source=user_profile---------4----------------",
        "title": "DenseNet \u2013",
        "text": "DenseNet architecture is new, it is a logical extension of ResNet.\n\nResNet architecture has a fundamental building block (Identity) where you merge (additive) a previous layer into a future layer. Reasoning here is by adding additive merges we are forcing the network to learn residuals (errors i.e. diff between some previous layer and current one). In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.\n\nRecent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections \u2014 one between each layer and its subsequent layer \u2014 our network has L(L+1)/ 2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.\n\nCounter-intuitive effect of this dense connectivity pattern is that it requires fewer parameters than traditional convolutional networks, as there is no need to relearn redundant feature maps. Traditional feed-forward architectures can be viewed as algorithms with a state, which is passed on from layer to layer. Each layer reads the state from its preceding layer and writes to the subsequent layer. It changes the state but also passes on information that needs to be preserved. ResNets [11] make this information preservation explicit through additive identity transformations. Recent variations of ResNets [13] show that many layers contribute very little and can in fact be randomly dropped during training. This makes the state of ResNets similar to (unrolled) recurrent neural networks [21], but the number of parameters of ResNets is substantially larger because each layer has its own weights. Our proposed DenseNet architecture explicitly differentiates between information that is added to the network and information that is preserved. DenseNet layers are very narrow (e.g., 12 feature-maps per layer), adding only a small set of feature-maps to the \u201ccollective knowledge\u201d of the network and keep the remaining feature-maps unchanged \u2014 and the final classifier makes a decision based on all feature-maps in the network.\n\nBesides better parameter efficiency, one big advantage of DenseNets is their improved flow of information and gradients throughout the network, which makes them easy to train. Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision [20]. This helps training of deeper network architectures. Further, we also observe that dense connections have a regularizing effect, which reduces over- fitting on tasks with smaller training set sizes.\n\nConcatenating feature maps learned by different layers increases variation in the input of subsequent layers and improves efficiency. This constitutes a major difference between DenseNets and ResNets. Compared to Inception networks [35, 36], which also concatenate features from different layers, DenseNets are simpler and more efficient.\n\nEach layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer\n\nBottleneck layers. Although each layer only produces k output feature maps, it typically has many more inputs. It has been noted in [36, 11] that a 1\u00d71 convolution can be introduced as bottleneck layer before each 3\u00d73 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv(1\u00d7 1)-BN-ReLU-Conv(3\u00d73) version of H`, as DenseNet-B. Unless otherwise specified, each 1\u00d71 convolution reduces the input to 4k feature-maps in all experiments.\n\nCompression. To further improve model compactness, we can reduce the number of feature-maps at transition layers. If a dense block contains m feature-maps, we let the following transition layer generate b\u03b8mc output featuremaps, where 0 <\u03b8 \u22641 is referred to as the compression factor. When \u03b8 = 1, the number of feature-maps across transition layers remains unchanged. We refer the DenseNet with \u03b8 <1 as DenseNet-C, and we set \u03b8 = 0.5 in our experiment. When both the bottleneck and transition layers with \u03b8 < 1 are used, we refer to our model as DenseNet-BC.\n\nSuperficially, DenseNets are quite similar to ResNets: Eq. (2) differs from Eq. (1) only in that the inputs to H`(\u00b7) are concatenated instead of summed. However, the implications of this seemingly small modification lead to substantially different behaviors of the two network architectures.\n\nModel compactness. As a direct consequence of the input concatenation, the feature maps learned by any of the DenseNet layers can be accessed by all subsequent layers. This encourages feature reuse throughout the network, and leads to more compact models.\n\nImplicit Deep Supervision. One explanation for the improved accuracy of dense convolutional networks may be that individual layers receive additional supervision from the loss function through the shorter connections. One can interpret DenseNets to perform a kind of \u201cdeep supervision\u201d. The benefits of deep supervision have previously been shown in deeply-supervised nets (DSN; [20]), which have classifiers attached to every hidden layer, enforcing the intermediate layers to learn discriminative features. DenseNets perform a similar deep supervision in an implicit fashion: a single classifier on top of the network provides direct supervision to all layers through at most two or three transition layers. However, the loss function and gradient of DenseNets are substantially less complicated, as the same loss function is shared between all layers.\n\nThis paper is interestingly titled: The One Hundred Layers Tiramisu\n\nState-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, \u00a9 a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation\n\nRecently, a new CNN architecture, called DenseNet, was introduced in [12]. DenseNets are built from dense blocks and pooling operations, where each dense block is an iterative concatenation of previous feature maps. This architecture can be seen as an extension of ResNets [10], which performs iterative summation of previous feature maps. However, this small modification has some interesting implications: (1) parameter efficiency, DenseNets are more effi- cient in the parameter usage; (2) implicit deep supervision, DenseNets perform deep supervision thanks to short paths to all feature maps in the architecture (similar to Deeply Supervised Networks [17]); and (3) feature reuse, all layers can easily access their preceding layers making it easy to reuse the information from previously computed feature maps. The characteristics of DenseNets make them a very good fit for semantic segmentation as they naturally induce skip connections and multi-scale supervision\n\nIn this paper, we extend DenseNets to work as FCNs by adding an upsampling path to recover the full input resolution. Naively building an upsampling path would result in a computationally intractable number of feature maps with very high resolution prior to the softmax layer. This is because one would multiply the high resolution feature maps with a large number of input filters (from all the layers below), resulting in both very large amount of computation and number of parameters. In order to mitigate this effect, we only upsample the feature maps created by the preceding dense block. Doing so allows to have a number of dense blocks at each resolution of the upsampling path independent of the number of pooling layers. Moreover, given the network architecture, the upsampled dense block combines the information contained in the other dense blocks of the same resolution. The higher resolution information is passed by means of a standard skip connection between the downsampling and the upsampling paths.\n\nThus, the contributions of the paper can be summarized as follows:\n\nFCNs are built from a downsampling path, an upsampling path and skip connections. Skip connections help the upsampling path recover spatially detailed information from the downsampling path, by reusing features maps. The goal of our model is to further exploit the feature reuse by extending the more sophisticated DenseNet architecture, while avoiding the feature explosion at the upsampling path of the network\n\nFCNs are built from a downsampling path, an upsampling path and skip connections. Skip connections help the upsampling path recover spatially detailed information from the downsampling path, by reusing features maps. The goal of our model is to further exploit the feature reuse by extending the more sophisticated DenseNet architecture, while avoiding the feature explosion at the upsampling path of the network.\n\nIn the downsampling path of our Fully Convolutional DenseNet (FC-DenseNet).the linear growth in the number of features is compensated by the reduction in spatial resolution of each feature map after the pooling operation. The last layer of the downsampling path is referred to as bottleneck.\n\nIn order to recover the input spatial resolution, FCNs introduce an upsampling path composed of convolution, upsampling operations (transposed convolutions or unpooling operations) and skip connections. In FC-DenseNets, we substitute the convolution operation by a dense block and an upsampling operation referred to as transition up. Transition up modules consist of a transposed convolution that upsamples the previous feature maps. The upsampled feature maps are then concatenated to the ones coming from the skip connection to form the input of a new dense block. Since the upsampling path increases the feature maps spatial resolution, the linear growth in the number of features would be too memory demanding, especially for the full resolution features in the pre-softmax layer. In order to overcome this limitation, the input of a dense block is not concatenated with its output. Thus, the transposed convolution is applied only to the feature maps obtained by the last dense block and not to all feature maps concatenated so far. The last dense block summarizes the information contained in all the previous dense blocks at the same resolution. Note that some information from earlier dense blocks is lost in the transition down due to the pooling operation. Nevertheless, this information is available in the downsampling path of the network and can be passed via skip connections. Hence, the dense blocks of the upsampling path are computed using all the available feature maps at a given resolution\n\nFirst, in Table 1, we define the dense block layer, transition down and transition up of the architecture. Dense block layers are composed of BN, followed by ReLU, a 3 \u00d7 3 same convolution (no resolution loss) and dropout with probability p = 0.2. The growth rate of the layer is set to k = 16. Transition down is composed of BN, followed by ReLU, a 1 \u00d7 1 convolution, dropout with p = 0.2 and a non-overlapping max pooling of size 2 \u00d7 2. Transition up is composed of a 3 \u00d7 3 transposed convolution with stride 2 to compensate for the pooling operation.\n\nSecond, in Table 2, we summarize all Dense103 layers. This architecture is built from 103 convolutional layers : a first one on the input, 38 in the downsampling path, 15 in the bottleneck and 38 in the upsampling path. We use 5 Transition Down (TD), each one containing one extra convolution, and 5 Transition Up (TU), each one containing a transposed convolution. The final layer in the network is a 1 \u00d7 1 convolution followed by a softmax non-linearity to provide the per class distribution at each pixel. It is worth noting that, as discussed in Subsection 3.2, the proposed upsampling path properly mitigates the DenseNet feature map explosion, leading to reasonable pre-softmax feature map number of 256. Finally, the model is trained by minimizing the pixelwise cross-entropy loss.\n\nIn this paper, we have extended DenseNets and made them fully convolutional to tackle the problem semantic image segmentation. The main idea behind DenseNets is captured in dense blocks that perform iterative concatenation of feature maps. We designed an upsampling path mitigating the linear growth of feature maps that would appear in a naive extension of DenseNets. The resulting network is very deep (from 56 to 103 layers) and has very few parameters, about 10 fold reduction w.r.t. state-of-the-art models. Moreover, it improves stateof-the-art performance on challenging urban scene understanding datasets (CamVid and Gatech), without neither additional post-processing, pretraining, nor including temporal information.\n\nFlowNetS is a conventional CNN architecture, consisting of a contracting part and an expanding part. Given adjacent frames as input, the contracting part uses a series of convolutional layers to extract high level semantic features, while the expanding part tries to predict the optical flow at the original image resolution by successive deconvolutions. In between, it uses skip connections [8] to provide fine image details from lower layer feature maps. This generic pipeline, contract, expand, skip connections, is widely adopted for per-pixel prediction problems, such as semantic segmentation [9], depth estimation [10], video coloring [11], etc.\n\nHowever, skip connections are a simple strategy for combining coarse semantic features and fine image details; they are not involved in the learning process. What we desire is to keep the high frequency image details until the end of the network in order to provide implicit deep supervision. Simply put, we want to ensure maximum information flow between layers in the network.\n\nDenseNet [12], a recently proposed CNN architecture, has an interesting connectivity pattern: each layer is connected to all the others within a dense block. In this case, all layers can access feature maps from their preceding layers which encourages heavy feature reuse. As a direct consequence, the model is more compact and less prone to overfitting. Besides, each individual layer receives direct supervision from the loss function through the shortcut paths, which provides implicit deep supervision. All these good properties make DenseNet a natural fit for per-pixel prediction problems. There is a concurrent work using DenseNet for semantic segmentation [9], which achieves state-of-the-art performance without either pretraining or additional post-processing. However, estimating optical flow is different from semantic segmentation. We will illustrate the differences in Section 3. In this paper, we propose to use DenseNet for optical flow prediction. Our contributions are two-fold. First, we extend current DenseNet to a fully convolutional network. Our model is totally unsupervised, and achieves performance close to supervised approaches. Second, we empirically show that replacing convolutions with dense blocks in the expanding part yields better performance.\n\nUnsupervised Motion Estimation: Supervised approaches adopt synthetic datasets for CNNs to learn optical flow prediction. However, synthetic motions/scenes are quite different from real world ones, thus limiting the generalizability of the learned model. Besides, even constructing synthetic datasets requires a lot of manual effort [3]. Hence, unsupervised learning is an ideal option for the naturally ill-conditioned motion estimation problem. Recall that the unsupervised approach [6] treats the optical flow estimation as an image reconstruction problem. The intuition is that if we can use the predicted flow and the next frame to reconstruct the previous frame, our network is learning useful representations about the underlying motions. To be specific, we denote the reconstructed previous frame as I 0 1 .\n\nThe goal is to minimize the photometric error between the previous frame I1 and the inverse warped next frame I 0 1 :"
    },
    {
        "url": "https://towardsdatascience.com/computer-vision-concepts-and-terminology-edd392a6f594?source=user_profile---------5----------------",
        "title": "Computer vision concepts and terminology \u2013",
        "text": "A superpixel is an image patch which is better aligned with intensity edges than a rectangular patch. Superpixels can be extracted with any segmentation algorithm, however, most of them produce highly irregular superpixels, with widely varying sizes and shapes. A more regular space tessellation may be desired.\n\nNon-maximum supression is often used along with edge detection algorithms. The image is scanned along the image gradient direction, and if pixels are not part of the local maxima they are set to zero.\n\nExtracting super pixels from an image is an example of this task or foreground-background segmentation. Semantic Segmentation: In semantic segmentation you have to label each pixel with a class of objects (Car, Person, Dog, \u2026) and non-objects (Water, Sky, Road, \u2026).\n\nSemantic segmentation is the task of clustering parts of images together which belong to the same object class. This type of algorithm has several usecases such as detecting road signs [MBLAGJ+07], detecting tumors [MBVLG02], detecting medical instruments in operations [WAH97], colon crypts segmentation [CRSS14], land use and land cover classification [HDT02]. In contrast, non-semantic segmentation only clusters pixels together based on general characteristics of single objects. Hence the task of non-semantic segmentation is not well-defined\n\nOptical flow is the pattern of apparent motion of image objects between two consecutive frames caused by the movemement of object or camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second. Consider the image below It shows a ball moving in 5 consecutive frames. The arrow shows its displacement vector.\n\nOptical flow has many applications in areas like :\n\nHOG stands for Histograms of Oriented Gradients. HOG is a type of \u201cfeature descriptor\u201d. The intent of a feature descriptor is to generalize the object in such a way that the same object (in this case a person) produces as close as possible to the same feature descriptor when viewed under different conditions. This makes the classification task easier.\n\nThe creators of this approach trained a Support Vector Machine (a type of machine learning algorithm for classification), or \u201cSVM\u201d, to recognize HOG descriptors of people. The HOG person detector uses a sliding detection window which is moved around the image. At each position of the detector window, a HOG descriptor is computed for the detection window. This descriptor is then shown to the trained SVM, which classifies it as either \u201cperson\u201d or \u201cnot a person\u201d.\n\nTo recognize persons at different scales, the image is subsampled to multiple sizes. Each of these subsampled images is searched.\n\nSIFT: Scale-invariant feature transform (SIFT) feature descriptors describe keypoints in an image. The image patch of the size 16 \u00d7 16 around the keypoint is taken. This patch is divided in 16 distinct parts of the size 4 \u00d7 4. For each of those parts a histogram of 8 orientations is calculated similar as for HOG features. This results in a 128-dimensional feature vector for each keypoint. It should be emphasized that SIFT is a global feature for a complete image.\n\nalso called bag of keypoints, is based on vector quantization. Similar to HOG features, BOV features are histograms which count the number of occurrences of certain patterns within a patch of the image\n\nPoselets rely on manually added extra keypoints such as \u201cright shoulder\u201d, \u201cleft shoulder\u201d, \u201cright knee\u201d and \u201cleft knee\u201d. They were originally used for human pose estimation. Finding those extra keypoints is easily possible for well-known image classes like humans. However, it is difficult for classes like airplanes, ships, organs or cells where the human annotators do not know the keypoints. Additionally, the keypoints have to be chosen for every single class. There are strategies to deal with those problems like viewpointdependent keypoints. Poselets were used in [BMBM10] to detect people and in [BBMM11] for general object detection of the PASCAL VOC dataset.\n\nA texton is the minimal building block of vision. The computer vision literature does not give a strict definition for textons, but edge detectors could be one example. One might argue that deep learning techniques with Convolution Neuronal Networks (CNNs) learn textons in the first filters.\n\nare undirected probabilistic graphical models which are wide-spread model in computer vision. The overall idea of MRFs is to assign a random variable for each feature and a random variable for each pixel\n\nThere are two things we are trying to accomplish with whitening:\n\nI asked a Neural Network expect I\u2019m connected with, Pavel Skribtsov, for more of an explanation on why this technique is beneficial:"
    },
    {
        "url": "https://medium.com/@ManishChablani/overfeat-paper-summary-b55060eeb991?source=user_profile---------6----------------",
        "title": "Overfeat paper \u2014 Summary \u2013 Manish Chablani \u2013",
        "text": "This is an old paper (2014) and has been replaced by SSD and YOLO for more realtime detections. However the paper does provide interesting perspective. On the ILSVRC 2013 dataset OverFeat ranked 4th in classification, 1st in localization, and 1st in detection.\n\nThree computer vision tasks in increasing order of difficulty: (i) classi- fication, (ii) localization, and (iii) detection.\n\nIn the classification task of this challenge, each image is assigned a single label corresponding to the main object in the image. Five guesses are allowed to find the correct answer (this is because images can also contain multiple unlabeled objects). The localization task is similar in that 5 guesses are allowed per image, but in addition, a bounding box for the predicted object must be returned with each guess. To be considered correct, the predicted box must match the ground truth by at least 50% (using the PASCAL criterion of union over intersection), as well as be labeled with the correct class (i.e. each prediction is a label and bounding box that are associated together). The detection task differs from localization in that there can be any number of objects in each image (including zero), and false positives are penalized by the mean average precision 3 (mAP) measure."
    },
    {
        "url": "https://medium.com/@ManishChablani/ssd-single-shot-multibox-detector-explained-38533c27f75f?source=user_profile---------7----------------",
        "title": "SSD: Single Shot MultiBox Detector explained \u2013 Manish Chablani \u2013",
        "text": "Key idea here is single network (for speed) and no need for region proposals instead it uses different bounding boxes and then adjust the bounding box as part of prediction. Different bounding box predictions is achieved by each of the last few layers of the network responsible for predictions for progressively smaller bounding box and final prediction is union of all these predictions.\n\nThe SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections ( for bounding boxes with most overlap keep the one with highest score)."
    },
    {
        "url": "https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006?source=user_profile---------8----------------",
        "title": "YOLO \u2014 You only look once, real time object detection explained",
        "text": "Compared to other region proposal classification networks (fast RCNN) which perform detection on various region proposals and thus end up performing prediction multiple times for various regions in a image, Yolo architecture is more like FCNN (fully convolutional neural network) and passes the image (nxn) once through the FCNN and output is (mxm) prediction. This the architecture is splitting the input image in mxm grid and for each grid generation 2 bounding boxes and class probabilities for those bounding boxes. Note that bounding box is more likely to be larger than the grid itself. From paper:\n\nChanges to loss functions for better results is interesting. Two things stand out:"
    },
    {
        "url": "https://medium.com/@ManishChablani/classification-concepts-e97ff174dba8?source=user_profile---------9----------------",
        "title": "Classification concepts \u2013 Manish Chablani \u2013",
        "text": "Here is my reference summary of random important concepts related to classification models. Credits: various sources online.\n\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \u201cclassifier\u201d)\n\nThis is a list of rates that are often computed from a confusion matrix for a binary classifier:\n\nF Score: This is a weighted average of the true positive rate (recall) and precision\n\nROC Curve: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class.\n\nTP/actual yes = 100/105 = 0.95 also known as \u201cSensitivity\u201d or \u201cRecall\u201d\n\nThis type of graph is called a Receiver Operating Characteristic curve (or ROC curve.) It is a plot of the true positive rate against the false positive rate for the different possible cutpoints of a diagnostic test.\n\nTwo common methods on deciding splits in decision tree: Gini impurity and information gain.\n\nIn information theory, systems are composed of three elements, a transmitter, a channel, and a receiver, and their measurable ability to convey a message. In this context, entropy (more specifically, Shannon entropy) is the expected value of the information contained in each message. In a more technical sense, there are reasons to define information as the negative logarithm of the probability distribution of possible events or messages. The amount of information of every event forms a random variable whose expected value, is called, Shannon entropy. Entropy is zero when one outcome is certain. Generally, entropy refers to disorder or uncertainty.\n\nEntropy is defined as below\n\nwhere p1, p2, .. are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.\n\nLook at the image below and think which node can be described easily. I am sure, your answer is C because it requires less information as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say that C is a Pure node, B is less Impure and A is more impure.\n\nNow, we can build a conclusion that less impure node requires less information to describe it. And, more impure node requires more information. Information theory is a measure to define this degree of disorganization in a system known as Entropy. If the sample is completely homogeneous, then the entropy is zero and if the sample is an equally divided (50% \u2014 50%), it has entropy of one.\n\nEntropy can be calculated using formula:-\n\nHere p and q is probability of success and failure respectively in that node. Entropy is also used with categorical target variable. It chooses the split which has lowest entropy compared to parent node and other splits. The lesser the entropy, the better it is.\n\nExample: Let\u2019s use this method to identify best split for student example.\n\nAbove, you can see that entropy for Split on Gender is the lowest among all, so the tree will split on Gender. We can derive information gain from entropy as 1- Entropy.\n\nUsed by the CART (classification and regression tree) algorithm, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Gini impurity can be computed by summing the probability pi of an item with label i being chosen times the probability 1 - pi of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category.\n\nIn statistics and machine learning, the bias\u2013variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\n\nCentral limit theorem (CLT): is a statistical theory that states that given a sufficiently large sample size from a population with a finite level of variance, the mean of all samples from the same population will be approximately equal to the mean of the population.\n\nBagging is a technique used to reduce the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same data set. The following figure will make it clearer:\n\nThe steps followed in bagging are:\n\nNote that, here the number of models built is not a hyper-parameters. Higher number of models are always better or may give similar performance than lower numbers. It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions.\n\nThere are various implementations of bagging models. Random forest is one of them and we\u2019ll discuss it next.\n\nRandom forest is a tree-based algorithm which involves building several trees (decision trees), then combining their output to improve generalization ability of the model. The method of combining trees is known as an ensemble method. Ensembling is nothing but a combination of weak learners (individual trees) to produce a strong learner.\n\nRandom forests does not overfit. You can run as many trees as you want. It is fast.\n\nHow random forests work: The training set for the current tree is drawn by sampling with replacement, about one-third of the cases are left out of the sample. This oob (out-of-bag) data is used to get a running unbiased estimate of the classification error as trees are added to the forest. It is also used to get estimates of variable importance.\n\nAfter each tree is built, all of the data are run down the tree, and proximities are computed for each pair of cases. If two cases occupy the same terminal node, their proximity is increased by one. At the end of the run, the proximities are normalized by dividing by the number of trees. Proximities are used in replacing missing data, locating outliers, and producing illuminating low-dimensional views of the data.\n\nIn random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows: Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.\n\nIn every tree grown in the forest, put down the oob cases and count the number of votes cast for the correct class. Now randomly permute the values of variable m in the oob cases and put these cases down the tree. Subtract the number of votes for the correct class in the variable-m-permuted oob data from the number of votes for the correct class in the untouched oob data. The average of this number over all trees in the forest is the raw importance score for variable m.\n\nIf the values of this score from tree to tree are independent, then the standard error can be computed by a standard computation. The correlations of these scores between trees have been computed for a number of data sets and proved to be quite low, therefore we compute standard errors in the classical way, divide the raw score by its standard error to get a z-score, ands assign a significance level to the z-score assuming normality.\n\nOutliers: Outliers are generally defined as cases that are removed from the main body of the data. Translate this as: outliers are cases whose proximities to all other cases in the data are generally small. A useful revision is to define outliers relative to their class. Thus, an outlier in class j is a case whose proximities to all other class j cases are small.\n\nIn some data sets, the prediction error between classes is highly unbalanced. Some classes have a low prediction error, others a high. This occurs usually when one class is much larger than another. Then random forests, trying to minimize overall error rate, will keep the error rate low on the large class while letting the smaller classes have a larger error rate. For instance, in drug discovery, where a given molecule is classified as active or not, it is common to have the actives outnumbered by 10 to 1, up to 100 to 1. In these situations the error rate on the interesting class (actives) will be very high.\n\nThe user can detect the imbalance by outputs the error rates for the individual classes. To illustrate 20 dimensional synthetic data is used. Class 1 occurs in one spherical Gaussian, class 2 on another. A training set of 1000 class 1\u2019s and 50 class 2\u2019s is generated, together with a test set of 5000 class 1\u2019s and 250 class 2's.\n\nThe final output of a forest of 500 trees on this data is: (num sample, total error, class A error, class B error): 500 3.7 0.0 78.4. There is a low overall test set error (3.73%) but class 2 has over 3/4 of its cases misclassified.\n\nThe error can balancing can be done by setting different weights for the classes. The higher the weight a class is given, the more its error rate is decreased. A guide as to what weights to give is to make them inversely proportional to the class populations. So set weights to 1 on class 1, and 20 on class 2, and run again. The output is: 500 12.1 12.7 0.0\n\nThe weight of 20 on class 2 is too high. Set it to 10 and try again, getting: 500 4.3 4.2 5.2. This is pretty close to balance. If exact balance is wanted, the weight on class 2 could be jiggled around a bit more.\n\nPruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n\nNon Parametric Method: Decision tree is considered to be a non-parametric method. This means that decision trees have no assumptions about the space distribution and the classifier structure.\n\nStep 1: The base learner takes all the distributions and assign equal weight or attention to each observation.\n\nStep 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n\nStep 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n\nFinally, it combines the outputs from weak learner and creates a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classi\ufb01ed or have higher errors by preceding weak rules.\n\nThere are many boosting algorithms which impart additional boost to model\u2019s accuracy. Two most commonly used algorithms i.e. Gradient Boosting (GBM) and XGboost.\n\nThe general idea of the method is additive training. At each iteration, a new tree learns the gradients of the residuals between the target values and the current predicted values, and then the algorithm conducts gradient descent based on the learned gradients. We can see that this is a sequential algorithm. Therefore, we can\u2019t parallelize the algorithm like Random Forest. We can only parallelize the algorithm in the tree building step. Therefore, the problem reduces to parallel decision tree building.\n\nSelecting the hyperplane: Maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Hyperlane C\n\nSVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. hyper-plane A (Linear regression would have picked B as RMSE is lower with B)\n\nSVM has a feature to ignore outliers and find the hyper-plane that has maximum margin. Hence, we can say, SVM is robust to outliers.\n\nSVM has a technique called the kernel trick. These are functions which takes low dimensional input space and transform it to a higher dimensional space i.e. it converts not separable problem to separable problem, these functions are called kernels. It is mostly useful in non-linear separation problem. we have various options available with kernel like, \u201clinear\u201d, \u201crbf\u201d,\u201dpoly\u201d and others (default value is \u201crbf\u201d).\n\nTF-IDF stands for \u201cterm frequency / inverse document frequency\u201d and is a method for emphasizing words that occur frequently in a given document, while at the same time de-emphasising words that occur frequently in many documents.\n\nwe\u2019d like to try n-grams, and for n-grams we better leave all the words in place. We covered n-grams before, they are combinations of n sequential words, starting with bigrams (two words): \u201ccat ate\u201d, \u201cate my\u201d, \u201cmy precious\u201d, \u201cprecious homework\u201d. Trigrams consist of three words: \u201ccat ate my\u201d, \u201cate my homework\u201d, \u201cmy precious homework\u201d; 4-grams of four, and so on.\n\nWhy do n-grams work? Consider this phrase: \u201cmovie not good\u201d. It has obviously negative sentiment, however if you take each word in separation you won\u2019t detect this. On the opposite, the model will probably learn that \u201cgood\u201d is a positive sentiment word, which doesn\u2019t help at all here.\n\nOn the other hand, bigrams will do the trick: the model will probably learn that \u201cnot good\u201d has a negative sentiment.\n\nTo use a more complicated example from Stanford\u2019s sentiment analysis page:\n\nFor this, bigrams will fail with \u201cthat funny\u201d and \u201csuper witty\u201d. We\u2019d need at least trigrams to catch \u201cneither that funny\u201d and \u201cnor super witty\u201d, however these phrases don\u2019t seem to be too common, so if we\u2019re using a restricted number of features, or regularization, they might not make it into the model. Hence the motivation for a more sophisticated model like a neural network, but we digress.\n\nIf computing n-grams sounds a little complicated, scikit-learn vectorizers can do it automatically.\n\nEach word is a feature: whether it\u2019s present in the document or not (0/1), or how many times it appears (an integer >= 0). We started with the original dimensionality from the tutorial, 5000. This makes sense for a random forest, which as a highly non-linear / expressive / high-variance classifier needs a relatively high ratio of examples to dimensionality. Linear models are less exacting in this respect, they can even work with d >> n.\n\nWe found out that if we don\u2019t constrain the dimensionality, we run out of memory, even with such a small dataset. We could afford roughly 40k features on a machine with 12 GB of RAM. More caused swapping.\n\nFor starters, we tried 20k features. The logistic regression scores 94.2% (before TF-IDF and n-grams), vs 92.9% with 5k features. More is even better: 96.0 with 30k, 96.3 with 40k (after TF-IDF and ngrams).\n\nTo deal with memory issues we could use the hashing vectorizer. However it only scores 93.2% vs 96.3% before, partly because it doesn\u2019t support TF-IDF.\n\nTopic modelling provides us with methods to organize, understand and summarize large collections of textual information. It helps in:\n\nIn the LDA model, each document is viewed as a mixture of topics that are present in the corpus. The model proposes that each word in the document is attributable to one of the document\u2019s topics.\n\nCollapsed Gibbs sampling is one way the LDA learns the topics and the topic representations of each document. The procedure is as follows:\n\n1. For each document d, go through each word w and compute:\n\n2. p(topic t | document d): proportion of words in document d that are assigned to topic t\n\n3. p(word w| topic t): proportion of assignments to topic t, over all documents d, that come from word w\n\nOn repeating the last step a large number of times, we reach a steady state where topic assignments are pretty good. These assignments are then used to determine the topic mixtures of each document.\n\nThere are many approaches for obtaining topics from a text such as \u2014 Term Frequency and Inverse Document Frequency. NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation is the most popular topic modeling technique and in this article, we will discuss the same.\n\nLDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.\n\nLDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. The following matrix shows a corpus of N documents D1, D2, D3 \u2026 Dn and vocabulary size of M words W1,W2 .. Wn. The value of i,j cell gives the frequency count of word Wj in Document Di.\n\nLDA converts this Document-Term Matrix into two lower dimensional matrices \u2014 M1 and M2.\n\nM1 is a document-topics matrix and M2 is a topic \u2014 terms matrix with dimensions (N, K) and (K, M) respectively, where N is the number of documents, K is the number of topics and M is the vocabulary size.\n\nNotice that these two matrices already provides topic word and document topic distributions, However, these distribution needs to be improved, which is the main aim of LDA. LDA makes use of sampling techniques in order to improve these matrices.\n\nIt Iterates through each word \u201cw\u201d for each document \u201cd\u201d and tries to adjust the current topic \u2014 word assignment with a new assignment. A new topic \u201ck\u201d is assigned to word \u201cw\u201d with a probability P which is a product of two probabilities p1 and p2.\n\nFor every topic, two probabilities p1 and p2 are calculated. P1 \u2014 p(topic t / document d) = the proportion of words in document d that are currently assigned to topic t. P2 \u2014 p(word w / topic t) = the proportion of assignments to topic t over all documents that come from this word w.\n\nThe current topic \u2014 word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word \u2014 topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current word\u2019s topic with new probability.\n\nAfter a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA.\n\nAlpha and Beta Hyperparameters \u2014 alpha represents document-topic density and Beta represents topic-word density. Higher the value of alpha, documents are composed of more topics and lower the value of alpha, documents contain fewer topics. On the other hand, higher the beta, topics are composed of a large number of words in the corpus, and with the lower value of beta, they are composed of few words.\n\nNumber of Topics \u2014 Number of topics to be extracted from the corpus. Researchers have developed approaches to obtain an optimal number of topics by using Kullback Leibler Divergence Score. I will not discuss this in detail, as it is too mathematical. For understanding, one can refer to this[1] original paper on the use of KL divergence.\n\nNumber of Topic Terms \u2014 Number of terms composed in a single topic. It is generally decided according to the requirement. If the problem statement talks about extracting themes or concepts, it is recommended to choose a higher number, if problem statement talks about extracting features or terms, a low number is recommended.\n\nNumber of Iterations / passes \u2014 Maximum number of iterations allowed to LDA algorithm for convergence."
    },
    {
        "url": "https://towardsdatascience.com/cyclegans-and-pix2pix-5e6a5f0159c4?source=user_profile---------10----------------",
        "title": "CycleGANS and Pix2Pix \u2013",
        "text": "Credits: Presenting abridged version of these blogs to explain the idea and concepts behind pix2pix and cycleGANs.\n\npix2pix uses a conditional generative adversarial network (cGAN) to learn a mapping from an input image to an output image.\n\nAn example of a dataset would be that the input image is a black and white picture and the target image is the color version of the picture. The generator in this case is trying to learn how to colorize a black and white image. The discriminator is looking at the generator\u2019s colorization attempts and trying to learn to tell the difference between the colorizations the generator provides and the true colorized target image provided in the dataset.\n\nThe structure of the generator is called an \u201cencoder-decoder\u201d and in pix2pix the encoder-decoder looks more or less like this:\n\nThe volumes are there to give you a sense of the shape of the tensor dimensions next to them. The input in this example is a 256x256 image with 3 color channels (red, green, and blue, all equal for a black and white image), and the output is the same.\n\nThe generator takes some input and tries to reduce it with a series of encoders (convolution + activation function) into a much smaller representation. The idea is that by compressing it this way we hopefully have a higher level representation of the data after the final encode layer. The decode layers do the opposite (deconvolution + activation function) and reverse the action of the encoder layers.\n\nIn order to improve the performance of the image-to-image transform in the paper, the authors used a \u201cU-Net\u201d instead of an encoder-decoder. This is the same thing, but with \u201cskip connections\u201d directly connecting encoder layers to decoder layers:\n\nThe skip connections give the network the option of bypassing the encoding/decoding part if it doesn\u2019t have a use for it.\n\nThese diagrams are a slight simplification. For instance, the first and last layers of the network have no batch norm layer and a few layers in the middle have dropout units.\n\nThe Discriminator has the job of taking two images, an input image and an unknown image (which will be either a target or output image from the generator), and deciding if the second image was produced by the generator or not.\n\nThe structure looks a lot like the encoder section of the generator, but works a little differently. The output is a 30x30 image where each pixel value (0 to 1) represents how believable the corresponding section of the unknown image is. In the pix2pix implementation, each pixel from this 30x30 image corresponds to the believability of a 70x70 patch of the input image (the patches overlap a lot since the input images are 256x256). The architecture is called a \u201cPatchGAN\u201d.\n\nTo train this network, there are two steps: training the discriminator and training the generator.\n\nTo train the discriminator, first the generator generates an output image. The discriminator looks at the input/target pair and the input/output pair and produces its guess about how realistic they look. The weights of the discriminator are then adjusted based on the classification error of the input/output pair and the input/target pair.\n\nThe generator\u2019s weights are then adjusted based on the output of the discriminator as well as the difference between the output and target image."
    },
    {
        "url": "https://towardsdatascience.com/attention-models-in-nlp-a-quick-introduction-2593c1fe35eb?source=user_profile---------11----------------",
        "title": "Attention models in NLP a quick introduction \u2013",
        "text": "Credits: Here is abridged version of wildml article: http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\n\nTypical seq2seq models usually are of the form explained in my blog: https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d\n\nWhen assuming language to language translation example: Decoder is supposed to generate a translation solely based on the last hidden state from the encoder. This vector must encode everything we need to know about the source sentence. It must fully capture its meaning. In more technical terms, that vector is a sentence embedding. In fact, if you plot the embeddings of different sentences in a low dimensional space using PCA or t-SNE for dimensionality reduction, you can see that semantically similar phrases end up close to each other. That\u2019s pretty amazing.\n\nStill, it seems somewhat unreasonable to assume that we can encode all information about a potentially very long sentence into a single vector and then have the decoder produce a good translation based on only that. Let\u2019s say your source sentence is 50 words long. The first word of the English translation is probably highly correlated with the first word of the source sentence. But that means decoder has to consider information from 50 steps ago, and that information needs to be somehow encoded in the vector. Recurrent Neural Networks are known to have problems dealing with such long-range dependencies. In theory, architectures like LSTMs should be able to deal with this, but in practice long-range dependencies are still problematic. For example, researchers have found that reversing the source sequence (feeding it backwards into the encoder) produces significantly better results because it shortens the path from the decoder to the relevant parts of the encoder. Similarly, feeding an input sequence twice also seems to help a network to better memorize things. It makes things work better in practice, but it\u2019s not a principled solution. Most translation benchmarks are done on languages like French and German, which are quite similar to English (even Chinese word order is quite similar to English). But there are languages (like Japanese) where the last word of a sentence could be highly predictive of the first word in an English translation. In that case, reversing the input would make things worse. So, what\u2019s an alternative? Attention Mechanisms.\n\nWith an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. Rather, we allow the decoder to \u201cattend\u201d to different parts of the source sentence at each step of the output generation. Importantly, we let the model learn what to attend to based on the input sentence and what it has produced so far. So, in languages that are pretty well aligned (like English and German) the decoder would probably choose to attend to things sequentially. Attending to the first word when producing the first English word, and so on. That\u2019s what was done in Neural Machine Translation by Jointly Learning to Align and Translate and look as follows:\n\nHere, The y\u2018s are our translated words produced by the decoder, and the x\u2018s are our source sentence words. The above illustration uses a bidirectional recurrent network, but that\u2019s not important. The important part is that each decoder output word yt now depends on a weighted combination of all the input states, not just the last state. The a\u2018s are weights that define in how much of each input state should be considered for each output. So, if a32 is a large number, this would mean that the decoder pays a lot of attention to the second state in the source sentence while producing the third word of the target sentence. The a\u2019s are typically normalized to sum to 1 (so they are a distribution over the input states). A big advantage of attention is that it gives us the ability to interpret and visualize what the model is doing. For example, by visualizing the attention weight matrix a when a sentence is translated, we can understand how the model is translating:\n\nHere we see that while translating from French to English, the network attends sequentially to each input state, but sometimes it attends to two words at time while producing an output, as in translation \u201cla Syrie\u201d to \u201cSyria\u201d for example.\n\nIf we look a bit more look closely at the equation for attention we can see that attention comes at a cost. We need to calculate an attention value for each combination of input and output word. If you have a 50-word input sequence and generate a 50-word output sequence that would be 2500 attention values. That\u2019s not too bad, but if you do character-level computations and deal with sequences consisting of hundreds of tokens the above attention mechanisms can become prohibitively expensive. That seems like a waste, and not at all what humans are doing. In fact, it\u2019s more akin to memory access, not attention, which in my opinion is somewhat of a misnomer (more on that below). Still, that hasn\u2019t stopped attention mechanisms from becoming quite popular and performing well on many tasks.\n\nAn alternative approach to attention is to use Reinforcement Learning to predict an approximate location to focus to. That sounds a lot more like human attention, and that\u2019s what\u2019s done in Recurrent Models of Visual Attention.\n\nAttention mechanism from above can be applied to any recurrent model.\n\nIn Show, Attend and Tell the authors apply attention mechanisms to the problem of generating image descriptions. They use a Convolutional Neural Network to \u201cencode\u201d the image, and a Recurrent Neural Network with attention mechanisms to generate a description. By visualizing the attention weights (just like in the translation example), we interpret what the model is looking at while generating a word:"
    },
    {
        "url": "https://medium.com/@ManishChablani/timeseries-forecasting-holtwinter-and-arima-498e70b052eb?source=user_profile---------12----------------",
        "title": "Timeseries forecasting: HoltWinter and ARIMA \u2013 Manish Chablani \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://towardsdatascience.com/probability-calibration-for-boosted-trees-24cbd0f0ccae?source=user_profile---------13----------------",
        "title": "Probability calibration for boosted trees \u2013",
        "text": "Boosted decision trees typically yield good accuracy, precision, and ROC area. However, because the outputs from boosting are not well calibrated posterior probabilities, boosting yields poor squared error and cross-entropy. It tends to predict probabilities conservatively, meaning closer to mid-range than to extremes. Here is the effect of boosting on predicted probability:\n\nWell calibrated classifiers are probabilistic classifiers for which the output of the model can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a prediction value close to 0.8, approximately 80% actually belong to the positive class. The following plot compares how well the probabilistic predictions of different classifiers are calibrated:\n\nLogisticRegression returns well calibrated predictions by default as it directly optimizes log-loss. In contrast, the other methods return biased probabilities; with different biases.\n\nTwo approaches for performing calibration of probabilistic predictions are: a parametric approach based on Platt\u2019s sigmoid model and a non-parametric approach based on isotonic regression. Probability calibration should be done using test/validation data that is not used for model fitting.\n\nPlatt\u2019s scaling amounts to training a logistic regression model on the classifier outputs.\n\nYou essentially create a new data set that has the same labels, but with one dimension (the probability output of the uncalibrated classifier. You then train on this new data set, and feed the probability output of the uncalibrated classifier as the input to this calibration method, which returns a calibrated probability. In Platt\u2019s case, we are essentially just performing logistic regression on the probability output of the uncalibrated classifier with respect to the true class labels.\n\nYou logistic regression model is f(x) = y. y is true label of input data and x is predicted probability of your base classifier. Now you use the predicted probability from logistic regression as your true probability from you calibrated classifier.\n\nThe idea is to fit a piecewise-constant non-decreasing function instead of logistic regression. Piecewise-constant non-decreasing means stair-step shaped. Implemented via Pool Adjacent Violators Algorithm (PAVA). PAVA is a linear time (and linear memory) algorithm for linear ordering isotonic regression.\n\nThe way you train a isotonic regression is similar:\n\nf(x) = y, y is true label of input data and x is predicted probability of your base classifier. Isotonic model is going to sort data by x(predicted probability of your base classifier) and then fit a step function to give probability of true label."
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-concepts-part-2-9aed45e5e7ed?source=user_profile---------14----------------",
        "title": "Deep learning concepts \u2014 PART 2 \u2013",
        "text": "Cross entropy is preferred loss function forvarious models like classification, segmentation, generative models,etc Here we explain the differences in behavior:\n\nSuppose you have just three training data items. Your neural network uses softmax activation for the output neurons so that there are three output values that can be interpreted as probabilities. For example suppose the neural network\u2019s computed outputs, and the target (aka desired) values are as follows:\n\nThis neural network has classification error of 1/3 = 0.33, or equivalently a classification accuracy of 2/3 = 0.67. Notice that the NN just barely gets the first two training items correct and is way off on the third training item. But now consider the following neural network:\n\nThis NN also has a classification error of 1/3 = 0.33. But this second NN is better than the first because it nails the first two training items and just barely misses the third training item. To summarize, classification error is a very crude measure of error.\n\nNow consider cross-entropy error. The cross-entropy error for the first training item in the first neural network above is:\n\nNotice that in the case of neural network classification, the computation is a bit odd because all terms but one will go away. (There are several good explanations of how to compute cross-entropy on the Internet.) So, the average cross-entropy error (ACE) for the first neural network is computed as:\n\nThe average cross-entropy error for the second neural network is:\n\nNotice that the average cross-entropy error for the second, superior neural network is smaller than the ACE error for the first neural network. The ln() function in cross-entropy takes into account the closeness of a prediction and is a more granular way to compute error.\n\nAlways a good idea to look at top few incorrect predictions (ones with largest loss) for each class of labels in validation set. It usually gives great insights on how your model behaves and also on how good or clean your labeled data is.\n\nBasic idea: Idea is to train a model using labeled data. use the model to predict labels for unlabeled data and then include the pseudo-labeled data as part of data to train the model along with labeled data assuming the pseudo-labels are true labels.\n\nFrom blog above: an efficient approach to do pseudo-labeling is, as mentioned here by the winner of 2015 National Data Science Bowl, to blend original data and pseudo-labeled data in a mini-batch by ratio of 67:33. This is also mentioned in fast.ai\u2019s lesson video.\n\nAnother implementation of pseudo-labeling in fast.ai lesson can be found in lesson 7 notebook here.\n\nIn the code above, Jeremy Howard, the lecturer of fast.ai course, wrote a custom iterator for mixing training data and pseudo-labeled data.\n\nmeasures the performance of a classification model where the prediction input is a probability value between 0 and 1. The goal of machine learning models is to minimize this value. A perfect model would have a log loss of 0. Log loss increases as the predicted probability diverges from the actual label.\n\nThe graph below shows the range of possible log loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n\nIn multi-class classification (M>2), we take the sum of log loss values for each class prediction in the observation.\n\nthe sum of all log loss values across classes\n\nLog Loss uses negative log to provide an easy metric for comparison. It takes this approach because the positive log of numbers < 1 returns negative values, which is confusing to work with when comparing the performance of two models.\n\nLog loss and cross-entropy are slightly different depending on the context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing. As a demonstration, where p and q are the sets p\u2208{y, 1\u2212y} and q\u2208{\u0177, 1\u2212\u0177} we can rewrite cross-entropy as:\n\nWhich is exactly the same as log loss!\n\nHere are good blog posts explaining this and apply to general machine learning/data mining:"
    },
    {
        "url": "https://towardsdatascience.com/gradient-descent-algorithms-and-adaptive-learning-rate-adjustment-methods-79c701b086be?source=user_profile---------15----------------",
        "title": "Gradient descent algorithms and adaptive learning rate adjustment methods",
        "text": "SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another [ 1 ], which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Image below. This helps accelerate SGD in the relevant direction and dampens oscillations\n\nVanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:\n\nMini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples.\n\nVanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \u03b8 for the entire training dataset.\n\nHere is a quick concise summary for reference. For more detailed explanation please read: http://ruder.io/optimizing-gradient-descent/\n\nWhile Momentum first computes the current gradient (small blue vector in Image 4) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks\n\nAbove methods adapt updates to the slope of our error function and speed up SGD in turn. Adagrad adapts updates to each individual parameter to perform larger or smaller updates depending on their importance.\n\nwe set g(t,i) to be the gradient of the objective function w.r.t. to the parameter \u03b8i at time step t:\n\nOne of Adagrad\u2019s main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.\n\nAdagrad\u2019s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.\n\nAdadelta [6] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size ww.\n\nRMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad\u2019s radically diminishing learning rates. RMSprop divides the learning rate by an exponentially decaying average of squared gradients.\n\nADAM computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum\n\nAdam can be viewed as a combination of RMSprop and momentum. We have also seen that Nesterov accelerated gradient (NAG) is superior to vanilla momentum. Nadam (Nesterov-accelerated Adaptive Moment Estimation) [24] thus combines Adam and NAG."
    },
    {
        "url": "https://towardsdatascience.com/using-gans-for-semi-supervised-learning-df1bfc289601?source=user_profile---------16----------------",
        "title": "Using GANS for semi-supervised learning \u2013",
        "text": "In supervised learning, we have a training set of inputs x and class labels y. We train a model that takes x as input and gives y as output.\n\nIn semi-supervised learning, our goal is still to train a model that takes x as input and generates y as output. However, not all of our training examples have a label y. We need to develop an algorithm that is able to get better at classification by studying both labeled (x,y) pairs and unlabeled x examples.\n\nWe will use a GAN discriminator as a n+1 class discriminator. It will recognize the n different classes of labeled data, as well as an (n+1)th class of fake images that come from the generator. The discriminator will get to train on real labeled images, real unlabeled images, and fake images. By drawing on three sources of data instead of just one, it will generalize to the test set much better than a traditional classifier trained on only one source of data.\n\nTypical generator that outputs fake images with same dimension as real images.\n\nDiscriminator is more complex than generator here due to unsupervised component.\n\nd_loss: the loss for the discriminator is combination of:\n\ng_loss: the loss for the generator is\u201cfeature matching\u201d loss invented by Tim Salimans at OpenAI. This loss consists of minimizing the absolute difference between the expected features on the data and the expected features on the generated samples. This loss works better for semi-supervised learning than the tradition GAN losses. Over time this will force generator to produce samples similar to real data."
    },
    {
        "url": "https://towardsdatascience.com/gans-part2-dcgans-deep-convolution-gans-for-generating-images-c5d3c7c3510e?source=user_profile---------17----------------",
        "title": "GANS \u2014 PART2: DCGANs (deep convolution GANS) for generating images",
        "text": "Deep Convolutional GAN, or DCGAN uses convolutional layers in the generator and discriminator. The DCGAN architecture was first explored in paper here. It\u2019s also necessary to use batch normalization to get the convolutional networks to train.\n\nThe first layer is a fully connected layer which is reshaped into a deep and narrow layer, something like 4x4x1024 as in the original DCGAN paper. Then we use batch normalization and a leaky ReLU activation. Next is a transposed convolution where typically you\u2019d halve the depth and double the width and height of the previous layer. Again, we use batch normalization and leaky ReLU. For each of these layers, the general scheme is convolution > batch norm > leaky ReLU.\n\nYou keep stacking layers up like this until you get the final transposed convolution layer with shape 32x32x3. Below is the archicture used in the original DCGAN paper:\n\nDiscriminator is basically just a convolutional classifier. Note that in the DCGAN paper, they did all the downsampling using only strided convolutional layers with no maxpool layers.\n\nUse batch normalization with on each layer except the first convolutional and output layers. Again, each layer should look something like convolution > batch norm > leaky ReLU."
    },
    {
        "url": "https://towardsdatascience.com/batch-normalization-8a2e585775c9?source=user_profile---------18----------------",
        "title": "Batch Normalization \u2013",
        "text": "The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to layers within the network. It\u2019s called \u201cbatch\u201d normalization because during training, we normalize each layer\u2019s inputs by using the mean and variance of the values in the current mini-batch (usually zero mean and unit variance).\n\nBatch normalization optimizes network training. It has been shown to have several benefits:"
    },
    {
        "url": "https://towardsdatascience.com/gan-introduction-and-implementation-part1-implement-a-simple-gan-in-tf-for-mnist-handwritten-de00a759ae5c?source=user_profile---------19----------------",
        "title": "GAN \u2014 Introduction and Implementation \u2014 PART1: Implement a simple GAN in TF for MNIST handwritten\u2026",
        "text": "The idea behind GANs is that you have two networks, a generator GG and a discriminator DD, competing against each other. The generator makes fake data to pass to the discriminator. The discriminator also sees real data and predicts if the data it\u2019s received is real or fake. The generator is trained to fool the discriminator, it wants to output data that looks as close as possible to real data. And the discriminator is trained to figure out which data is real and which is fake. What ends up happening is that the generator learns to make data that is indistinguishable from real data to the discriminator.\n\nThe general structure of a GAN is shown in the diagram above, using MNIST images as data. The latent sample is a random vector the generator uses to construct it\u2019s fake images. As the generator learns through training, it figures out how to map these random vectors to recognizable images that can fool the discriminator.\n\nThe output of the discriminator is a sigmoid function, where 0 indicates a fake image and 1 indicates an real image. If you\u2019re interested only in generating new images, you can throw out the discriminator after training.\n\nThe discriminator network is almost exactly the same as the generator network, except that we\u2019re using a sigmoid output layer.\n\nWe want to update the generator and discriminator variables separately.\n\nNote that when minimizing d_loss we want optimizer to only be updating the discriminator vars and similar for generator."
    },
    {
        "url": "https://towardsdatascience.com/autoencoders-introduction-and-implementation-3f40483b0a85?source=user_profile---------20----------------",
        "title": "Autoencoders \u2014 Introduction and Implementation in TF.",
        "text": "Autoencoders (AE) are a family of neural networks for which the input is the same as the output (they implement a identity function). They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation.\n\nA really popular use for autoencoders is to apply them to images. The trick is to replace fully connected layers by convolutional layers. These, along with pooling layers, convert the input from wide and thin (let\u2019s say 100 x 100 px with 3 channels \u2014 RGB) to narrow and thick. This helps the network extract visual features from the images, and therefore obtain a much more accurate latent space representation. The reconstruction process uses upsampling and convolutions.\n\nConvolutional autoencoders can be useful for reconstruction. They can, for example, learn to remove noise from picture, or reconstruct missing parts.\n\nTo do so, we don\u2019t use the same image as input and output, but rather a noisy version as input and the clean version as output. With this process, the networks learns to fill in the gaps in the image.\n\nLet\u2019s see what a CAE can do to replace part of an image of an eye. Let\u2019s say there\u2019s a crosshair and we want to remove it. We can manually create the dataset, which is extremely convenient.\n\nNow that our autoencoder is trained, we can use it to remove the crosshairs on pictures of eyes we have never seen!\n\nLets go over a sample implementation using MNIST dataset in tensorflow.\n\nThe encoder part of the network will be a typical convolutional pyramid. Each convolutional layer will be followed by a max-pooling layer to reduce the dimensions of the layers. The decoder needs to convert from a narrow representation to a wide reconstructed image.\n\nUsually, you\u2019ll see transposed convolution layers used to increase the width and height of the layers. They work almost exactly the same as convolutional layers, but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a 3x3 kernel, a 3x3 patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded to a 3x3 path in a transposed convolution layer. The TensorFlow API provides us with an easy way to create the layers, .\n\nAutoencoders can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1. We\u2019ll use noisy images as input and the original, clean images as targets."
    },
    {
        "url": "https://towardsdatascience.com/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d?source=user_profile---------21----------------",
        "title": "Sequence to sequence model: Introduction and concepts",
        "text": "If we take a high-level view, a seq2seq model has encoder, decoder and intermediate step as its main components:\n\nWe use embedding, so we have to first compile a \u201cvocabulary\u201d list containing all the words we want our model to be able to use or read. The model inputs will have to be tensors containing the IDs of the words in the sequence.\n\nThere are four symbols, however, that we need our vocabulary to contain. Seq2seq vocabularies usually reserve the first four spots for these elements:\n\nNote: Other tags can be used to represent these functions. For example I\u2019ve seen <s> and </s> used in place of <GO> and <EOS>. So make sure whatever you use is consistent through preprocessing, and model training/inference.\n\nPreparing the inputs for the training graph is a little more involved for two reasons:\n\nOne of the original sequence to sequence papers, Sutskever et al. 2014, reported better model performance if the inputs are reversed. So you may also choose to reverse the order of words in the input sequence.\n\nDuring the preprocessing we do the following:"
    },
    {
        "url": "https://towardsdatascience.com/sentiment-analysis-using-rnns-lstm-60871fa6aeba?source=user_profile---------22----------------",
        "title": "Sentiment analysis using RNNs(LSTM) \u2013",
        "text": "Here we use the example of reviews to predict sentiment (even though it can be applied more generically to other domains for example sentiment analysis for tweets, comments, customer feedback, etc). Whole idea here is that movie reviews are made of sequence of words and order of words encode lot of information that is useful to predict sentiment. Step 1 is to map words to word embeddings (see post 1 and 2 for more context on word embeddings). Step 2 is the RNN that receives a sequence of vectors as input and considers the order of the vectors to generate prediction.\n\nThe architecture for this network is shown below.\n\nFrom the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data. Finally, the LSTM cells will go to a sigmoid output layer here. We\u2019re using the sigmoid because we\u2019re trying to predict if this text has positive or negative sentiment. The output layer will just be a single unit then, with a sigmoid activation function.\n\nWe don\u2019t care about the sigmoid outputs except for the very last one, we can ignore the rest. We\u2019ll calculate the cost from the output of the last step and the training label.\n\nHave fixed length reviews encoded as integers and then converted to embedding vectors passed to LSTM layers in recurrent manner and pick the last prediction as output sentiment.\n\nOne thing in my experiments I could not explain is when I encode the words to integers if I randomly assign unique integers to words the best accuracy I get is 50\u201355% (basically the model is not doing much better than random guessing). However if the words are encoded such that highest frequency words get the lowest number then the model accuracy is 80% in 3\u20135 epochs. My guess is this is necessary to train the embedding layer but cannot find an explanation on why anywhere.\n\nTake all the words in reviews and encode them with integers. Now each review is an ordered array of integers. Make each review fixed size (say 200), so shorter reviews get padded with 0\u2019s in front and longer reviews get truncated to 200. Since we are padding with 0\u2019s the corpus of words to int mapping starts with 1. Labels are are encoded as 1s and 0s for \u2018positive\u2019 and \u2018negative\u2019."
    },
    {
        "url": "https://towardsdatascience.com/word2vec-skip-gram-model-part-2-implementation-in-tf-7efdf6f58a27?source=user_profile---------23----------------",
        "title": "Word2Vec (skip-gram model): PART 2 \u2014 Implementation in TF",
        "text": "Tensor flow has built in support for most of the scaffolding needed for skip-gram Word2Vect including embedding lookup and negative sampling.\n\nTokenize the input and convert the input into int representation. Have look ups for word to int and vice versa.\n\nWords that show up often such as \u201cthe\u201d, \u201cof\u201d, and \u201cfor\u201d don\u2019t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word wi in the training set, we\u2019ll discard it with probability given by\n\nNote labels is 2 dimensional as required by tf.nn.sampled_softmax_loss used for negative sampling.\n\nThe embedding matrix has a size of the number of words by the number of units in the hidden layer. So, if you have 10,000 words and 300 hidden units, the matrix will have size 10,000\u00d7300. Remember that we\u2019re using tokenized data for our inputs, usually as integers, where the number of tokens is the number of words in our vocabulary.\n\nWe\u2019ll update the weights for the correct label, but only a small number of incorrect labels. This is called \u201cnegative sampling\u201d. Tensorflow has a convenient function to do this, ."
    },
    {
        "url": "https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b?source=user_profile---------24----------------",
        "title": "Word2Vec (skip-gram model): PART 1 - Intuition. \u2013",
        "text": "Most of the content here is from Chris\u2019s blog. I have condensed it and made minor adaptations.\n\nThe algorithm exists in two flavors CBOW and Skip-Gram. Given a set of sentences (also called corpus) the model loops on the words of each sentence and either tries to use the current word of to predict its neighbors (its context), in which case the method is called \u201cSkip-Gram\u201d, or it uses each of these contexts to predict the current word, in which case the method is called \u201cContinuous Bag Of Words\u201d (CBOW). The limit on the number of words in each context is determined by a parameter called \u201cwindow size\u201d.\n\nThe skip-gram neural network model is actually surprisingly simple in its most basic form. Train a simple neural network with a single hidden layer to perform a certain task, but then we\u2019re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer\u2013we\u2019ll see that these weights are actually the \u201cword vectors\u201d that we\u2019re trying to learn.\n\nWe\u2019re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the \u201cnearby word\u201d that we chose.\n\nThe output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word \u201cSoviet\u201d, the output probabilities are going to be much higher for words like \u201cUnion\u201d and \u201cRussia\u201d than for unrelated words like \u201cwatermelon\u201d and \u201ckangaroo\u201d.\n\nWe\u2019ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence \u201cThe quick brown fox jumps over the lazy dog.\u201d I\u2019ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.\n\nWe\u2019re going to represent an input word like \u201cants\u201d as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we\u2019ll place a \u201c1\u201d in the position corresponding to the word \u201cants\u201d, and 0s in all of the other positions.\n\nThe output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n\nHere\u2019s the architecture of our neural network.\n\nThere is no activation function on the hidden layer neurons, but the output neurons use softmax.\n\nFor our example, we\u2019re going to say that we\u2019re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n\n300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a \u201chyper parameter\u201d that you would just have to tune to your application (that is, try different values and see what yields the best results).\n\nIf you look at the rows of this weight matrix, these are actually what will be our word vectors!\n\nSo the end goal of all of this is really just to learn this hidden layer weight matrix \u2014 the output layer we\u2019ll just toss when we\u2019re done! The word vector for \u201cants\u201d then gets fed to the output layer. The output layer is a softmax regression classifier.\n\nSpecifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes.\n\nHere\u2019s an illustration of calculating the output of the output neuron for the word \u201ccar\u201d.\n\nAnd what does it mean for two words to have similar contexts? I think you could expect that synonyms like \u201cintelligent\u201d and \u201csmart\u201d would have very similar contexts. Or that words that are related, like \u201cengine\u201d and \u201ctransmission\u201d, would probably have similar contexts as well.\n\nThis can also handle stemming for you \u2014 the network will likely learn similar word vectors for the words \u201cant\u201d and \u201cants\u201d because these should have similar contexts.\n\nWe need few additional modifications to the basic skip-gram model which are important for actually making it feasible to train. Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast. The authors of Word2Vec addressed these issues in their second paper.\n\nThere are three innovations in this second paper:\n\nIt\u2019s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.\n\nThere are two \u201cproblems\u201d with common words like \u201cthe\u201d:\n\nWord2Vec implements a \u201csubsampling\u201d scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word\u2019s frequency.\n\nIf we have a window size of 10, and we remove a specific instance of \u201cthe\u201d from our text:\n\nNote how these two effects help address the two problems stated above.\n\nAs we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!\n\nNegative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here\u2019s how it works.\n\nWhen training the network on the word pair (\u201cfox\u201d, \u201cquick\u201d), recall that the \u201clabel\u201d or \u201ccorrect output\u201d of the network is a one-hot vector. That is, for the output neuron corresponding to \u201cquick\u201d to output a 1, and for all of the other thousands of output neurons to output a 0.\n\nWith negative sampling, we are instead going to randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for. (In this context, a \u201cnegative\u201d word is one for which we want the network to output a 0 for). We will also still update the weights for our \u201cpositive\u201d word (which is the word \u201cquick\u201d in our current example).\n\nThe paper says that selecting 5\u201320 words works well for smaller datasets, and you can get away with only 2\u20135 words for large datasets.\n\nRecall that the output layer of our model has a weight matrix that\u2019s 300 x 10,000. So we will just be updating the weights for our positive word (\u201cquick\u201d), plus the weights for 5 other words that we want to output 0. That\u2019s a total of 6 output neurons, and 1,800 weight values total. That\u2019s only 0.06% of the 3M weights in the output layer!\n\nIn the hidden layer, only the weights for the input word are updated (this is true whether you\u2019re using Negative Sampling or not).\n\nThe \u201cnegative samples\u201d (that is, the 5 output words that we\u2019ll train to output 0) are chosen using a \u201cunigram distribution\u201d.\n\nEssentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples."
    },
    {
        "url": "https://towardsdatascience.com/rnn-training-tips-and-tricks-2bf687e67527?source=user_profile---------25----------------",
        "title": "RNN Training Tips and Tricks: \u2013",
        "text": "Here\u2019s some good advice from Andrej Karpathy on training the RNN pipeline. Link to where it originally came from.\n\nIf you\u2019re somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n\nThe two most important parameters that control the model are and . I would advise that you always use of either 2/3. The can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n\nThese two should be about the same order of magnitude. It\u2019s a little tricky to tell. Here are some examples:\n\nThe winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you\u2019re willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n\nIt is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n\nBy the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-concepts-part-1-ea0b14b234c8?source=user_profile---------26----------------",
        "title": "Deep learning concepts \u2014 PART 1 \u2013",
        "text": "Previously, we\u2019ve been using the sigmoid function as the activation function on our hidden units and, in the case of classification, on the output unit. However, this is not the only activation function you can use and actually has some drawbacks.\n\nAs noted in the backpropagation material, the derivative of the sigmoid maxes out at 0.25 (see above). This means when you\u2019re performing backpropagation with sigmoid units, the errors going back into the network will be shrunk by at least 75% at every layer. For layers close to the input layer, the weight updates will be tiny if you have a lot of layers and those weights will take a really long time to train. Due to this, sigmoids have fallen out of favor as activations on hidden units.\n\nInstead of sigmoids, most recent deep learning networks use rectified linear units (ReLUs) for the hidden layers. A rectified linear unit has output 0 if the input is less than 0, and raw output otherwise. That is, if the input is greater than 0, the output is equal to the input. Mathematically, that looks like\n\nThe output of the function is either the input, x, or 0, whichever is larger. So if x=\u22121, then f(x)=0 and if x=0.5, then f(x)=0.5. Graphically, it looks like:\n\nReLU activations are the simplest non-linear activation function you can use. When the input is positive, the derivative is 1, so there isn\u2019t the vanishing effect you see on backpropagated errors from sigmoids. Research has shown that ReLUs result in much faster training for large networks. Most frameworks like TensorFlow and TFLearn make it simple to use ReLUs on the the hidden layers, so you won\u2019t need to implement them yourself.\n\nIt\u2019s possible that a large gradient can set the weights such that a ReLU unit will always be 0. These \u201cdead\u201d units will always be 0 and a lot of computation will be wasted in training.\n\nPreviously we\u2019ve seen neural networks used for regression (bike riders) and binary classification (graduate school admissions). Often you\u2019ll find you want to predict if some input belongs to one of many classes. This is a classification problem, but a sigmoid is no longer the best choice. Instead, we use the softmax function. The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid. It also divides each output such that the total sum of the outputs is equal to 1. The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.\n\nThe only real difference between this and a normal sigmoid is that the softmax normalizes the outputs so that they sum to one. In both cases you can put in a vector and get out a vector where the outputs are a vector of the same size, but all the values are squashed between 0 and 1. You would use a sigmoid with one output unit for binary classification. But if you\u2019re doing multinomial classification, you\u2019d want to use multiple output units (one for each class) and the softmax activation on the output.\n\nFor example if you have three inputs to a softmax function, say for a network with three output units, it\u2019d look like:\n\nMathematically the softmax function is shown below, where z is a vector of the inputs to the output layer (if you have 10 output units, then there are 10 elements in z). And again, j indexes the output units.\n\nThis admittedly looks daunting to understand, but it\u2019s actually quite simple and it\u2019s fine if you don\u2019t get the math. Just remember that the outputs are squashed and they sum to one.\n\nTo understand this better, think about training a network to recognize and classify handwritten digits from images. The network would have ten output units, one for each digit 0 to 9. Then if you fed it an image of a number 4 (see below), the output unit corresponding to the digit 4 would be activated.\n\nBuilding a network like this requires 10 output units, one for each digit. Each training image is labeled with the true digit and the goal of the network is to predict the correct label. So, if the input is an image of the digit 4, the output unit corresponding to 4 would be activated, and so on for the rest of the units.\n\nFor the example image above, the output of the softmax function might look like:\n\nExample softmax output for a network predicting the digit shown above\n\nThe image looks the most like the digit 4, so you get a lot of probability there. However, this digit also looks somewhat like a 7 and a little bit like a 9 without the loop completed. So, you get the most probability that it\u2019s a 4, but also some probability that it\u2019s a 7 or a 9.\n\nThe softmax can be used for any number of classes. As you\u2019ll see next, it will be used to predict two classes of sentiment, positive or negative. It\u2019s also used for hundreds and thousands of classes, for example in object recognition problems where there are hundreds of different possible objects.\n\nBelow are the few properties of softmax function.\n\nPreviously we\u2019ve been using the sum of squared errors as the cost function in our networks, but in those cases we only have singular (scalar) output values.\n\nWhen you\u2019re using softmax, however, your output is a vector. One vector is the probability values from the output units. You can also express your data labels as a vector using what\u2019s called one-hot encoding.\n\nThis just means that you have a vector the length of the number of classes, and the label element is marked with a 1 while the other labels are set to 0. In the case of classifying digits from before, our label vector for the image of the number 4 would be:\n\nAnd our output prediction vector could be something like\n\nWe want our error to be proportional to how far apart these vectors are. To calculate this distance, we\u2019ll use the cross entropy. Then, our goal when training the network is to make our prediction vectors as close as possible to the label vectors by minimizing the cross entropy. The cross entropy calculation is shown below:\n\nAs you can see above, the cross entropy is the sum of the label elements times the natural log of the prediction probabilities. Note that this formula is not symmetric! Flipping the vectors is a bad idea because the label vector has a lot of zeros and taking the log of zero will cause an error.\n\nWhat\u2019s cool about using one-hot encoding for the label vector is that y\u200bj\u200b\u200b is 0 except for the one true class. Then, all terms in that sum except for where y\u200bj\u200b\u200b=1 are zero and the cross entropy is simply D=\u2212ln\u200by\u200b^\u200b\u200b for the true label. For example, if your input image is of the digit 4 and it\u2019s labeled 4, then only the output of the unit corresponding to 4 matters in the cross entropy cost."
    },
    {
        "url": "https://medium.com/@ManishChablani/jupyter-notebook-hacks-b16823a44381?source=user_profile---------27----------------",
        "title": "Jupyter notebook hacks \u2013 Manish Chablani \u2013",
        "text": "With the Python kernel, you can turn on the interactive debugger using the magic command %pdb ON. When you cause an error, you'll be able to inspect the variables in the current namespace.\n\nAt some point, you\u2019ll probably spend some effort optimizing code to run faster. Timing how quickly your code runs is essential for this optimization. You can use the magic command to time how long it takes for a function to run. If you want to time how long it takes for a whole cell to run, you\u2019d use\n\nSince notebooks are JSON, it is simple to convert them to other formats. Jupyter comes with a utility called for converting to HTML, Markdown, slideshows, etc.\n\nFor example, to convert a notebook to an HTML file, in your terminal use\n\nConverting to HTML is useful for sharing your notebooks with others who aren\u2019t using notebooks. Markdown is great for including a notebook in blogs and other text editors that accept Markdown formatting."
    },
    {
        "url": "https://towardsdatascience.com/https-medium-com-manishchablani-useful-keras-features-4bac0724734c?source=user_profile---------28----------------",
        "title": "Useful Keras features \u2013",
        "text": "Here is the summary of interesting features that I feel I will find useful to reference when I am building a deep learning pipeline a.k.a things I usually don\u2019t remember. From Keras documentation at https://keras.io and other online posts.\n\nYou can use an callback:\n\nOne simple way is to create a new that will output the layers that you are interested in:\n\nAlternatively, you can build a Keras function that will return the output of a certain layer given a certain input, for example:\n\nSimilarly, you could build a Theano and TensorFlow function directly.\n\nNote that if your model has a different behavior in training and testing phase (e.g. if it uses , , etc.), you will need to pass the learning phase flag to your function:\n\nCode and pre-trained weights are available for the following image classification models:\n\nThey can be imported from the module :\n\nFor a few simple usage examples, see the documentation for the Applications module.\n\nFor a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see this blog post.\n\nThe VGG16 model is also the basis for several Keras example scripts:\n\nThese are just a few of the options available (for more, see the documentation). Let's quickly go over what we just wrote:\n\nNow let\u2019s start generating some pictures using this tool and save them to a temporary directory, so we can get a feel for what our augmentation strategy is doing \u2014 we disable rescaling in this case to keep the images displayable:\n\nLet\u2019s prepare our data. We will use to generate batches of image data (and their labels) directly from our jpgs in their respective folders.\n\nWe can now use these generators to train our model. Each epoch takes 20\u201330s on GPU and 300\u2013400s on CPU. So it\u2019s definitely viable to run this model on CPU if you aren\u2019t in a hurry.\n\nThis approach gets us to a validation accuracy of 0.79\u20130.81 after 50 epochs (a number that was picked arbitrarily \u2014 because the model is small and uses aggressive dropout, it does not seem to be overfitting too much by that point)\n\nOur strategy will be as follow: we will only instantiate the convolutional part of the model, everything up to the fully-connected layers. We will then run this model on our training and validation data once, recording the output (the \u201cbottleneck features\u201d from th VGG16 model: the last activation maps before the fully-connected layers) in two numpy arrays. Then we will train a small fully-connected model on top of the stored features.\n\nThe reason why we are storing the features offline rather than adding our fully-connected model directly on top of a frozen convolutional base and running the whole thing, is computational effiency. Running VGG16 is expensive, especially if you\u2019re working on CPU, and we want to only do it once. Note that this prevents us from using data augmentation.\n\nWe reach a validation accuracy of 0.90\u20130.91: not bad at all. This is definitely partly due to the fact that the base model was trained on a dataset that already featured dogs and cats (among hundreds of other classes).\n\nTo further improve our previous result, we can try to \u201cfine-tune\u201d the last convolutional block of the VGG16 model alongside the top-level classifier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n\nIn the case of classification with 10 categories (CIFAR10, MNIST).\n\nIt means that if you have a 3D 8,8,128 tensor at the end of your last convolution, in the traditional method, you flatten it into a 1D vector of size 8x8x128. And you then add one or several fully connected layers and then at the end, a softmax layer that reduces the size to 10 classification categories and applies the softmax operator.\n\nThe global average pooling means that you have a 3D 8,8,10 tensor and compute the average over the 8,8 slices, you end up with a 3D tensor of shape 1,1,10 that you reshape into a 1D vector of shape 10. And then you add a softmax operator without any operation in between. The tensor before the average pooling is supposed to have as many channels as your model has classification categories.\n\nThe paper is not clear, but when they say \u201csoftmax layer\u201d they mean softmax operator only, not a fully conneted layer with a softmax activation.\n\nMaking a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.\n\nWhen using stateful RNNs, it is therefore assumed that:\n\nTo use statefulness in RNNs, you need to:\n\nNotes that the methods , , , , etc. will all update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction."
    }
]