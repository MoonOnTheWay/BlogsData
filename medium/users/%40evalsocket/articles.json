[
    {
        "url": "https://medium.com/@evalsocket/horovod-tensor-flow-uber-ccbaa857f84d?source=user_profile---------1----------------",
        "title": "Horovod ? Tensor flow ? Uber ? \u2013 Yuvraj \u2013",
        "text": "Horovod is a distributed training framework for TensorFlow. The goal of Horovod is to make distributed Deep Learning fast and easy to use.\n\nThe primary motivation for this project is to make it easy to take a single-GPU TensorFlow program and successfully train it on many GPUs faster. This has two aspects:\n\nInternally at Uber they found the MPI model to be much more straightforward and require far less code changes than the Distributed TensorFlow with parameter servers. See the Usage section for more details.\n\nHorovod is fast. Below is a chart representing the benchmark that was done on 32 servers with 4 Pascal GPUs each connected by RoCE-capable 25 Gbit/s network\n\nHorovod achieves 90% scaling efficiency for both Inception V3 and ResNet-101, and 79% scaling efficiency for VGG-16. See the Benchmarks page to find out how to reproduce these numbers.\n\nTo use Horovod, make the following additions to your program:\n\nExample (see the examples directory for full training examples)\n\nAs a parting note I wish you to remain productive, and optimize your tools as much as you can (without using this as an excuse not to work )"
    },
    {
        "url": "https://medium.com/@evalsocket/k8s-crd-for-tensorflow-c51dd236c4fa?source=user_profile---------2----------------",
        "title": "K8s CRD for Tensorflow \u2013 Yuvraj \u2013",
        "text": "we all know that Kubernetes is an open source system for managing containerized applications across multiple hosts, providing basic mechanisms for deployment, maintenance, and scaling of applications.\n\nKubernetes builds upon a decade and a half of experience at Google running production workloads at scale using a system called Borg, combined with best-of-breed ideas and practices from the community.\n\nTake a free course on Scalable Microservices with Kubernetes.\n\nTfJob provides a Kubernetes custom resource that makes it easy to run distributed or non-distributed TensorFlow jobs on Kubernetes.\n\nFor additional information about motivation and design for the CRD please refer to tf_job_design_doc.md.\n\nThere is 3 open source project for performing large scale machine learning Jobs or real time predication architecture\n\nCombine 3 Projects give you a easily salable architecture for your tensor flow Model\u2019s training and prediction\n\nEach replicaSpec defines a set of TensorFlow processes. The tfReplicaType defines the semantics for the set of processes. The semantics are as follows\n\nFor each replica you define a template which is a K8s PodTemplateSpec. The template allows you to specify the containers, volumes, etc\u2026 that should be created for each replica.\n\nAs a parting note I wish you to remain productive, and optimize your tools as much as you can (without using this as an excuse not to work ;))."
    },
    {
        "url": "https://medium.com/@evalsocket/polygram-7f9e8ca6472f?source=user_profile---------3----------------",
        "title": "Polygram :) \u2013 Yuvraj \u2013",
        "text": "Human facial expressions can be easily classified into 7 basic emotions: happy, sad, surprise, fear, anger, disgust, and neutral. Our facial emotions are expressed through activation of specific sets of facial muscles.\n\nHumans are well-trained in reading the emotions of others, in fact, at just 14 months old, babies can already tell the difference between happy and sad. But can computers do a better job than us in accessing emotional states? To answer the question\n\nThe dataset I used for training the model is from a Kaggle Facial Expression Recognition Challenge a few years back (FER2013).\n\nIt comprises a total of 35887 pre-cropped, 48-by-48-pixel grayscale images of faces each labeled with one of the 7 emotion classes: anger, disgust, fear, happiness, sadness, surprise, and neutral.\n\nI chose convolutional neural network (CNN) layers as building blocks to create my model architecture. CNNs are known to imitate how the human brain works when analyzing visuals.\n\nA typical architecture of a convolutional neural network will contain an input layer, some convolutional layers, some dense layers (aka. fully-connected layers), and an output layer . These are linearly stacked layers ordered in sequence.\n\nThe input layer has pre-determined, fixed dimensions, so the image must be pre-processed before it can be fed into the layer.\n\nI used OpenCV, a computer vision library, for face detection in the image. The in OpenCV contains pre-trained filters and uses to quickly find and crop the face.\n\nThe cropped face is then converted into grayscale using and resized to 48-by-48 pixels with .\n\nThis step greatly reduces the dimensions compared to the original RGB format with three color dimensions (3, 48, 48). The pipeline ensures every image can be fed into the input layer as a (1, 48, 48) numpy array.\n\nThe numpy array gets passed into the layer where I specify the number of filters as one of the hyperparameters. The set of filters are unique with randomly generated weights. Each filter, (3, 3) receptive field, slides across the original image with shared weights to create a feature map.\n\nConvolution generates feature maps that represent how pixel values are enhanced, for example, edge and pattern detection.\n\nPooling is a dimension reduction technique usually applied after one or several convolutional layers. It is an important step when building CNNs as adding more convolutional layers can greatly affect computational time.\n\nI used a popular pooling method called that uses (2, 2) windows across the feature map only keeping the maximum pixel value. The pooled pixels form an image with dimentions reduced by 4.\n\nThe dense layer, is inspired by the way neurons transmit signals through the brain. It takes a large number of input features and transform features through layers connected with trainable weights.\n\nThese weights are trained by forward propagation of training data then backward propagation of its errors. Back propagation starts from evaluating the difference between prediction and true value, and back calculates the weight adjustment needed to every layer before. We can control the training speed and the complexity of the architecture by tuning the hyper-parameters, such as learning rate and network density\n\nInstead of using sigmoid activation function, I used softmax at the output layer. This output presents itself as a probability for each emotion class.\n\nI built a simple CNN with an input, three convolution layers, one dense layer, and an output layer to start with. As it turned out, the simple model preformed poorly. The low accuracy of 0.0500 showed that it was merely random guessing one of the six emotions\n\nThe simple net architecture failed to pick up the subtle details in facial expressions. This could only mean one thing\n\nThis is where deep learning comes in. Given the pattern complexity of facial expressions, it is necessary to build with a deeper architecture in order to identify subtle signals. So I fiddled combinations of three components to increase model complexity:\n\nThe model performs really well on classifying positive emotions resulting in relatively high precision scores for happy and surprised. Happy has a precision of 85.7% which could be explained by having the most examples (~7000) in the training set. Interestingly, surprise has a precision of 74.3% having the least examples in the training set. There must be very strong signals in the suprise expressions.\n\nModel performance seems weaker across negative emotions on average. In particularly, the emotion sad has a low precision of only 42%. The model frequently misclassified angry, fear and neutral as sad. In addition, it is most confused when predicting sad and neutral faces because these two emotions are probably the least expressive (excluding crying faces)."
    },
    {
        "url": "https://medium.com/@evalsocket/connection-pooling-php-3a9324a897ec?source=user_profile---------4----------------",
        "title": "Connection Pooling PHP \u2013 Yuvraj \u2013",
        "text": "After a long conversation,I started writing code on reactPHP and now i am fan of it. In this example Let\u2019s only focus on event-loop , socket and http and build something that make sense.\n\nYou can download example from github \ud83d\ude09\n\nLike every language php also have its own package manger called composer. To start doing awesome stuff you just need composer\n\nLine 3\u20136 : First of all we require all libraries and use their name space to access them\n\nLine 11\u201313 : we define two variable config and connection\n\nLine 15 : We add a timer in loop similar like setTimeOut. timer emit a callback after 0.001s with two parameter (value by address)connection and config. config use to connect any db connection or may be a queue connection. we simple create an array and assign to a variable i.e connection.\n\nLine 22- 30 : we create a server handler and pass a parameter called connection and we can use it for our business logic then return\n\nLine 32 : Create a Server object and start listen to that server at 3001"
    },
    {
        "url": "https://medium.com/@evalsocket/async-php-f6a28ad2b8e0?source=user_profile---------5----------------",
        "title": "Async PHP ? \u2013 Yuvraj \u2013",
        "text": "A few months ago, my colleague show me a project ReatPHP. ReactPHP is an awesome topic on its own, but it\u2019s not the essence of what I\u2019ve recently experienced. PHP is an old language, but we can use it to build new and exciting things. It can work with old concepts (like message queues and event emitters) to create fascinating, new designs.\n\nLet\u2019s divert our attention to the problem ReactPHP was made to solve, and then we\u2019ll look at how ReactPHP solves that problem\n\nHalf of the work is being done by Apache/Nginx/etc. They are handling all the socket management and HTTP message negotiation. They are divvying up the traffic and sending the relevant requests to mod_php.\n\nBy the time us PHP folk get these requests, they\u2019ve already been parsed and prepared. Fantastic!\n\nThis is one of the reasons PHP is so popular. It\u2019s so easy to get started. Yet that ease-of-use is limiting our mental model for application architecture. The closest our dear PHP gets to the NodeJS Way Of Doing Things is the development server.\n\nwe have an \u201copcode cache\u201d we have to declare classes, instantiate your objects, read your caches, etc for every single request. As you can surely imagine this is very time consuming and far away from being a perfect setup for high performance.\n\nSo, why are we actually doing that? Why, after every request, are we destroying the memory used by PHP for our application and re-creating it every time? Well, it\u2019s because PHP wasn\u2019t designed to be a server itself but only a template engine, a set of tools, when it was created. Also PHP itself isn\u2019t really designed to be asynchronous \u2014 almost all functions are \u201cblocking\u201d. Over the years, things have changed dramatically. Today, we have very powerful template engines written in PHP. We have a big ecosystem of thousands of useful libraries easily installable thanks to Composer. We have implemented very powerful design patters in PHP coming from Java and other languages and we even have libraries for asynchronous web servers in PHP.\n\nWait, we have asynchronous tools in PHP? Yes, we have. ReactPHP is one of the most promising libraries to me. It brings the powerful concept of event-driven, non-blocking I/O (Hi NodeJS) to PHP. With this technology in mind we are able to write our own HTTP stack in PHP and have control back over the memory without destroying it at the end of each request.\n\nI guess it\u2019s pretty easy to understand, that when we don\u2019t have to instantiate all of our objects, read the cache, etc we would gain much more performance as most of the time the bootstrap of our application is a significant factor of our response time. When we remove that factor we would be in the same boat as Java & NodeJS , which means performance! Yay\n\nChris Boden and Igor Wiedler joined forces to bring the PHP community a precious gift. And though others have stepped in to help, these gifted developers were instrumental in the evolution of PHP.\n\nReactPHP is a collection of libraries, but those that seem to get most of the spotlight are the event loop and socket/HTTP servers that go with it. Already heard of ReactPHP? It\u2019s probably because of code like this:\n\nIt looks a lot like the NodeJS reference HTTP server implementation, doesn\u2019t it? There\u2019s a bit more to do with the event loop, but otherwise it\u2019s almost the same. It\u2019s based on similar operating principles, though it has to achieve them using many different tools.It\u2019s also a little clearer that the event loop is involved\n\nWhat was I saying, again? Oh yes! PHP is mostly blocking, which means we\u2019re still stuck for how to make all this non-blocking HTTP stuff work for us. What good is high-concurrency if requests are still blocking the moment we need to do any meaningful work?\n\nWe could use any of the wonderful options; like Pthreads and Gearman.\n\nIt\u2019s pretty easy. What we need is ReactPHP, Docker :)\n\nor you can do some manual stuff ReactPHP read ReactPHP\u2019s README.md. example are available on example folder."
    },
    {
        "url": "https://medium.com/@evalsocket/tensor-flow-start-with-zero-nothing-to-loose-ac4b2dd8ef22?source=user_profile---------7----------------",
        "title": "Tensor flow,Start With Zero Nothing To Loose \u2013 Yuvraj \u2013",
        "text": "Tensor Flow is the machine learning framework that Google created and used to design, build, and train deep learning models.You can use the Tensor Flow library do to numerical computations, which in itself doesn\u2019t seem all too special, but these computations are done with data flow graphs. In these graphs, nodes represent mathematical operations, while the edges represent the data, which usually are multidimensional data arrays or tensors, that are communicated between these edges.\n\nYou see? The name \u201cTensor Flow\u201d is derived from the operations which neural networks perform on multidimensional data arrays or tensors! It\u2019s literally a flow of tensors.\n\nTo understand tensors well, it\u2019s good to have some working knowledge of linear algebra and vector calculus. Tensors are implemented in Tensor Flow as multidimensional data arrays, but some more introduction is maybe needed in order to completely grasp tensors and their use in machine learning.\n\nA tensor, then, is the mathematical representation of a physical entity that may be characterized by magnitude and multiple directions.\n\nBefore learning Tensor flow, Let\u2019s start with, what is a computational graph\n\nThe idea behind TensorFlow is to ability to create these computational graphs in code and allow significant performance improvements via parallel operations and other efficiency gains.\n\nDont worry, I\u2019m going to explain each line of code below. I\u2019d advise you to open this post in two windows, so you could look at the code and explanations simultaneously. Before that let\u2019s talk about about how Tensorflow works; To make our code execute faster and avoid drawbacks of languages like Python, which inherently trades speed with readability, Everything in TensorFlow is based on creating a computational graph. Think of a computational graph as a network of nodes, with each node known as an operation, running some function that can be as simple as addition or subtraction to as complex as some multi variate equation. Then the graph is executed in a Tensorflow session with fast C++ backend. Variables in TensorFlow are managed by the Session.\n\nNow we know enough to dive in and get our hands dirty with code, which is the fastest way to learn.\n\nLine 1: It simply imports the Tensorflow library where all the awesomeness resides.\n\nLine 4\u201316: We simply assigned the value of 1 and 0 (2nd rank tensor),We have 3 dimensional space and 4 record and label output with a 1st rank tensor.\n\nLine 19\u201320: w is a constant we choose (typically 3*1 matrix ) and let our computational graph decide the weight with which the constant term should be multiplied to get the desired function we intend to approximate. To learn more about bias; refer to this Stack-overflow thread. The values are initialized to normally-distributed random numbers, for that reason we used tf.random_normal module.\n\nLine 23: Input (data) and output (label) training data will remain constant so our model is going to approximate the function by approximating the \u2018weight\u2019 (w) , In this case 3 * 1 since 3 inputs corresponds to 1 output, model will keep adjusting the weight to find the optimal value during the training process, hence its\u2019s a Variable object. I hope the commonly used equation for neural networks \u201cy = Wx \u201dmakes more sense to you now\n\nLine 26\u201328: We can simply define output, Error and Mean squared error in three lines. Error basically computes, how \u201coff\u201d our model output predictions are from the real \u201coutput\u201d of training set. We will then proceed to compute the Mean-squared error, Why? Well, There is very beautiful piece of math behind it, You\u2019d already know if you took Probability class in school, but here is a nice short tutorial explaining it.\n\nLine 29: The evaluation of certain functions like weights (w) will help adjust the value of our predefined Variable, So, in this piece of code- we computed the desired adjustment (delta) based on the error we earlier computed. And then proceed to add it to our weights.\n\nLine 32\u201333: The model has to be evaluated by a TensorFlow session, which we instantiate before initializing all variables to their specified values, Remember? We got to run our model using fast C++ backend of Tensorflow. The magic is behind sessions\n\nLine 34\u201340: We can now run our model through training epochs, adjusting the weights each time by evaluating train. Since we\u2019re using a binary output, we can expect to reach a perfect result with a mean squared error of 0. We will reach it very quickly. Note that Session.run will return the result of whatever is evaluated till then. On each epoch, we evaluate mse in order to track progress, and train to actually adjust the weights.\n\nNow go ahead and run this code."
    },
    {
        "url": "https://medium.com/@evalsocket/wikileaks-7f21b87e5316?source=user_profile---------8----------------",
        "title": "Wikileaks \ud83d\ude09 \u2013 Yuvraj \u2013",
        "text": "Man is least himself when he talks with his own person. But if you give him a mask, he will tell you the truth. Two people, and a secret: the beginning of all conspiracies. More people, and, more secrets. But if we could find one moral man, one whistle-blower. Someone willing to expose those secrets, that man can topple the most powerful and most repressive of regimes"
    },
    {
        "url": "https://medium.com/@evalsocket/tensorflow-threat-or-out-of-the-box-f8af9e7c2527?source=user_profile---------9----------------",
        "title": "Tensorflow, Threat or Out of the box ? \u2013 Yuvraj \u2013",
        "text": "Tensorflow is open source software for creating machine learning models, especially deep neural networks (Tensor Flow: Large-Scale Machine Learning on Heterogeneous Distributed Systems )\n\nTensorflow is the technology that already provides the intelligence for Google\u2019s image and speech recognition, foreign language translation and various other applications.\n\nOutside Google some companies adopted Tensorflow in his early days like Airbnb,snapchat, Polygram,uber, Twitter,Dropbox etc.\n\nAnd Deepmind switch from the theano to tensorflow.\n\nGoogle also start tensorflow in medical usecase like skin cancer and dieabtese blindness by just one click on app.\n\nTensorflow is very generalize used by companies,data scientist, individual and researchers.\n\nWhat is the meaning of production for a hacker.?\n\nLet\u2019s start with history, 1Tbps traffic generated by hackers. Attack is very simple like send a command to a botnet network.\n\nMay be you are thinking that it can\u2019t be that easy. Yes,you are right. It\u2019s not that easy. To generate that type of traffic you need a powerful botnet Networks.\n\nSome hackers(NSA is one of them) invest there time to hack mobile phones and IoT devices (raspberry Pi) and create their botnet network.They have access so they can upgrade their code with some AI on your device it\u2019s hard without tensorflow.\n\nBut what if they start serving tensorflow models on your mobile and IoT devices . Now things are more dangerous Babu.\n\nOn tor you can get users credit cards and personal information. May be in future we can get user\u2019s blue print\n\nWaiting for the future ? What happened next in cyber world."
    },
    {
        "url": "https://medium.com/@evalsocket/who-am-i-d9925d61a49c?source=user_profile---------10----------------",
        "title": "Who Am I? \u2013 Yuvraj \u2013",
        "text": "Am I the same little boy, who got up to play\n\nWith mud in the cold winter mornings?\n\nOr am I the one who sat on my brother\u2019s shoulders\n\nWatching him take care of sheep as they grazed\n\nThe almost barren lands in the small village?\n\nOr am I the one who jumped around in the little hut\n\nWhile my mother and sisters cleaned it up?\n\nOr am I the one who sat in the shade of a huge banyan tree\n\nWatching my father carrying loads of bricks to fill up tractors?\n\nOr am I, am I . . .\n\nNo, I am the same little boy who wept for\n\nHis parents late into the night.\n\nGrowing up to find a new home as paradise\n\nOn the outskirts of Tamil Nadu.\n\nA place which holds so much values toward\n\nLove, care, and responsibility.\n\nA world which changes anyone entering it\n\nInto someone who will uphold the greatest of virtues\n\nThroughout their lives.\n\nI am a 16-year-old boy\n\nWho enjoys his friends\n\nWho will never leave his side,\n\nWho learns from teachers and aunties\n\nWho never stop showing him the right path,\n\nWho eats the tasty food cooked by the kitchen staff,\n\nWho takes pride in growing up in a family\n\nWho will always be there for him when he needs them.\n\nI am a 16-year-old boy\n\nWorking very hard to get a 100% in everything he does.\n\nI am Devaraj Kali\n\nFrom the paradise we call Shanti Bhavan,\n\nA haven of peace.\n\nBy Devaraj \u2013 written in 11th Grade(Student from Shanti Bhavan)"
    },
    {
        "url": "https://medium.com/@evalsocket/hidden-in-a-place-where-no-one-knows-88f3bca387d3?source=user_profile---------11----------------",
        "title": "Hidden in a Place Where No One Knows \u2013 Yuvraj \u2013",
        "text": "Hidden in a place where no one knows\n\nDeeper you go, darker it gets.\n\nThere stood something really tall,\n\nSo tall, that the clouds hid the tip of it.\n\nAll this hidden in a place where no one knows\n\nDeeper you go, darker it gets.\n\nSuddenly, a light attracts you,\n\nLead you to a door\n\nWith a slow creak you open it.\n\nAll this hidden in a place where no one knows.\n\nNow deeper you go, the more curious you get.\n\nSurprise to see a man covered in black\n\nHe turns towards you,\n\nHis look is weird.\n\nHe opens his mouth.\n\nAnd all he said was Book, Book, Book\n\nAll this hidden in a place where no one knows\n\nThe closer you get, you feel shivers going down your spine.\n\nTurning around to look,\n\nAll you see are books.\n\nHuge ones, small ones, thick ones and thin ones.\n\nAll this hidden in a place where no one knows.\n\nThe man had a thick book,\n\nThat held the secrets of magic powers.\n\nThe world was searching, for this thick book.\n\nAll this hidden in a place where no one knows.\n\nWith a touch, the book and the man all disappeared.\n\nWalking out confused, then, you notice that you were\n\nInside a tall skyscraper with the disappearing man who wrote\n\nThe Book of Magic Power.\n\nAll this hidden in a place where no one knows.\n\nBy Rethika \u2013 written in 6th Grade (Student from Shanti Bhavan)"
    },
    {
        "url": "https://medium.com/@evalsocket/under-the-hood-docker-98f99189f38a?source=user_profile---------12----------------",
        "title": "Under the hood Docker \u2013 Yuvraj \u2013",
        "text": "The virtualization method can be categorised based on how it mimics hardware to a guest operating system and emulates guest operating environment. There are 3 types of virtualization.\n\nEmulation : Emulation, also known as full virtualization runs the virtual machine OS kernel entirely in software. The hypervisor used in this type is known as Type 2 hypervisor. It is installed on the top of host operating system which is responsible for translating guest OS kernel code to software instructions. The downside of this type of virtualization is additional system resource overhead that leads to decrease in performance compared to other types of virtualizations.\n\nExamples in this category include VMware Player, VirtualBox, QEMU, Bochs, Parallels, etc.\n\nParavirtualization : Paravirtualization, also known as Type 1 hypervisor, runs directly on the hardware, or \u201cbare-metal\u201d, and provides virtualization services directly to the virtual machines running on it. It helps the operating system, the virtualized hardware, and the real hardware to collaborate to achieve optimal performance. These hypervisors typically have a rather small footprint and do not, themselves, require extensive resources\n\nExamples in this category include Xen, KVM, etc.\n\nContainer-based virtualization :Container-based virtualization, also know as operating system-level virtualization, enables multiple isolated executions within a single operating system kernel. It has the best possible performance and density and features dynamic resource management. The isolated virtual execution environment provided by this type of virtualization is called container and can be viewed as a traced group of processes.\n\n1 Unlike a virtual machine, a container does not need to boot the operating system kernel, so containers can be created in less than a second.\n\n2 Since container-based virtualization adds little or no overhead to the host machine, container-based virtualization has near-native performance\n\n3 All containers on a host machine share the scheduler of the host machine saving need of extra resources.\n\n4 Container states (Docker or LXC images) are small in size compared to virtual machine images, so container images are easy to distribute.\n\nAnd many more\n\nCompanies use Docker to run and manage apps side-by-side in isolated containers to get better compute density.\n\ndocker is lightweight and uses LXC/libcontainer ( They start with LXC and latter they develope their own container runtime libcontainer ) and does not have machine/hardware emulation such as hypervisor, KVM and Xen which are heavy. Lets go deeper\n\nYou got some hash dir they are layers. They follow tree structure to form that environment.\n\n#now you are inside docker container\n\n#do some installation.you can create your environment and exit from container and check container id by docker ps\n\n# check docker images you found your image name\n\n#check hashes count before installation in test folder and after installation. You found different number. In installation you just create some node in the tree in form of hash. That\u2019s docker images.We can reuse these layers they store in cache.\n\nNow talk about containers. Container is a process controlled by cgroups and namespace.\n\nWhat is cgroups and namespace ?\n\nThey help to controlle a process in terms of storage,network,cpus etc. They are responsible to manage process in your os. When we create a container meaning we create a process who has its own world and controlled by cgroups and namespace.\n\nSo what the fuck docker do ?\n\ndocker is just a run time who help us to manage things easily.\n\nWe just create our docker image by container commit. It\u2019s a manual process on production we can\u2019t do manual things.\n\nDocker file is the solution. It also help us to create same environment on any system but how it work?\n\nDocker provide some command (API) like FROM,ADD,RUN,COPY, etc\n\nThey tell docker client to exec some instructions. For example\n\nBy using that file you can setup your environment in any system. On production it help us a lot.\n\nWhen you install docker you got a docker host and client.\n\nKubernetes is an open source container orchestration framework built by teams at Google and bases its container-worldview on how Google develops within their walls in Mountain View.\n\nAbove part about docker only help us to work on single node. But on production we use cluster. Cluster size depends upon situation traffic.\n\nAnd by the way comparing Kubernete vs Docker really comes down to comparing Kubernetes to Docker\u2019s Swarm product\n\nResources : Play Ground Docker (Beginner can start from here)"
    },
    {
        "url": "https://medium.com/@evalsocket/beinganonymous-aba733bbac93?source=user_profile---------13----------------",
        "title": "#BeingAnonymous \u2013 Yuvraj \u2013",
        "text": "Anonymity, I want to achieve\n\nWhy ?? Don\u2019t Know\n\nWhen ?? Don\u2019t Know\n\nHow ?? Don\u2019t Know\n\nAnother Wh ??\n\nThe answer still is \u201cDon\u2019t know\u201d\n\nIf I Don\u2019t Know Then\n\nHow would I achieve ??\n\nDo you Know I ??\n\nNo \u201cDon\u2019t Know\u201d\n\nThat\u2019s How I achieved it and will always be doing so."
    },
    {
        "url": "https://medium.com/@evalsocket/contribution-38a19d5a69c7?source=user_profile---------14----------------",
        "title": "Contribution ? \u2013 Yuvraj \u2013",
        "text": "Let\u2019s start with Tor. Good or bad I don\u2019t know because the TOR network was created by nefarious forces. I doubt it, but you never really know.\n\nwe always hear tor name for the dark side like drugs,guns,user data,hit man etc we all have a long list\n\nI am not a bad guy. If you are more focusing on bad things than the post is not for you. It\u2019s about possibilities of good and bad.\n\nFor example silk road is a site who is dealing with all illegal things but they run their business very smoothly.\n\nWhat is their business model ?\n\nSome facts about them .First their all products are illegal. Second they hide you using tor. Third they hide your txn through bitcoin. Forth targeting audience are those who is interesting in these illegal things. Means all business are running same as before they just revamp their technology to make you anonymous buyer.\n\nI hope that all of you see all these illegal things surrounding us. They work on hard cash they believe in bitcoin. They believe in F2F they believe in tor and pgp. Crime rate are same as before.\n\nBitcoin, Tor and wikileaks are not bad projects. Bitcoin founder anonymous? wikileaks founder under bad condition ? Tor is under donation ?\n\nTor bitcoin & PGP is using for illegal things because bad guys are more smarter then good guys. Bad guys use them before good guys. Tor and bitcoin is all about hiding yourself. So we can use them to hide us not only for illegal things. You are not a objects for spy by a govt or an individual(hacker)\n\nHow to use these projects (contribute)?\n\nA guide for securing yourself from bad guys(hackers) or may be govt.\n\n1 . Download Tor Browser and for Android their are tor projects orbot and orfox. They are easy to use. Speed would be very low because they have limited relay and bandwidth . They run tor project on donation. Use VPN for second layer protection. And if you have resources than run a relay\n\n2. Use PGP encryption(wait for next post)\n\n3. Use Bitcoin. But how? I know govt is against to bitcoin but be a volunteer you can start exchanging money,start accepting bitcoin on your business . Use it because it\u2019s safer than banks, cheaper than banks. Bitcoin or tor both are distributed system.\n\nThese points are not about download Tor and find wikki for market place. I hope you will contribute for something good so that we can set some good example of these tools."
    },
    {
        "url": "https://medium.com/@evalsocket/anonymous-wallet-30a727240b83?source=user_profile---------15----------------",
        "title": "Anonymous Wallet ?? \u2013 Yuvraj \u2013",
        "text": "You don\u2019t know how to download a Linux LiveCD, then proceed with extreme caution; downloading an ISO file and burning it to a DVD is pretty damned easy. Easier than anonymity. Those who refuse to learn are at risk.\n\nIt\u2019s arguable which software you should use, but I recommend connecting to the TOR network using TAILS, a live DVD or live USB that aims at preserving your privacy and anonymity. TAILS helps you to use the Internet anonymously, leave no trace on the computer you\u2019re using, and to use state-of-the-art cryptographic tools to encrypt your files, email and instant messaging. Don\u2019t trust anyone\n\nWisdom of words: For an extra layer of protection, download the ISO from public internet or while you\u2019re sipping a mocha at Starbuck\u2019s. Then burn it to a DVD and take it home. Place it in your crap computer (the one without a hard-drive) and turn it on. Enter the BIOS menu and boot from CD if your computer doesn\u2019t do it automatically.\n\nDO NOT CONNECT TO THE NETWORK FROM YOUR HOME\n\nI repeat, for an extra layer of security, DO NOT CONNECT TO YOUR HOME WIFI USING TAILS IF YOU WANT TO DO SHADY THINGS. That\u2019s just common sense. TAILS itself isn\u2019t illegal. But if you\u2019re the type to do shady things, you don\u2019t want to practice on your home Wifi, which you probably pay for with a bank account or credit card.\n\nAfter you\u2019ve spent a day or two using TAILS and familiarizing yourself with the LinuxOS, and once you feel comfortable enough to continue, then head back to your local Starbucks, boot up the LiveCD, and connect. Browse the TOR network and triple-check that you are protected. You can do this by checking your IP address for DNS LEAKS. Only if you feel comfortably hidden from prying eyes will you want to continue.\n\nThere are several different ways to to this, but the easiest way is to use the code at bitadress.org.\n\nFunding your wallet will be the most difficult part of this process. Obviously you don't want to go to a site like Coinbase or Mt. Gox and link up you bank account, then start sending coins to your anonymous address. That would be stupid. Very stupid.\n\nProbably the best way to get coins is to know someone who is willing to send you a few, but even then you lead a trail back to your friend.\n\nMy suggestion is to make cash deposits through ZipZap or Bitinstant, and give them false information (for example, use the new email you created, over the TOR network, from a site like Hotmail or Yahoo, which doesn't require a phone number to sign up \u2013 I'm looking at you Gmail. Make sure your new account forwards your email to yet another account, perhaps Tormail or a temp address. You probably won't need to use the email more than once anyway, for confirmation, if you need it. And you might want to create a new address with every deposit, just to be safe). There are other options of course. Some companies will sell you Bitcoins anonymously through Bank of America cash deposits. But remember that the moment you walk into a Big Bank and give them money, you are caught on camera. Maybe offer a homeless man some money to make the deposit for you. And hope he doesn't just pocket your money. Regardless, you want to stay away from Big Banks if you can. It really isn't that hard.\n\nIf you absolutely must make deposits from your bank account, you could send your coins to an anonymous online wallet first and then to cold storage, but make sure to use several mixing services over a period of several days. And then have trouble sleeping at night.\n\nAnother great idea is to use the local Bitcoin ; meet with a seller locally; pay cash and GTFO."
    }
]