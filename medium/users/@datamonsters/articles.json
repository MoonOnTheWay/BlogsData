[
    {
        "url": "https://medium.com/@datamonsters/13-deep-learning-frameworks-for-natural-language-processing-in-python-2b84a6b6cd98?source=user_profile---------1----------------",
        "title": "13 Deep Learning Frameworks for Natural Language Processing in Python",
        "text": "A comparative table was specially created. Every cell with a plus sign contains a link to a framework usage example in NLP task and network type perspectives.\n\nIn this paper, we discuss the most popular neural network frameworks and libraries that can be utilized for natural language processing (NLP) in the Python programming language. We also look at existing examples of these tools.\n\nChainer, developed by the Japanese company Preferred Networks founded in 2014, is a powerful, flexible, and intuitive Python-based framework for neural networks that adopts a \u201cdefine-by-run\u201d scheme [1]. It stores the history of computation instead of programming logic. Chainer supports CUDA computation and multi-GPU. The framework released under the MIT License and is already applied for sentiment analysis, machine translation, speech recognition, question answering, and so on using different types of neural networks like convolutional networks, recurrent networks, and sequence to sequence models [2].\n\nDeeplearning4j is a deep learning Java programming library, but it also has a Python API, Keras that will be described below. Distributed CPUs and GPUs, parallel training via iterative reduce, and micro-service architecture adaptation are its main features [3]. Vector space modeling enables the tool to solve text-mining problems. Parts of speech (PoS) tagging, dependency parsing, and word2vec for creating word embedding are discussed in the documentation.\n\nDeepnl is another neural network Python library especially created for natural language processing by Giuseppe Attardi. It provides tools for part-of-speech tagging, named entity recognition, semantic role labeling (using convolutional neural networks [4]), and word embedding creation [5].\n\nDynet is a tool developed by Carnegie Mellon University and many others. It supports C++ and Python languages, runs on either CPU or GPU [6]. Dynet is based on the dynamic declaration of network structure [7]. This tool was used for creating outstanding systems for NLP problems including syntactic parsing, machine translation, morphological inflection, and many others.\n\nKeras is a high-level neural-network based Python API that runs on CPU or GPU. It supports convolutional and recurrent networks and may run on top of TensorFlow, CNTK, or Theano. The main focus is to enable users fast experimentation [8]. There are many examples of Keras usage in the comparative table: classification, text generation and summarization, tagging, parsing, machine translation, speech recognition, and others.\n\nErick Rocha Fonseca\u2019s nlpnet is also a Python library for NLP tasks based on neural networks. Convolutional networks enable users to perform part-of-speech tagging, semantic role labeling, and dependency parsing [9]. Most of the architecture is language independent [10].\n\nOpenNMT is a Python machine translation tool that works under the MIT license and relies on the PyTorch library. The system demonstrates efficiency and state-of-the-art translation accuracy and is used by many translation providers [11]. It also incorporates text summarization, speech recognition, and image-to-text conversion blocks [12].\n\nPyTorch is a fast and flexible neural network framework with an imperative paradigm. It builds neural networks on a tape-based autograd system and provides tensor computation with strong GPU acceleration [13]. Recurrent neural networks are mostly used in PyTorch for machine translation, classification, text generation, tagging, and other NLP tasks.\n\nDevelopers called spaCy the fastest system in the world. They also affirm that their tool is the best way to prepare text for deep learning. Spacy works excellent with well-known Python libraries like gensim, Keras, TensorFlow, and scikit-learn. Matthew Honnibal, the author of the library, says that spaCy\u2019s mission is to make cutting-edge NLP practical and commonly available [14]. Text classification, named entity recognition, part of speech tagging, dependency parsing, and other examples are presented in the comparative table.\n\nStanford\u2019s CoreNLP is a flexible, fast, and modern grammatical analysis tool that provides APIs for most common programming languages including Python. It also has an ability to run as a simple web service. As mentioned on the official website, the framework has a part-of-speech (POS) tagger, named entity recognizer (NER), parser, coreference resolution system, sentiment analysis, bootstrapped pattern learning, and open information extraction tools [15].\n\nThe Google Brain Team developed TensorFlow and released it in 2015 for research purposes. Now many companies like Airbus, Intel, IBM, Twitter and others use TensorFlow at production scale. The system architecture is flexible, so it is possible to perform computations on CPUs or GPUs. The main concept is flow graphs usage. Nodes of the graph reflect mathematical operations, while the edges represent multidimensional data arrays (tensors) communicated between them [16]. One of the most known of TensorFlow\u2019s NLP application is Google Translate. Other applications are text classification and summarization, speech recognition, tagging, and so on.\n\nAs Tensorflow is a low-level API, many high-level APIs were created to run on top of it to make the user experience faster and more understandable. TFLearn is one of these tools that runs on CPU and GPU. It has a special graph visualization tool with details about weights, gradients, activations, and so on [17]. The library is already used for sentiment analysis, text generation, and named entity recognition. It lets users work with convolutional neural networks and recurrent neural networks (LSTM).\n\nTheano is a numerical computation Python library that enables users to create their own machine learning models [18]. Many frameworks like Keras are built on top of Theano. There are tools for machine translation, speech recognition, word embedding, and text classification. Look at Theano\u2019s applications in the table.\n\nIn this paper, we described neural network supporting Python tools for natural language processing. These tools are Chainer, Deeplearning4j, Deepnl, Dynet, Keras, Nlpnet, OpenNMT, PyTorch, SpaCy, Stanford\u2019s CoreNLP, TensorFlow, TFLearn, and Theano. A table lets readers easily compare the frameworks discussed above."
    },
    {
        "url": "https://medium.com/@datamonsters/deep-learning-with-python-by-fran%C3%A7ois-chollet-bda00e20ae95?source=user_profile---------2----------------",
        "title": "Deep Learning with Python by Fran\u00e7ois Chollet \u2013 Data Monsters \u2013",
        "text": "I occasionally give talks on deep learning, and I know how important it is to present the material in the right way \u2014 practical and easy to grasp. I was recently impressed by a new book on deep learning, and I decided to share it here.\n\nThe book is called \u201cDeep Learning with Python\u201d. The title would not catch my attention, if not the fact that the book is written by Fran\u00e7ois Chollet. Fran\u00e7ois Chollet is the author of Keras. Keras is an open-source deep learning framework for Python. Because of it\u2019s beautiful simplicity, it is extremely popular: over two hundred thousand users, which is more than one third of all the deep learning engineers in the world.\n\n\u201cDeep Learning with Python\u201d is a gem. I envy those who are just about to read it. It takes you through the journey of deep learning in a very captivating way. Here are the two things that I think make the book stand out compared to other deep learning books:\n\n1. Easy. The material is targeted to anyone with basic Python coding experience. Mathematical concepts are explained using code snippets. No need to dive into math notation or equations, which can be inscrutable for people without a relevant degree.\n\n2. Practical and advanced. Typically, advanced theoretical books are disconnected from concrete problems, and practical books are superficial.\n\nIn a solid way, the book covers the topics of:\n\nIf you are interested in starting strong with deep learning, you will enjoy this book: https://www.manning.com/books/deep-learning-with-python"
    },
    {
        "url": "https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2?source=user_profile---------3----------------",
        "title": "7 types of Artificial Neural Networks for Natural Language Processing",
        "text": "What is an artificial neural network? How does it work? What types of artificial neural networks exist? How are different types of artificial neural networks used in natural language processing? We will discuss all these questions in the following article.\n\nAn artificial neural network (ANN) is a computational nonlinear model based on the neural structure of the brain that is able to learn to perform tasks like classification, prediction, decision-making, visualization, and others just by considering examples.\n\nAn artificial neural network consists of artificial neurons or processing elements and is organized in three interconnected layers: input, hidden that may include more than one layer, and output.\n\nThe input layer contains input neurons that send information to the hidden layer. The hidden layer sends data to the output layer. Every neuron has weighted inputs (synapses), an activation function (defines the output given an input), and one output. Synapses are the adjustable parameters that convert a neural network to a parameterized system.\n\nThe weighted sum of the inputs produces the activation signal that is passed to the activation function to obtain one output from the neuron. The commonly used activation functions are linear, step, sigmoid, tanh, and rectified linear unit (ReLu) functions.\n\nTraining is the weights optimizing process in which the error of predictions is minimized and the network reaches a specified level of accuracy. The method mostly used to determine the error contribution of each neuron is called backpropagation that calculates the gradient of the loss function.\n\nIt is possible to make the system more flexible and more powerful by using additional hidden layers. Artificial neural networks with multiple hidden layers between the input and output layers are called deep neural networks (DNNs), and they can model complex nonlinear relationships.\n\nA multilayer perceptron (MLP) has three or more layers. It utilizes a nonlinear activation function (mainly hyperbolic tangent or logistic function) that lets it classify data that is not linearly separable. Every node in a layer connects to each node in the following layer making the network fully connected. For example, multilayer perceptron natural language processing (NLP) applications are speech recognition and machine translation.\n\nA convolutional neural network (CNN) contains one or more convolutional layers, pooling or fully connected, and uses a variation of multilayer perceptrons discussed above. Convolutional layers use a convolution operation to the input passing the result to the next layer. This operation allows the network to be deeper with much fewer parameters.\n\nConvolutional neural networks show outstanding results in image and speech applications. Yoon Kim in Convolutional Neural Networks for Sentence Classification describes the process and the results of text classification tasks using CNNs [1]. He presents a model built on top of word2vec, conducts a series of experiments with it, and tests it against several benchmarks, demonstrating that the model performs excellent.\n\nIn Text Understanding from Scratch, Xiang Zhang and Yann LeCun, demonstrate that CNNs can achieve outstanding performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language [2]. Semantic parsing [3], paraphrase detection [4], speech recognition [5] are also the applications of CNNs.\n\nA recursive neural network (RNN) is a type of deep neural network formed by applying the same set of weights recursively over a structure to make a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order [6]. In the simplest architecture, a nonlinearity such as tanh, and a weight matrix that is shared across the whole network are used to combine nodes into parents.\n\nA recurrent neural network (RNN), unlike a feedforward neural network, is a variant of a recursive artificial neural network in which connections between neurons make a directed cycle. It means that output depends not only on the present inputs but also on the previous step\u2019s neuron state. This memory lets users solve NLP problems like connected handwriting recognition or speech recognition. In a paper, Natural Language Generation, Paraphrasing and Summarization of User Reviews with Recurrent Neural Networks, authors demonstrate a recurrent neural network (RNN) model that can generate novel sentences and document summaries [7].\n\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao created a recurrent convolutional neural network for text classification without human-designed features and described it in Recurrent Convolutional Neural Networks for Text Classification. Their model was compared to existing text classification methods like Bag of Words, Bigrams + LR, SVM, LDA, Tree Kernels, Recursive neural network, and CNN. It was shown that their model outperforms traditional methods for all used data sets [8].\n\nLong Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs [9]. LSTM does not use activation function within its recurrent components, the stored values are not modified, and the gradient does not tend to vanish during training. Usually, LSTM units are implemented in \u201cblocks\u201d with several units. These blocks have three or four \u201cgates\u201d (for example, input gate, forget gate, output gate) that control information flow drawing on the logistic function.\n\nIn Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling, Hasim Sak, Andrew Senior, and Fran\u00e7oise Beaufays showed that the deep LSTM RNN architectures achieve state-of-the-art performance for large scale acoustic modeling.\n\nIn the work, Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network by Peilu Wang, Yao Qian, Frank K. Soong, Lei He, and Hai Zhao, a model for part-of-speech (POS) tagging was presented [10]. The model achieved a performance of 97.40% tagging accuracy. Apple, Amazon, Google, Microsoft and other companies incorporated LSTM as a fundamental element into their products.\n\nUsually, a sequence-to-sequence model consists of two recurrent neural networks: an encoder that processes the input and a decoder that produces the output. Encoder and decoder can use the same or different sets of parameters.\n\nSequence-to-Sequence models are mainly used in question answering systems, chatbots, and machine translation. Such multi-layer cells have been successfully used in sequence-to-sequence models for translation in Sequence to Sequence Learning with Neural Networks study [11].\n\nIn Paraphrase Detection Using Recursive Autoencoder, a novel recursive autoencoder architecture is presented. The representations are vectors in an n-dimensional semantic space where phrases with similar meanings are close to each other [12].\n\nBesides deep neural networks, shallow models are also popular and useful tools. For example, word2vec is a group of shallow two-layer models that are used for producing word embeddings. Presented in Efficient Estimation of Word Representations in Vector Space, word2vec takes a large corpus of text as its input and produces a vector space [13]. Every word in the corpus obtains the corresponding vector in this space. The distinctive feature is that words from common contexts in the corpus are located close to one another in the vector space.\n\nIn this paper, we described different variants of artificial neural networks, such as deep multilayer perceptron (MLP), convolutional neural network (CNN), recursive neural network (RNN), recurrent neural network (RNN), long short-term memory (LSTM), sequence-to-sequence model, and shallow neural networks including word2vec for word embeddings. We showed how these networks function and how different types of them are used in natural language processing tasks. We demonstrated that convolutional neural networks are primarily utilized for text classification tasks while recurrent neural networks are commonly used for natural language generation or machine translation. In the next part of this series, we will study existing tools and libraries for the discussed neural network types."
    },
    {
        "url": "https://medium.com/@datamonsters/artificial-neural-networks-in-natural-language-processing-bcf62aa9151a?source=user_profile---------4----------------",
        "title": "10 Applications of Artificial Neural Networks in Natural Language Processing",
        "text": "Since artificial neural networks allow modeling of nonlinear processes, they have turned into a very popular and useful tool for solving many problems such as classification, clustering, regression, pattern recognition, dimension reduction, structured prediction, machine translation, anomaly detection, decision making, visualization, computer vision, and others. This wide range of abilities makes it possible to use artificial neural networks in many areas. In this article, we discuss applications of artificial neural networks in Natural Language Processing tasks (NLP).\n\nNLP includes a wide set of syntax, semantics, discourse, and speech tasks. We will describe prime tasks in which neural networks demonstrated state-of-the-art performance.\n\nText classification is an essential part in many applications, such as web searching, information filtering, language identification, readability assessment, and sentiment analysis. Neural networks are actively used for these tasks.\n\nIn Convolutional Neural Networks for Sentence Classification by Yoon Kim, a series of experiments with Convolutional Neural Networks (CNN) built on top of word2vec was presented. The suggested model was tested against several benchmarks. In Movie Reviews (MR) and Customer Reviews (CR), the task was to detect positive/negative sentiment. In Stanford Sentiment Treebank (SST-1), there were already more classes to predict: very positive, positive, neutral, negative, very negative. In Subjectivity data set (Subj), sentences were classified into two types, subjective or objective. In TREC the goal was to classify a question into six question types (whether the question is about person, location, numeric information, etc.) The results of numerous tests described in the paper show that after little tuning of hyperparameters the model performs excellent suggesting that the pre-trained vectors are universal feature extractors and can be utilized for various classification tasks [1].\n\nThe article Text Understanding from Scratch by Xiang Zhang and Yann LeCun shows that it\u2019s possible to apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts with help of temporal Convolutional Networks (ConvNets) (CNN). Here, the authors assert that ConvNets can achieve excellent performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language [2]. To prove their assertion several experiments were conducted. The model was tested on the DBpedia ontology classification data set with 14 classes (company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film, written work). The results indicate both good training (99.96%) and testing (98.40 %) accuracy, with some improvement from thesaurus augmentation. In addition, the sentiment analysis test was performed on the Amazon Review data set. In this study, the researchers constructed a sentiment polarity data set with two negative and two positive labels. The result is 97.57% training accuracy and 95.07% testing accuracy. The model was also tested on Yahoo! Answers Comprehensive Questions and Answers data set with 10 classes (Society & Culture, Science & Mathematics, Health, Education & Reference, Computers & Internet, Sports, Business & Finance, Entertainment & Music, Family & Relationships, Politics & Government) and on AG\u2019s corpus where the task was a news categorization into four categories (World, Sports, Business, Sci/Tech.). Obtained results confirm that to achieve good text understanding ConvNets require a large corpus in order to learn from scratch.\n\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao introduced recurrent convolutional neural networks for text classification without human-designed features in their document Recurrent Convolutional Neural Networks for Text Classification [3]. The team tested their model using four data sets: 20Newsgroup (with four categories such as computers, politics, recreation, and religion), Fudan Set (a Chinese document classification set that consists of 20 classes, including art, education, and energy), ACL Anthology Network (with five languages: English, Japanese, German, Chinese, and French), and Sentiment Treebank (with Very Negative, Negative, Neutral, Positive, and Very Positive labels). After testing, the model was compared to existing text classification methods like Bag of Words, Bigrams + LR, SVM, LDA, Tree Kernels, RecursiveNN, and CNN. It turned out that neural network approaches outperform traditional methods for all four data sets, and the proposed model outperforms CNN and RecursiveNN.\n\nThe main task of named entity recognition (NER) is to classify named entities, such as Guido van Rossum, Microsoft, London, etc., into predefined categories like persons, organizations, locations, time, dates, and so on. Many NER systems were already created, and the best of them use neural networks.\n\nIn the paper, Neural Architectures for Named Entity Recognition, two models for NER were proposed. The models require character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora [4]. Numerous tests were carried on using different data sets like CoNLL-2002 and CoNLL-2003 in English, Dutch, German, and Spanish languages. The team concluded that without a requirement of any language-specific knowledge or resources, such as gazetteers, their models show state-of-the-art performance in NER.\n\nPart-of-speech (POS) tagging has many applications including parsing, text-to-speech conversion, information extraction, and so on. In the work, Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network a recurrent neural network with word embedding for part-of-speech (POS) tagging task is presented [5]. The model was tested on the Wall Street Journal data from Penn Treebank III data set and achieved a performance of 97.40% tagging accuracy.\n\nQuestion Answering systems automatically answer different types of questions asked in natural languages including definition questions, biographical questions, multilingual questions, and so on. Neural networks usage makes it possible to develop high performing question answering systems.\n\nIn Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao described the developed semantic parsing framework for question answering using a knowledge base. Authors say their method uses the knowledge base at an early stage to prune the search space and thus simplifies the semantic matching problem [6]. It also applies an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences. The model was tested on WebQuestions data set, and it outperforms previous methods substantially.\n\nParaphrase detection determines whether two sentences have the same meaning. This task is especially important for question answering systems since there are many ways to ask the same question.\n\nDetecting Semantically Equivalent Questions in Online User Forums suggests a method for identifying semantically equivalent questions based on a convolutional neural network. The experiments are performed using the Ask Ubuntu Community Questions and Answers (Q&A) site and Meta Stack Exchange data. It was shown that the proposed CNN model achieves high accuracy especially when the words embedded are pre-trained on in-domain data. The authors compared their model\u2019s performance with Support Vector Machines and a duplicate detection approach. They demonstrated that their CNN model outperforms the baselines by a large margin [7].\n\nIn the study, Paraphrase Detection Using Recursive Autoencoder, a novel recursive autoencoder architecture is presented. It learns phrasal representations using recursive neural networks. These representations are vectors in an n-dimensional semantic space where phrases with similar meanings are close to each other [8]. For evaluating the system, the Microsoft Research Paraphrase Corpus and English Gigaword Corpus were used. The model was compared to three baselines, and it outperforms them all.\n\nNatural language generation has many applications such as automated writing of reports, generating texts based on analysis of retail sales data, summarizing electronic medical records, producing textual weather forecasts from weather data, and even producing jokes.\n\nIn a recent paper, Natural Language Generation, Paraphrasing and Summarization of User Reviews with Recurrent Neural Networks, researchers describe a recurrent neural network (RNN) model capable of generating novel sentences and document summaries. The paper described and evaluated a database of 820,000 consumer reviews in the Russian language. The design of the network permits users control of the meaning of generated sentences. By choosing sentence-level features vector, it is possible to instruct the network; for example, \u201cSay something good about a screen and sound quality in about ten words\u201d [9]. The ability of language generation allows production of abstractive summaries of multiple user reviews that often have reasonable quality. Usually, the summary report makes it possible for users to quickly obtain the information contained in a large cluster of documents.\n\nMachine translation software is used around the world despite its limitations. In some domains, the quality of translation is not good. To improve the results researchers try different techniques and models, including the neural network approach. The purpose of Neural-based Machine Translation for Medical Text Domain study is to inspect the effects of different training methods on a Polish-English machine translation system used for medical data. To train neural and statistical network-based translation systems The European Medicines Agency parallel text corpus was used. It was demonstrated that a neural network requires fewer resources for training and maintenance. In addition, a neural network often substituted words with other words occurring in a similar context [10].\n\nSpeech recognition has many applications, such as home automation, mobile telephony, virtual assistance, hands-free computing, video games, and so on. Neutral networks are widely used in this area.\n\nIn Convolutional Neural Networks for Speech Recognition, scientists explain how to apply CNNs to speech recognition in a novel way, such that the CNN\u2019s structure directly accommodates some types of speech variability like varying speaking rate [11]. TIMIT phone recognition and a large-vocabulary voice search tasks were used.\n\nCharacter Recognition systems also have numerous applications like receipt character recognition, invoice character recognition, check character recognition, legal billing document character recognition, and so on. The article Character Recognition Using Neural Network presents a method for the recognition of handwritten characters with 85% accuracy [12].\n\nMost text editors let users check if their text contains spelling mistakes. Neural networks are now incorporated into many spell-checking tools.\n\nIn Personalized Spell Checking using Neural Networks a new system for detecting misspelled words was proposed. This system is trained on observations of the specific corrections that a typist makes [13]. It outwits many of the shortcomings that traditional spell-checking methods have.\n\nIn this article, we described Natural Language Processing problems that can be solved using neural networks. As we showed, neural networks have many applications such as text classification, information extraction, semantic parsing, question answering, paraphrase detection, language generation, multi-document summarization, machine translation, and speech and character recognition. In many cases, neural networks methods outperform other methods."
    },
    {
        "url": "https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-2-7f3a75c262a3?source=user_profile---------5----------------",
        "title": "Sentiment Analysis Tools Overview, Part 2 \u2013 Data Monsters \u2013",
        "text": "Information contained in this article has been mostly obtained from sentiment analysis tools\u2019 official websites.\n\nThe most common application of sentiment analysis is in consumer products and services reviews. The main task of sentiment analysis is to determine whether the text expresses a positive or a negative sentiment and to assign it a polarity value. The goal of this article is to review the most known sentiment analysis tools. We will cover lexicon-based analysis methods, rule-based analysis methods, and machine learning techniques.\n\nThis Natural Language Toolkit (NLTK), a Sentiment Analysis tool, is based on machine learning approaches. The module nltk.sentiment.util contains a special function: demo_liu_hu_lexicon(sentence, plot=False). The function counts the number of positive, negative and neutral words in the input and classifies them depending on which polarity is most frequently represented [1]. Words that do not appear in the lexicon are considered as neutral. This is an example of sentiment classification using Liu and Hu opinion lexicon. The Liu and Hu opinion lexicon is a list of positive and negative words.\n\nMany sentiment analysis tools rely on lists of words and phrases with positive and negative connotations. Many lists are already available. You can read about the most known lists in the previous article.\n\nAnother useful function is demo_vader_instance(text). It returns polarity scores for a text using VADER approach.\n\nPattern is a web mining module for the Python programming language [2]. It has many tools for data mining including sentiment analysis tools. Input text that can be a string, text, sentence, chunk, word or a synset (a set of one or more synonyms), is divided into two types: facts and opinions. Opinions carry people\u2019s sentiments. The module has several useful functions:\n\nThe sentiment() function returns a (polarity, subjectivity) tuple, an ordered set of values, for the given sentence, based on the adjectives it contains, where polarity is a value between -1.0 and +1.0 and subjectivity between 0.0 and 1.0.\n\nThe positive() function returns True if the given sentence\u2019s polarity is above the threshold.\n\nTextBlob is another text processing Python library. The sentiment property returns a named tuple of the form Sentiment (polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective [3].\n\nMany sentiment analysis tools lose important information by using a lexicon-based approach. A new deep learning model used in Stanford CoreNLP makes it possible to compute the sentiment based on how individual words change the meaning of longer phrases. A new type of recursive neural network that builds on grammatical structures was created, and a sentiment treebank was developed. The sentiment treebank includes fine-grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences. To address them use the Recursive Neural Tensor Network [4]. The code is written in Java. It is possible to use Stanford CoreNLP from the command-line, via its Java programmatic API, via third party APIs for most major modern programming languages, or via a web service. It works on Linux, macOS, and Windows.\n\nRapidMiner allows users to extract information from publicly available data sources that lets them understand and optimize what is being said about their company or products. It is possible to crowd-source your customers\u2019 feedback, conduct social market research, determine the polarity of product reviews, and hold discussions around the web [5]. The RapidMiner Studio offers free edition, which is limited to 1 logical processor and 10,000 data rows. Commercial pricing starts at $2,500 and is available from the developer.\n\nGeneral Architecture for Text Engineering (GATE) is a Java open-source, natural language processing tool developed at the University of Sheffield in 1995. It contains several sentiment analysis systems. One of them uses supervised machine learning methods, trained on human-annotated data, co-occurrence statistics, and lexicons of positive and negative words [6]. Others use lightly-supervised and unsupervised approaches. Developers also consider richer user-sensitive models of opinions to reflect the fact that the same opinions could be positive for one user group and negative for another. Author authority models and temporal elements are used that make it possible to detect shifts in attitudes over time.\n\nSyuzhet extracts sentiment and sentiment-derived plot arcs from text using four sentiment dictionaries [7]. Its get_sentiment function assesses the sentiment of each word or sentence. The function takes two arguments: a character vector of sentences or words and a method. The method determines which sentiment extraction methods to use. After the sentiment values are determined, it is possible to obtain a measure of the overall emotional valence in the text by summing the determined values [8].\n\nThe RSentiment package lets users analyze the sentiment of a sentence and assign a score to it. A special function calculate_sentiment predicts the sentiment of sentences. Sentences can be classified into six categories: positive, negative, very positive, very negative, sarcasm and neutral [9].\n\nSentimentAnalysis uses various existing dictionaries of positive and negative words and can create customized dictionaries using the generateDictionary() function. The latter uses LASSO regularization as a statistical approach to select relevant terms based on an exogenous response variable [10]. The function analyzeSentiment() returns sentiment scores from contents stored in different formats. ConvertToBinary() and convertToDirection() functions convert the continuous scores to either binary sentiment classes (negative or positive) or tertiary directions (negative, neutral or positive). CompareToResponse() function performs a statistical evaluation, plotSentimentResponse() \u2014 enables a visual comparison.\n\nGoogle Cloud Prediction API provides a representational state transfer (RESTful) API to build machine learning models. It helps to analyze a text string and classify it with one of the labels that the user provides. Labels could be positive and negative, or happy, frustrated, or sad [11]. After collecting training samples, users need to pre-classify each sample with a label then upload the data to Google Cloud Storage. Use the prediction.trainedmodels.insert() method to train the model and make predictions with the Google Prediction API in your application. If needed, you can improve your model.\n\nSentiment Classifier uses Word Sense Disambiguation (WSD) drawing on WordNet and word occurrence statistics from the movie review corpus NLTK. WSD determines which meaning of a word is used in a sentence if the word has more than one meaning. As features, bigrams on Naive Bayes and maximum entropy classifier are used. The sentiment analysis tool classifies input into positive or negative labels [12].\n\nWatson enables data scientists to analyze the sentiment of a specific target phrases or the sentiment of the whole document. You can also obtain sentiment information for detected entities and keywords [13]. A deeper analysis of the text permits the determination of a positive tone while a customer review may have an overall negative sentiment. The Sentiment object has a score property for the concept ranging from -1 to 1. Negative scores indicate negative sentiments, and positive scores indicate positive sentiments. The tool supports Arabic, English, French, German, Italian, Portuguese, Russian, and Spanish. It has Node SDK, Java SDK, Python SDK, iOS SDK, and Unity SDK.\n\nSentiMetrix offers cutting-edge data science and social analytics solutions like SentiGrade and Sent\u0435\u041cotion to address the \u201cbig data\u201d needs of customers who have their own analytical and visualization solutions that could benefit from accurate sentiment scores extracted from the customers\u2019 data [14]. SentiGrade has an API that opens programmatic access to an automatic Sentiment Analysis engine. The system supports multiple languages. SentiMetrix also developed Sent\u0435\u041cotion, emotion-based search engine that tracks the intensity of different emotions in any given document.\n\nSome popular sentiment analysis tools and methods were presented above. All these tools make it possible to determine the sentiment of texts, which can be positive or negative. A number of tools classify text into additional categories: very positive, very negative, sarcastic, neutral, happy, frustrated, and sad. We described several Python libraries including NLTK, Pattern, TextBlob, Sentiment Classifier, some R packages like Syuzhet, RSentiment, Sentiment Analysis, and other tools like IBM Watson Natural Language Understanding, Stanford CoreNLP, RapidMiner, GATE, Google Prediction API, SentiGrade and Sent\u0435\u041cotion."
    },
    {
        "url": "https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c?source=user_profile---------6----------------",
        "title": "Sentiment Analysis Tools Overview, Part 1. Positive and Negative Words Databases",
        "text": "Sentiment analysis tools rely on lists of words and phrases with positive and negative connotations. Many dictionaries of positive and negative opinion words were already developed. In this paper, we will look at most known words databases.\n\nLiu and Hu opinion lexicon contains around 6800 positive and negative opinion words or sentiment words for English language [1]. This list was composed over many years.\n\nSentiWordNet is a lexical resource for opinion mining that assigns to each synset of WordNet three sentiment scores: positivity, negativity, and objectivity [2]. It has a Web-based graphical user interface, and it is freely available for research purposes. The development of the resource is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectoral term representations for semi-supervised synset classification. Positivity, negativity, and objectivity are derived by combining the results produced by a committee of eight ternary classifiers [3].\n\nThe next resource containing roughly 155,000 English words is SentiWords [4]. Words are associated with a sentiment score included between -1 and 1. Words are in the form lemma#PoS and are aligned with WordNet lists that include adjectives, nouns, verbs and adverbs.\n\nAFINN is a manually labeled by Finn \u00c5rup Nielsen in 2009\u20132011 list of English words rated for valence with an integer between minus five (negative) and plus five (positive) [5]. It is possible to try it on this site [6].\n\nThe WordStat Sentiment Dictionary includes more than 9164 negative and 4847 positive word patterns, but sentiment is not measured with those two lists [7]. Negative sentiment is measured by using the following two rules instead. The first rule is negative words not preceded by a negation within three words in the same sentence. The second rule is positive words preceded by a negation within three words in the same sentence. The rules for the positive sentiment are the same: positive words not preceded by a negation as well as negative terms following a negation.\n\nSenticNet provides polarity associated with 50,000 natural language concepts [8]. A polarity is a floating number between -1 and +1. Minus one is extreme negativity, and plus one is extreme positivity. The knowledge base is free. It can be downloaded as XML file. Its latest version is also accessible as an API.\n\nThe Affective Norms for English Words (ANEW) is a set of normative emotional words ratings in terms of pleasure, arousal, and dominance [9]. It was developed to create a standard for use in studies of emotion and attention.\n\nThe Whissell Dictionary of Affect in Language\n\nThe Whissell Dictionary of Affect in Language is a freeware software for the statistical analysis of individual words, not according to their meaning, but the way they \u2018feel\u2019 [10]. Including 348,000 words, it covers 90% of spoken English. Scores of volunteers rated words according to how pleasant they felt, how active they seemed, and how well the word brought an image to mind.\n\nIn Pattern, written text is categorized into two types firstly: facts and opinions [11]. Opinions can express people\u2019s sentiments toward the word. The package has the sentiment.xml, which includes 2888 words scored for polarity, subjectivity, intensity and reliability. The words are mostly adjectives. There are two useful functions: sentiment() and positive() . The sentiment() function returns a (polarity, subjectivity)-tuple, an ordered set of values, for the given sentence. A polarity is a value between -1.0 and +1.0 and subjectivity between 0.0 and 1.0. The positive() function returns True if the given sentence\u2019s polarity is above the threshold.\n\nThe Sentiment140 Lexicon was created from the Sentiment140 emoticon corpus of 1.6 million tweets and contains a list of words and their associations with positive and negative sentiment [12].\n\nLinguistic Inquiry and Word Count (LIWC) is a computer program for language analysis [13]. It supports English, German, Spanish, Dutch, and Italian. This commercial word list lets you extract around 60 different word categories, including positive emotions, negative emotions, aggression, affective processes, anxiety, anger, profanities, and so on.\n\nThe MPQA (Multi-Perspective Question Answering) Subjectivity Lexicon is a list of subjectivity clues that is part of OpinionFinder [14]. It helps to determine text polarity.\n\nThe list on this page contains more than 6000 positive words and phrases making it one of the longest and best list of positive words [15].\n\nOther examples of databases are positive words list [16] and negative words list [17].\n\nThe most popular positive and negative words databases that can help to perform sentiment analysis were described: Liu and Hu opinion lexicon, SentiWordNet, SentiWords, AFINN, WordStat Sentiment Dictionary, SenticNet, the Affective Norms for English Words, the Whissell Dictionary of Affect in Language, Pattern, Sentiment140 Lexicon, Linguistic Inquiry and Word Count, the MPQA Subjectivity Lexicon."
    },
    {
        "url": "https://chatbotsjournal.com/25-chatbot-platforms-a-comparative-table-aeefc932eaff?source=user_profile---------7----------------",
        "title": "25 Chatbot Platforms: A Comparative Table \u2013",
        "text": "25 of the best-known platforms for building chatbots, such as IBM Watson, Microsoft Bot Framework, LUIS, Wit.ai, Api.ai, Chatfuel, and others were studied, and a comparative table was composed.\n\nThe relevance of this research is proved by the massive deployment of chatbots. Indeed, today chatbots are used to solve a number of business tasks across many industries like E-Commerce, Insurance, Banking, Healthcare, Finance, Legal, Telecom, Logistics, Retail, Auto, Leisure, Travel, Sports, Entertainment, Media and many others. Gartner Summits [1] projects that more than 85% of customer interactions will be managed without a human by 2020. Chatbots are expected to be the number one consumer application of AI over the next five years according to TechEmergence [2] .\n\nMany experts called 2016 \u201cthe year of the chatbots.\u201d Thousands of chatbots already help businesses improve customer service, sell more, and increase earnings. This paper reports a Data Monsters overview of research on the best-known platforms for building chatbots.\n\nBelow is an overview of the most popular bot platforms.\n\nAccording to the research study by Mindbowser in association with Chatbots Journal [3] IBM Watson is the first choice as a bot-building platform for 61% of businesses. One of the Watson\u2019s most important parts is a Conversation Service. It is built on a neural network (one billion Wikipedia words), understands intents, interprets entities and dialogs, supports English and Japanese languages, and provides developer tools like Node SDK (Software Development Kit), Java SDK, Python SDK, iOS SDK and Unity SDK. IBM offers free, standard, and premium plans [4].\n\nRoyal Bank of Scotland launched Luvo, a chat bot taught to answer customers\u2019 questions in near-real time. It routes overly complex requests to advisers using IBM Watson Conversation [5]. Typical questions it answers with staff via the web chat include: I have lost my card \u2014 what steps do I need to take now? I have locked my PIN \u2014 how do I unlock it? How do I order a card reader for my business?\n\nForty-one percent of the businesses in Mindbowser\u2019s study [3] said they prefer Microsoft Bot Framework. It has its own Bot Builder SDK that includes .NET SDK and Node.js SDK. The entire system consists of three parts: Bot Connector, Developer Portal, and Bot Directory. The framework provides the Direct Line REST API, which can be used to host a bot in an application or website [6]. It is open source and available to all on Github, and it supports automatic translation to more than 30 languages. Microsoft Bot Framework understands users\u2019 intents. It is possible to incorporate LUIS for natural language understanding, Cortana for voice, and the Bing APIs for search.\n\nMicrosoft Language Understanding Intelligent Service (LUIS) uses intents and entities. All its applications are centered on a domain-specific topic or are content related. Active learning technology is one of LUIS\u2019s features. It is possible to use pre-existing, world-class, pre-built models from Bing and Cortana. Models deployment to an HTTP endpoint is a one-click operation; it returns easy-to-use JavaScript Object Notation (JSON). LUIS offers a set of programmatic REST APIs that can be used by developers to automate the application creation process. In addition, several SDKs are available: C# SDK, Python SDK, Node JS SDK, and Android SDK [7]. Supported languages are English, French, Italian, German, Spanish, Brazilian Portuguese, Japanese, Korean and Chinese. LUIS offers free and standard plans.\n\nForty-five percent of the respondents in a study by Mindbowser [3] said they trust Wit.ai more than any other bot-building platform. By April 2017, it was used by over 100,000 developers [8]. Wit.ai allows using entities, intents, contexts, and actions, and it incorporates natural language processing (NLP). There are several clients: Node.js, Python, Ruby, and HTTP API for other platforms. It is available for developers to use with iOS, Android, Windows Phone, Raspberry Pi, Python, C, and Rust; it also has a JavaScript plugin. Wit.ai supports about 50 languages, and it is free.\n\nAnother conversational platform for bots, applications, services, and devices is Api.ai. Api.ai matches the query to the most suitable intent based on information contained in the intent (examples, entities used for annotations, contexts, parameters, events) and the agent\u2019s machine learning model. Api.ai transforms the query text into actionable data and returns output data as a JSON response object. There are predefined knowledge packages collected over several years. Available SDKs are Android, iOS, Cordova, HTML, JavaScript, Node.js, .NET, Unity, Xamarin, C++, Python, Ruby, PHP, Epson Moverio, Botkit, and Java. Brazilian Portuguese, Chinese English, Dutch, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, and Ukrainian are the languages Api.ai can support. The platform is free. Google bought Api.ai in September 2016. Now it is possible to integrate your Api.ai agent with Actions on Google that lets you build applications for the Google Assistant, which is the only assistant in Google Home. Google Home enables users to interact with services through voice commands.\n\nSemantic Machines created a proprietary conversational AI. The features of the system include a conversation engine, speech synthesis, deep learning, reinforcement learning, speech recognition, semantic intent extraction, and language generation (NLG) technology [9]. Semantic Machines goes beyond understanding commands to understanding conversations. The platform is language independent.\n\nDigitalGenius developed Human + AI Customer Service. It\u2019s not exactly a chatbot platform, but rather a deep learning agent efficiency tool that works in any language. AI predicts case metadata and suggests the right answers to your customer service agents. AI learns from every agent interaction. It leverages a deep neural network model, word vectors, statistical operations, and deep learning algorithms [10]. Human + AI Customer Service installs as a layer into existing customer service software like Salesforce, Zendesk, etc.\n\nMore than 360,000 chatbots have been created using Chatfuel, serving more than 17 million users globally [11]. A block is a basic building tool of a bot. It consists of one or more message cards that are sent together to a bot user. The next step is to link blocks with each other using buttons in text cards or in gallery cards. Many plugins were developed: Google search, Bing search, JSON API, RSS Import, Subscribe plugin, Digest, IFTTT, Zapier, user input, and LiveChat. Chatfuel supports about 50 languages, and it is free.\n\nPypestream\u2019s Smart Messaging Platform uses a patented framework of \u2018Pypes\u2019 and \u2018Streams\u2019 for natural language processing and keyword parsing. An open and flexible API platform allows custom integrations and development of third party connectors, plugins, and extensions. The platform includes the Smart Messaging Framework, Pypeconnect SDK, Pypemanager, the Pypestream mobile app, as well as API plug-ins and integrations [12]. By April 2016, the company had 500 businesses signed up and using the messaging platform, including Washington Gas and Billboard.\n\nThe Pandorabots API allows you to integrate a bot hosting service and natural language processing engine into your own application. Developed SDKs are Java, Node.js, Python, Ruby, PHP, and Go. Pandorabots uses AIML (Artificial Intelligence Markup Language) and includes A.L.I.C.E. (The Artificial Linguistic Internet Computer Entity) \u2014 a natural language processing chatterbot. It is multilingual [13]. Common use cases include advertising, virtual assistance, e-learning, entertainment, and education. Academics and universities use the platform for teaching and research. Pandorabots is an AIaaS platform \u2014 Artificial Intelligence as a Service.\n\nAgentBot was specially developed for Latin America so it supports English, Spanish, and Portuguese languages. It uses Aivo\u2019s own natural language processing technology. The platform understands natural language, has memory to maintain coherence during long conversations, gathers customer information to deliver customized solutions, applies continuous evolution, and clarifies intents [14]. AgentBot integrates with any CRM, internal system, human chat, or third party application. For integration, REST API is used.\n\nChatterBot is a Python library that makes it possible to generate responses based on collections of known conversations [15]. ChatterBot is language independent. The software license is the 3-Clause BSD License.\n\nChatScript is the next generation chatbot engine that has won the Loebner\u2019s 4 times and is the basis for natural language company for a variety of tech startups. ChatScript is a rule-based engine. Rules are created in program scripts through a process called dialog flow scripting. These use a scripting metalanguage (a script) as their source code [16]. ChatScript engine has many features like powerful pattern matching aimed at detecting meaning, simple rule layout combined with C-style general scripting, built-in WordNet dictionary, extensive extensible ontology, local machine control via popen/tcpopen/jsonopen, structured JSON data reading from websites, and others. ChatScript runs on Windows, Linux, Mac, iOS, or Android. It has integrated tools to support maintaining and testing large systems. UTF8 support allows scripts written in any language. ChatScript works under the MIT License.\n\nPlatforms like IBM Watson, Microsoft Bot Framework, Api.ai, ChatScript and Pandorabots were developed ten or more years ago. Therefore, their experience provides the most advanced tools and offers the most flexible solutions for businesses. These platforms make it possible to use different programming languages. Each platform has developed its own SDKs, uses cutting edge data processing and analysis technologies, supports dozens of natural languages, and are already embedded in customer services, sales, marketing, order processing, social media, payment, recruitment and other industries. Just the same, many startups were created over last few years. Some of them grow fast and are already well-known:\n\nTwyla learns from agent/customer live chats, blends machine learning and rule-based methods, answers questions, and deflects tickets [17].\n\nMsg.ai integrates with popular customer support offerings while leveraging intent models and tone classifications [18]. It supports deep learning, interactive smart cards, and A/B testing.\n\nRasa NLU has HTTP API and Python support, intent classification, and entity extraction [19]; it is an open source tool that runs locally.\n\nReply.ai is a visual bot builder that easily leverages natural language processing (NLP) engines wit.ai and api.ai for your advanced use cases [20] with analytics about every single response from your users.\n\nThe basic functionality of ManyChat lets you welcome new users, send them content, schedule posts, set up keyword auto-responses (text, pictures, menus), automatically broadcast your RSS feed, and much more [21]. No coding is required. It is free.\n\nKITT.AI built its proprietary ChatFlow platform that enables users to create conversational agents, or smart bots, using a simple drag-and-drop interface that visually describes a dialogue and at the same time implements the flow that can be executed on the server as the dialogue is designed. The features of the platform are hotword detection (no internet required), semantic parsing, natural language understanding, conversational engine (multi-turn support), and neural network powered machine learning model [22]. ChatFlow supports Alexa, Facebook Messenger, Kik, Skype, Slack, Telegram, and Twilio. The platform is free now and lets you sign up for the beta. In the future the team is going to provide free or cheap access for individual developers and small teams, and higher charges for enterprise customers.\n\nIt\u2019s Alive is a free Facebook page chatbot building platform. The core feature of the platform is recipes. This feature enables you to automatically respond when your users write specific keywords or phrases. If your chatbot has missed a keyword, it will be added to new or existing recipes. The platform enables you to send periodic content (RSS) each day, week or month, or to manually send a one-time message to each chatbot subscriber. The team believes in decision trees and buttons that drive users towards the answer they are looking for [23].\n\nIf it is necessary for your business to create a chatbot, hundreds of various chatbot-building platforms are available. There are tools that are centered on a domain-specific topic or universal tools, platforms that allow you to program your chatbot using SDKs, platforms that do not require coding, and environments that support English or your primary language. There are paid and free platforms."
    }
]