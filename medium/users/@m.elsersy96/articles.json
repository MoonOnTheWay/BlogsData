[
    {
        "url": "https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-2-b209e8617c5a?source=user_profile---------1----------------",
        "title": "Reinforcement Learning Demystified: Markov Decision Processes (Part 2)",
        "text": "As we mentioned before, the state-value function can be decomposed into immediate reward Rt+1, and discounted value of successor state \ud835\udefeV\ud835\udf0b(St+1) on policy \ud835\udf0b,\n\nThe idea is wherever you are, you take one step and you get your immediate reward for that step, and then you look at the value where you end up. The sum of those things together tells you how good it was to be in your original state.\n\nIn this case, you start in state S, following policy \ud835\udf0b, but the value being in that state is the immediate reward you get, added to the value of the successor state, if you know your are going to follow the policy \ud835\udf0b from that state on-wards.\n\nSimilarly, the action-value function can be decomposed,\n\nIf I\u2019m in state S, and I take action a from there, I\u2019ll get an immediate reward for that action, and then I\u2019ll look at where I end up, and I can ask, what is the action-value in the state I end up in under the action I\u2019ll pick from that point onwards. The sum of those things together tells us how good it was to take that action from that particular state.\n\nSince we have multiple actions from one state S, and the policy defines a probability distribution over those actions, we are gonna average, and that is the bellman expectation equation,\n\nFrom a particular state S, there are multiple actions, I\u2019m gonna average over the actions that I might take. There is a probability that I\u2019m gonna take the first action and another probability that I\u2019m gonna take the second action and so on. This probability distribution is defined by a policy \ud835\udf0b.\n\nWhen we reach q (the action value) for the action that we took, it tells us how good is it to take that action from that state. Averaging over possible action-values tells us how good is it to be in state S.\n\nAfter I took that action, I\u2019m committed to it. The environment might blow me left or right, since there is stochasticity in the environment. I want to ask for each of these situations I might get blown to, how good is it?, what is the value of being in that situation following my policy onwards.\n\nSo we are gonna average over possible things that might happen, i.e. possible successor states the agent might land in, meaning multiplying each state value on policy \ud835\udf0b we might land in by the probability that we land in it. summing all those things together tells us how good it was to take that particular action from that particular state S.\n\nRemember V\ud835\udf0b(s) is telling us how good is it to be in a particular state, and q\ud835\udf0b(s, a) is telling us how good is it to take a particular action from a given state.\n\nAt the root, we got the value function for state S. It tells us how good is it to be in that particular state. We consider all the actions we might take next and we consider all the things the environment might do to us after we took some action. For each of the things the environment might do, there are successor states. We might land in one of those states.\n\nWe want to know how good is it to be in that state we landed in, and carry on with our usual policy, i.e. how much reward I\u2019ll get for carrying on from that point onwards. We are gonna average over all those together, we\u2019re weighting each of the first two arcs by a probability the policy will select, and weighting each of the second level arcs by a probability the environment will select, and this gives us the value of being in the root.\n\nStarting from an action a, we now can look ahead, considering where the environment might blow us in after we took that action. First we get the immediate reward for our action, and then we average over possible states we might land in, i.e. the value of each state we might land in multiplied by a probability the environment will select and average over all those things together.\n\nThen consider from the state we\u2019re blown to which action might I take next. There is a probability distribution over possible actions from there, and then we average over.\n\nFor our state space, we evaluate the state in red. The policy defines a uniform probability distribution for the two possible actions, we\u2019re gonna weight each of the things that might happen after taking one action by 0.5.\n\nFor the action \u201cStudy\u201d, we\u2019re gonna weight it by 0.5 multiplied by the immediate reward for that action, and since the state we\u2019re gonna land in is the terminal state, i.e. it has zero value, the action value will be just the probability of the action multiplied by the reward for that action.\n\nFor the second action that we might take \u201cPub\u201d, we\u2019re gonna weight the things that might happen after taking that action by the probability that we take that action.\n\nFirst, we get the immediate reward for that action +1, added to an average over possible successor states. There is a chance node that we go to some state, or some other state, or return to the state where we started.\n\nWe multiply the value of each state we might land in after taking the action by the probability that we land in in, which is defined by the environment. The sum of those things together gives us the value of being in our original state.\n\nBellman expectation equation can be expressed concisely using the induced MRP,\n\nWe can solve it directly,"
    },
    {
        "url": "https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690?source=user_profile---------2----------------",
        "title": "Reinforcement Learning Demystified: Markov Decision Processes (Part 1)",
        "text": "A Markov Reward Process or an MRP is a Markov process with value judgment, saying how much reward accumulated through some particular sequence that we sampled.\n\nAn MRP is a tuple (S, P, R, \ud835\udefe) where S is a finite state space, P is the state transition probability function, R is a reward function where,\n\nit says how much immediate reward we expect to get from state S at the moment.\n\nThere is the notion of the return Gt, which is the total discounted rewards from time step t. This is what we care about, the goal is to maximize this return,\n\n\ud835\udefe is a discount factor, where \ud835\udefe \u2208 [0, 1]. It informs the agent of how much it should care about rewards now to rewards in the future. If (\ud835\udefe = 0), that means the agent is short-sighted, in other words, it only cares about the first reward. If (\ud835\udefe = 1), that means the agent is far-sighted, i.e. it cares about all future rewards.What we care about is the total rewards that we\u2019re going to get.\n\nYou might be confused, why put a discounting factor ?. why not get all the rewards undiscounted ?. It turns out to be mathematically convenient to discount rewards, here we guarantee that the algorithm will converge, and avoid infinite returns in loopy Markov processes.\n\nNote that, although the return is a sum of an infinite number of terms, it is still\n\nfinite if the reward is nonzero and constant, if \ud835\udefe < 1. For example, if the reward is a constant +1, then the return is,\n\nAnother reason to discount rewards is that, the agent is not certain about what would happen in the future, it might be better to take the immediate reward rather than waiting in the hope to get a larger reward in the future, so \ud835\udefe defines a kind of finite horizon for what to care about. \ud835\udefe implicitly encoded the animal/human cognitive model, which shows preferences for immediate rewards.\n\nBear with me, imagine you are in a situation, where someone offered you to get a 1000$ now, or to get 1000$ after each passing hour for 10 hours, but with each passing hour the value of a 1000$ is decreasing by some factor \ud835\udefe to the power of the passed hours. As a rational person trying to get the maximum possible amount of money, your choice depends on \ud835\udefe.\n\nIf (\ud835\udefe = 0.1), with simple calculation, your return would be,\n\non the other hand, if (\ud835\udefe = 0.9), the return would be,\n\nNotice that, if the decreasing factor is near zero, you\u2019ll care only about first hour or 2 hours, and stop wasting your time and just leave, i.e. get to the terminal state, all that comes afterwards is worthless. If \ud835\udefe is near 1, you will wait the 10 hours to get the money, i.e. still taking actions to get rewards. So \ud835\udefe defined a horizon for acting in the environment.\n\nSometimes, it\u2019s possible to use undiscounted MRPs (i.e. \ud835\udefe = 1), if we are sure that all sequences will terminate\n\nAs we mentioned in the previous blog post, the value function tells us how good is each state and/or action, i.e. how good is it to be in a particular state, and how good is it to take a particular action. It informs the agent of how much reward to expect if it takes a particular action in a particular state.\n\nThe state-value function of an MRP is the expected return starting from state s,\n\nE.g. the return of the previously mentioned episode [Class 1 \u2192Class 2 \u2192 Class 3 \u2192 Pass \u2192Sleep] would be,\n\nThe value of state Class 1 would be = -2.25."
    },
    {
        "url": "https://towardsdatascience.com/reinforcement-learning-demystified-36c39c11ec14?source=user_profile---------3----------------",
        "title": "Reinforcement Learning Demystified: A Gentle Introduction",
        "text": "In a long blog post series starting with this episode, I\u2019ll try to simplify the theory behind the science of reinforcement learning and its applications and cover code examples to make a solid illustration.\n\nReinforcement learning or RL for short is the science of decision making or the optimal way of making decisions. When an infant plays, waves its arms, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about consequences of actions, and about what to do in order to achieve goals.\n\nThis is the key idea behind RL, we have an environment which represents the outside world to the agent and an agent that takes actions, receives observations from the environment that consists of a reward for his action and information of his new state. That reward informs the agent of how good or bad was the taken action, and the observation tells him what is his next state in the environment.\n\nThe agent tries to figure out the the best actions to take or the optimal way to behave in the environment in order to carry out his task in the best possible way.\n\nThis is a simulation of a humanoid that learned how to run after executing the sequence of acting, observing, and then acting until it finally figured out the best action to take at each time step to achieve its task, i.e. running efficiently.\n\nHere is a successful example of an RL agent that learned to play Breakout like any human being after 400 training episodes. After 600 training episodes, the agent finds and exploits the best strategy of tunnelling and then hitting the ball behind the wall.\n\nKeep in mind there is no explicit human supervisor, the agent learns by trial and error.\n\nAnother amazing success story is how Deepmind used RL to simulate locomotion behavior on mujoco\u2019s simulation models. The agent is given proprioception and simplified vision to perceive the environment.\n\nThe agent learns to run, jump, crouch, and climb through relentless attempts of trials and learning from its errors.\n\nWe can\u2019t ignore the biggest event in AI community, the ultimate smackdown of human versus machine, where Deepmind\u2019s AlphaGo ruthlessly managed to defeat Lee Sedol, the South Korean professional Go player of 9 dan rank, 4 matches against 1 in March 2016. This guy has 18 world championships under his bed."
    }
]