[
    {
        "url": "https://medium.com/blockimmo/a-simple-layer-enabling-a-smart-contract-based-web-app-691f0b64c475?source=user_profile---------1----------------",
        "title": "A simple layer enabling a smart contract based web app",
        "text": "In a previous post I introduced the core/foundational layer of blockimmo \u2014 our smart contracts. Mission-critical state and functionality live on-chain, encapsulated by these contracts. The rights/ownership to a plot of land is an example of on-chain state. The exchange/transfer of these rights/ownership is an example of on-chain functionality.\n\nOur approach is to integrate seamlessly with the existing processes and systems powering the Swiss real-estate market, offloading bottlenecks/critical components on-chain. Playing to its strengths, while simplifying and streamlining where most effective, to enable a more efficient and robust market.\n\nThis post focuses on the interface between our contracts (living on the Ethereum blockchain) and our frontend/users. This layer enables easy, secure blockchain interaction, and seamless integration with our frontend. The result is an intuitive UI/UX for our users to buy, sell, and invest in Swiss \ud83c\udde8\ud83c\udded real-estate.\n\nOur general user should not care about blockchain, or any other parts of our software stack. In the same way nobody cares about what happens under-the-hood when they do an online bank transaction. We aim to facilitate the interaction between our users and smart contracts at a reasonable level of abstraction.\n\nWith a browser, the power of the internet is at a user\u2019s fingertips. With a \u00d0App, blockchain is. From here only a very basic understanding of the technology is required (i.e. web addresses in the internet world, public addresses in the blockchain world, and a few other concepts). Both the internet and blockchain user must always take caution though. And since blockchain users haven\u2019t built an intuition for safety yet, we need to hold their hand a bit to ensure they don\u2019t stray off the beaten path.\n\nOnly the most fundamental state is stored on-chain. One such example is a property\u2019s E-Grid (Eidgen\u00f6ssische Grundst\u00fccksidentifikation \u2014 a universally unique identifier). By storing this (immutable) state, we can retrieve any other information related to a property dynamically/lazily (instead of explicitly maintaining/storing it) through existing, standard systems. This (functional) approach greatly simplifies and leads to a more resilient, robust platform. The end result is a smooth user-experience.\n\nUsers interact with our smart contracts in two ways: performing calls (reads) and transactions (writes). Calls are cheap, instant, and completely abstracted from the user \u2014 they happen in the background without any user intervention required. Just like reading from any normal database. By establishing a connection to a public Ethereum node (i.e. via the Infura API) a client isn\u2019t required to run a node themselves.\n\nTransactions are expensive \u2014 they must be signed by the user with their private key(s), and then mined by the network (slow). This has two major implications: (1) blockimmo must provide users with an easy, secure way to sign transactions, and (2) these operations should be performed only when absolutely necessary.\n\nOur first-principles approach to state puts us in a good position for (2). Because only the most fundamental state is stored on-chain, transactions occur rarely and are only performed when a user lists a property for-sale, or buys/invests. This small surface area allows us to really focus on an intuitive UI/UX around these processes to ensure the user fully understands what is being signed and complete transparency.\n\nThis leads us to (1). Instead of reinventing the wheel we rely on MetaMask \u2014 probably the most solid, well-tested solution enabling our users to review and sign blockchain transactions in-app.\n\nTransactions are the weak link in the chain when it comes to building a secure \u00d0App. While blockchains have been proven to be extremely secure, and we can develop secure, audited smart contracts, if an attacker tricks a user into signing a spoofed transaction (i.e. sending funds to a different address), or steals a user\u2019s private key(s), all this security doesn\u2019t matter. It is our job to make these kind of attacks as difficult/impractical as possible. Making transactions end-to-end secure is the major driving factor in our approaches to (1) and (2).\n\nWe\u2019ve identified MetaMask as the best default wallet for reviewing and signing transactions in-app. Users are not required to install the MetaMask Chrome/Firefox extension for browsing (performing calls/read-only) or buying/investing (optional here as explained in the coming sections). MetaMask is only required when listing a property for-sale . During this process we tokenize the property and deploy the smart contracts around/attached to it (i.e. the and ), and MetaMask integrates best with our UI/UX to make this process easy and intuitive for the seller.\n\nIt is important to note that MetaMask is a hot wallet \u2014 it lives on an internet connected device. If an attacker has full access to a user\u2019s computer and MetaMask (either unlocked or the attacker also has the user\u2019s vault\u2019s password) then that user would be compromised. However, a has no real value until it is verified in the by blockimmo. If anything goes wrong up until this point, the entire listing process can be reverted without harm. This ensures a safe, fool-proof listing process for sellers, and guarantees a property\u2019s legitimacy for buyers/investors.\n\nOnce a is live, blockimmo has verified the in the . An attacker would gain nothing from having access to a seller\u2019s private keys (used to list the property) at this point. This is because when configuring a , the seller provides a public address (hardware wallet recommended) where raised funds (and/or any unsold tokens) are transferred upon completion of the . This wallet is verified to be in the seller\u2019s control by blockimmo in the listing\u2019s verification process.\n\nInvesting in a property (buying tokens via a ) is simply sending ETH to the smart contract in exchange for tokens of the property. This is easily accomplished in-app via MetaMask, or out-of-band via any software/hardware wallet.\n\nWhen exchanging value (verified tokens of property and/or ETH) we recommend using hardware wallets. We developed our smart contracts from the ground-up to enable seamless use of hardware wallets. In the near-future hardware wallets will be supported in MetaMask and our solution will become even simpler!\n\nSo far we\u2019ve introduced our smart contracts and how we interact with them. In our next post we will tie everything together to introduce our web-app!"
    },
    {
        "url": "https://medium.com/blockimmo/the-smart-contracts-powering-blockimmo-fc16e1bbee09?source=user_profile---------2----------------",
        "title": "The smart contracts powering blockimmo \u2013 blockimmo \u2013",
        "text": "This is a semi-technical post and assumes the reader has a basic understanding of blockchain and smart contracts. It gives a high-level overview of blockimmo\u2019s core smart contracts and highlights some of the design principles guiding development. Detailed documentation will be released when we open-source our contracts in the coming months. For an introductory post see:\n\nThe \ud83d\udd34 is a simple smart contract living on the Ethereum blockchain. The first step of creating a listing on blockimmo is tokenizing a property for sale (if not already on-chain). This is the process of moving a property to its on-chain representation. A \ud83c\udf4a smart contract is deployed (completely independent of and other properties that already/will live on-chain) that becomes the official representation of the rights/ownership to this specific property. At this point, blockimmo initiates processes in the background to verify/validate the authenticity of this listing, and append the to the if \ud83d\udcaf.\n\nA \ud83c\udf15 contract is then deployed (completely independent of ), and granted control of . It enables owners of a property to make major decisions related to the property via a de-centralized voting process.\n\nLastly, a \ud83d\udd35 contract is deployed (completely independent of and ), and given access to the tokens to be sold. Multiple investors can now buy tokens of property (through the ) with ETH! The seller of the property will receive ETH, and the investors will receive tokens!\n\nblockimmo aims to be flexible and enable an eco-system of on and off-chain services. We will go into more detail about our vision and how our technology enables this in a future post. The important thing to note is that these smart contracts are designed to be modular and plugged into by other contracts/parties. In the following sections we will briefly look into each of these contracts in a bit more detail.\n\nThe Swiss land registry (Grundbuchauszug) is an immutable, append-only, public ledger (sound familiar \ud83e\udd14?) recording the rights/ownership pertaining to plots of land. Each and every parcel of land in Switzerland is recorded here, indexed by it\u2019s Grundst\u00fcck Number (a universally unique identifier) by canton. All information relevant to this land is publicly available and attainable with this number. This information is maintained by Swiss land registrars, and is easily accessible to the general public. There is no need to duplicate this information and store it on-chain.\n\nThese fundamental characteristics of the Swiss land registry make it a perfect fit for blockchain \u2014 an extremely simple smart contract can encompass this functionality. This contract has a single public state variable , mapping \u27a1\ufe0f . Each is an independent smart contract that controls and manages that plot of land.\n\nFurthermore, the Swiss land registry is partially decentralized \u2014 there is no central land registrar. Rather, the individual cantons are responsible for maintaining their land register, and the cadastral surveying is mostly done by private parties acting as public agents, providing a decentralized service.\n\nAs the blockimmo platform matures, we envision completely decentralizing this system \u2014 cadastral surveying will be commissioned and delivered/executed on-chain. This is a next step, and highlights our modular, one-step-at-a-time approach towards moving the Swiss real-estate industry/market on-chain.\n\nLet\u2019s zoom-in on . This is an ERC20 compliant token denominated such that ownership can occur across many parties in small stakes. This contract maps the (Ethereum addresses) to the they own. Owning 50% of a property\u2019s tokens correlates to 50% ownership of that property.\n\nGiven a property\u2019s Grundst\u00fcck Number, we can look it up in the to find it\u2019s on-chain representation \u2014 . We can read it\u2019s state (who owns what, etc\u2026), and call it\u2019s functions to execute additional functionality (to be discussed in a future posts/case-studies). Furthermore, we can view other useful information like a property\u2019s entire transaction history to-date. Useful meta-data (i.e. information surrounding a transaction) is stored off-chain via IPFS and indexed in\n\nA given property may have multiple owners with varying stakes of ownership. We need a solid mechanism to enable them to effectively make and execute decisions. This is accomplished by a simple . Each is an independent smart contract that is attached to a . Owners of can and in favor or against an open .\n\nAny property owner can extend a proposal, and if majority consensus is reached, the proposal will be executed. An owner\u2019s is determined by their stake in the property (a party that owns over 50% of a property has full decision making power). determines / / by reading \u2019s state \u2014 .\n\nA decision is usually in the form of sending ETH (goods) to a party for (off-chain) services. We will explain this further in a future post/case-study.\n\nThe complete management of an investment property will usually be outsourced to a professional partner active in the region the property is located. This ensures the building stays in good condition, that tenants receive excellent service, and that accounting and finances are done by a trustworthy party. In a future case-study we will demonstrate a property\u2019s owner/ assigning management of the property to a management company.\n\nInvestment properties (both commercial and residential) generate income. In the case a property is under control of a management company, this income is routed (in ETH) to the smart contract on a quarterly basis (without a management company tenants would be required to pay the smart contract directly, in ETH). This income is used to pay any of the property\u2019s expenses (as determined by and/or the management company), and the remaining funds are automatically distributed to the property\u2019s owners (according to their stake). The behavior is similar to dividend payouts.\n\nAn investment property will usually be sold via crowd-sale. A crowd-sale is an ideal mechanism for exchanging tokens of property (or any digital asset) for ETH. A is an independent smart contract that a seller attaches to the (giving access to their tokens). The behavior is then the same as a normal crowd-sale (i.e. an ICO). A is configurable by the seller (i.e. , , , etc\u2026), but the end result is investors receive rights/ownership of property (tokens), and the seller receives ETH.\n\nIn this post we presented a simplified view of the suite of smart contracts powering blockimmo. In future posts/case-studies we will go into more detail \u2014 especially when we open source our contracts. These contracts are abstracted from a user\u2019s-perspective with a clean and intuitive UI/UX. A user only needs to sign/verify blockchain transactions (via MetaMask, and uPort for 2FA):\n\nFurthermore, there are legal and regulatory processes/workflows that occur in parallel/asynchronously to our smart contracts/\u00d0App. We are partnered with a strong legal company to ensure these are executed correctly and smoothly."
    },
    {
        "url": "https://blog.waya.ai/deploy-deep-machine-learning-in-production-the-pythonic-way-a17105f1540e?source=user_profile---------3----------------",
        "title": "Serverless deep/machine learning in production \u2014 the pythonic \ud83d\udc0d way \u262f",
        "text": "In this post we will serve a pyt\ud83d\udd25rch deep learning model with AWS lambda. The simplicity and effectiveness of this approach is pretty amazing. For many use-cases this will greatly simplify our production pipeline. On top of that we will likely see improvements in various metrics across the board \u2014 we will touch on some of these over the course of this post. Code will be provided as we go to get you up and running quickly \ud83c\udfc3.\n\nAWS provides great documentation to quickly get started with Lambda which I won\u2019t repeat. I\u2019ll be helping you through the tricky, deep learning specific parts. After reading this post follow the tutorial below and refer to the code and tips I provide as needed.\n\nLambda imposes some limitations that can be tight for deep learning use-cases.\n\nThe major limit we will run into is the 250 MB limit on the uncompressed deployment package size (the 50 MB limit on the compressed deployment package size is not enforced if we pull from s3) < major \ud83d\udd11 #2. The key to reducing the size of our deployment package is building large dependencies (in this example numpy and pytorch) from source (building from source also builds character \ud83d\udcaa). No hacks or complicated workarounds necessary \ud83d\ude4f.\n\npytorch alone is larger than 1GB when installed from pre-built binaries. By building pytorch from source we can reduce its size to ~124 MB. We save so much space because we specify that we aren\u2019t using CUDA (AWS Lambda doesn\u2019t have this capability yet anyways).\n\nThat\u2019s it\u2026 This is just the tip of the ice-burg. Python dependencies can be stripped down much more if needed (some build flags/optimizations and a simple script like this can cut total size in half by avoiding data duplication). If you hit scale you can think about exporting your trained model to onnx and then importing it to a more production oriented deep learning library such as caffe2.\n\nStay tuned for an example of a complete production pipeline following this approach!\n\nIf your company needs an experienced software engineer specialized in data engineering/science and deep/machine learning contact me or visit https://waya.ai for consulting service."
    },
    {
        "url": "https://blog.waya.ai/deep-adversarial-learning-is-finally-ready-and-will-radically-change-the-game-f0cfda7b91d3?source=user_profile---------4----------------",
        "title": "Deep adversarial learning is finally ready \ud83d\ude80 and will radically change the game",
        "text": "Adversarial learning is one of the most hyped areas in deep learning. If you browse arxiv-sanity, you\u2019ll notice much of the most popular recent research explores this area.\n\nHere is a presentation I gave on the topic at the Re-Work 2017 Deep Learning in Healthcare summit:\n\nIn the opening lecture of a course I took at UIUC on analog signals and systems, the professor confidently asserted something along the lines of:\n\nDeep neural networks learn hierarchical representations of data. The layers in a network and the representations they learn build on each other, with layers representing data at a progressively higher level of abstraction. Given raw data, a question to ask the network, and an objective function to evaluate the network\u2019s answer, a network learns to optimally represent (abstract) this data.\n\nA \ud83d\udd11 consequence of this concept is that feature engineering is learned and performed by the network. As opposed to the classical machine learning approach where features that are expected to contain information relevant to the task-at-hand are manually identified and extracted from data, reducing the dimensionality of input to the \u2018learning\u2019 algorithm.\n\nWhen the underlying structure, patterns, and mechanisms of data are learned instead of hand-crafted \u270d\ufe0f, previously infeasible applications of AI are enabled and super-human performance is made possible.\n\nYears ago I had a boxing coach who wouldn\u2019t let new boxers ask questions. New boxers asked the wrong questions, got answers they didn\u2019t need, and then focused on the wrong things.\n\nThe beauty of adversarial learning is that our networks learn entirely from data \u2014 the questions to ask, the corresponding answers, and the evaluation of these answers are learned. As opposed to the classical deep learning approach where questions that are expected to be relevant to the task-at-hand are manually identified, and hand-crafted objective functions guide the optimization of our networks towards learning the corresponding answers.\n\nDeep Mind recently demonstrated the amazing potential of deep (adversarial) learning with AlphaGo, showing that AlphaGo invents new knowledge and teaches new theories in the game of Go. This ushered in a new era of Go and moved players past a local maxima they\u2019d been stuck in for thousands of years. AlphaGo was able to achieve this by learning an evaluation function that describes \u2018the score\u2019 at any given moment to the system, rather than attempting to hand-craft and pre-program this. AlphaGo was then trained against itself through millions of simulated games. Sound like adversarial learning \ud83e\udd14?\n\nAlphaGo didn\u2019t just brute force \ud83d\udc4a its way towards becoming the best Go player in the world. It truly mastered the game and all its subtleties and intricacies. This was possible because it wasn\u2019t constrained by human input or our (what we now realize is limited) understanding of the problem domain (both in asking, answering, and evaluating questions). The next step will be to apply these approaches in learning to the real-world \ud83c\udf0f. It\u2019s hard to imagine how AI will reinvent agriculture \ud83c\udf31, healthcare \ud83c\udfe5, etc\u2026 but it will happen.\n\nThe above quote motivated me to start working with GANs. GANs pose the training process as a game between two networks and allow adversarial learning on generic data.\n\nWith the goal of modeling the true data distribution, the generator learns to generate realistic samples of data while the discriminator learns to determine if these samples are real or not. With the belief that the ultimate expression to understanding something is being able to recreate it, our goal seems like a worthy one. If we are able to successfully train our GAN to equilibrium (generated samples indistinguishable from real samples by a perfect discriminator), we should be able to apply this gained understanding of our data to almost any task with top performance \ud83c\udfaf.\n\nGANs are difficult to optimize and training is unstable. The network architectures must be carefully designed and the balance between the generator and discriminator must be carefully maintained for training to converge. On top of that mode dropping is typical in GANs (generator learns only a very small subset of the true distribution), and they are difficult to debug due to near-meaningless learning curves.\n\nStill, state-of-the-art results have been achieved with GANs, but practical usefulness has been limited by these problems.\n\nGANs are trained to minimize the distance between the generated and true data distributions. Initially, the Jensen-Shannon divergence was used as this distance metric. However, Wasserstein GAN (wGAN) provided extensive theoretical work and showed empirically that minimizing a reasonable and efficient approximation of the Earth Mover\u2019s (EM) distance is a theoretically sound optimization problem that cures the main problems of GANs (described in the section above). For this approximation of the EM distance to be valid, wGAN imposed weight clipping constraints on the critic (referred to as the discriminator pre-Wasserstein) which caused some training failures.\n\nImproved training of Wasserstein GANs enables very stable GAN training by penalizing the norm of the gradient of the critic with respect to its input instead of clipping weights. This \u2018gradient penalty\u2019 is simply added to the Wasserstein distance described above for the total loss.\n\nFinally, for the first time, we can train a wide variety of GAN architectures with almost no hyper-parameter tuning, including 101-layer ResNets and language models over discrete data \ud83d\udcaa!\n\nOne of the \ud83d\udd11 benefits of using the Wasserstein distance is that as the critic improves, the generator will receive improved gradients from it. When using the Jensen-Shannon divergence, gradients vanish as the discriminator improves and the generator has nothing to learn from (a major source of training instability).\n\nI recommend reading both papers if interested in gaining a solid theoretical understanding of these concepts:\n\nThe way I visualize GANs has changed with the introduction of this new objective function as illustrated below:\n\nAdversarial learning allows us to free our models of any constraints or limitations in our understanding of the problem domain \u2014 there is no preconception of what to learn and the model is free to explore \ud83d\udd75 the data.\n\nIn the next post we will see how we can utilize the representations learned by our generator for image classification."
    },
    {
        "url": "https://blog.waya.ai/deep-residual-learning-9610bb62c355?source=user_profile---------5----------------",
        "title": "Understand Deep Residual Networks \u2014 a simple, modular learning framework that has redefined\u2026",
        "text": "Deep residual networks took the deep learning world by storm when Microsoft Research released Deep Residual Learning for Image Recognition. These networks led to 1st-place winning entries in all five main tracks of the ImageNet and COCO 2015 competitions, which covered image classification, object detection, and semantic segmentation. The robustness of ResNets has since been proven by various visual recognition tasks and by non-visual tasks involving speech and language.\n\nThis post will summarize the three papers below, with simple and clean Keras implementations of the network architectures discussed. You will have a solid understanding of residual networks and their implementation by the end of this post.\n\nLots of paraphrasing/quotes from these papers throughout this article.\n\nNetwork depth is of crucial importance in neural network architectures, but deeper networks are more difficult to train. The residual learning framework eases the training of these networks, and enables them to be substantially deeper \u2014 leading to improved performance in both visual and non-visual tasks. These residual networks are much deeper than their \u2018plain\u2019 counterparts, yet they require a similar number of parameters (weights).\n\nWe have reformulated the fundamental building block (figure above) of our network under the assumption that the optimal function a block is trying to model is closer to an identity mapping than to a zero mapping, and that it should be easier to find the perturbations with reference to an identity mapping than to a zero mapping. This simplifies the optimization of our network at almost no cost. Subsequent blocks in our network are thus responsible for fine-tuning the output of a previous block, instead of having to generate the desired output from scratch.\n\nWide (width refers to the number of channels in a layer) residual networks attempt to address the problem of diminishing feature reuse (few residual blocks learning useful representations or many blocks sharing very little information with small contribution to the final goal) in very deep (thin) residual networks.\n\nWe will take this insight and skip over their proposed architecture as the performance improvements they achieve seem to be due to increased model capacity (number of parameters) and adding additional regularization in the form of dropout \u2014 not the actual architecture. We will see in the next section (ResNeXt) that there is a much more effective way to increase model capacity than just increasing width. In fact, there is a balance between the depth and width of these networks \u2014 if layers are too wide the model will learn extraneous information (added noise), and if layers are too thin subsequent layers will not have much to learn from (i.e. diminishing feature reuse).\n\nIt is worth noting that training time is reduced because wider models take advantage of GPUs being more efficient in parallel computations on large tensors even though the number of parameters and floating point operations has increased.\n\nIn Deep Residual Learning for Image Recognition a residual learning framework was developed with the goal of training deeper neural networks. Wide Residual Networks showed the power of these networks is actually in residual blocks, and that the effect of depth is supplementary at a certain point. Aggregated Residual Transformations for Deep Neural Networks builds on this, and exposes a new dimension called cardinality as an essential network parameter, in addition to depth and width. Cardinality is demonstrated to be more effective than going deeper or wider when increasing model capacity, especially when increasing depth and width leads to diminishing returns.\n\nResidual connections are helpful for simplifying a network\u2019s optimization, whereas aggregated transformations lead to stronger representation power (as shown by the fact that they perform consistently better than their counterparts with or without residual connections).\n\nThe result is a homogenous, multi-branch architecture with only a few hyper-parameters to set.\n\nPlease comment on Gist if you spot any errors/possible improvements in code. pyt\ud83d\udd25rch implementation.\n\nI\u2019ll be working ResNeXt into my improved Wasserstein GAN code (wGAN implemented on top of keras/tensorflow as described in: Wasserstein GAN with improvements as described in: Improved Training of Wasserstein GANs) and running some experiments (i.e. a pre-trained discriminator). Expect a blog post on this in the near future.\n\nHere is a rough draft of a GAN network architecture where generator and discriminator are ResNeXt. Please comment on Gist if you spot any errors/possible improvements. https://gist.github.com/mjdietzx/600751b780e1ab2b8802f7788f17882e\n\nI\u2019ll be talking about GANs at the Deep Learning in Healthcare Summit in Boston on Friday, May 26th. Feel free to stop by and say \ud83d\udc4b!"
    },
    {
        "url": "https://blog.waya.ai/quick-start-pyt-rch-on-an-aws-ec2-gpu-enabled-compute-instance-5eed12fbd168?source=user_profile---------6----------------",
        "title": "Quick start \u2014 pyt\ud83d\udd25rch on an AWS EC2 GPU enabled compute instance",
        "text": "No, a neckbeard isn\u2019t required to install tensorflow/pytorch and their dependencies properly. In this tutorial we\u2019ll install pytorch on an AWS EC2 GPU enabled compute instance.\n\nThis post serves as a general quick start to get up and running with your favorite deep learning library/framework on an AWS EC2 GPU enabled compute instance as pretty much everything is the same.\n\nLaunch an instance from the AWS EC2 console and select .\n\nI recommend choosing a instance as there is a ~2\u20133x speed-up when compared to the previous generation of instances (see a detailed comparison here).\n\nCUDA and cuDNN ship with pytorch. No need to download/install these. Simply run the script attached below to install the required NVIDA driver and reboot:\n\nIf you are using a different deep learning library/framework then run the script attached below to install CUDA and cuDNN:\n\nNow all we need to do is install pytorch with : .\n\nTo verify everything is working open the Python interpreter and ."
    },
    {
        "url": "https://blog.waya.ai/ground-up-hands-on-deep-learning-tutorial-diagnosing-skin-cancer-w-dermatologist-level-61a90fe9f269?source=user_profile---------7----------------",
        "title": "Comprehensive tutorial \u2014 deep learning to diagnose skin cancer with the accuracy of a dermatologist",
        "text": "Waya.ai recently open sourced the core components of its skin cancer diagnostic software. The objective of this effort is to release a free and open source product in early May that has been validated to diagnose skin cancer with dermatologist-level accuracy or better.\n\nTo learn more about this project and why is doing this see:\n\nThe code we\u2019ll be referencing lives here (contributions are very welcome):\n\nWorking with computer vision/image classification is one of the best paths towards a fundamental understanding of deep learning. It will enable you to visualize concepts effectively and build a solid understanding that transfers to other areas of deep learning/AI. In addition, there\u2019s troves of prior work and public data sets, allowing us to get started quickly \u2014 learning by doing (how Jobs and Edison did it)!\n\nRead the beginning (sections , , and ) of a medium post I made in February 2017. Reading the entire article is optional \u2014 no need to understand GANs yet:\n\nLet\u2019s consider a deep neural network, referred to as a model in this post, as a black box with millions of tune-able knobs.\n\nOur goal then is to train the model to learn the optimal position (value) of each knob (parameter) such that the model transforms a sample of data (input) into its true annotation/label (output).\n\nYou can imagine there are an infinite combination of possible inputs. So how can the model possibly learn to take pixels as input and predict benign/malignancy probability!? The key:\n\nWe call it deep learning because models are composed of many layers (deep). The first layer in a model takes a data sample as input and learns to transform this data into a form that is easier to solve the given task.\n\nThe next layer in the model takes the previous layer\u2019s output as its input, and learns to transform this data into a form that\u2019s even easier to solve the given task! As the data flows through the model\u2019s layers, it continues to be transformed in this way.\n\nWhy is this concept of building hierarchical representations so important?\n\nFor the electrical and computer engineers, imagine building a computer using only transistors directly \ud83e\udd14. Instead:\n\nFor the programmers, imagine building Soylent\u2019s website \ud83d\ude02\ud83e\udd17 in machine code (not even assembly), and remember you have no operating system or anything to build on. Not even a text editor to code in. Again, \ud83e\udd14. The software industry is also enabled by hierarchical art.\n\nFor the art majors \ud83d\ude09, here is a cool \ud83d\ude0e, MRI-like, visualization of (the default base model uses in its skin cancer diagnosis model.\n\nHow do we train the model to learn these hierarchical representations (or in our knob analogy, the correct position of each of the many knobs responsible for transforming our input to output)? The back-bone of training any deep learning model is back-propagation and gradient descent.\n\nThis training is composed of 3 steps:\n\nRepeat this process for all the data in our training set over and over again (AKA gradient descent) until learning plateaus (at this point the model will start to memorize the training data set instead of learning general features).\n\nThere are various types of neural networks that are used to accomplish specific tasks in deep learning/AI. These architectures arise because different data has different characteristics (i.e. images are organized spatially, sound is organized temporally, etc\u2026). We can take advantage of our data\u2019s characteristics and modify our neural network\u2019s architecture to make it easier to train! The important thing to realize is that these architectures are just variations of the standard artificial neural network.\n\nA convolutional neural network makes it much easier to train a model for computer vision tasks such as in this application. The underlying assumption is that features in the data set are spatially invariant (i.e. an object in the upper right hand corner of an image and the same object in the lower left hand corner is still represented by the exact same data). Allowing us to share weights and reduce the dimensionality of our data at each layer in our network. Reducing the parameters the model has to learn (knobs) by orders-of-magnitude (which means much less training data is required, etc\u2026) with no trade-offs in practice. When it comes to computer vision, convolutional networks are the right tool for the job \ud83d\udd28, and we will be using these to diagnose skin cancer!\n\nThere are other important fundamental concepts in deep learning like activations (non-linearities), error/objective functions (briefly mentioned above), optimizers, regularization, etc\u2026 and higher level concepts like transfer learning, adversarial learning, etc\u2026 The important thing is that we continue to build up our intuition behind deep learning and learn these concepts as we go.\n\nThe following materials have been helpful to me and I\u2019m sharing these in particular because they are very concise:\n\nWe\u2019ll be using Python as it\u2019s the language of choice for deep learning. We\u2019ll need to choose a deep learning framework to work with and I\u2019ll review that below. Deep learning has been an extremely collaborative field with the major players publishing research and open sourcing much of their software.\n\nThe frameworks I want to mention are:\n\nMost of open sourced code is written on top of Keras. Keras is the framework I would recommend to anyone getting started with deep learning. TensorFlow is difficult to use. Starting with Keras will provide the listed above and help you learn to use TensorFlow correctly and to leverage its features (putting you in a great position to migrate to direct usage of TensorFlow in the future if necessary \u2014 see Keras\u2019s listed above). Note: Keras is officially set to be merged into TensorFlow.\n\nThe above repo is a simple, solid and general starting point for image classification tasks. It is built on some simple but powerful concepts:\n\nThe code is well documented, read through it! As you come across unfamiliar concepts take some time to learn and understand them.\n\nIt\u2019s time to get going \ud83c\udfc3! The repo\u2019s README has detailed instructions on starting training! Feel free to create issues, make pull requests and get involved with this effort!\n\nThe best way to get involved with this effort is to start contributing on GitHub and to join this project\u2019s Slack channel:\n\nPlease share any interesting results/research you do on this project the best way you see fit (i.e. a GitHub contribution, a blog post, updates in Slack, etc\u2026).\n\nIf you want to get involved in other ways (i.e. contribute data, sponsor this project, help validate our algorithms, or whatever) please get in contact with me!\n\nWaya.ai is a company whose vision is a world where medical conditions are addressed early on, in their infancy. This approach will shift the health-care industry from a constant fire-fight against symptoms to a preventative approach where root causes are addressed and fixed. Our first step to realize this vision is easy, accurate and available diagnosis. Our current focus is concussion diagnosis, recovery tracking and brain health monitoring. Please get in contact with me if this resonates with you!"
    },
    {
        "url": "https://blog.waya.ai/waya-ai-uiucs-sigai-partner-to-release-free-open-source-skin-cancer-diagnosis-software-534a2f04f137?source=user_profile---------8----------------",
        "title": "Waya.ai & UIUC\u2019s SIGAI partner to release free & open source skin cancer diagnosis software",
        "text": "The first step Waya.ai is taking on its journey to fix healthcare is easy, accurate and available diagnosis.\n\nToday Waya.ai is announcing a semester long partnership with UIUC\u2019s special interest group on artificial intelligence (SIGAI) to build upon the work Waya.ai has done in skin cancer diagnosis:\n\nWaya.ai will open source the core components of its skin cancer diagnosis software and the work we do over the next few months will live here:\n\nOur current approach to skin cancer diagnosis uses deep learning to classify images as either benign or malignant. Our R&D has been mostly based around convolutional neural networks and generative adversarial networks (GANs).\n\nExperience in areas other than machine learning/deep learning are also needed for this project to be successful. Specifically:\n\nIf you have experience in any of the above fields please consider joining this effort! Lack of experience can be more than made up for with hard work and passion \u2014 you are still welcome to join!\n\nCommunication for the project will happen in Slack (you must have an email address or request an invite to join):\n\nWe will meet weekly at SIGAI\u2019s project meetings: Sundays 1\u20132pm, Siebel room 1109. http://sigai.ml/\n\nI\u2019ll also hold office hours at various times and locations (announced on the fly in Slack channel) over the course of the week. This will be a good opportunity to coordinate with me, get help, or just do some work together.\n\nThis project will be completely open source and Waya.ai does not intend to profit directly from it. Waya.ai is doing this in hopes that:\n\nWhile not directly profiting from this project, Waya.ai does see it as an opportunity to build credibility in the healthcare field as well as strengthen our brand."
    },
    {
        "url": "https://blog.waya.ai/waya-ai-at-uiucs-startup-career-fair-a-little-about-us-bba7f49f2b38?source=user_profile---------9----------------",
        "title": "Waya.ai at UIUC\u2019s startup career fair \u2014 a little about us",
        "text": "Come see us (me) at the Illini Union this Thursday (Feb 9th), 2pm-6pm.\n\nOne of my goals for Waya.ai is to keep the company lean. A small group of rock solid people with a common vision: a world where medical conditions are addressed early on, in their infancy. This approach will shift the health-care industry from a constant fire-fight against symptoms to a preventative approach where root causes are addressed and fixed.\n\nI\u2019m in this for the long run. When I started my first company (concussion detecting mouthguard) I had a 3 year time-frame in my head. This time I\u2019m thinking 10\u201320 years. It\u2019s a huge problem and one that I\u2019m passionate about and honestly there\u2019s nothing I\u2019d rather be doing. I generally think technology is changing the world faster than humans (and nature) can adapt to. I won\u2019t go into this here\u2026 let\u2019s just leave it at: fixing healthcare is critical.\n\nIt\u2019s a huge vision. The way I see us realizing it is incrementally. Our first step is easy, accurate and available diagnosis.\n\nThis actually started out as a practice project. What I\u2019m really interested in as an initial product is traumatic brain injury (TBI)/concussion diagnosis and recovery tracking/brain health monitoring via eye-tracking using only a smartphone. I won\u2019t go into the details in this post, get in contact with me for more info.\n\nBut the general idea is the same. The smartphone is pretty much an extension of us at this point. And it has three of the five senses (sight, sound and touch \u2014 using sensors like the accelerometer/gyroscope). I don\u2019t think doctors really use taste or smell \ud83d\ude2c. And who knows what kind of hardware we will have on us in 5 years \ud83e\udd14.\n\nThe two most obvious fields we need to develop expertise in is and . But I believe the medical field is broken, expertise in medical isn\u2019t limited to doctors and PhDs in my definition. I also see the need for good software engineering (we won\u2019t get to this vision with a bunch of researcher code). I also see the need for good use of infrastructure and tools (staying lean means using and fully-utilizing the best tools for the job \u2014 SaaS, PaaS, IaaS\u2026 it\u2019s a good thing i guess)\u2026 Not just software either, for example: https://www.zenefits.com/.\n\nOur office situation right now is looking like some combination of Research Park at EnterpriseWorks and working from wherever whenever. But I\u2019m thinking about getting a nice high rise apartment on Green and converting it to an office and pimping it out. That probably depends on what the team looks like in a few months.\n\nLooking forward to meeting some of you! Feel free to get in touch with me if you won\u2019t be there and are interested."
    },
    {
        "url": "https://blog.waya.ai/introduction-to-gans-a-boxing-match-b-w-neural-nets-b4e5319cc935?source=user_profile---------10----------------",
        "title": "On the intuition behind deep learning & GANs \u2014 towards a fundamental understanding",
        "text": "A generative adversarial network (GAN) is composed of two separate networks - the generator and the discriminator. It poses the unsupervised learning problem as a game between the two. In this post we will see why GANs have so much potential, and frame GANs as a boxing match between two opponents.\n\nDeep learning is famously biologically inspired and many of the major concepts in deep learning are intuitive and grounded in reality. The fundamental truth of deep learning is that it\u2019s hierarchical \u2014 the layers in a network and the representations they learn build on each other. This is also the case in reality: electrons, protons, neutrons -> atoms -> molecules -> \u2026 It makes sense that the best way to model a hierarchical world is hierarchically, and this is why deep learning has been so successful in providing simple, elegant, and general solutions to very difficult problems.\n\nNow let\u2019s apply this biologically inspired mind set to the way we currently train our networks. Supervised learning is standard in the current state of machine learning - for each data sample a ground-truth annotation/label is required in training. But unsupervised learning is how most learning is done in the real world. Just think about how we learn to walk, talk, etc\u2026 While supervised learning has performed well on many tasks, unsupervised learning seems to be the key to real artificial intelligence.\n\nIt\u2019s often impractical to accurately annotate data. Ideally an unsupervised model could be trained on data that doesn\u2019t have the required annotations, and then fine-tuned with a much smaller properly annotated dataset. Circling back to the hierarchical view of the world, it should be possible to train AI to understand the world\u2019s basic building blocks, and then build on top of that existing knowledge base, fine-tuning it in a more supervised manner for specific use-cases.\n\nA convolutional neural network is trained on millions of unlabelled images of skin. Some of these images could be of healthy skin, others of diseased skin, and everything in between. Eventually the network would gain a very deep understanding of skin and all its intricacies. A specific use-case (i.e. diagnosing skin cancer instantly and accurately) could then be built on top of this network.\n\nSince the model has already learned general, powerful representations of the most important information contained in images of skin, it should be able to quickly learn the new task of diagnosing skin cancer with a much smaller labelled dataset than if it was trained using only supervised methods. This is the basic concept of transfer learning & fine-tuning.\n\nGANs are one of the most promising areas of research in unsupervised learning and we will see that they are a simple, general approach to learning powerful representations from data.\n\nLet\u2019s break down a GAN into its basic components:\n\nData: Mathematically, we think about a dataset as samples from a true data distribution. This data could be anything: images, speech, sensor readings, etc\u2026\n\nGenerator: Takes some code (i.e. random noise) as input, and transforms it, outputting a sample of data. The goal of the generator is to eventually output diverse data samples from the true data distribution.\n\nDiscriminator: Takes a sample of data as input, and classifies it as real (from the true data distribution) or fake (from the generator). The goal of the discriminator is to be able to discriminate between real and generated images with high precision.\n\nThe overall goal of a standard GAN is to train a generator that generates diverse data samples from the true data distribution, leading to a discriminator that can only classify images as real/generated with a 50/50 guess. In the process of training this network, both the generator and the discriminator learn powerful, hierarchical representations of the underlying data that can then transfer to a variety of specific tasks like classification, segmentation, etc\u2026 and use-cases.\n\nThe pseudo-code below might be confusing at first so we\u2019ll step through it with a simple real-world example of the adversarial learning procedure right after.\n\nWe are all very familiar with the general concept of GANs and adversarial learning whether we realize it or not. For example, consider learning to play a song on guitar:\n\nWe repeat some variation of this procedure, where steps 2 & 3 are pretty much merged together, and step 1 is partially memorized and revisited every once in a while when the memory needs to be refined, until what we are playing sounds close enough to the actual song and we are happy.\n\nAs you become a more skilled guitarist your ability to learn new songs improves until you reach a point where you can play songs you\u2019ve never heard or played before with very little practice (i.e. transfer learning/fine-tuning).\n\nIn this example, the song is the data, our ears/brain is the discriminator, and our hands/brain is the generator. This is probably similar to how we learned to move, talk, etc\u2026 Taking this one step further, think about when a deaf person talks - it sounds funny because they don\u2019t have a discriminator to facilitate the adversarial learning (maybe they can pick up on other cues like people\u2019s reactions which serve as a form of weak discriminator).\n\nNow that we\u2019ve built up some intuition behind GANs, let\u2019s see how they are currently implemented in software. You should think about the similarities & differences between GANs in reality & software along the way. Highlighting one difference, the adversarial learning procedure that occurs in reality seems collaborative between the generator and discriminator, while the software implementation of GANs seems adversarial (\u2026 a boxing match).\n\nAt first it might seem like the discriminator is the coach, and the generator is the boxer. But really they are both boxers. The real data is actually the coach. The thing here is that only the discriminator has direct access to the data.\n\nIn step 1 of the training procedure above, the discriminator is trained for a round on the heavy bag by his coach. The coach critiques his technique and the discriminator adapts. In step 2, the discriminator watches a round of the generator shadowboxing, studying the generator and preparing accordingly for their upcoming round of sparring.\n\nNow step 3, sparring! The generator is a scrappy boxer from Philly who is relaxed & focused when sparring, studying every movement and mistake the discriminator makes and learning from it - adapting after each round. The discriminator hates sparring, and is so scared and nervous every time that he learns absolutely nothing from it. The discriminator may be more athletically gifted and talented than the generator (it\u2019s easier to classify data as real/fake than it is to actually generate realistic data), but the generator\u2019s mindset helps level the playing field. Even though the generator doesn\u2019t have a coach (no access to the real dataset), it learns so much from the discriminator during sparring that it picks up on the fundamental things the discriminator was taught by his coach.\n\nThis process goes on for rounds and rounds until eventually the discriminator and generator are both well-rounded boxers ready to compete. The coach has taught the discriminator every important detail of the game he knows, and the generator and discriminator have learned a lot from each other in their sparring wars. Ideally they are both so equally matched at the end of training that a match between them would have 50/50 odds.\n\nAs you dive deeper into GANs you will see that one of the major difficulties we currently face is training these networks to converge properly - we want the generator and discriminator to reach some desired equilibrium but most of the time this doesn\u2019t happen. There is a lot of information and research out there on what can go wrong: https://www.quora.com/Do-generative-adversarial-networks-always-converge and more and more information out there on how to counteract these problems: https://github.com/soumith/ganhacks.\n\nJust to highlight a few of the most common fail cases of GANs:\n\nNow that we have a fundamental understanding of GANs, let\u2019s revisit their purpose: to learn powerful representations from unlabelled data (i.e. take our data from its original dimension and learn to represent its most important features in a much smaller dimension => less labelled data required to achieve desired performance). After training a GAN, most current methods use the discriminator as a base model for transfer learning and the fine-tuning of a production model, or the generator as a source of data that is used to train a production model. In our boxing analogy this means that the discriminator gets his boxing license and competes but the generator doesn\u2019t. It\u2019s unfortunate because the generator seems like he has the potential to be the better boxer, and he is either completely discarded or only used as a sparring partner/coach for the production model.\n\nA well-trained generator has learned the true data distribution so well that it can generate samples belonging to it from a much smaller dimension of input. This suggests that it has developed extremely powerful representations of the data. It would be ideal to leverage what the generator has learned directly in production models but I don\u2019t know of any methods to do this. If you do please comment.\n\nFor a clean & simple implementation of a standard GAN (as well as other types of GANs like InfoGAN and ACGAN) see:\n\nThere are types of GANs that produce an extremely valuable generator, even if its only a \u2018sparring partner/coach\u2019:\n\nWaya.ai is a company whose vision is a world where medical conditions are addressed early on, in their infancy. This approach will shift the health-care industry from a constant fire-fight against symptoms to a preventative approach where root causes are addressed and fixed. Our first step to realize this vision is easy, accurate and available diagnosis. Our current focus is concussion diagnosis, recovery tracking & brain health monitoring. Please get in contact with me if this resonates with you!"
    },
    {
        "url": "https://blog.waya.ai/simgans-applied-to-autonomous-driving-5a8c6676e36b?source=user_profile---------11----------------",
        "title": "SimGANs - a game changer in unsupervised learning, self driving cars, and more",
        "text": "Apple\u2019s Learning from Simulated and Unsupervised Images through Adversarial Training (S+U Learning) lays down the blueprint for training state-of-the-art neural nets from only synthetic and unlabelled data. In this post we will see why this has huge potential, and apply it to an interesting problem: autonomous driving. We will refer to an implementation of SimGAN: https://github.com/wayaai/SimGAN, and to my 2nd favorite company behind waya.ai: comma.ai, throughout the post.\n\nIf a simple and intuitive introduction to generative adversarial networks (GANs) is needed:\n\nThe core idea behind Apple\u2019s S+U learning is that labelled data is often scarce and expensive. However, synthetic labelled data can be generated using powerful engines like Unity or with other methods. The problem is that the gap between the synthetic and real datasets leads to a network trained on synthetic data not generalizing well enough to the real world. What S+U learning suggests is that with a real (and not necessarily labelled) dataset this gap can be bridged. A generator can learn to refine synthetic data such that it\u2019s closer to the real dataset\u2019s distribution while maintaining the synthetic data\u2019s annotations (i.e. it\u2019s labels remain valid) through adversarial training.\n\nThe following links will provide relevant code snippets for the concepts being explained. The refiner (generator) refines the synthetic data such that:\n\nThe refiner takes as input a synthetic data sample, and outputs a refined sample of the same dimension(s). The discriminator takes as input a data sample and classifies it as refined or real.\n\nThe GAN architecture and training procedure is standard, but S+U Learning suggests two simple and intuitive methods for improving the quality of generated data that applies to GANs in general.\n\nand you have a large amount of real unlabelled driving data collected by Dash. While your current method of labelling data is awesome, it\u2019s not quite cutting it, and you only have a small subset of labelled data. Using a SimGAN, you could train a refiner network to refine synthetic data from Grand Theft Auto\n\nsuch that it looks like it came from the distribution of your real dataset and it\u2019s annotations are preserved. Now you can train your production models on this nearly unlimited refined labelled dataset, and use your smaller real labelled dataset as validation/test.\n\nI haven\u2019t taken any of the self driving car courses out there, but I\u2019ve seen that they use GTA and simulated environments to train their models. With a technique like this, their software could be closer to real world ready.\n\nThere seem to be many possible applications of SimGANs in the real world and autonomous driving is just an interesting one I chose to use as an example.\n\nWaya.ai is a company whose vision is a world where medical conditions are addressed early on, in their infancy. This approach will shift the health-care industry from a constant fire-fight against symptoms to a preventative approach where root causes are addressed and fixed. Our first step to realize this vision is easy, accurate and available diagnosis. Our current focus is concussion diagnosis, recovery tracking & brain health monitoring. Please get in contact with me if this resonates with you!"
    },
    {
        "url": "https://blog.waya.ai/deep-learning-aws-ec2-tmux-3b96777016e2?source=user_profile---------12----------------",
        "title": "tmux + EC2 + deep learning \u2013",
        "text": "If you are training deep neural nets on AWS EC2 instances tmux can make you more efficient. It takes a long time to train these models and since you are connecting to your EC2 instance using your interface is a single shell instance. The naive approach would leave you in a situation where you can\u2019t do anything while your model is training. helps solve this problem.\n\nYou may want to:\n\nis already installed if your instance is running Ubuntu. If running Amazon Linux installing is simple: .\n\nThen before you start training your model run:\n\nand you will enter a session. Once in this session train your model as you normally would, for example: . Now you can leave the session with and you can re-enter the session with . To end the session when you are done or want to stop training your model run: .\n\nThis is a minimal guide to getting started w/ tmux & EC2. If you have any input/tips please comment and share w/ us!\n\nWaya.ai is a company whose vision is a world where medical conditions are addressed early on, in their infancy. This approach will shift the health-care industry from a constant fire-fight against symptoms to a preventative approach where root causes are addressed and fixed. Our first step to realize this vision is easy, accurate and available diagnosis. Please get in contact with me if this resonates with you!"
    }
]