[
    {
        "url": "https://medium.com/octavian-ai/review-prediction-with-neo4j-and-tensorflow-1cd33996632a?source=---------0",
        "title": "Review prediction with Neo4j and TensorFlow \u2013 Octavian \u2013",
        "text": "We show how to create an embedding to predict product reviews, using the TensorFlow machine learning framework and the Neo4j graph database. It achieves 97% validation accuracy.\n\nA common problem in business is product recommendation. Given what a person has liked so far, what should we suggest they purchase next? Just as a waiter asking if you\u2019d like another drink drives higher revenues, so does successful recommendations.\n\nThere are many approaches to recommendation. We\u2019re going to focus on review prediction: given a product a person has not reviewed, what review would they give it? We can then recommend to that person the products we predict they will favorably review.\n\nThe code for the completed system is available in our GitHub.\n\nWe\u2019re going to use a graph database as the data-source for this system. Graph databases are a powerful way to store and analyze data. Often the relationships between things, for example between people, are as important as the properties of those things themselves. In a graph database it\u2019s easy to store and analyze those relationships. In this review prediction system we\u2019ll be analyzing the network of reviews between different people and different products.\n\nWe\u2019ll use Neo4j as our graph database. Neo4j is a popular, fast and free-to-use graph database (we provide a hosted database for this article\u2019s dataset to save you having to set one up for yourself).\n\nFor the machine learning part of this system, we\u2019ll use TensorFlow. TensorFlow is primarily a model building and training framework, letting us express our model and train it on our data. TensorFlow has quickly become one of the most popular and actively developed machine learning libraries.\n\nTensorFlow can save a lot of development time. Once a model has been built in TensorFlow, the same code can be used for experimentation, development and deployment to production. Platforms like Google\u2019s CloudML provide model hosting as a service, serving your model\u2019s predictions as a REST API.\n\nWe\u2019re going to be predicting product reviews. In our world there are people, who write reviews of products. Here\u2019s what this looks like in a graph:\n\nIn a graph database we can query information based on patterns. Neo4j, the database we\u2019ll use here, uses a query language called Cypher. The above graph was generated by a simple query:\n\nThis looks for a node, of label PERSON, with a relationship of label WROTE, to a node of label REVIEW, with a relationship of label OF to a node of label PRODUCT. The qualifier \u201cLIMIT 1\u201d asks the database to just return one instance that matches this pattern.\n\nNeo4j implements a property graph model, in which nodes and relationships can have properties. This is a really flexible model allowing us to conveniently put data where we want.\n\nOur dataset contains 250 people and 50 products. Each person has 40 reviews, giving a total of 10,000 reviews.\n\nYou can use our hosted database, or generate the data into your own Neo4j instance using our generation codebase:\n\nThe dataset is synthetic \u2014 we generated it ourselves from a probabilistic model.\n\nUsing a synthetic dataset is useful technique during model development. If you\u2019re applying an unproven method to unknown data and it fails to train, you cannot tell if the problem is the data or the model. By synthesizing the data one unknown is removed and you can focus on finding a successful model.\n\nA synthetic dataset has limitations: it lacks the irregularities and errors typical of real world data. For this learning exercise, the synthetic dataset is very useful, but any real-world system will require more steps of cleaning the data and experimenting to find a model that fits it.\n\nOur synthetic data generation uses a simple probabilistic model. During generation each product and person has a randomly chosen category, and these categories are used to generate review scores. We save the people, products and reviews to the database and discard the category assignments (it would make the review prediction too easy).\n\nWe generate a set of 250 people and 50 products. Each person reviews 40 randomly chosen products.\n\nEach person and product has a one-hot encoded vector of width 6. Think of this as choosing one category from six choices. For example, each product can be one of six colors (its style). Each person prefers one of those six colors (their preference).\n\nEach review from a person to a product is calculated as the dot product of their vectors, giving 1.0 if they share the same style and preference, or 0.0 otherwise.\n\nFinally, we assign the test property to a randomly selected 10% of the reviews. This data is used for evaluating the model, and is not used for training it.\n\nSince each person reviews 40 randomly chosen products, it\u2019s highly likely (although not certain) they will review one product of each of the six styles \u2014 therefore our review prediction challenge is well-constrained and we should be able to get close to 100%.\n\nIn academic literature our problem is known as \u201ccollaborative filtering\u201d. By combining the reviews of many people (\u2018collaborative\u2019) we can better recommend products for one person (\u2018filtering\u2019).\n\nWe\u2019re going to solve this review prediction problem by estimating a style vector for each product and a preference vector for each person. We\u2019ll predict review scores by taking the dot product of those two vectors (since we know the data was generated using the dot product, it\u2019s easy for us to guess this might be a successful solution).\n\nThe input to our model is the ID of the person and the ID of the product. The output of the model is the review score.\n\nReview prediction is an interesting problem because we do not know the style of each product, nor the preference of each person, therefore we have to determine both simultaneously. A mistake in predicting a product\u2019s style will then cause mistakes in predicting people\u2019s preferences, so solving this is not trivial.\n\nIt should be noted that this is not \u201cdeep learning\u201d (though it is machine learning). We\u2019re using TensorFlow as a convenient framework to train a shallow model via gradient descent.\n\nAs an aside, adding deep layers to the model we defined above has been reported as successful in some academic papers.\n\nThe first step in our model is to transform person IDs and product IDs into estimated preference and style tensors. Thankfully, this is quite straightforward in TensorFlow.\n\nWe\u2019ll store the preference and style estimations for all of the people and products as two variables, of shape [number_of_ids, width_of_tensor]:\n\nWe can use to transform an ID into a tensor of shape [width_of_tensor]\n\nEach embedding tensor will be floating point with 20 dimensions. Unlike in the data generation, there is no restriction for the value to be one-hot encoded.\n\nTogether, this design allows the model a lot of room to maneuver during training \u2014 this is helpful as gradient descent updates the variables with many small steps, and if it had to make a \u201cbig leap\u201d to get to successful variables it may never get there. This design was determined through experimentation and grid search.\n\nThe model for our prediction is just eight lines long:\n\nThe next step is to wrap up this model in the other pieces needed to train it. We\u2019ll use the high-level Estimator API as it has pre-built routines for training, evaluating and serving the model, which we\u2019d otherwise have to re-write.\n\nThe core of the Estimator framework is a model function.\n\nThis is a function we write and hand to TensorFlow, so that the framework can instantiate our model as often as it needs to (for instance, it might run multiple models across different GPUs/machines, or it might re-run the model with different learning rates to determine the best).\n\nThe model function is a python function that takes the input feature tensors (and some other parameters) and returns an which contains a few things:\n\nFor measuring loss we\u2019ll use the built-in mean squared error:\n\nAnd for training operation we\u2019ll use the built in Adam optimizer to minimize the loss:\n\nAnd we\u2019ll measure one evaluation metrics, the accuracy:\n\nYou can see the code all together in model.py.\n\nWe\u2019ll use a Cypher query to get the data from our graph database and format it for training:\n\nThis returns one row for each review in our database. We then format each row for TensorFlow as a tuple of (input_dict, expected_output_score):\n\nNext, we construct a TensorFlow Dataset. This is high-level TensorFlow API that allows the framework to do a lot of the hard-work transforming and distributing our data for training. We\u2019ll use the API to create a dataset from our generator, shuffle the data and batch it:\n\nShuffling helps the network learn as it will encounter different combinations of people and products in each batch.\n\nSimilar to the model function we created earlier, we will now create an input function. TensorFlow will construct a dataset many times during training (for example, when it reaches the end of the data and wishes to restart) and the input function gives it the ability to do so.\n\nWe create an for TensorFlow that requires no arguments:\n\nNow that we have our and our we\u2019re ready to train! We\u2019re going to use the method of the Estimator API to coordinate the training and evaluation for us.\n\nWe construct an Estimator, specify the training data and number of steps in a and specify the evaluation data in an :\n\nWe specify so that the whole evaluation set will be used (instead of just the first 100 items).\n\nNow we\u2019re ready to go, we can run the whole training and evaluation:"
    },
    {
        "url": "https://medium.com/octavian-ai/which-optimizer-and-learning-rate-should-i-use-for-deep-learning-5acb418f9b2?source=---------1",
        "title": "How to pick the best learning rate for your machine learning project",
        "text": "How to pick the best learning rate for your machine learning project A common problem we all face when working on deep learning projects is choosing a learning rate and optimizer (the hyper-parameters). If you\u2019re like me, you find yourself guessing an optimizer and learning rate, then checking if they work (and we\u2019re not alone). To better understand the affect of optimizer and learning rate choice, I trained the same model 500 times. The results show that the right hyper-parameters are crucial to training success, yet can be hard to find. Finally, I\u2019ll discuss solutions to this problem, using automated methods to choose optimal hyper-parameters. I trained the basic convolutional neural network from TensorFlow\u2019s tutorial series, which learns to recognize MNIST digits. This is a reasonably small network, with two convolutional layers and two dense layers, a total of roughly 3,400 weights to train. The same random seed is used for each training. It should be noted that the results below are for one specific model and dataset. The ideal hyper-parameters for other models and datasets will differ. (If you\u2019d like to donate some GPU time to run a larger version of this experiment on CIFAR-10, please get in touch). The first thing we\u2019ll explore is how learning rate affects model training. In each run the same model is trained from scratch, varying only the optimizer and learning rate. The model was trained with 6 different optimizers: Gradient Descent, Adam, Adagrad, Adadelta, RMS Prop and Momentum. For each optimizer it was trained with 48 different learning rates, from 0.000001 to 100 at logarithmic intervals. In each run, the network is trained until it achieves at least 97% train accuracy. The maximum time allowed was 120 seconds. The experiments were run on an Nvidia Tesla K80, hosted by FloydHub. The source code is available for download. Here is the training time for each choice of learning rate and optimizer:\n\nThe above graph is interesting. We can see that: For every optimizer, the majority of learning rates fail to train the model. There is a valley shape for each optimizer: too low a learning rate never progresses, too high a learning rate causes instability and never converges. In between there is a band of \u201cjust right\u201d learning rates that successfully train. There is no learning rate that works for all optimizers. Learning rate can affect training time by an order of magnitude. Summarizing the above, it\u2019s crucial you choose the correct learning rate as otherwise your network will either fail to train, or take much longer to converge. To illustrate how each optimizer differs in its optimal learning rate, here is the the fastest and slowest model to train for each learning rate, across all optimizers. Notice that the maximum time is 120s (e.g. network failed to train) across the whole graph \u2014 there is no single learning rate that works for every optimizers:\n\nAnother observation on the above graph is the wide range of learning rates (from 0.001 to 30) that achieve success with at least one optimizer. Now that we\u2019ve identified the best learning rates for each optimizer, let\u2019s compare the performance of each optimizer training with the best learning rate found for it in the previous section. Here is the validation accuracy of each optimizer over time. This lets us observe how quickly, accurately and stably each performs:\n\nAll of the optimizers, apart from RMSProp (see final point), manage to converge in a reasonable time. Adam is more stable than the other optimizers, it doesn\u2019t suffer any major decreases in accuracy. RMSProp was run with the default arguments from TensorFlow (decay rate 0.9, epsilon 1e-10, momentum 0.0) and it could be the case these do not work well for this task. This is a good use case for automated hyper-parameter search (see the last section for more about that). Adam also had a relatively wide range of successful learning rates in the previous experiment. Overall, Adam is the best choice of our six optimizers for this model and dataset. Now lets look at how the size of the model affects how it trains. We\u2019ll vary the model size by a linear factor. That factor will linearly scale the number of convolutional filters and the width of the first dense layer, thus approximately linearly scaling the total number of weights in the model. There are two aspects we\u2019ll investigate: How does the training time change as the model grows, for a fixed optimizer and training rate? Which learning rate trains fastest on each size of model, for a fixed optimizer? How does training time change as the model grows? Below shows the time taken to achieve 96% training accuracy on the model, increasing its size from 1x to 10x. We\u2019ve used one of our most successful hyper-parameters from earlier: Red line is the data, grey dotted line is a linear trend-line, for comparison The time to train grows linearly with the model size. The same learning rate successfully trains the network across all model sizes. This is a nice result. Our choice of hyper-parameters was not invalidated by linearly scaling the model. This may hint that hyper-parameter search can be performed on a scaled-down version of a network, to save on computation time. This also shows that as the network gets bigger it doesn\u2019t incur any O(n\u00b2) work in converging the model (the linear growth in time can be explained by the extra operations incurred for each weight\u2019s training). This result is further reassuring, as it shows our deep learning framework (here TensorFlow) is scales efficiently. Which learning rate performs best for different sizes of model? Let\u2019s run the same experiment for multiple learning rates and see how training time responds to model size:\n\nLearning rates 0.0005, 0.001, 0.00146 performed best \u2014 these also performed best in the first experiment. We see here the same \u201csweet spot\u201d band as in the first experiment. Each learning rate\u2019s time to train grows linearly with model size. Learning rate performance did not depend on model size, the same rates that performed best for 1x size performed best for 10x size. Above 0.001, increasing the learning rate increased the time to train and also increased the variance in training time (as compared to a linear function of model size). Time to train can roughly be modeled as c + kn for a model with n weights, fixed cost c and learning constant k=f(learning rate). In summary, the best performing learning rate for size 1x was also the best learning rate for size 10x. As the earlier results show, it\u2019s crucial for model training to have an good choice of optimizer and learning rate. Manually choosing these hyper-parameters is time-consuming and error-prone. As your model changes, the previous choice of hyper-parameters may no longer be ideal. It is impractical to continually perform new searches by hand. There are a number of ways to automatically pick hyper-parameters. I\u2019ll outline a couple of different approaches here. Grid search is what we performed in the first experiment \u2014for each hyper-parameter, create a list of possible values. Then for each combination of possible hyper-parameter values, train the network and measure how it performs. The best hyper-parameters are those that give the best observed performance. Grid search is very easy to implement and understand. It\u2019s also easy to verify that you\u2019ve searched a sufficiently broad section of the parameter search. It\u2019s very popular in research because of these reasons. Population based training (DeepMind) is an elegant implementation of using a genetic algorithm for hyper-parameter choice. In PBT, a population of models are created. They are all continuously trained in parallel. When any member of the population has had sufficiently long to train to show improvement, its validation accuracy is compared to the rest of the population. If its performance is in the lowest 20%, then it copies and mutates the hyper-parameters and variables of one of the top 20% performers. In this way, the most successful hyper-parameters spawn many slightly mutated variants of themselves and the best hyper-parameters are likely discovered. Thanks for reading this investigation into learning rates. I began these experiments out of my own curiosity and frustration around hyper-parameter turning, and I hope you enjoy the results and conclusions as much as I have. If there is a particular topic or extension you\u2019re interested in seeing, let me know. Also, if you\u2019re interested in donating some GPU time to run a much bigger version of this experiment, I\u2019d love to talk."
    },
    {
        "url": "https://medium.com/octavian-ai/an-introduction-to-machine-learning-on-graph-databases-24ee502fd12e?source=---------2",
        "title": "An introduction to using Keras with Neo4j \u2013 Octavian \u2013",
        "text": "Graph databases are a powerful way to store and analyse data. Often the relationships between things, for example between people, are as important as the properties of those things themselves. In a graph database it\u2019s easy to store and analyse those relationships.\n\nThanks to their focus on relationships, graph databases enable many sorts of algorithms that are difficult to perform on a traditional SQL database. For example, finding the shortest path along roads between cities can be done trivially on a graph database but may be impossible on a SQL database.\n\nMachine learning (\u2018ML\u2019) is a powerful technology that\u2019s being rapidly adopted by technology teams around the world. It\u2019s able to solve problems that are very difficult to solve with hand-written code; for example, it was possible with hand-written code to beat a grand-master in chess in 1997, but it required the advent of machine learning for a computer to beat a Go professional in 2015.\n\nBuilding a graph-native ML system (\u2018graph ML\u2019) has numerous benefits. Firstly, it allows the learning system to explore more of your data. Traditional learning systems train on a single table prepared by the researcher, whereas a graph-native system can access more than just that table. Secondly, graph ML can analyse the relationships between entities as well as their properties. This brings an additional dimension of information that graph ML can harness for prediction and categorisation.\n\nThere are not many resources on how to build graph-native machine learning systems. With Andrew Jefferson I\u2019ve been researching different approaches. Over the upcoming months we\u2019ll be sharing what we\u2019ve found.\n\nIn this article I\u2019ll demonstrate a very basic graph ML system, that can solve a simple prediction challenge. Whilst this example barely scratches the surface of what is possible, it\u2019s a good launchpad to introduce the technologies involved.\n\nWe\u2019re going to be looking at product reviews. In our world there are people, who write reviews of products. Here\u2019s what this looks like in a graph:\n\nIn a graph database we can query information based on patterns. Neo4j, the database we\u2019ll use here, uses a query language called Cypher. The above graph was generated by a simple query:\n\nThis looks for a node, of label PERSON, with a relationship of label WROTE, to a node of label REVIEW, with a relationship of label OF to a node of label PRODUCT. The qualifier \u201cLIMIT 1\u201d asks the database to just return one instance that matches this pattern.\n\nNeo4j implements a property graph model, in which nodes and relationships can have properties. This is a really flexible model allowing us to conveniently put data where we want.\n\nIn our example graph, reviews have a score property between 0 and 1:\n\nWe\u2019ve generated a very simple product review graph. It\u2019s a synthetic dataset, generated from a probabilistic model we\u2019ve designed for the purpose of learning about graph ML.\n\nThe challenge is to predict what review score a person will give to a product.\n\nIn our graph we have the following nodes with properties:\n\nNote on and : These vectors are provided as one-hot encodings to keep the machine-learning model very simple. In a more realistic graph database these might be encoded as relationships.\n\nReview are calculated as the dot product between a person\u2019s and a product\u2019s\n\nFor example, Jane with [0,1,0,0,0,0] reviews product Nintendo Switch with [0,1,0,0,0,0], and her review is 1.0. When she reviews a PS4, with [1,0,0,0,0,0], her review is 0.0.\n\nYou can download the code for this article from our public github:\n\nWe\u2019ve written this system in Python. Python is a popular choice amongst data-scientists and AI researchers because it has many useful data analysis libraries including Tensorflow, SciPy and Numpy. We\u2019re going to use Keras for machine learning because it makes it very easy to write and train our simple network.\n\nWe\u2019ve hosted a Neo4j database for you with the dataset already loaded. When you run it will use our hosted database by default.\n\nIf you want to host the data locally, install Neo4j, and use our generate-data repository to generate dataset then update settings.json to point at your database.\n\nYou can inspect the data for yourself. Open the Neo4j browser with this username and password , then try the query :"
    },
    {
        "url": "https://medium.com/octavian-ai/learning-multiple-tasks-with-gradient-descent-f1fce477cb7a?source=---------3",
        "title": "Learning multiple tasks with gradient descent \u2013 Octavian \u2013",
        "text": "It\u2019s often remarked about how machine learning struggles to simultaneously learn multiple tasks (\u201ccontinual learning\u201d):\n\nI\u2019m curious to see these problems first-hand, so in this blog post I\u2019m going to run a series of multi-task learning experiments to demonstrate the limits and properties of today\u2019s machine learning.\n\nI\u2019m using a very simple network for these experiments:\n\nThe Tensorflow code used to generate these graphs and experiments is available as a FloydHub project and a github you can run yourself.\n\nFirst of all, let\u2019s look at how this network performs on two simple tasks:\n\nMNIST is a popular toy dataset of hand-written digits. There are 60,000 labelled training images and 10,000 labelled test images. Given its popularity and familiarity, it\u2019s a good task for this investigation."
    }
]