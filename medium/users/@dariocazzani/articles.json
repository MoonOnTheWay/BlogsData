[
    {
        "url": "https://towardsdatascience.com/generating-digits-and-sounds-with-artificial-neural-nets-ca1270d8445f?source=user_profile---------1----------------",
        "title": "Generating Digits and Sounds with Artificial Neural Nets",
        "text": "At Cisco Emerge we like to experiment with old and new tools and ideas. Recently we started tinkering with Generative Models.\n\nCheck out the source code for these experiments on GitHub\n\nGenerative Models are Artificial Neural Networks that are able to create \u201cfake\u201d data that no human has ever seen before, with the goal of making it indistinguishable from real data.\n\nHere is an example of what a particular set of Generative Models - called Generative Adversarial Networks - are able to create after having observed hundreds of thousands of images of faces:\n\nIt is as if the Neural Networks learned to create an alternate reality about the world that they have observed.\n\nA particular set of Generative Models are called Variational Autoencoders.\n\nApart from the fancy name, the concept is pretty simple, and the following figure can help explain what we can achieve with them.\n\nDuring training the Encoder tries to compress the input in the \u201cleast lossy\u201d way possible, while the Decoder tries to match the original input as best as possible given only the latent vector as input.\n\nWhat happens if we feed corrupted inputs in the Encoder and we ask the Decoder to reconstruct an uncorrupted input?\n\nSomething quite magical happens: the output of the Decoder is a complete image and the whole digit is drawn with precision.\n\nThe digits are clearly smoother and a bit fuzzier than the originals because the information about the \u201cimperfections\u201d is not carried through the Network. Only the information that makes a digit a digit, remains.\n\nThis all means that:\n\nArtificial Neural Networks are very versatile: one can perform object recognition, speech generation, natural language processing, etc\u2026\n\nFor this reason we kept the same Neural Net architecture and tried the same experiments with sounds.\n\nInstead of using 10 different images of digits, we used 7 musical notes \u2014 a C major scale. More precisely they are sinusoids (so they sound more or less like a flute) with added white noise (so they sound like a flute where the player uses a lot of air).\n\nSimilarly to our previous experiment, we fed the Encoder chopped inputs.\n\nThis is how the corrupted input sounds. Keep in mind that the Neural Net has been trained with a short input with only one portion missing, while the complete sound is a longer input with many random missing parts.\n\nThe reconstructed input sounds like this:\n\nYou can find all the source code that we used to run these experiments and produce these results in this GitHub repo, so that you can try yourself to replicate them for both digits, sounds and perhaps more complex inputs. Feel free to leave comments and fork the repo.\n\nYou will not need any particular dataset or hardware, just a computer and TensorFlow installed (the MNIST dataset comes with TensorFlow).\n\nIf you get any interesting results, please share them in the comments.\n\nYou might want to check out this project by Google: Generate your own sounds with NSynth.\n\nSimilar to the experiments we just showed, they built neural networks capable of learning and directly generating raw audio samples.\n\nAs mentioned above with Google Magenta\u2019s project, there already exist efforts to generate audio with Artificial Neural Networks, and the results they achieve are impressive.\n\nWe tried to keep this experiment as simple as possible so that anyone, without the need for huge datasets and expensive machines with GPUs, could try it out and get an insight into how Variational Autoencoders work for both images and raw sounds.\n\nAt Cisco Emerge, we are using the latest machine learning technologies to advance the future of work.\n\nFind out more on our website."
    },
    {
        "url": "https://towardsdatascience.com/audio-processing-in-tensorflow-208f1a4103aa?source=user_profile---------2----------------",
        "title": "Audio processing in TensorFlow \u2013",
        "text": "There are countless ways to perform audio processing. The usual flow for running experiments with Artificial Neural Networks in TensorFlow with audio inputs is to first preprocess the audio, then feed it to the Neural Net.\n\nWhat happens though when one wants to perform audio processing somewhere in the middle of the computation graph?\n\nTensorFlow comes with an implementation of the Fast Fourier Transform, but it is not enough.\n\nIn this post we will explain how we implemented it and provide the code so that the Short Time Fourier Transform can be used anywhere in the computation graph.\n\nWhen developing a Speech Recognition engine using Deep Neural Networks we need to feed the audio to our Neural Network, but\u2026 what is the right way to preprocess this input?\n\nThere are 2 common ways to represent sound:\n\nDespite the fact that Deep Neural Networks are extremely good at learning features automagically, it is always a good idea to rely on known features that carry the information needed for the task that we are trying to solve.\n\nFor most application, a Speech Recognition Engine included, the features we are interested in are encoded in the frequency domain representation of the sound.\n\nA spectrogram shows how the frequency content of a signal changes over time and can be calculated from the time domain signal.\n\nThe operation, or transformation, used to do that is known as the Short Time Fourier Transform.\n\nWe could let the Neural Network figure out how to learn this operation, but it turns out to be quite complex to learn with 1 hidden layer. (refer to the Universal approximation theorem)\n\nWe could add more layers, but we want to keep the complexity of the Neural Networks as small as possible and learn features only where it is most needed.\n\nWe have used the example of developing an Automatic Speech Recognition engine, but the use of the spectrogram as input to Deep Neural Nets is common also for similar tasks involving non-speech audio like noise reduction, music genre classification, whale call detection, etc.\n\nA particular project that we want to mention is Magenta, from the Google Brain team, who\u2019s aim is to advance the state of the art in machine intelligence for music and art generation.\n\nWe mainly use TensorFlow when implementing Artificial Neural Networks and, because we haven\u2019t found an implementation of the Short Time Fourier Transform in TF, we decided to implement our own.\n\nThere can also be multiple reasons why a deep learning practitioner might want to include the Short Time Fourier Transform (STFT for my friends) in the computation graph, and not just as a separate preprocessing step.\n\nKeep in mind that we haven\u2019t focused on making this efficient. It should (and will) be improved before being used in production.\n\nIn order to understand how the STFT is calculated, we need to understand how to compute the Discrete Fourier Transform.\n\nThis part can appear quite technical for those who are not familiar with these concepts, but we think it is important to go through some maths in order give a complete understanding of the code.\n\nTheory\n\nFourier analysis is fundamentally a method for expressing a function as a sum of periodic components, and for recovering the function from those components. When both the function and its Fourier transform are replaced with discretized counterparts, it is called the discrete Fourier transform (DFT).\n\nGiven a vector x of n input amplitudes such as:\n\nThe DFT is defined by this equation:\n\nFast Fourier Transform\n\nThe Fast Fourier Transform is an efficient implementation of the DFT equation. The signal must be restricted to be of size of a power of 2.\n\nThis explains why we want N (the size of the signal in input to the DFT function) to be power of 2 and why it must be zero-padded otherwise.\n\nWe can detect whether x is a power of 2 very simply in python:\n\nWe only need half of it\n\nReal sine waves can be expressed as the sum of complex sine waves using Euler\u2019s identity\n\nBecause the DFT is a linear function, the DFT of a sum of sine waves is the sum of the DFT of each sine wave. So for our spectral case, we get 2 DFTs, one for the positive frequencies and one for the negative frequencies, which are symmetric.\n\nThis symmetry occurs for real signals that can be viewed as an infinite (or finite in our case) sum of sine waves.\n\nWindowing\n\nTruncating a signal in the time domain will lead to ripples appearing in the frequency domain.\n\nThis can be understood if we think of truncating the signal as if we applied a rectangular window. Applying a window in the time domain results in a convolution in the frequency domain.\n\nThe ripples are caused when we convolve the 2 frequency domain representations together.\n\nFind out more about spectral_leakage if you\u2019re interested.\n\nHere is an example of an implementation of windowing in Python:\n\nIn order to use the FFT, we need to have the input signal to have a power of 2 length. If the input signal does not have the right length, we have to append zeros to the signal itself both at the beginning and at the end.\n\nBecause the zero sample is originally at the center of the input signal, we have to split the padded signal through the middle and swap the order of these 2 parts.\n\nThe next code snippet shows how to do this in TensorFlow for a batch of inputs:\n\nFFT, Magnitude and Phase\n\nWe now have everything we need to calculate the magnitude of the spectrogram in decibels and the phase of the signal:\n\nWe now know how to compute the DFT to evaluate the frequency content of a signal.\n\nThe STFT is used to analyze the frequency content of signals when that frequency content varies with time.\n\nWe can do this by:\n\nWe get DFT coefficients as a function of both time and frequency.\n\nThe complete code is divided in 2 parts: helpers.py and stft.py.\n\nThe possibility of doing the STFT in TensorFlow allows Machine Learning practitioners to perform the transformation of a signal, from time-domain to frequency domain, anywhere in the computation graph.\n\nNew tools always bring new ideas and we hope this post will be the source of new ideas for developing new Deep Learning solutions.\n\nAt Cisco Emerge, we are using the latest machine learning technologies to advance the future of work.\n\nFind out more on our website."
    },
    {
        "url": "https://medium.com/cisco-emerge/how-we-built-a-smart-voice-activity-detection-system-using-adaptive-custom-language-models-d9536331f07a?source=user_profile---------3----------------",
        "title": "How We Built a Smart Voice Activity Detection System Using Adaptive Custom Language Models",
        "text": "Digital assistants, bots, and devices configured to interface with your voice need to know when a command has been spoken in its entirety in order to process the command and provide proper feedback to the user.\n\nVoice Activity Detection (VAD) systems determine when a user has finished speaking. However, VAD systems are oblivious to the actual words being spoken and instead base their determination on recognizing whether the analyzed sound is speech (as opposed to non-speech noise).\n\nFormally, but without any math, we can define a language model as a function that determines the probability of a sequence of words.\n\nWithout knowing what a function is, picture a Language Model as a black box that takes any sequence of words as input and provides a number that describes how likely this particular sequence can happen during a conversation in English.\n\nIt is possible to use a language model to determine whether a given sentence is complete or not.\n\nAs an example, Sentence 1 is a complete sentence, while Sentence 2 is not:\n\nConventional techniques take advantage of the language model used in speech-to-text engines to detect whether the spoken utterance is a complete sentence.\n\nThe amount of time required to wait for non-speech (VAD_PATIENCE), before processing the utterance, is shortened or lengthened accordingly. However, these techniques do not take into account that different users have different ways of issuing commands, and above all it does not adapt to a user\u2019s common sentences / way of speaking.\n\nWhen building a voice interface it is possible to use third party speech-to-text engines. However, this solution does not enable a smart VAD that can adjust the VAD_PATIENCE because the language model used in the third party speech-to-text is not customizable/modifiable.\n\nThe solution we\u2019ve come up with is to include the customizable language model in the client so that every time the user issues commands the language model may be shaped around the domain of commands to which the voice activated device needs to respond.\n\nState-of-the-art language models use recurrent neural networks, while more portable solutions are based on n-grams and Markov models (e.g., the text predictor in mobile phones). Our portable solution permits the voice client to carry the adaptation locally, while the recurrent neural network may require the adaptation to run as a separate service.\n\nIt is possible to implement a smart and adaptive VAD even when using third party speech-to-text services that do not allow language model customization. From a user experience point of view, the dialogue between the voice activated device becomes faster and more user friendly. The time for processing the commands issued by the user is shorter and the whole experience improves as the device is used and learns which commands are issued.\n\nAt Cisco Emerge, we are using the latest machine learning technologies to advance the future of work.\n\nFind out more on our website."
    }
]