[
    {
        "url": "https://medium.com/@gitlab/gitlabs-global-compensation-calculator-the-next-iteration-d06b2395b4e4?source=user_profile---------1----------------",
        "title": "GitLab\u2019s Global Compensation Calculator: The next iteration",
        "text": "We know many of you have thoughts about our Compensation Calculator! We see your comments on Hacker News; we are listening and continually working on improving it. In line with our value of iteration, we have made additional changes to our Compensation Calculator. In January 2018, we released a new version to align the calculator closer to market rates, and adjust all current team members\u2019 pay to be in line with the outputs of the iterated version. Here\u2019s how it works.\n\nThis is the employee salary at the 50th percentile for the role in San Francisco (SF), which we determine using various sources of market data including Comptryx.\n\nThis is taken from Numbeo, which expresses the ratio of cost of rent in many metro areas. Since we are using San Francisco benchmarks, we divide by 1.26 to normalize the rent index to San Francisco. A minimum Rent Index of 0.2 is applied so no one is paid less than 41 percent of San Francisco\u2019s market.\n\nWe multiply the Rent Index by 0.7 and then add 0.3, so the sum would equal 1 (i.e. we pay San Francisco rates in San Francisco).\n\nThis is an adjustment to any US-based metro area where the geographical area Rent Index is less than the Hot Market Adjustment plus the Numbeo Rent Index, to recognize that \u201chot markets\u201d tend to have a Rent Index that is trailing (i.e. lower than) what one would expect based on compensation rates in the area.\n\nThis is currently defined as junior (0.8), intermediate (1.0), senior (1.2), staff (1.4), or manager (1.4), and will be defined as II (.8), III (1.0), Senior (1.2), Staff (1.4), or manager (1.4).\n\nThis falls between 0.8\u20131.2 based on our Experience Factor Guidelines:\n\nThis is a ratio of the calculator to market data. We determine this ratio by looking at how our calculator aligns to market in the region. If the calculator comes in higher than market, a factor lower than 1 is applied. If the calculator is in line with market, the factor stays at 1.\n\nThis distinguishes between employee (1) or contractor (1.17). A contractor may carry the costs of their own health insurance, social security taxes, etc, leading to a 17 percent higher compensation for the contractor to account for the extra expenses to these GitLabbers.\n\nThe calculator can be found on each position description. For example, take a look at our Compensation Calculator for Developers.\n\nThe first step in this iteration was to gather market data and incorporate it as the benchmarks for each role. After obtaining a global data set to map to our positions, we needed to decide if New York was still the right city to pivot the benchmarks around. After some analysis, we determined that San Francisco was a better source of data, so we adjusted the formula. We also analyzed and adjusted the parameters around rent index to ensure in San Francisco you make San Francisco\u2019s benchmark.\n\nEarlier in 2017, we instituted a Geographical Areas iteration to the compensation calculator to ensure that there are not large pay differences in regions that have a similar job market. We looked at the rent indexes by region, determined any outliers on the high or low end of the rent index, and set the regional rent index at the highest of the remaining data set. With the January iteration of the compensation calculator, we also set a Minimum Rent Index so no one would be paid less than 41 percent of San Francisco\u2019s market.\n\nWith this iteration of the compensation calculator, we wanted to align our team\u2019s salaries according to market. We first looked at how experienced the team member is in their role by having the manager conduct an Experience Factor Review. This review verified we are paying our team in line with their experience, and not determining their experience to fit compensation. This review generates an output which is applied in the compensation calculator, but is also a great way to start the conversation around growth within each role. Managers and direct reports were able to review the experience factors and have constructive conversations around experience. Once we had all of the calculator inputs, including the up-to-date Experience Factor, our People Ops team reviewed all salaries to match the new compensation calculator. At the same time as the calculator was released, the increases to pay were also communicated.\n\nWe\u2019ll continue to add more countries to our Country Factors list, review adding an additional factor for specialization within Development roles, review how the levels overlap when it comes to promotions, and review the Rent Indexes for countries with many data points (like the United States and United Kingdom).\n\nWe want to continue to make the calculator as reflective of market in as many locations as we can, given possible data constraints. This will go some way towards eliminating pay inequality among underrepresented groups, promote salary transparency on what each team member and candidate\u2019s market value is, and save valuable recruiting time.\n\nWe also want to hear from you on where this calculator can continue to improve! Please let us know what you think in the comments."
    },
    {
        "url": "https://medium.com/@gitlab/how-do-developers-and-managers-feel-about-their-jobs-23e3e809aec7?source=user_profile---------2----------------",
        "title": "How do developers and managers feel about their jobs?",
        "text": "Surveys are tricky, and humans are trickier, so we had to brainstorm a bit on what exactly we were interested in learning, and how we could coax out this information without introducing our own biases. We used a series of likert scales to get at these groups\u2019 perceptions of their autonomy, team dynamics, support, and other fuzzy things that we think can really drive happiness in a role (we also asked about details on tooling and workflow later on in the survey). We\u2019ve published before on what happens when your business and engineering teams are out of sync, and we wanted to ask about other symptoms of that same problem. Here are some of the questions, along with the raw data that we used to compare satisfaction between developers and management.\n\nOne of the goals of our developer survey was to establish a benchmark for how satisfied software professionals generally are in their jobs. Using the detailed demographic information we captured at the beginning, we were able to sort and compare the opinions of different groups within our sample of over 5,000 respondents. One of our key findings was that, for all their differences, developers and managers agree with each other on a lot of things, but managers tend to have a slightly rosier outlook when their views diverge.\n\nWhat are some other things that might contribute to a frustrating or dysfunctional culture? To try to hint at big, sometimes implicit things like psychological safety, bureaucracy, and whether their team is more democratic or autocratic, we had to come up with a list of concrete indicators, which you can see below:\n\nWhen we asked about the biggest challenges teams face when adopting new processes or tools, the top three responses were replacing ingrained practices, resistance to change, and cross-team communication. Developers and managers are in agreement here almost exactly, although developers are slightly more likely to name resistance to chance (51 percent) than managers (46 percent).\n\nWe saw this echoed in other ways, with the greatest number of developers (42 percent) naming unclear direction as their top challenge to getting work done. Relatedly, just 57 percent of developers say they have visibility into what their team members in operations, security, and product are working on. Managers feel slightly better off in this regard, with 69 percent reporting that they have visibility (we also found some differences in how remote versus in-office teams view the issue, which you can read more about here).\n\nCommunication, and structures or habits that might enable or impede it, is a theme that we\u2019re interested in learning more about. It\u2019s a predictable problem with no easy fix, so we ran a Twitter poll to get some input on how teams have wrestled with communication issues in the past.\n\nOne suggestion for how to overcome the cultural barriers to adopting DevOps is to embed team members to improve cross-team collaboration, but that doesn\u2019t always seem doable because it\u2019s an organizational change, requiring buy-in from many more people than just the developers involved. It wasn\u2019t surprising, then, that this option was chosen the least. Regular social activities and working sessions seem like much cheaper options, but were barely more popular. The greatest number of people simply chose our equivalent of \u00af\\_(\u30c4)_/\u00af.\n\nWe heard from a few devs about solutions that didn\u2019t make our short list, and they\u2019re rarely about just talking to each other more. Tellingly, the responses we got were much more likely tying communication to big, pervasive cultural things, like compensation incentives and respect for others\u2019 work.\n\nWhen we asked Netflix engineer Randall Koutnik for more details on his tweet (below) he wrote a post with examples of how dev teams can be undermined by policies tying financial incentives and promotion criteria to individual performance goals, rather than company performance.\n\nWhy is this predictable problem so stubborn? What has your team tried? Tweet us @gitlab."
    },
    {
        "url": "https://medium.com/@gitlab/how-the-gitlab-ux-team-uses-epics-b294573266e3?source=user_profile---------3----------------",
        "title": "How the GitLab UX team uses epics \u2013 GitLab \u2013",
        "text": "One of the challenges for UX here at GitLab is how to work iteratively, making the smallest changes possible, while maintaining a holistic view of the application. As the manager for the UX department, I was curious to see how we could use epics to better plan and track UX efforts over time.\n\nThe term \u2018epic\u2019 is most commonly associated with Agile methodology. In Agile, an epic is a collection of user stories that describe a larger user flow, typically consisting of multiple features. So, what does \u2018epic\u2019 mean at GitLab? Here, epics contain a title and description, much like an issue, and allow you to attach multiple child issues to indicate hierarchy. In short, an epic is a feature that allows you to manage a portfolio of projects more efficiently and with less effort by tracking groups of issues that share a theme, across projects and milestones.\n\nWhat this meant for the UX team was that we finally had an efficient way to plan, track, and execute a group of thematically related issues. Take the merge request page for example. We have over 100 issues related to UX improvements for this feature alone! Each issue, taken on its own, represents just one piece of a much bigger picture. Epics would allow us to define the goal we have for the entire page and organize issues specific to that effort.\n\nTo get started with epics, we put together a UX strategy template. This template would be filled out and added to the epic description. The template defined the following:\n\nThe template also includes links to any relevant personas and research we should consider when working toward the overall goal.\n\nWith the template ready to go, we chose the merge request page as our first area of focus. We started by reviewing the existing UX research for this page. It was essential to use data to understand the pain points and opportunities. We also examined the entire backlog of issues related to this page, matching existing issues to the research findings. With the significant pain points identified, we were able to fill out the template and create our very first epic.\n\nWith a holistic view of what we wanted to achieve, we could go back and find issues in the backlog that were critical to the vision. These issues were added to the epic and ordered according to priority. As we discover new information, we can reorder these issues to match the change in priority. As the scope expands, we can aggressively break things out into new epics for development at a later time or parallel to the existing epic. In the future, sub-epics will make this process even more fluid.\n\nIssues are listed under the epic description. They can be easily reordered by dragging and dropping them into place.\n\nWe also set a time frame for this overall effort to be achieved. Having a set timeframe allows us to resource plan with the product team and make adjustments accordingly.\n\nSo far, epics have proven to be well suited for planning long-term UX efforts. It has allowed us to maintain a holistic view of product area while still working iteratively. Epics also give other departments better visibility into what UX considers important. We are already looking beyond the merge request page and using epics to plan other efforts spanning multiple milestones. Epics are still relatively new, and there are many additions yet to come. In future releases, they will support labeling, discussions, project-level epics, and integration with issues and roadmaps.\n\nThe Roadmap feature, pictured above, is set to be released in 10.5. Roadmaps offer a graphical, high level overview of an epic, or multiple epic\u2019s, goals and deliverables presented on a timeline. The blue roadmap bar and the epic list item are clickable and will navigate to that epic\u2019s detail page.\n\nHow the GitLab UX team uses epics via @svesselov Click to tweet!"
    },
    {
        "url": "https://medium.com/@gitlab/how-working-at-gitlab-has-changed-my-view-on-work-and-life-75285a7dda40?source=user_profile---------4----------------",
        "title": "How working at GitLab has changed my view on work and life",
        "text": "I will have been at GitLab for two years in June of this year. Working at GitLab is a fresh experience for me. Joining a company outside of Asia and working 100 percent remotely was not something that I had previously done. It not only affects my work but my entire life. I am so grateful to have the opportunity to work with talented and friendly people around the world. I think it would be good to share my reflections about what I\u2019ve learned during this 19-month journey.\n\nWe have an open source handbook that everyone can access, and it includes our six values, (CREDIT) which support our everyday work. Keeping these values in mind benefits me a lot both in my work and in my life, and I would love to share them with you here:\n\nCollaboration is essential in our everyday work. At GitLab, we prefer asynchronous communication instead of synchronous communication since we are spread around the world, from America, Europe, Africa, to Asia. We rely on text-based communication heavily. However, words are cold without the body language support, and they could easily lead to misunderstanding and conflict. So how we express our thoughts clearly and kindly in text becomes crucial.\n\nAfter joining GitLab, I always think twice before sending out messages or comments, even in my personal life. I started to choose my words more carefully both in English and Chinese. I also have tried to explain as much as possible. I found that if I did these two things, I can avoid the misunderstanding and increase the efficiency of communication. The most important thing is that people feel comfortable while discussing with you in the text. So don\u2019t be afraid to completely express your thoughts, in a careful and sensitive manner.\n\nWe have \u201cSay thanks\u201d in our values, and we often say \u201cThank you\u201d to each other, especially in our \u201cThanks\u201d channel on Slack.\n\nDue to my personality and culture, at first I was shy to express my appreciation to my friends, family, and colleagues. At GitLab, we have a unique culture that encourages people to say \u201cthanks,\u201d so I try not to be too shy to show my gratitude. As I practiced this more and more, it became a habit and a natural thing to me. Now I say \u201cthanks\u201d very often, even for little things, and it feels positive and makes me happy every day.\n\nExpressing gratitude not only makes me feel satisfied, it also makes the person that I expressed my appreciation for have a beautiful mood.\n\n\u201cIteration\u201d is critical to our product improvement and development. We see what each of us produce initially as a draft. This helps us reduce the cycle time and have a prototyping mindset towards the features we are working on. We are not afraid of failure since we are always flexible in adjusting our products based on the feedback from both our external and internal communities.\n\nI have applied this mindset to my personal life as well. In my culture, we value the smart person who never makes mistakes. So we try as hard as possible to avoid errors and losing face. However, the prototyping mindset changed my thoughts and reactions towards the things that previously may have made me feel embarrassed or uncomfortable. I became more open-minded in accepting positive and negative feedback from others. I no longer get upset or offended if someone corrects something that I did. I realized that my life is also a kind of product and it will be better and better in every iteration.\n\nWhen you trust your team members, you will be brave enough to leave your comfort zone because you believe they will give you the support whenever you need it.\n\nA good example of trust concerns my English. English is my second language and therefore it is a weakness of mine. When you lack confidence in something, you often refuse to do the things outside of your comfort zone as you fear it would make you look stupid. This was exactly my situation when I joined GitLab. However, when I realized that the people around me weren\u2019t as concerned about my shortcomings in English as much as they valued me for my contributions to the company. It gives me the courage to face my linguistic challenges.\n\nI am still not 100 percent as confident in English as I am in Mandarin, yet my confidence has increased from 30 percent to almost 70 percent if one puts a number to it. As you can see, I am writing this blog post in English to share my experience at GitLab now. This is only my second blog post.\n\nGitLab provides a very positive environment where I can improve and grow professionally as well as personally. I appreciate that my colleagues are always supportive and patient. I feel safe and comfortable while doing challenging things, not just concerning my English but in all of the tasks that I face at GitLab.\n\nI felt that it was harder to befriend managers and colleagues at a company in Asia. I am not the sure what the reason is, but I think perhaps it is because of Confucianism which impacts our culture a lot.\n\nAt GitLab, I speak freely about numerous things to my manager, Sarrah Vesselov, since I know she cares about our team and wants our team to grow. I also feel that GitLab is like a big family even though we are a large and distributed team. We try as hard as we can to get people together in both virtual and practical ways.\n\nFor example, we have the team call, and people can share a bit about their lives. We also encourage our team members to join the \u201cvirtual coffee breaks\u201d to get to know each other. Moreover, we host a summit to get together in person every nine months. This year we will meet in Cape Town, South Africa.\n\nGitLab promotes diversity and hires globally. We believe \u201cCulture add\u201d much more than \u201cCulture fit.\u201d We include different race, color, religion, gender, national origin, age, disability, or genetics. We also support inclusive benefits, for instance, Transgender Medical Services and Pregnancy and Maternity Care. We have a LGBTQ+ channel on Slack as well. Embracing differences powers our creativity.\n\nWorking with people from diverse backgrounds is fantastic. I have learned from others\u2019 communicative styles and different ways of thinking. I have broadened my views and now see the world from different perspectives. I am much more open-minded. The most important thing is that I completely understand that we are equal regardless of who we are.\n\nWorking at GitLab is a unique experience for me. I feel excited to start my work every day and enjoy the job I am doing.\n\nFor those that may be interested in working at Gitlab, we are currently hiring people from everywhere. If you want to join the journey, you can check out our jobs page and feel free to apply for the position if you feel that you are qualified. We are looking forward to hearing from you!"
    },
    {
        "url": "https://medium.com/@gitlab/how-our-production-team-runs-the-weekly-on-call-handover-6b7cae8e68fb?source=user_profile---------5----------------",
        "title": "How our production team runs the weekly on-call handover",
        "text": "How do you manage on-call incidents among a team of eight distributed across three time zones? Every week, production engineers are assigned to the role of handling on-call. With this, comes the expectation of being available to respond to any issue that results in a critical alert. Additionally, on-call individuals act as an umbrella for other members of the team by triaging and handling all issues related to GitLab.com infrastructure.\n\nThe production team structures on-call shifts so that they follow the sun, to avoid waking up members of the team in the middle of the night. This works well for GitLab\u2019s remote-only culture where there are engineers in multiple time zones. Occasionally, an on-call engineer will need to respond to an issue outside normal working hours; in these situations, GitLab encourages members to take time off after your shift to recover.\n\nAs the team members working on-call shifts are distributed and their working hours don\u2019t always overlap, you can see how it would be easy for things to slip through the cracks between one shift and the next. To prevent this happening, once a week, the production team holds a 30-minute meeting called the on-call handover. One of the key tenets of GitLab is that everything starts with an issue, and the on-call handover is no exception! From a generated report, the team reviews incidents that occurred during the last seven days and decide whether they need additional attention or escalation.\n\nAfter that, we check all GitLab issues with the on-call label to see if there are any that need to move from the current shift to the next one. At the end, there is a brief review of seven-day graphs. These help us keep an eye out for anything anomalous in our key metrics. If there is anything that seems out of the ordinary or warrants further investigation, the team will dig into them to see if we can identify the root cause. The production team at GitLab encourages leads of other groups to attend the review, as this helps bring to our attention any particular high-priority items specific to individual services.\n\nDrinking our own wine by using GitLab for on-call report generation has proven to be a good way to automate some of the more tedious work of the handover. To aid with this, the production team developed a program called the on-call robot assistant. It pulls data from relevant sources such as PagerDuty, Grafana and GitLab itself to generate a report with a GitLab issue.\n\nThe program automates the following tasks:\n\nThese data sources are set in a simple configuration file, making it easy to iterate as we add new metrics to monitor. At GitLab, most of what we do is out in the open so our on-call handover reports are available for anyone to check out. If you want to see previous reports from the on-call handovers check them out in our issue tracker.\n\nFor example, here is one recent report that shows a report for a previous week:\n\nAs well as some graphs for key metrics the production team is monitoring:\n\nWhen the team is finished reviewing the report, the current on-call engineer closes it and the shift officially ends.\n\nAre you interested in making GitLab a world-class platform for mission-critical tasks? If you live and breathe automation and infrastructure, love flexible work hours and working on an open platform, we are hiring \u2014 get in touch!"
    },
    {
        "url": "https://medium.com/@gitlab/2018-is-the-year-for-open-source-and-devops-b795ebd62f4b?source=user_profile---------6----------------",
        "title": "2018 is the year for open source and DevOps \u2013 GitLab \u2013",
        "text": "From the junior developer with just a handful of years\u2019 experience to the software professional who\u2019s been in the game for decades, our Global Developer Survey set out to see how the people behind the software are dealing with a rapidly changing technology landscape. This year\u2019s survey reveals that unclear direction is a developer\u2019s greatest challenge, IT managers are investing the most in continuous integration and delivery, and nearly all agree that the importance of open source cannot be overstated.\n\nThe focus of GitLab\u2019s 2018 Global Developer survey was to understand developers\u2019 attitudes toward their workplace, uncover disparities between developers and their management, and benchmark the state of culture, workflow, and tooling within IT organizations. We asked a broad set of questions covering everything from developers\u2019 opinions on their teams\u2019 ability to collaborate and succeed at work to their preferences on workflow methodology and tooling.\n\nWe found that the majority of developers are satisfied with the conditions of their workplace, and managers should focus on improving the planning and testing phases of the development lifecycle. We also found that IT management is more optimistic in their perception of overall workplace satisfaction with roughly 10 percent more respondents agreeing their team is set up to succeed, and that project requirements and deadlines are set up front.\n\nDelays during the planning phase emerged as a top challenge for all respondents and unclear direction remains the greatest challenge to getting work done for developers.\n\nCommitment to and demand for DevOps is growing, despite challenges posed by outmoded tooling and cultural resistance to change. Adoption is still in early stages, with 23 percent identifying DevOps as their development methodology, but this is sure to increase with IT management naming it as one of their top three areas for technology investment in 2018. The tide of developer opinion is following suit: we found that the majority of developers agree that a DevOps workflow saves valuable time during the development process. Teams currently practicing DevOps confirm the productivity gains \u2014 high performers, who told us they deploy their code on demand, and who estimated that they spend 50 percent or more of their time on new work, report having a clear DevOps culture at rates more than double that of lower-performing teams.\n\nOpen source projects like Kubernetes and CoreOS have gained a lot of recent attention and this year\u2019s survey underscores the value of creating software in the open. 92 percent of total respondents agree that open source tools are important to software innovation and nearly 50 percent report that most of their tools are open source.\n\nGitLab surveyed 5,296 software professionals of varying backgrounds and industries around the world. The margin of error is two percent, assuming a population size of 21 million software professionals and 99 percent confidence level.\n\nWe launched this Global Developer Survey on November 17, 2017, collecting responses until December 18, 2017. During that time, we promoted the survey primarily on GitLab\u2019s social media channels and newsletter. In order to correct for the gender imbalance developing in our survey sample, we made an extra push via Twitter on December 5 to encourage women involved in the software development lifecycle to take the survey. By the end of the open period, we achieved approximately 25 percent female respondents, the same percentage of women who currently hold computing roles, according to NCWIT.\n\nHow can I access the report? You can view the complete report here.\n\nAre the raw results publicly available? Yes, you can view the raw data here.\n\nDid only GitLab users take the survey? No, it was open to all who work in software production. You can view the survey demographics here.\n\nHow can I ask questions or give feedback about the survey and results? You can direct questions or comments about the survey to surveys@gitlab.com.\n\nI\u2019d like to participate in the next survey. Can I sign up for alerts? The best way to receive news about the Global Developer Survey is to sign up for our bi-weekly newsletter \u2014 you can do that below or visit our Subscription Center."
    },
    {
        "url": "https://medium.com/@gitlab/gitlabs-2018-product-vision-prototype-demo-6bd3c5d26348?source=user_profile---------7----------------",
        "title": "GitLab\u2019s 2018 Product Vision: Prototype demo \u2013 GitLab \u2013",
        "text": "At GitLab, we believe there\u2019s something magical about a video demo as a way to convey strategic vision. We\u2019ve created this video to internally align where we\u2019re going; and since we\u2019re transparent by default, you get to see it as well!\n\nSo sit back, watch the video, follow along with the presentation, or read below for a lightly edited transcript of the video. You can also play with the prototype yourself (click the header to move to the next page, click the left sidebar to move back) or follow our progress.\n\nToday I\u2019m going to talk about GitLab\u2019s product vision for 2018. Specifically, I\u2019m going to show a prototype of what the product might look like.\n\nAs you can imagine with a product vision as extensive as ours, there\u2019s a lot to cover. So if you only remember three things from this presentation, know that:\n\nSo hopefully it\u2019s obvious by now that we\u2019re going from covering the development lifecycle to covering the complete DevOps lifecycle.\n\nBut traditional DevOps tools only focus on the intersection between Dev and Ops, and GitLab is going to deliver a complete scope for both Dev and Ops. In particular, that means we\u2019re not just looking at how Developers can get their code into production, but how Operations can then monitor and manage those applications and underlying infrastructure. A big milestone for GitLab will be when Operations people log into GitLab every day and consider it their main interface for getting work done.\n\nBut even that\u2019s not really sufficient, as we\u2019re redefining what the scope of DevOps even is; we\u2019re also covering Security and Business needs (such as project managers). Rather than coming up with some crazy DevSecBizOps name, we\u2019re just calling it DevOps, and putting it all into a single application.\n\nAnd with that, each group gets an experience tailored to their needs, but shares the same data and interface as everyone else, so collaboration is easy. Imagine an Ops person finds an issue in production, drills down to find the application with the problem, and sees that a recent deploy caused the problem. Simultaneously, a dev gets alerted that their recent deploy triggered a change in production, goes to the merge request and sees the performance change right there. When Dev, Ops, and Security talk, they\u2019re looking at the same data, but from their own point of view.\n\nNow the scope we\u2019re going after is quite large, with a lot of new categories being introduced this year. I won\u2019t go into all of these today, but instead I want to focus on a couple flows that paint a picture of how this could look.\n\nFor this, I\u2019ll switch over to an interactive prototype. [Note: if you want to try it for yourself, click the header to move to the next page, click the left sidebar to move back.] While this may look like a fully functioning instance of GitLab, it is just a demo and many of these features have not been implemented yet.\n\nOne of the new elements we see is a \u201cTest summary\u201d which shows a deeper understanding of your test results. Using standard JUnit XML output, we can tell exactly which tests fail, and provide that information in a nice summary format.\n\nWe also see links to the binary artifacts and container images associated with this merge request.\n\nAs I scroll down, we see a lot of information about the extensive collection of tests we\u2019ve run on the code.\n\nFirst we see the code quality section, which we\u2019ve had for a while.\n\nThen the relatively new Security section with static application security testing to find vulnerabilities in your code or your code\u2019s dependencies, dynamic application security testing to find vulnerabilities while actually running your app, and an analysis of any vulnerabilities in any of your underlying Docker layers.\n\nWe\u2019ll also show how your application performance has changed.\n\nAnd lastly, we\u2019ll check your dependencies for any violations of your company\u2019s license policy.\n\nNow, this is a LOT to cover for every merge request, so we have separate issues to redesign for all this new information, but I wanted to show it all to you now to see how much we\u2019re doing automatically for you.\n\nDown below all of that is an enhanced code diff that highlights any code you should pay attention to because of code quality concerns or missing test coverage.\n\nThis is all part of the \u201cshift left\u201d movement, where important quality, security, and performance tests that may have once been run manually, if at all, and usually much later in the development lifecycle, are now being run automatically as soon as the first code is written.\n\nThere\u2019s a lot more planned, but this is a good idea of the direction we\u2019re going in to help Developers get their ideas into production faster.\n\nBut that only covers part of our vision, because there\u2019s also the Operations point of view. And a big milestone for our DevOps vision is when Operations start using GitLab as their primary interface.\n\nThere\u2019s a long way to go, but here we\u2019re answering the question, \u201cHow is production doing?\u201d In this case we\u2019re seeing a group with four projects in it, and a quick green/yellow/red indicator of how those projects are doing. We\u2019ve put a graph of the Apdex score there to represent the one-metric-to-watch.\n\nBelow the projects is a view of the cluster, including CPU and memory usage, possibly indicating when you need to scale up or down the cluster size.\n\nNow, if there was an indication that something was wrong, you\u2019d be able to drill down and see more details and rectify the situation.\n\nBut that\u2019s only the first-level understanding of operations. I mean, if we\u2019ve got the data about how things are doing, why not proactively alert you to the problem? Well, that\u2019s the second level, and a natural step. But we\u2019re not going to stop there. The third level is to automatically detect and resolve any issues. If your app needs more resources, just autoscale it. If you then hit a limit on the cluster, well, add a node to the cluster automatically. The Operations experience then should really just be that I go to work in the morning and see an email summary of what has happened, without me having to do anything.\n\nBut autoscaling is just scratching the surface, as Operations involves a lot more, from application, infrastructure, and network monitoring, to security patches. After we\u2019ve got this breadth as a structure, we look forward to the customer feature requests.\n\nSo that covers Dev and Ops, but we\u2019ve got a lot of security features in the product now. How about treating Security folks as first-class citizens and giving them their own Security Audit view?\n\nThis is your one-stop-shop to see what security vulnerabilities have been detected across the group, showing any automatic or manual actions taken to address the vulnerabilities, and of course letting you click into details.\n\nIn the top left we\u2019re reporting an overall success rate in hitting our own internal SLAs for security vulnerabilities.\n\nLet\u2019s drill down on one of these vulnerabilities.\n\nWe see that the GitLab Bot automatically created a merge request to upgrade one of our dependencies because it noticed that a new version was released.\n\nSince the tests all pass, and of course the merge request fixed the vulnerability, the merge request was automatically merged by the Bot as well.\n\nBut, to bring it full circle, l\u2019m showing here that after merging, the CI/CD pipeline started deploying automatically to Production. I mean, why leave a known, fixable security vulnerability live any longer than it needs to, right?\n\nBut, in this case, even though all tests passed, we still saw the error rate jump to more than five percent, so we automatically stopped the rollout process, and actually rolled back to the last-known good version immediately.\n\nThen, the Bot detects this and automatically reverts the merge request so we can leave in a good state.\n\nSo, wrapping it up:\n\nAnd that\u2019s the GitLab Product Vision for 2018!"
    },
    {
        "url": "https://medium.com/@gitlab/3-things-that-are-wrong-with-devops-today-324c94702d2b?source=user_profile---------8----------------",
        "title": "3 things that are wrong with DevOps today \u2013 GitLab \u2013",
        "text": "I\u2019m continually impressed by the benefits achieved by modern ways of working. Lean processes, Conversational Development, and automation have helped us ship more value, faster. Those achievements have led customers to expect a lot more from their service providers. DevOps has been critical to those gains, but we\u2019ve got more work to do \u2014 DevOps still has its problems.\n\nI have the privilege of talking with GitLab users every day. We celebrate impressive technical achievements, work through complex problems with CI/CD, or discuss new needs for their organization. The needs and problems seem to align themselves to one of three different areas:\n\nDev and Ops are still at war in some environments. In just the past couple of weeks I\u2019ve heard the lack of collaboration between these groups called \u201cthe wall,\u201d a \u201cchasm,\u201d and a \u201cjoke\u201d by people in both areas! We\u2019re simply not communicating well enough yet. We\u2019re disappointed that after this much investment, there\u2019s still so much room for improvement. Development and Operations continue to use different tools and to follow different rules.\n\nBut it doesn\u2019t end there. Now we\u2019ve got more people in the mix analyzing concerns like security, performance, and business metrics. It\u2019s like we\u2019re really doing DevSecBizPerfOps or some such thing, and so our flow continues to be interrupted. Silos continue to exist, if not multiply. It also feels like Ops hasn\u2019t gotten enough love, which is why GitLab is working toward better Operations views as part of our product vision for 2018.\n\nAs we continue to shift left with build, test, and security, admin costs continue to rise. Developers are often being empowered at the cost of their own productivity. Administration efforts can actually consume half a developer\u2019s time each week! Unfortunately, this is a growing form of waste. A core DevOps goal is to reduce administration time, but the admin costs of DevOps tools can be some of the highest in the the software development lifecycle ecosystem due to extensive plug-in architectures, support of quickly evolving environments, and asynchronous vendor update woes. We continually increase complexity and add requirements to existing stacks without looking for more modern solutions. Despite all the loss of time, I still hear commonly that there\u2019s no way to visualize the flow of the code from requirement to production, especially once code is committed to a repository.\n\nThe good news is that more of us are taking the time to re-examine our ecosystems because they\u2019ve become bloated with a wide variety of tools from a wide variety of vendors for very specific purposes. I wouldn\u2019t consider the current trend to be a tooling consolidation so much as a streamlining or simplification of toolsets. Questions I hear most often tend to focus on optimizing our efficiency and reliability while minimizing administration of laborious plug-in and trigger-driven architectures. We\u2019re trending in the right direction.\n\nWe\u2019ve spent and continue to spend billions on software tools annually. Tooling can be extremely costly! Sometimes we\u2019ve invested so much money in old tooling that we simply can\u2019t let it go. Too often we hold onto tools and processes just because we spent a lot of time and money on them while newer, time-saving products are available for less than the cost of the renewal of the old beasts. And so we hold onto the past as we try to implement new technologies. It\u2019s no surprise that shoving new technology into old tools can generate enormous friction and unique problems.\n\nPerhaps we bought best-in-breed tools. Those products commonly require excessive coding efforts to integrate and maintain because \u201cbest in breed\u201d typically means we bought from a number of vendors. Interconnectivity of those tools typically doesn\u2019t come out of the box. And of course, once the API is mentioned as a solution, the admin and maintenance burden increases once again. We spend a lot of money on specific solutions but inevitably end up with holes in our end-to-end process, too often as it relates to security or performance.\n\nBut this way of looking at tooling is beginning to change! I\u2019m hearing more frequently that dramatic price increases, as well as the outsourcing of product maintenance and support, are triggering enterprises to reconsider the past. When we\u2019ve invested all that time and money into a product, but that product then gets sold to three different parent companies within a decade, our ROI calculations lose their luster. Outsourcings and vendor-level product sales are being viewed as indicators of a potentially declining market. Enterprises are using that as a trigger to seek out updated tools for the years ahead, reducing cost and enabling modern workflows.\n\nNo matter whether we\u2019re talking about disappointment in collaboration, shift-left waste, or tooling admin costs, it comes down to this: it all negatively impacts our ability to deliver securely with speed and efficiency. If we truly want to meet and exceed the expectations of our customers, we\u2019ll need to continually hone and improve our DevOps processes and tools to reflect modern ways of working.\n\n3 things that are wrong with DevOps today via @JoelKroos Click to tweet!"
    },
    {
        "url": "https://medium.com/@gitlab/if-you-do-business-in-europe-you-need-to-know-about-gdpr-eaa87278afa?source=user_profile---------9----------------",
        "title": "If you do business in Europe, you need to know about GDPR",
        "text": "An explainer on the European Union\u2019s General Data Protection Regulation, which is set to take effect in May 2018.\n\nIf your company does business involving the personal data of EU residents through the offering of services and goods or otherwise, there\u2019s a good chance that your firm may need to be compliant with the European Union\u2019s General Data Protection Regulation (GDPR).\n\nThe law will go into effect on May 25 and replaces the EU\u2019s 1995 Data Protection Directive. It\u2019s meant to give EU residents more control over their personal data, specifically in how it is collected, controlled, and processed. As a result, companies that control and/or process the personal information of EU residents for their own company\u2019s purposes, or on behalf of another business, will be required to adhere to GDPR standards.\n\nPersonal data includes a vast range of information including social security numbers, gender, location, ethnicity, online identifiers, and genetic or biometric markers, such as fingerprints and facial recognition.\n\nControllers are a company or organization that determines the purpose for and manner in which personal data is processed.\n\nControllers can also be processors.\n\nData processors take the information controllers have accumulated and process the personal information.\n\nIt is recommended that companies conduct data discovery activities like data mapping and a gap analysis in order to get a true handle on the amount and nature of the personal data they control and process. A recent report from Forrester warned against approaching GDPR readiness from a fragmented framework that relies heavily on IT for specific compliance requirements \u2014 like focusing on data breach notifications, stating that such tactics are \u201cshort-sighted, and most likely will need radical revision after the enforcement of GDPR rules start in May.\u201d\n\nFailure to comply with the GDPR requirements could result in serious penalties, with the worst case scenario being a fine of \u20ac20 million or 4 percent of the company\u2019s previous year\u2019s total global revenue, whichever is greater.\n\nFor a more detailed look at the law and how organizations can ensure they\u2019re compliant, check out GitLab\u2019s GDPR page."
    },
    {
        "url": "https://medium.com/@gitlab/using-gitlab-to-project-manage-home-renovation-priorities-c54114f19629?source=user_profile---------10----------------",
        "title": "Using GitLab to project manage home renovation priorities",
        "text": "Last summer my wife and I bought a new house for our ever-growing family. Before we moved in, we had a couple of improvements made \u2014 wood floors to replace the aging carpet in the master bedroom, some required structural fixes. However, when we bought the house, we knew there would be a lot more we wanted to do over the years. When it came to organizing those ideas into things that need to happen sooner rather than later and those that could wait, however, we found ourselves struggling to keep all of the plans in order.\n\nI\u2019ve been able to complete a few other projects since we moved in \u2014 but most were small in scale. A built-in shelf wall for my wife\u2019s office, painting and staining the new deck, and of course a DIY standing desk to use in my new office kitchen (which is also the house\u2019s kitchen\u2026 working from home for the win!). These projects were great, but we needed a way to organize and prioritize larger renovation projects.\n\nI was a GitLab user for years before I even became a GitLabber. I\u2019ve always hosted my side-project code in GitLab.com since GitLab offers unlimited private repositories for free. For project management in my \u201cday job\u201d I\u2019ve used dozens of other tools outside of GitLab, so when I joined it was the first time I saw the full breadth of what GitLab offers in issue management.\n\nIn thinking about the other tools I\u2019ve used in the past, they didn\u2019t seem to meet the full bar of what I was looking for to solve our problem. As a mother of four young children, my wife is always on the go\u2026 but I\u2019m on a computer all day long. So we needed something that worked seamlessly between platforms. We also needed to be able to easily re-arrange and re-prioritize items. Also, I fancy myself a bit of a DIY-er, so I wanted to be able to label some items as at least possible for me to maybe complete myself. All of these requirements had me wondering what tool would be best for my wife and me to collaborate on.\n\nWith these requirements, and my newfound GitLab knowledge, I was able to come up with a novel solution to the problem we were having: why not use a GitLab Issue Board to manage our ever-changing home renovation priorities?\n\nWith Issue Boards, we would have a fantastic solution for mobile and desktop (shout out to the GitLab UX team!). With labels, I could organize and group issues however we wanted. And the customizable columns would allow us to prioritize, track and manage the various issues and ideas.\n\nTo start, I created a new group on GitLab.com to house (pun intended) everything for our family. I made a project in that group called to be the central place to collect all the renovation ideas we had. In the future, I may have a project for a specific renovation, managing purchases, and contractors, etc.\n\nAs with every GitLab project, issues and issue boards were baked right in. I started adding issues right away \u2014 beginning with those that were at the top of mind, like the water heater that is at the end of its usable life, repairs to our front entryway, and window replacement. My wife didn\u2019t have a GitLab.com account yet, but it was easy to add her to the project as a member just by putting her email address in on the member\u2019s page, allowing her to sign up and get access to the project in one step.\n\nTo get organized, I created a few labels: for top priority items, for those I might be able to tackle on my own, and for those that involved furnishing various rooms. The labels will help filter issues so that if I find a free weekend, I can search for issues to maybe get started on. Or if we go to a furniture store, we could filter to those issues to get an idea of cost while we are there.\n\nFor the board columns, I decided to use as the first column after Backlog to highlight those issues. From there, it's a matter of agreeing on an organization of priority \ud83d\ude03\n\nNow it\u2019s time to execute! One thing we didn\u2019t account for in the first iteration was the scope of issues. Some things were relatively minor regarding time and investment. Others (like replacing all 27 windows!) are larger projects for which we need to budget. For this, we will be using issue weight to understand how different projects align with budget and time investment to pull off.\n\nIt\u2019s been an exciting experience using GitLab Issue Boards for something outside of the development space. We\u2019d love to hear from you too about \u201cnon-standard\u201d uses for GitLab\u2019s features. Feel free to comment on this post or tweet us @GitLab."
    },
    {
        "url": "https://medium.com/@gitlab/gemnasium-has-joined-the-gitlab-team-7c6aa8802f24?source=user_profile---------11----------------",
        "title": "Gemnasium has joined the GitLab team! \u2013 GitLab \u2013",
        "text": "Today we\u2019re happy to share the news that GitLab has acquired Gemnasium, the dependency monitoring solution.\n\nThe Gemnasium team have joined GitLab and are working on implementing robust security scanning functionality natively into GitLab\u2019s CI/CD pipelines, as part of our vision for delivering the complete DevOps lifecycle in a single application. For all the details, please check out the announcement on Gemnasium\u2019s blog.\n\nGemnasium is joining the GitLab team via @sytses Click to tweet!"
    },
    {
        "url": "https://medium.com/@gitlab/how-a-fix-in-go-1-9-sped-up-our-gitaly-service-by-30x-58079e5add8c?source=user_profile---------12----------------",
        "title": "How a fix in Go 1.9 sped up our Gitaly service by 30x",
        "text": "Gitaly is a Git RPC service that we are currently rolling out across GitLab.com, to replace our legacy NFS-based file-sharing solution. We expect it to be faster, more stable and the basis for amazing new features in future.\n\nWe\u2019re still in the process of porting Git operations to Gitaly, but the service has been running in production on GitLab.com for about nine months, and currently peaks at about 1,000 gRPC requests per second. We expect the migration effort to be completed by the beginning of April at which point all Git operations in the GitLab application will use the service and we\u2019ll be able to decommission NFS infrastructure.\n\nThe first time we realized that something might be wrong was shortly after we\u2019d finished deploying a new release.\n\nWe were monitoring the performance of one of the gRPC endpoints for the Gitaly service and noticed that the 99th percentile performance of the endpoint had dropped from 400ms down to 100ms.\n\nLatencies drop from 400ms to 100ms after a deploy, for no good reason\n\nThis should have been fantastic news, but it wasn\u2019t. There were no changes that should have led to faster response times. We hadn\u2019t optimized anything in that release; we hadn\u2019t changed the runtime and the new release was using the same version of Git.\n\nEverything should have been exactly the same.\n\nWe started digging into the data a little more and quickly realised that 400ms is a very high latency for an operation that simply confirms the existence of a Git reference.\n\nHow long had it been this way? Well it started about 24 hours after the previous deployment.\n\nLatencies rising over a 24 hour period following a deployment, for no good reason\n\nWhen browsing our Prometheus performance data, it quickly became apparent that this pattern was being repeated with each deployment: things would start fast and gradually slow down. This was occurring across all endpoints. It had been this way for a while.\n\nThe first assumption was that there was some sort of resource leak in the application, causing the host to slow down over time. Unfortunately the data didn\u2019t back this up. CPU usage of the Gitaly service did increase, but the hosts still had lots of capacity.\n\nGitaly CPU increasing with process age, but not enough to explain the problem\n\nAt this point, we still didn\u2019t have any good leads as to the cause of the problem, so we decided to further improve the observability of the application by adding pprof profiling support and cAdvisor metrics.\n\nAdding pprof support to a Go process is very easy. The process already has a Prometheus listener and we added a pprof handler on the same listener.\n\nSince production teams would need to be able to perform the profiling without our assistance, we also added a runbook.\n\nGo\u2019s pprof support is easy to use and in our testing, we found that the overhead it added to production workloads was negligible, meaning we could use it in production without concern about the impact it would have on site performance.\n\nThe Gitaly service spawns Git child processes for many of its endpoints. Unfortunately these Git child processes don\u2019t have the same instrumentation as the parent process so it was difficult to tell if they were contributing to the problem. (Note: we record metrics for Git processes but cannot observe grandchild processes spawned by Git, which often do much of the heavy lifting)\n\nOn GitLab.com, Gitaly is managed through systemd, which will automatically create a cgroup for each service it manages.\n\nThis means that Gitaly and its child processes are contained within a single cgroup, which we could monitor with cAdvisor, a Google monitoring tool which supports cgroups and is compatible with Prometheus.\n\nAlthough we didn\u2019t have direct metrics to determine the behavior of the Git processes, we could infer it using the cgroup metrics and the Gitaly process metrics: the difference between the two would tell us the resources (CPU, memory, etc) being consumed by the Git child processes.\n\nAt our request, the production team added cAdvisor to the Gitaly servers.\n\nHaving cAdvisor gives us the ability to know what the Gitaly service, including all its child processes, is doing.\n\nIn the meantime, the situation had got far worse. Instead of only seeing gradual latency increases over time, we were now seeing far more serious lockups.\n\nIndividual Gitaly server instances would grind to a halt, to the point where all new incoming TCP connections were not being accepted. This proved to be a problem to using pprof: during the lockup the connection would time out when attempting to profile the process. Since the reason we added pprof was to observe the process under duress, that approach was a bust.\n\nInterestingly, during a lock-up, CPU would actually decrease \u2014 the system was not overloaded, but actually idled. Iops, iowait and CPU would all drop way down.\n\nEventually, after a few minutes the service would recover and there would be a surge in backlogged requests. Usually though, as soon as the state was detected, the production team would restart the service manually.\n\nThe team spent a significant amount of time trying to recreate the problem locally, with little success.\n\nWithout pprof, we fell back to SIGABRT thread dumps of hung processes. Using these, we determined that the process had a large amount of contention around during the lockups. In one dump, 1,400 goroutines were blocked waiting on \u2013 most for several minutes.\n\nhas the following documentation:\n\nEach Gitaly server instance was 'ing Git processes about 20 times per second so we seemed to finally have a very promising lead.\n\nResearching ForkLock led us to an issue on the Go repository, opened in 2013, about switching from to with and on systems that support it: golang/go#5838\n\nThe syscall with and is the same as the c function, but the latter is easier to refer to, so let's use that.\n\nWhen using , the child process will start with a copy of the parent processes' memory. Unfortunately this process takes longer the larger the virtual memory footprint the process has. Even with copy-on-write, it can take several hundred milliseconds in a memory-intensive process. doesn't copy the parent processes' memory space and has a roughly constant time.\n\nSome good benchmarks of vs. can be found here: https://github.com/rtomayko/posix-spawn#benchmarks\n\nThis seemed like a possible explanation. Over time, the virtual memory size (VMM) of the Gitaly process would increase. As VMM increased, each syscall would take longer. As fork latency increased, contention would increase. If time exceeded the frequency of requests, the system could temporarily lock up entirely.\n\nBy some incredibly good luck, golang/go#5838, the switch from to , had, after several years' delay, recently landed in Go 1.9, just in time for us. Gitaly had been compiled with Go 1.8. We quickly built and tested a new binary with Go 1.9 and manually deployed this on one of our production servers.\n\nHere\u2019s the CPU usage of Gitaly processes across the fleet:\n\nCPU after recompiling with Go 1.9\n\nHere\u2019s the 99th percentile latency figures. This chart is using a logarithmic scale, so we\u2019re talking about two orders of magnitude faster!\n\nEndpoint latency after recompiling with Go 1.9 (log scale)\n\nRecompiling with Go 1.9 solved the problem, thanks to the switch to . We learnt several other lessons in the process too:"
    },
    {
        "url": "https://medium.com/@gitlab/conducting-remote-ux-research-at-gitlab-1c89ed9a5ee5?source=user_profile---------13----------------",
        "title": "Conducting remote UX research at GitLab \u2013 GitLab \u2013",
        "text": "GitLab is a remote-only organization and just like our team, our users are spread across the globe. Conducting remote UX research allows us to quickly connect with GitLab users anywhere in the world. It provides us with the opportunity to gather insight into users\u2019 behaviors, motivations and goals when using GitLab. This helps us to determine what features should be built and how they should behave. But how do we do all this remotely?\n\nThese are some of the remote UX research methods we use at GitLab.\n\nCard sorting is a research method for discovering how people understand and categorize information. Each card represents an item or a topic and we ask users to group the cards in a way that makes sense to them. We may also ask them to help us label these groups.\n\nCard sorting can be used to:\n\nWhen analyzing a card sort, we look for common patterns such as which cards appear together the most and which cards are labeled in a similar way.\n\nAt GitLab, we\u2019re currently using card sorting to restructure the sidebar navigation at a project and group level. We want to understand how you, our users, would expect our features to be grouped and classified. Our aim is to improve the ease and the speed at which you navigate around GitLab. We conduct remote card sorting via Optimal Workshop.\n\nFirst-click testing explores what users click on first when completing a task within an interface. It tells us whether users are able to find what they\u2019re looking for quickly and easily. This research method is based on the principle that users are two to three times more likely to find what they are looking for if their initial click is correct, rather than a click in the wrong direction.\n\nWe\u2019ve used first-click testing at GitLab to quickly evaluate multiple design ideas against one another. We share our designs with users via UsabilityHub. We measure whether users take the correct path and how long it takes them to decide where to click. A slower click time would suggest a user has hesitated about where to click.\n\nFirst-click testing is great for providing an indication of whether a design is intuitive to users and helps us to quickly narrow down multiple design concepts.\n\nSurveys are used to investigate the opinions or experiences of users by asking them questions through an online form. A survey invites people to share open and honest feedback. Some people find them less intimidating than other forms of research as there is the option to remain anonymous when providing answers. They also allow us to track how the attitudes and behaviors of our users change over time.\n\nWe\u2019ve used surveys to understand our users and form personas, to generate new ideas for future GitLab improvements and to help measure users\u2019 satisfaction with our existing features.\n\nIf you take part in a user interview at GitLab, you\u2019ll usually be speaking one on one with a UX researcher. In order to do this, you\u2019ll need a desktop or laptop computer and a headset with a microphone.\n\nWe find that most of our users like to talk with us on their lunch break at their work station, whether situated at home or in an office. We love this, as it provides some insight into the environment in which you use GitLab.\n\nOften our interviews are focused on you! We\u2019ll ask you to chat about things such as your background, occupation and experience with GitLab. Sometimes we might have a particular topic we\u2019d like to discuss, such as how you\u2019ve incorporated GitLab into your workflow. We\u2019ll always tell you our intentions ahead of the call so you have time to think about what you\u2019d like to contribute to the discussion. We also welcome you to share your screen with us during the call. We understand that it is sometimes easier to show and demonstrate something than it is to just talk about it!\n\nWe\u2019ve used feedback from user interviews to:\n\nUsability testing is a technique used to evaluate a product by testing it with representative users. Usability testing can be divided into two categories: moderated and unmoderated research.\n\nIf you participate in moderated usability testing at GitLab, you\u2019ll complete a series of tasks whilst being observed by one of our UX researchers. In order to see what you\u2019re doing, we\u2019ll ask you to share your screen with us. We use Zoom to run our moderated usability testing sessions.\n\nAs you use GitLab, we\u2019ll ask you to try and think out loud: tell us what you\u2019re looking at, what you\u2019re trying to do and what you\u2019re thinking. We\u2019re interested in hearing your honest feedback. Sound scary? It really isn\u2019t! It\u2019s important to remember that we\u2019re testing GitLab, not you. You can\u2019t say or do anything wrong during a study.\n\nModerated research allows for conversation between a user and the UX researcher, because both are online simultaneously. It gives the researcher the opportunity to ask a user follow-up questions regarding something they\u2019ve said or done. Subsequently, moderated research provides us with a lot of in-depth qualitative research about our users\u2019 needs. It can help us to uncover usability problems that we weren\u2019t aware of and to generate solutions to solve these problems.\n\nUnlike moderated research, unmoderated research doesn\u2019t involve any conversation between a user and a UX researcher. Instead, unmoderated usability testing sessions are completed alone by a user. As users can complete sessions at their own convenience and studies can be run simultaneously, they\u2019re good for collecting data quickly.\n\nWe use Validately to serve the tasks to you and to record your actions. We then analyze the data collected asynchronously. It is, however, still very helpful to us if you try and think out loud while you\u2019re completing tasks.\n\nUnmoderated research can provide some qualitative data. However, as there\u2019s no opportunity to ask users follow-up questions related to their actions, the study should focus on a few specific elements or relatively minor changes. Unmoderated research is usually better at addressing specific quantitative questions, such as:\n\nAs a researcher cannot view an unmoderated usability testing session until it\u2019s completed, there\u2019s a risk of a study being unusable if the user didn\u2019t complete the tasks as specified or if they ran into technical difficulties.\n\nWe conduct both moderated and unmoderated usability testing sessions at GitLab to test new features and changes to existing features.\n\nWe\u2019re always looking for people to participate in our research, whether you\u2019re a GitLab user or not. You can get involved by signing up to our research panel. Besides being instrumental in shaping the future of GitLab, you\u2019ll have the opportunity to earn gift cards and win awesome tech prizes by sharing your feedback with us.\n\nConducting remote UX research at GitLab via @saraheod Click to tweet!"
    },
    {
        "url": "https://medium.com/@gitlab/scaling-design-the-start-of-system-thinking-1e78f72bd8ef?source=user_profile---------14----------------",
        "title": "Scaling design: The start of system thinking \u2013 GitLab \u2013",
        "text": "Scaling design within an application is a struggle. Design systems help alleviate problems that arise with scaling by making it easier to find inconsistent interactions or conflicting messaging. However, it can be extremely difficult to introduce a new system to teams that are already functioning without one. Here\u2019s how we got started.\n\nWe took the initial step towards establishing our own system by creating a pattern library of reusable components that can be shared and reused across the application.\n\nConsistency within the UI and increased iteration speed are clear benefits for using a design library. This helps keep the application DRY and allows designers to focus their efforts on solving user needs, rather than recreating elements and reinventing solutions. In an effort to create a library that is understood by multiple teams, it\u2019s important to begin thinking about design as a language.\n\nYour design language is an integral part of a design system that clearly defines the semantics of your visual designs and allows your team to thoroughly document guidelines. It\u2019s important that the team not only understands how the system is built, but also the reasoning behind the choices made. This will ultimately help enable your team to build a library of components that support the semantics you have established.\n\nKnowing where to start can be daunting. We began by first understanding the current state of our application. By auditing current designs that were implemented, we found numerous inconsistencies across our interface and determined that we lacked a solid design language to build from. A search within our variables revealed that we had 82 different gray values defined within the UI. We also had an undefined type scale that included at least 30 different values in pixels, rems, and percentages.\n\nBy understanding the problems our current system had, we were able to start building a solid foundation to work from. We defined and documented our perceptual patterns which included styles that aid in the aesthetic of the brand: typography, icons, colors, and a measurement system.\n\nOnce our perceptual patterns were defined, we started applying them to our components. We took a couple core pieces of our application and mocked them up using our new guidelines to ensure that our new rules were not too rigid and would be flexible enough to still encourage the creation of new ideas and methods while designing new components.\n\nOnce we nailed down our styles, we were able to start identifying functional patterns that needed to be built out using our new guidelines. Functional patterns include global modules that can be reused throughout your application, such as buttons, dropdowns, and tabs.\n\nThere were a few instances where our newly defined styles did not work well in our actual designs. For example, we determined that our 8px measurement system was too strict for right and left padding on horizontal tabs, buttons, and inputs. Although it was not a part of our measurement system, we decided as a team to create a new rule that would allow for a 12px measure in order better align stacked items while giving elements enough room to breathe.\n\nBuilding out these components gave us the opportunity to alter and add to our new perceptual patterns. It is okay to allow some flexibility within your design library, so long as the rules and use cases are clearly defined.\n\nWe set up our design library using a primary sketch file that includes all the components and styles that have been added to our team library. As we began building out multiple components, it was important to define a structure that would mimic the way components are implemented on the frontend. This would allow the design and frontend teams to work more closely together, ensuring that components were DRY and reusable. We chose to implement Brad Frost\u2019s Atomic Design principles in order to accomplish this. Atomic design \u201cbreak[s] entire interfaces down into fundamental building blocks,\u201d ensuring that everything is constructed in a methodical way. These building blocks consist of:\n\nAtoms: Elements that cannot be broken down further. This can include type styles, buttons, labels, and inputs\n\nMolecules: A group of atoms that function as a unit, such as a form.\n\nOrganisms: A high-level component that consists of several molecules to make up its own structure. This can include a header or a sidebar.\n\nThere has been a lot written on Atomic Design. To learn more I recommend:\n\nFollowing this structure forces the team to think carefully about what each part of a design is made up of, as well as easily define global components. If a modifier consists of atoms that are not used elsewhere, we encourage designers to think about whether a specific atom is necessary for that paradigm or if an existing global component would work in its place.\n\nIn the following example, we\u2019ve built out our left navigational sidebar. This organism comprises molecules, and these molecules comprise globally used atoms (an avatar, badge, typography, and icons). We also include molecule modifiers, which make it easy to see the different states that a molecule can have. These together build the basis of the sidebar.\n\nWe use symbols within Sketch to create our atoms and molecules, while leaving organisms as groups so that we can easily modify and override specific aspects to fit the design we are working on.\n\nChoosing tools can be an arduous task, especially with the number of options available for designers today. It is easy to get caught up in the latest tool and turn progress into tool churn. At GitLab, we took the time to evaluate multiple tools that would assist in the creation of a team library.\n\nSome of the issues we ran into while evaluating plugins were:\n\nWe eventually decided to move forward using Brand.ai as a plugin for Sketch. This plugin solved many of the issues we were running into with other tools. However, while this plugin was the best that we found at the time, no tool is perfect:\n\nAt GitLab, we don\u2019t look at Brand.ai as the answer. It is solely a tool to help aid us in the creation process. Since deciding on using Brand.ai, Sketch has released their own library feature, Brand.ai was acquired by InVision, and Figma has added numerous new features to aid in the creation of a design library. Tools are constantly transforming, but it\u2019s important to keep in mind that constantly changing tools may slow progress. Evaluate your tools carefully and decide what is best for your team at this moment. Remember that pattern libraries are only one aspect of a design system that helps make it more effective. The tools and technologies you use to create the library are meant to help your team, not act as the solution.\n\nConversations around design systems have exploded in recent years. Just over the last few months, Figma has begun sponsoring Design System Dinners, InVision has created a Design Systems Handbook, and Smashing Magazine released Design Systems as their newest book.\n\nAt GitLab, we have only just begun the work on our design system. A design library is only the first part of our overall goal and it is our first step towards ensuring that our design will scale within the growing organization. We have begun thinking about design with a system in mind by creating a design language that captures the visual styles of our brand, as well as creating reusable and robust components. We\u2019ve chosen tools and technologies that help aid us in this process while remembering that they are always evolving and are not the system itself.\n\nBeyond continuing to build out new paradigms within our design library, our next step is to begin linking our design library with our frontend code. This will allow us to include not only our designs and documentation, but also code snippets that can be used and referenced in our application. We have only just started this process and are in the very early stages of setting up a repository to showcase our system.\n\nIf you have any tips, tricks, or lessons that you discovered while building out your own design library or system, we would love to hear from you!\n\nScaling design: The start of system thinking via @tauried Click to tweet!"
    },
    {
        "url": "https://medium.com/@gitlab/2018-global-developer-survey-aims-to-uncover-developer-needs-and-preferences-at-work-acbd93378e12?source=user_profile---------15----------------",
        "title": "2018 Global Developer Survey aims to uncover developer needs and preferences at work",
        "text": "What do you need to do your best work? From overall developer satisfaction at work and with management, to the use of open source tools and preferred workflow and collaboration methods, we want to uncover the needs and preferences of the modern developer.\n\nAs an open core company, we value the input, contributions, and needs of our community. Our intention in running this survey and openly sharing the results is to improve the daily work lives of the global development community. We want to empower developers and their managers with the information they need to work better, together. It\u2019s our hope that the results of this survey can act as an advocate for the needs of developers, reduce the perception gap between management and developers, and shed light on what high-functioning organizations are doing differently.\n\nIt takes about 15 minutes to complete and includes approximately 25 required questions and a handful of optional, short answer questions for elaboration. The survey is anonymous, and data and results will be reviewed in aggregate. Topics range from overall developer satisfaction, open source technology, workflows and collaboration, adoption of CI/CD practices, to developer tooling preferences.\n\nWe\u2019re committed to putting out a quality and insightful report that is useful to the developer community at large. To ensure this, we tested the survey with our internal GitLab engineering team to gather feedback and suggestions to make it better.\n\nIt\u2019s open to anyone involved in the software development lifecycle \u2014 from developers and engineers to DevOps managers and IT executives, we want to hear from you!\n\nAs thanks for participating, we\u2019re giving away five exclusive GitLab robes and one Nintendo Switch! We\u2019ll give away a robe per week until they\u2019re all gone, using the email addresses from respondents that week. Completing the survey and sharing on social can enter you to win one Nintendo Switch during the final week! Just send a link to your post to giveaways@gitlab.com. We can\u2019t wait to share the results!\n\nYou must complete the survey and provide an email address to be eligible to win. Your privacy is important to us; email addresses will only be used for the draw and will not be saved. Read official sweepstake rules here."
    },
    {
        "url": "https://medium.com/@gitlab/were-switching-to-a-dco-for-source-code-contributions-b07af070ea20?source=user_profile---------16----------------",
        "title": "We\u2019re switching to a DCO for source code contributions",
        "text": "We\u2019re committed to being good stewards of open source, and part of that commitment means we never stop re-evaluating how we do that. Saying \u201ceveryone can contribute\u201d is about removing barriers to contribution. For some of our community, the Contributor License Agreement is a deterrent to contributing to GitLab, so we\u2019re changing to a Developer\u2019s Certificate of Origin instead.\n\nMany large open source projects want to be masters of their own destiny. Having the freedom to run your own infrastructure based on open source software, together with the ability to modify and audit source code and not be dependent on a vendor, makes open source appealing. We want GitLab to be an option for everyone.\n\nA Contributor License Agreement (CLA) is the industry standard for open source contributions to other projects, but it\u2019s unpopular with developers, who don\u2019t want to enter into legal terms and are put off by having to review a lengthy contract and potentially give up some of their rights. Contributors find the agreement unnecessarily restrictive, and it\u2019s deterring developers of open source projects from using GitLab. We were approached by Debian developers to consider dropping the CLA, and that\u2019s what we\u2019re doing.\n\nAs of November 1, we\u2019re rolling out changes so that contributors to the GitLab source code will only be required to make contributions and bug fixes under a project license (MIT for all repositories with the exception of Omnibus which would be licensed under Apache) and a Developer\u2019s Certificate of Origin (DCO). The DCO gives developers greater flexibility and portability for their contributions, and it\u2019s one of the reasons that Debian and GNOME plan to migrate their communities and projects to GitLab. We hope this change encourages more developers to contribute to GitLab. Thank you Debian, for prompting us to make this change.\n\nYou can read the analysis that informed our decision. Read all about our stewardship of GitLab Community Edition."
    },
    {
        "url": "https://medium.com/@gitlab/announcing-20-million-in-series-c-round-funding-led-by-gv-to-complete-devops-5601f13557c3?source=user_profile---------17----------------",
        "title": "Announcing $20 million in Series C round funding led by GV to complete DevOps",
        "text": "Today we are thrilled to announce our $20 million Series C funding led by GV. This follows our Series B round last September. With the help of our investors (and community!) we\u2019re gearing up to bring you Complete DevOps, a reimagined scope of DevOps that unifies development and operations work into a single user experience.\n\nNot a GitLab user? Install GitLab or sign in to get started!\n\nIn addition to our Series C funding round, we\u2019re excited to announce two new board members, Matt Mullenweg, founder of WordPress, and Dave Munichiello, GV General Partner.\n\nSince our start in 2014, we\u2019ve had one mission: change all creative work from read-only to read-write so that everyone can contribute. Last year we unveiled GitLab\u2019s Master Plan on September 13th, committing to shipping every stage of idea to production (which we completed in 8.15!). This was a major step forward in simplifying the software development process. Now, we\u2019re taking it a step further to unite development and operations in one user experience. Watch the recording of our earlier live stream announcing our #CompleteDvevOps vision below, and keep scrolling for a recap and the slides from the presentation.\n\nBefore DevOps, the world of software iteration was slow, insecure, and error prone. DevOps came to the intersection of development and operations to create faster iteration cycles with greater quality and security.\n\nBut it didn\u2019t go far enough\u2026\n\nIn the current landscape, developers and operations use different tools, they don\u2019t have the ability to fully collaborate, and the need to integrate many disparate tools continues to be a point of friction that slows progress and leads to insecure, poor quality code.\n\nComplete DevOps reimagines the scope of tooling to include both developers and operations teams in one unified solution. This dramatically reduces friction, increases collaboration, and drives a competitive advantage.\n\nIn 10.0, we shipped the first iteration of Auto DevOps, which just scratches the surface of the Complete DevOps features we have in the works. You can read our Head of Product Mark Pundsack\u2019s detailed vision in his blog post, but to summarize:\n\nWe want to build GitLab into the complete DevOps tool chain. We already cover every stage of the software development lifecycle. Why stop at production? Why not go beyond that, into operations? We want to close the loop between Dev and Ops, automating processes and reducing complexity so that you can focus on a great customer experience.\n\nShare your thoughts, comments, and questions about #CompleteDevOps with us on Twitter!\n\nThe software world is moving from virtual machines to cloud-native development. We want to help ease this transition for companies, by offering a complete development and operations solution for cloud-native development.\n\nWe \ud83d\udc9c our community! At GitLab, everyone can contribute and we owe GitLab\u2019s existence to your enthusiasm, drive, and hard work. Without our contributors\u2019 belief in open source software, we would not be where we are today. We need your help to make our collective vision a reality.\n\nWe are committed to standing by our promise to be good stewards of open source, and keeping communication and collaboration amongst the community a high priority. Our open core business model ships both open and closed software. In an effort to maintain an unprecedented level of transparency, we follow three key principles:\n\nRead more about our company values in our open source handbook, licensed by CC BY-SA 4.0."
    },
    {
        "url": "https://medium.com/@gitlab/scaling-the-gitlab-database-c0972da3c754?source=user_profile---------18----------------",
        "title": "Scaling the GitLab database \u2013 GitLab \u2013",
        "text": "For a long time GitLab.com used a single PostgreSQL database server and a single replica for disaster recovery purposes. This worked reasonably well for the first few years of GitLab.com\u2019s existence, but over time we began seeing more and more problems with this setup. In this article we\u2019ll take a look at what we did to help solve these problems for both GitLab.com and self-hosted GitLab instances.\n\nFor example, the database was under constant pressure, with CPU utilization hovering around 70 percent almost all the time. Not because we used all available resources in the best way possible, but because we were bombarding the server with too many (badly optimized) queries. We realized we needed a better setup that would allow us to balance the load and make GitLab.com more resilient to any problems that may occur on the primary database server.\n\nWhen tackling these problems using PostgreSQL there are essentially four techniques you can apply:\n\nOptimizing the application code is something we have been working on actively for the past two years, but it\u2019s not a final solution. Even if you improve performance, when traffic also increases you may still need to apply the other two techniques. For the sake of this article we\u2019ll skip over this particular subject and instead focus on the other techniques.\n\nIn PostgreSQL a connection is handled by starting an OS process which in turn needs a number of resources. The more connections (and thus processes), the more resources your database will use. PostgreSQL also enforces a maximum number of connections as defined in the max_connections setting. Once you hit this limit PostgreSQL will reject new connections. Such a setup can be illustrated using the following diagram:\n\nHere our clients connect directly to PostgreSQL, thus requiring one connection per client.\n\nBy pooling connections we can have multiple client-side connections reuse PostgreSQL connections. For example, without pooling we\u2019d need 100 PostgreSQL connections to handle 100 client connections; with connection pooling we may only need 10 or so PostgreSQL connections depending on our configuration. This means our connection diagram will instead look something like the following:\n\nHere we show an example where four clients connect to pgbouncer but instead of using four PostgreSQL connections we only need two of them.\n\nFor PostgreSQL there are two connection poolers that are most commonly used:\n\npgpool is a bit special because it does much more than just connection pooling: it has a built-in query caching mechanism, can balance load across multiple databases, manage replication, and more.\n\nOn the other hand pgbouncer is much simpler: all it does is connection pooling.\n\nLoad balancing on the database level is typically done by making use of PostgreSQL\u2019s \u201chot standby\u201d feature. A hot-standby is a PostgreSQL replica that allows you to run read-only SQL queries, contrary to a regular standby that does not allow any SQL queries to be executed. To balance load you\u2019d set up one or more hot-standby servers and somehow balance read-only queries across these hosts while sending all other operations to the primary. Scaling such a setup is fairly easy: simply add more hot-standby servers (if necessary) as your read-only traffic increases.\n\nAnother benefit of this approach is having a more resilient database cluster. Web requests that only use a secondary can continue to operate even if the primary server is experiencing issues; though of course you may still run into errors should those requests end up using the primary.\n\nThis approach however can be quite difficult to implement. For example, explicit transactions must be executed on the primary since they may contain writes. Furthermore, after a write we want to continue using the primary for a little while because the changes may not yet be available on the hot-standby servers when using asynchronous replication.\n\nSharding is the act of horizontally partitioning your data. This means that data resides on specific servers and is retrieved using a shard key. For example, you may partition data per project and use the project ID as the shard key. Sharding a database is interesting when you have a very high write load (as there\u2019s no other easy way of balancing writes other than perhaps a multi-master setup), or when you have a lot of data and you can no longer store it in a conventional manner (e.g. you simply can\u2019t fit it all on a single disk).\n\nUnfortunately the process of setting up a sharded database is a massive undertaking, even when using software such as Citus. Not only do you need to set up the infrastructure (which varies in complexity depending on whether you run it yourself or use a hosted solution), but you also need to adjust large portions of your application to support sharding.\n\nOn GitLab.com the write load is typically very low, with most of the database queries being read-only queries. In very exceptional cases we may spike to 1500 tuple writes per second, but most of the time we barely make it past 200 tuple writes per second. On the other hand we can easily read up to 10 million tuples per second on any given secondary.\n\nStorage-wise, we also don\u2019t use that much data: only about 800 GB. A large portion of this data is data that is being migrated in the background. Once those migrations are done we expect our database to shrink in size quite a bit.\n\nThen there\u2019s the amount of work required to adjust the application so all queries use the right shard keys. While quite a few of our queries usually include a project ID which we could use as a shard key, there are also many queries where this isn\u2019t the case. Sharding would also affect the process of contributing changes to GitLab as every contributor would now have to make sure a shard key is present in their queries.\n\nFinally, there is the infrastructure that\u2019s necessary to make all of this work. Servers have to be set up, monitoring has to be added, engineers have to be trained so they are familiar with this new setup, the list goes on. While hosted solutions may remove the need for managing your own servers it doesn\u2019t solve all problems. Engineers still have to be trained and (most likely very expensive) bills have to be paid. At GitLab we also highly prefer to ship the tools we need so the community can make use of them. This means that if we were going to shard the database we\u2019d have to ship it (or at least parts of it) in our Omnibus packages. The only way you can make sure something you ship works is by running it yourself, meaning we wouldn\u2019t be able to use a hosted solution.\n\nUltimately we decided against sharding the database because we felt it was an expensive, time-consuming, and complex solution to a problem we do not have.\n\nFor connection pooling we had two main requirements:\n\nReviewing the two solutions (pgpool and pgbouncer) was done in two steps:\n\npgpool was the first solution we looked into, mostly because it seemed quite attractive based on all the features it offered. Some of the data from our tests can be found in this comment.\n\nUltimately we decided against using pgpool based on a number of factors. For example, pgpool does not support sticky connections. This is problematic when performing a write and (trying to) display the results right away. Imagine creating an issue and being redirected to the page, only to run into an HTTP 404 error because the server used for any read-only queries did not yet have the data. One way to work around this would be to use synchronous replication, but this brings many other problems to the table; problems we prefer to avoid.\n\nAnother problem is that pgpool\u2019s load balancing logic is decoupled from your application and operates by parsing SQL queries and sending them to the right server. Because this happens outside of your application you have very little control over which query runs where. This may actually be beneficial to some because you don\u2019t need additional application logic, but it also prevents you from adjusting the routing logic if necessary.\n\nConfiguring pgpool also proved quite difficult due to the sheer number of configuration options. Perhaps the final nail in the coffin was the feedback we got on pgpool from those having used it in the past. The feedback we received regarding pgpool was usually negative, though not very detailed in most cases. While most of the complaints appeared to be related to earlier versions of pgpool it still made us doubt if using it was the right choice.\n\nThe feedback combined with the issues described above ultimately led to us deciding against using pgpool and using pgbouncer instead. We performed a similar set of tests with pgbouncer and were very satisfied with it. It\u2019s fairly easy to configure (and doesn\u2019t have that much that needs configuring in the first place), relatively easy to ship, focuses only on connection pooling (and does it really well), and had very little (if any) noticeable overhead. Perhaps my only complaint would be that the pgbouncer website can be a little bit hard to navigate.\n\nUsing pgbouncer we were able to drop the number of active PostgreSQL connections from a few hundred to only 10\u201320 by using transaction pooling. We opted for using transaction pooling since Rails database connections are persistent. In such a setup, using session pooling would prevent us from being able to reduce the number of PostgreSQL connections, thus brining few (if any) benefits. By using transaction pooling we were able to drop PostgreSQL\u2019s setting from 3000 (the reason for this particular value was never really clear) to 300. pgbouncer is configured in such a way that even at peak capacity we will only need 200 connections; giving us some room for additional connections such as consoles and maintenance tasks.\n\nA side effect of using transaction pooling is that you cannot use prepared statements, as the and commands may end up running in different connections; producing errors as a result. Fortunately we did not measure any increase in response timings when disabling prepared statements, but we did measure a reduction of roughly 20 GB in memory usage on our database servers.\n\nTo ensure both web requests and background jobs have connections available we set up two separate pools: one pool of 150 connections for background processing, and a pool of 50 connections for web requests. For web requests we rarely need more than 20 connections, but for background processing we can easily spike to a 100 connections simply due to the large number of background processes running on GitLab.com.\n\nToday we ship pgbouncer as part of GitLab EE\u2019s High Availability package. For more information you can refer to \u201cOmnibus GitLab PostgreSQL High Availability.\u201d\n\nWith pgpool and its load balancing feature out of the picture we needed something else to spread load across multiple hot-standby servers.\n\nFor (but not limited to) Rails applications there is a library called Makara which implements load balancing logic and includes a default implementation for ActiveRecord. Makara however has some problems that were a deal-breaker for us. For example, its support for sticky connections is very limited: when you perform a write the connection will stick to the primary using a cookie, with a fixed TTL. This means that if replication lag is greater than the TTL you may still end up running a query on a host that doesn\u2019t have the data you need.\n\nMakara also requires you to configure quite a lot, such as all the database hosts and their roles, with no service discovery mechanism (our current solution does not yet support this either, though it\u2019s planned for the near future). Makara also does not appear to be thread-safe, which is problematic since Sidekiq (the background processing system we use) is multi-threaded. Finally, we wanted to have control over the load balancing logic as much as possible.\n\nBesides Makara there\u2019s also Octopus which has some load balancing mechanisms built in. Octopus however is geared towards database sharding and not just balancing of read-only queries. As a result we did not consider using Octopus.\n\nUltimately this led to us building our own solution directly into GitLab EE. The merge request adding the initial implementation can be found here, though some changes, improvements, and fixes were applied later on.\n\nOur solution essentially works by replacing with a proxy object that handles routing of queries. This ensures we can load balance as many queries as possible, even queries that don't originate directly from our own code. This proxy object in turn determines what host a query is sent to based on the methods called, removing the need for parsing SQL queries.\n\nSticky connections are supported by storing a pointer to the current PostgreSQL WAL position the moment a write is performed. This pointer is then stored in Redis for a short duration at the end of a request. Each user is given their own key so that the actions of one user won\u2019t lead to all other users being affected. In the next request we get the pointer and compare this with all the secondaries. If all secondaries have a WAL pointer that exceeds our pointer we know they are in sync and we can safely use a secondary for our read-only queries. If one or more secondaries are not yet in sync we will continue using the primary until they are in sync. If no write is performed for 30 seconds and all the secondaries are still not in sync we\u2019ll revert to using the secondaries in order to prevent somebody from ending up running queries on the primary forever.\n\nChecking if a secondary has caught up is quite simple and is implemented in as follows:\n\nMost of the code here is standard Rails code to run raw queries and grab the results. The most interesting part is the query itself, which is as follows:\n\nHere is the WAL pointer as returned by the PostgreSQL function , which is executed on the primary. In the above code snippet the pointer is passed as an argument, which is then quoted/escaped and passed to the query.\n\nUsing the function we can get the WAL pointer of a secondary, which we can then compare to our primary pointer using . If the result is greater than 0 we know the secondary is in sync.\n\nThe check is added to ensure the query won't fail when a secondary that we're checking was just promoted to a primary and our GitLab process is not yet aware of this. In such a case we simply return since the primary is always in sync with itself.\n\nOur background processing code always uses the primary since most of the work performed in the background consists of writes. Furthermore we can\u2019t reliably use a hot-standby as we have no way of knowing whether a job should use the primary or not as many jobs are not directly tied into a user.\n\nTo deal with connection errors our load balancer will not use a secondary if it is deemed to be offline, plus connection errors on any host (including the primary) will result in the load balancer retrying the operation a few times. This ensures that we don\u2019t immediately display an error page in the event of a hiccup or a database failover. While we also deal with hot standby conflicts on the load balancer level we ended up enabling on our secondaries as doing so solved all hot-standby conflicts without having any negative impact on table bloat.\n\nThe procedure we use is quite simple: for a secondary we\u2019ll retry a few times with no delay in between. For a primary we\u2019ll retry the operation a few times using an exponential backoff.\n\nFor more information you can refer to the source code in GitLab EE:\n\nDatabase load balancing was first introduced in GitLab 9.0 and only supports PostgreSQL. More information can be found in the 9.0 release post and the documentation.\n\nIn parallel to working on implementing connection pooling and load balancing we were working with Crunchy Data. Until very recently I was the only database specialist which meant I had a lot of work on my plate. Furthermore my knowledge of PostgreSQL internals and its wide range of settings is limited (or at least was at the time), meaning there\u2019s only so much I could do. Because of this we hired Crunchy to help us out with identifying problems, investigating slow queries, proposing schema optimisations, optimising PostgreSQL settings, and much more.\n\nFor the duration of this cooperation most work was performed in confidential issues so we could share private data such as log files. With the cooperation coming to an end we have removed sensitive information from some of these issues and opened them up to the public. The primary issue was gitlab-com/infrastructure#1448, which in turn led to many separate issues being created and resolved.\n\nThe benefit of this cooperation was immense as it helped us identify and solve many problems, something that would have taken me months to identify and solve if I had to do this all by myself.\n\nFortunately we recently managed to hire our second database specialist and we hope to grow the team more in the coming months.\n\nCombining connection pooling and database load balancing allowed us to drastically reduce the number of resources necessary to run our database cluster as well as spread load across our hot-standby servers. For example, instead of our primary having a near constant CPU utilisation of 70 percent today it usually hovers between 10 percent and 20 percent, while our two hot-standby servers hover around 20 percent most of the time:\n\nHere is our primary while the other two hosts are our secondaries.\n\nOther load-related factors such as load averages, disk usage, and memory usage were also drastically improved. For example, instead of the primary having a load average of around 20 it barely goes above an average of 10:\n\nDuring the busiest hours our secondaries serve around 12 000 transactions per second (roughly 740 000 per minute), while the primary serves around 6 000 transactions per second (roughly 340 000 per minute):\n\nUnfortunately we don\u2019t have any data on the transaction rates prior to deploying pgbouncer and our database load balancer.\n\nAn up-to-date overview of our PostgreSQL statistics can be found at our public Grafana dashboard.\n\nSome of the settings we have set for pgbouncer are as follows:\n\nWith that all said there is still some work left to be done such as: implementing service discovery (#2042), improving how we check if a secondary is available (#2866), and ignoring secondaries that are too far behind the primary (#2197).\n\nIt\u2019s worth mentioning that we currently do not have any plans of turning our load balancing solution into a standalone library that you can use outside of GitLab, instead our focus is on providing a solid load balancing solution for GitLab EE.\n\nIf this has gotten you interested and you enjoy working with databases, improving application performance, and adding database-related features to GitLab (such as service discovery) you should definitely check out the job opening and the database specialist handbook entry for more information.\n\nScaling the GitLab database via @yorickpeterse Click to tweet!"
    },
    {
        "url": "https://medium.com/@gitlab/unveiling-gitlabs-new-navigation-91478a0dec60?source=user_profile---------19----------------",
        "title": "Unveiling GitLab\u2019s New Navigation \u2013 GitLab \u2013",
        "text": "We received an incredible number of responses in the issue created to gather feedback. The feedback gave us valuable insight into the many different types of workflows our users have. It reaffirmed some of the decisions made and challenged us to rethink others. Using this feedback, we iterated on the navigation for two release cycles, focusing on the changes that would add the most benefit. Here are some of the high-level additions we made:\n\nFrom the beginning, we knew that the sidebar would need to be collapsible in order to maximize screen space. With the right sidebar present in issues and merge requests, we didn\u2019t want to box you in. The addition of icons enabled us to collapse the sidebar down to a mere 50px.\n\nA fly-out menu has been introduced in order to reduce the number of clicks and the time necessary to access a sub-page. Now, if you want to access Issue Boards, there is no need to click on Issues and wait for the initial \u2018Issue List\u2019 to load. When hovering over a section with second-level items, the fly-out drop-down menu will appear to offer quick access to those second-level sections.\n\nWe\u2019ve also adjusted the hover color of the menu items after many of you expressed that the intensity of the color was harsh and distracting. The colors changed from purple to whites and grays without sacrificing the overall contrast.\n\nNo more clicking on Projects and waiting for the Projects page to load! In order to provide quicker access to projects, a dropdown has been added to the Projects link in the top bar. The dropdown opens on click, following the behavior of the + button and personal dropdowns in the top bar.\n\nThe dropdown contains direct links to the different subsections of the Projects dashboard (Your Projects, Starred Projects and Explore projects). Better still, on the right-hand side of the dropdown is a list of your most frequently accessed projects. A search box allows you to navigate to your projects that are not present in the list.\n\nOn the subject of colors, one of the most requested features was the ability to change the navigation colors. Previous versions of GitLab allowed users to customize the navigation sidebar with a color theme. Many used this to differentiate between different GitLab instances. The new navigation presented the opportunity to bring back this valuable feature! The default palette will remain indigo, based on the GitLab identity. You will now be able to choose between four additional color schemes; Dark, Light, Blue, and Green.\n\nWe received a lot of feedback on the breadcrumbs. While many of you found them to be helpful, many also found them to be repetitive, inconsistent, and taking up too much overall space. We began by removing GitLab from the start of the breadcrumbs and moving all breadcrumb items onto one line. In order to improve the movement between elements in the breadcrumb, we replaced the slashes with chevrons. We also removed the action buttons from the breadcrumb bar altogether.\n\nWhen multiple subgroups are present, we place them inside of an ellipsis button. This reduces the cognitive load while keeping them accessible. For each breadcrumb element, we have fixed the min-width and the max-width to make sure the whole breadcrumb contracts and expands according to the available space.\n\nThe breadcrumb labels themselves are more consistent and intuitive. A list of the paths and corresponding breadcrumb titles can be found in the issue description.\n\nWe reduced the overall header height to give you as much vertical screen space as possible. By popular request, all global links are shown by default and collapse into the \u2018More\u2019 dropdown as space gets tighter. The header active/hover/dropdown styles have been redesigned with a bold new style and Todo/Issue/MR badges are centered to the icons themselves.\n\nWe feel confident that GitLab\u2019s overall navigation has been greatly improved over the last two releases. That is why, as of the 10.0 release, we will remove it from the feature flag and make it the only way to navigate. As always here at GitLab, everything is in draft. We will continue to monitor feedback, test, and iterate.\n\nLooking forward, the UX team has some big things planned. In addition to improving user flows, we are working hard to increase the overall quality and polish of the UX experience. Stay tuned for a series of blog posts dedicated to explaining our processes as we work on the following key initiatives:"
    },
    {
        "url": "https://medium.com/@gitlab/whats-a-vp-of-scaling-d8eee61d252e?source=user_profile---------20----------------",
        "title": "\u201cWhat\u2019s a VP of Scaling?\u201d \u2013 GitLab \u2013",
        "text": "Fast-growing companies sometimes need leadership in new initiatives before there\u2019s time to hire a team member dedicated to them. This is how we tackled this challenge.\n\nIn the last two years GitLab has grown from about 15 people in a handful of countries to now well over 180 people in more than 30 countries. In a company that is growing as fast as GitLab is, there is always some team that needs to be built, or some team to be temporarily led while a leader for the longer term is found, or some initiative to be started that doesn\u2019t (yet) fit within existing teams or departments. We can \u2014 and do \u2014 add people to the GitLab team to tackle these challenges. But hiring takes time and isn\u2019t always appropriate for a one-off or early-stage initiative. GitLab is also a fully remote and international organization that moves fast, and we can\u2019t afford to wait for these challenges to sit idle.\n\nAt GitLab, we\u2019ve addressed this with the role of VP of Scaling. The word \u201cscaling\u201d in this case relates to the organization instead of, for example, sales or user-base. Think of the VP of Scaling as a full-time interim manager rotating between vastly different functions, building teams and scaleable processes. The job is to \u201cget in\u201d and to figure out how to \u201cget out\u201d responsibly. (As an aside: at first we struggled to come up with a good name for this role and considered everything from janitor/plumber (sweeping /connecting the entire company \u2014 vetoed), to Mr. Wolf (fixes problems on demand \u2014 too negative), until eventually settling on the key word of \u201cscaling.\u201d)\n\nA VP of Scaling should be broadly deployable in the company and go where the challenges are. For us, the first task at hand was to scale up our team, starting with our ability to recruit and hire quickly and efficiently. And so it was that I began in the role of Interim Head of People Operations; from sending out employment agreements and setting up an applicant tracking system, to laying the groundwork for our hiring process, building the beginnings of the People Operations team, and developing the first iteration of the global compensation calculator.\n\nOnce the People Operations team was left in more experienced hands I moved on to help as (interim) Support Lead, followed more recently by time as interim Director of Infrastructure, and currently interim Director of Security. With each of the teams that I\u2019ve worked with, the challenges they\u2019ve faced are a direct result of the success of the company. The Support team \u201cfeels\u201d it through more customer tickets, and the Infrastructure team \u201cfeels\u201d the increased usage of GitLab.com. Although no two teams are identical, there are some common approaches that I have found to be helpful in an interim leadership role.\n\nPerhaps the most important point is to listen to the team \u2014 and to never stop asking questions. The individuals in our team are smart, they have domain expertise, and they often have great ideas on what needs to be done in order to be successful as a team. Regarding the \u201cnever stop asking questions\u201d part, well, I think I\u2019ve had that bit covered ever since I had the ability to talk.\n\nComing onboard with a new team, I listen to the concerns and ideas from the team and from the management chain that they report into, and sort the challenges into those that need to be addressed right now (e.g. add more people to the team through hiring or borrowing; unblock a decision on topic X) from those that need to be addressed on a longer timescale. Once the immediate needs are taken care of, with the help of the team and sometimes outside experts we start sketching out what Utopia looks like for this team. What does the team, and the service the team provides, look like in a world where GitLab is 10x more popular? How about 100x?\n\nFor example, the Support Team faces the dual challenge of a growing customer base as well as a growing product in terms of product scope and capabilities \u2014 straining the team. The \u201cright now\u201d solution involved adding support turbos and hiring people in multiple timezones to spread the customer ticket load evenly. To make it scaleable beyond the immediate needs is part of the Utopia for any team. In this case, our Support Engineers iterated quickly with the new hires to enable a mostly self-guided onboarding process as well as self-guided pathways for continuous learning.\n\nJumping from team to team in an interim role also provides for a great opportunity to help spread best practices from team to team, and to erase or manage \u201cinterfaces\u201d between teams. For example, the Support Team feels the customer\u2019s sense of urgency around needing bug fixes or feature development, but did not have a great way to effectively communicate that sense of urgency to the rest of the team without just making a lot of noise. So the team came up with a quantitative metric using issue priority labels, with good success. When we noticed that the Infrastructure team \u2014 as the largest \u201ccustomer\u201d of GitLab Enterprise Edition \u2014 was having similar escalation problems, it was easy to adopt priority labels for security as well as availability and performance.\n\nA key challenge (and attraction) of this role is that I need to get up to speed quickly on areas of the company and product in which I do not have much prior experience. I rely on the kindness and the expertise of the team, and benefit a lot from our dedication to documenting everything (which we do as an integral part of being successful in a remote-only setting). Of course I contribute back to this documentation as well: as we worked on reducing the latency of GitLab.com, I found myself wondering, \u201cWhat actually happens when a user enters a GitLab.com URL in their browser?\u201d and then documented the answer(s) on our handbook page about GitLab.com performance. Another challenge is, unsurprisingly, that I get somewhat attached to the teams that I\u2019m actively working with. I enjoy learning from them, I enjoy working with and enabling them, and I enjoy getting to know the people behind the GitLab handle. It can be difficult to fully move on to the next assignment, with a few pending issues tenaciously hanging on to my todo list for way too long.\n\nDespite the odd job title and the fluid nature of the job itself, I like to think that it has worked well for us here at GitLab. Do you have a similar role at your company? We\u2019d love to hear about it!"
    },
    {
        "url": "https://medium.com/@gitlab/the-evolution-of-gitlab-issue-board-aae84c6a9793?source=user_profile---------21----------------",
        "title": "The Evolution of the GitLab Issue Board \u2013 GitLab \u2013",
        "text": "Collaboration is driven by conversation. It should be a natural and integrated practice throughout the development lifecycle, not a manual, contrived process that requires administration and maintenance. By integrating every step of the software development lifecycle and providing all the necessary collaboration tools, GitLab by default provides all the capabilities modern development teams need to support cross-functional collaboration at scale. GitLab was created so teams could focus more on their work, not on configuring their tools.\n\nLast year, we announced the first iteration of the GitLab Issue Board, a major milestone in our mission to create an open source and integrated product for modern software development. Built on top of our integrated issue tracking system, it became possible to visualize your work and customize your workflow inside of GitLab.\n\nToday, GitLab comes with everything you need to plan and track projects and releases, including issues (tracker and board), milestones, burndown charts, chat integration, and more. Communication is centralized, plans and progress are visible, and work is linked, making collaboration frictionless.\n\nTo celebrate our progress over the past year, no small thanks to the community and feedback from our customers, we wanted to take a look at where the Issue Board has gone since we launched its first iteration in GitLab 8.11.\n\nAt GitLab, we practice Conversational Development. Our software development is centered around conversations on what can be improved, how to implement it, whether or not it worked, and if it achieved the expected value. Instead of waiting months to release a \u201cperfect\u201d feature, we work on smaller, functional changes that can get into the hands of our users much more quickly. There\u2019s a few reasons why we develop software this way: in addition to being able to deliver value faster, we can also react to the market and iterate faster with more frequent feedback loops, and if something goes wrong, it\u2019s easier to spot and fix the problem.\n\nThis is how we built the GitLab Issue Board. We started by shipping the basic functionality needed to allow users to visualize and track their issues. Over the last 12 months, we\u2019ve released small changes every month. Today, the GitLab Issue Board has everything you need to plan and track your projects and releases.\n\nHere\u2019s how we built it over multiple monthly releases:\n\nThe GitLab Issue Board is released in 8.11. Built on top of our integrated issue tracking system, it uses labels from issues to create lists on a board. You can drag and drop lists to organize your workflow, move issues between lists, and labels are updated automatically as you move them across the board.\n\nUsers now have the ability to create workflows inside of GitLab.\n\nMultiple Issue Boards are released in 8.13. Users can now create multiple workflows, allowing different teams to create their own customized boards with the same issues. Once a board is finished, you can leave it as is to review later, or recycle it.\n\nNew search and filter interface is added to the Issue Tracker in 8.16, making it easier for users to search and filter their issues by different attributes such as author, assignee, milestone, and label.\n\nThe new search and filter interface is added to the Issue Board in GitLab 8.17, improving usability. A modal window is added to display all issues that don\u2019t belong to a list for easier search and filtering. Issues can be added to a list from the modal, and issues can be removed from a list on the board.\n\nMilestones are added to the Issue Board in GitLab 9.0 enabling users to organize issues into cycles or sprints with a start date and deadline. Issues can be filtered on the board by milestone, or new boards can be created for individual milestones.\n\nThe ability to reorder issues in a Board list is also introduced in 9.0. Now, users prioritize issues within a list simply by dragging and dropping the issue card.\n\nStart building your project and release management workflows using the Issue Board.\n\nIn this video, Discussion Product Manager Victor Wu demonstrates how to use GitLab Issue Boards for Agile and Scrum-style planning and tracking.\n\nEveryone has an Issue Board story. Maybe you spent months on a long conversation to get your legal team to embrace the Kanban and promptly blew their minds. Maybe your team is full of diehards driven by strongly held opinions over exactly how many stages yours should have. Maybe you\u2019re a remote worker and your issue board is one of the main ways you keep up with teammates spread across the globe.\n\nWhatever your story is, we want to hear from you! Help us celebrate a year of the GitLab Issue Board by sending us your Issue Board story for your chance to win free GitLab swag. We\u2019ll tweet out our favorites and announce the winners in mid-September."
    },
    {
        "url": "https://medium.com/@gitlab/9-attributes-of-successful-development-teams-f717e6337112?source=user_profile---------22----------------",
        "title": "7 Attributes of Successful Development Teams \u2013 GitLab \u2013",
        "text": "There\u2019s no substitute for the work of the human brain, but by automating some of the more time-consuming (and sometimes tedious) aspects of a developer\u2019s job, you not only free up time that could be spent on other, more creative tasks, but you ensure that your software development process is easily repeatable, and consistent every release cycle. \u201cWe have a big release every month,\u201d says GitLab Platform Backend Lead, Douwe Maan, \u201cbut we have numerous patch releases. We have tons of scripts in our release tools repository that automate that so that we don\u2019t miss little bits and so that it\u2019s identical and reproducible every time.\u201d Automation also means doing things like leveraging continuous integration to run scripts, so a portion of your code review is taken over, offloading work that would otherwise be done manually.\n\n\u201cDocumentation of processes and guidelines is a way of kind of scripting or automating your team\u2019s behavior,\u201d explains Douwe. Because we\u2019re a distributed team, it\u2019s important that if a question comes up often, that there\u2019s somewhere people can find what they need without having to wait for team members in other time zones to come online and answer a question \u2014 this saves time not just for the person with the question, but for others who are spared answering the same questions over and over. \u201cIf people on my team have a certain question or things that are blocking them repeatedly, that probably means it\u2019s something we should either document better or invent a process around,\u201d says Douwe.\n\n\u201cAs a consequence of that, for example, we have the process around getting merge requests and issues into a patch release when it\u2019s a regression or security issue and so on. At one point we had so few people that we could just say, \u2018Hey that needs to go into the patch release,\u2019 but of course as the team grows, that doesn\u2019t scale anymore, because you\u2019d have 20 people asking one person. To address that we invented the process around labels, where we use labels and milestones very heavily in GitLab to signal to people what has to happen with an issue.\u201d\n\nHaving space for comments, questions or suggestions at each stage of the cycle is critical for fostering collaboration and making sure that everyone can follow the latest progress of a project. \u201cFor us, the single source of truth is always the issue. Tools like labels, milestones, assigning to people, these all make sure that the right person knows when it\u2019s their turn to do something, and the handoff happens through issue comments,\u201d explains Douwe. This is another area in which we\u2019ve had to become disciplined about documenting the latest update on an issue, because as a distributed team we can\u2019t just walk over to a colleague\u2019s desk to check something. \u201cWe don\u2019t have the problem where after two days people will be like, \u2018Hmm, what did we decide again?\u2019\u201d says Douwe. Discussion takes place right in the same environment where we\u2019re working \u2014 whether it\u2019s a comment on an issue or a merge request or an inline question on the code itself \u2014 so it\u2019s easy for everyone to see the context for it and to refer back to it later.\n\nHaving all your software development tools in one environment reduces context-switching, tiresome maintenance when APIs change, and administrative complexity. It also makes the development process smoother, as an integrated tool often offers shortcuts within the UI that would not exist if you were using two separate products.\n\nDouwe explains: \u201cFor a long time GitLab CI used to be a separately deployed web application: the UIs were not integrated in any way and they felt like separate products, as if they were not even made by the same company. At one point we decided to integrate it, and literally within one or two weeks we started seeing new possibilities of interlinking these applications. We\u2019d think, \u2018Hey, wouldn\u2019t it be neat if we added a button to the latest status of this page?\u2019 which previously is something we never would have thought about, because we really thought of the two as separate products that need to talk over an established channel, instead of just putting a link in everywhere where it would useful to have a link to CI. So with a built-in solution, the integration you get is not just tighter, it\u2019s integrated in ways which other, separate products being developed by siloed development teams would never think of. We\u2019re not just approaching it as an issue tracker, a code review tool or a CI tool, we\u2019re seeing it as a development environment.\u201d\n\nUsing version control for source code is widely accepted as a good idea, but it\u2019s useful for a number of other purposes too. Take documentation, for example: if you use a wiki, there is no concept of a merge request. \u201cThere\u2019s no way of suggesting an improvement without immediately making it,\u201d says Douwe, \u201cso what that means is that in a lot of places that use a wiki, changing it is kind of scary.\u201d This creates a feeling that suggesting updates or improvements is for senior team members only, and discourages participation and collaboration on the documentation. \u201cUsing source control here means that even the most junior person who\u2019s like, \u2018Hmm, I spotted a typo here,\u2019 or \u2018Hey, this wasn\u2019t super clear, let\u2019s do it like this,\u2019 won\u2019t be hesitant to bring up that suggestion.\u201d Having time to write a merge request that clearly outlines the advantages of what they\u2019re proposing makes it less intimidating to suggest a change. \u201cIt really makes the documentation, similar to the source code, an open source and living document that everyone can contribute to.\u201d The same principle applies to things like your CI/CD configuration, tests, and infrastructure code.\n\nImproved collaboration and learning opportunities aren\u2019t the only benefits of using version control for things beyond your source code: the ability to roll back in the event of something breaking, and to pinpoint where a bug was introduced are advantages both in that it\u2019s easier to fix something, but also in that team members feel freer to experiment without the fear of causing irreparable damage if something breaks.\n\nBy opening up your development platform, other team members can discover, contribute to and learn from other team members\u2019 work. \u201cIf you hide your CI configuration in an area that can only be accessed by the masters of the project, that means that very few developers on the team and especially very few developers on other teams will ever see that config,\u201d says Douwe. \u201cYou really shouldn\u2019t see your code as just a product of your team, you should also see it as a resource for everyone else in the company. If you ask a developer how they learned to code, most of them will not mention university or this book that they read, most of them will mention \u2018code I read that was written by people who have more experience than me.\u2019 So by giving them access to as much code as you can, that will actually make them better coders than if you have them work in a silo.\u201d\n\nThis isn\u2019t just about less experienced developers learning from more experienced developers. Sometimes a fresh perspective from someone who isn\u2019t as close to a project can spark solutions that aren\u2019t apparent when you\u2019ve been deep inside the code for a week. \u201cLike if someone reads code and they ask a question like \u2018Hey, what\u2019s the reasoning behind this? I just found your code and I was wondering\u2026\u2019 it enables a conversation.\u201d Douwe explains. \u201cIt helps us avoid Not Invented Here syndrome.\u201d Someone from another team might have already done work you require on a previous project of theirs \u2014 why not use it on your team? \u201cIt\u2019s working together to make all of our code better, because if we use a shared library \u2014 even if it\u2019s just a company internal one, like innersourcing \u2014 if one person improves it or fixes a bug or increases the functionality of that application, that\u2019s work by one person that will immediately affect all the different teams.\u201d\n\n\u201cWe don\u2019t have explicit 20 percent time,\u201d says Douwe, \u201cbut at GitLab, as we are working on improving the same platform we use to do all our work, our job is to make our own job easier. Which means that even if something is not scheduled for this release, if it\u2019s something that you think you can get done in couple hours, and it would save you more than a couple hours in the future, just do it.\u201d\n\nSometimes going through the formal process for approving time to work on a new feature just isn\u2019t necessary. \u201cUnless you have something urgent you should be working on, it really helps to ask developers to feel responsible for the product, and also own the product in the sense that they can also suggest new features, and even spearhead the development of them without going through product management for example,\u201d says Douwe.\n\nHaving your work reviewed can feel like a personal judgement on whether you\u2019re good enough or not. When code review is about \u201cThis is wrong, change it to this,\u201d it can be really demotivating. Douwe explains: \u201cA much better way, even if it seems like something is obviously wrong, would be to ask, \u2018What do you think about changing to this/Did you consider X, Y, Z/I suggest changing it to this, if you think that makes sense.\u2019 The communication there really helps and it also means that people don\u2019t feel review is the time where they as person are being told if their work is good enough, if they are good enough, it\u2019s really just talking about the actual code, the implementation, the best way to solve a problem.\u201d Everyone on a team is free to review each other\u2019s code or to ask for a review, so it becomes about just improving the merge request instead of passing a judgement about that person\u2019s work. \u201cIf review feels like something that\u2019s just done by the higher-up people and it\u2019s a time when your code is deemed perfect or not, it might feel kind of scary to review someone\u2019s code, especially if that someone is more experienced than you or has more experience in this area of the application. What really helps with collaboration is having everyone feel free to question each other\u2019s code or question, \u2018Is this the best way to go about this?\u2019 without saying \u2018This is wrong.\u2019\u201d\n\nProblem solving is what developers do. So if a customer has a request for some feature that they would find useful, it can be helpful to ask the team to solve the problem the customer is experiencing, rather than presenting them with a spec which might not be the optimal solution. \u201cIn a lot of cases developers are aware of solutions either that already exist within the codebase or that exist in other people\u2019s codebases because of innersourcing, or they might even have just read about this cool, open source tool, which a product manager might not be aware of,\u201d says Douwe. \u201cSo allowing developers to be really critical of the proposal and having product managers not be too rigid in their specs also gives you better code and makes for happier developers.\u201d This can also help you to build features that don\u2019t just address one particular customer\u2019s problem and fix it the way they would like, but rather work on solutions that can be useful to everyone.\n\nTo learn more about what makes development teams successful today, watch our webcast, \u201cManaging the DevOps Culture Shift\u201d on demand."
    },
    {
        "url": "https://medium.com/@gitlab/invite-your-engineers-to-talk-business-heres-why-485ce02c4d18?source=user_profile---------23----------------",
        "title": "Invite Your Engineers to Talk Business. Here\u2019s Why.",
        "text": "Every business today is a technology business. Whether it supports your business model or itself is the business model, your technology stack plays a key role. What your engineering team ships, and how they ship, has direct and measurable consequences for your business, even if they traditionally prefer to think otherwise. Engineering goals are now closer than ever to business goals, and understanding that will help companies thrive in the business technology world today.\n\nThe idea of software craftsmanship dictates that the profession of software engineer should be analogous to that of civil engineer. Increased professionalism (in the style of Uncle Bob) suggests that software engineers\u2019 work should carry the same weight, and their failures the same consequences, as physicians and civil engineers.\n\nThese are noble goals for software engineers, especially if you are building software to operate a bascule (draw) bridge! Unfortunately, they are challenged daily by business managers who just want functionality, but may not care about the underlying mess supporting it; they think that engineers who constantly try to clean up that mess are wasting time chasing after esoteric technical standards. Engineers, on the other hand, want to always deliver and maintain quality work, and they are less concerned about the business and the end users. This lack of communication and alignment creates distrust and inefficiencies \u2014 it just doesn\u2019t work.\n\nA better approach that more and more organizations \u2014 including GitLab \u2014 have embraced is lending a greater business voice to your engineering organization, and of course demanding greater responsbility. Have your engineering team communicate the benefits of maintaining quality code, and why the costs of increased engineering resources (people and time, typically) are justified by tangible business outcomes. Along with a clean and adaptable codebase, quality code allows you to:\n\nNote that all the benefits above are business benefits! Your software engineers already know these are possible and goals that they themselves already aspire to achieve.\n\nThe short answer is: it depends on the company. For mature organizations shipping well-established products, it\u2019s likely the market cannot absorb shocks or big changes in the product. Performance, security, and privacy are very important, especially for highly regulated industries. Code quality is crucial for these companies, and luckily, they typically have more resources to easily handle the cost of maintaining that quality. Code quality should always be the rule, with very little exceptions, and engineering and business should communicate along those lines.\n\nIn either a small startup or a large organization shipping a product quickly to compete, I\u2019d argue that you can and should skimp on code quality. Engineers may not be a fan when reading this, but this draws on the concept of technical debt or tech debt. Sometimes a startup has to strategically incur tech debt to push out a product or feature update more quickly, to get the timing just right.\n\nIf you instead choose to maintain that 100 percent quality code, you lose out on the opportunity, and don\u2019t even have a business in 2 months, totally defeating the purpose of your software development in the first place. You\u2019d have gained a beautiful codebase, but lost a viable business. Just as any healthy business regularly pays back old debt and takes on new, you need to clean up previously incurred tech debt so that your codebase doesn\u2019t slow down your business.\n\nIn the past, engineers were often removed from business discussions. Business managers established requirements, and threw those over the wall to engineering managers who would reciprocate with delivery timelines. In today\u2019s technology-driven business landscape, that\u2019s no longer a viable option for success. Both parties need to work closely together in order to survive in a competitive software world.\n\nMaintaining code quality is a great bridge in this effort, since engineers can easily articulate its practical benefits that are readily translated into positive business outcomes. You should continually analyze your business and take advantage of new tech debt as the need arises. There is a lot of technical nuance to this delicate balance, and so your engineering team should be presenting the options and offering a strong recommendation. Your engineers need to be strong business leaders."
    }
]