[
    {
        "url": "https://medium.com/@richardherbert/in-search-of-the-autoencoder-a-machine-learning-odyssey-part-1-28163f9f80e3?source=user_profile---------1----------------",
        "title": "In Search of the Autoencoder: A Machine Learning Odyssey, Part 1",
        "text": "So wrote Paul Churchland in his seminal 2007 work, Plato\u2019s Camera, on many-tiered auto-associative networks, many years preceding the current deep learning craze. In fact, neural networks make up a substantial proportion of the book, which is ostensibly about how human brains \u2014 and not machines \u2014 learn to comprehend the world.\n\nYet, despite the inevitable warnings and cautions to not take the brain-to-computer metaphor too literally \u2014 especially with neural networks, whose similarities to neurons mostly stop at their name \u2014 it nonetheless persists as one of the most accessible and efficient ways of reasoning about organic minds.\n\nFor nearly two years now, I have been in search of the general visual autoencoder. I\u2019m not a math major, researcher or PhD, so you may all deride me in the comments for my lack of theoretical background, but I nonetheless spend most of my spare time running deep models \u2014 mainly in the form of the auto-associative networks given so much attention to in the early parts of Plato\u2019s Camera. They are also known as autoencoders.\n\nAn autoencoder performs a deceptively simple task. It takes in an input, compresses it, and then reconstructs it. Most of the time, the compression will take place by passing the input through many layers of parameters (or weights), before decoding it back to its original form. This compression is very lossy, meaning it loses lots of information along the way, and so even the best autoencoders tend to perform far worse than popular compression algorithms like JPEG or MP3.\n\nWhat\u2019s interesting about autoencoders is they learn the compression mechanism itself, unsupervised. Presuming you know how to shape your inputs, the same network could learn to encode and decode images, audio, or even written text. Through backpropogation, which is the slow and subtle process of altering the parameters of your network to learn the distribution of your data, a well-tuned network will reach convergence, where the weights are in an optimal state for the task at hand. A network that reaches the point where it can not minimize its loss any more has reached a minima \u2014 the global minima if tuned well and a local minima if it gets stuck somewhere else.\n\nWhat is the point of modeling a network to do something general algorithms have already solved, like encoding and decoding an image? Let\u2019s get back to Paul Churchland, who discusses an experiment by scientists named Usui, Nakauichi, and Nakano, wherein they \u201cconstructed a \u2018wine-glass\u2019 feed-forward network whose 81 input \u2018cone cells\u2019 sampled an input reflectance profile at 81 distinct wavelengths.\u201d"
    },
    {
        "url": "https://medium.com/@richardherbert/the-future-of-language-is-video-6da29d10590e?source=user_profile---------2----------------",
        "title": "The Future of Language is Video \u2013 Richard Herbert \u2013",
        "text": "Most recent attempts at synthesizing language involve recurrent neural networks, particularly long-short-term memory units and gated recurrent units (LSTM and GRU henceforth). But while these are impressive at mimicking everything from words to grammar to certain global styles, and produce sentences that \u201clook\u201d like sentences, they fail to achieve convincing semantic or narrative content. There is no clear story or much cohesion to these models\u2019 output.\n\nThis not news to anyone in the field. Such a model would be a momentous achievement in artificial intelligence.\n\nThat being said, I believe such a breakthrough is currently within our grasp, or will be very soon. But it will take a decoupling of our conceptions that teaching a computer language should be rooted in text. In contrast, what if producing a bot that forms full sentences, follows one thought with another in logical progression, and maintains coherent semantic content is a matter not just of language, but of video?\n\nMore specifically, of empirical spatio-temporal analysis. To think of language, narrative, and logic \u2014 the foundation of human textual knowledge \u2014 as purely dependent upon written words themselves is to overlook a crucial aspect of human development, on which the production of language ultimately depends: the physical world through time.\n\nBabies learn to understand before they learn to speak. We can communicate with them well before they can successfully communicate back. Babies also do something profound: they learn to represent the world without highly-specific linguistics. They understand the concepts of cup, of bunny, of talking, of play, of dinner, of spoon before they learn the words to identify them. The addition of linguistic monikers and grammatical abilities unleashes the power to communicate and label relationships they already implicitly understand; it does not build those relationships from scratch.\n\nThis does not mean that language cannot hone, alter or create relationships on its own. It absolutely can. But humanity\u2019s most salient tool for the conceptualization of relationships and forms comes down to a complex process composed of sundry, self-referencing perceptual abilities rooted in the empirical world shifting through time.\n\nTo deconstruct that, what might ultimately be the most important step in language generation and convincing chatbots is not the prediction of thought and concepts purely through textual analysis and generation, but of a recurrent feedback loop between spatio-temporal and linguistic models.\n\nTo put this in more concrete terms, let us suppose that I would like to create a bot that can tell a story. My first impulse, based on what I know of the current literature in machine learning and artificial intelligence, would be to run an LSTM or GRU network on a huge dataset of collected stories by various authors. This model could, in theory, capture both grammatical and temporal qualities of language \u2014 meaning each predicted character or word would be a product of the probabilities of all characters or words that came before it \u2014 but it would fail to produce a coherent narrative, if any narrative at all, and likely would make no semantic sense.\n\nThere have been some interesting examples of this technology. The Google bot trained on screenplays can answer some simple questions like \u201cwhat is the meaning of life?\u201d. Another bot named Benjamin produced a screenplay that was turned into a short film. But to create a model that could come up with a coherent original story, one that mimicked a human\u2019s imaginative ability to \u201cspin a yarn\u201d, doesn\u2019t seem possible given this structure.\n\nThis is because although an LSTM can learn rules about what is allowed and not allowed in text through time, it is not learning anything about the world the text is referencing through time. It understands that a sentence should contain a subject and a predicate, but it knows nothing about those concepts beyond the space they take up in said sentence. Their relationship is purely linguistic. Human language, on the other hand, also embodies an imaginative physical space, where we can \u201cpicture\u201d what happens and use our conceptions, senses, and memories of the real world in order to parse and validate words spoken by others, or even ourselves.\n\nA model that could not only understand how to structure language, but also how to speak in a way that maintains verisimilitude to the physical world humans embody through time would come much closer to passing the Turing test than ones based on a hard-coded or otherwise stringent set of rules defining what it can or can\u2019t say. I believe we have the tools to achieve this, or almost do.\n\nThe proposed story model is rooted in two burgeoning machine learning fields: video-to-text prediction, and video-to-video prediction. Important to state at the outset is that the video prediction, otherwise denoted here as spatio-temporal prediction, does not need to achieve pixel accuracy. Like human memory and imagination, it would require only the ability to recognize unique forms moving through time.\n\nSuppose I feed the model a starting sentence (conceivably, it could also come up with a starting sentence of its own, based on an understanding of how stories typically begin, which it seems is feasible even for current recurrent modules). For illustration, we will feed it \u201ca woman walks into a room\u201d. Immediately, the model builds a spatio-temporal representation of said sentence as time-series that could be decoded into video. Using that time series it then \u2014 based on the spatio-temporal representation and not the starting words \u2014 makes a prediction or analysis in video of what might happen next.\n\nBecause the model has been trained on realistic videos of human behavior, it has some conception of not only what humans are likely to do in a behavioral sense, but what they can or cannot do in the physical world. Presumably, it also understands many similar axioms about various non-human objects and relationships, as well. So it generates a video prediction of the next few moments. Perhaps to us it would look like a woman turning on a light.\n\nThis spatio-temporal prediction is then fed back into a video-to-text module, which due to its recurrent nature \u201cremembers\u201d the starting text and knows how to concatenate onto it. The result is \u201ca woman walks into a room and turns on a light\u201d. Not only does this make syntactic and grammatical sense, it also makes semantic sense. Doing this through enough time steps might produce a story that maintains narrative cohesion and characters whose actions have strong fidelity to the conceptual properties of our shared physical universe.\n\nOne sentence is, of course, not enough. The first line of Benjamin\u2019s screenplay is \u201cWe see H pull a book from a shelf, flip through it while speaking, and then put it back.\u201d This satisfies the requirements of semantic and grammatical coherence, as well as being physical plausible. But Benjamin cannot layer sentences on top of each other to form longer stories the way our theoretical spatio-temporal model would be able to.\n\nIn short, a video representation acts as a collated form of sensory memory for the computer, which can then be checked against the text, to form an \u201cagreement\u201d. Our conscious brains do these kind of checks-and-balances between sensory, linguistic, logical, and emotional modules all the time. They parse out the most accurate version of reality; the one that can be best corroborated by all of the sundry evidence that we have acquired.\n\nThe physics of consciousness is far from being solved, \n\nbut to get machines to talk to us in ways that appear human won\u2019t require a complete replica of a human brain. Like we don\u2019t have to reconcile relativity and quantum mechanics to accurately simulate the trajectory of a ball being thrown, neither should we have to reverse engineer exact neurological processes to accurately simulate the human thinking and speaking process. But we may need to understand how tightly integrated many of those processes are with the physical world, or our perception of it."
    },
    {
        "url": "https://medium.com/@richardherbert/faces-from-noise-super-enhancing-8x8-images-with-enhancegan-ebda015bb5e0?source=user_profile---------3----------------",
        "title": "Faces from Noise: Super Enhancing 8x8 Images with EnhanceGAN",
        "text": "Earlier this month, Google released a paper detailing a method for producing 16x image super-resolution from minute amounts of pixel data (8 x 8 images). By combining their ResNet deep convolutional network with their PixelRNN architecture, the latter of which using layered Long Short Term Memory Units to construct pixel-by-pixel image generations, Deep Mind was able to both upscale and enhance extremely low-res pictures of celebrities and bedrooms to 32x32 pixels, adding detail where there was none originally. Though it can\u2019t be considered upscaling in the traditional sense, as there is often too little perceptual information to expect pixel-perfect reconstructions, predicting likely high-resolution images from low-resolution pixel data has many obvious potential applications, not to mention the broader potential for generative models dealing with sundry low-dimensional or corrupted data.\n\nCurious as to whether or not an adversarial network could produce similar results, I have spent this month working on EnhanceGAN, a three-stage image super resolution and enhancing deep deconvolutional network in Torch7.\n\nEnhanceGAN has three stages of training, similar to that of a deep belief network, with combined adversarial nets and autoencoders in place of Restricted Boltzmann Machines. In simpler terms, I trained three separate models, with Model A taking in 8x8 images from the CelebA dataset and outputting its best 32x32 guess. Model B receives the output from a saved version of Model A and trains as a traditional autoencoder, compressing its 32x32 input images and then decoding them back to their original size. This has the effect of Model A laying out the rough, broad strokes of a celeb profile, with Model B then refining the shape and details to something more convincing.\n\nFinally, Model C, consisting of six residual blocks, is placed atop Model A and B and all three are trained contiguously in a final stage of optimization. The residual net sharpens and enhances facial, lighting and environmental detail, making the generated images more photorealistic. All three stages use the ground truth 32x32 images as their targets.\n\nThe first two stages/layers share the same architecture except for their input dimensions (thus, the stage two encoder involves more deconvolutions). Deconvolutional layers encode and decode by backpropogating error based on \u201cperceptual loss\u201d on the ground truth image. To achieve this, I took inspiration from SRGAN and Neural Algorithm of Artistic Style and trained the autoencoder by running mean squared loss on features obtained from the publicly available VGG19 network weights. Perceptual loss (loss based on feature detection, instead of pixel-by-pixel reconstruction error), has shown to produce more realistic image reconstructions, and my results bear that out.\n\nOn top of a perceptual loss, the encoder and decoder also train with separate adversarial loss: an adversarial autoencoder (AAE) and generative adversarial network (GAN), respectively. The GAN adds additional information to the face and is a replacement for the PixelRNN stage of the Google model, while the AAE provides stability and nicer image generations, a neat bit of extra functionality for EnhanceGAN.\n\nThe final stage is a refinement layer consisting of six residual blocks, taking in the sigmoid-activated output of the first two stages. There is no upscaling or downscaling, and every block is followed by a ReLU activation except the last. There is no adversarial loss over either decoder in this stage; it is simply an AAE whose total error backpropogates over all three layers.\n\nAlthough in a previous VAE+GAN version of my model this final layer did not seem to improve results, in EnhanceGAN it makes an obvious and important difference in the quality and realism of the output images.\n\nEnhanceGAN is able to capture an entire celebrity profile, including hair, clothing and occasionally backgrounds, though has varying degrees of success with side views and faces obfuscated by objects, limbs or hairstyles, presumably due to the sparsity of training examples. EnhanceGAN\u2019s deconvolutional generations do not seem to achieve the same level of detail as autoregressive and pixel recurrent architectures like PixelRNN\u2014 there remains a fair amount of noise, which makes them easily distinguishable from the ground truth images \u2014 but they are easier to understand and more efficient to train, generate and scale. Ultimately, this is a relatively simple experiment running on a single AWS instance, but I think it shows the potential for deconvolutional and adversarial nets to efficiently \u201cgenerate\u201d probable high-resolution data based on noisy or minimal input data.\n\nIt will also be very interesting to continue to explore this kind of \u201cdeep belief\u201d architecture for autoencoders, where separate models enhance and refine the output of the last, and are only trained together optionally at the end. This allows for each model to learn functions unique to different stages of image development, and would seem a natural fit for generative or creative machine learning, as the human brain and body presumably uses a staging process for any kind of generative task. A similar idea is used in StackGAN to produce convincing pictures of birds and flowers based on text embeddings alone."
    },
    {
        "url": "https://medium.com/@richardherbert/generating-fine-art-in-300-lines-of-code-4d37218216a6?source=user_profile---------4----------------",
        "title": "Generating Fine Art in 300 Lines of Code \u2013 Richard Herbert \u2013",
        "text": "[Full code for the model can be found here]\n\n[This article was originally posted here]\n\nStumbling onto Alec Radford\u2019s deep convolutional generative adversarial network, or DCGAN, was one of the few genuinely jaw-dropping moments I\u2019ve experienced in my life. It needs no further introduction; visit the link and see the madness.\n\nOr check it out right here:\n\nThe above images, clearly of bedrooms, were not captured by cameras, but by statistics. They are generated from what you might call the \u201cprobability space\u201d of bedroom pictures; the statistical distribution that contains all the various features you would expect to find in a picture of a bedroom, neatly separated so that a random sample from said distribution can be \u201cmorphed\u201d into something that resembles a photograph.\n\nThe way this is done is both shockingly simple and dreadfully complicated. It\u2019s simple in that it doesn\u2019t require a ton of code, appears to work on various kinds of image data, and can be run on a medium-grade GPU to start producing interesting results in a few hours. It\u2019s complicated because modifying, debugging or adding to the model requires understanding the myriad kinks of generative adversarial networks, which are slightly odd beasts.\n\nI have stitched together the Torch implementation of DCGAN and Kaixhin\u2019s variational autoencoder, the latter of which performs variational inference on complicated integrals like the ones that define photographic probability distributions and can be trained with vanilla stochastic gradient descent. By tinkering with the heuristics, I was able to successfully generate fine-art-like images from the wikiart.org dataset, which was compiled by Small Yellow Duck and hosted on Kaggle. The examples below are picked from thousands upon thousands of samples generated by the model and do not exist in the training data.\n\nWhile the base of the DCGAN remains the same, I have changed a few things, as well as added the variational autoencoder. Compelled by a post on stabilizing GANs, I have both set limits on the generator and discriminator, so they only update when they or their adversary is performing either particularly well or particularly poorly, as well as added noise to the inputs feeding into the discriminator. Simulated annealing then lowers the amount of that noise as training progresses to encourage convergence of the GAN.\n\nThe variational autoencoder uses a modified version of the discriminator to produce the latent variables, which then feed into the generator, so the entire model is convolutional. All three networks contribute equally to the loss, although the generator and discriminator are not always updating their gradients (due to the balancing act that must be played to keep them from outperforming one another). All these tricks combined seem to work consistently and stably for up to 64x64 color images, and although I suspect convergence is possible for larger dimensions, I have yet to successfully do it myself."
    }
]