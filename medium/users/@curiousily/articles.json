[
    {
        "url": "https://medium.com/@curiousily/solving-an-mdp-with-q-learning-from-scratch-deep-reinforcement-learning-for-hackers-part-1-45d1d360c120?source=user_profile---------1----------------",
        "title": "Solving an MDP with Q-Learning from scratch \u2014 Deep Reinforcement Learning for Hackers (Part 1)",
        "text": "Here\u2019s an example of how well-trained agents can act in their environments given the proper incentive:\n\nWhy do we need the discount factor ? The total reward that your agent will receive from the current time step t to the end of the task can be defined as:\n\nThat looks ok, but let\u2019s not forget that our environment is stochastic (the supermarket might close any time now). The discount factor allows us to value short-term reward more than long-term ones, we can use it as:\n\nOur agent would perform great if he chooses the action that maximizes the (discounted) future reward at every step.\n\nIt would be great to know how \u201cgood\u201d a given state is. Something to tell us: no matter the state you\u2019re in if you transition to state your total reward will be , word! If you start from and follow policy . That would spare us from revisiting same states over and over again. The value function does this for us. It depends on the state we\u2019re in and the policy your agent is following. It is given by:\n\nThere exists an optimal value function that has the highest value for all states. It is given by:\n\nYet, your agent can\u2019t control what state he ends up in, directly. He can influence it by choosing some action . Let\u2019s introduce another function that accepts state and action as parameters and returns the expected total reward \u2014 the Q function (it represents the \u201cquality\u201d of a certain action given a state). More formally, the function gives the expected return when starting in , performing aa and following .\n\nAgain, we can define the optimal Q-function that gives the expected total reward for your agent when starting at and picks action . That is, the optimal Q-function tells your agent how good of a choice is picking when at state .\n\nThere is a relationship between the two optimal functions and . It is given by:\n\nThat is, the maximum expected total reward when starting at is the maximum of over all possible actions.\n\nUsing we can extract the optimal policy by choosing the action aa that gives maximum reward for state . We have:\n\nThere is a nice relationship between all functions we defined so far. You now have the tools to identify states and state-action pairs as good or bad. More importantly, if you can identify or , you can build the best possible agent there is (for the current environment). But how do we use this in practice?\n\nLet\u2019s focus on a single state and action . We can express recursively, in terms of the Q value of the next state :\n\nThis equation, known as the Bellman equation, tells us that the maximum future reward is the reward the agent received for entering the current state plus the maximum future reward for the next state . The gist of Q-learning is that we can iteratively approximate using the Bellman equation described above. The Q-learning equation is given by:\n\nwhere is the learning rate that controls how much the difference between previous and new Q value is considered.\n\nCan your agent learn anything using this? At first \u2014 no, the initial approximations will most likely be completely random/wrong. However, as the agent explore more and more of the environment, the approximated Q values will start to converge to .\n\nOkay, it is time to get your ice cream. Let\u2019s try a simple case first:\n\nThe initial state looks like this:\n\nWe will wrap our environment state in a class that holds the current grid and car position. Having a constant-time access to the car position on each step will help us simplify our code:\n\nYour agent needs a way to interact with the environment, that is, choose actions. Let\u2019s define a function that takes the current state with an action and returns new state, reward and whether or not the episode has completed:\n\nIn our case, one episode is starting from the initial state and crashing into a Zombie or eating the ice cream.\n\nOk, it is time to implement the Q-learning algorithm and get the ice cream. We have a really small state space, only 4 states. This allows us to keep things simple and store the computed Q values in a table. Let\u2019s start with some constants:\n\nWe will decay the learning rate, , every episode - as your agent explores more and more of the environment, he will \u201cbelieve\u201d that there is not that much left to learn. Additionally, limits for the number of training episodes and steps are defined.\n\nDicts in Python can be a bit clunky, so we\u2019re using a helper function that gives the Q value for a state-action pair or for all actions, given a state:\n\nChoosing an action given the current state is really simple \u2014 act with random action with some small probability or the best action seen so far (using our ):\n\nWhy your agent uses random actions, sometimes? Remember, the environment is unknown, so it has to be explored in some way \u2014 your agent will do so using the power of randomness.\n\nUp next, training your agent using the Q-learning algorithm:\n\nHere, we use all of the helper functions defined above to ultimately train your agent to behave (hopefully) kinda optimal. We start with the initial state, at every episode, choose an action, receive reward and update our Q values. Note that the implementation looks similar to the formula for Q-learning, discussed above.\n\nYou can clearly observe that the agent learns how to act efficiently, very quickly. Our MDP is really small and this might be just a fluke. Moreover, looking at some episodes. you can see that the agent hit a Zombie.\n\nLet\u2019s extract the policy your agent has learned by selecting the action with maximum Q value at each step, we will do that manually, like a boss. First up, the :\n\nUP seems to have the highest Q value, let\u2019s take that action:\n\nThe new state looks like this:\n\nWhat is the best thing to do now?\n\nBut of course, going left will get you the ice cream! Hooray! Your agent seems to know it\u2019s way around here.\n\nIsn\u2019t this amazing? Your agent doesn\u2019t know anything about the \u201crules of the game\u201d, yet it manages to learn that Zombies are bad and ice cream is great! Also, it tries to reach the ice cream as quickly as possible. The reward seems to the ultimate signal that drives the learning process.\n\nWe\u2019re done here! You can now build complex agents that find optimal policies quickly. Except, maybe not. This was a very simple MDP. Next, we will find how Neural Networks fit into the Reinforcement Learning framework."
    },
    {
        "url": "https://medium.com/@curiousily/getting-your-feet-rewarded-deep-reinforcement-learning-for-hackers-part-0-900ca5bb83e5?source=user_profile---------2----------------",
        "title": "Introduction to Reinforcement Learning \u2014 Deep Reinforcement Learning for Hackers (Part 0)",
        "text": "The best way to understand what Reinforcement Learning is to watch this video:\n\nRemember the first time you went behind the wheel of a car? Your dad, mom or driving instructor was next to you, waiting for you to mess something up. You had a clear goal \u2014 make a couple of turns and get to the supermarket for ice cream. The task was infinitely more fun if you had to learn to drive stick. Ah, good times. Too bad that your kids might never experience that. More on that later.\n\nReinforcement learning (RL) is learning what to do, given a situation and a set of possible actions to choose from, in order to maximize a reward. The learner, which we will call agent, is not told what to do, he must discover this by himself through interacting with the environment. The goal is to choose its actions in such a way that the cumulative reward is maximized. So, choosing the best reward now, might not be the best decision, in the long run. That is greedy approaches might not be optimal.\n\nBack to you, behind the wheel with running engine, properly strapped seatbelt, adrenaline pumping and rerunning the latest Fast & Furious through your mind \u2014 you have a good feeling about this, the passenger next to you does not look that scared, after all\u2026\n\nHow all of this relates to RL? Let\u2019s try to map your situation to an RL problem. Driving is really complex, so for your first lesson, your instructor will do everything, except turning the wheel. The environment is the nature itself and the agent is you. The state of the environment (situation) can be defined by the position of your car, surrounding cars, pedestrians, upcoming crossroads etc. You have 3 possible actions to choose from \u2014 turn left, keep straight and turn right. The reward is well defined \u2014 you will eat ice cream if you are able to reach the supermarket. Your instructor will give your intermediate rewards based on your performance. At each step (let\u2019s say once every second), you will have to make a decision \u2014 turn left, right or continue straight ahead. Whether or not the ice cream is happening is mostly up to you.\n\nLet\u2019s summarise what we\u2019ve learned so far. We have an agent and an environment. The environment gives the agent a state. The agent chooses an action and receives a reward from the environment along with the new state. This learning process continues until the goal is achieved or some other condition is met.\n\nLet\u2019s have a look at some example applications of RL:\n\nYou started thinking that all RL researchers are failed pro-gamers, didn\u2019t you? In practice, that doesn\u2019t seem to be the case. For example, somewhat \u201cmeta\u201d applications include \u201cDesigning Neural Network Architectures using Reinforcement Learning\u201d.\n\nMarkov Decision Process (MDP) is mathematical formulations of the RL problem. They satisfy the Markov property:\n\nMarkov property \u2014 the current state completely represents the state of the environment (world). That is, the future depends only on the present.\n\nAn MDP can be defined by (S,A,R,P,\u03b3) where:\n\nAt the initial time step t=0, the environment chooses initial state s_o\u223cp(s_o). That state is used as a seed state for the following loop:\n\nfor t=0 until done:\n\nMore formally, the environment does not choose, it samples from the reward and transition probability distributions.\n\nWhat is the objective of all this? Find a function \u03c0\u2217, known as optimal policy, that maximizes the cumulative discounted reward:\n\nA policy \u03c0 is a function that maps state s to action a, that our agent believes is the best given that state.\n\nLet\u2019s get back to you, cruising through the neighborhood, dreaming about that delicious ice cream. Here is one possible situation, described as an MDP:\n\nYour objective is to get to the bitten ice cream on a stick, without meeting a zombie. The reasoning behind the new design is based on solid data science \u2014 people seem to give a crazy amount of cash for a bitten fruit and everybody knows that candy is much tastier. Putting it together you get \u201cthe all-new ice cream\u201d. And honestly, it wouldn\u2019t be cool to omit the zombies, so there you have it.\n\nThe state is fully described by the grid. At the first step you have the following actions:\n\nCrashing into a zombie (Carmageddon anyone?) gives a reward of -100 points, taking an action is -1 points and eating the ice cream gives you the crazy 1000 points. Why -1 points for taking an action? Well, the store might close anytime now, so you have to get there as soon as possible.\n\nCongrats, you just created your first MDP. But how do we solve the problem? Stay tuned for that :)\n\nOops, almost forgot, your reward for reading so far:"
    },
    {
        "url": "https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd?source=user_profile---------3----------------",
        "title": "Credit Card Fraud Detection using Autoencoders in Keras \u2014 TensorFlow for Hackers (Part VII)",
        "text": "It\u2019s Sunday morning, it\u2019s quiet and you wake up with a big smile on your face. Today is going to be a great day! Except, your phone rings, rather \u201cinternationally\u201d. You pick it up slowly and hear something really bizarre \u2014 \u201cBonjour, je suis Michele. Oops, sorry. I am Michele, your personal bank agent.\u201d. What could possibly be so urgent for someone from Switzerland to call you at this hour? \u201cDid you authorize a transaction for $3,358.65 for 100 copies of Diablo 3?\u201d Immediately, you start thinking of ways to explain why you did that to your loved one. \u201cNo, I didn\u2019t !?\u201d. Michele\u2019s answer is quick and to the point \u2014 \u201cThank you, we\u2019re on it\u201d. Whew, that was close! But how did Michele knew that this transaction was suspicious? After all, you did order 10 new smartphones from that same bank account, last week \u2014 Michele didn\u2019t call then.\n\nAnnual global fraud losses reached $21.8 billion in 2015, according to Nilson Report. Probably you feel very lucky if you are a fraud. About every 12 cents per $100 were stolen in the US during the same year. Our friend Michele might have a serious problem to solve here.\n\nIn this part of the series, we will train an Autoencoder Neural Network (implemented in Keras) in unsupervised (or semi-supervised) fashion for Anomaly Detection in credit card transaction data. The trained model will be evaluated on pre-labeled and anonymized dataset.\n\nThe source code and pre-trained model are available on GitHub here.\n\nWe will be using TensorFlow 1.2 and Keras 2.0.4. Let\u2019s begin:\n\nThe dataset we\u2019re going to use can be downloaded from Kaggle. It contains data about credit card transactions that occurred during a period of two days, with 492 frauds out of 284,807 transactions.\n\nAll variables in the dataset are numerical. The data has been transformed using PCA transformation(s) due to privacy reasons. The two features that haven\u2019t been changed are Time and Amount. Time contains the seconds elapsed between each transaction and the first transaction in the dataset.\n\n31 columns, 2 of which are Time and Amount. The rest are output from the PCA transformation. Let\u2019s check for missing values:\n\nWe have a highly imbalanced dataset on our hands. Normal transactions overwhelm the fraudulent ones by a large margin. Let\u2019s look at the two types of transactions:\n\nHow different are the amount of money used in different transaction classes?\n\nDo fraudulent transactions occur more often during certain time?\n\nDoesn\u2019t seem like the time of transaction really matters.\n\nAutoencoders can seem quite bizarre at first. The job of those models is to predict the input, given that same input. Puzzling? Definitely was for me, the first time I heard it.\n\nMore specifically, let\u2019s take a look at Autoencoder Neural Networks. This Autoencoder tries to learn to approximate the following identity function:\n\nWhile trying to do just that might sound trivial at first, it is important to note that we want to learn a compressed representation of the data, thus find structure. This can be done by limiting the number of hidden units in the model. Those kind of autoencoders are called undercomplete.\n\nHere\u2019s a visual representation of what an Autoencoder might learn:\n\nWe optimize the parameters of our Autoencoder model in such way that a special kind of error \u2014 reconstruction error is minimized. In practice, the traditional squared error is often used:\n\nIf you want to learn more about Autoencoders I highly recommend the following videos by Hugo Larochelle:\n\nFirst, let\u2019s drop the Time column (not going to use it) and use the scikit\u2019s StandardScaler on the Amount. The scaler removes the mean and scales the values to unit variance:\n\nTraining our Autoencoder is gonna be a bit different from what we are used to. Let\u2019s say you have a dataset containing a lot of non fraudulent transactions at hand. You want to detect any anomaly on new transactions. We will create this situation by training our model on the normal transactions, only. Reserving the correct class on the test set will give us a way to evaluate the performance of our model. We will reserve 20% of our data for testing:\n\nOur Autoencoder uses 4 fully connected layers with 14, 7, 7 and 29 neurons respectively. The first two layers are used for our encoder, the last two go for the decoder. Additionally, L1 regularization will be used during training:\n\nLet\u2019s train our model for 100 epochs with a batch size of 32 samples and save the best performing model to a file. The ModelCheckpoint provided by Keras is really handy for such tasks. Additionally, the training progress will be exported in a format that TensorBoard understands.\n\nAnd load the saved model (just to check if it works):\n\nThe reconstruction error on our training and test data seems to converge nicely. Is it low enough? Let\u2019s have a closer look at the error distribution:\n\nROC curves are very useful tool for understanding the performance of binary classifiers. However, our case is a bit out of the ordinary. We have a very imbalanced dataset. Nonetheless, let\u2019s have a look at our ROC curve:\n\nThe ROC curve plots the true positive rate versus the false positive rate, over different threshold values. Basically, we want the blue line to be as close as possible to the upper left corner. While our results look pretty good, we have to keep in mind of the nature of our dataset. ROC doesn\u2019t look very useful for us. Onward\u2026\n\nPrecision and recall are defined as follows:\n\nLet\u2019s take an example from Information Retrieval in order to better understand what precision and recall are. Precision measures the relevancy of obtained results. Recall, on the other hand, measures how many relevant results are returned. Both values can take values between 0 and 1. You would love to have a system with both values being equal to 1.\n\nLet\u2019s return to our example from Information Retrieval. High recall but low precision means many results, most of which has low or no relevancy. When precision is high but recall is low we have the opposite \u2014 few returned results with very high relevancy. Ideally, you would want high precision and high recall \u2014 many results with that are highly relevant.\n\nA high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n\nYou can see that as the reconstruction error increases our precision rises as well. Let\u2019s have a look at the recall:\n\nHere, we have the exact opposite situation. As the reconstruction error increases the recall decreases.\n\nOur model is a bit different this time. It doesn\u2019t know how to predict new values. But we don\u2019t need that. In order to predict whether or not a new/unseen transaction is normal or fraudulent, we\u2019ll calculate the reconstruction error from the transaction data itself. If the error is larger than a predefined threshold, we\u2019ll mark it as a fraud (since our model should have a low error on normal transactions). Let\u2019s pick that value:\n\nAnd see how well we\u2019re dividing the two types of transactions:\n\nI know, that chart might be a bit deceiving. Let\u2019s have a look at the confusion matrix:\n\nOur model seems to catch a lot of the fraudulent cases. Of course, there is a catch (see what I did there?). The number of normal transactions classified as frauds is really high. Is this really a problem? Probably it is. You might want to increase or decrease the value of the threshold, depending on the problem. That one is up to you.\n\nWe\u2019ve created a very simple Deep Autoencoder in Keras that can reconstruct what non fraudulent transactions looks like. Initially, I was a bit skeptical about whether or not this whole thing is gonna work out, bit it kinda did. Think about it, we gave a lot of one-class examples (normal transactions) to a model and it learned (somewhat) how to discriminate whether or not new examples belong to that same class. Isn\u2019t that cool? Our dataset was kind of magical, though. We really don\u2019t know what the original features look like.\n\nKeras gave us very clean and easy to use API to build a non-trivial Deep Autoencoder. You can search for TensorFlow implementations and see for yourself how much boilerplate you need in order to train one. Can you apply a similar model to a different problem?\n\nThe source code and pre-trained model are available on GitHub here."
    },
    {
        "url": "https://medium.com/@curiousily/human-activity-recognition-using-lstms-on-android-tensorflow-for-hackers-part-vi-492da5adef64?source=user_profile---------4----------------",
        "title": "Human Activity Recognition using LSTMs on Android \u2014 TensorFlow for Hackers (Part VI)",
        "text": "Ever wondered how your smartphone, smartwatch or wristband knows when you\u2019re walking, running or sitting?\n\nWell, your device probably has multiple sensors that give various information. GPS, audio (i.e. microphones), image (i.e. cameras), direction (i.e. compasses) and acceleration sensors are very common nowadays.\n\nWe will use data collected from accelerometer sensors. Virtually every modern smartphone has a tri-axial accelerometer that measures acceleration in all three spatial dimensions. Additionally, accelerometers can detect device orientation.\n\nIn this part of the series, we will train an Neural Network (implemented in TensorFlow) for Human Activity Recognition (HAR) from accelerometer data. The trained model will be exported/saved and added to an Android app. We will learn how to use it for inference from Java.\n\nThe source code for this part is available (including the Android app) on GitHub.\n\nWe will use data provided by the Wireless Sensor Data Mining (WISDM) Lab. It can be download from here. The dataset was collected in controlled, laboratory setting. The lab provides another dataset collected from real-world usage of a smartphone app. You\u2019re free to use/explore it as well. Here\u2019s a video that presents how a similar dataset was collected:\n\nOur dataset contains rows and columns. There are no missing values. There are activities that we\u2019ll try to recognize: Walking, Jogging, Upstairs, Downstairs, Sitting, Standing. Let\u2019s have a closer look at the data:\n\nThe columns we will be most interested in are activity, x-axis, y-axis and z-axis. Let\u2019s dive into the data:\n\nThe columns we will be most interested in are activity, x-axis, y-axis and z-axis. Let\u2019s dive into the data:\n\nI wonder whether or not number received the same paycheck as number . Now, for some accelerometer data:\n\nIt seems reasonable to assume that this data might be used to train a model that can distinguish between the different kinds of activities. Well, at least the first 200 entries of each activity look that way.\n\nOur (covered in the previous part of the series) model expects fixed-length sequences as training data. We\u2019ll use a familiar method for generating these. Each generated sequence contains training examples:\n\nOur training dataset has drastically reduced size after the transformation. Note that we take the most common activity and assign it as a label for the sequence.\n\nThe shape of our tensor looks kinda strange. Let\u2019s transform it into sequences of rows, each containing x, y and z. Let\u2019s apply a one-hot encoding to our labels, as well:\n\nFinally, let\u2019s split the data into training and test (20%) set:\n\nOur model contains and (stacked on each other) with units each:\n\nNow, let create placeholders for our model:\n\nNote that we named the input tensor, that will be useful when using the model from Android. Creating the model:\n\nAgain, we must properly name the tensor from which we will obtain predictions. We will use regularization and that must be noted in our loss op:\n\nThe training part contains a lot of TensorFlow boilerplate. We will train our model for epochs and keep track of accuracy and error:\n\nWhew, that was a lot of training. Do you feel thirsty? Let\u2019s store our precious model to disk:\n\nOur model seems to learn well with accuracy reaching above and loss hovering at around . Let\u2019s have a look at the confusion matrix for the model\u2019s predictions:\n\nAgain, it looks like our model performs real good. Some notable exceptions include the misclassification of Upstairs for Downstairs and vice versa. Jogging seems to fail us from time to time as well!\n\nNow that most of the hard work is done we must export our model in a way that TensorFlow for Android will understand it:\n\nA sample app that uses the exported model can be found on GitHub. It is based heavily based on the Activity Recognition app by Aaqib Saeed. Our app uses the text-to-speech Android API to tell you what the model predicts at some interval and includes our pre-trained model.\n\nThe most notable parts of the Java code include defining our input and output dimensions and names:\n\nThe result is a float array that contains the probability for each possible activity, according to our model.\n\nWe\u2019ve built an model that can predict human activity from time-step sequence with over accuracy on the test set. The model was exported and used in an Android app. I had a lot of fun testing it on my phone, but it seems like more fine tuning (or changing the dataset) is required. Did you try the app? Can you improve it?\n\nThe source code for this part is available (including the Android app) on GitHub."
    },
    {
        "url": "https://medium.com/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218?source=user_profile---------5----------------",
        "title": "Making a Predictive Keyboard using Recurrent Neural Networks \u2014 TensorFlow for Hackers (Part V)",
        "text": "Welcome to another part of the series. This time we will build a model that predicts the next word (a character actually) based on a few of the previous. We will extend it a bit by asking it for 5 suggestions instead of only 1. Similar models are widely used today. You might be using one without even knowing! Here\u2019s one example:\n\nOur weapon of choice for this task will be Recurrent Neural Networks (RNNs). But why? What\u2019s wrong with the type of networks we\u2019ve used so far? Nothing! Yet, they lack something that proves to be quite useful in practice \u2014 memory!\n\nIn short, models provide a way to not only examine the current input but the one that was provided one step back, as well. If we turn that around, we can say that the decision reached at time step t-1 directly affects the future at step t.\n\nIt seems like a waste to throw out the memory of what you\u2019ve seen so far and start from scratch every time. That\u2019s what other types of Neural Networks do. Let\u2019s end this madness!\n\nRNNs define a recurrence relation over time steps which is given by:\n\nWhere St is the state at time step t, Xt an exogenous input at time t, Wrec and Wx are weights parameters. The feedback loops gives memory to the model because it can remember information between time steps.\n\ncan compute the current state St from the current input Xt and previous state St\u22121 or predict the next state from St+1 from the current St and current input Xt. Concretely, we will pass a sequence of 40 characters and ask the model to predict the next one. We will append the new character and drop the first one and predict again. This will continue until we complete a whole word.\n\nTwo major problems torment the \u2014 vanishing and exploding gradients. In traditional the gradient signal can be multiplied a large number of times by the weight matrix. Thus, the magnitude of the weights of the transition matrix can play an important role.\n\nIf the weights in the matrix are small, the gradient signal becomes smaller at every training step, thus making learning very slow or completely stops it. This is called vanishing gradient. Let\u2019s have a look at applying the sigmoid function multiple times, thus simulating the effect of vanishing gradient:\n\nConversely, the exploding gradient refers to the weights in this matrix being so large that it can cause learning to diverge.\n\nmodel is a special kind of that learns long-term dependencies. It introduces new structure \u2014 the memory cell that is composed of four elements: input, forget and output gates and a neuron that connects to itself:\n\nfight the gradient vanishing problem by preserving the error that can be backpropagated through time and layers. By maintaining a more constant error, they allow for learning long-term dependencies. On another hand, exploding is controlled with gradient clipping, that is the gradient is not allowed to go above some predefined value.\n\nLet\u2019s properly seed our random number generator and import all required modules:\n\nThis code works with 1.1 and 2.\n\nWe will use Friedrich Nietzsche\u2019s Beyond Good and Evil as a training corpus for our model. The text is not that large and our model can be trained relatively fast using a modest . Let\u2019s use the lowercase version of it:\n\nLet\u2019s find all unique chars in the corpus and create char to index and index to char maps:\n\nNext, let\u2019s cut the corpus into chunks of characters, spacing the sequences by characters. Additionally, we will store the next character (the one we need to predict) for every sequence:\n\nIt is time for generating our features and labels. We will use the previously generated sequences and characters that need to be predicted to create one-hot encoded vectors using the map:\n\nLet\u2019s have a look at a single training sequence:\n\nThe character that needs to be predicted for it is:\n\nThe encoded (one-hot) data looks like this:\n\nAnd for the dimensions:\n\nWe have training examples, each sequence has length of with unique chars.\n\nThe model we\u2019re going to train is pretty straight forward. Single layer with neurons which accepts input of shape ( \u2014 the length of a sequence, \u2014 the number of unique characters in our dataset). A fully connected layer (for our output) is added after that. It has neurons and softmax for activation function:\n\nOur model is trained for epochs using optimizer and uses of the data for validation:\n\nIt took a lot of time to train our model. Let\u2019s save our progress:\n\nAnd load it back, just to make sure it works:\n\nLet\u2019s have a look at how our accuracy and loss change over training epochs:\n\nFinally, it is time to predict some word completions using our model! First, we need some helper functions. Let\u2019s start by preparing our input text:\n\nRemember that our sequences must be characters long. So we make a tensor with shape , initialized with zeros. Then, a value of is placed for each character in the passed text. We must not forget to use the lowercase version of the text:\n\nNext up, the sample function:\n\nThis function allows us to ask our model what are the next most probable characters. Isn\u2019t that heap just cool?\n\nNow for the prediction functions themselves:\n\nThis function predicts next character until space is predicted (you can extend that to punctuation symbols, right?). It does so by repeatedly preparing input, asking our model for predictions and sampling from them.\n\nThe final piece of the puzzle \u2014 wraps everything and allow us to predict multiple completions:\n\nLet\u2019s use sequences 40 characters that we will use as seed for our completions. All of these are quotes from Friedrich Nietzsche himself:\n\nApart from the fact that the completions look like proper words (remember, we are training our model on characters, not words), they look pretty reasonable as well! Perhaps better model and/or more training will provide even better results?\n\nWe\u2019ve built a model using just a few lines of code in that performs reasonably well after just 20 training epochs. Can you try it with your own text? Why not predict whole sentences? Will it work that well in other languages?"
    },
    {
        "url": "https://medium.com/@curiousily/tensorflow-for-hackers-part-iv-neural-network-from-scratch-1a4f504dfa8?source=user_profile---------6----------------",
        "title": "Creating a Neural Network from Scratch \u2014 TensorFlow for Hackers (Part IV)",
        "text": "Developing models using TensorFlow is easy and fun, but real understanding can be achieved only via reading and implementing the algorithms on your own. This time we will skip TensorFlow entirely and build a Neural Network (shallow one) from scratch, using only pure Python and NumPy. The real challenge is to implement the core algorithm that is used to train (Deep) Neural Networks \u2014 Backpropagation. Shall we dance?\n\nLet\u2019s begin by preparing our environment and seeding the random number generator properly:\n\nWe are importing 3 custom modules that contain some helper functions that we are going to use along the way!\n\nThe sigmoid function is used quite commonly in the realm of deep learning, at least it was until recently. It has distinct S shape and it is a differentiable real function for any real input value. Additionally, it has a positive derivative at each point. More importantly, we will use it as an activation function for the hidden layer of our model. Here\u2019s how it is defined:\n\nIt\u2019s first derivative (which we will use during the backpropagation step of our training algorithm) has the following formula:\n\nSo, the derivative can be expressed using the original sigmoid function. Pretty cool, eh? Don\u2019t like formulas? Let\u2019s look at a picture:\n\nThe derivative shows us the rate of change of a function. We can use it to determine the \u201cslope\u201d of that function. The highest rate of change for the sigmoid function is when x=0x=0, as it is evident from the derivative graph (in green).\n\nIf your Calculus feels a bit rusty take a look at this worked example. That should get you there.\n\nThe softmax function can be easily differentiated, it is pure (output depends only on input) and the elements of the resulting vector sum to 1. Here it is:\n\nIn probability theory, the output of the softmax function is sometimes used as a representation of a categorical distribution. Let\u2019s see an example result:\n\nHere it is:\n\nThe output has most of its weight corresponding to the input 8. The softmax function highlights the largest value(s) and suppresses the smaller ones.\n\nBackpropagation is the backbone of almost anything we do when using Neural Networks. The algorithm consists of 3 subtasks:\n\nIn the first step, backprop uses the data and the weights of the network to compute a prediction. Next, the error is computed based on the prediction and the provided labels. The final step propagates the error through the network, starting from the final layer. Thus, the weights get updated based on the error, little by little.\n\nLet\u2019s build more intuition about what the algorithm is actually doing:\n\nWe will try to create a Neural Network (NN) that can properly predict values from the XOR function. Here is its truth table:\n\nLet start by defining some parameters:\n\nOur data looks like this:\n\nInitialize the weights of our NN to random numbers (using proper size):\n\nThe output of our training effort is the following:\n\nThat error seems to be decreasing! Yay! And the implementation is not that scary, isn\u2019t it? We just multiply the matrix containing our training data with the matrix of the weights of the hidden layer. Then, we apply the activation function (sigmoid) to the result and multiply that with the weight matrix of the output layer.\n\nThe error is computed by doing simple subtraction. During the backpropagation step, we adjust the weight matrices using the already computed error and use the derivative of the sigmoid function.\n\nLet\u2019s try to predict using our trained model (doing just the forward step):\n\nWhat is this sorcery? The prediction is correct! You can try some of the other input examples.\n\nThe \u201chello world\u201d dataset MNIST (\u201cModified National Institute of Standards and Technology\u201d), released in 1999, contains images of handwritten digits. Our goal is to build a model that correctly identify digits from a dataset of tens of thousands of handwritten digits.\n\nWe will build our own \u201cvanilla\u201d Neural Network classifier that learns from raw pixels using only Python and NumPy. Let\u2019s start by reading the data:\n\nLet\u2019s reserve 500 training examples for evaluation of our model:\n\nLet\u2019s take a look at how some handwritten digits look like:\n\nLet\u2019s define a class, called NNClassifier that does all the dirty work for us. We will implement a somewhat more sophisticated version of our training algorithm shown above along with some handy methods:\n\nAll the magic is hidden within the _forward, _backward, _error and _backprop_step methods. We measure the error using cross-entropy loss function. Additionally, L1 and L2 regularizations are used to drive our training into simpler models. One preprocessing step that our model is doing internally is the encoding of the labels as one-hot vectors via the helper function \u2014 one_hot.\n\nOur NN has a neat interface, too! Use the fit method to train it, predict to predict the class of a digit and score to assess the overall performance of the model.\n\nIt\u2019s time to reap the benefits of our hard work. Let\u2019s train our NN for 300 epochs with 50 neurons in the hidden layer:\n\nFirst, let\u2019s have a look at the error change as the number of training epochs increase:\n\nGood, it look like it is converging to a low value. More importantly, let\u2019s check how good our model\u2019s predictions are on the training and test sets:\n\nOur test accuracy is not that good, especially when compared to the results obtained via other models. Let\u2019s check the probability distribution for a single example:\n\nLooks like this:\n\nYou can \u201cclearly\u201d see that the most probable digit is 6.\n\nLet\u2019s look at the image itself:\n\nOur model looks quite sure about its prediction. Let\u2019s have a look at a wrong prediction:\n\nCome on, look at that picture. How is that a 5?\n\nOk, but how does the prediction work? Simply put, it uses the most probable value in the class distribution:\n\nIf we use the method:\n\nWe obtain the same result:\n\nThe performance of our model was not that great. Can we improve on that?\n\nLet\u2019s try to scale our input data:\n\nNot bad, about 3% increase using simple preprocessing. What if we fiddle with the parameters a bit:\n\nThe new result is:\n\nAnother 2% increase. Now, we\u2019re in the \u201cacceptable\u201d range. Will the combination of the two approaches yield an even better result? Why don\u2019t you try it out?\n\nWhat a journey, right? We\u2019ve learned a lot about the inner workings of the Neural Network models. More importantly, we\u2019ve implemented the backpropagation algorithm \u2014 twice! Hopefully, you got some practical understanding of the processes involved in training a Neural Network. Can you adapt the code and make a Deep Neural Network?"
    },
    {
        "url": "https://medium.com/@curiousily/tensorflow-for-hackers-part-iii-convolutional-neural-networks-c077618e590b?source=user_profile---------7----------------",
        "title": "Building a Cat Detector using Convolutional Neural Networks \u2014 TensorFlow for Hackers (Part III)",
        "text": "Have you ever stood still, contemplating about how cool would it be to build a model that can distinguish cats from dogs? Don\u2019t be shy now! Of course you did! Let\u2019s get going!\n\nWe have 25,000 labeled pictures of dogs and cats. The data comes from Kaggle\u2019s Dogs vs Cats challenge. That\u2019s how a bunch of them look like:\n\nLet\u2019s focus on a specific image. Each picture can be represented as a 3-dimensional array. We will resize all training image to 50 x 50 pixels. Here\u2019s a crazy example:\n\nAdditionally, we will remove all color and turn them pictures into grayscale ones. First things first, let\u2019s prepare our environment:\n\nDownload the train and test zip files from Kaggle and extract them into your current working directory.\n\nWe have 25,000 images for training and 12,500 for testing. Let\u2019s create a function that encodes the labels of the training images:\n\nNow, for the actual reading of training and test data. Every image will be resized to 50 x 50 pixels and read as grayscale:\n\nNow, let\u2019s split the data. 24,500 images for training and 500 for testing. We also need to reshape the data appropriately for TensorFlow:\n\nHow will we do it? Isn\u2019t that just too hard of a task? Convolutional Neural Networks to the rescue!\n\nIn the past, people had to think of and code different kinds of features that might be relevant to the task at hand. Examples of that would be whiskers, ears, tails, legs, fur type detectors. These days, we can just use Convolutional NNs. They can learn features from raw data. How do they work?\n\nOk, got it? It was a great explanation. You can think of convolutions as small sliding lenses (let\u2019s say a 5 x 5) that are \u201cactivated\u201d when are placed above some feature that is familiar to them. That way, convolutions can make sense of larger portions of the image, not just single pixels.\n\nFinally, the fun part begins! We will use tflearn to build our Convolutional Neural Network. One additional bonus will be the use of a Dropout layer. Here\u2019s the model:\n\nWe resized our images to 50 x 50 x 1 matrices and that is the size of the input we are using.\n\nNext, a convolutional layer with 32 filters and stride = 5 is created. The activation function is ReLU. Right after that, a max pool layer is added. That same trickery is repeated again with 64 filters.\n\nNext, a fully-connected layer with 1024 neurons is added. Finally, a dropout layer with keep probability of 0.8 is used to finish our model.\n\nWe use Adam as optimizer with learning rate set to 0.001. Our loss function is categorical cross entropy. Finally, we train our Deep Neural Net for 10 epochs.\n\nAll that is great, but our validation accuracy doesn\u2019t seem that good. Flipping a coin might be a better model than the one we created. Let\u2019s go bigger and better (hopefully):\n\nThat is pretty much the same model. One difference is the number of convolutional and max pool layers we added. So, our model has much more parameters and can learn more complex functions. One proof of that is the validation accuracy that is around 0.8. Let\u2019s take our model for a spin!\n\nLet\u2019s have a look at a single prediction:\n\nThat doesn\u2019t look right. How about some more predictions:\n\nThere you have it! Santa is a dog! More importantly, you built a model that can distinguish cats from dogs using only raw pixels (albeit, with a tiny bit of preprocessing). Additionally, it trains pretty fast on relatively old machines! Can you improve the model? Maybe change the architecture, keep probability parameter of the Dropout layer or the optimizer? What results did you get?\n\nThe only thing left for you to do is snap a photo of your cat or dog and run it through your model. Was the net correct?\n\nAn Intuitive Explanation of Convolutional Neural Networks\n\nCS231n \u2014 Convolutional Neural Networks (CNNs / ConvNets)\n\nCats and dogs and convolutional neural networks\n\nGradient-based learning applied to document recognition"
    },
    {
        "url": "https://medium.com/@curiousily/tensorflow-for-hackers-part-ii-building-simple-neural-network-2d6779d2f91b?source=user_profile---------8----------------",
        "title": "Building a Simple Neural Network \u2014 TensorFlow for Hackers (Part II)",
        "text": "In this one, you will learn how to create a Neural Network (NN) and use it for deciding whether a student has alcohol consumption problems.\n\nDo students drink too much? How can you predict that? What predicts it best? How much too much is exactly?\n\nThose questions might be difficult to answer, yet we can start somewhere. We can use a very limited dataset to get a sense of what the answers might look like. Something like this one.\n\nThe dataset contains 1044 instances and 32 variables (most of which binary and categorical). Actually, it consists of 2 other datasets. The first provides data for students enrolled in Portuguese class. The second describes students enrolled in a math course. There is overlap (yep, I know) between the datasets, that is some students attend both classes.\n\nLet\u2019s build an NN model for classifying whether a student has alcohol consumption problem. For that, we will use our trusty old friend \u2014 TensorFlow.\n\nBefore getting there, we have a bit of dirty work to do. Our dataset is not clean enough to just start and feed the data to our NN model. A bit of wrangling is required. But first, let\u2019s start with some setting up:\n\nSome styling and making our experiments reproducible:\n\nRemember, our data is stored in two separate files. Let\u2019s load them, assign proper course attendance to each student and merge them into one:\n\nHere it is:\n\nExactly as promised \u2014 1044 rows, but we have duplicates. The dataset archive contains instructions on how to find them. The merged result contains 382 instances. We will update the course column for those students, too:\n\nWe will use the following formula to quantify the amount of alcohol taken during the week per student:\n\nThe new value changes in the interval . Furthermore, we will classify student as a drinker if that value is greater than 2.\n\nFinally, we can get a feel for our data. Let\u2019s take a look at the course distribution:\n\nAnd the alcohol consumption from the formula:\n\nThe actual variable that we are going to predict:\n\nLet\u2019s have a look at a general correlations matrix:\n\nIt is time for the fun part. Well, not just yet.\n\nMost of our variables are categorical and we must one-hot encode them four our NN to work properly. First, let\u2019s define a little helper function:\n\nOur features and target variable using our little helper function:\n\nLet\u2019s allocate 90% of the data for training and use 10% for testing:\n\nOur NN consists of input, output and 1 hidden layer. We are using ReLU as activation function of the hidden layer and for our output layer. As an additional bonus we will use \u2014 simple way to reduce overfitting during the training of our network. Let\u2019s wrap our model in a little helper function:\n\nLet\u2019s set the number of neurons in the hidden layer to 38 and randomly initialize the weights and biases considering their proper dimensions:\n\nWe will train our model for 5,000 epochs (training steps) with a batch size of 32. That is, at each step, we will train our NN using 32 rows of our data. Granted, in our case you can just train on the whole dataset. However, when the data is huge and you can\u2019t fit it in memory, you would love to split it and feed it to the model at batches (chunks):\n\nIn order for our model to learn, we need to define what is good. Actually, we will define what is bad and try to minimize it. We will call the \u201cbadness\u201d \u2014 error or cost (hence, the cost function). It represents how far off of the true result our model is at some point during training. We would love that error to be 0 for all possible inputs. Currently, that happens only in Sci-Fi novels (not that I discourage dreaming about it).\n\nThe cost function that we are going to use is called . It is defined as:\n\nWhere is the predicted distribution for our alcohol consumption and is the ground truth. This guide might be helpful for better understanding Cross-Entropy. TensorFlow has a little helper function with the sweet little name . It uses as activation function for our output layer and as error function.\n\nNow, for the actual workhorse \u2014 (nope, not the from the Bible \u2014 although, that would\u2019ve been fun). Adam is a type of gradient descent optimization algorithm which essentially tries as hard as he can to find proper weights and biases for our network via minimizing the cost function that we specified above. It is well beyond the scope of this post to describe Adam in details, but you can find all the necessary information over here \u2014 with tons of nice pictures!\n\nUsing in TensorFlow is quite easy, we just have to specify learning rate (you can fiddle with that one) and pass the cost function we defined above:\n\nOur model is created by just calling our helper function with the proper arguments:\n\nOur finished NN looks something like this (much reduced input and hidden layer sizes):\n\nTime to see how well our model can predict. During the training, we will set the keep probability of the to 0.8 and reset it to 1.0 during test time:\n\nHere are the results:\n\nYes, you did it! You survived another part of this tutorial. But what did you achieved? Our model got roughly 73% accuracy on the test set. Is this good? Well\u2026 no, it is not!\n\nHow is that possible? The authors of the paper linked from the dataset attained 92% accuracy. Which is (as they state) acceptable. So, why our model performs so badly?\n\nFor one thing, we excluded overlapping student data, which made our dataset considerably smaller \u2014 from 1044 to just 662 instances (I haven\u2019t found any type of duplicate reduction technique used by the authors. Please, write me a comment if I am wrong about that one). Due to the high prevalence of no drinkers, this might have a decremental effect on our model performance.\n\nOf course, you can try different parameters, architecture, training epochs etc\u2026 Feel free to do so! Till next time!\n\nStudent Alcohol Consumption \u2014 Description of the used dataset\n\n Using Data Mining to Predict Secondary School Student Alcohol Consumption \u2014 A paper using this dataset and comparing 3 different models on it (including NN)\n\n Student Alcohol Consumption Prediction \u2014 Possibly source code used in the previous paper\n\n MNIST classification using TensorFlow \u2014 Use Deep Neural Network to classify handwritten digits\n\n How to choose the number of hidden layers and neurons in NN?\n\n How to handle ordinal data in NN models \u2014 Lots of the variables are ordinal. This paper presents an approach to handling that kind of data in NN models\n\n Simpler way to handle ordinal data in NN models"
    },
    {
        "url": "https://medium.com/@curiousily/tensorflow-for-hackers-part-i-basics-2c46bc99c930?source=user_profile---------9----------------",
        "title": "TensorFlow Basics \u2014 TensorFlow for Hackers (Part I)",
        "text": "The ONLY way to understand TensorFlow is by watching the following video:\n\nNow, I can safely assume that we are at the same level of understanding. Ready to fire up some tensors?\n\nTensorFlow is a library for number crunching created and maintained by Google. It\u2019s used mainly for machine learning (especially deep learning) tasks. While still in beta (version 1.0 is currently in alpha), the library was open sourced more than a year ago (November 9, 2015). Since then it pretty much took the Deep Learning (DL) community by a storm. Tons of companies are using it in production, also. The best place to learn more is the official page of TensorFlow.\n\nOn the more technical side, TensorFlow allows you to do computations on your PC/Mac (CPU & GPU), Android, iOS and lots more places. Of course, being created by Google, it aims to bring massive parallelism to your backprop musings. The main abstraction behind all the magic is stateful dataflow graphs.\n\nThe glossary of TensorFlow states that a tensor is:\n\nSo, you can think of a tensor as a matrix on steroids \u2014 expanded to n more dimensions. The concept might feel a bit strange at first, but don\u2019t worry it will come around eventually.\n\nIf you want basic installation without all the fuss, just do this:\n\nOr install it with GPU support:\n\nOtherwise, you might have a look here if you want to build from source (might need this if you want to support custom cuDNN version) or whatever.\n\nNow that you have everything installed. Let\u2019s check that we can import TensorFlow.\n\nSuccess! As you can see, I am using version 1.0 alpha. Let\u2019s get those tensors flowing.\n\nWriting TensorFlow code might require some getting use to at first. There are some concepts that you must familiarize yourself with.\n\nVariables are pretty standard stuff. You just have to remember one thing \u2014 define them before using them in the computational graph.\n\nPlaceholders are used to feed in data from outside the computational graph. So, if you need to pass data to the model from outside TensorFlow, you have to define a placeholder. Each placeholder must specify a data type. You specify your data using when running your computation.\n\nIn order to run any meaningful operation on your graph, you need a Session. In sessions, we trust (not cookies), most of the time. Here is a short example:\n\nThe result of that execution is:\n\nThis very well known model is a good way to start your exploration in TensorFlow. It is described by the following equation:\n\nWhere Y is the dependent and X is the independent variable. Our task is to adjust the parameters a \u2014 \u201cslope\u201d and b \u2014 \u201cintercept\u201d so that we best describe the data using a line.\n\nFor our example, let\u2019s find out how eating burgers affect your resting heart rate. The data will be simulated, so no conclusions, please! Our data represents the average number of burgers eaten per day.\n\nThe slope and intercept we are looking for are respectively a=50 and b=40.\n\nLet\u2019s have a look at what our model should figure out:\n\nLet\u2019s make things a tiny bit more interesting by adding a bit of noise to our dependent variable.\n\nOur task will be to minimize the mean squared error or in TensorFlow parlance \u2014 reduce the mean.\n\nSo, let\u2019s try to minimize it using gradient descent.\n\nLet\u2019s use our optimizer for 300 steps of learning\n\nLet\u2019s get the final and best predictions for a and b\n\nAnd we have:\n\nLet\u2019s compare the predicted and actual values for y:\n\nThat\u2019s a nice fit. Those two lines overlap pretty good, what did you expect? Pretty good for a couple of lines of code.\n\nThere you have it. Eating hamburgers affects your health in a bad way (probably that one is true). Most importantly, you know a bit of TensorFlow and how to do a simple linear regression. Next up \u2014 deep neural networks.\n\nGetting to Know TensorFlow \n\nLearning TensorFlow Basics \n\nDeep Learning with TensorFlow"
    }
]