[
    {
        "url": "https://medium.com/@stephenhky/summarizing-text-summarization-5d83ff2863a2?source=user_profile---------1----------------",
        "title": "Summarizing Text Summarization \u2013 Stephen Ho \u2013",
        "text": "There are many tasks in natural language processing that are challenging. This blog entry is on text summarization, which briefly summarizes the survey article on this topic. (arXiv:1707.02268) The authors of the article defined the task to be\n\nThere are basically two approaches to this task:\n\nMost algorithmic methods developed are of the extractive type, while most human writers summarize using abstractive approach. There are many methods in extractive approach, such as identifying given keywords, identifying sentences similar to the title, or wrangling the text at the beginning of the documents.\n\nHow do we instruct the machines to perform extractive summarization? The authors mentioned about two representations: topic and indicator. In topic representations, frequencies, tf-idf, latent semantic indexing (LSI), or topic models (such as latent Dirichlet allocation, LDA) are used. However, simply extracting these sentences out with these algorithms may not generate a readable summary. Employment of knowledge bases or considering contexts (from web search, e-mail conversation threads, scientific articles, author styles etc.) are useful.\n\nIn indicator representation, the authors mentioned the graph methods, inspired by PageRank. (see this) \u201cSentences form vertices of the graph and edges between the sentences indicate how similar the two sentences are.\u201d And the key sentences are identified with ranking algorithms. Of course, machine learning methods can be used too.\n\nEvaluation on the performance on text summarization is difficult. Human evaluation is unavoidable, but with manual approaches, some statistics can be calculated, such as ROUGE."
    },
    {
        "url": "https://medium.com/@stephenhky/development-of-neural-architecture-search-4372eb37631a?source=user_profile---------2----------------",
        "title": "Development of Neural Architecture Search \u2013 Stephen Ho \u2013",
        "text": "Google launches her AutoML project last year, in an effort to automate the process of seeking the most appropriate neural net designs for a particular classification problem. Designing neural networks have been time consuming, despite the use of TensorFlow / Keras or other deep learning architecture nowadays. Therefore, the Google Brain team devised the Neural Architecture Search (NAS) using a recurrent neural network to perform reinforcement learning. (See their blog entry.) It is used to find the neural networks for image classifiers. (See their blog entry.)\n\nApparently, with a state-of-the-art hardware, it is of Google\u2019s advantage to perform such an experiment on the CIFAR-10 dataset using 450 GPUs for 3\u20134 days. But this makes the work inaccessible for small companies or personal computers.\n\nThen it comes an improvement to NAS: the Efficient Neural Architecture Search via Parameter Sharing (ENAS), which is a much more efficient method to search for a neural networks, by narrowing down the search in a subgraph. It reduces the need of GPUs.\n\nWhile I do not think it is a threat to machine learning engineers, it is a great algorithm to note. It looks to me a brute-force algorithm, but it needs scientists and engineers to gain insights. Still, I believe development of the theory behind neural networks is much needed."
    },
    {
        "url": "https://medium.com/@stephenhky/essential-python-packages-5ae1cce6f587?source=user_profile---------3----------------",
        "title": "Essential Python Packages \u2013 Stephen Ho \u2013",
        "text": "Almost three years ago, I wrote a blog entry titled Useful Python Packages, which listed the essential packages that I deemed important. How has the list been changed over the past three years?\n\nFirst of all, three years ago, most people were still writing Python 2.7. But now there is a trend to switch to Python 3. I admitted that I still have not started the switch yet, but in the short term, I will have no choice and I will.\n\nWhat are some of the essential packages?\n\nI can probably list more, but I think I covered most of them. If you do not find something useful, it is probably time for you to write a brand new package."
    },
    {
        "url": "https://medium.com/@stephenhky/interpretability-of-neural-networks-f06e01a16843?source=user_profile---------4----------------",
        "title": "Interpretability of Neural Networks \u2013 Stephen Ho \u2013",
        "text": "The theory and the interpretability of deep neural networks have always been called into questions. In the recent few years, there have been several ideas uncovering the theory of neural networks.\n\nMehta and Schwab analytically connected renormalization group (RG) with one particular type of deep learning networks, the restricted Boltzmann machines (RBM). (See their paper and a previous post.) RBM is similar to Heisenberg model in statistical physics. This weakness of this work is that it can only explain only one type of deep learning algorithms.\n\nHowever, this insight gives rise to subsequent work, with the use of density matrix renormalization group (DMRG), entanglement renormalization (in quantum information), and tensor networks, a new supervised learning algorithm was invented. (See their paper and a previous post.)\n\nLin and Tegmark were not satisfied with the RG intuition, and pointed out a special case that RG does not explain. However, they argue that neural networks are good approximation to several polynomial and asymptotic behaviors of the physical universe, making neural networks work so well in predictive analytics. (See their paper, Lin\u2019s reply on Quora, and a previous post.)\n\nTishby and his colleagues have been promoting information bottleneck as a backing theory of deep learning. (See previous post.) In recent papers such as arXiv:1612.00410, on top of his information bottleneck, they devised an algorithm using variation inference.\n\nRecently, Kawaguchi, Kaelbling, and Bengio suggested that \u201cdeep model classes have an exponential advantage to represent certain natural target functions when compared to shallow model classes.\u201d (See their paper and a previous post.) They provided their proof using generalization theory. With this, they introduced a new family of regularization methods.\n\nRecently, Lei, Su, Cui, Yau, and Gu tried to offer a geometric view of generative adversarial networks (GAN), and provided a simpler method of training the discriminator and generator with a large class of transportation problems. However, I am still yet to understand their work, and their experimental results were done on low-dimensional feature spaces. (See their paper.) Their work is very mathematical."
    },
    {
        "url": "https://medium.com/@stephenhky/a-computational-model-of-tensorflow-d5eab1b3684a?source=user_profile---------5----------------",
        "title": "A Computational Model of TensorFlow \u2013 Stephen Ho \u2013",
        "text": "If you have been taking Andrew Ng\u2019s deeplearning.ai course on Coursera, you must have learned in Course 1 about the graph operations, and the method of back propagation using derivatives in terms of graph. In fact, it is the basis of TensorFlow, a Python package commonly used in deep learning. Because it is based on the graph model of computation, we can see it as a \u201cprogramming language.\u201d\n\nGoogle published a paper about the big picture of computational model in TensorFlow:\n\nBeware that this model is not limited to deep learning."
    },
    {
        "url": "https://medium.com/@stephenhky/neural-network-representation-of-quantum-many-body-states-7545d2eb1b4?source=user_profile---------6----------------",
        "title": "Neural-Network Representation of Quantum Many-Body States",
        "text": "There are many embeddings algorithm for representations. Sammon embedding is the oldest one, and we have Word2Vec, GloVe, FastText etc. for word-embedding algorithms. Embeddings are useful for dimensionality reduction.\n\nTraditionally, quantum many-body states are represented by Fock states, which is useful when the excitations of quasi-particles are the concern. But to capture the quantum entanglement between many solitons or particles in a statistical systems, it is important not to lose the topological correlation between the states. It has been known that restricted Boltzmann machines (RBM) have been used to represent such states, but it has its limitation, which Xun Gao and Lu-Ming Duan have stated in their article published in Nature Communications:\n\nPEPS is a generalization of matrix product states (MPS) to higher dimensions. (See this.)\n\nHowever, Gao and Duan were able to prove that deep Boltzmann machine (DBM) can bridge the loophole of RBM, as stated in their article:"
    },
    {
        "url": "https://medium.com/@stephenhky/sammon-embedding-with-tensorflow-54ffb50458ea?source=user_profile---------7----------------",
        "title": "Sammon Embedding with Tensorflow \u2013 Stephen Ho \u2013",
        "text": "Embedding algorithms, especially word-embedding algorithms, have been one of the recurrent themes of this blog. Word2Vec has been mentioned in a few entries (see this); LDA2Vec has been covered (see this); the mathematical principle of GloVe has been elaborated (see this); I haven\u2019t even covered Facebook\u2019s fasttext; and I have not explained the widely used t-SNE and Kohonen\u2019s map (self-organizing map, SOM).\n\nI have also described the algorithm of Sammon Embedding, (see this) which attempts to capture the likeliness of pairwise Euclidean distances, and I implemented it using Theano. This blog entry is about its implementation in Tensorflow as a demonstration.\n\nAssume there are high dimensional data described by d-dimensional vectors,\n\nwhere i=1, 2, \u2026, N. And they will be mapped into vectors\n\nwith dimensions 2 or 3. Denote the distances to be\n\nIn this problem, Y\u2019s are the variables to be learned. The cost function to minimize is\n\nUnlike in previous entry and original paper, I am going to optimize it using first-order gradient optimizer. If you are not familiar with Tensorflow, take a look at some online articles, for example, \u201cTensorflow demystified.\u201d This demonstration can be found in this Jupyter Notebook in Github.\n\nFirst of all, import all the libraries required:\n\nLike previously, we want to use the points clustered around at the four nodes of a tetrahedron as an illustration, which is expected to give equidistant clusters. We sample points around them, as shown:\n\nRetrieve the number of points, N, and the resulting dimension, d:\n\nOne of the most challenging technical difficulties is to calculate the pairwise distance. Inspired by this StackOverflow thread and Travis Hoppe\u2019s entry on Thomson\u2019s problem, we know it can be computed. Assuming Einstein\u2019s convention of summation over repeated indices, given vectors\n\nwhere the first and last terms are simply the norms of the vectors. After computing the matrix, we will flatten it to vectors, for technical reasons omitted to avoid gradient overflow:\n\nAs we said, we used first-order gradient optimizers. For unknown reasons, the usually well-performing Adam optimizer gives overflow. I then picked Adagrad:\n\nThe last line initializes all variables in the Tensorflow session when it is run. Then start a Tensorflow session, and initialize all variables globally:\n\nThen extract the points and close the Tensorflow session:\n\nThis gives, as expected,\n\nThis code for Sammon Embedding has been incorporated into the Python package , which is a collection of numerical routines. You can install it, and call:"
    },
    {
        "url": "https://medium.com/@stephenhky/word-movers-distance-as-a-linear-programming-problem-6b0c2658592e?source=user_profile---------8----------------",
        "title": "Word Mover\u2019s Distance as a Linear Programming Problem",
        "text": "Much about the use of word-embedding models such as Word2Vec and GloVe have been covered. However, how to measure the similarity between phrases or documents? One natural choice is the cosine similarity, as I have toyed with in a previous post. However, it smoothed out the influence of each word. Two years ago, a group in Washington University in St. Louis proposed the Word Mover\u2019s Distance (WMD) in a PMLR paper that captures the relations between words, not simply by distance, but also the \u201ctransportation\u201d from one phrase to another conveyed by each word. This Word Mover\u2019s Distance (WMD) can be seen as a special case of Earth Mover\u2019s Distance (EMD), or Wasserstein distance, the one people talked about in Wasserstein GAN. This is better than bag-of-words (BOW) model in a way that the word vectors capture the semantic similarities between words.\n\nThe formulation of WMD is beautiful. Consider the embedded word vectors\n\n, where d is the dimension of the embeddings, and n is the number of words. For each phrase, there is a normalized BOW vector\n\n, where i\u2019s denote the word tokens. The distance between words are the Euclidean distance of their embedded word vectors, denoted by\n\n, where i and j denote word tokens. The document distance, which is WMD here, is defined by\n\n, where T is a n times n matrix. Each element\n\ndenote how nuch of word i in the first document (denoted by d) travels to word j in the new document (denoted by d\u2019).\n\nThen the problem becomes the minimization of the document distance, or the WMD, and is formulated as:\n\nThis is essentially a simplified case of the Earth Mover\u2019s distance (EMD), or the Wasserstein distance. (See the review by Gibbs and Su.)\n\nThe WMD is essentially a linear optimization problem. There are many optimization packages on the market, and my stance is that, for those common ones, there are no packages that are superior than others. In my job, I happened to handle a missing data problem, in turn becoming a non-linear optimization problem with linear constraints, and I chose limSolve, after I shop around. But I actually like a lot of other packages too. For WMD problem, I first tried out cvxopt first, which should actually solve the exact same problem, but the indexing is hard to maintain. Because I am dealing with words, it is good to have a direct hash map, or a dictionary. I can use the Dictionary class in gensim. But I later found out I should use PuLP, as it allows indices with words as a hash map (dict in Python), and WMD is a linear programming problem, making PuLP is a perfect choice, considering code efficiency.\n\nAn example of using PuLP can be demonstrated by the British 1997 UG Exam, as in the first problem of this link, with the Jupyter Notebook demonstrating this.\n\nThe demonstration can be found in the Jupyter Notebook.\n\nThen define the functions the gives the BOW document vectors:\n\nThen implement the core calculation. Note that PuLP is actually a symbolic computing package. This function return a class:\n\nTo extract the value, just run\n\nWe use Google Word2Vec. Refer the\n\nmatrices in the Jupyter Notebook. Running this by a few examples:\n\nThere are more examples in the notebook.\n\nWMD is a good metric comparing two documents or sentences, by capturing the semantic meanings of the words. It is more powerful than BOW model as it captures the meaning similarities; it is more powerful than the cosine distance between average word vectors, as the transfer of meaning using words from one document to another is considered. But it is not immune to the problem of misspelling.\n\nThis algorithm works well for short texts. However, when the documents become large, this formulation will be computationally expensive. The author actually suggested a few modifications, such as the removal of constraints, and word centroid distances.\n\nExample codes can be found in my Github repository: stephenhky/PyWMD."
    },
    {
        "url": "https://medium.com/@stephenhky/a-first-glimpse-of-rigettis-quantum-computing-cloud-e9c995672e60?source=user_profile---------9----------------",
        "title": "A First Glimpse of Rigetti\u2019s Quantum Computing Cloud",
        "text": "Quantum computing has been picking up the momentum, and there are many startups and scholars discussing quantum machine learning. A basic knowledge of quantum two-level computation ought to be acquired.\n\nRecently, Rigetti, a startup for quantum computing service in Bay Area, published that they opened to public their cloud server for users to simulate the use of quantum instruction language, as described in their blog and their White Paper. It is free.\n\nGo to their homepage, http://rigetti.com/, click on \u201cGet Started,\u201d and fill in your information and e-mail. Then you will be e-mailed keys of your cloud account. Copy the information to a file .pyquil_config, and in your .bash_profile, add a line\n\nMore information can be found in their Installation tutorial. Then install the Python package pyquil, by typing in the command line:\n\nSome of you may need to root (adding sudo in front).\n\nThen we can go ahead to open Python, or iPython, or Jupyter notebook, to play with it. For the time being, let me play with creating an entangled singlet state,\n\nThe corresponding quantum circuit is like this:\n\nFirst of all, import all necessary libraries:\n\nYou can see that the package includes a lot of quantum gates. First, we need to instantiate a quantum simulator:\n\nThen we implement the quantum circuit with a \u201cprogram\u201d as follow:\n\nThe last line gives the final wavefunction after running the quantum circuit, or \u201cprogram.\u201d For the ket, the rightmost qubit is qubit 0, and the left of it is qubit 1, and so on. Therefore, in the first line of the program, H, the Hadamard gate, acts on qubit 0, i.e., the rightmost qubit. Running a simple print statement:\n\nThe coefficients are complex, and the imaginary part is described by j. You can extract it as a numpy array:\n\nIf we want to calculate the metric of entanglement, we can use the Python package pyqentangle, which can be installed by running on the console:\n\nBecause pyqentangle does not recognize the coefficients in the same way as pyquil, but see each element as the coefficients of |ji>, we need to reshape the final state first, by:\n\nThen perform Schmidt decomposition (which the Schmidt modes are actually trivial in this example):\n\nCalculate the entanglement entropy and negativity from its reduced density matrix:\n\nThe calculation can be found in this thesis.\n\nP.S.: The circuit was drawn by using the tool in this website, introduced by the Marco Cezero\u2019s blog post. The corresponding json for the circuit is:"
    },
    {
        "url": "https://medium.com/@stephenhky/kick-off-be1f76a2c7af?source=user_profile---------10----------------",
        "title": "Kick Off \u2013 Stephen Ho \u2013",
        "text": "Actually I already have a site on WordPress, but it looks like there are increasingly many people on Medium here. I will start using this. In the mean time, I will do this:\n\nMy WordPress page: Everything About Data Analytics. It is about data analysis, text mining, and all sorts of quantitative modelling."
    }
]