[
    {
        "url": "https://medium.com/@annishared/searching-for-optimal-policies-in-python-an-intro-to-optimization-7182d6fe4dba?source=user_profile---------1----------------",
        "title": "Searching for Policies in Python: An intro to Optimization",
        "text": "Space and time complexity grow in parallel with depth and width of a policy update.\n\nDynamic programming and exhaustive search are the two extremes that use full backups to update the policy \u2014 width dimension. These methods take the expectation values of updates over all states(based on a distribution of possible trajectories). Expected updates require a distribution model.\n\nWhile one-step TD learning and pure Monte Carlo approaches use sample backups(sample trajectories of s-a-r). How do we sample trajectories? We execute Bellman backups only on states that are visited under good policies. In this way, we can solve the scaling problem of looping over all possible states. Sample updates require only a sample model, or can be done from actual experience with no model at all (this could be another dimension of variation).\n\nWhat is Bootstrapping? The short answer is how far you can see on the time horizon. That is, the updates estimates are based on other estimates (shallow back ups or one step ahead) and not done by running all the steps to terminal states(deep backups). The degree of bootstrapping is another dimension of variation in backups.\n\nIn practice, the applicability of dynamic programming and reinforcement learning is limited by the enormous size of the underlying state spaces. This is known as Bellman\u2019s \u201ccurse of dimensionality\u201d or \u201cexpanding grid\u201d.\n\nManipulating the value of every state in a tabular representation is not feasible in large or continuous domains. Consider adding one state in the transition table of state space. This means we add one row and one column, namely adding one cell to every existing column and row.\n\nA solution is to use function approximators like neural networks, decision trees etc. This idea is termed as Neuro dynamic programming, approximate dynamic programming or in the case of RL deep reinforcement learning.\n\nIn value function approximation, you use simulation to tune the parameters. That is to compute an approximate solution to Bellman\u2019s equation and then construct near-optimal policies.\n\nThe alternative is to use policy optimization, also called optimization in policy space. That is tuning policy parameters in a direction of improvement. Policy optimization approach involves i. Evolutionary algorithms and ii. Policy Gradients.\n\nPolicy gradient methods relies upon optimizing parametrized policies w.r.t. the reward by gradient descent. The parameters are the policies indexed by a parameter vector of numbers, analogous to classification or regression with input the states and output the actions. You can use the exact same approximation functions as in : i. classification network (discrete action space) outputs vector of probabilities, and ii. regression network (continuous action space) outputs mean and diagonal covariance of Gaussian"
    },
    {
        "url": "https://medium.com/@annishared/learning-representation-evaluation-optimization-c73fce64281f?source=user_profile---------2----------------",
        "title": "Learning = Representation + Evaluation + Optimization",
        "text": "To implement learning algorithms and data science workflows, we need to understand what learning is and how data are linked to models. There are many possible ways and each one has its own restrictions and strengths. In all cases, as more data are available, both more complex problems can be tackled and more accurate results can be achieved. When we have not historical data available, we use simulation.\n\nIn this post we briefly discuss about the steps involved in every data modeling process and give an overview of common distinctions among the different learning types.\n\nThe three components of every learning algorithm is representation, evaluation and optimization. Let\u2019s see each one closer.\n\nFeature Engineering: a representation problem that turn the inputs into things the algorithm can understand. With a good set of features you are closer to the true underlying structure of the problem. This procedure almost always incorporates domain knowledge. Maybe you think ok I\u2019ll include many features in my model so as to provide it with all the important information of the problem; this probably will lead you in an overfitted model. Overfitting is nothing more than your model being sticked to random patterns instead of the true evidence that generated the dataset. Handling overfitting and feature engineering are beautiful topics by themselves and need to be studied far further whereas here we just made a reference.\n\nWhen analysis lies in distributed or parallel systems, the choice of representation also affects how well one can decompose the data set into smaller components so that analysis can be performed independently on each component.\n\nBesides probabilistic graphical models (bayesian networks, conditional random fields), we can have instances models (KNN, SVM), rule based models(Decision trees), hyperplanes (Naive Bayes, logistic regression, linear regression), or neural networks data representations. Do not confuse Bayesian networks with Naive Bayes; the second assumes independence among the features. Additionally, although the representation of probabilistic graphical models seems similar to neural nets, in the latter case the nodes are non-linear computational units. This means that each node h performs a sigmoid function, hyperbolic tangent nonlinearity, or rectifier linear unit given its inputs from the previous layer.\n\nThe function depends on the representation you choose including the type of function\u2019s output:\n\nOptimization can be based on combinatorial like greedy search, on unconstrained continuous optimization like gradient descents, and on constrained continuous optimization like linear programming. For a visualized taxonomy of optimization methods, you can have a look at Searching for Optimal Policies in Python: An intro to optimization .\n\nThe additional data modeling steps that repeatedly intervene the learning process are the following:\n\nCross validation: All observations are used for both training and validation. Each observation is used for validation exactly once. K-Fold means k equal sized subsamples where you record error on each K and take the average."
    },
    {
        "url": "https://medium.com/@annishared/descriptive-statistics-ec6e9c60989c?source=user_profile---------3----------------",
        "title": "Descriptive Statistics \u2013 Anni Sap \u2013",
        "text": "Inferential: Drawing conclusions on population based on what observed in samples\n\nLet\u2019s find a simple example to play with functions of descriptive statistics:\n\nWebsite Visitors: Let\u2019s say that each value of the dataset represents the number of users that visit your website during the first five days that you lunch it.\n\nThough, this example does not correspond to real-life problems since the number of data points is too small and there is no randomness in data selection. Hence, data are not representative of the population, which introduces the bias.\n\nIn our example, a selection bias would be that on Monday & Tuesday, we have got many visitors since these days succeed weekend, in which users have not access to our site.\n\nWhere our most data values are located?\n\nMean: \u2018How representative is the data set?\u2019\n\nWe do not use the mean when there are many outliers or many missing values. Additionally we do not use it in ordinal data, but only in ration or interval data.\n\nMedian: the value(s) of which both right and left, lie the 50% of the remaining data points;\n\nThe mode usually used either accompanied with the two aforementioned measures or for the computation of categorical values\u2019 central tendency. It is the data point with the higher frequency\n\nTo what extent data values differ from each other?\n\nKnowing the central tendency of data points is not adequate for estimating the representativeness of the dataset, since different data shapes can have the same mean, median or mode. It is useful to know the size of a dispersion, as well.\n\nThe range is changed every time a new data point is added. It\u2019s also being affected by the outliers.\n\nQuartiles: Split your data into four equal parts.\n\nStep 3: Now find the medians of the first (Q1) and of the second part (Q3).\n\nThe interquartile range IQR covers the 50% of data points distribution. We calculated as the difference of quartiles: Q3 \u2014 Q1. Each quartile covers 25% of our observations. IQR usually used together with the mode. Though, the most reliable measures of variability are variance and standard deviation. They use the mean as benchmarking point and they take into account all the distances of data points from it.\n\nVariance shows us the average dispersion from the center. The expected value of sample variance converges to sample population and what this means is that variance is a very good population measure.\n\nThere are many ways to calculate the variance and the most popular is the one that includes the computation of the mean.\n\nThe standard deviation is the average variation.\n\nIt measures how spread out all the values are, how far away they are from the mean. Not so important now but good to know is that, the expected value of sample standard deviation does not converge to the population ones.\n\nWhen standard deviation increases by twofold, the variance increases by 2\u00b2.\n\nVariation Coefficient: Compare variation between two different things.\n\nStep 1: Calculate for each one the standard deviation\n\nStep 2: Divide each standard deviation by its mean\n\nStep 3: The largest value is the one with the largest variation\n\nEqually useful to know is the shape of the data set, namely if the data points are massed to the left or right to the mean.\n\nSkewness describes the long-tail of the distribution and it can be negative, positive, without skewness or undefined. Consider for instance the income distribution as long right tail(the blue one). This shows the inequality in income since the average of income is above the median and mode.\n\nA perfectly symmetrical data set will have a skewness of 0. The normal distribution has a skewness of 0.\n\n-1 < skewness < \u2014 0.5 or 0.5 < skewness < 1, the data are moderately skewed\n\n- 1 < skewness or skewness < -1, the data are highly skewed\n\nKurtosis describes how peaked the curve is. It sees whether the data are heavy-tailed (fat) or light-tailed relative to a normal distribution(the tallness of the central peak). It also measures the amount of probability in the tails (the sharpness of the central peak).\n\nThe value is often compared to the kurtosis of the normal distribution, which is equal to 3.\n\nIf the kurtosis > 3, then the dataset has more probabilities in the tails (heavier tails than a normal distribution).\n\nIf the kurtosis < 3, then the dataset has less probabilities in the tails (lighter tails than a normal distribution).\n\nImagine the blue and red curve to be the world-wide income in 2000 and 2020 respectively. This would imply that we move toward more income -equality because more people are rich(peak) and less people are poor(tails).\n\nThe histogram is an effective graphical technique for summarising the shape, the distribution of a numeric data set and showing both the skewness and kurtosis of it.\n\nBar plots (histogram with gaps among the bars) are used for qualitative data points instead.\n\n4. Why do we care about distribution?\n\nProbability distributions are listing every probability such as \u2018how likely is the route to my work to be 10\u201320 minutes\u2019, \u2018how likely to be 30\u201340 minute route\u2019 and so forth\u2019.\n\nParametric models or the math behind many functions have assume that data follow a specific distribution. We\u2019ll take inaccurate results if we neglect this assumption. Let\u2019s put it from an engineer view who designs a learning algorithm. Firstly he/she should define what the real-world problem is, and then will design the algorithm to solve it.\n\nBefore modelling this problem to math prepositions, we must define the objects of interest in a structured form. The distribution shows us the different values these objects could take(range), what it is considered as the \u2018normal\u2019(mean) and what as \u2018rare\u2019(outliers) which contribute to representativeness.\n\nThere are also some other functions like percentile, moment, correlation coefficients, covariance, and confidence level percentage but in the favour of simplicity, let\u2019s discuss them in another post.\n\nWhen you are using statistical tools, you don\u2019t need to know exactly what the function is since you just have to call the name of the function and you get the result. I computed the results for the example using Octave. But to understand the results I had to be serious and know at least the basic idea behind the functions logic.\n\nWe randomly select data that will constitute our data set. For a given predictor/feature and most importantly for the outcome variable we check the probability distribution they follow. This is known in statistics as sampling distribution."
    },
    {
        "url": "https://medium.com/@annishared/build-your-first-neural-network-in-python-c80c1afa464?source=user_profile---------4----------------",
        "title": "Build your first neural network in Python \u2013 Anni Sap \u2013",
        "text": "The machine learning workflow consists of six steps where the first three are more theoretical-oriented:\n\n\u201cProcess and transform the iris flowers dataset to create a prediction model. This model must predict in which of the three specific flower species each flower is likely to belong, with 95% or greater accuracy.\u201d\n\nIris is a genus of flowering plants species. It takes its name from the goddess of rainbow in Greek mythology.\n\nThis dataset is perhaps best known by its appearance in the pattern recognition literature and specifically in linear discriminant analysis (LDA). LDA used as a dimensionality reduction technique or to find a linear combination of features that separates the data points into different region of data points. A perceptron finds this linear discriminant function (approximation function).\n\nThe Iris dataset has three classes where one class is linearly separable from the other 2; the latter two are not linearly separable from each other. Each class refers to a type of iris plant and contains 50 instances.\n\nThe below command prints a table with statistics for each numerical column in our dataset. We need to see how representative the dataset is and what kind of preprocessing techniques it may need.\n\nThe next step is to visualize the dataset to capture relationships between the features and the regarding classes. We have more than two features (multivariate data) in our data that involves relationships between them. We will use bivariate visualisations for every two features to capture all of their relationships.\n\nBefore diving into preprocessing we have to split our data into training (80%) and test data (20%). We will use the training data (contain classes the labels for our iris species) to make the model learn; and test data ( contain only the features, without the labels) to eventually test the accuracy of our prediction model.\n\nBecause MLP is sensitive to feature scaling, we will scale our data with StandarEncoder(). We will also encode categorical variables since prediction models do work with numerical variables.\n\nEncode Categorical Variables with LabelEncoder(): a model does not understand categorical variables, hence we have to convert them to numerical ones.\n\nLabelEncoder introduced a new problem to our dataset: added numerical relationships in the features (ordinal variables). That means Iris-versicolor(1) is not higher than Iris-setosa(0) and Iris-setosa(0) is not smaller than Iris-virginica(2). Instead of 0,1,2 we would like to have a three dimensional vector. Thus, setosa would be [1,0,0], versicolor would be [0,1,0] and virginica [0,0,1].\n\nThe second preprocessing technique is to scale our data with StandardScaler(). This helps us to speed up our algorithm (gradient descent) and have a better classifier.\n\nWe can use a plot to see what feature scaling did to our data values.\n\nClassifier -the learning task-: correlates features of data with class properties to group data objects.\n\nPerceptron -the learning algorithm-, the most basic form of a neural network: a simple supervised linear feedforward classifier.\n\nThe perceptron is binary in the sense that the neuron is activated by a function that returns boolean values. The weights are updated based on these Boolean values so that only the input can be used for weight correction (reinforcement learning).\n\nReinforcement: The perceptron algorithm is an example of reinforcement learning. After each presentation of an input-output example we only know whether the network produces the expected result output or not. The alternative would be learning with error correction; updates on weights are not only effected by the input but also by the magnitude of the error.\n\nMultilayer -a modelling approach- : network consists of more than one layer of output nodes; that means, there is at least one hidden layer that passes the inputs to the output layer.\n\nAlternative to MLP: probabilistic neural network based on the Bayesian approach. It supports inference apart from parameters estimation."
    },
    {
        "url": "https://medium.com/@annishared/learning-cost-in-machines-and-humans-649a3c5b1bd1?source=user_profile---------5----------------",
        "title": "Learning cost in machines and humans \u2013 Anni Sap \u2013",
        "text": "We human beings love to learn, but we are unable to learn all the vastness of things we would like to. Choosing one \u2018learning opportunity\u2019 always comes with the cost of not choosing another. We can observe the pattern of cost in many domains. In economics, we have the opportunity cost while in statistical analysis we have the Type I- TypeII error trade off. In alchemy there is also a trade-off, expressed as equivalent exchange.\n\nLimited time and brainpower are two causes that restrict what we can teach to ourselves. In implementing algorithms to teach machines, we have the time-space trade off. In model fitting, we have the underfiting-overfitting trade off while in software programs we have to choose between better efficiency or accuracy.\n\nImagine now our brains deciding to learn something against another. We use rules of thumb \u2014 heuristics \u2014 to maximize the reward of a decision. While machine learning uses heuristic or metaheuristics optimization algorithms or again rules of thumb to minimize the cost function between truth-approximation.\n\nThe brain is eager to estimating and planning for the future so as to select a wise behavior towards optimal outcomes from decisions.The combination of past and prior belief is maybe the most important factor that guides future actions either consciously or subconsciously. But the past is always a distorted evidence of reality, since reality does not longer exist. Hence, we take an approximation -a filtered version- of it which each one can interpret it in many different ways. At many times, our prior influences the posterior-the future because of our confirmation bias.\n\nSometimes, there is an overfitting in our predictions because we are stick to past experiences; and sometimes there is an underfitting because we neglect important facets of the past. Hence, we have to balance our model\u2019s adherence to the past. One cool thing to do that, would be to fit a common line in the space of principles and values.\n\nPredictions are always an approximation to reality\n\nWould be there a time that machines would be able to feel emotions like love and what we would like to achieve with that?Science fiction, the demon, the next revolution or whatever. Future is unpredictable. We can make a prediction of it, but it would always be just an approximation based on current beliefs and behaviors.\n\nIn the late of 19th century, researchers proved their observations that neurons were electrically excitable.\n\nThirty years later or so, the first automatic electronic digital computer is built, aka Atanasoff\u2013Berry computer. It used electricity in Logic gates to do calculations.\n\nTen years later or so, basic brain physiology and function of neurons, together with proportional logic formulation and Turing\u2019s theory of computation, form the first work that is now generally recognized as AI (1943). A model of artificial neurons in which each neuron is characterized as being \u201con\u201d or \u201coff\u201d . A switch to \u201con\u201d occurs in response to stimulation by a sufficient number of neighboring neurons, known as Artificial Neural Networks.\n\nIt would be fun to dive into ANN learning, but before that, let\u2019s explore something more familiar: how the brain learns. This attempt stems from the fact that, our mind perceives new or complex information while trying to recognise patterns based on what it already knows, what it\u2019s been trained on. Of course, it would be silly to claim that machines and humans learn the same way. Instead, doing this connection is useful for 3 reasons: (1)easier to comprehend ANN if it is connected to a more familiar concept BNN, (2) these two concepts can form a story since they share similarities, and humanity has been always acquiring knowledge through narratives, (3) ANN, BNN and learning can all be explained by drawing theories from neuroscience.\n\nA biological neural network(BNN) is a collection of electrically excitable cells (neurons), connected together through synapses. There is no central processing like CPU where all our memories stored. On the contrary memories are stored throughout many brain regions. Synapses transfer information at \u201cdistributed-graph-databases\u201d. When information is processed, actions, skills, thoughts, or feelings emerge.\n\nHow do we acquire new knowledge?\n\nIn the deep level, our sensors perceive data that travel in different interconnected brain regions and through their interactions and doses of practice, somehow knowledge appears. In the surface level, the structure of the brain cells is altered and manifest changes in our abilities which come after optimal behaviour patterns. You have to find out what of your behaviours work best for you, and then make them habits. It sound simple right? It isn\u2019t, but training helps to be.\n\nThe brain changes in three very basic ways to support learning according to neuroplasticity:\n\n1.Chemical: transfer chemical signals between neurons. It supports short-term memory.\n\n2.Structural: altering or creating the connections between neurons. It supports long-term memory.\n\n3.Functional: The more we use a brain region, the more excitable and easier is to use it again and again.\n\nThere are five schools of thought in artificial intelligence:\n\niv. bayesian, inspired by Bayesian probability theory and statistical inference, and\n\nFor a machine to learn a task (or skill in our terms), all it needs are: data, algorithm, and a theoretical model that follow one or more of the former five approaches.\n\nArtificial neural networks or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. The ANN should learn the input\u2013output mapping without overfitting the data.\n\nSuch systems learn to do tasks by considering examples. In other words, every supervised model need labeled data to learn. The difficultness lies in the absent of labelled data. On the contrary, unsupervised learning model doesn\u2019t require labelled data or let\u2019s say the answers, but it is not an interpretable model.\n\nANN supervised learning happens in three basic steps build, train, and test:\n\n1.Fit: build the architecture of neurons and their connections and pass the data (or computed data) through weighted connections\n\n2.Optimize: train is about altering the weight of connections between neurons until minimum cost is reached\n\n3.Evaluate: test the results. The more data you have, the more accurate the model will be.\n\nMachine learning was earlier an unapproachable expensive problem due to the lack of data and the less powerful computers. The learning cost of machines is related to\n\ni. efficiency: computational resources like the training time, the complexity of the model and the memory space needed for the model to run\n\nii. accuracy: how close to the truth is the result.\n\nWhile the cost function is about minimizing the error of approximation and ground truth\n\n\u201cYou can have anything you want, but not everything.\u201d\n\nDeep understanding occurs when different small parts manage to be connected in a meaningful way. Knowledge is a majesty with a cost.\n\nLearning is an expensive problem that in the most basic form requires information loading, storing and practising. We unconsciously decide to learn something new after evaluating the below three quantities alone or/and against other choices:\n\ni. effort,\n\nii. time,\n\niii.result expectation.\n\nHow our brain can process images?\n\nV1 visual processing center identifies simple forms like vertical, horizontal, and diagonal edges of contrasting intensities, or lines. Downstream visual centers weave together these basic visual forms to create the beginnings of a visual tapestry. The human brain does so in a fraction of a second and automatically organize information into a \u201cwhole\u201d.\n\nIn the past, we taught a machine to detect, let\u2019s say the letter T by rules such as \u2018these pixels have to be vertical and these horizontal.\u2019 Now, it does so by just seeing thousands T\u2019s. Luckily, the human brain doesn\u2019t need to see a thousand images of a T and to be told a thousand times that this is a T (training data), so as to learn detecting T\u2019s and not T\u2019s ( binary) or even other letters (multiclass classification). Still, it needs unstructured input and functions to train its sense.\n\nIn the next article, we will build, train and test an artificial neural network to learn a binary classification problem."
    }
]