[
    {
        "url": "https://medium.freecodecamp.org/making-your-own-face-recognition-system-29a8e728107c?source=user_profile---------1----------------",
        "title": "Making your own Face Recognition System \u2013",
        "text": "Face recognition is the latest trend when it comes to user authentication. Apple recently launched their new iPhone X which uses Face ID to authenticate users. OnePlus 5 is getting the Face Unlock feature from theOnePlus 5T soon. And Baidu is using face recognition instead of ID cards to allow their employees to enter their offices. These applications may seem like magic to a lot of people. But in this article we aim to demystify the subject by teaching you how to make your own simplified version of a face recognition system in Python.\n\nGithub link for those who do not like reading and only want the code\n\nBefore we get into the details of the implementation I want to discuss the details of FaceNet. Which is the network we will be using in our system.\n\nFaceNet is a neural network that learns a mapping from face images to a compact Euclidean space where distances correspond to a measure of face similarity. That is to say, the more similar two face images are the lesser the distance between them.\n\nFaceNet uses a distinct loss method called Triplet Loss to calculate loss. Triplet Loss minimises the distance between an anchor and a positive, images that contain same identity, and maximises the distance between the anchor and a negative, images that contain different identities.\n\nFaceNet is a Siamese Network. A Siamese Network is a type of neural network architecture that learns how to differentiate between two inputs. This allows them to learn which images are similar and which are not. These images could be contain faces.\n\nSiamese networks consist of two identical neural networks, each with the same exact weights. First, each network take one of the two input images as input. Then, the outputs of the last layers of each network are sent to a function that determines whether the images contain the same identity.\n\nIn FaceNet, this is done by calculating the distance between the two outputs.\n\nNow that we have clarified the theory, we can jump straight into the implementation.\n\nIn our implementation we\u2019re going to be using Keras and Tensorflow. Additionally, we\u2019re using two utility files that we got from deeplearning.ai\u2019s repo to abstract all interactions with the FaceNet network.:\n\nThe first thing we have to do is compile the FaceNet network so that we can use it for our face recognition system.\n\nWe\u2019ll start by initialising our network with an input shape of (3, 96, 96). That means that the Red-Green-Blue (RGB) channels are the first dimension of the image volume fed to the network. And that all images that are fed to the network must be 96x96 pixel images.\n\nNext we\u2019ll define the Triplet Loss function. The function in the code snippet above follows the definition of the Triplet Loss equation that we defined in the previous section.\n\nIf you are unfamiliar with any of the Tensorflow functions used to perform the calculation, I\u2019d recommend reading the documentation (for which I have added links to for each function) as it will improve your understanding of the code. But comparing the function to the equation in Figure 1 should be enough.\n\nOnce we have our loss function, we can compile our face recognition model using Keras. And we\u2019ll use the Adam optimizer to minimise the loss calculated by the Triplet Loss function.\n\nNow that we have compiled FaceNet, we are going to prepare a database of individuals we want our system to recognise. We are going to use all the images contained in our images directory for our database of individuals.\n\nNOTE: We are only going to use one image of each individual in our implementation. The reason is that the FaceNet network is powerful enough to only need one image of an individual to recognise them!\n\nFor each image, we will convert the image data to an encoding of 128 float numbers. We do this by calling the function img_path_to_encoding. The function takes in a path to an image and feeds the image to our face recognition network. Then, it returns the output from the network, which happens to be the encoding of the image.\n\nOnce we have added the encoding for each image to our database, our system can finally start recognising individuals!\n\nAs discussed in the Background section, FaceNet is trained to minimise the distance between images of the same individual and maximise the distance between images of different individuals. Our implementation uses this information to determine which individual the new image fed to our system is most likely to be.\n\nThe function above feeds the new image into a utility function called img_to_encoding. The function processes an image using FaceNet and returns the encoding of the image. Now that we have the encoding we can find the individual that the image most likely belongs to.\n\nTo find the individual, we go through our database and calculate the distance between our new image and each individual in the database. The individual with the lowest distance to the new image is then chosen as the most likely candidate.\n\nFinally, we must determine whether the candidate image and the new image contain the same person or not. Since by the end of our loop we have only determined the most likely individual. This is where the following code snippet comes into play.\n\nNow the tricky part here is that the value 0.52 was achieved through trial-and-error on my behalf for my specific dataset. The best value might be much lower or slightly higher and it will depend on your implementation and data. I recommend trying out different values and see what fits your system best!\n\nNow that we know the details on how we recognise a person using a face recognition algorithm, we can start having some fun with it.\n\nIn the Github repository I linked to at the beginning of this article is a demo that uses a laptop\u2019s webcam to feed video frames to our face recognition algorithm. Once the algorithm recognises an individual in the frame, the demo plays an audio message that welcomes the user using the name of their image in the database. Figure 3 shows an example of the demo in action.\n\nBy now you should be familiar with how face recognition systems work and how to make your own simplified face recognition system using a pre-trained version of the FaceNet network in python!\n\nIf you want to play around with the demonstration in the Github repository and add images of people you know then go ahead and fork the repository.\n\nHave some fun with the demonstration and impress all your friends with your awesome knowledge of face recognition!"
    },
    {
        "url": "https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5?source=user_profile---------2----------------",
        "title": "How to Generate Music using a LSTM Neural Network in Keras",
        "text": "We start by loading each file into a Music21 stream object using the converter.parse(file) function. Using that stream object we get a list of all the notes and chords in the file. We append the pitch of every note object using its string notation since the most significant parts of the note can be recreated using the string notation of the pitch. And we append every chord by encoding the id of every note in the chord together into a single string, with each note being separated by a dot. These encodings allows us to easily decode the output generated by the network into the correct notes and chords.\n\nNow that we have put all the notes and chords into a sequential list we can create the sequences that will serve as the input of our network.\n\nFirst, we will create a mapping function to map from string-based categorical data to integer-based numerical data. This is done because neural network perform much better with integer-based numerical data than string-based categorical data. An example of a categorical to numerical transformation can be seen in Figure 1.\n\nNext, we have to create input sequences for the network and their respective outputs. The output for each input sequence will be the first note or chord that comes after the sequence of notes in the input sequence in our list of notes.\n\nIn our code example, we have put the length of each sequence to be 100 notes/chords. This means that to predict the next note in the sequence the network has the previous 100 notes to help make the prediction. I highly recommend training the network using different sequence lengths to see the impact different sequence lengths can have on the music generated by the network.\n\nThe final step in preparing the data for the network is to normalise the input and one-hot encode the output.\n\nFinally we get to designing the model architecture. In our model we use four different types of layers:\n\nLSTM layers is a Recurrent Neural Net layer that takes a sequence as an input and can return either sequences (return_sequences=True) or a matrix.\n\nDropout layers are a regularisation technique that consists of setting a fraction of input units to 0 at each update during the training to prevent overfitting. The fraction is determined by the parameter used with the layer.\n\nDense layers or fully connected layers is a fully connected neural network layer where each input node is connected to each output node.\n\nThe Activation layer determines what activation function our neural network will use to calculate the output of a node.\n\nNow that we have some information about the different layers we will be using it is time to add them to the network model.\n\nFor each LSTM, Dense, and Activation layer the first parameter is how many nodes the layer should have. For the Dropout layer the first parameter is the fraction of input units that should be dropped during training.\n\nFor the first layer we have to provide a unique parameter called input_shape. The purpose of the parameter is to inform the network of the shape of the data it will be training.\n\nThe last layer should always contain the same amount of nodes as the number different outputs our system has. This assures that the output of the network will map directly to our classes.\n\nFor this tutorial we will use a simple network consisting of three LSTM layers, three Dropout layers, two Dense layers and one activation layer. I would recommend playing around with the structure of the network to see if you can improve the quality of the predictions.\n\nTo calculate the loss for each iteration of the training we will be using categorical cross entropy since each of our outputs only belongs to a single class and we have more than two classes to work with. And to optimise our network we will use a RMSprop optimizer as it is usually a very good choice for recurrent neural networks.\n\nOnce we have determined the architecture of our network the time has come to start the training. The model.fit() function in Keras is used to train the network. The first parameter is the list of input sequences that we prepared earlier and the second is a list of their respective outputs. In our tutorial we are going to train the network for 200 epochs (iterations), with each batch that is propagated through the network containing 64 samples.\n\nTo make sure that we can stop the training at any point in time without losing all of our hard work, we will use model checkpoints. Model checkpoints provide us with a way to save the weights of the network nodes to a file after every epoch. This allows us to stop running the neural network once we are satisfied with the loss value without having to worry about losing the weights. Otherwise we would have to wait until the network has finished going through all 200 epochs before we could get the chance to save the weights to a file.\n\nNow that we have finished training the network it is time to have some fun with the network we have spent hours training.\n\nTo be able to use the neural network to generate music you will have to put it into the same state as before. For simplicity we will reuse code from the training section to prepare the data and set up the network model in the same way as before. Except, that instead of training the network we load the weights that we saved during the training section into the model.\n\nNow we can use the trained model to start generating notes.\n\nSince we have a full list of note sequences at our disposal we will pick a random index in the list as our starting point, this allows us to rerun the generation code without changing anything and get different results every time. However, If you wish to control the starting point simply replace the random function with a command line argument.\n\nHere we also need to create a mapping function to decode the output of the network. This function will map from numerical data to categorical data (from integers to notes).\n\nWe chose to generate 500 notes using the network since that is roughly two minutes of music and gives the network plenty of space to create a melody. For each note that we want to generate we have to submit a sequence to the network. The first sequence we submit is the sequence of notes at the starting index. For every subsequent sequence that we use as input, we will remove the first note of the sequence and insert the output of the previous iteration at the end of the sequence as can be seen in Figure 2.\n\nTo determine the most likely prediction from the output from the network, we extract the index of the highest value. The value at index X in the output array correspond to the probability that X is the next note. Figure 3 helps explain this.\n\nThen we collect all the outputs from the network into a single array.\n\nNow that we have all the encoded representations of the notes and chords in an array we can start decoding them and creating an array of Note and Chord objects.\n\nFirst we have to determine whether the output we are decoding is a Note or a Chord.\n\nIf the pattern is a Chord, we have to split the string up into an array of notes. Then we loop through the string representation of each note and create a Note object for each of them. Then we can create a Chord object containing each of these notes.\n\nIf the pattern is a Note, we create a Note object using the string representation of the pitch contained in the pattern.\n\nAt the end of each iteration we increase the offset by 0.5 (as we decided in a previous section) and append the Note/Chord object created to a list.\n\nNow that we have a list of Notes and Chords generated by the network we can create a Music21 Stream object using the list as a parameter. Then finally to create the MIDI file to contain the music generated by the network we use the write function in the Music21 toolkit to write the stream to a file.\n\nNow it is time to marvel at the results. Figure 4 contains sheet music representation of music that was generated using the LSTM network. At a quick glance we can see that there is some structure to it. This is especially obvious in the third to last line on the second page.\n\nPeople that are knowledgeable about music and can read musical notation can see that there are some weird notes strewn about the sheet. This is a result of the neural network not being able to create perfect melodies. With our current implementation there will always be some false notes and to be able to achieve better results we will need a bigger network."
    }
]