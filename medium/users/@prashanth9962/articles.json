[
    {
        "url": "https://medium.com/@prashanth9962/activation-functions-in-deep-learning-1da530b8a708?source=user_profile---------1----------------",
        "title": "Activation functions in Deep learning \u2013 prashanth k \u2013",
        "text": "Deep learning is an exciting branch of machine learning that uses lots of data, to teach computers how to do things only humans were capable of before, such as recognizing what\u2019s in an image, what people are saying when they are talking on their phones, translating a document into another language, and helping robots explore the world and interact with it.\n\nEach layer in deep neural networks are comprised of nodes, which combines input data with set of weights. The summation of input data and weights are passed through activation function to determine to what extent the value should progress through the network to affect the final prediction\n\nThe sigmoid activation function used in neural networks has an output boundary of (0, 1), and \u03b1 is the offset parameter to set the value at which the sigmoid evaluates to 0.\n\nThe sigmoid function often works fine for gradient descent as long as the input data x is kept within a limit. For large values of x, y is constant. Hence, the derivatives dy/dx (the gradient) equates to 0, which is often termed as the vanishing gradient problem.\n\nThis is a problem because when the gradient is 0, multiplying it with the loss (actual value \u2014 predicted value) also gives us 0 and ultimately networks stop learning.\n\nA neural network can be built by combining some linear classifiers with some non-linear functions. The Rectified Linear Unit (ReLU) has become very popular in the last few years. It computes the function f(x)=max(0,x). In other words, the activation is simply thresholded at zero. Unfortunately, ReLU units can be fragile during training and can die, as a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again, and so the gradient flowing through the unit will forever be zero from that point on.\n\nTo overcome this problem, a leaky function will have a small negative slope (of 0.01, or so) instead of zero when x<0:\n\nThe mean of ReLU activation is not zero and hence sometimes makes learning difficult for the network. The Exponential Linear Unit (ELU) is similar to ReLU activation function when the input x is positive, but for negative values it is a function bounded by a fixed value -1, for \u03b1=1 (the hyperparameter \u03b1 controls the value to which an ELU saturates for negative inputs). This behavior helps to push the mean activation of neurons closer to zero; that helps to learn representations that are more robust to noise."
    },
    {
        "url": "https://medium.com/@prashanth9962/autoencoders-308f14bbbcf6?source=user_profile---------2----------------",
        "title": "Autoencoders \u2013 prashanth k \u2013",
        "text": "Autoencoders are feed-forward , non-recurrent neural networks, which learn by unsupervised learning. They have an inherent capability to learn a compact representation of data. Autoencoders consists of two parts ,an encoder and a decoder.\n\nEncoder : The encoder will read the input and compress(encodes) it to a compact representation.\n\nDecoder : The decoder will read the compact representation and recreate the input from it.\n\nerror e is calculated by taking the difference between the original input x and reconstructed signal r, e = x-r . The autoencoder network then learns by reducing the MSE, and the error is propagated back to the hidden layers. Weights of decoder and encoder are Transpose of each other, which makes it faster to learn training parameters.\n\nTypes of autoencoders based on size of the hidden layer:\n\nTypes of autoencoders based on restrictions imposed on loss :"
    },
    {
        "url": "https://medium.com/@prashanth9962/challenges-of-learning-in-data-science-ec0836c328b5?source=user_profile---------3----------------",
        "title": "Challenges of learning in Data Science \u2013 prashanth k \u2013",
        "text": "The following is an overview of the challenges and issues that we will face when trying to build a model\n\nIt is one of the crucial steps, If we manage to do a good job in this by selecting the proper/right number of features then the rest of the learning process will be easy. In this step we have to derive variable/values that are informative about the learning task and facilitate the next steps of learning and evaluation. Selecting informative features need a prior domain knowledge. Some data will have lots of features and fewer training examples, which is often called as curse of dimensionality. On that time dimensionality reduction techniques such as PCA(Principal Component Analysis) to reduce the vast number of features.\n\nn the fish recognition task, you can see that the length, weight, fish color, as well as the boat color may vary, and there could be shadows, images with low resolution, and other objects in the image. All these issues affect the significance of the proposed explanatory features that should be informative about our fish classification task.\n\nWork-arounds will be helpful in this case. For example, someone might think of detecting the boat ID and mask out certain parts of the boat that most likely won\u2019t contain any fish to be detected by our system. This work-around will limit our search space.\n\nAs we have seen in our fish recognition task, we have tried to enhance our model\u2019s performance by increasing the model complexity and perfectly classifying every single instance of the training samples. As we will see later, such models do not work over unseen data (such as the data that we will use for testing the performance of our model). Having trained models that work perfectly over the training samples but fail to perform well over the testing samples is called overfitting.\n\nIf you sift through the latter part of the chapter, we build a learning system with an objective to use the training samples as a knowledge base for our model in order to learn from it and generalize over the unseen data. Performance error of the trained model is of no interest to us over the training data; rather, we are interested in the performance (generalization) error of the trained model over the testing samples that haven\u2019t been involved in the training phase.\n\nSometimes you are unsatisfied with the execution of the model that you have utilized for a particular errand and you need an alternate class of models. Each learning strategy has its own presumptions about the information it will utilize as a learning base. As an information researcher, you have to discover which suspicions will fit your information best; by this you will have the capacity to acknowledge to attempt a class of models and reject another.\n\nAs discussed in the concepts of model selection and feature extraction, the two issues can be dealt with, if you have prior knowledge about:\n\nHaving prior knowledge of the explanatory features in the fish recognition system enabled us to differentiate amid different types of fish. We can go promote by endeavoring to envision our information and get some feeling of the information types of the distinctive fish classifications. On the basis of this prior knowledge, apt family of models can be chosen.\n\nMissing features mainly occur because of a lack of data or choosing the prefer-not-to-tell option. How can we handle such a case in the learning process? For example, imagine we find the width of specific a fish type is missing for some reason."
    },
    {
        "url": "https://medium.com/@prashanth9962/getting-started-with-tensorflow-b969963e706e?source=user_profile---------4----------------",
        "title": "Getting Started with Tensorflow \u2013 prashanth k \u2013",
        "text": "First let\u2019s learn what exactly TensorFlow is? why is it so popular among many researchers and engineers? and how it works?.\n\nTensorflow is open source deep learning library released by Google research labs which allows to deploy deep neural networks.\n\nWhy is it so popular?\n\nIt works on multi platforms, which means we can train a model and use it on various devices like mobile. It has better computational graph visualisation which makes debugging easier. Also, It has very good community support.\n\nThe underlying concept used in tensorflow is Computation graph. There are two steps involved in Tensorflow :\n\nA computational graph contains nodes and edges. Each node in the compuatation graph can have zero or more inputs but it can have only one output. Nodes in the graph are called ops (short for operations). Inputs to the node are in the form of zero or more Tensors. In TensorFlow terminology, a Tensor is a typed multi-dimensional array.\n\nEdges in the computation graph represents flow of tensors between nodes. Each node can have Constants, Variables , Placeholder. Values which are initialized or defined in nodes are just abstract values. Computation of those values will happens during graph execution.\n\nThe graph is executed with the help of Session Object. The Session Object encapsulates the environment in which tensor and Operation Objects are evaluated. This is the place where actual calculations and transfer of information from one layer to another takes place. The values of different tensor Objects are initialized, accessed, and saved in Session Object only\n\nLets dive into Hello World Program and see how it works\n\nLets breakdown each line and try to understand how it works\n\n2.) We are trying to display a string which is a constant for that tf.constant is used. This object represents a node in the computation graph\n\n3.) To execute the graph element , we need to define the Session using with and run the session using run()"
    },
    {
        "url": "https://medium.com/@prashanth9962/types-of-machine-learning-algorithms-ab08807c6a86?source=user_profile---------5----------------",
        "title": "Types of machine learning Algorithms \u2013 prashanth k \u2013",
        "text": "There are variations of how to define the types of Machine Learning Algorithms but commonly they can be divided into categories according to their purpose and the main categories are the following:\n\nIn the previous two types, either there are no labels for all the observation in the dataset or labels are present for all the observations. Semi-supervised learning falls in between these two. In many practical situations, the cost to label is quite high, since it requires skilled human experts to do that. So, in the absence of labels in the majority of the observations but present in few, semi-supervised algorithms are the best candidates for the model building. These methods exploit the idea that even though the group memberships of the unlabeled data are unknown, this data carries important information about the group parameters.\n\nmethod aims at using observations gathered from the interaction with the environment to take actions that would maximize the reward or minimize the risk. Reinforcement learning algorithm (called the agent) continuously learns from the environment in an iterative fashion. In the process, the agent learns from its experiences of the environment until it explores the full range of possible states.\n\nReinforcement Learning is a type of Machine Learning, and thereby also a branch of Artificial Intelligence. It allows machines and software agents to automatically determine the ideal behavior within a specific context, in order to maximize its performance. Simple reward feedback is required for the agent to learn its behavior; this is known as the reinforcement signal\n\nThere are many different algorithms that tackle this issue. As a matter of fact, Reinforcement Learning is defined by a specific type of problem, and all its solutions are classed as Reinforcement Learning algorithms. In the problem, an agent is supposed decide the best action to select based on his current state. When this step is repeated, the problem is known as a Markov Decision Process.\n\nIn order to produce intelligent programs (also called agents), reinforcement learning goes through the following steps:\n\nSome applications of the reinforcement learning algorithms are computer played board games (Chess, Go), robotic hands, and self-driving cars."
    },
    {
        "url": "https://medium.com/@prashanth9962/machine-learning-things-you-should-know-319cfa181457?source=user_profile---------6----------------",
        "title": "Machine Learning Things you should know \u2013 prashanth k \u2013",
        "text": "Machine learning is the latest buzzword on the web now. It is prevalent every where right from biggest Tech giants like Google, Facebook to small startups. So what is machine learning ? How it is different from normal algorithms? What are the steps involved in machine learning ? What are the types of machine learning and how it is used in real time?\n\nLets try to get answers for the above questions in this post.\n\nThe idea of machine learning sounds like a science fiction thriller or action movie where a computer takes over the world, but it is not the case. In layman\u2019s term given a set of observations, or data and past experience Machine learning algorithm tries to form a set of rules which best fits the data, and uses those rules to answer future questions.\n\nRegular algorithm produces an output on the basis of steps described in the algorithm.\n\nFor example: To know if a number is odd or even, steps are defined which says if modulus 2 of any number is 0 then it is even otherwise it is odd.\n\nML algorithm predicts an output on the basis of learning through the input provided to it. This learning through input is called as Training process.\n\nFor example: To know if a person is going to develop diabetes, data regarding the sugar intake, eating habits, daily lifestyle, workout durations etc of few diabetic person are fed into ML algorithm. On the basis of this data, after the training process is completed the algorithm predicts whether another person is going to develop diabetes or not.\n\nData Collection and Data Preprocessing : It is the systematic approach to gathering and measuring information from a variety of sources to get a complete and accurate picture of an area of interest. Data collection enables a person or organization to answer relevant questions, evaluate outcomes and make prediction about future probabilities and trends. Performance of the algorithm is based on the quality of data. Features with highly correlated values affects the performance, techniques such as dimensionality reduction are used to eliminated those kind of features. Also features which are on different scale need to be transformed to same scale.\n\nTraining the Algorithm : Training involves the running of the actual machine learning algorithm on the training data. The output of this is called a model which is capable of taking an input and performing the task it has learned. So the input to the model should be the same format as that of the training and testing data.\n\nEvaluating the model : The model is then run on the test data and results are recorded to estimate how well the model performs on new data and generalize error. These recorded data are then gathered and mathematical functions are applied to calculate the model\u2019s accuracy. So, when someone says that \u201d This classifier model is 95% accurate\u201d, it means that the classifier can correctly categorize the input 95% of the time."
    },
    {
        "url": "https://medium.com/@prashanth9962/think-yourself-as-the-end-user-5c6b82a72661?source=user_profile---------7----------------",
        "title": "Think from user\u2019s shoulders to get the answer \u2013 prashanth k \u2013",
        "text": "Think from user\u2019s shoulders to get the answer\n\nProduct Management comes with a spec and developer builds the feature.This is the traditional method for product development.When my manager or PM ask me built a feature small or big, I follow these things before I begin\n\nIf there is less clarity about what I\u2019m working, I sit with PM/Engg manager and clearly understand the user story\n\n\u201cEver find yourself working on something without knowing exactly why? Someone just told you to do it. It\u2019s pretty common, actually. That\u2019s why it\u2019s important to ask why you\u2019re working on. What is this for? Who benefits? What\u2019s the motivation behind it? Knowing the answers to these questions will help you better understand the work itself.\u201d \n\n\u2015 Jason Fried, Rework"
    },
    {
        "url": "https://medium.com/@prashanth9962/flip-a-coin-to-take-decision-c0906d73707f?source=user_profile---------8----------------",
        "title": "Flip A Coin to take decision \u2013 prashanth k \u2013",
        "text": "When we come cross hard decision to take we TOSS a coin to help us take a decision.When you toss a coin the probability of getting a Head/Tail is 0.5, which means your choice has 50\u201350 chance.But most of the times we don\u2019t let a coin flip guide our life.Lets see how the coin strategy helps us.\n\nNow you have analysed the pros and cons of a situation but still feel hard to take a step move forward. Designate each side of the coin for one of your two choices. Flip the coin and cover it in hand, you don\u2019t need to see what you got.Instead of looking at coin, ask yourself which result you hoped and thats the path you should take.\n\nWhen we toss the coin our subconscious mind has a desire for one decision or another. It will trigger our intuition and help us to over come decisions that cant be decided by reasons alone."
    },
    {
        "url": "https://medium.com/@prashanth9962/how-facebook-edgerank-works-6e75b1e1381c?source=user_profile---------9----------------",
        "title": "How Facebook EdgeRank works ? \u2013 prashanth k \u2013",
        "text": "You might have noticed Facebook posts will not be in chronological order, but it might follow some pattern. Facebook uses a algorithm that ranks the posts, stories you post. Lets take a look how the algorithm works.\n\nFacebook uses EdgeRank algorithm that decides which stories appear in each user\u2019s newsfeed.So the algorithm is about to predict a story that might be interesting to the particular user.Actions or Triggers by users are called Edges eg, a friend creates post, shares, tags a photos , join a group,RVSP.Facebook ranks these edges and filters newsfeed to only show the top-ranked stories.\n\nThe basic ingredients of the algorithm. The formula used to calculate EdgeRank is\n\nAffinity Score means how \u201cconnected\u201d a particular user is to the Edge. Facebook calculates affinity score by looking at explicit actions that users take, and factoring in\n\nEdge is what action was taken by the user on the content.Facebook changes edge weight based on the user actions like comments, likes, share etc. Eg, photos and videos have a higher weight than links.\n\nAs the post gets older, it loses some points because it became old.Facebook calculates edge weight by multiplying the story by 1/x, where x is the time since the action happened.\n\nIf your posts doesn\u2019t get more likes even if you have 500 friends in your list , then your story has a poor edge rank score. Being more active Facebook by acting in stories posted by friends and tagging places, users will have a better edge rank for your story."
    },
    {
        "url": "https://medium.com/@prashanth9962/open-source-photo-editing-tools-8be532ae4838?source=user_profile---------10----------------",
        "title": "Open source Photo editing tools \u2013 prashanth k \u2013",
        "text": "Editing a photo is important to fix blemishes and make photo look better. For a beginner or part-timer in photography its hard to purchase a photoshop license. There are some open source Image editors for Linux, Mac, Windows.I would list some of the editing tools i have personally used.\n\nGIMP is the one of the most powerful open source editing tool . The UI/UX is bit hard if your from photoshop background.You can even run photoshop plugins using PSPI.\n\nKirtu has professional look and feel like Photoshop.Its been developed by 10years. Its mostly used by artists, illustrators for editing original artworks.\n\nInkscape is an open-source vector graphics editor similar to Adobe Illustrator.Inkscape by default supports the use of Scalable Vector Graphics (SVG).\n\nDarktable is an open source photography workflow application and raw developer. A virtual light table and darkroom for photographers.\n\nFor all the above tools , GIMP has a proper documentation compared to others. Also all of them have excellent community support."
    },
    {
        "url": "https://medium.com/@prashanth9962/how-to-do-bokeh-with-18-55mm-lens-kit-lens-9ef9d66aa7?source=user_profile---------11----------------",
        "title": "How to do bokeh with 18\u201355mm lens(kit lens) \u2013 prashanth k \u2013",
        "text": "Bokeh effect is to blur out the background of the subject. Using bokeh we can create a smooth background and make subject standout.(shallow depth of field).\n\nThe factors that affect the depth of field are\n\nNote : You can get basic idea of how exposure triangle(Aperture ,shutterspeed, ISO) works here.\n\nwe want to use a lens with at least an f/2.8 aperture, with faster apertures of f/2, f/1.8 or f/1.4 being ideal. Many photographers use prime lens to achieve bokeh. But its, possible to achieve bokeh effect using kit lens also. Lets see how we can do it.\n\nAperture is the opening in the lens, the amount of light that enters. To achieve we must keep the aperture to lowest possible number.\n\nThe below photos are taken at two different apertures.Image in the left is shot with a larger aperture number and the right with a smaller aperture number."
    },
    {
        "url": "https://medium.com/@prashanth9962/habits-can-get-you-gold-e9b41544b184?source=user_profile---------12----------------",
        "title": "Habits can get you Gold \u2013 prashanth k \u2013",
        "text": "Behaviour is the way how we represent ourselves to others. Each person will react different in same situation.\n\nA habit is behaviour which we are accustomed to and became a part of ourself.Like carrying a id-card, wallet will be part of our daily behaviour and become a habit.\n\nThis image below shows how these three factors work together to build new habits.\n\nCue: Event that starts the habit. Eg, wake up at 6:00Am.\n\nRoutine: The behaviour that you perform, the habit itself. Eg, do fitness exercises.\n\nReward: the benefit that is associated with the behaviour. Eg, you lead healthier lifestyle.\n\nHabits can get you gold\n\nAugust 2013 olympics, Phelps goggles got filled with moisture during race and he couldn\u2019t see anything.Phelps looked comfortable as he swings his \n\narms and won 4th gold metal. Phelps mentally rehearsed goggle failure so went into habit mode when it occurred.Bowman, Phelps coach had trained him to swim in the dark so that he will be able to face goggle failures in pool.\n\nWhen people asked how he felt swim blind, Phelps said, \u201cLike I imagined it would.\u201d Michael phelps visualizes the entire race, stroke by stroke, from start to finish.\n\nChanging some habits are bit harder, once you understand how habit loop works- diagnose the cue, routine and reward. Its upto us now to decide which habits we should develop or drop."
    },
    {
        "url": "https://medium.com/@prashanth9962/memoization-caching-function-results-5afb14b8db67?source=user_profile---------13----------------",
        "title": "Memoization \u2014 caching function results \u2013 prashanth k \u2013",
        "text": "Memoization is a technique used to cache the expensive function calls to speed up your code.The cache will store the results of the function call for future use.I will explain the use of Memoization with a small example.\n\nHow do you do memoization ?\n\n2. Every time a function is invoked check for the below\n\nLets take a recursive factorial function that takes n as input, calculate the sum of the fibonacci series.\n\nLets, take a look at the callstack for fibonacci(6)\n\nIf you notice the above tree, we are re-calculating f(3) for 3, f(2) for 5.There will be huge compilation time difference between f(5) and f(100).\n\nLets, optimise our function by caching the results to cache\n\nThe above code, we have memoized our recursive function.Also, reduced the time complexity to O(n)."
    },
    {
        "url": "https://medium.com/@prashanth9962/aperture-vs-iso-vs-shutter-speed-bf217d28c7c8?source=user_profile---------14----------------",
        "title": "Exposure Triangle- Aperture vs ISO vs Shutter speed",
        "text": "Most of the DSLR camera comes with auto mode, which automatically pick the right config of ISO,Shutter Speed,Aperture to take a picture.But, using auto mode we are limiting ourselves what we can achieve with it.A better understanding of ISO,Shutter speed and Aperture allow us to take full advantage of camera using manual mode.Lets understand Aperture,ISO,Shutter speed before moving to exposure triangle.\n\nISO is the sensistivity of the image sensor in the range(100,200,400,800,etc). A low iso will require more light Eg. if your shooting in a sunny day a lower ISO 100 works fine. Higher ISO is used when you need to capture at low light Eg. when your shooting at night.\n\nNote:\n\nIf there is plenty of light, higher iso will add more noise or grains.\n\nHigher ISO will help us to shoot at faster shutter speed.\n\nAperture is the opening in the lens, when you hit the shutter release button, the light travels through the opening and hit the camera\u2019s image sensor.Larger the hole, the more light is passed, Smaller the hole, less light is passed.It is measured in f-stops (f/1.4,f/2.8,f/5.6,f/8,f/22).Adding a f-stop will halve the size of the opening in your lens\n\nDepth of field is part of image which gets sharpness and focus.If we have a larger part of the image it\u2019s called deep depth of field, on the contrary it\u2019s called shallow depth of field.Landscape shots will have aperture of f/16.The main factors that affect the depth of field is aperture, distance between the object and camera, and focal length of the lens.\n\nShutter speed also known as exposure time, the length of time the image sensor is exposed to light.Shutter speed is measured in seconds.Shutter speed of 1/100 means 1/100th of a second or 0.01 seconds.Some cameras may omit the fraction and display 500.\n\nchanging the shutter speed from 1/125 to 1/60 means you have increased the shutter speed by 1 step.\n\nShutter speed slower than 1 second are shown by \u02dd symbol.Shutter speed of 1 second or larger is expressed as 1\u02dd,10\u02dd,30\u02dd.\n\nExposure triangle is associating the three factors that affect the exposure of image- Aperture,ISO,Shutter speed.we need to balance all the three factors to achieve desired outcome.Put your camera in manual try to experiment with the 3 attributes and understand how they affect the composition of the image."
    }
]