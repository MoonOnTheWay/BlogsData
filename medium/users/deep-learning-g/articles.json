[
    {
        "url": "https://medium.com/deep-learning-g/handwritten-digit-recognition-using-neural-network-67d7ec76a013?source=---------0",
        "title": "Handwritten Digit Recognition Using Neural Network \u2013 Deep Learning#g \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/deep-learning-g/backpropagation-dc7fe398a0f?source=---------1",
        "title": "Backpropagation \u2013 Deep Learning#g \u2013",
        "text": "We use gradient descent to find the best parameter set to minimize the cost, so it is necessary to compute the gradients of the cost function on the different weights and bias. Backpropagation is an effective way to calculate the gradients.\n\nThe concept of backpropagation originates from the chain rule. By calculating the separated partial derivaives and then multiplying each, the result is obtained. Therefore, the gradient of the cost function can be written as:\n\nFirst, we analyze the first term, \ud835\udf15z/\ud835\udf15w, by considering which layer is:\n\nSo, we know that we first term, \ud835\udf15z/\ud835\udf15w, is the output of previous layer. How about the second term?\n\nFrom the derivation above, the second term, \ud835\udf15C/\ud835\udf15z, can be obtained if we know the next layer\u2019s second term. If we are able to calculate the last layer(output layer)\u2019 s one, we can get the first layer\u2019s. There is the key of \u201cbackpropagation\u201d. Here show the process:(the activation function is sigmoid function, and \ud835\udf15C/\ud835\udf15y depends on the definition of the cost function)\n\nBased on these equations, we obtain a backward netork."
    },
    {
        "url": "https://medium.com/deep-learning-g/stochastic-gradient-descent-63a155ba3975?source=---------2",
        "title": "Stochastic Gradient Descent \u2013 Deep Learning#g \u2013",
        "text": "To begin, we formulate a \u201cCost Function\u201d(or called loss function) to evaluate how bad the parameter set of the neural network is, represented as C(\ud835\udef3). We have to find out the \ud835\udef3* which minimizes the cost C(\ud835\udef3*).\n\nFor example, we use the defined function in the handwritten digit classificatoin. The number of the given training data x is R(1,\u2026, r,\u2026, R) and y is the correspondent ground truth. Our goal is to minimize the C(\ud835\udef3*).\n\nImagine that dropping a ball somewhere, we can then get the local minimun where the ball stop moving just as the picture show.\n\nSo, we repeat the calculation of the gradient at different places, until the gradient approach to zero. Suppose that \ud835\udef3 has two variables {\ud835\udef31, \ud835\udef32}.\n\n\ud835\udf02 represents the learning rate, meaning that \u201chow far\u201d the distance between the before point and the next one calculated. If we pick a small learning rate, it might takes longer time to reach the local minimum while if we set a larger one, we will miss the minimum.\n\nTo calculus the cost, we have to sum all the examples in our training data because of the algorithm of gradient descend, but if there are millions of training data, it takes much time. To simplify, we use stochastic gradient descent.\n\nPick an example x, we assume that all example have equal probabilities to be picked."
    },
    {
        "url": "https://medium.com/deep-learning-g/to-begin-34f7da92b9f8?source=---------3",
        "title": "To Begin \u2013 Deep Learning#g \u2013",
        "text": "Deep learning is one of the domains of machine learning, based on \u201cNeural Networks\u201d.\n\nLiterally, neural network means a bulk of neurons (or called nodes) connect to each others and string to a network. Each neuron transmits information to the next neuron one by one, and the connections between them are computational relationships as the figure shown.\n\nA result will be generated once something is input the network. Lenet, which is deem as the success of using convolution neural networks in computer vision, predicts the handwritten digits by feeding images to the network.\n\nAs networks become larger, we call it \u201cdeep\u201d.\n\nConvolution is a mathematic operation by weight and sum. Convolution in the two-dimensional space amplifies the features in the original image and also elimiates the noises. Therefore, we are able to use the extracted features to recognize images. We can see the edges of items in the image appear when applying convoluton. There is the demo website.\n\nAs we know, there are mathematic relations between all the neurons. Our purpose is to figure out the best parameter combination in networks which manage to produce a nice result. To do so, we put the ground truth and the result produced from the network to formula a \u201ccost function\u201d. As the cost converges, the ideal reult is closer. We call the process \u201clearning\u201d."
    }
]