[
    {
        "url": "https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f?source=user_profile---------1----------------",
        "title": "Python 3 Quick Tip: The easy way to deal with file paths on Windows, Mac and Linux",
        "text": "One of programming\u2019s little annoyances is that Microsoft Windows uses a backslash character between folder names while almost every other computer uses a forward slash:\n\nThis is an accident of early 1980\u2019s computer history. The first version of MS-DOS used the forward slash character for specifying command-line options. When Microsoft added support for folders in MS-DOS 2.0, the forward slash character was already taken so they used a backslash instead. Thirty-five years later, we are still stuck with this incompatibility.\n\nIf you want your Python code to work on both Windows and Mac/Linux, you\u2019ll need to deal with these kinds of platform-specific issues. Luckily, Python 3 has a new module called pathlib that makes working with files nearly painless.\n\nLet\u2019s take a quick look at the different ways of handling filename paths and see how pathlib can make your life better!\n\nLet\u2019s say you have a data folder that contains a file that you want to open in your Python program:\n\nThis is the wrong way to code it in Python:\n\nNotice that I\u2019ve hardcoded the path using Unix-style forward slashes since I\u2019m on a Mac. This will make Windows users angry.\n\nTechnically this code will still work on Windows because Python has a hack where it will recognize either kind of slash when you call open() on Windows. But even still, you shouldn\u2019t depend on that. Not all Python libraries will work if you use wrong kind of slash on the wrong operating system \u2014 especially if they interface with external programs or libraries.\n\nAnd Python\u2019s support for mixing slash types is a Windows-only hack that doesn\u2019t work in reverse. Using backslashes in code will totally fail on a Mac:\n\nFor all these reasons and more, writing code with hardcoded path strings is the kind of thing that will make other programmers look at you with great suspicion. In general, you should try to avoid it.\n\nPython\u2019s os.path module has lots of tools for working around these kinds of operating system-specific file system issues.\n\nYou can use os.path.join() to build a path string using the right kind of slash for the current operating system:\n\nThis code will work perfectly on both Windows or Mac. The problem is that it\u2019s a pain to use. Writing out os.path.join() and passing in each part of the path as a separate string is wordy and unintuitive.\n\nSince most of the functions in the os.path module are similarly annoying to use, developers often \u201cforget\u201d to use them even when they know better. This leads to a lot of cross-platform bugs and angry users.\n\nPython 3.4 introduced a new standard library for dealing with files and paths called pathlib \u2014 and it\u2019s great!\n\nTo use it, you just pass a path or filename into a new Path() object using forward slashes and it handles the rest:\n\nAnd if that\u2019s all pathlib did, it would be a nice addition to Python \u2014 but it does a lot more!\n\nFor example, we can read the contents of a text file without having to mess with opening and closing the file:\n\nIn fact, pathlib makes most standard file operations quick and easy:\n\nYou can even use pathlib to explicitly convert a Unix path into a Windows-formatted path:\n\nAnd if you REALLY want to use backslashes in your code safely, you can declare your path as Windows-formatted and pathlib can convert it to work on the current operating system:\n\nIf you want to get fancy, you can even use pathlib to do things like resolve relative file paths, parse network share paths and generate file:// urls. Here\u2019s an example that will open a local file in your web browser with just two lines a code:\n\nThis was just a tiny peak at pathlib. It\u2019s a great replacement for lots of different file-related functionality that used to be scattered around different Python modules. Check it out!"
    },
    {
        "url": "https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710?source=user_profile---------2----------------",
        "title": "How to break a CAPTCHA system in 15 minutes with Machine Learning",
        "text": "Everyone hates CAPTCHAs \u2014 those annoying images that contain text you have to type in before you can access a website. CAPTCHAs were designed to prevent computers from automatically filling out forms by verifying that you are a real person. But with the rise of deep learning and computer vision, they can now often be defeated easily.\n\nI\u2019ve been reading the excellent book Deep Learning for Computer Vision with Python by Adrian Rosebrock. In the book, Adrian walks through how he bypassed the CAPTCHA on the E-ZPass New York website using machine learning:\n\nAdrian didn\u2019t have access to the source code of the application generating the CAPTCHA image. To break the system, he had to download hundreds of example images and manually solve them to train his system.\n\nBut what if we want to break an open source CAPTCHA system where we do have access to the source code?\n\nI went to the WordPress.org Plugin Registry and searched for \u201ccaptcha\u201d. The top result is called \u201cReally Simple CAPTCHA\u201d and has over 1 million active installations:\n\nAnd best of all, it comes with source! Since we\u2019ll have the source code that generates the CAPTCHAs, this should be pretty easy to break. To make things a little more challenging, let\u2019s give ourself a time limit. Can we fully break this CAPTCHA system in less than 15 minutes? Let\u2019s try it!\n\nImportant note: This is in no way a criticism of the \u2018Really Simple CAPTCHA\u2019 plugin or its author. The plugin author himself says that it\u2019s not secure anymore and recommends that you use something else. This is just meant as a fun and quick technical challenge. But if you are one of the remaining 1+ million users, maybe you should switch to something else :)\n\nTo form a plan of attack, let\u2019s see what kinds of images Really Simple CAPTCHA generates. On the demo site, we see this:\n\nOk, so the CAPTCHA images seem to be four letters. Let\u2019s verify that in the PHP source code:\n\nYep, it generates 4-letter CAPTCHAs using a random mix of four different fonts. And we can see that it never uses \u201cO\u201d or \u201cI\u201d in the codes to avoid user confusion. That leaves us with a total of 32 possible letters and numbers that we need to recognize. No problem!\n\nBefore we go any further, let\u2019s mention the tools that we\u2019ll use to solve this problem:\n\nPython is a fun programming language with great libraries for machine learning and computer vision.\n\nOpenCV is a popular framework for computer vision and image processing. We\u2019ll use OpenCV to process the CAPTCHA images. It has a Python API so we can use it directly from Python.\n\nKeras is a deep learning framework written in Python. It makes it easy to define, train and use deep neural networks with minimal coding.\n\nTensorFlow is Google\u2019s library for machine learning. We\u2019ll be coding in Keras, but Keras doesn\u2019t actually implement the neural network logic itself. Instead, it uses Google\u2019s TensorFlow library behind the scenes to do the heavy lifting.\n\nOk, back to the challenge!\n\nTo train any machine learning system, we need training data. To break a CAPTCHA system, we want training data that looks like this:\n\nSince we have the source code to the WordPress plug-in, we can modify it to save out 10,000 CAPTCHA images along with the expected answer for each image.\n\nAfter a couple of minutes of hacking on the code and adding a simple \u2018for\u2019 loop, I had a folder with training data \u2014 10,000 PNG files with the correct answer for each as the filename:\n\nThis is the only part where I won\u2019t give you working example code. We\u2019re doing this for education and I don\u2019t want you to actually go out and spam real WordPress sites. However I will give you the 10,000 images I generated at the end so that you can replicate my results.\n\nNow that we have our training data, we could use it directly to train a neural network:\n\nWith enough training data, this approach might even work \u2014 but we can make the problem a lot simpler to solve. The simpler the problem, the less training data and the less computational power we\u2019ll need to solve it. We\u2019ve only got 15 minutes after all!\n\nLuckily the CAPTCHA images are always made up of only four letters. If we can somehow split the image apart so that that each letter is a separate image, then we only have to train the neural network to recognize a single letter at a time:\n\nI don\u2019t have time to go through 10,000 training images and manually split them up into separate images in Photoshop. That would take days and I\u2019ve only got 10 minutes left. And we can\u2019t just split the images into four equal-size chunks because the CAPTCHA randomly places the letters in different horizontal locations to prevent that:\n\nLuckily, we can still automate this. In image processing, we often need to detect \u201cblobs\u201d of pixels that have the same color. The boundaries around those continuous pixels blobs are called contours. OpenCV has a built-in findContours() function that we can use to detect these continuous regions.\n\nAnd then we\u2019ll convert the image into pure black and white (this is called thresholding) so that it will be easy to find the continuous regions:\n\nNext, we\u2019ll use OpenCV\u2019s findContours() function to detect the separate parts of the image that contain continuous blobs of pixels of the same color:\n\nThen it\u2019s just a simple matter of saving each region out as a separate image file. And since we know each image should contain four letters from left-to-right, we can use that knowledge to label the letters as we save them. As long as we save them out in that order, we should be saving each image letter with the proper letter name.\n\nBut wait \u2014 I see a problem! Sometimes the CAPTCHAs have overlapping letters like this:\n\nThat means that we\u2019ll end up extracting regions that mash together two letters as one region:\n\nIf we don\u2019t handle this problem, we\u2019ll end up creating bad training data. We need to fix this so that we don\u2019t accidentally teach the machine to recognize those two squashed-together letters as one letter.\n\nA simple hack here is to say that if a single contour area is a lot wider than it is tall, that means we probably have two letters squished together. In that case, we can just split the conjoined letter in half down the middle and treat it as two separate letters:\n\nNow that we have a way to extract individual letters, let\u2019s run it across all the CAPTCHA images we have. The goal is to collect different variations of each letter. We can save each letter in it\u2019s own folder to keep things organized.\n\nHere\u2019s a picture of what my \u201cW\u201d folder looked like after I extracted all the letters:\n\nSince we only need to recognize images of single letters and numbers, we don\u2019t need a very complex neural network architecture. Recognizing letters is a much easier problem than recognizing complex images like pictures like cats and dogs.\n\nWe\u2019ll use a simple convolutional neural network architecture with two convolutional layers and two fully-connected layers:"
    },
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196?source=user_profile---------3----------------",
        "title": "Machine Learning is Fun Part 8: How to Intentionally Trick Neural Networks",
        "text": "This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8!\n\nYou can also read this article in \u0420\u0443\u0441\u0441\u043a\u0438\u0439, Ti\u1ebfng Vi\u1ec7t or \ud55c\uad6d\uc5b4.\n\nAlmost as long as programmers have been writing computer programs, computer hackers have been figuring out ways to exploit those programs. Malicious hackers take advantage of the tiniest bugs in programs to break into systems, steal data and generally wreak havoc.\n\nBut systems powered by deep learning algorithms should be safe from human interference, right? How is a hacker going to get past a neural network trained on terabytes of data?\n\nIt turns out that even the most advanced deep neural networks can be easily fooled. With a few tricks, you can force them into predicting whatever result you want:\n\nSo before you launch a new system powered by deep neural networks, let\u2019s learn exactly how to break them and what you can do to protect yourself from attackers.\n\nLet\u2019s imagine that we run an auction website like Ebay. On our website, we want to prevent people from selling prohibited items \u2014 things like live animals.\n\nEnforcing these kinds of rules are hard if you have millions of users. We could hire hundreds of people to review every auction listing by hand, but that would be expensive. Instead, we can use deep learning to automatically check auction photos for prohibited items and flag the ones that violate the rules.\n\nThis is a typical image classification problem. To build this, we\u2019ll train a deep convolutional neural network to tell prohibited items apart from allowed items and then we\u2019ll run all the photos on our site through it.\n\nFirst, we need a data set of thousands of images from past auction listings. We need images of both allowed and prohibited items so that we can train the neural network to tell them apart:\n\nTo train then neural network, we use the standard back-propagation algorithm. This is an algorithm were we pass in a training picture, pass in the expected result for that picture, and then walk back through each layer in the neural network adjusting their weights slightly to make them a little better at producing the correct output for that picture:\n\nWe repeat this thousands of times with thousands of photos until the model reliably produces the correct results with an acceptable accuracy.\n\nThe end result is a neural network that can reliably classify images:\n\nNote: If you want more detail on how convolution neural networks recognize objects in images, check out Part 3.\n\nConvolutional neural networks are powerful models that consider the entire image when classifying it. They can recognize complex shapes and patterns no matter where they appear in the image. In many image recognition tasks, they can equal or even beat human performance.\n\nWith a fancy model like that, changing a few pixels in the image to be darker or lighter shouldn\u2019t have a big effect on the final prediction, right? Sure, it might change the final likelihood slightly, but it shouldn\u2019t flip an image from \u201cprohibited\u201d to \u201callowed\u201d.\n\nBut in a famous paper in 2013 called Intriguing properties of neural networks, it was discovered that this isn\u2019t always true. If you know exactly which pixels to change and exactly how much to change them, you can intentionally force the neural network to predict the wrong output for a given picture without changing the appearance of the picture very much.\n\nThat means we can intentionally craft a picture that is clearly a prohibited item but which completely fools our neural network:\n\nWhy is this? A machine learning classifier works by finding a dividing line between the things it\u2019s trying to tell apart. Here\u2019s how that looks on a graph for a simple two-dimensional classifier that\u2019s learned to separate green points (acceptable) from red points (prohibited):\n\nRight now, the classifier works with 100% accuracy. It\u2019s found a line that perfectly separates all the green points from the red points.\n\nBut what if we want to trick it into mis-classifying one of the red points as a green point? What\u2019s the minimum amount we could move a red point to push it into green territory?\n\nIf we add a small amount to the Y value of a red point right beside the boundary, we can just barely push it over into green territory:\n\nSo to trick a classifier, we just need to know which direction to nudge the point to get it over the line. And if we don\u2019t want to be too obvious about being nefarious, ideally we\u2019ll move the point as little as possible so it just looks like an honest mistake.\n\nIn image classification with deep neural networks, each \u201cpoint\u201d we are classifying is an entire image made up of thousands of pixels. That gives us thousands of possible values that we can tweak to push the point over the decision line. And if we make sure that we tweak the pixels in the image in a way that isn\u2019t too obvious to a human, we can fool the classifier without making the image look manipulated.\n\nIn other words, we can take a real picture of one object and change the pixels very slightly so that the image completely tricks the neural network into thinking that the picture is something else \u2014 and we can control exactly what object it detects instead:"
    },
    {
        "url": "https://medium.com/@ageitgey/quick-tip-speed-up-your-python-data-processing-scripts-with-process-pools-cf275350163a?source=user_profile---------4----------------",
        "title": "Quick Tip: Speed up your Python data processing scripts with Process Pools",
        "text": "Python is a great programming language for crunching data and automating repetitive tasks. Got a few gigs of web server logs to process or a million images that need resizing? No problem! You can almost always find a helpful Python library that makes the job easy.\n\nBut while Python makes coding fun, it\u2019s not always the quickest to run. By default, Python programs execute as a single process using a single CPU. If you have a computer made in the last decade, there\u2019s a good chance it has 4 (or more) CPU cores. That means that 75% or more of your computer\u2019s power is sitting there nearly idle while you are waiting for your program to finish running!\n\nLet\u2019s learn how to take advantage of the full processing power of your computer by running Python functions in parallel. Thanks to Python\u2019s module, it only takes 3 lines of code to turn a normal program into one that can process data in parallel.\n\nLet\u2019s say we have a folder full of photos and we want to create thumbnails of each photo.\n\nHere\u2019s a short program that uses Python\u2019s built-in glob function to get a list of all the jpeg files in a folder and then uses the Pillow image processing library to save out 128-pixel thumbnails of each photo:\n\nThis program follows a simple pattern you\u2019ll often see in data processing scripts:\n\nLet\u2019s test this program on a folder with 1000 jpeg files and see how long it takes to run:\n\nThe program took 8.9 seconds to run. But how hard was the computer working?\n\nLet\u2019s run the program again and check Activity Monitor while it\u2019s running:\n\nThe computer is 75% idle! What\u2019s up with that?\n\nThe problem is that my computer has 4 CPU cores, but Python is only using one of them. So while I\u2019m maxing out the capacity of one CPU, the other three CPUs aren\u2019t doing anything. I need a way to split the work load into 4 separate chunks that I can run in parallel. Luckily Python has an easy way to do this!\n\nHere\u2019s an approach we can use to process this data in parallel:\n\nFour copies of Python running on four separate CPUs should be able to do roughly 4 times as much work as one CPU, right?\n\nThe neat part is that Python handles all the grunt work for us. We just tell it which function we want to run and how many instances of Python to use, and it does the rest. We only have to change 3 lines of code.\n\nFirst, we need to import the library. This library is built right into Python:\n\nNext, we need to tell Python to boot up 4 extra Python instances. We do that by telling it to create a Process Pool:\n\nBy default, it will create one Python process for each CPU in your machine. So if you have 4 CPUs, this will start up 4 Python processes.\n\nThe final step is to ask the Process Pool to execute our helper function on our list of data using those 4 processes. We can do that by replacing the original loop we had:\n\nWith this new call to :\n\nThe function takes in the helper function to call and the list of data to process with it. It does all the hard work of splitting up the list, sending the sub-lists off to each child process, running the child processes, and combining the results. Neat!\n\nThis also gives us back the result of each function call. The function returns results in the same order as the list of data we gave it to process. So I\u2019ve used Python\u2019s function as a shortcut to grab the original filename and the matching result in one step.\n\nHere\u2019s how our program looks with those three changes:\n\nLet\u2019s run the program and see if it finishes any faster:\n\nIt finished in 2.2 seconds! That\u2019s a 4x speed-up over the original version. The elapsed time was faster because we are using 4 CPUs instead of just one.\n\nBut if you look closely, you\u2019ll see that the \u201cuser\u201d time was almost 9 seconds. How did the program finish in 2.2 seconds but still somehow run for 9 seconds? That seems\u2026 impossible?\n\nThat\u2019s because the \u201cuser\u201d time is a sum of CPU time across all CPUs. We completed the same 9 seconds of work as last time \u2014 but we finished it with 4 CPUs in only 2.2 real-world seconds!\n\nNote: There is some overhead in spawning more Python processes and shuffling data around between them, so you won\u2019t always get this much of a speed improvement. If you are working with huge data sets, there is a trick with setting the chunksize parameter that can help a lot.\n\nUsing Process Pools is a great solution when you have a list of data to process and each piece of data can be processed independently. Here are some examples of when multiprocessing is a good fit:\n\nBut Process Pools aren\u2019t always the answer. Using a Process Pool requires passing data back and forth between separate Python processes. If the data you are working with can\u2019t be efficiently passed between processes, this won\u2019t work. The data you are processing needs to be a type that Python knows how to pickle.\n\nAlso, the data won\u2019t be processed in a predictable order. If you need the result from processing the previous piece of data to process the next piece of data, this won\u2019t work.\n\nYou might have heard that Python has a Global Interpreter Lock, or GIL. That means that even if your program is multi-threaded, only one instruction of Python code can be executed at once by any thread. In other words, multi-threaded Python code can\u2019t truly run in parallel.\n\nBut Process Pools work around this issue! Because we are running truly separate Python instances, each instance has it\u2019s own GIL. You get true parallel execution of Python code (at the cost of some extra overhead).\n\nWith the library, Python gives you a simple way to tweak your scripts to use all the CPU cores in your computer at once. Don\u2019t be afraid to try it out. Once you get the hang of it, it\u2019s as simple as using a loop, but often a whole lot faster."
    },
    {
        "url": "https://medium.com/@ageitgey/learn-how-to-use-static-type-checking-in-python-3-6-in-10-minutes-12c86d72677b?source=user_profile---------5----------------",
        "title": "How to Use Static Type Checking in Python 3.6 \u2013 Adam Geitgey \u2013",
        "text": "One of the most common complaints about the Python language is that variables are Dynamically Typed. That means you declare variables without giving them a specific data type. Types are automatically assigned at based on what data was passed in:\n\nIn this case, the variable is created as str type because we passed in a string. But Python didn\u2019t know it would be a string until it actually ran that line of code.\n\nBy comparison, a language like Java is Statically Typed. To create the same variable in Java, you have to declare the string explicitly with a String type:\n\nBecause Java knows ahead of time that can only hold a String, it will give you a compile error if you try to do something silly like store an integer in it or pass it into a function that expects something other than a String.\n\nIt\u2019s usually faster to write new code in a dynamically-typed language like Python because you don\u2019t have to write out all the type declarations by hand. But when your codebase starts to get large, you\u2019ll inevitably run into lots of runtime bugs that static typing would have prevented.\n\nHere\u2019s an example of an incredibly common kind of bug in Python:\n\nAll we are doing is asking the user for their name and then printing out \u201cHi, <first name>!\u201d. And if the user doesn\u2019t type anything, we want to print out \u201cHi, UserFirstName!\u201d as a fallback.\n\nThis program will work perfectly if you run it and type in a name\u2026 BUT it will crash if you leave the name blank:\n\nThe problem is that isn\u2019t a string \u2014 it\u2019s a Dictionary. So calling on fails horribly because it doesn\u2019t have a function.\n\nIt\u2019s a simple and obvious bug to fix, but what makes this bug insidious is that you will never know the bug exists until a user happens to run the program and leave the name blank. You might test the program a thousand times yourself and never notice this simple bug because you always typed in a name.\n\nStatic typing prevents this kind of bug. Before you even try to run the program, static typing will tell you that you can\u2019t pass into because it expects a str but you are giving it a Dict. Your code editor can even highlight the error as you type!\n\nWhen this kind of bug happens in Python, it\u2019s usually not in a simple function like this. The bug is usually buried several layers down in the code and triggered because the data passed in is slightly different than previously expected. To debug it, you have to recreate the user\u2019s input and figure out where it went wrong. So much time is wasted debugging these easily preventable bugs.\n\nThe good news is that you can now use static typing in Python if you want to. And as of Python 3.6, there\u2019s finally a sane syntax for declaring types.\n\nLet\u2019s update the buggy program by declaring the type of each variable and each function input/output. Here\u2019s the updated version:\n\nIn Python 3.6, you declare a variable type like this:\n\nIf you are assigning an initial value when you create the variable, it\u2019s as simple as this:\n\nAnd you declare a function\u2019s input and output types like this:\n\nIt\u2019s pretty simple \u2014 just a small tweak to the normal Python syntax. But now that the types are declared, look what happens when I run the type checker:\n\nWithout even executing the program, it knows there\u2019s no way that line 16 will work! You can fix the error right now without waiting for a user to discover it three months from now.\n\nAnd if you are using an IDE like PyCharm, it will automatically check types and show you where something is wrong before you even hit \u201cRun\u201d:\n\nDeclaring or variables is simple. The headaches happen when you are working with more complex data types like nested lists and dictionaries. Luckily Python 3.6\u2019s new syntax for this isn\u2019t too bad\u2014 at least not for a language that added typing as an afterthought.\n\nThe basic pattern is to import the name of the complex data type from the module and then pass in the nested types in brackets.\n\nThe most common complex data types you\u2019ll use are , and . Here\u2019s what it looks like to use them:\n\nTuples are a little bit special because they let you declare the type of each element separately:\n\nYou also create aliases for complex types just by assigning them to a new name:\n\nSometimes your Python functions might be flexible enough to handle several different types or work on any data type. You can use the type to declare a function that can accept multiple types and you can use to accept anything.\n\nPython 3.6 also supports some of the fancy typing stuff you might have seen in other programming languages like generic types and custom user-defined types.\n\nWhile Python 3.6 gives you this syntax for declaring types, there\u2019s absolutely nothing in Python itself yet that does anything with these type declarations. To actually enforce type checking, you need to do one of two things:\n\nI\u2019d recommend doing both. PyCharm and mypy use different type checking implementations and they can each catch things that the other doesn\u2019t. You can use PyCharm for realtime type checking and then run mypy as part of your unit tests as a final verification.\n\nThis type declaration syntax is very new to Python \u2014 it only fully works as of Python 3.6. If you show your typed Python code to another Python developer, there\u2019s a good chance they will think you are crazy and not even believe that the syntax is valid. And the mypy type checker is still under development and doesn\u2019t claim to be stable yet.\n\nSo it might be a bit early to go whole hog on this for all your projects. But if you are working on a new project where you can make Python 3.6 a minimum requirement, it might be worth experimenting with typing. It has the potential to prevent lots of bugs.\n\nOne of the neat things is that you easily can mix code with and with out type declarations. It\u2019s not all-or-nothing. So you can start by declaring types in the places that are the most valuable without changing all your code. And since Python itself won\u2019t do anything with these types at runtime, you can\u2019t accidentally break your program by adding type declarations."
    },
    {
        "url": "https://medium.com/@ageitgey/quick-tip-the-easiest-way-to-grab-data-out-of-a-web-page-in-python-7153cecfca58?source=user_profile---------6----------------",
        "title": "Quick Tip: The easiest way to grab data out of a web page in Python",
        "text": "Let\u2019s say you are searching the web for some raw data you need for a project and you stumble across a webpage like this:\n\nYou found exactly what you need \u2014 an up-to-date page with exactly the data you need!\n\nBut the bad news is that the data lives inside a web page and there\u2019s no API that you can use to grab the raw data. So now you have to waste 30 minutes throwing together a crappy script to scrape the data. It\u2019s not hard, but it\u2019s a waste of time that you could spend on something useful. And somehow 30 minutes always ends up being 2 hours.\n\nFor me, this kind of thing happens all the time.\n\nLuckily, there\u2019s a super simple answer. The Pandas library has a built-in method to scrape tabular data from html pages called read_html():\n\nIt\u2019s that simple! Pandas will find any significant html tables on the page and return each one as a new DataFrame object.\n\nTo upgrade our program from toy to real, let\u2019s tell Pandas that row 0 of the table has column headers and ask it to convert text-based dates into time objects:\n\nWhich gives you this beautiful output:\n\nAnd how that the data lives in a DataFrame, the world is yours. Wish the data was available as json records? That\u2019s just one more line of code!\n\nIf you run that, you\u2019ll get this beautiful json output (even with proper ISO 8601 date formatting!):\n\nYou can even save the data right to a CSV or XLS file:\n\nRun that and double-click on calls.csv to open it up in your spreadsheet app:\n\nAnd of course Pandas makes it simple to filter, sort or process the data further:\n\nNone of this is rocket science or anything, but I use it so often that I thought it was worth sharing. Have fun!"
    },
    {
        "url": "https://medium.com/@ageitgey/mobile-deep-linking-part-2-using-a-hosted-deep-links-provider-c61fa58d083e?source=user_profile---------7----------------",
        "title": "Mobile Deep Linking Part 2: Using a Hosted Deep Links Provider",
        "text": "Previously I wrote about how to implement iOS and Android Mobile Deep Linking yourself. You can tell from the length of that article that implementing deep linking can be time consuming and painful, especially when you hit annoying corner cases where things don\u2019t work as expected.\n\nBut you don\u2019t have to build it yourself. Hosted deep linking services are also popular. Lots of big companies like Pinterest, Jet and Skyscanner use hosted solutions. This time, let\u2019s talk about when you might (or might not) want to use one of these hosted deep link providers and how they work as an alternative to building it yourself.\n\nImportant disclosure: I researched this topic for Branch. They liked part one and paid for my time to write this follow-up on using hosted deep linking solutions. They didn\u2019t tell me what to write, but I just want to be clear that they sponsored this work.\n\nNormally when a mobile user clicks on a link to your company\u2019s website, the website opens in their web browser \u2014 even if the user already has your mobile app installed.\n\nWith deep linking, you can make it so that your company\u2019s mobile app opens up instead. You can also make sure that your app opens to the correct screen that corresponds with the link:\n\nSince native apps tend to convert better and offer a richer experience than mobile websites, most of the time it makes sense to implement deep linking. It\u2019s a great way to get users back into your app if they aren\u2019t in the habit of launching the app themselves. Users can disable it if they don\u2019t like it.\n\nAnd even if your company is app-centric and doesn\u2019t have much of a website, implementing mobile deep linking still makes sense since you can use it to build a web-based user acquisition funnel to drive app downloads.\n\nBoth Apple\u2019s iOS and Google\u2019s Android provide their own standards for how to implement deep linking. Apple calls their version \u2018Universal Links\u2019 and Google calls theirs \u2018App Links\u2019, but they are roughly the same thing done in different ways.\n\nBut since Apple and Google already give you a way to set up deep linking yourself, why would you want to use a hosted service instead?\n\nThe first thing to consider is that deep linking is full of corner cases where things break in unexpected ways. Here are a handful of examples I\u2019ve run into personally:\n\nThe list goes on. Someone on your team will have to become a deep linking expert to maintain everything and debug problems. If you don\u2019t want to dedicate time to that, using a hosted provider avoids most of these headaches (with some caveats I\u2019ll mention later).\n\nThe hosted providers also offer additional features on top of what Apple and Google provide. Some of these are features that you\u2019d probably end up building yourself anyway:\n\nApple\u2019s deep linking system doesn\u2019t give you a way to remember which web page the user was on before they installed your app. That means you often end up with this broken on boarding experience:\n\nMaybe you\u2019ve had that experience yourself when trying out a new app. It\u2019s pretty annoying.\n\nWhen you use an hosted provider\u2019s SDK, the SDK can capture the user\u2019s context before installation and pass that to your app so it can show the right screen to continue the user\u2019s flow. This is called \u201cDeferred Deep Linking\u201d and it makes a huge difference in how slick your app feels to the user.\n\nIf your company has a separate marketing team, you\u2019ll want to make sure any links they use in marketing or advertisements are deep links. But this means that the marketing team will come back to you over and over asking how to link to specific content in a way that will work as a deep link and be tracked correctly. This can suck up a lot of your time.\n\nThe hosted deep link providers all give you a web-based dashboard you use to compose new deep links and view traffic reports. You can just give your marketing team an account and not mess with it yourself. It\u2019s a big time saver in my experience:\n\nThe hosted deep link provider SDKs also let you compose new deep links on-the-fly inside the app itself. You can use this to implement dynamic features like user-to-user link sharing or user-to-user referral tracking.\n\nThere are several companies and start-ups that offer hosted deep linking services. The services I see developers use most often are Firebase Dynamic Links and Branch \u2014 probably because both offer free plans. I also know some folks who use adjust, but I won\u2019t cover here since it doesn\u2019t offer a free plan as far as I know.\n\nFirebase Dynamic Links SDK is an add-on to Google\u2019s Firebase Mobile App Platform. The Firebase Mobile App Platform provides a collection of libraries and SDKs that apps can use to save time implementing common features like login, file storage, and analytics. Dynamic Links SDK is the specific module that handles deep linking.\n\nTake a look at the whole Firebase suite of offerings:\n\nDynamic Links is way over on the right. In other words, Firebase tries to do everything and deep links is just one of those things.\n\nBranch is a stand-alone SDK specifically for deep linking. It\u2019s a single-purpose tool (though they also have add-on products for specialized use cases like sending high-volume emails with deep links).\n\nThe amount of work required to integrate with either the Firebase Dynamic Links SDK or the Branch SDK is similar. And even if you use other Firebase modules in your app (like Store or Authentication), you can still use the Branch SDK for deep linking just as easily as using the Dynamic Links SDK.\n\nBoth services are free! There\u2019s no subscription fee to use either of them. But any free service makes me curious about the business model behind it because you want to be sure the service will stick around.\n\nGoogle\u2019s core business is built on being able to track how users behave on the web. When users go into native apps, Google can\u2019t normally track that traffic on iOS. My guess is that Google offers the Firebase Dynamic Links SDK for free as a way to collect more information on users linking into native apps.\n\nBranch has a different strategy. They use the information collected from their free SDK to power their user matching feature that tells you which web user clicked through into your app:\n\nIn other words, the more apps that include the Branch SDK, the better they can track when a specific user installs your app. So it\u2019s advantageous for them to get as many different apps as possible using their SDK to build their user database. To actually make money to support the product, Branch offers additional paid products that build on the free feature set.\n\nTo make deep linking work, you need to permanently tie a website domain name to your app. Then any time the user clicks on links to that domain name, your app will get a chance to intercept those links and open itself.\n\nHowever when you are using a third party deep linking service, you can\u2019t use your your normal website domain name for deep links. You have to set up an alternate domain name to intercept the links.\n\nUnfortunately Firebase is very limited in the domain names that you can use. With Firebase, all your deep link urls will look like https://<something>.app.goo.gl.\n\nTo me, that\u2019s a big problem. I don\u2019t want to link my users to rpb3r.app.goo.gl instead of the my normal website domain (in this case, that should be https://www.npr.org). Unbranded urls like this have a negative affect on click-through rate. I know that I wouldn\u2019t click on a link like that because I\u2019d be suspicious it was a link to a scam website.\n\nBranch\u2019s solution still doesn\u2019t let your use your main website domain name (though they say they are working on it), but it gives you a lot more flexibility with how you name your deep links:\n\nWhile I\u2019d rather use my normal web site address, Branch supporting subdomains for deep links is workable. For the NPR example, that might be something like https://go.npr.org/ instead of https://rpb3r.app.goo.gl/.\n\nFirebase currently only supporting \u201c.app.goo.gl\u201d links is a deal breaker for me. Hopefully they\u2019ll expand their capabilities in the future.\n\nBoth Firebase and Branch give you tools for attributing user actions to different channels (i.e. tracking how the user found you) and building reports from the data.\n\nWithout a tool like this, it can be hard to figure out which marketing channels are leading to new app installs.\n\nOne trick that you can only do with Branch is track which exact user just installed your app from a link on your website. Some companies use this feature to implement automatic login for known users the very first time they launch the app without ever asking for a password.\n\nBoth iOS and Android have a built-in way to show a promo banner on your website if the user hasn\u2019t downloaded your app yet:\n\nUnfortunately you don\u2019t have much control over these default banners. They display only when Apple or Google decide to show them (with different rules on each platform) and you can\u2019t change how they look.\n\nBranch gives you a toolkit for building custom mobile promo banners. You can control when these banners show up, which users see them, and how they look:\n\nBranch also provides tools for promoting you app to desktop users. You can display a promo webpage that lets users text themselves a link to download your app:\n\nFirebase doesn\u2019t currently have any customized app banner features.\n\nThe Twitter and Facebook iOS apps block deep links and force all links to open inside a web preview window. Because of this, if anyone posts a link to your content on those social networks, your deep links won\u2019t fire and your app won\u2019t open. This is frustrating if you depend on social media to drive traffic to your app.\n\nBecause so many companies depend on Twitter and Facebook traffic, both Firebase and Branch provide workarounds to allow users to still be able to open your app with an extra click.\n\nWith Firebase, the user will see a blank interstitial page that says \u201cOpen link in app?\u201d:\n\nIf the user clicks \u201cOpen\u201d, your app will open. If not, nothing happens.\n\nIt\u2019s a confusing user experience and unfortunately you can\u2019t customize how it looks or works.\n\nBranch has a much more flexible version of this idea that they call Deepviews. It lets you create a custom interstitial pages that preview the actual content that the user will see inside the app while encouraging the user to click through.\n\nYou can even edit the content, style and layout of what the user sees in the preview page:\n\nIf you depend on social media traffic, having a good solution here will save you a lot of headaches. Hopefully Firebase will improve their solution over time.\n\nEven if you like the idea of using a hosted deep links provider, there are some cases where you might still want to implement deep links by hand using Apple and Google\u2019s tools.\n\nFor example, maybe you have a receipt email that you send to users after they purchase and the email has a link to view order status. If you\u2019ve already sent millions of those emails, you can\u2019t change the links in those emails you\u2019ve already sent \u2014 so you can\u2019t point those users to a new deep-link-enabled url.\n\nIn that case, it might make sense to implement deep linking yourself to support those older links. But it\u2019s still worth considering using a hosted provider for newer links. You can use both together.\n\nThe second case is that you just might not be comfortable using a third party for deep linking for whatever reason. Some companies work in specialized industries where you aren\u2019t allowed to share data with anyone else (i.e. heath care or security). In that case, you might be forced to build deep linking yourself.\n\nIf I am building a new app for a start-up, I\u2019d opt to use a hosted deep linking provider. It\u2019s easier than maintaining it yourself, especially if you have a small team and can\u2019t dedicate a person to it.\n\nOn the other hand if I was implementing deep links for a large company that already had millions of users and a mobile website, I\u2019d go with to a hybrid solution:\n\nBetween Firebase Dynamic Links and Branch, I\u2019d personally choose Branch. The biggest issue to me is that Firebase makes you use those tacky http://xyz.yoursite.app.goo.gl links instead of being able to use a custom domain. The other reason I\u2019d use Branch is because they do a better job of handling how deep links work inside the Facebook and Twitter iOS apps."
    },
    {
        "url": "https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b?source=user_profile---------8----------------",
        "title": "Try Deep Learning in Python now with a fully pre-configured VM",
        "text": "I love to write about face recognition, image recognition and all the other cool things you can build with machine learning. Whenever possible, I try to include code examples or even write libraries/APIs to make it as easy as possible for a developer to play around with these fun technologies.\n\nBut the number one question I get asked is \u201cHow in the world do I get all these open source libraries installed and working on my computer?\u201d\n\nIf you aren\u2019t a long-time Linux user, it can be really hard to figure out how to get a system fully configured with all the required machine learning libraries and tools like TensorFlow, Keras, OpenCV, and dlib. The majority of the issues that get filed on my own open source projects are about how to install these tools. A lot of people get stuck while installing everything and give up before ever getting to play around with any code. That\u2019s a shame!"
    },
    {
        "url": "https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7?source=user_profile---------9----------------",
        "title": "Machine Learning is Fun Part 7: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art",
        "text": "So why exactly are AI researchers building complex systems to generate slightly wonky-looking pictures of bedrooms?\n\nThe idea is that if you can generate pictures of something, you must have an understanding of it.\n\nLook at this picture:\n\nYou instantly know this is a picture of a dog \u2014 a furry thing with four legs and a tail. But to a computer, the picture is just a grid of numbers representing the color of each pixel. The computer has no understanding that the picture represents a concept.\n\nBut now imagine that we showed a computer thousands of pictures of dogs and after seeing those pictures, the computer was able to generate new pictures of dogs on its own \u2014 including different dog breeds and pictures from different angles. Maybe we could even ask it for certain types of pictures, like \u201ca side view of a beagle\u201d.\n\nIf the computer was able to do this and the pictures it produced had the right number of legs, tails, and ears, it would prove that the computer knows what parts go into making up a \u201cdog\u201d even though no one told it explicitly. So in a sense, a good generative model is proof of basic understanding \u2014 at least on a toddler-level.\n\nThat\u2019s why researchers are so excited about building generative models. They seem to be a way to train computers to understand concepts without being explicitly taught the meaning of those concepts. That\u2019s a big step over current systems that can only learn from training data that has been painstakingly pre-labeled by humans.\n\nBut if all this research results in programs that generate pictures of dogs, how many years until we get the first computer-generated Dog-A-Day calendar as a side effect?\n\nAnd if you can build a program that understands dogs, why not a program that understands anything else? What about a program that could generate an unlimited number of stock photos of people shaking hands? I\u2019m sure someone would pay for that.\n\nOk, maybe a program that generates bad stock photos wouldn\u2019t be that interesting. But given the rate of progress in generative models over just the past year, who knows where we\u2019ll be in 5 or 10 years. What happens if someone invents a system to generate entire movies? Or music? Or video games?\n\nIf you look forward 20\u201330 years and squint, you can already imagine a world where entertainment could be 100% machine generated:\n\nThe video game industry is the first area of entertainment to start seriously experimenting with using AI to generate raw content. Aside from the obvious Venn diagram overlap between computer gaming and machine learning engineers, there\u2019s a huge cost incentive to invest in video game development automation given the $300+ million budgets of modern AAA video games.\n\nWe are still in the earliest days of machine-learning-based generative models and their practical uses are currently pretty narrow, but they are a lot of fun to play around with. Let\u2019s see what we can do with one.\n\nTo build a DCGAN, we create two deep neural networks. Then we make them fight against each other, endlessly attempting to out-do one another. In the process, they both become stronger.\n\nLet\u2019s pretend that the first deep neural network is a brand new police officer who is being trained to spot counterfeit money. It\u2019s job is to look at a picture and tell us if the picture contains real money.\n\nSince we are looking for objects in pictures, we can use a standard Convolutional Neural Network for this job. If you aren\u2019t familiar with ConvNets, you can read my earlier post. But the basic idea is that the neural network that takes in an image, processes it through several layers that recognize increasingly complex features in the image and then it outputs a single value\u2014in this case, whether or not the image contains a picture of real money.\n\nThis first neural network is called the Discriminator:\n\nNow let\u2019s pretend the second neural network is a brand new counterfeiter who is just learning how to create fake money. For this second neural network, we\u2019ll reverse the layers in a normal ConvNet so that everything runs backwards. So instead of taking in a picture and outputting a value, it takes in a list of values and outputs a picture.\n\nThis second neural network is called the Generator:\n\nSo now we have a police officer (the Discriminator) looking for fake money and a counterfeiter (the Generator) that\u2019s printing fake money. Let\u2019s make them battle!\n\nIn the first round, the Generator will create pathetic forgeries that barely resemble money at all because it knows absolutely nothing about what money is supposed to look like:\n\nBut right now the Discriminator is equally terrible at it\u2019s job of recognizing money, so it won\u2019t know the difference:\n\nAt this point, we step in and tell the Discriminator that this dollar bill is actually fake. Then we show it a real dollar bill and ask it how it looks different from the fake one. The Discriminator looks for a new detail to help it separate the real one from the fake one.\n\nFor example, the Discriminator might notice that real money has a picture of a person on it and the fake money doesn\u2019t. Using this knowledge, the Discriminator learns how to tell the fake from the real one. It gets a tiny bit better at its job:\n\nNow we start Round 2. We tell the Generator that it\u2019s money images are suddenly getting rejected as fake so it needs to step up it\u2019s game. We also tell it that the Discriminator is now looking for faces, so the best way to confuse the Discriminator is to put a face on the bill:\n\nAnd the fake bills are being accepted as valid again! So now the Discriminator has to look again at the real dollar and find a new way to tell it apart from the fake one.\n\nThis back-and-forth game between the Generator and the Discriminator continues thousands of times until both networks are experts. Eventually the Generator is producing near-perfect counterfeits and the Discriminator has turned into a Master Detective looking for the slightest mistakes.\n\nAt the point when both networks are sufficiently trained so that humans are impressed by the fake images, we can use the fake images for whatever purpose we want.\n\nSo now that we know how DCGANs work, let\u2019s see if we can use one to generate new artwork for 1980s-style video games.\n\nLet\u2019s build a DCGAN that tries to produce screenshots of imaginary video games for the Nintendo Entertainment System (or NES) based on screenshots of real games:\n\nThe idea is that if we can generate convincing screenshots of imaginary video games, we could copy and paste bits of art from those screenshots and use it in our own retro-style video game. Since the generated video games never existed, it wouldn\u2019t even be stealing (Maybe... more on this later).\n\nVideo game art in those days was very simple. Since the NES had such a small amount of memory (the games used way less memory than this article takes up!), programmers had to use lots of tricks to fit the game art into memory. To maximize the limited space, games used tile-based graphics where each screen in the game is made up of just a few (usually 16x16 pixel) repeated graphical tiles.\n\nFor example, the starting screen of \u2018The Legend of Zelda\u2019 is made up of only 8 unique tiles:\n\nHere are the tiles for entire \u2018The Legend of Zelda\u2019 game map:\n\nOur goal is to create a similar tile sheet for our game. Because of that, we don\u2019t really care if the game screenshots we generate look completely realistic. Instead, we\u2019re just looking for the shapes and patterns that we can use as 16 x 16 tiles in our game \u2014 things like stones, water, bridges, etc. Then we can use those tiles to build our own 8-bit-style video game levels.\n\nTo train our system, we need lots of data. Luckily there are over 700 games for the NES that we can pull from.\n\nI used wget to download all the NES game screenshots on The Video Game Museum website (sorry for scraping your site!). After a few minutes of downloading, I had a little over 10,000 screenshots of hundreds of NES games:\n\nRight now, DCGANs only work on pretty small images \u2014 256 pixels square or so. But the entire screen resolution of the NES was only 256 pixels by 224 pixels, so that\u2019s not a problem. To make things simple, I cropped each NES screenshot to 224 pixels square.\n\nThere are several open-source implementations of DCGANs on github that you can try out. I used Taehoon Kim\u2019s Tensorflow implementation. Since DCGANs are unsupervised, all you have to do is put the data in a folder, tweak the basic parameters, start it training and then wait to see what results you get.\n\nHere\u2019s what a sample of the original training data looks like:\n\nNow training begins. At first, the output from the Generator is pure noise. But it slowly start to take shape as the Generator learns to do a better job:\n\nAfter several more training rounds, the images start to resemble nightmare-ish versions of classic Nintendo games:\n\nAs training continues further, we start to see the bricks and blocks we are hoping to find. You can also see screen elements like life bars and even some text:\n\nThis is where things get complicated. How do we know the computer is creating brand new art and not just regurgitating art directly from the training images? In two of these images, you can clearly see the menu bar from Super Mario Bros. 3 and the header bar and bricks from the original Super Mario Bros.\n\nRegurgitating training data is definitely something that can happen. By using a large training data set and not training too long, we can try to reduce the chance that this happens. But it\u2019s a thorny issue and research on it continues.\n\nSince I\u2019m just going for aesthetics, I tweaked the model until it produced art that looked original to me. But I can\u2019t prove that the new art is totally original except by searching the training data for similar art and verifying that there isn\u2019t any.\n\nWith a few hours of training, the generated images contained 16 x 16 tiles that looked nice to me. I was looking for some variations on a basic stone block, brick patterns, water patterns, bushes, and some general \u201cspooky-looking\u201d background atmosphere tiles.\n\nNext I need to pre-process the generated images to the make sure they only used the 64 colors that are available on the NES:\n\nThen I\u2019ll open up the 64-color images in the Tiled Map Editor. From there, I can easily grab the 16 x 16 tiles that match the aesthetic I want:\n\nThen inside of Tiled Map Editor, I\u2019ll arrange those 16 x 16 tiles into a simple level layout reminiscent of the NES game \u2018Castlevania\u2019:"
    },
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a?source=user_profile---------10----------------",
        "title": "Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning",
        "text": "Machine Learning is Fun Part 6: How to do Speech Recognition with Deep Learning Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in \u666e\u901a\u8bdd , \ud55c\uad6d\uc5b4, Ti\u1ebfng Vi\u1ec7t or \u0420\u0443\u0441\u0441\u043a\u0438\u0439. Speech recognition is invading our lives. It\u2019s built into our phones, our game consoles and our smart watches. It\u2019s even automating our homes. For just $50, you can get an Amazon Echo Dot \u2014 a magic box that allows you to order pizza, get a weather report or even buy trash bags \u2014 just by speaking out loud: The Echo Dot has been so popular this holiday season that Amazon can\u2019t seem to keep them in stock! But speech recognition has been around for decades, so why is it just now hitting the mainstream? The reason is that deep learning finally made speech recognition accurate enough to be useful outside of carefully controlled environments. Andrew Ng has long predicted that as speech recognition goes from 95% accurate to 99% accurate, it will become a primary way that we interact with computers. The idea is that this 4% accuracy gap is the difference between annoyingly unreliable and incredibly useful. Thanks to Deep Learning, we\u2019re finally cresting that peak. Let\u2019s learn how to do speech recognition with deep learning! If you know how neural machine translation works, you might guess that we could simply feed sound recordings into a neural network and train it to produce text: That\u2019s the holy grail of speech recognition with deep learning, but we aren\u2019t quite there yet (at least at the time that I wrote this \u2014 I bet that we will be in a couple of years). The big problem is that speech varies in speed. One person might say \u201chello!\u201d very quickly and another person might say \u201cheeeelllllllllllllooooo!\u201d very slowly, producing a much longer sound file with much more data. Both both sound files should be recognized as exactly the same text \u2014 \u201chello!\u201d Automatically aligning audio files of various lengths to a fixed-length piece of text turns out to be pretty hard. To work around this, we have to use some special tricks and extra precessing in addition to a deep neural network. Let\u2019s see how it works! The first step in speech recognition is obvious \u2014 we need to feed sound waves into a computer. In Part 3, we learned how to take an image and treat it as an array of numbers so that we can feed directly into a neural network for image recognition: Images are just arrays of numbers that encode the intensity of each pixel But sound is transmitted as waves. How do we turn sound waves into numbers? Let\u2019s use this sound clip of me saying \u201cHello\u201d:\n\nBut thanks to the Nyquist theorem, we know that we can use math to perfectly reconstruct the original sound wave from the spaced-out samples \u2014 as long as we sample at least twice as fast as the highest frequency we want to record. I mention this only because nearly everyone gets this wrong and assumes that using higher sampling rates always leads to better audio quality. It doesn\u2019t. We now have an array of numbers with each number representing the sound wave\u2019s amplitude at 1/16,000th of a second intervals. We could feed these numbers right into a neural network. But trying to recognize speech patterns by processing these samples directly is difficult. Instead, we can make the problem easier by doing some pre-processing on the audio data. Let\u2019s start by grouping our sampled audio into 20-millisecond-long chunks. Here\u2019s our first 20 milliseconds of audio (i.e., our first 320 samples):\n\nThis recording is only 1/50th of a second long. But even this short recording is a complex mish-mash of different frequencies of sound. There\u2019s some low sounds, some mid-range sounds, and even some high-pitched sounds sprinkled in. But taken all together, these different frequencies mix together to make up the complex sound of human speech. To make this data easier for a neural network to process, we are going to break apart this complex sound wave into it\u2019s component parts. We\u2019ll break out the low-pitched parts, the next-lowest-pitched-parts, and so on. Then by adding up how much energy is in each of those frequency bands (from low to high), we create a fingerprint of sorts for this audio snippet. Imagine you had a recording of someone playing a C Major chord on a piano. That sound is the combination of three musical notes\u2014 C, E and G \u2014 all mixed together into one complex sound. We want to break apart that complex sound into the individual notes to discover that they were C, E and G. This is the exact same idea. We do this using a mathematic operation called a Fourier transform. It breaks apart the complex sound wave into the simple sound waves that make it up. Once we have those individual sound waves, we add up how much energy is contained in each one. The end result is a score of how important each frequency range is, from low pitch (i.e. bass notes) to high pitch. Each number below represents how much energy was in each 50hz band of our 20 millisecond audio clip:\n\nA spectrogram is cool because you can actually see musical notes and other pitch patterns in audio data. A neural network can find patterns in this kind of data more easily than raw sound waves. So this is the data representation we\u2019ll actually feed into our neural network. Now that we have our audio in a format that\u2019s easy to process, we will feed it into a deep neural network. The input to the neural network will be 20 millisecond audio chunks. For each little audio slice, it will try to figure out the letter that corresponds the sound currently being spoken. We\u2019ll use a recurrent neural network \u2014 that is, a neural network that has a memory that influences future predictions. That\u2019s because each letter it predicts should affect the likelihood of the next letter it will predict too. For example, if we have said \u201cHEL\u201d so far, it\u2019s very likely we will say \u201cLO\u201d next to finish out the word \u201cHello\u201d. It\u2019s much less likely that we will say something unpronounceable next like \u201cXYZ\u201d. So having that memory of previous predictions helps the neural network make more accurate predictions going forward. After we run our entire audio clip through the neural network (one chunk at a time), we\u2019ll end up with a mapping of each audio chunk to the letters most likely spoken during that chunk. Here\u2019s what that mapping looks like for me saying \u201cHello\u201d:\n\nOur neural net is predicting that one likely thing I said was \u201cHHHEE_LL_LLLOOO\u201d. But it also thinks that it was possible that I said \u201cHHHUU_LL_LLLOOO\u201d or even \u201cAAAUU_LL_LLLOOO\u201d. We have some steps we follow to clean up this output. First, we\u2019ll replace any repeated characters a single character: HE_L_LO becomes HELLO That leaves us with three possible transcriptions \u2014 \u201cHello\u201d, \u201cHullo\u201d and \u201cAullo\u201d. If you say them out loud, all of these sound similar to \u201cHello\u201d. Because it\u2019s predicting one character at a time, the neural network will come up with these very sounded-out transcriptions. For example if you say \u201cHe would not go\u201d, it might give one possible transcription as \u201cHe wud net go\u201d. The trick is to combine these pronunciation-based predictions with likelihood scores based on large database of written text (books, news articles, etc). You throw out transcriptions that seem the least likely to be real and keep the transcription that seems the most realistic. Of our possible transcriptions \u201cHello\u201d, \u201cHullo\u201d and \u201cAullo\u201d, obviously \u201cHello\u201d will appear more frequently in a database of text (not to mention in our original audio-based training data) and thus is probably correct. So we\u2019ll pick \u201cHello\u201d as our final transcription instead of the others. Done! You might be thinking \u201cBut what if someone says \u2018Hullo\u2019? It\u2019s a valid word. Maybe \u2018Hello\u2019 is the wrong transcription!\u201d Of course it is possible that someone actually said \u201cHullo\u201d instead of \u201cHello\u201d. But a speech recognition system like this (trained on American English) will basically never produce \u201cHullo\u201d as the transcription. It\u2019s just such an unlikely thing for a user to say compared to \u201cHello\u201d that it will always think you are saying \u201cHello\u201d no matter how much you emphasize the \u2018U\u2019 sound. Try it out! If your phone is set to American English, try to get your phone\u2019s digital assistant to recognize the world \u201cHullo.\u201d You can\u2019t! It refuses! It will always understand it as \u201cHello.\u201d Not recognizing \u201cHullo\u201d is a reasonable behavior, but sometimes you\u2019ll find annoying cases where your phone just refuses to understand something valid you are saying. That\u2019s why these speech recognition models are always being retrained with more data to fix these edge cases. Can I Build My Own Speech Recognition System? One of the coolest things about machine learning is how simple it sometimes seems. You get a bunch of data, feed it into a machine learning algorithm, and then magically you have a world-class AI system running on your gaming laptop\u2019s video card\u2026 Right? That sort of true in some cases, but not for speech. Recognizing speech is a hard problem. You have to overcome almost limitless challenges: bad quality microphones, background noise, reverb and echo, accent variations, and on and on. All of these issues need to be present in your training data to make sure the neural network can deal with them. Here\u2019s another example: Did you know that when you speak in a loud room you unconsciously raise the pitch of your voice to be able to talk over the noise? Humans have no problem understanding you either way, but neural networks need to be trained to handle this special case. So you need training data with people yelling over noise! To build a voice recognition system that performs on the level of Siri, Google Now!, or Alexa, you will need a lot of training data \u2014 far more data than you can likely get without hiring hundreds of people to record it for you. And since users have low tolerance for poor quality voice recognition systems, you can\u2019t skimp on this. No one wants a voice recognition system that works 80% of the time. For a company like Google or Amazon, hundreds of thousands of hours of spoken audio recorded in real-life situations is gold. That\u2019s the single biggest thing that separates their world-class speech recognition system from your hobby system. The whole point of putting Google Now! and Siri on every cell phone for free or selling $50 Alexa units that have no subscription fee is to get you to use them as much as possible. Every single thing you say into one of these systems is recorded forever and used as training data for future versions of speech recognition algorithms. That\u2019s the whole game! Don\u2019t believe me? If you have an Android phone with Google Now!, click here to listen to actual recordings of yourself saying every dumb thing you\u2019ve ever said into it: You can access the same thing for Amazon via your Alexa app. Apple unfortunately doesn\u2019t let you access your Siri voice data. So if you are looking for a start-up idea, I wouldn\u2019t recommend trying to build your own speech recognition system to compete with Google. Instead, figure out a way to get people to give you recordings of themselves talking for hours. The data can be your product instead. Where to Learn More The algorithm (roughly) described here to deal with variable-length audio is called Connectionist Temporal Classification or CTC. You can read the original paper from 2006. Adam Coates of Baidu gave a great presentation on Deep Learning for Speech Recognition at the Bay Area Deep Learning School. You can watch the video on YouTube (his talk starts at 3:51:00). Highly recommended."
    },
    {
        "url": "https://medium.com/@ageitgey/the-new-macbook-pro-is-kind-of-great-for-hackers-64c1c577a4d2?source=user_profile---------11----------------",
        "title": "The new MacBook Pro is kind of great for hackers \u2013 Adam Geitgey \u2013",
        "text": "A million hot takes have been posted about how the late-2016 MacBook Pro with USB-C is the undeniable proof that Apple doesn\u2019t care about developers anymore. They took away all the ports! No Esc key! It\u2019s just a more expensive MacBook Air!\n\nBut in some ways, the new MacBook Pro is the most techy and expandable laptop Apple has ever made. They are trusting their pro users to wade into murky USB-C waters in search of the holy grail of a universal, open standard for moving data and power between devices.\n\nI\u2019m not here to change your mind about the MacBook Pro. Yes, it\u2019s probably too expensive and more RAM is better than less RAM. But everyone posting complaints without actually using a MBP for a few weeks is missing out on all the clever things you can do because it is built on USB-C. Over the past week or two with a new MacBook Pro (15in, 2.9ghz, TouchBar), I\u2019ve been constantly surprised with how USB-C makes new things possible. It\u2019s a kind of a hacker\u2019s dream.\n\nThe new charging block that comes with the MBP looks exactly the same as any traditional MBP charger:\n\nBut this charger is totally different than old MacBook Pro chargers in a vital way \u2014 it\u2019s just a generic usb charger. There\u2019s nothing about it that makes it \u201cspecial\u201d for your Mac.\n\nMy current cell phone is a Google Nexus 6P. Guess what kind of charging port it uses?\n\nRight, USB-C \u2014 just like lots of other recent phones and laptops. I can plug my phone into my MacBook Pro charger and it works perfectly. Now I only need to bring one power cable to the caf\u00e9 instead of two and I can charge my computer or my phone interchangeably. This is so nice!\n\nBut this goes both ways. I can just as easily plug the MacBook Pro into the same USB car charger that I use for my phone. A car charger doesn\u2019t put out a ton of power so you have to charge the MBP while it\u2019s off, but it works fine.\n\nAnd gone are the days where you have to spend $90 to buy an official MBP charger from Apple. You can buy any USB-C charger with enough power output and use it with your MBP.\n\nYou can even charge your MBP and phone together from one of those generic portable USB-C backup batteries:\n\nA big complaint about the new MacBook Pro is that you have to carry around a bag of dongles in order to plug in your external devices. Want to plug in your cell phone? Dongle! Want to download pictures from your camera? Dongle! Want to use ethernet or an external monitor? Dongle! Apple was so overwhelmed with complaints that they had to slash prices on dongles to make good. It\u2019s a bunch of hassle with no real benefits, right?\n\nI think people are thinking of these dongles as a useless Apple tax they have to pay in order to use their devices with the latest Mac. But that\u2019s not exactly the case. These \u201cdongles\u201d are generic USB-C devices. They work with any USB-C device.\n\nThis means you can take that same USB-C Ethernet adapter you have for your MBP and plug it right into your cell phone:\n\nWhat about that SD card reader you had to buy to download your photos?\n\nIt works on your phone too! And notice that this isn\u2019t some expensive Apple-branded SD card reader. Any USB-C SD card reader works fine because it\u2019s an open standard.\n\nYou can even do silly stuff like plug your USB-C keyboard and mouse right into your cell phone:\n\nUniversal sharing of accessories between devices is a hacker\u2019s dream. It\u2019s the exact opposite opposite of vendor lock-in. You can just plug anything into anything and it (mostly) works.\n\nThis is just the beginning of what you can do with USB-C. Here are some other fun tricks.\n\nIf you get any of the new USB-C compatible monitors (pretty much every vendor has at least one now), you only need to plug one single cable into your MBP:\n\nYou can then plug all your other devices into your monitor and everything flows over one USB-C to your laptop \u2014 power, video, data and even sound. Your monitor is now your docking station and breakout box!\n\nUSB-C on the MacBook Pro supports the new USB Power Delivery (UPD) spec. Beyond just basic wall charging, this spec lets you do fancy things like charge one USB-C device from another in either direction. You can plug your MacBook Pro into another USB-C laptop (like a Chromebook Pixel or a Lenovo Yoga) and one laptop can charge the other! And if you don\u2019t want to do that, they can also use each other\u2019s wall adapters interchangeably.\n\nUPD also allows the MacBook Pro to power external devices with high power requirements over the data connection. For example, you can plug in an external USB-C hard drive and power it over USB-C without needing an external wall wart:\n\nAs USB-C continues to grow in popularity, you can easily imagine the next Raspberry Pi supporting it. This means you could power your RPi from your laptop while you are developing without any extra cables. You could even plug in all the same external devices to your MacBook and the RPi without any new adapters.\n\nMagSafe solved the problem of someone tripping over your power cable and knocking your computer on the ground. It was brilliant. It is a bummer that Apple killed MagSafe, but this cloud has a silver lining of sorts.\n\nUSB-C is an open standard. Anyone can make USB-C cables. So Griffin made a break-away USB-C cable that works like MagSafe:\n\nYou don\u2019t have to use a weird off-brand power adapter to use this cable. It\u2019s just a standard USB-C cable that plugs into your original Apple USB-C charger block. Simple.\n\nAnd if you think this first design is a little clunky or if you wish this cable worked for data instead of just power, then you\u2019re in luck! Anyone can make their unique take on a break-away USB-C cable. I\u2019m sure it\u2019s only a matter of time until Amazon is flooded with cheap versions of this idea that tweak it just enough to avoid patent issues. I look forward to buying $3 breakaway USB-C cables in the future. That kind of competition could have never happened with the old proprietary MagSafe connector!\n\nI/O-wise, the new MacBook Pro is possibly the most open device Apple has ever built. There is literally not a single proprietary port on it. You get four universal high-speed ports that can each draw or supply power, send and receive data and transfer video and audio. It\u2019s really pretty neat.\n\nAnd yes, it\u2019s annoying if you have a bunch of older devices that need adapters. But you don\u2019t have to buy Apple\u2019s over-sized and over-priced dongles. You can buy tiny little USB-C adapters like this by the handful on Amazon for cents:\n\nIn a year or two when we all have junk drawers packed full of extra generic USB-C cables that cost nearly nothing, we\u2019re going to look back on this and wonder why everyone was so worked up."
    },
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa?source=user_profile---------12----------------",
        "title": "Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences",
        "text": "So how do we program a computer to translate human language?\n\nThe simplest approach is to replace every word in a sentence with the translated word in the target language. Here\u2019s a simple example of translating from Spanish to English word-by-word:\n\nThis is easy to implement because all you need is a dictionary to look up each word\u2019s translation. But the results are bad because it ignores grammar and context.\n\nSo the next thing you might do is start adding language-specific rules to improve the results. For example, you might translate common two-word phrases as a single group. And you might swap the order nouns and adjectives since they usually appear in reverse order in Spanish from how they appear in English:\n\nThat worked! If we just keep adding more rules until we can handle every part of grammar, our program should be able to translate any sentence, right?\n\nThis is how the earliest machine translation systems worked. Linguists came up with complicated rules and programmed them in one-by-one. Some of the smartest linguists in the world labored for years during the Cold War to create translation systems as a way to interpret Russian communications more easily.\n\nUnfortunately this only worked for simple, plainly-structured documents like weather reports. It didn\u2019t work reliably for real-world documents.\n\nThe problem is that human language doesn\u2019t follow a fixed set of rules. Human languages are full of special cases, regional variations, and just flat out rule-breaking. The way we speak English more influenced by who invaded who hundreds of years ago than it is by someone sitting down and defining grammar rules.\n\nAfter the failure of rule-based systems, new translation approaches were developed using models based on probability and statistics instead of grammar rules.\n\nBuilding a statistics-based translation system requires lots of training data where the exact same text is translated into at least two languages. This double-translated text is called parallel corpora. In the same way that the Rosetta Stone was used by scientists in the 1800s to figure out Egyptian hieroglyphs from Greek, computers can use parallel corpora to guess how to convert text from one language to another.\n\nLuckily, there\u2019s lots of double-translated text already sitting around in strange places. For example, the European Parliament translates their proceedings into 21 languages. So researchers often use that data to help build translation systems.\n\nThe fundamental difference with statistical translation systems is that they don\u2019t try to generate one exact translation. Instead, they generate thousands of possible translations and then they rank those translations by likely each is to be correct. They estimate how \u201ccorrect\u201d something is by how similar it is to the training data. Here\u2019s how it works:\n\nFirst, we break up our sentence into simple chunks that can each be easily translated:\n\nNext, we will translate each of these chunks by finding all the ways humans have translated those same chunks of words in our training data.\n\nIt\u2019s important to note that we are not just looking up these chunks in a simple translation dictionary. Instead, we are seeing how actual people translated these same chunks of words in real-world sentences. This helps us capture all of the different ways they can be used in different contexts:\n\nSome of these possible translations are used more frequently than others. Based on how frequently each translation appears in our training data, we can give it a score.\n\nFor example, it\u2019s much more common for someone to say \u201cQuiero\u201d to mean \u201cI want\u201d than to mean \u201cI try.\u201d So we can use how frequently \u201cQuiero\u201d was translated to \u201cI want\u201d in our training data to give that translation more weight than a less frequent translation.\n\nNext, we will use every possible combination of these chunks to generate a bunch of possible sentences.\n\nJust from the chunk translations we listed in Step 2, we can already generate nearly 2,500 different variations of our sentence by combining the chunks in different ways. Here are some examples:\n\nBut in a real-world system, there will be even more possible chunk combinations because we\u2019ll also try different orderings of words and different ways of chunking the sentence:\n\nNow need to scan through all of these generated sentences to find the one that is that sounds the \u201cmost human.\u201d\n\nTo do this, we compare each generated sentence to millions of real sentences from books and news stories written in English. The more English text we can get our hands on, the better.\n\nTake this possible translation:\n\nIt\u2019s likely that no one has ever written a sentence like this in English, so it would not be very similar to any sentences in our data set. We\u2019ll give this possible translation a low probability score.\n\nBut look at this possible translation:\n\nThis sentence will be similar to something in our training set, so it will get a high probability score.\n\nAfter trying all possible sentences, we\u2019ll pick the sentence that has the most likely chunk translations while also being the most similar overall to real English sentences.\n\nOur final translation would be \u201cI want to go to the prettiest beach.\u201d Not bad!\n\nStatistical machine translation systems perform much better than rule-based systems if you give them enough training data. Franz Josef Och improved on these ideas and used them to build Google Translate in the early 2000s. Machine Translation was finally available to the world.\n\nIn the early days, it was surprising to everyone that the \u201cdumb\u201d approach to translating based on probability worked better than rule-based systems designed by linguists. This led to a (somewhat mean) saying among researchers in the 80s:\n\nStatistical machine translation systems work well, but they are complicated to build and maintain. Every new pair of languages you want to translate requires experts to tweak and tune a new multi-step translation pipeline.\n\nBecause it is so much work to build these different pipelines, trade-offs have to be made. If you are asking Google to translate Georgian to Telegu, it has to internally translate it into English as an intermediate step because there\u2019s not enough Georgain-to-Telegu translations happening to justify investing heavily in that language pair. And it might do that translation using a less advanced translation pipeline than if you had asked it for the more common choice of French-to-English.\n\nWouldn\u2019t it be cool if we could have the computer do all that annoying development work for us?\n\nThe holy grail of machine translation is a black box system that learns how to translate by itself\u2014 just by looking at training data. With Statistical Machine Translation, humans are still needed to build and tweak the multi-step statistical models.\n\nIn 2014, KyungHyun Cho\u2019s team made a breakthrough. They found a way to apply deep learning to build this black box system. Their deep learning model takes in a parallel corpora and and uses it to learn how to translate between those two languages without any human intervention.\n\nTwo big ideas make this possible \u2014 recurrent neural networks and encodings. By combining these two ideas in a clever way, we can build a self-learning translation system.\n\nWe\u2019ve already talked about recurrent neural networks in Part 2, but let\u2019s quickly review.\n\nA regular (non-recurrent) neural network is a generic machine learning algorithm that takes in a list of numbers and calculates a result (based on previous training). Neural networks can be used as a black box to solve lots of problems. For example, we can use a neural network to calculate the approximate value of a house based on attributes of that house:\n\nBut like most machine learning algorithms, neural networks are stateless. You pass in a list of numbers and the neural network calculates a result. If you pass in those same numbers again, it will always calculate the same result. It has no memory of past calculations. In other words, 2 + 2 always equals 4.\n\nA recurrent neural network (or RNN for short) is a slightly tweaked version of a neural network where the previous state of the neural network is one of the inputs to the next calculation. This means that previous calculations change the results of future calculations!\n\nWhy in the world would we want to do this? Shouldn\u2019t 2 + 2 always equal 4 no matter what we last calculated?\n\nThis trick allows neural networks to learn patterns in a sequence of data. For example, you can use it to predict the next most likely word in a sentence based on the first few words:\n\nRNNs are useful any time you want to learn patterns in data. Because human language is just one big, complicated pattern, RNNs are increasingly used in many areas of natural language processing.\n\nIf you want to learn more about RNNs, you can read Part 2 where we used one to generate a fake Ernest Hemingway book and then used another one to generate fake Super Mario Brothers levels.\n\nThe other idea we need to review is Encodings. We talked about encodings in Part 4 as part of face recognition. To explain encodings, let\u2019s take a slight detour into how we can tell two different people apart with a computer.\n\nWhen you are trying to tell two faces apart with a computer, you collect different measurements from each face and use those measurements to compare faces. For example, we might measure the size of each ear or the spacing between the eyes and compare those measurements from two pictures to see if they are the same person.\n\nYou\u2019re probably already familiar with this idea from watching any primetime detective show like CSI:\n\nThe idea of turning a face into a list of measurements is an example of an encoding. We are taking raw data (a picture of a face) and turning it into a list of measurements that represent it (the encoding).\n\nBut like we saw in Part 4, we don\u2019t have to come up with a specific list of facial features to measure ourselves. Instead, we can use a neural network to generate measurements from a face. The computer can do a better job than us in figuring out which measurements are best able to differentiate two similar people:\n\nThis is our encoding. It lets us represent something very complicated (a picture of a face) with something simple (128 numbers). Now comparing two different faces is much easier because we only have to compare these 128 numbers for each face instead of comparing full images.\n\nGuess what? We can do the same thing with sentences! We can come up with an encoding that represents every possible different sentence as a series of unique numbers:\n\nTo generate this encoding, we\u2019ll feed the sentence into the RNN, one word at time. The final result after the last word is processed will be the values that represent the entire sentence:\n\nGreat, so now we have a way to represent an entire sentence as a set of unique numbers! We don\u2019t know what each number in the encoding means, but it doesn\u2019t really matter. As long as each sentence is uniquely identified by it\u2019s own set of numbers, we don\u2019t need to know exactly how those numbers were generated.\n\nOk, so we know how to use an RNN to encode a sentence into a set of unique numbers. How does that help us? Here\u2019s where things get really cool!\n\nWhat if we took two RNNs and hooked them up end-to-end? The first RNN could generate the encoding that represents a sentence. Then the second RNN could take that encoding and just do the same logic in reverse to decode the original sentence again:"
    },
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=user_profile---------13----------------",
        "title": "Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning",
        "text": "Let\u2019s tackle this problem one step at a time. For each step, we\u2019ll learn about a different machine learning algorithm. I\u2019m not going to explain every single algorithm completely to keep this from turning into a book, but you\u2019ll learn the main ideas behind each one and you\u2019ll learn how you can build your own facial recognition system in Python using OpenFace and dlib.\n\nThe first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart!\n\nIf you\u2019ve used any camera in the last 10 years, you\u2019ve probably seen face detection in action:\n\nFace detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we\u2019ll use it for a different purpose \u2014 finding the areas of the image we want to pass on to the next step in our pipeline.\n\nFace detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We\u2019re going to use a method invented in 2005 called Histogram of Oriented Gradients \u2014 or just HOG for short.\n\nTo find faces in an image, we\u2019ll start by making our image black and white because we don\u2019t need color data to find faces:\n\nThen we\u2019ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:\n\nOur goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:\n\nIf you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image:\n\nThis might seem like a random thing to do, but there\u2019s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!\n\nBut saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.\n\nTo do this, we\u2019ll break up the image into small squares of 16x16 pixels each. In each square, we\u2019ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc\u2026). Then we\u2019ll replace that square in the image with the arrow directions that were the strongest.\n\nThe end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:\n\nTo find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:\n\nUsing this technique, we can now easily find faces in any image:\n\nIf you want to try this step out yourself using Python and dlib, here\u2019s code showing how to generate and view HOG representations of images.\n\nWhew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:\n\nTo account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.\n\nTo do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan.\n\nThe basic idea is we will come up with 68 specific points (called landmarks) that exist on every face \u2014 the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:\n\nHere\u2019s the result of locating the 68 face landmarks on our test image:\n\nNow that we know were the eyes and mouth are, we\u2019ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won\u2019t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations):\n\nNow no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.\n\nIf you want to try this step out yourself using Python and dlib, here\u2019s the code for finding face landmarks and here\u2019s the code for transforming the image using those landmarks.\n\nNow we are to the meat of the problem \u2014 actually telling faces apart. This is where things get really interesting!\n\nThe simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?\n\nThere\u2019s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can\u2019t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.\n\nWhat we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you\u2019ve ever watched a bad crime show like CSI, you know what I am talking about:\n\nOk, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?\n\nIt turns out that the measurements that seem obvious to us humans (like eye color) don\u2019t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\n\nThe solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.\n\nThe training process works by looking at 3 face images at a time:\n\nThen the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:\n\nAfter repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.\n\nMachine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.\n\nThis process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.\n\nBut once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!\n\nSo all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here\u2019s the measurements for our test image:\n\nSo what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn\u2019t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.\n\nIf you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this.\n\nThis last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.\n\nYou can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We\u2019ll use a simple linear SVM classifier, but lots of classification algorithms could work.\n\nAll we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!\n\nSo let\u2019s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:"
    },
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=user_profile---------14----------------",
        "title": "Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks",
        "text": "Machine learning only works when you have data \u2014 preferably a lot of data. So we need lots and lots of handwritten \u201c8\u201ds to get started. Luckily, researchers created the MNIST data set of handwritten numbers for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some \u201c8\u201ds from the data set:\n\nWe also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let\u2019s modify this same neural network to recognize handwritten text. But to make the job really simple, we\u2019ll only try to recognize one letter \u2014 the numeral \u201c8\u201d.\n\nIn Part 2 , we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:\n\nBefore we learn how to recognize pictures of birds, let\u2019s learn how to recognize something much simpler \u2014 the handwritten number \u201c8\u201d.\n\nSo let\u2019s do it \u2014 let\u2019s write a program that can recognize birds!\n\nIn the last few years, we\u2019ve finally found a good approach to object recognition using deep convolutional neural networks. That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one.\n\nThe goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years.\n\nYou might have seen this famous xkcd comic before.\n\nTo feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:\n\nThe answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:\n\nThe neural network we made in Part 2 only took in a three numbers as the input (\u201c3\u201d bedrooms, \u201c2000\u201d sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers?\n\nAll that\u2019s left is to train the neural network with images of \u201c8\u201ds and not-\u201c8\"s so it learns to tell them apart. When we feed in an \u201c8\u201d, we\u2019ll tell it the probability the image is an \u201c8\u201d is 100% and the probability it\u2019s not an \u201c8\u201d is 0%. Vice versa for the counter-example images.\n\nOur neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone.\n\nNotice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an \u201c8\u201d and thee second output will predict the likelihood it isn\u2019t an \u201c8\u201d. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups.\n\nThe handle 324 inputs, we\u2019ll just enlarge our neural network to have 324 input nodes:\n\nWe can train this kind of neural network in a few minutes on a modern laptop. When it\u2019s done, we\u2019ll have a neural network that can recognize pictures of \u201c8\u201ds with a pretty high accuracy. Welcome to the world of (late 1980\u2019s-era) image recognition!\n\nIt\u2019s really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic! \u2026right?\n\nWell, of course it\u2019s not that simple.\n\nFirst, the good news is that our \u201c8\u201d recognizer really does work well on simple images where the letter is right in the middle of the image:\n\nBut now the really bad news:\n\nOur \u201c8\u201d recognizer totally fails to work when the letter isn\u2019t perfectly centered in the image. Just the slightest position change ruins everything:\n\nThis is because our network only learned the pattern of a perfectly-centered \u201c8\u201d. It has absolutely no idea what an off-center \u201c8\u201d is. It knows exactly one pattern and one pattern only.\n\nThat\u2019s not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the \u201c8\u201d isn\u2019t perfectly centered.\n\nWe already created a really good program for finding an \u201c8\u201d centered in an image. What if we just scan all around the image for possible \u201c8\u201ds in smaller sections, one section at a time, until we find one?\n\nThis approach called a sliding window. It\u2019s the brute force solution. It works well in some limited cases, but it\u2019s really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this!\n\nWhen we trained our network, we only showed it \u201c8\u201ds that were perfectly centered. What if we train it with more data, including \u201c8\u201ds in all different positions and sizes all around the image?\n\nWe don\u2019t even need to collect new training data. We can just write a script to generate new images with the \u201c8\u201ds in all kinds of different positions in the image:\n\nUsing this technique, we can easily create an endless supply of training data.\n\nMore data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns.\n\nTo make the network bigger, we just stack up layer upon layer of nodes:\n\nWe call this a \u201cdeep neural network\u201d because it has more layers than a traditional neural network.\n\nThis idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play Overwatch can be used to train neural networks incredibly quickly.\n\nBut even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn\u2019t going to get us all the way to a solution. We need to be smarter about how we process images into our neural network.\n\nThink about it. It doesn\u2019t make sense to train a network to recognize an \u201c8\u201d at the top of a picture separately from training it to recognize an \u201c8\u201d at the bottom of a picture as if those were two totally different objects.\n\nThere should be some way to make the neural network smart enough to know that an \u201c8\u201d anywhere in the picture is the same thing without all that extra training. Luckily\u2026 there is!\n\nAs a human, you intuitively know that pictures have a hierarchy or conceptual structure. Consider this picture:\n\nAs a human, you instantly recognize the hierarchy in this picture:\n\nMost importantly, we recognize the idea of a child no matter what surface the child is on. We don\u2019t have to re-learn the idea of child for every possible surface it could appear on.\n\nBut right now, our neural network can\u2019t do this. It thinks that an \u201c8\u201d in a different part of the image is an entirely different thing. It doesn\u2019t understand that moving an object around in the picture doesn\u2019t make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks.\n\nWe need to give our neural network understanding of translation invariance \u2014 an \u201c8\u201d is an \u201c8\u201d no matter where in the picture it shows up.\n\nWe\u2019ll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images).\n\nInstead of feeding entire images into our neural network as one grid of numbers, we\u2019re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture.\n\nHere\u2019s how it\u2019s going to work, step by step \u2014\n\nSimilar to our sliding window search above, let\u2019s pass a sliding window over the entire original image and save each result as a separate, tiny picture tile:\n\nBy doing this, we turned our original image into 77 equally-sized tiny image tiles.\n\nEarlier, we fed a single image into a neural network to see if it was an \u201c8\u201d. We\u2019ll do the exact same thing here, but we\u2019ll do it for each individual image tile:\n\nHowever, there\u2019s one big twist: We\u2019ll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we\u2019ll mark that tile as interesting.\n\nWe don\u2019t want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this:\n\nIn other words, we\u2019ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting.\n\nThe result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big:\n\nTo reduce the size of the array, we downsample it using an algorithm called max pooling. It sounds fancy, but it isn\u2019t at all!\n\nWe\u2019ll just look at each 2x2 square of the array and keep the biggest number:\n\nThe idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we\u2019ll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits.\n\nSo far, we\u2019ve reduced a giant image down into a fairly small array.\n\nGuess what? That array is just a bunch of numbers, so we can use that small array as input into another neural network. This final neural network will decide if the image is or isn\u2019t a match. To differentiate it from the convolution step, we call it a \u201cfully connected\u201d network.\n\nSo from start to finish, our whole five-step pipeline looks like this:\n\nOur image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network.\n\nWhen solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data.\n\nThe basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize.\n\nFor example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it\u2019s knowledge of sharp edges, the third step might recognize entire birds using it\u2019s knowledge of beaks, etc.\n\nHere\u2019s what a more realistic deep convolutional network (like you would find in a research paper) looks like:\n\nIn this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories!\n\nSo how do you know which steps you need to combine to make your image classifier work?\n\nHonestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error!\n\nNow finally we know enough to write a program that can decide if a picture is a bird or not.\n\nAs always, we need some data to get started. The free CIFAR10 data set contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we\u2019ll also add in the Caltech-UCSD Birds-200\u20132011 data set that has another 12,000 bird pics.\n\nHere\u2019s a few of the birds from our combined data set:"
    },
    {
        "url": "https://medium.com/@ageitgey/dont-fear-criticism-follow-it-130c85ec9f34?source=user_profile---------15----------------",
        "title": "Don\u2019t Fear Criticism, Follow It \u2013 Adam Geitgey \u2013",
        "text": "If you are building, creating, writing, recording, performing, publishing or otherwise doing anything interesting in public, there will be people on the internet who will say horrible and painful things about it.\n\nBut consider consider internet criticism as confirmation that you are doing something interesting enough to talk about. It means you are on the right track. Keep going. Nothing interesting ever got created without someone saying it was horrible."
    },
    {
        "url": "https://medium.com/@ageitgey/everything-you-need-to-know-about-implementing-ios-and-android-mobile-deep-linking-f4348b265b49?source=user_profile---------16----------------",
        "title": "Everything you need to know about implementing iOS and Android Mobile Deep Linking",
        "text": "Normally when a user clicks a link to a website, the website opens in a web browser. Mobile App Deep Linking allows a corresponding native iOS/Android native app to open instead of a web browser:\n\nApple calls it Universal Links and Google calls it App Links, but those are just brand names for the same thing \u2014 opening a specific app instead of a web browser.\n\nThere are some good arguments for pushing users into your native app instead of towards your website:\n\nIf your business depends on search traffic from Google, you run the risk of Google displacing your entire business when they decide to compete with you. They do this by showing their own competing products on search result pages above even the top organic search results. Getting a number one rank on a search result page doesn\u2019t matter of the user doesn\u2019t see any of the search results!\n\nTo fight back, some companies are trying to covert as many SEO users as possible into native app users by forcing them into downloading a native app. The theory is that Google will continually siphon off new SEO traffic, but new native app users will build a habit to skip Google and come back to your app instead. No one knows if this will be a winning strategy in the long run.\n\nIf your native app isn\u2019t top notch, don\u2019t force your users into using it. Only force users into your app if you really believe it\u2019s a better user experience than your website. Otherwise you will frustrate your users.\n\nDeep linking works well for apps that provide a dedicated market place experience (like eBay) or apps that need features like push notifications or location that you can\u2019t do as well on a website (like Twitter). But they don\u2019t work well when users feel like you are forcing them to use your app for no good reason. That results in angry users and attrition.\n\nBoth iOS (9.0 and newer) and Android (all versions) provide good APIs for deep linking. You can implement deep linking on both platforms at the same time and a lot of the work overlaps.\n\nFirst, you need to decide which urls on your website should link to which screens in your mobile app.\n\nFor example, let\u2019s pretend you have a shopping website is called example.com. On your website, you have products with urls like https://www.example.com/products/skinny-jeans. You also have a blog on your website at https://www.example.com/company-blog.\n\nBased on this, you might create a mapping like this:\n\nAccording to this mapping, you only want to deep link the user into the app when they click on a /products/ link. If they click on a link to your company blog, it\u2019s probably better to just show them the normal website.\n\nMake sure you only deep link users into your app when you can actually show them the exact information they wanted from inside the native app. Few things are as frustrating as clicking a link to a blog post and then getting redirected into an iPhone app and seeing a generic welcome page instead of that blog post. That kind of poor app integration is a good way to lose customers.\n\nApple introduced a new deep linking API in iOS 9.0 called \u201cUniversal Links\u201d. It provides a better user experience than the hacky deep linking options that existed in iOS 8.0 and below.\n\nI recommend that you only implement Universal Links and don\u2019t implement support for iOS 8.0 and below if you can possibly avoid it. Not only is this a lot less work, but the older deep linking solutions for iOS 8 constantly break in iOS 9 updates. Apple is sending a not so subtle message that you shouldn\u2019t use the old methods anymore.\n\nOf course, that means deep linking won\u2019t work for users still on iOS 8. Apple provides estimates of how many users have upgraded to iOS 9. Right now, around 85% of users globally have upgraded. That percentage is growing daily and is probably higher for US/Canada and western Europe.\n\nIn Step 1, we decided that links to https://www.example.com/products/* should launch our iOS app and show the correct product screen. This requires changes on both our website and in our native app.\n\nFirst, you have to create a file on your website called https://www.example.com/.well-known/apple-app-site-association that tells Apple that you own your mobile app and that it should intercept all links to /products/.\n\nThe second part of implementing Universal Links is updating your iOS app to respond correctly when it receives a deep link.\n\nThe docs from Apple are pretty straightforward, so check those out for specific details.\n\nCongrats, deep links should now work for iOS!\n\nAndroid has supported deep links via Intent Filters for many years. But in Android Marshmallow (6.0), they expanded deep linking with App Links to give you more control over the user experience.\n\nIn Android, Intent Filters let your app declare all the ways it can be launched by other apps. By adding a BROWSABLE intent filter, you are saying that your app can be started by a user clicking on a website url.\n\nAdding Intent Filters to your Android app does not require any server-side changes at all. You only have to modify your app.\n\nFirst, you update your AndroidManifest.xml file and add a new <intent-filter> declaration telling Android which Activity to start when the user clicks on a https://www.example.com/products/* link:\n\nNext, you add code to that Activity to grab the url the user clicked and show the right feature in the app:\n\nGoogle\u2019s Docs on implementing Intent Filters are really simple and clear, so check them out for more details.\n\nThere\u2019s one big problem with Intent Filters \u2014 any app can register them! You don\u2019t have to prove you own a website to capture clicks on that website. This means your competition\u2019s app can capture deep links from your website instead of your app!\n\nLook what happens when I click on a link to www.reddit.com on my phone:\n\nInstead of opening the official Reddit app, I have to choose from a list of all the apps on my phone that claim to support reddit.com urls. I have seven apps on my phone that are competing to be opened!\n\nAndroid App Links with verification was created to address this. It lets you declare that you own the official app for your website and it your app should be used by default.\n\nFirst, update your AndroidManifest.xml file again to request that Android verify that you own your website:\n\nSecond, you need to create a file on your website called https://www.example.com/.well-known/assetlinks.json that proves to Google that you own the app.\n\nCongrats, deep links should now work for Android!\n\nNow you have deep links implemented and it is working great for all of your existing users!\n\nBut deep links only trigger when a user already has your native app installed before they visit your site. Brand new users will still land on your website. For those website users, you\u2019ll probably want to ask them to download your app.\n\nYou don\u2019t need to build a custom banner to promote downloading your app. Both iOS and Android provide a nice built-in way to easily pop up a banner on your website prompting the user to download your app. Using the banner provided by the operating system (instead of building your own) gives you less control, but it results in a more integrated and slick app installation experience with less user annoyance.\n\nOn iOS, you can choose to show users a Smart Banner like this:\n\nThe banner is \u201csmart\u201d because it can tell if the user has the native app installed and adapt it\u2019s behavior. If the user doesn\u2019t have the app, it will prompt them to download it. If the user already has the app, it will open the app to the corresponding screen.\n\nImplementing this Smart Banner is really easy. You just need to add a <meta> tag to each page on your website where you want to show a banner:\n\nGoogle also has their own version of a smart banner. They call it an App Install Banner. It pops up inside the user\u2019s web browser when they visit your page:\n\nGoogle\u2019s implementation is slick. You can install and launch the app without ever leaving the web browser.\n\nBut it comes with more limitations than Apple\u2019s version:\n\nLet\u2019s implement it. First, you need to create a web app manifest json file and host it on your site:\n\nSecond, you need to add a <meta> tag to each page of your site linking to the manifest file:\n\nSo you\u2019ve implemented deep links and smart banners. Everything is working perfectly, right?\n\nUnfortunately there are areas where things don\u2019t work as expected \u2014 especially on iOS with Universal Links. Here are the most common issues:\n\nApple made a really strange UI decision when they implemented Universal Links. When you click on a Universal Link, you\u2019ll see the app open like this:\n\nBut watch out! Clicking the website link on the right doesn\u2019t just open the website once. It disables Universal Links permanently for this app!\n\nA lot of users click this once by mistake and don\u2019t realize they are turning off deep linking permanently. The only way to turn deep linking back on for that user is to have a Smart Banner on the corresponding webpage so the user has the path back into the app. That\u2019s another reason I highly recommend implementing Smart Banners on your website.\n\nYep, Universal Links won\u2019t trigger for users clicking on email links in the Gmail or Inbox apps. It sucks. Google seems to care more about keeping users in their apps than providing a consistent iOS user experience.\n\nThere are some gross ways to hack around this if you really need to, but those are beyond the scope of this article because they constantly break with new iOS updates. Good luck.\n\nYep, Universal Links won\u2019t trigger in the Facebook iOS app just like with Gmail. Again, Facebook seems to care more about keeping you inside their app (using a webview widget) than providing a consistent UX.\n\nIf you pay Facebook to post mobile ads, they do support deep linking on those ad links. Amazing how that works\u2026\n\nIf a user is already viewing your website in Safari, clicking a link to a different page on that same site won\u2019t make your app launch. This is by design on iOS (note that Android has the opposite behavior).\n\nIf you want to implement an \u201cOpen this page in our app\u201d button on your web site, you\u2019ll have to do something hacky like sending the user to a different domain name and enabling Universal Links for that second domain name as well. I\u2019d avoid this if possible."
    },
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3?source=user_profile---------17----------------",
        "title": "Machine Learning is Fun! Part 2 \u2013 Adam Geitgey \u2013",
        "text": "Back in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this:\n\nWe ended up with this simple estimation function:\n\nIn other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house\u2019s value.\n\nInstead of using code, let\u2019s represent that same function as a simple diagram:\n\nHowever this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn\u2019t so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn\u2019t matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model?\n\nTo be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases:\n\nNow we have four different price estimates. Let\u2019s combine those four price estimates into one final estimate. We\u2019ll run them through the same algorithm again (but using another set of weights)!\n\nOur new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model.\n\nLet\u2019s combine our four attempts to guess into one big diagram:\n\nThis is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions.\n\nThere\u2019s a lot that I\u2019m skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click:\n\nIt\u2019s just like LEGO! We can\u2019t model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together:\n\nThe neural network we\u2019ve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it\u2019s a stateless algorithm.\n\nIn many cases (like estimating the price of house), that\u2019s exactly what you want. But the one thing this kind of model can\u2019t do is respond to patterns in data over time.\n\nImagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess?\n\nI can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter.\n\nOur model might look like this:\n\nBut let\u2019s make the problem harder. Let\u2019s say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem.\n\nLet\u2019s use the first few words of Ernest Hemingway\u2019s The Sun Also Rises as an example:\n\nWhat letter is going to come next?\n\nYou probably guessed \u2019n\u2019 \u2014 the word is probably going to be boxing. We know this based on the letters we\u2019ve already seen in the sentence and our knowledge of common words in English. Also, the word \u2018middleweight\u2019 gives us an extra clue that we are talking about boxing.\n\nIn other words, it\u2019s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English.\n\nTo solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently.\n\nKeeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters.\n\nThis is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory.\n\nPredicting the next letter in a story might seem pretty useless. What\u2019s the point?\n\nOne cool use might be auto-predict for a mobile phone keyboard:\n\nBut what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over \u2014 forever? We\u2019d be asking it to write a complete story for us!\n\nWe saw how we could guess the next letter in Hemingway\u2019s sentence. Let\u2019s try generating a whole story in the style of Hemingway.\n\nTo do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github.\n\nWe\u2019ll create our model from the complete text of The Sun Also Rises \u2014 362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway\u2019s style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example.\n\nAs we just start to train the RNN, it\u2019s not very good at predicting letters. Here\u2019s what it generates after a 100 loops of training:\n\nYou can see that it has figured out that sometimes words have spaces between them, but that\u2019s about it.\n\nAfter about 1000 iterations, things are looking more promising:\n\nThe model has started to identify the patterns in basic sentence structure. It\u2019s adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there\u2019s also still a lot of nonsense.\n\nBut after several thousand more training iterations, it looks pretty good:\n\nAt this point, the algorithm has captured the basic pattern of Hemingway\u2019s short, direct dialog. A few sentences even sort of make sense.\n\nCompare that with some real text from the book:\n\nEven by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing!\n\nWe don\u2019t have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters.\n\nFor fun, let\u2019s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of \u201cEr\u201d, \u201cHe\u201d, and \u201cThe S\u201d:\n\nBut the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern.\n\nIn 2015, Nintendo released Super Mario Maker\u2122 for the Wii U gaming system.\n\nThis game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It\u2019s like a virtual LEGO set for people who grew up playing Super Mario Brothers.\n\nCan we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels?\n\nFirst, we need a data set for training our model. Let\u2019s take all the outdoor levels from the original Super Mario Brothers game released in 1985:\n\nThis game has 32 levels and about 70% of them have the same outdoor style. So we\u2019ll stick to those.\n\nTo get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game\u2019s memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game\u2019s memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime.\n\nHere\u2019s the first level from the game (which you probably remember if you ever played it):"
    },
    {
        "url": "https://medium.com/@ageitgey/how-to-shoot-in-manual-mode-720d7ccbcd68?source=user_profile---------18----------------",
        "title": "How to shoot in Manual Mode \u2013 Adam Geitgey \u2013",
        "text": "Thanks to modern technology, a high-end digital SLR camera is just as easy to use as a cellphone camera. Just set the camera on Auto, point, and shoot! The camera takes care of everything else automatically. Pretty great, right?\n\nBut if you have read anything online about taking better pictures, you\u2019ve certainly heard of a mysterious manual mode. You\u2019ve probably also heard that it is \u201cbetter than automatic\u201d for some vague and unspecified reasons.\n\nIs there really anything to this manual mode nonsense? If your camera can take perfectly good pictures automatically, then why try to outsmart it?\n\nIt turns out that there\u2019s a huge difference. There\u2019s nothing more important that you can do to be better at photography than to learn how to shoot in Manual mode. And it\u2019s not hard to learn!\n\nThe automatic exposure system on your camera has one goal \u2014 To make sure that just enough light hits your camera\u2019s sensor to produce an picture that doesn\u2019t look obviously terrible. Not enough light or too much light will ruin the picture.\n\nThere are three ways your camera can control how much light is captured in the picture:\n\nIn automatic mode, your camera will decide in a split second how wide to open it\u2019s aperture, how long to hold it open, and how sensitive the sensor needs to be to properly capture apicture.\n\nThe problem is that changing each of these variables drastically alters the final look of your picture!\n\nOpening the aperture wider causes the background of the picture to go out of focus. Keeping the shutter open too long makes everything in the image blurry. Increasing the sensitivity of the sensor too high adds ugly noise to your picture.\n\nIn automatic mode, you are letting the camera decide how these choices are made. In other words, you are letting the camera decide how your picture will look. That\u2019s why you end up with a bunch of generic-looking snapshots when you use automatic mode.\n\nWith a tiny bit of practice, you can learn how to use manual mode to take pictures that are much more interesting than what the camera would come up with by default.\n\nAs a photographer, you have exactly two jobs:\n\nIn automatic mode, your camera completely handles the technical job for you (and quite well!). The problem is that handling the first job requires the camera to make all the decisions for you which prevents you from doing the second job.\n\nLet\u2019s go through each way you can control the amount of light coming into your camera. Each method has side effects which change the final look of your image. These will be your main tools to change how your picture looks.\n\nYour digital camera has an image sensor inside of it. This image sensor is what records the picture \u2014 it is the digital version of film.\n\nLike the volume knob on your television or radio, this image sensor has a \u201cvolume\u201d control that controls how much light it records. The higher you turn up the sensitivity, the less light you need to make a picture. However, higher sensitivity levels push your camera to the limit and add ugly \u201cnoise\u201d to your image.\n\nCamera sensitivity is measured according to a confusing scale called the ISO scale. The history behind the name of the scale is convoluted, but all you need to know is that lower numbers mean the sensor is less sensitive and higher numbers mean the sensor is more sensitive. Typically, ISO 100 is the least sensitive setting. The most sensitive setting varies according to the capabilities of your camera.\n\nHere is an example of how an image appears brighter with higher ISO settings:\n\nThe ISO scale doubles each time the image is twice as bright. So ISO 1600 is only twice as bright as ISO 800 \u2014 it\u2019s not 800% brighter.\n\nNote that higher ISO settings make the final image look worse by adding noise that appear as ugly dots in the final image. More expensive cameras typically can operate at higher sensitivity levels without nearly as much noise. That\u2019s one reason that some digital cameras cost $5000 while others only cost $400. The more expensive ones can often shoot high-quality pictures with much less light.\n\nThe next variable you can control is how long the camera\u2019s opening stays open. The opening in your camera lens is covered by a shutter. When you click the button on your camera to take a picture, the shutter opens for a brief moment. The longer it stays open, the more light comes into the camera.\n\nShutter speed is measured in a very simple scale \u2014 fractions of a second. So a 1/50 shutter speed means the camera stays open for one fiftieth of a second. That sounds like barely any time at all, but it\u2019s actually quite a long time in photographic terms. Anything that moves while the shutter is open will appear as blurry in the final image. By using a faster shutter speed, you can \u201cfreeze\u201d the action. But to use a faster shutter speed, you need more light because the shutter wasn\u2019t open as long and not as much light reached the image sensor.\n\nThe moving bird appears quite blurry at 1/50th of a second. But at 1/800th of a second, the flying bird is very sharp. That\u2019s because even a flying bird doesn\u2019t move much in 1/800th of a second so it appears to be \u201cstill\u201d in the picture.\n\nIn most cases, you want the shutter speed to be fast enough to capture your image without any blurriness. However, there are many cool effects that you can do by keeping the shutter open for a long time while the camera is fixed on a tripod. For example, moving water will take on a smoothy, dreamy effect if you leave the shutter open for a long time.\n\nThe third way to control light coming into your camera is to change the size of the opening in the camera\u2019s lens. This hole, called the aperture, is the place where light enters the camera. Obviously a bigger hole lets in more light than a smaller hole.\n\nBut this is where things get complicated. Because of the way light focusing works, a larger opening causes the background of the image to become blurry. A small opening causes more of the background to be in focus.\n\nThe scale used to measure the size of the size of the opening (again, called the aperture) is the \u201cf-number\u201d scale. It sounds a lot more complicated than it is. There is just one trick- a smaller number means the opening is bigger. The f-number scale is what a math major would call a geometric sequence. If you aren\u2019t a math major and don\u2019t know what \u201ca power sequence of the square root of 2\u201d means, you can just memorize it: f/1.4, f/2, f/2.8, f/4, f/5.6, f/8, f/11, f/16, f/22.\n\nHere\u2019s a chart that shows how the background is very blurry at big settings and sharp at small settings:\n\nEach lens you own will have a number written on it like \u201cF4\u201d or \u201cF1.8\u201d. The number written on the lens is the biggest opening the lens can handle. A lens can always make the hole smaller to let less light in. Lenses with low numbers like F1.4 or F1.2 are more expensive because it takes more optics inside a lens to support larger openings. It can also make the lenses heavier.\n\nSince an F1.8 lens can open wider than an F4 lens, it can also blur the background more. That\u2019s why lenses with low f-numbers (and thus large openings) are often preferred for portraits. Portraits usually look good with the background blurry. This blurry background effect is called \u201cbokeh\u201d.\n\nOf course that doesn\u2019t mean every portrait should automatically have a blurry background. You can make the aperture smaller to keep everything in focus. This is useful if the background itself is a key part of the picture.\n\nBut when you are first learning manual mode, try using a large aperture for close-up portraits. Then once you get the hang of the blurry background \u2018look\u2019, try experimenting with other looks too.\n\nWhen you are taking a landscape photo, you usually don\u2019t want a large aperture opening. That\u2019s because you want the entire landscape in focus from front-to-back. It is the opposite of portraits.\n\nBut just like with portraits, feel free to experiment with other looks. You can use a blurred background to focus the viewer in on one area of the landscape.\n\nI\u2019m sure you already know how to take a picture in Automatic Mode:\n\nTaking a picture in Manual is just four extra steps:\n\nDo all these steps sound complicated? It really isn\u2019t bad at all. You just need to practice! Pick up your camera right now and try shooting in manual! Remember, the light meter on your camera is your friend. It will help you get your exposure correct. Listen to it and it will guide you in the right direction.\n\nOnce you are used to setting the shutter speed and aperture size quickly, move on to the last section of this guide.\n\nAs you practice shooting in manual mode, you will start to see a pattern. You are setting either the aperture size and then adjusting the shutter speed to match or you are setting the shutter speed and then adjusting the aperture size to match.\n\nWell here\u2019s a secret: Most pro photographers don\u2019t shoot in manual mode outside of a studio! Instead they shoot in \u201cAperture Priority mode\u201d or \u201cShutter Priority mode\u201d.\n\nAperture Priority mode is where you set the aperture size and then your camera sets the shutter speed automatically.\n\nShutter Priority mode is where you set the shutter speed and then your camera sets the aperture size automatically.\n\nBoth modes will save you a lot of time when you are shooting. Now that you have mastered manual, give them both a try. Most photographers use aperture priority mode. Shutter priority mode is more common for wildlife or sports photography where freezing the action is vital.\n\nMost cameras have also added an \u201cAutomatic ISO\u201d option which allows the camera to adjust the ISO level automatically with each shot. This is great when you are moving fast between different locations that have different lighting."
    },
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471?source=user_profile---------19----------------",
        "title": "Machine Learning is Fun! \u2013 Adam Geitgey \u2013",
        "text": "Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.\n\nFor example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It\u2019s the same algorithm but it\u2019s fed different training data so it comes up with different classification logic.\n\n\u201cMachine learning\u201d is an umbrella term covering lots of these kinds of generic algorithms.\n\nYou can think of machine learning algorithms as falling into one of two main categories \u2014 supervised learning and unsupervised learning. The difference is simple, but really important.\n\nLet\u2019s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there\u2019s a problem \u2014 you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don\u2019t have your experience so they don\u2019t know how to price their houses.\n\nTo help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it\u2019s size, neighborhood, etc, and what similar houses have sold for.\n\nSo you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details \u2014 number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:\n\nUsing that training data, we want to create a program that can estimate how much any other house in your area is worth:\n\nThis is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.\n\nTo build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.\n\nThis kind of like having the answer key to a math test with all the arithmetic symbols erased:\n\nFrom this, can you figure out what kind of math problems were on the test? You know you are supposed to \u201cdo something\u201d with the numbers on the left to get each answer on the right.\n\nIn supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type!\n\nLet\u2019s go back to our original example with the real estate agent. What if you didn\u2019t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning.\n\nThis is kind of like someone giving you a list of numbers on a sheet of paper and saying \u201cI don\u2019t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something \u2014 good luck!\u201d\n\nSo what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you\u2019d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts.\n\nAnother cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions.\n\nSupervised learning is what we\u2019ll focus on for the rest of this post, but that\u2019s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer.\n\nSide note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start.\n\nAs a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a \u201cfeel\u201d for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers.\n\nBut current machine learning algorithms aren\u2019t that good yet \u2014 they only work when focused a very specific, limited problem. Maybe a better definition for \u201clearning\u201d in this case is \u201cfiguring out an equation to solve a specific problem based on some example data\u201d.\n\nUnfortunately \u201cMachine Figuring out an equation to solve a specific problem based on some example data\u201d isn\u2019t really a great name. So we ended up with \u201cMachine Learning\u201d instead.\n\nOf course if you are reading this 50 years in the future and we\u2019ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human.\n\nSo, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further.\n\nIf you didn\u2019t know anything about machine learning, you\u2019d probably try to write out some basic rules for estimating the price of a house like this:\n\nIf you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change.\n\nWouldn\u2019t it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number:\n\nOne way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there\u2019s an exact ratio of ingredients to stir in to make the final price.\n\nThat would reduce your original function (with all those crazy if\u2019s and else\u2019s) down to something really simple like this:\n\nNotice the magic numbers in bold \u2014 .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices!\n\nA dumb way to figure out the best weights would be something like this:\n\nStart with each weight set to 1.0:\n\nRun every house you know about through your function and see how far off the function is at guessing the correct price for each house:\n\nFor example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house.\n\nNow add up the squared amount you are off for each house you have in your data set. Let\u2019s say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That\u2019s how \u201cwrong\u201d your function currently is.\n\nNow, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the cost of your function.\n\nIf you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that\u2019s our goal \u2014 get this cost to be as low as possible by trying different weights.\n\nRepeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you\u2019ve solved the problem!\n\nThat\u2019s pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow!\n\nBut here\u2019s a few more facts that will blow your mind:\n\nOk, of course you can\u2019t just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you\u2019d never run out of numbers to try.\n\nTo avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here\u2019s one way:\n\nFirst, write a simple equation that represents Step #2 above:\n\nNow let\u2019s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):\n\nThis equation represents how wrong our price estimating function is for the weights we currently have set.\n\nIf we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we\u2019d get a graph that might look something like this:\n\nIn this graph, the lowest point in blue is where our cost is the lowest \u2014 thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we\u2019ll have our answer!\n\nSo we just need to adjust our weights so we are \u201cwalking down hill\u201d on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we\u2019ll eventually get there without having to try too many different weights.\n\nIf you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function\u2019s tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill.\n\nSo if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we\u2019ll reach the bottom of the hill and have the best possible values for our weights. (If that didn\u2019t make sense, don\u2019t worry and keep reading).\n\nThat\u2019s a high level summary of one way to find the best weights for your function called batch gradient descent. Don\u2019t be afraid to dig deeper if you are interested on learning the details.\n\nWhen you use a machine learning library to solve a real problem, all of this will be done for you. But it\u2019s still useful to have a good idea of what is happening.\n\nThe three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you\u2019ve never seen before based where that house would appear on your line. It\u2019s a really powerful idea and you can solve \u201creal\u201d problems with it.\n\nBut while the approach I showed you might work in simple cases, it won\u2019t work in all cases. One reason is because house prices aren\u2019t always simple enough to follow a continuous line.\n\nBut luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies.\n\nAlso, I ignored the idea of overfitting. It\u2019s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren\u2019t in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully.\n\nIn other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it\u2019s a skill that any developer can learn!\n\nOnce you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data!\n\nBut it\u2019s important to remember that machine learning only works if the problem is actually solvable with the data that you have.\n\nFor example, if you build a model that predicts home prices based on the type of potted plants in each house, it\u2019s never going to work. There just isn\u2019t any kind of relationship between the potted plants in each house and the home\u2019s sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two.\n\nSo remember, if a human expert couldn\u2019t use the data to solve the problem manually, a computer probably won\u2019t be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly.\n\nIn my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn\u2019t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it\u2019s getting a little better every day.\n\nIf you want to try out what you\u2019ve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try!\n\nIf you want to go deeper, Andrew Ng\u2019s free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math.\n\nAlso, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It\u2019s a python framework that has \u201cblack box\u201d versions of all the standard algorithms."
    }
]