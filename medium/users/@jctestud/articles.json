[
    {
        "url": "https://towardsdatascience.com/food-ingredient-reverse-engineering-through-gradient-descent-2a8d3880dd81?source=user_profile---------1----------------",
        "title": "Food Ingredient Reverse-Engineering Through Gradient Descent",
        "text": "Let\u2019s take our beloved palm oil spread, Nutella, as an example. \n\nHere are the ingredients:\n\nIn my linear regression model, the parameters (weights) are the amounts in grams for the different ingredients:\n\nIn some cases, some percentage are known. It is the case for European Nutella where we know the amounts for the healthier ingredients (hazelnut, cocoa, etc.). In such cases, the weights are set and not trainable.\n\nLet\u2019s take the \u201cTotal Fat\u201d component as an example, it gives us a (x, y) tuple.\n\nx is a row vector containing the percentage of fat in each ingredient:\n\nThese one are straightforward. But for some ingredients, it gets harder to guess the composition (lecithin anyone?). For this experiment, I used the USDA National Nutrient Database which contains this information for most basic ingredients.\n\nOn the other side of my very deep 1-layer neural network, y is a scalar, containing the amount of fat in the final product. This information can easily be found in the nutrition facts tables:\n\n12g of fat \u201cper serving\u201d for Nutella, or, in a more civilized nutrition labeling system: 31% (thank you France)\n\nSince this labeling is quite verbose, we can get about ten (x, y) samples.\n\nOf course, in the model, the linear unit has no bias. There is no dark matter to find in food ingredients, everything is accounted for in the weighted sum of the quantities.\n\nDeclaring all of this in PyTorch is relatively easy (this is my first time), this library is very natural and straightforward, I think I understand the hype now. Now, should we just reduce the raw L2-loss? No, there are a lot of constraints we have to set on the model for it to converge to a plausible local minima and not to some weird recipe where I have to put a negative amount of cocoa.\n\nSome of these constraints are enforced when updating the weights and some through alchemist tricks in the loss function. None of this is pretty, I am afraid.\n\nI used the whole dataset (batch gradient descent) to compute the loss function at each step. Here are the results:"
    },
    {
        "url": "https://towardsdatascience.com/game-of-thrones-word-embeddings-does-r-l-j-part-2-30290b1c0b4b?source=user_profile---------2----------------",
        "title": "Game of Thrones Word Embeddings, does R+L = J ? \u2014 part 2",
        "text": "GloVe is built so that the vectors for similar words get a similar direction in space (it tries to maximize their dot product). One way to measure a direction similarity is through cosine similarity (~angle between the vectors). If vectors have the same amplitude, dot product and cosine similarity are equivalent. So to make everything a dot product, the first step is to normalize the word vectors.\n\nWe can now find similar words to a known word, or any vector we may produce, like the ones that answer analogy questions. To check whether or not, creating word embeddings on this unusual dataset is completely useless, let\u2019s see how our vectors fare on the Google word analogy dataset we mentioned earlier (part 1). It is a set of 19000+ word analogies. Here are the results:\n\nA lot of analogies were not testable since some words are not in the vocabulary (there is no mention of Paris or Berlin in the books, go figure). For the rest, our vectors achieve almost 10% accuracy. It is obviously very bad but it shows also that this vector space is not useless. Below, some of the \u201csolved\u201d analogies:"
    },
    {
        "url": "https://towardsdatascience.com/game-of-thrones-word-embeddings-does-r-l-j-part-1-8ca70a8f1fad?source=user_profile---------3----------------",
        "title": "Game of Thrones Word Embeddings, does R+L = J ? \u2014 part 1",
        "text": "By applying the same \u201che-to-she\u201d transformation to something else, we are actually swapping gender. \u201cman\u201d becomes \u201cwoman\u201d and \u201cactor\u201d becomes \u201cactress\u201d. To illustrate this, here is a glimpse at part 2 . Below, a visualization of word vectors built from Game of Thrones books (only):\n\nOne particular aspect of word embeddings that blew everyone\u2019s mind in 2013, is the fact that the resulting vector space has a lot of unexpected properties. It accidentally encodes several high-level semantic concepts. For example, pronouns like \u201che\u201d and \u201cshe\u201d are very closed to each other (as expected), but at the same time, the path from \u201che\u201d to \u201cshe\u201d (another vector) has amazing properties.\n\nIn a completely unsupervised fashion, an algorithm has encoded the concept of \u201cgender\u201d (without naming it), how is this possible?\n\nThink of it like a space where words are attracted to each other for different reasons. Let\u2019s take this short text as an example:\n\nWith this text, we expect people from the same family to be attracted to each other, and also people from the same sex.\n\nOn the left, a truncated co-occurrence matrix with only some specific target words (rows) and context words (columns).\n\nWith a direct 2-D approximation (PCA factorization) of the co-occurrence matrix, we get the following word representations (this operation is obviously different and more complex in GloVe). In this 2-D space, four \u201cforces\u201d are at work. Men are pulling other men towards them, and, at the same time, women, Starks, and Baratheons do the same.\n\nIt reaches an equilibrium where Baratheons end up at the bottom, Starks on top, women on the left and men on the right, all of it without any guidance.\n\nEmbeddings capture a lot of these semantic relationships (or \u201clinguistic regularities\u201d like some are calling them). They can be used to solve word analogy tasks like:\n\nExample with one of the four terms missing:\n\nThis is called \u201canalogical reasoning\u201d, the task was introduced by word2vec authors Tomas Mikolov et al. who built a complete dataset containing a lot of these questions.\n\nOur 4-sentence word space can already solve some things like:\n\nIn our space, it gives us a vector that is more or less \u201cStarks\u201d (tada!).\n\nYou can see that a lot is already going on in our short text example, imagine now in 5 books, or in Wikipedia, and also with more dimensions (hundreds) to let the algorithm some space to express itself.\n\nIdeally, to \u201clearn\u201d the English language, you have to use a massive corpus. The Stanford\u2019s group (as well as Google) provide pre-trained word embeddings where the corpus contained more than 10 billion words (usually from Wikipedia). In our case, we only have 10 million words, and they come from a medieval fantasy book (so please be indulgent).\n\nYou are now ready for part 2!"
    },
    {
        "url": "https://towardsdatascience.com/yet-another-text-generation-project-5cfb59b26255?source=user_profile---------4----------------",
        "title": "Yet another text generation project \u2013",
        "text": "I created a char-rnn with Keras 2.0.6 (with the tensorflow backend). Everything is available at this address. It takes the form of two python notebooks, one for training and one for testing. You can use the dataset, train a model from scratch, or skip that part and use the provided weights to play with the text generation (have fun!).\n\nI built a text dataset based on the Trump Twitter Archive which is an up-to-date archive of all the President\u2019s tweets (~32000 tweets starting in 2009).\n\nThis represents 3.3MB of text data which is roughly 3.5 million characters. From what I gather, it is not very large for what we are trying to do.\n\nThe character \u201cvocabulary\u201d is fixed to the 98 printable ASCII characters in order (working on a stable vocabulary can be useful to re-purpose the model)\n\nTraining is based on feeding the network fixed-size sequences of 60 characters. With my GPU, I was able to work on large batches containing 512 of these samples.\n\nOne batch is a list of 512 sequences of 60 characters. Each character can be only one of the 98 available entries in the vocabulary. For each 60-char sequence, the expected output is the following 61-st character. The prediction is just of one step (sequence of length 1).\n\nPreparing these batches can take a lot of memory. As a consequence, batch creation is done live while training.\n\nThe architecture is inspired by the one from Fran\u00e7ois Chollet, available in the Keras repository (it was itself derived from Karpathy\u2019s work).\n\nThe neural network has 4 stacked 512-unit LSTM layers, each one followed by a 20% random neuron dropout for regularization.\n\nThe last recurrent layer is set to return its final state only (one step).\n\nOne fully-connected regular layer takes the \u201chidden\u201d vector of size 512 and brings it back to the size of the vocabulary (98).\n\nOne last processing is to apply the softmax function to this vector for it to become a character probability distribution.\n\nFinally, the network is trained with the Adam optimizer to minimize the cross-entropy loss function (more or less a distance between distributions).\n\nAfter a dozen epochs of training, the output starts sounding coherent.\n\nNote: For Keras connoisseur out there, I am using \u201cstateless\u201d mode for training, meaning that nothing is saved in the network between two (even consecutive) sequences of 60 characters. However, the network is used in a 1-to-1 character \u201cstateful\u201d mode while generating text (RNN unit states are never reset)\n\nThis model can now be used to generate Trump-like text. The network can be given a seed (some text) to warm up and get some context before going 100% autonomous in its generation process.\n\nSome improvisation is also added while generating. At each step, the chosen character is not always the one with the highest probability in the network output. This additional step prevents loops and give more human results.\n\nHere is an example with low diversity:\n\nWith the same seed and a higher diversity:\n\nThe model is interesting and fun as it is, but it is rarely coherent. It fails at keeping context for a long time. It is easy to see the network forget something it started not a long time ago.\n\nThe relatively low 60-character sequence length is probably a reason. This length dictates how far the gradient can propagate backwards in time when training (truncated back-propagation through time). A larger \u201clook-back\u201d could make it possible to learn longer dependencies. An ideally trained char-rnn should be able, for example, to keep track of quotes and parenthesis in order to be sure to close them, to remember the subject of the current sentence, etc.\n\nThe poor results could also come from the data itself. It may be too limited in size. Also, tweets are probably not the best way to learn language. Tweets are short and independent, they contain weird text like URLs and user handles, etc."
    }
]