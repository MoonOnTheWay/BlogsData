[
    {
        "url": "https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54?source=user_profile---------1----------------",
        "title": "NLP \u2014 Building a Question Answering model \u2013",
        "text": "I recently completed a course on NLP through Deep Learning (CS224N) at Stanford and loved the experience. Learnt a whole bunch of new things. For my final project I worked on a question answering model built on Stanford Question Answering Dataset (SQuAD). In this blog, I want to cover the main building blocks of a question answering model.\n\nYou can find the full code on my Github repo.\n\nStanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.\n\nThere has been a rapid progress on the SQuAD dataset with some of the latest models achieving human level accuracy in the task of question answering!\n\nExamples of context, question and answer on SQuAD\n\nContext \u2014 Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973\u201374, and the Apollo\u2013Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n\nQuestion \u2014 What space station supported three manned missions in 1973\u20131974?\n\ni) It is a closed dataset meaning that the answer to a question is always a part of the context and also a continuous span of context\n\nii) So the problem of finding an answer can be simplified as finding the start index and the end index of the context that corresponds to the answers\n\niii) 75% of answers are less than equal to 4 words long\n\nThe training dataset for the model consists of context and corresponding questions. Both of these can be broken into individual words and then these words converted into Word Embeddings using pretrained vector like GloVe vectors. To learn more about Word Embeddings please check out this article from me. Word Embeddings are much better at capturing the context around the words than using a one hot vector for every word. For this problem I used 100 dimension GloVe word embeddings and didn\u2019t tune them during the training process since we didn\u2019t have sufficient data.\n\nThe next layer we add in the model is a RNN based Encoder layer. We would like each word in the context to be aware of words before it and after it. A bi-directional GRU/LSTM can help do that. The output of the RNN is a series of hidden vectors in the forward and backward direction and we concatenate them. Similarly we can use the same RNN Encoder to create question hidden vectors.\n\nUp til now we have a hidden vector for context and a hidden vector for question. To figure out the answer we need to look at the two together. This is where attention comes in. It is the key component in the Question Answering system since it helps us decide, given the question which words in the context should I \u201cattend\u201d to. Lets start with the simplest possible attention model:\n\nThe dot product attention would be that for each context vector c i we multiply each question vector q j to get vector e i (attention scores in the figure above). Then we take a softmax over e i to get \u03b1 i(attention distribution in the figure above). Softmax ensures that the sum of all e i is 1. Finally we calculate a i as the product of the attention distribution \u03b1 i and the corresponding question vector(attention output in the figure above). Dot product attention is also described in the equations below\n\nThe above attention has been implemented as baseline attention in the Github code.\n\nYou can run the SQuAD model with the basic attention layer described above but the performance would not be good. More complex attention leads to much better performance.\n\nLets describe the attention in the BiDAF paper. The main idea is that attention should flow both ways \u2014 from the context to the question and from the question to the context.\n\nWe first compute the similarity matrix S \u2208 R N\u00d7M, which contains a similarity score Sij for each pair (ci , qj ) of context and question hidden states. Sij = wT sim[ci ; qj ; ci \u25e6 qj ] \u2208 R Here, ci \u25e6 qj is an elementwise product and wsim \u2208 R 6h is a weight vector. Described in equation below:\n\nNext, we perform Context-to-Question (C2Q) Attention. (This is similar to the dot product attention described above). We take the row-wise softmax of S to obtain attention distributions \u03b1 i , which we use to take weighted sums of the question hidden states q j , yielding C2Q attention outputs a i .\n\nNext, we perform Question-to-Context(Q2C) Attention. For each context location i \u2208 {1, . . . , N}, we take the max of the corresponding row of the similarity matrix, m i = max j Sij \u2208 R. Then we take the softmax over the resulting vector m \u2208 R N \u2014 this gives us an attention distribution \u03b2 \u2208 R N over context locations. We then use \u03b2 to take a weighted sum of the context hidden states c i \u2014 this is the Q2C attention output c prime. See equations below\n\nFinally for each context position c i we combine the output from C2Q attention and Q2C attention as described in the equation below\n\nIf you found this section confusing, don\u2019t worry. Attention is a complex topic. Try reading the BiDAF paper with a cup of tea :)\n\nAlmost there. The final layer of the model is a softmax output layer that helps us decide the start and the end index for the answer span. We combine the context hidden states and the attention vector from the previous layer to create blended reps. These blended reps become the input to a fully connected layer which uses softmax to create a p_start vector with probability for start index and a p_end vector with probability for end index. Since we know that most answers the start and end index are max 15 words apart, we can look for start and end index that maximize p_start*p_end.\n\nOur loss function is the sum of the cross-entropy loss for the start and end locations. And it is minimized using Adam Optimizer.\n\nThe final model I built had a bit more complexity than described above and got to a F1 score of 75 on the test set. Not bad!\n\nGive me a \u2764\ufe0f if you liked this post:) Hope you pull the code and try it yourself.\n\nPS: I have my own deep learning consultancy and love to build interesting deep learning models. I have helped several startups deploy innovative AI based solutions. If you have a project that we can collaborate on, then please contact me at priya.toronto3@gmail.com"
    },
    {
        "url": "https://towardsdatascience.com/using-tensorflow-object-detection-to-do-pixel-wise-classification-702bf2605182?source=user_profile---------2----------------",
        "title": "Using Tensorflow Object Detection to do Pixel Wise Classification",
        "text": "In the past I have used Tensorflow Object Detection API to implement object detection with the output being bounding boxes around different objects of interest in the image. For more please look at my article. Tensorflow recently added new functionality and now we can extend the API to determine pixel by pixel location of objects of interest. See example below:\n\nThe code is on my Github .\n\nInstance segmentation is an extension of object detection, where a binary mask (i.e. object vs. background) is associated with every bounding box. This allows for more fine-grained information about the extent of the object within the box.\n\nSo when would we need this extra granularity? Some examples that come to mind are:\n\ni) Self Driving Cars \u2014 May need to know exactly where another car is on the road or the location of a human crossing the road\n\nii) Robotic systems \u2014 Robots that say join two parts together will perform better if they know the exact locations of the two parts\n\nThere are several algorithms that implement instance segmentation but the one used by Tensorflow Object Detection API is Mask RCNN.\n\nFaster RCNN is a very good algorithm that is used for object detection. Faster R-CNN consists of two stages. The first stage, called a Region Proposal Network (RPN), proposes candidate object bounding boxes. The second stage, which is in essence Fast R-CNN, extracts features using RoIPool from each candidate box and performs classification and bounding-box regression. The features used by both stages can be shared for faster inference.\n\nMask R-CNN is conceptually simple: Faster R-CNN has two outputs for each candidate object, a class label and a bounding-box offset; to this we add a third branch that outputs the object mask \u2014 which is a binary mask that indicates the pixels where the object is in the bounding box. But the additional mask output is distinct from the class and box outputs, requiring extraction of much finer spatial layout of an object. To do this Mask RCNN uses the Fully Convolution NetworkMask RCNN Paper (FCN) described below.\n\nFCN is a popular algorithm for doing semantic segmentation. This model uses various blocks of convolution and max pool layers to first decompress an image to 1/32th of its original size. It then makes a class prediction at this level of granularity. Finally it uses up sampling and deconvolution layers to resize the image to its original dimensions.\n\nSo in short we can say that Mask RCNN combines the two networks \u2014 Faster RCNN and FCN in one mega architecture. The loss function for the model is the total loss in doing classification, generating bounding box and generating the mask.\n\nMask RCNN has a couple of additional improvements that make it much more accurate than FCN. You can read more about them in their paper.\n\nTo test this model on images, you can leverage the code shared on the tensorflow website. I tested their most lightweight model \u2014 mask_rcnn_inception_v2_coco. Just download the model and upgrade to tensorflow 1.5 (this is important!). See sample result below:\n\nFor me the more interesting exercise was to run the model on sample videos from you tube. I used keepvid to download a few videos from you tube. And I love the library moviepy for manipulating video files.\n\nYou can find the full code on my Github.\n\nCouple of additional ideas for further exploration of this API:\n\nGive me a \u2764\ufe0f if you liked this post:) Hope you pull the code and try it yourself.\n\nPS: I have my own deep learning consultancy and love to build interesting deep learning models. I have helped several startups deploy innovative AI based solutions. If you have a project that we can collaborate on, then please contact me at priya.toronto3@gmail.com"
    },
    {
        "url": "https://towardsdatascience.com/handwriting-recognition-using-tensorflow-and-keras-819b36148fe5?source=user_profile---------3----------------",
        "title": "Handwriting recognition using Tensorflow and Keras \u2013",
        "text": "Handwriting recognition aka classifying each handwritten document by its writer is a challenging problem due to huge variation in individual writing styles. The traditional approach to solving this would be to extract language dependent features like curvature of different letters, spacing b/w letters etc. and then use a classifier like SVM to distinguish between writers. In the blog, I want to demonstrate a deep learning based approach to identifying these features. We will pass small patches of handwritten images to a CNN and train with a softmax classification loss.\n\nTo demonstrate the effectiveness of this technique, lets use it to classify English Handwritten text.\n\nYou can find the full code on my Github repo\n\nThe IAM Handwriting database is the biggest database of English handwriting images. It has 1539 pages of scanned text written by 600+ writers. For the purpose of this demo we will take the top 50 writers with the most amount of data. For each writer, the database is a collection of individual sentences written by them. See samples below for one writer:\n\nNeural networks don\u2019t require much preprocessing of raw data. So we will not make any modifications to these images however instead of passing the full image to the neural network, we will pass small patches of text.\n\nWe want the neural network to understand the writing style of individual writer and we would prefer that this neural network be text independent (can work on any language). So instead of passing individual sentences or words, we will pass it random patches of text. This is done by randomly cropping 113x113 sized patches from every sentence. The image below is a collage of 8 such patches.\n\nWe can write a generator function to move over each sentence and generate random patches of images from it. For every image we will limit the no. of patches to 30% of total patches that can be generated. For more information on how to write this generator function, please check out my Github repo.\n\nFor this task we build a convolution neural network (CNN) in Keras using Tensorflow backend. We will use a standard CNN with multiple convolution and maxpool layers, a few dense layers and a final output layer with softmax activation. RELU activation was used between the convolution and dense layers and model was optimized using Adam optimizer.\n\nThe size of the model needs to be proportional to the size of the data. Three blocks of convolution -maxpool layers and couple of dense layers was sufficient for this problem. See model summary below:\n\nAfter a bit of hyper parameter tuning we got to a loss of 94% on the test dataset which the model was never exposed to.\n\nSee two patches below that the model classified as the same writer. The shape of \u201ct\u201d seems very similar so would also make intuitive sense that they belong to the same writer."
    },
    {
        "url": "https://towardsdatascience.com/training-and-visualising-word-vectors-2f946c6430f8?source=user_profile---------4----------------",
        "title": "Training and Visualising Word Vectors \u2013",
        "text": "In this tutorial I want to show how you can implement a skip gram model in tensorflow to generate word vectors for any text you are working with and then use tensorboard to visualize them. I found this exercise super useful to i) understand how skip gram model works and ii) get a feel for the kind of relationship these vectors are capturing about your text before you use them downstream in CNNs or RNNs.\n\nI trained a skip gram model on text8 dataset which is collection of English Wikipedia articles. I used Tensorboard to visualize the embeddings. Tensorboard allows you to see the whole word cloud by using PCA to select 3 main axis to project the data. Super cool! You can type in any word and it will show its neighbours. You can also isolate the 101 points closest to it.\n\nYou can find the full code on my Github repo.\n\nTo visualize training, I also looked at the closest predicted word to a random set of words. In the first iteration the closest predicted words seem very arbitrary which makes sense since all word vectors were randomly initialized\n\nBy the end of training, the model had become much better at finding relationship between words.\n\nCreating word vectors is the process of taking a large corpus of text and creating a vector for each word such that words that share common contexts in the corpus are located in close proximity to one another in the vector space.\n\nThese word vectors can get amazingly good at capturing contextual relationship between words (example vectors for black, white and red would be close together) and we get far better performance with using these vectors instead of raw words for NLP tasks like text classification or new text generation.\n\nThere are two main models for generating these word vectors \u2014 Continuous Bag of Words (CBOW) and Skip Gram Model. The CBOW model tries to predict the center word given context word while skip gram model tries to predict context words given center word. A simplified example would be:\n\nCBOW: The cat ate _____. Fill in the blank, in this case, it\u2019s \u201cfood\u201d.\n\nSkip-gram: ___ ___ ___ food. Complete the word\u2019s context. In this case, it\u2019s \u201cThe cat ate\u201d\n\nIf you are interested in a more detailed comparison of these two methods, then please see this link.\n\nVarious papers have found that Skip gram model results in better word vectors and so I have focused on implementing that\n\nHere I will list the main steps to build the model. Please see the detailed implementation on my Github\n\nWe first clean our data. Remove any punctuation, digits and split the text into individual words. Since programs deal much better with integers than words we map every word to an int by creating a vocab to int dictionary. Code below.\n\nWords that show up often such as \u201cthe\u201d, \u201cof\u201d, and \u201cfor\u201d don\u2019t provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word in the training set, we\u2019ll discard it with probability given by inverse of its frequency.\n\nThe input for skip gram is each word (coded as int) and the target is words around that window. Mikolov et al found that performance was better if this window was variable in size and words closer to to the center word were sampled more frequently.\n\n\u201cSince the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples\u2026 If we choose window size=5, for each training word we will select randomly a number R in range between 1 and window size, and then use R words from history and R words from the future of the current word as correct labels.\u201d\n\nFrom Chris McCormick\u2019s blog, we can see the general structure of the network that we will build.\n\nWe\u2019re going to represent an input word like \u201cants\u201d as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we\u2019ll place a \u201c1\u201d in the position corresponding to the word \u201cants\u201d, and 0s in all of the other positions.\n\nThe output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n\nAt the end of training the hidden layer will have the trained word vectors. The size of the hidden layer corresponds to the num of dimensions in our vector. In the example above each word will have a vector of length 300.\n\nYou may have noticed that the skip-gram neural network contains a huge number of weights\u2026 For our example with 300 features and a vocab of 10,000 words, that\u2019s 3M weights in the hidden layer and output layer each! Training this on a large dataset would be prohibitive, so the word2vec authors introduced a number of tweaks to make training feasible. You can read more about them in the link. The code on Github implements these to speed up training.\n\nYou can using the embeddings projector in Tensorboard to visualize the embeddings. To do this you need to do a few things:\n\nGive me a \u2764\ufe0f if you liked this post:) Hope you pull the code and try it yourself. If you have other ideas on this topic please comment on this post or mail me at priya.toronto3@gmail.com\n\nPS: I have a deep learning consultancy and love to work on interesting problems. If you have a project that we can work collaborate on then please contact me at priya.toronto3@gmail.com"
    },
    {
        "url": "https://towardsdatascience.com/using-object-detection-for-a-smarter-retail-checkout-experience-3f39acef857b?source=user_profile---------5----------------",
        "title": "Using Object detection for a Smarter Retail Checkout Experience",
        "text": "I have been playing around with the Tensorflow Object Detection API and have been amazed by how powerful these models are. I want to share the performance of the API for some practical use cases.\n\nThe first use case is a smarter retail checkout experience. This is a hot field right now after the announcement of Amazon Go stores.\n\nStores can be designed so they have smart shelves that track what a customer is picking from them. I did this by building two object detection models \u2014 one that tracks hand and captures what the hand has picked. And the second independent model that monitors shelf space. See GIF below. By using two models you minimise the error from a single approach.\n\nAnother application of computer vision for retail checkout can be that instead of scanning items one by one at a checkout system , everything is placed together and cameras are able to detect and log everything. Maybe we don\u2019t even need a checkout lane. Shopping carts can be equipped with cameras and you can simply walk out with your cart which can bill you as you step out of the store! Won\u2019t this be cool! I used the API to design a \u201cmini\u201d model with 3 random items and the model could easily detect what was placed and in what quantity. See GIF below. Through various experimentation, I found that the API performs very well even on items that are only partially visible.\n\nSo how do we build this?\n\nImages can be collected by looking through online publicly available data sets or by creating your own data. Each approach has its pros and cons. I generally use a mix of two. For example, the hand detector can be built by using publicly available data sets like the Ego Hand data set. This data set has a lot of variability in hand shapes, colours and poses which will be useful when the model is applied in real world. On the other hand, for items on shelf or in a cart, it is best to collect your own data since we don\u2019t expect much variability as well as we want to ensure we collect data from all sides. Before you build your model, it is always a good idea to augment your data by using image processing libraries like PIL an OpenCV to create additional images which have random variations in brightness, zoom, rotation etc. This process can create a lot of additional sample and make the model robust.\n\nFor object detection models, we need to annotations \u2014 bounding boxes around objects of interest. I use labelimg for this purpose. It is written in Python and uses Qt for interface. This is a very handle tool and annotations are created in the Pascal VOC format which makes it easy to create TFRecord files using the scripts shared in the Tensorflow Github \u2014 and\n\nI have written a very detailed tutorial on training Tensorflow Object Detection API on your custom data set \u2014 Building a Toy Detector with Tensorflow Object Detection API. And the associated Github. Please use this to get started.\n\nOne of the big decisions that you have to make when building the model is which object detection model to use as the fine tune checkpoint. The latest list of models available that have been trained on the COCO data set are:\n\nThere is a direct trade off b/w speed and accuracy. For a real time hand detection, it is best to use either the SSD models or the Faster RCNN Inception which I personally prefer. For item detection on shelf or shopping cart, I would prefer a slower but higher accuracy model like the Faster RCNN Resnet or the Faster RCNN Inception Resnet.\n\nI personally think the real work starts after you build the first version of the model! Since no model is perfect,when you start using it, you will notice gaps in its performance. Then you will need to use your intuition to decide if these gaps can be plugged and the model refined or if the situation needs another model or non model hack to get to the accuracy you desire. If you are lucky all you need is to add additional data to improve the performance.\n\nIf you want to know more about Object Detection and the Tensorflow Object Detection API, please checkout my article \u2014 Is Google Tensorflow Object Detection API the easiest way to implement image recognition?\n\nGive me a \u2764\ufe0f if you liked this post:) Hope you pull the code and try it yourself. If you have other ideas on this topic please comment on this post or mail me at priya.toronto3@gmail.com\n\nPS: I have my own deep learning consultancy and love to work on interesting problems. I have helped several startups deploy innovative AI based solutions. If you have a project that we can collaborate on, then please contact me at priya.toronto3@gmail.com"
    },
    {
        "url": "https://towardsdatascience.com/building-a-toy-detector-with-tensorflow-object-detection-api-63c0fdf2ac95?source=user_profile---------6----------------",
        "title": "Building a Toy Detector with Tensorflow Object Detection API",
        "text": "This project is second phase of my popular project -Is Google Tensorflow Object Detection API the easiest way to implement image recognition? In the original article I used the models provided by Tensorflow to detect common objects in youtube videos. These models were trained on the COCO dataset and work well on the 90 commonly found objects included in this dataset.\n\nHere I extend the API to train on a new object that is not part of the COCO dataset. In this case I chose a toy that was lying around. See gif below. So far, I have been impressed by the performance of the API. The steps highlighted here can be extended to any single or multiple object detector that you want to build.\n\nYou can find the code on my Github repo\n\nThe first step is collecting images for your project. You could download them from google ensuring you have a wide variation in angles, brightness, scale etc. In my case I created a video of the little aeroplane toy and used Opencv to extract images from the video. This saved me a lot of time. I ensured that images were taken from multiple angles. You can also randomly change brightness for some of the images so that the detector can work under different conditions of lightning. Overall 100\u2013150 pics will suffice. See some sample images below:\n\nPS: Since video was taken from my Iphone, the original images were pretty big \u2014 1920x1090. This would have required a lot of memory so used PIL resize to resize them to 500x300 kind of keeping the aspect ratio.\n\nI used labelimg to annotate the images. This is a very handle tool and annotations are created in the Pascal VOC format which is useful later on. It is written in Python and uses Qt for interface. I used Python3 + Qt5 with no problems. See example of annotated image. Essentially we identify xmin, ymin, xmax and ymax for the object and pass that to the model along with the image for training\n\nTensorflow API wants the datasets to be in TFRecord file format. This is probably the trickiest part. However tensorflow has provided a couple of handy scripts to get you started \u2014 and I was able to used the with minimal edits since labelimg already creates annotations in the correct format. I also like that this script randomly takes 30% of the data and creates a validation TFR file.\n\nYou will also need to create a label.pbtxt file that is used to convert label name to a numeric id. For my case it was as simple as\n\nI have included the label_map.pbtxt file and the create_pet_tf_records.py file on my github. In case you are getting stuck anywhere, I highly recommend the Oxfort Pets walkthrough provided by Tensorflow.\n\nOnce the TFR datasets are created, then first you need to decide if you will use an existing model and fine tune it or build from scratch. I highly recommend using an existing model since most of the features that are learnt by CNNs are often object agnostic and fine tuning an existing model is usually an easy and accurate process. Please note that if you do decide to build from scratch you will need much more than 150 images and training will take days. The API provides 5 different models that provide a trade off between speed of execution and the accuracy in placing bounding boxes. See table below:\n\nFor this project I decided to use the faster_rcnn_resnet101 that was trained on coco dataset. This is a very nice link if you want to learn more about RCNN models.\n\nTensorflow provides several sample config files to get started. I decided to use the faster_rcnn_resnet101_coco file and updated any paths that need to be configured in the file.Don\u2019t forget to update the num. of classes too.\n\nFinally! All the hard (and boring) part is done and we can start training the model. Since I have a reasonable GPU, I decided to train locally. However you can train on the cloud. Again tensorflow documentation has made this easy and provided all the steps.\n\nYou can start the training job and the evaluation jobs on two separate terminals at the same time. And initiate tensorboard to monitor performance. After training for 2\u20133 hours, I could see total loss get down to 0.077 and precision up to 0.99. By looking at images in Tensorboard we can see that model becomes accurate fairly quickly.\n\nTo test the model, we first select a model checkpoint (usually the latest) and export into a frozen inference graph. The script for this is also on my github. I tested the model on a new video recorded on my Iphone. As in my previous article, I used the Python moviepy library to parse the video into frames and then run object detector on each frame and collate results back into the video.\n\nCouple of things I noticed and additional explorations for the future\n\nGive me a \u2764\ufe0f if you liked this post:) Hope you pull the code and try it yourself.\n\nPS: I have my own deep learning model building consultancy and love to work on interesting problems. I have helped several startups deploy innovative AI based solutions. If you have a project that we can collaborate on, then please contact me at priya.toronto3@gmail.com"
    },
    {
        "url": "https://towardsdatascience.com/planning-the-path-for-a-self-driving-car-on-a-highway-7134fddd8707?source=user_profile---------7----------------",
        "title": "Planning the path for a Self-Driving Car on a Highway",
        "text": "Path planning is the brain of a self driving car. It is the module that tries to replicate the thinking and decision making we humans do while driving \u2014 read the map, analyze our environment (other vehicles and pedestrians) and decide the optimal action based on safety, speed and traffic rules.\n\nFor my first project of term 3 in the Udacity Self Driving Car Engineer Nanodegree program, I implemented a highway path planner to drive a car on a simulated high way with multiple lanes and traffic. See the gif and video below. The simulator provides us with current position and velocity of our car and traffic at each time step back and in turn we send to the simulator the next x and y positions so it can drive the car there. The goal is to drive the car successfully avoiding any collisions, safely changing lanes, staying below speed limit but not too slow and minimizing jerk on the passengers.\n\nYou can find the full code on my Github repo\n\nAnd here is a video of the car starting, changing speed and lanes and avoiding collisions. Notice how the terminal is displaying all the useful information for the user.\n\nSTEP 1: Analyze the sensor fusion data and categorize it meaningfully\n\nFrom the simulator, we get data for other vehicle in the simulator. This data includes the car_id, car_position(x and y), car_velocity (vx and vy), car_s (distance along the lane) and car_d (distance along the width of the lane)..\n\nI took the data and divided all cars in the simulator into either my own lane, my left lane or my right lane if those lanes exist.\n\nThen I calculate the important measures I needed for making decisions. These were\n\nI also ensured that \u2014 If I am in the left most lane then I set violate_left = 1 to make sure I don\u2019t turn left. Similarly if I am in the right most lane then I set violate_right = 1. If my lane is empty or if any other lanes is empty then the closest car distance was set to 999\n\nSTEP 2: Calculate cost of decisions and select the action with minimal cost\n\nI thought of 4 possible decisions, my car could make at any instance:\n\n1. Continue in my lane with max velocity\n\n2. Continue in my lane but slow down to about the speed of the car in front\n\n4. Change to the right\n\nI decided to assign costs for all these different functions: \n\nCost function \u2014 Continue in my lane\n\nCost Function \u2014 Slow down in my lane\n\nI set the cost of slow down = 200. This was done so that the action is less preferred than lane change but better than collision.\n\nAt every instance, the cost associated with all the decisions are calculated and the optimal decision is the one with the minimal cost\n\nSTEP 3: Once decision has been made, decide the target speed and lane for that decision\n\nTo drive the car in the simulator, I took the approach of deciding the target lane using its d-value (parameter indicating distance along the width of the lane). Lane 1 has a target d-value of 2, lane 2 has a target d-value of 6 and lane 3 has a target d-value of 10. So depending on the decision (stay in lane, or turn left or right) I decided the target d- value\n\nTo control speed in the simulator, I used a s increment (parameter indicating distance along the length of the lane).. I found that the closer s increments slowed down the car whereas farther speeded the car. To drive at max speed, I chose the s increment of 0.415. When I needed to brake in my lane, I reduced s increment based on the speed of the car in front of me.\n\nTo have a smooth driving experience, we should minimize jerk which is the derivative of acceleration. Essentially we should have smoother increases and decreases in acceleration. I did this by storing the previous values of s and d parameters and changing them gradually over many time steps. Finally I used a spline fitted to the map and the target s and d to decide the next_x and next_y values. These were sent to the simulator and the car moved to this position. After some playing around with the parameters the car drove smoothly in the simulator.\n\nMy code works in the simulator most of the time however there are some opportunities for improvement\n\nThe idea being if another car is getting close to the right side of its current lane, then there is a high probability that it is planning to turn right. And so on.\n\n2. Sometimes my car is in the leftmost lane, the middle lane has traffic and is unsafe to turn into but the rightmost lane is empty. A better logic would identify that and perform a series of quick lane changes to get to the rightmost lane instead of slowing down in its current lane. Essentially the logic currently only looks at the immediate adjacent lane in making its decisions\n\n3. I would like to have better ways to design jerk minimizing trajectories using some of the mathematical methods suggested in the lecture\n\nGive me a \u2764\ufe0f if you liked this post:) Hope you pull the code and try it yourself.\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity , Sebastian Thrun, David Silver and other people involved in developing this course for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. The images and the videos references here were shared in the lectures"
    },
    {
        "url": "https://towardsdatascience.com/is-google-tensorflow-object-detection-api-the-easiest-way-to-implement-image-recognition-a8bd1f500ea0?source=user_profile---------8----------------",
        "title": "Is Google Tensorflow Object Detection API the easiest way to implement image recognition?",
        "text": "There are many different ways to do image recognition. Google recently released a new Tensorflow Object Detection API to give computer vision everywhere a boost. Any offering from Google is not to be taken lightly, and so I decided to try my hands on this new API and use it on videos from you tube :) See the result below:\n\nYou can find the full code on my Github repo\n\nI added a second phase for this project where I used the Tensorflow Object Detection API on a custom dataset to build my own toy aeroplane detector. You can check out my article at\n\nSo what was the experience like? First lets understand the API.\n\nThe API has been trained on the COCO dataset (Common Objects in Context). This is a dataset of 300k images of 90 most commonly found objects. Examples of objects includes:\n\nThe API provides 5 different models that provide a trade off between speed of execution and the accuracy in placing bounding boxes. See table below:\n\nHere mAP (mean average precision) is the product of precision and recall on detecting bounding boxes. It\u2019s a good combined measure for how sensitive the network is to objects of interest and how well it avoids false alarms. The higher the mAP score, the more accurate the network is but that comes at the cost of execution speed.\n\nYou can get more information about these models at this link\n\nI decided to try the most light weight model (ssd_mobilenet). The main steps were:\n\nOverall a fairly simple set of steps. The API documentation also provides a handy Jupyter notebook that walks through the main steps.\n\nThe model had pretty good performance on the sample image (see below):\n\nNext I decided to try this API on some videos. To do this, I used the Python moviepy library. The main steps are:\n\nThis code takes a bit of time to run (~ 1 minute) for a 3\u20134 second clip. But since we are using a frozen model loaded to memory, all of this can be done on a computer without a GPU.\n\nI was very impressed! With just a little bit of code, you can detect and draw bounding boxes on a good number of commonly found objects with decent accuracy.\n\nThere were cases where I felt that the performance could be better. See example below. The birds are not detected at all in this video.\n\nCouple of additional ideas for further exploration of this API\n\nGive me a \u2764\ufe0f if you liked this post:) Hope you pull the code and try it yourself.\n\nPS: I have my own deep learning consultancy and love to work on interesting problems. I have helped several startups deploy innovative AI based solutions. If you have a project that we can collaborate on, then please contact me at priya.toronto3@gmail.com"
    },
    {
        "url": "https://towardsdatascience.com/traffic-signs-classification-for-self-driving-car-67ce57877c33?source=user_profile---------9----------------",
        "title": "Reading Traffic Signs with Human Like Accuracy \u2013",
        "text": "Self-Driving Cars will have to interpret all the traffic signs on our roads in real time and factor them in their driving. In this blog we use deep learning to train the car to classify traffic signs with 93% accuracy.\n\nI have shared the link to my GitHub with the full code in Python.\n\nThe dataset used for this exercise (can be downloaded from here) consists of 43 different traffic signs seen on German roads. The traffic sign images were cropped from actual road images and hence are under varying conditions of lighting as seen below.\n\nIt has a total 50,000 images. Images are 32x32 pixels and are in color.\n\nThe first step for any modeling exercise should be to get comfortable with the data involved. In this case its 43 different traffic signs. The data is distributed unevenly as seen below. Some signs have only 200 images whereas others has over 1200 images.\n\nVisualizing the images in the dataset as shown below, we can see that the images are small, have varying brightness and some are blurry. Also the traffic sign isn\u2019t always centered in the image.\n\nBefore feeding the images to a neural network, I normalized the images so that pixel values are between 0 and 0.5. I did this by diving all pixel values by 255. This is done since neural networks perform better when raw data is between 0 and 1. I decided to use colored traffic signs instead of converting them to gray since humans use the color of the sign for classifying it and so a machine could also benefit from this extra information. Finally I divided the dataset into training, validation and testing sets. The testing set is 30% of the sample that the model will never see.\n\nFor this project, I decided to use the LeNet architecture which is a simple Convolution Neural Network (CNN) that has performed well on MNIST data set. As seen below, this model has two convolution layers followed by two max pool layers. The first convolution layer uses a patch size of 5x5 and a filter of depth 6. The second convolution layer also uses a patch size of 5x5 but a filter of depth 16. After the convolutions, we flatten the output and then use two fully connected layers. The first has 120 neurons and the second has 84 neurons. RELU activation is used between all the layers. Finally we have the Output layer that uses Softmax to classify images in one of 43 classes.\n\nThe LeNet architecture works surprisingly well for this problem and within 30 epochs, we get a 98%+ accuracy on the validation sample. See below a plot of accuracy and loss :\n\nThe accuracy on the test sample that was never exposed to the model is around 93% which is pretty solid.\n\nAccuracy can be improved further if we use Image Augmentation \u2014 changes in brightness, rotation, translation etc. to increase the sample size.\n\nWhile neural networks can be a great learning device they are often referred to as a black box. We can understand what the neural network is learning by plotting its feature maps \u2014 which is the output from the filters. From these plotted feature maps, it\u2019s possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign\u2019s boundary outline or to the contrast in the sign\u2019s painted symbol.\n\nCheck out this link to learn more about visualizing neural networks.\n\nLets see how the 6 different filters for the first convolution layer respond to a \u201cDo Not Enter\u201d sign. Here bright spots reflect where the neurons are being activated. As can be seen the network is focusing on the circular shape of the sign and the flat line in the middle of it.\n\nIn contrast we look at feature map from an image of the sky without the traffic sign. Most of the filters are black meaning neural network is not recognizing anything distinct in this image.\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity and Sebastian Thrun for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. Most of the code used by me was suggested in classroom lectures. The images and the videos references here were also shared in the lectures"
    },
    {
        "url": "https://towardsdatascience.com/how-self-driving-cars-steer-c8e4b5b55d7f?source=user_profile---------10----------------",
        "title": "Steering Control for self-driving car \u2013",
        "text": "A typical self driving car starts with the perception system, which estimates the state of the surrounding environment, including landmarks and vehicles and pedestrians.\n\nThe localization block compares what it has learned to a map to figure out where the vehicle is.\n\nOnce the vehicle\u2019s location is known, the path planning block charts a trajectory to the destination.\n\nFinally, the control loop decides the steering and throttle/brake at every time step to guide the vehicle on this trajectory. In this blog I will talk about how control loop decides steering and throttle values\n\nFor my project 10 in the Udacity Self Driving car nano degree, I implemented predictive control to drive the car around in the simulator. See pic below. The simulator provides position and velocity of the car at each time step back to the user and in turn the user provides the steering and acceleration to the simulator to apply to the car. Big thanks to Udacity for creating this simulator and for providing me the expertise to implement model predictive control\n\nI have shared the link to my GitHub with the full code in C++.\n\nState: State of the vehicle at any point is described by 4 vectors \u2014 x position, y position, velocity (v) and the orientation (psi).\n\nActuators or control inputs are the steering angle (delta) and the throttle value (a). Braking can be expressed as negative throttle. So throttle can have values between -1 an +1. Steering angle is usually set to be between -30 deg to +30 deg.\n\nThe Kinematic motion model can be used to predict new state from previous state after steering and throttle are applied.\n\nHere Lf is the distance between the front of the vehicle and its center of gravity. The larger the vehicle, the slower the turn rate\n\nTo estimate the ideal steering angle and throttle, we estimate the error of our new state from our ideal state \u2014 actual trajectory we want to follow and the velocity and orientation we want to maintain and use a Ipopt Solver to minimize this error. This helps select the steering and throttle that minimizes the error with the desired trajectory.\n\nSo the key to good motion control is to define this error well. I set error to be based on the following\n\n3. Error in proportion to change in actuators \u2014 This additional term in the cost function captures the difference between the next actuator state and the current one. This ensures smooth changes in actuator values\n\nThe above terms can be multiplied by weights and these weights increase the importance of that term in the overall cost equation. For example if we multiply multiply by 500 that will ensure very smooth changes in steering and will prevent jerky motion in the simulator\n\nThe C++ Ipopt solver was used to minimize the error and calculate the optimal values of steering and throttle.\n\nTwo other fairly important parameters were no. of time steps in to the future (N) we want to predict steering and throttle for. And time step length (dt).\n\nOnce all the model is set and all the parameters are defined,\n\n2. We call the optimization solver. Given the initial state, the solver will return the vector of actuators that minimizes the cost function.\n\n3. We apply the steering and throttle to the vehicle. and Back to step 1.\n\nDriving the car around the simulator by predicting the correct steering and throttle at each step was not an easy task but an awesome learning experience. Big thanks to Udacity for creating this course!\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity and Sebastian Thrun for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. Most of the code used by me was suggested in classroom lectures. The images and the videos references here were also shared in the lectures"
    },
    {
        "url": "https://towardsdatascience.com/helping-a-self-driving-car-localize-itself-88705f419e4a?source=user_profile---------11----------------",
        "title": "Tracking a self-driving car with high precision \u2013",
        "text": "Localization, or knowing \u201cwhere am I\u201d is critical for a self driving car. At every instant it needs to know where in the world it is. When we drive a car we use GPS and Maps App to know where our car is. But GPS have an accuracy of 1\u20133 meters, sometimes more. This won\u2019t work for a self driving car since this is a width of a lane. The car can drift off from its lane which can be dangerous. A self driving car needs a much higher accuracy \u2014 less than 10 cm. Here we will discuss a technique called Particle Filters which can be used to help the car locate itself in a map.\n\nI was able to implement a particle filter that could track a car with a mean x-error and y-error of less than 0.07. I have shared the link to my GitHub with the full code in C++.\n\nBelow is a video showing a particle filter in action tracking a car in a simulated environment. The circular points are the landmarks in the map, the green lines are the LASER sensors from the car to these landmarks. The blue circle is the closest particle that is tracking the car. Notice how closely the particle tracks the actual car.\n\nHere is how Particle Filters work. To begin with we have a map of the world. The map has landmarks which could be houses, buildings, lamp posts etc. and we know the location of these landmarks within the map\n\nLIDAR uses LASER for measurement and generates a point cloud of the world around it providing the car with fairly accurate position x and position y values. However LIDAR is not very accurate in poor weather conditions or if the sensor gets dirty.\n\nBelow is an image of LIDAR point cloud\n\nNow comes the critical step. For each particle we calculate the distance between the particle and the known landmarks in the map. We compare how close this distance is to the LIDAR measurements from the car and use that to assign weight to the particle. Particles that are closer to the actual car will be given higher weight.\n\nCouple of practical things to take care of when applying the LIDAR measurements:\n\ni) Data Association \u2014 Multiple LIDAR measurement can be associated with each landmark. If that\u2019s the case we use nearest neighbor to chose the closest LIDAR reading to a landmark as the LIDAR reading as shown in the image below\n\nii) Transformation \u2014 When LIDAR measurements are taken from the car, their x and y position will be with reference to the car which is called the local coordinates of the car. These need to be converted to the global \u201cmap\u201d coordinates by also taking into consideration the position of the car/particle\n\n4. We re-sample particles such that particles with higher weight (and hence higher probability to be close to the true location of the car) are picked and those with lower weight are dropped.\n\nThen we go back to the step 2 and repeat this process.\n\nVery quickly this technique can help us localize a car.\n\nHere is an interesting question? \u2014 How would we use this technique for a real self driving car traveling between City A and City B? Particle filters assumes we have a map of the world with known location of many landmarks. How can we determine location of hundreds of landmarks and feed those to the car?\n\nOne approach that was used by the Google car was to match the image from the car\u2019s camera to the image from Google maps street view. This was used to generate landmarks and localize the car. Also some suppliers are creating detailed maps of the world with this information\n\nOverall building my first Particle Filter and using it for localization was a great experience.\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity and Sebastian Thrun for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. Most of the code used by me was suggested in classroom lectures. The images and the videos references here were also shared in the lectures"
    },
    {
        "url": "https://towardsdatascience.com/building-an-ai-chat-bot-e3a05aa3e75f?source=user_profile---------12----------------",
        "title": "Building an AI Chat bot! \u2013",
        "text": "Domain specific chat bots are becoming a reality! Using deep learning chat bots can \u201clearn\u201d about the topic provided to it and then be able to answer questions related to it. The applications of a technology like this are endless. You just provide data about a topic and watch the bot become an expert at it.\n\nIn this blog, we will build a blog that can answer logical reasoning questions. Yes it can be done! See gif below:\n\nLink to my GitHub with the full code in Python.\n\nHere is how we do this:\n\nData: The data used here was made open source by Facebook AI research. bAbI dataset was created by Facebook towards the goal of automatic text understanding and reasoning. It is a set of 20 QA tasks, each consisting of several context-question-answer triplets. Each task aims to test a unique aspect of reasoning and is, therefore, geared towards testing a specific capability of QA learning models.\n\nAn example from the second task, Two Supporting Facts (QA2), is below:\n\n1 John moved to the bedroom.\n\n2 Mary grabbed the football there.\n\n3 Sandra journeyed to the bedroom.\n\n4 Sandra went back to the hallway.\n\n5 Mary moved to the garden.\n\n6 Mary journeyed to the office.\n\nQuestion: Where is the football?\n\nThere are 1000 training examples for each question. The dataset has synthetically generated stories and hence the vocabulary is very limited and the sentence forms are very constrained. On the one hand, these limitations make bAbI an ideal dataset for learning \u2014 not much data cleansing needs to be done and one can focus on model building. On the other hand, they raise questions about the ability to generalize results on bAbI to QA in a less tightly controlled environment.\n\nDeep learning model: For this problem we use an end to end memory network which was designed by Facebook AI Research.\n\nLets first discuss why a recurrent neural network with LSTM cells won\u2019t work well for this problem.\n\n2. We need a long term memory if we want to network to \u201cmemorize\u201d a book or a video and LSTM cells practically don\u2019t work for long term memory problems\n\nThe end to end memory networks use an external memory to solve this problem. And it can perform multiple look ups(hops) in the data to solve for out of order data problem. What does this mean in simple terms?\n\nThis model has two modules \u2014 a memory module and a controller module. The memory module has the story we want to tell written as vectors. Since computers cannot understand words, we need to convert words into numbers/vectors. Instead of just converting every word to a number, we use a technique called embedding which gives words with similar meanings similar vectors. For example since the words \u201cpick up\u201d or \u201clift\u201d have similar meaning they will have similar vectors and it helps the machine better understand the context between words. Tensorflow embeddings functionality can be used for generating word to vectors.\n\nThe controller module has the question that is being asked written as a vector. See example below:\n\nWe then do a dot product of the controller function and the memory vectors. And after that we do a softmax which scales the total probability to 1. And this is passed back to the controller. In the example above, since this neural network takes into account the order of sentences, it gives higher weigh-age to the second sentence. We can think of the resulting probabilities (0.1,0.7, 02 above) as implying which vector is the network paying \u201cattention\u201d to.\n\nThis process can be repeated several times (also called hops) and the controller has the ability to learn and re look at the memory vectors and make a choice again. See below how the network focuses on different vectors during hops by building on what is has learned in the previous hop\n\nFinally the input is passed to a decoder and the vectors and converted to words.\n\nThis is a fairly complicated neural network architecture and is leading edge of technology in this space. Please consult references below if you want to learn more.\n\nOverall it was a great experience for me and I am very happy with the learning I got.\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Deep Learning Nano Degree for providing me the motivation to explore this field in detail"
    },
    {
        "url": "https://towardsdatascience.com/tracking-pedestrians-for-self-driving-cars-ccf588acd170?source=user_profile---------13----------------",
        "title": "Tracking pedestrians for self driving cars \u2013",
        "text": "A self driving car needs a map of the world around it as it drives. It must be able to track pedestrians, cars, bikes and other moving objects on the road continuously. In this article I will talk through a technique called Extended Kalman Filter which is being used by Google self driving car to track moving objects on the road.\n\nBelow is a video of a car tracking a pedestrian in a simulator. Lidar measurements are red circles, radar measurements are blue circles and estimation markers are green triangles. It is interesting to see how good the accuracy of LIDAR is compared to RADAR. With some playing around the accuracy of Kalman Filter can be improved to a Root Mean Square Error of 0.09.\n\nLink to my GitHub with the full code for implementing the above video in C++.\n\nBefore I go into Kalman Filter, I want to touch on the three main types of sensors used in a car \u2014 Camera (front and rear), LIDAR and RADAR.\n\nLIDAR uses LASER for measurement and generates a point cloud of the world around it providing the car with fairly accurate position x and position y values. It is able to detect objects in the vicinity of the car (20\u201340m) with very high accuracy. However LIDAR is not very accurate in poor weather conditions or if the sensor gets dirty. A LIDAR cloud looks like:\n\nThe RADAR on the other hand is less accurate but is able to provide an estimate of the both position and velocity of an object. The velocity is estimated using the Doppler effect as seen in the image below. RADAR is able to detect objects up to 200 m from the car. It is also less impervious to weather conditions.\n\nA Kalman Filter is an algorithm that can be used to track the position and velocity of a moving pedestrian over time and also measure the uncertainty associated with them. It has two main steps \u2014 A prediction step which predicts where the pedestrian is likely to be at the next time step assuming they are moving with a constant velocity and an update step which uses sensor data (LIDAR and RADAR) to update our estimate. And these two steps repeat endlessly.\n\nLet\u2019s go through this in a bit more detail:\n\nFirst we designate the state of the pedestrian as a 4 dimension vector denoting- position_x , position_y, velocity_x, velocity_y. Here are the main steps in a Kalman Filter:\n\nSimilarly we can update our y position and velocity estimate\n\n3. Measurement Update Step \u2014 Now we read the measurements from RADAR and LIDAR and use that to update our estimate of pedestrian\u2019s state calculated in step 2. The new state after the measurement step is used for the next prediction step.\n\nThis process is repeated continuously to update the current state as shown in the diagram below:\n\nOne other thing to note is that in the Kalman filter, the pedestrian's state is estimated as a Gaussian (bell curve) with a mean and co variance. Here is how a 1D and a 2D Guassian looks like:\n\nHere is an interesting question? \u2014 Why do we need to do a prediction step if we have a sensor measurement? Wouldn\u2019t it be best to just update the state with the sensor (LIDAR/RADAR) measurement? Those should be pretty accurate. Right?\n\nNot really! Measurements can also have uncertainty. The manufacturer for the equipment provides information on what the noise of measurement would be for their sensor. Also local environment condition like rain/fog can make sensors less accurate.\n\nIt is a good practice to not trust the measurement blindly. By combining information, the Gaussian for prediction and measurement get multiplied and when that happens, the most amazing thing happens, the resulting uncertainty is lowered as shown below.\n\nThe Kalman Filter equations are a bit involved and you can check them on Wikipedia\n\nI also built an Unscented Kalman Filter which is able to accommodate a non-linear motion and is more accurate in predicting the state of a pedestrian. My Github also has the Unscented Kalman Filter built in C++.\n\nOverall building my first Kalman Filter and tracking objects with it was a great experience and I am very happy with the outcome.\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. Most of the code used by me was suggested in classroom lectures."
    },
    {
        "url": "https://becominghuman.ai/teaching-a-car-to-drive-itself-8c612604fdc5?source=user_profile---------14----------------",
        "title": "Cloning Human Driving in a Simulator \u2013",
        "text": "This is one of the most interesting projects I have ever done! I had to teach a car to drive autonomously in a simulated environment.\n\nThe simulated environment provided by Udacity was very similar to a video game. See image below.\n\nThe car could be controlled around the track using keyboard, mouse or joystick. As the car drove in the simulator, the software saved screenshots from the simulator and the steering angle/speed at those instances. Using this information, I had to train a car to drive autonomously in the simulator. Boy! was this challenging\n\nHere is a video of the car driving itself in the simulator.\n\nLink to my GitHub with the full code in Python.\n\nHere is how we do this:\n\nThe training data that I used was from driving the car as close to the center of the track for 2\u20133 laps. This data was provided by Udacity and was pretty good quality meaning relatively smooth driving in the simulator. I also collected a little bit of data by myself around 1 turn where my model was constantly under performing. Here are some characteristics of the training data:\n\nBelow are a few samples of images from the dataset:\n\nThe dataset I chose had several biases that resulted in poor performance such as:\n\nTo address these challenges, I performed a lot of pre-processing on the dataset\n\n1. Upsampled left and right turns (model.py lines 56\u201371): As shown in the image above, the dataset contained only a little bit of non-zero steering data, so I upsampled left and right turns by doubling the sample to improve the ability to predict turns.\n\n2. Downsampled near zero steering (model.py lines 73\u201376): I downsampled near zero steering by discarding 10% of the data. This parameter was tuned.\n\nWhile upsampling, I randomly changed the recorded steering b.w (-0.01 and 0.01) to teach the model that small variations in steering were okay.\n\n3. Included left and right images (model.py lines 159\u2013180): To enable recovery I used data from all 3 cameras. As suggested in the lectures I applied a steering correction factor of 0.20. This parameter was tuned. Furthermore I randomly chose only one of the images (center, left or right) for training in an epoch.\n\n4. Flipping images (model.py lines 182\u2013188): The track has a left turn bias. To correct for that the images, were randomly flipped and the steering was reversed. I got best performance by randomly flipping 80% of the images.\n\n5. Randomly change brightness (model.py lines 190\u2013202): To generate more data and reduce overfitting as well as to allow the model to generalize, I randomly changed brightness by updating the v channel of hsv.\n\n6. Random shifts in X direction(model.py lines 205\u2013219): This was done to simulate the effect of car being at different positions on the road, and add an offset corresponding to the shift to the steering angle.\n\n7. Crop images(model.py line 278): Within the Keras model, a cropping layer was added to remove 60 pixels from the top (correspoding to the tree line and horizon) and 20 pixels from the bottom to remove the car\u2019s hood.\n\n8. Resizing of images (model.py line 281): Within the Keras model, a lambda layer was used to resize images to 64x64. This was found to not adversely affect model performance while significantly reducing the number of neurons required in the model.\n\n9. Normalizing data (model.py line 283): Within the Keras model, a lambda layer was used to normalize inputs b/w -0.5 and 0.5\n\n3. Use of data generators for training and validation\n\nIn order to reduce the amount of data to be stored in memory and to speed up processing, data generators were used for training and validation.\n\nThe training generator created a sample of 20,000 images by selecting one of left, right or center images and performing several augmentations on it as discussed in section 2.\n\nThe validation generator created a sample of 2000 images by only selecting the center image without any augmentations. Validation data was left untouched so that it closely resembled the data that would be generated in the simulator when running the model in autonomous mode.\n\nA significant amount of time was spent on exploring several different neural network archictectures. The two that I looked into was the Nvidia architecture suggested in their paper and the comma.ai steering model. Both these sources are in the references at the end.\n\nI found that the Nvidia model was very complicated and it had long training time and at also led to severe overfitting and bad performance on my data.\n\nComma.ai model had better performance but still couldn\u2019t help me navigate turns in the simulator.\n\nIn the end I stuck with a very simple neural network with 3 convolution layers and 2 fully connected layers. The simplicity of this model avoided overfitting on my data and led to beautiful recovery. The car has a strong preference for driving in the center of the lane.\n\nBelow is the detailed architecture of my model (model.py lines 302\u2013331):\n\nThe model uses a cropping, resize and a normalization layer as described in the previous section. The first layer is a convolution layer of kernel size 1x1 and a depth of 3 and the goal of this layer is so the model can figure out the best color space. Following this the model uses 3 convolution layers each followed by RELU activation and a maxpool layer of size (2x2). The first convolution layer has a kernel size of 3x3, stride of 2x2 and a depth of 32. The second convolution layer has a kernel size of 3x3, stride of 2x2 and a depth of 64. The third convolution layer has a kernel size of 3x3, stride of 1x1 and a depth of 128.\n\nAfter this the output is flattened. Dropout of 50% is applied and then there are 2 dense layers of 128 neurons. The final layer is an output layer of 1 neuron. All the layers are followed by RELU activation to introduce non-linearity.\n\nWithin the neural network the following parameters were tuned:\n\n3. No. Of epoches \u2014 I found a value b/w 5\u20138 worked best. All the intermediate models were saved using Keras checkpoint (model.py lines 329\u2013333) and tested in the simulator\n\n4. Training samples per epoch \u2014 I found 20k to be the optimal number\n\nI found that validation loss was not a very good indicator of the quality of the model and the true test was performance in the simulator. However models with very high validation loss performed poorly. But within different epochs, models with higher validation loss could have better performance.\n\nThis was a very challenging and time consuming assignment. However I learned a ton and finally being able to drive the car autonomously was a great achievement. Here are a few things that made this assignment challenging:\n\n1. Generating recovery data in the simulator was challenging. Driving too much to the side and then recovering added a lot of fluctuation and adversely affected the performance of the model. The key was to do \u201cgentle recovery\u201d but this was hard to do in practice\n\n2. Validation loss was not a very good indicator of model performance and testing every optimization in the simulator was very time consuming.\n\nOverall it was a great experience and I am very happy with the outcome.\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. Most of the code used by me was suggested in classroom lectures."
    },
    {
        "url": "https://towardsdatascience.com/automatic-vehicle-detection-for-self-driving-cars-8d98c086b161?source=user_profile---------15----------------",
        "title": "Automatic Vehicle Detection for Self Driving Cars \u2013",
        "text": "How do we teach self driving cars to \u201csee\u201d other cars on the road and track them as they move? Can this be done using just a camera which is much cheaper than RADAR and LIDAR?\n\nYes! See the video below.\n\nThis is being done by using computer vision techniques and the Support Vector Machines (SVM) classifier. The same can be done using Deep Learning as well however I prefer computer vision since it is more straightforward in contrast to deep learning which can be a black box.\n\nLink to my GitHub with the full code in Python.\n\nHere is how we do this:\n\nHOG (Histogram of gradient descents) is a powerful computer vision technique to identify the shape of an object using the direction of gradient along its edges. We can implement it using skimage.hog() function. The key parameters are \u2018orientations\u2019, \u2018pixels_per_cell\u2019 and \u2018cells_per_block\u2019. Orientations is the number of gradient directions. The pixels_per_cell parameter specifies the cell size over which each gradient histogram is computed. The cells_per_block parameter specifies the local area over which the histogram counts in a given cell will be normalized. To get a feel for the affect of pixels_per_cell and cells_per_block, I looked at hog images with different settings for pixels per cell and cells per block.\n\nLow no. of pixels per call and high cells per block (last image above) has the most HOG features making it fairly easy to detect a car. However it would also mean slow computation\n\nOnce we have extracted the HOG features from a car, we can train a SVM classifier to learn between car and not car images. For this exercise Udacity provided a data set of 2826 cars and 8968 not car images. The not car images is anything visualized by the front facing camera which is not a car.\n\nTo choose the optimal HOG parameters for the SVM, I tried various combinations of parameters and observed performance in the video. The best choice was:\n\n3. Implement sliding windows to detect presence of car across the image\n\nTo detect car across an image, we define windows of different sizes and slide them across the image. At each point, we extract HOG features and pass them through our classifier to predict presence of car in that window. If car is detected, we save the window location. Below is an example of searching with windows of different sizes.\n\nHere are some examples of running the above steps on a few test images. As you can see there are multiple detection and false positives. To smoothen out multiple detection and to remove false positives, we can create a heat map of all detection and threshold it to remove false positives.\n\nAlmost there! As you can see that the above technique is prone to generating false positives and multiple bounding boxes. To solve for this, we can combine detection across multiple frames of window (~20) and threshold the heat map (~22 detection) to remove false positive\u201cnoise\u201d that occurs in a few frames but not consistently.\n\nThe result is a fairly robust detection of other vehicles on the road. This technique can also be extended to detection of pedestrians on the road.\n\nThe detection of lane lines is covered in a separate blog post by me.\n\nHowever this technique has a few shortcomings. Firstly, I am not sure this model would perform well when it is a heavy traffic situations when there are multiple vehicles. You need something with near perfect accuracy to avoid bumping into other cars or to ensure there are no crashes on a crossing. More importantly, the model was slow to run. It took 6\u20137 minutes to process 1 minute of video. I am not sure this model would work in a real life situation with cars and pedestrians on the road.\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. Most of the code used by me was suggested in classroom lectures.\n\nPython OpenCV library for making it so easy to apply computer vision techniques"
    },
    {
        "url": "https://towardsdatascience.com/https-medium-com-priya-dwivedi-automatic-lane-detection-for-self-driving-cars-4f8b3dc0fb65?source=user_profile---------16----------------",
        "title": "Automatic Lane Detection for Self Driving Cars \u2013",
        "text": "Can we use the front camera in a car to automatically detect lane lines?\n\nIt turns out we can do it very well! Here is a video where lane lines are being \u201cdrawn\u201d as the car drives. Also you can see the radius of curvature is being calculated to help the car steer.\n\nWhy do we want to do this? Because it is cheap to equip cars with a front facing camera. Much cheaper than RADAR or LIDAR. And self driving cars are coming soon!\n\nLink to my GitHub with the full code in Python.\n\nHere is how we do this. Once we get a camera image from the front facing camera of self driving car, we make several \u201cmodifications\u201d to it. The steps I followed are detailed below:\n\nImage distortion occurs when a camera looks at 3D objects in the real world and transforms them into a 2D image. This transformation isn\u2019t always perfect and distortion can result in a change in apparent size, shape or position of an object. So we need to correct this distortion to give the camera an accurate view of the image. This is done by computing a camera calibration matrix by taking several chessboard pictures of a camera and using cv2.calibrateCamera() function.\n\nSee example below of a distortion corrected image. Please note that the correction is very small in normal lenses and the difference isn\u2019t visible much to the naked eye\n\nNow that we have the undistorted image, we can start our analysis. We need to explore different schemes so that we can clearly detect the object of interest on the road, in this case lane lines while ignoring the rest. I did this in two ways:\n\nThe gradient of an image can be used to identify sharp changes in color in a black and white image. It is a very useful technique to detect edges in an image. For the image of a road, we usually have a lane line in either yellow or white on a black road and so x-gradient can be very useful. Example below of an image and its gradient in x-direction computed using cv2.sobel()\n\nHow about using different color spaces? HSV (Hue, Saturation and Value) color space can be very useful in isolating the yellow and line white lines because it isolates color (hue), amount of color (saturation) and brightness (value). We can use the S color channel in the image to \u201csee\u201d the lane lines as in the image below:\n\nNow we combine the sobel x-gradient and S color channel image to obtain the final binary image in which lane lines are much more contrasted as compared to the background. See image below:\n\nAfter the thresholding operation, we perform a perspective transform to change the image to bird\u2019s eye view. This is done because from this top view we can identify the curvature of the lane and decide how to steer the car. To perform the perspective transform, I identified 4 source points that form a trapezoid on the image and 4 destination points such that lane lines are parallel to each other after the transformation. The destination points were chosen by trial and error but once chosen works well for all images and the video since the camera is mounted in a fixed position. cv2.getPerspective and cv2.warpPerspective can be used to perform this. See how clearly the curvature of the lane lines is visible in this view. Isn\u2019t it amazing!\n\nIn order to better estimate where the lane is, we use a histogram of the bottom half of image to identify potential left and right lane markings.I modified this function to narrow down the area in which left and right lanes can exist so that highway lane separators or any other noise doesn\u2019t get identified as a lane. Once the initial left and right lane bottom points are identified, we use the numpy polyfit function to find the best second order polynomial to represent the lanes, as in image below.\n\n5. Plot result back down onto tho road such that the lane area is identified clearly\n\nAlmost there! Once we have lane lines identified and fit a polynomial to them, we can again use cv2.warpPerspective to warp lane lines back onto original image. We also do a weightedadd to show the lane lines on the undistorted image. Here is how this looks.\n\nThats all! To do this for a video we use the Moviepy library to reach each frame and run all the above steps on it. For some frames, we may fail to properly identify lane lines. We can detect that by a few simple checks like are the lane lines identified separated by a logical width, are the lane lines in birds eye view parallel to each other etc? If the checks fail, then predicted lane lines are discarded and last good left and right lane values are used. This works surprisingly well.\n\nWhile this logic will work well for most situations, it is not perfect. It can fail when camera image is poor quality \u2014 as in case of poor lighting or if lane lines are not visible \u2014 presence of snow or debris on the road. Also if the roads are wavy or S shaped we may not be able to fit a second order polynomial to it.\n\nPS: I live in Toronto and I am looking to switch career into deep learning. If you like my post and can connect me to anyone, I will be grateful :). My email is priya.toronto3@gmail.com\n\nUdacity Self Driving Cars Nano Degree \u2014 I thank Udacity for giving me the chance to be part of their new Self Driving Car program. It has been a very interesting journey. Most of the code used by me was suggested in classroom lectures.\n\nPython OpenCV library for making it so easy to apply computer vision techniques"
    }
]