[
    {
        "url": "https://medium.com/@gabogarza/exoplanet-hunting-with-machine-learning-and-kepler-data-recall-100-155e1ddeaa95?source=user_profile---------1----------------",
        "title": "Exoplanet Hunting with Machine Learning and Kepler Data -> Recall 100%",
        "text": "In this post, our goal is to build a model that can predict the existence of an exoplanet (i.e. a planet that orbits a distant star system) given the light intensity readings from that star over time. The dataset we\u2019ll be using comes from NASA\u2019s Kepler telescope currently in space. I\u2019ll be taking you through the steps I followed to get from a low performing model to a high performing model.\n\nThe Kepler telescope was launched by NASA in 2009, its mission is to discover Earth like planets orbiting other stars outside our solar system. Kaggle published a dataset containing clean observations/readings from Kepler in a challenge to find exoplanets (planets outside our solar system) orbiting other stars.\n\nHere\u2019s how it works. Kepler observes many thousands of stars and records the light intensity (flux) that the stars emit. When a planet orbits a star, it slightly changes/lowers that light intensity. Over time, you can see a regular dimming of the star\u2019s light (e.g. t=2 in the image below), and this is evidence that there might be a planet orbiting the star (candidate system). Further light studies can confirm the existence of an exoplanet on the candidate system.\n\nThe Kaggle / Kepler dataset is composed of a training set and a test set, with labels 1 for confirmed non-exoplanet and 2 for confirmed exoplanet.\n\nAs an example, here is the light flux for an example with that is confirmed non-exoplanet (left) and an example that is confirmed exoplanet (right):\n\nDue to the highly imbalanced dataset we are working with, we\u2019ll be using Recall as our primary success metric and Precision as our secondary success metric (accuracy would be a bad metric because predicting non-exoplanet all across would get you very high accuracy).\n\nFirst off, we have too few confirmed exoplanet examples in our data. There are several techniques that help overcome a highly imbalanced dataset and synthesize or create new examples. One we\u2019ll use here is an algorithm called SMOTE (Synthetic Minority Over-sampling Technique). Instead of creating copies of examples, the algorithm essentially creates new examples by slightly modifying existing ones. This way we can have a balance of positive vs. negative examples in our training dataset.\n\nAnytime you are dealing with an intensity value over time, you can think of it as a signal or a mix of different frequencies jumbled up together. One idea to improve our model would be to ask ourselves, is there any difference between the frequencies that compose confirmed exoplanet light intensity signals vs. the frequencies that compose non-exoplanet signals. Fortunately, we can use the Fourier Transform to decompose these signals into its original frequencies, giving our model more rich/discriminative features.\n\nOur primary goal will be to maximize Recall on the dev set, but we\u2019ll also maximize Precision as a secondary goal. For our model we\u2019ll be using a Support Vector Machines model. In my testing, this model performed better than others I tested including several neural network architectures.\n\nThe graphs below are of examples index 150 (non-exoplanet) and index 4 (exoplanet).\n\nWithout any processing, we do well on the training set but our model doesn\u2019t generalize well to the dev set. We can evidently diagnose our overfitting or high variance by looking at the big difference in train vs dev set errors. Since our model is already very simple, we\u2019ll rely on feature engineering to make improvements.\n\nAs a first step we will use the SMOTE technique to balance our training examples with the same amount of negative and positive examples. As you can see from the train confusion matrix below, we\u2019ve increased our positive examples to be 5050, the same amount as negative examples. This will hopefully allow the model to better generalize to examples it hasn\u2019t seen before. Notice how we are keeping the dev dataset untouched. This is important, you always want to test on real examples that you would expect ones you release the model to be used in the real world.\n\nAfter normalizing, standardizing, and applying a Gaussian filter to our data, we can see an big improvement in recall and precision.\n\nThis is where it gets more interesting. By applying the Fourier Transform, we\u2019re essentially converting an intensity over time function to an intensity by frequency function. From looking at the chart, it seems that (at least for this particular example) there are some clear frequency spikes for the confirmed Exoplanet, giving our model richer and more discriminative features to train on.\n\nI also tried the model without performing the SMOTE technique. Interestingly, it looks like, in this case, we can improve the precision of the model by not using SMOTE. I would be eager to test with/without SMOTE over a bigger dataset before coming to a conclusion on whether or not the technique should be used for this model.\n\nIt\u2019s amazing we are able to gather light from distant stars, study this light that has been traveling for thousands of years, and make conclusions about what potential worlds these stars might harbor.\n\nAchieving a Recall of 1.0 and Precision of 0.55 on the dev set was not easy and required a lot of iteration on data pre processing and models.\n\nThis was one of the most fun projects/datasets that I\u2019ve played around with and learned a lot in the process. As a next step, I\u2019d be excited to try this model on new unexamined Kepler data to see if it can find new Exoplanets.\n\nFinally, it\u2019d also be very interesting if NASA could provide datasets which include confirmed Exoplanets vs. Exoplanets in the Goldilocks Zone!\n\nAny comments / suggestions are welcome below ;)"
    },
    {
        "url": "https://medium.com/@gabogarza/deep-reinforcement-learning-policy-gradients-8f6df70404e6?source=user_profile---------2----------------",
        "title": "Deep Reinforcement Learning \u2014 Policy Gradients \u2014 Lunar Lander!",
        "text": "In this post we\u2019ll build a Reinforcement Learning model using a Policy Gradient Network. We\u2019ll Tensorflow to build our model and use Open AI\u2019s Gym to measure our performance against the Lunar Lander game. Full source code here.\n\nI\u2019ve been amazed by the power of deep reinforcement learning algorithms. There are several powerful methods such as Deep Q Learning, popularized by Deep Mind with their Atari Pong player in 2015, and in this post we\u2019ll go through my favorite RL method, Policy Gradients.\n\nThere are three main branches in machine learning: Supervised Learning (learning from labeled data), Unsupervised Learning (learning patterns from unlabeled data), and Reinforcement Learning (discovering data/labels through exploration and a reward signal). Reinforcement Learning is a lot like supervised learning, except not only do you start without labels, but without data too. This is why I believe RL is so important, as it allows us start learning from zero experience, much like we humans do as we\u2019re born with zero experience. The human reward function has been tuned for millions of years to optimize for survival, that is, for every action we take in the world we get a positive reward if its expected value increases our chance of survival or a negative reward if it lowers our chance of survival (think pleasure when you eat or pain when you fall down). As we go along in life we learn to take actions that are more likely to lead to positive rewards. If you are interested in RL I highly recommend checking out David Silver\u2019s course and Fei-Fei Li\u2019s Stanford course.\n\nIn RL, an agent interacts with it\u2019s environment through a sequence of events:\n\nOpenAI\u2019s Gym gives us a great way to train and test out our RL models through games, which are great for RL, as we have clear actions (left, right, etc.), states (could be position of players or pixels of screen, etc.), and rewards (points).\n\nOur objective is to learn a policy or model that will maximize expected rewards. Concretely, we need to train a neural network (the policy) to predict the action, given a state, that will maximize future rewards!\n\nWe\u2019ll use one of my favorite OpenAI Gym games, Lunar Lander, to test our model. The goal, as you can imagine, is to land on the moon! There are four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine. The state is the coordinates and position of the lander. The reward is a combination of how close the lander is to the landing pad and how close it is to zero speed, basically the closer it is to landing the higher the reward. There are other things that affect the reward such as, firing the main engine deducts points on every frame, moving away from the landing pad deducts points, crashing deducts points, etc. This reward function is determined by the Lunar Lander environment. The game or episode ends when the lander lands, crashes, or flies off away from the screen.\n\nTo approximate our policy, we\u2019ll use a 3 layer neural network with 10 units in each of the hidden layers and 4 units in the output layer:\n\nThe input vector is the state that we get from the Gym environment. These could be pixels or any kind of state such as coordinates and distances. The lunar Lander game gives us a vector of dimensions (8,1) for our state, and we\u2019ll map those to the probability of taking a certain action.\n\nFor our loss function we\u2019ll use a softmax cross entropy, which gives us the negative log probability of our actions as an output. We\u2019ll then multiply this by our reward (reward guided loss), so we can update our parameters in a way that encourages actions that lead to high rewards and discourages actions that lead to low rewards. We\u2019ll use the Adam Optimizer to train our model. Our loss will look something like this:\n\nDiscounting future rewards and normalizing them (subtracting mean then dividing by standard deviation) helps to reduce variance. There are ways we could improve our loss function, by crafting a better Advantage Function. For example, add a baseline to our rewards or using methods such as the Actor Critic, which combines Policy Gradients with Deep Q-learning, would help to reduce variance. In our case, our Advantage Function is simply our discounted and normalized rewards.\n\nWhere is the training data coming from? How do we get the and in the code above? Initially, we don\u2019t have any data. for every game we play (episode) we will be saving the state, action, and reward for every step in the sequence, you can think of each of these steps as a training example. This will serve as our training data. Our input vector is the state. Our logits are the outputs (before softmax) of the network and our labels are the actions we took.\n\nWe\u2019ll play 5,000 episodes (games) total and for each episode here are the step\u2019s we\u2019ll follow:\n\nInitially the agent is not very good at landing, it\u2019s basically taking random actions:\n\nAfter several hundred episodes the agent starts to learn how to fly!\n\nEventually after about 3 thousand episodes, the agent learns how to land in the landing zone!"
    },
    {
        "url": "https://medium.com/@gabogarza/1-mining-rig-free-food-forever-6c9bad835c02?source=user_profile---------3----------------",
        "title": "1 mining rig == universal income ? \u2013 Gabriel Garza \u2013",
        "text": "Imagine if it was as common as having a heater (hint hint) in your home.\n\nCurrently my 6-GPU (AMD RX580s) Ethereum rig is hashing away at 155 mh/s, yielding around 0.8 ether/month. At the current price of $800 USD per ether, that\u2019s about $20 per day, which would get you free food for that day. I invested ~$3,500 USD in all the computer parts and put it together in a couple of hours (in retrospect could\u2019ve spent about $500 less and get the same hash rate, knowing what I know now).\n\nIf we believe we are headed to a future where cryptocurrencies play any kind of important role in the economy, we\u2019re gonna need a lot of miners.\n\nMining could end up being some sort of universal basic income.\n\nVery excited to see where this all goes. What do you think is the future of mining? Please feel free to leave your thoughts in the comments."
    },
    {
        "url": "https://medium.com/@gabogarza/simple-genetic-algorithm-6d6aafcc310a?source=user_profile---------4----------------",
        "title": "Simple Genetic Algorithm \u2013 Gabriel Garza \u2013",
        "text": "Whether it\u2019s building maple seed shaped drones or inventing Velcro based on burdock burrs, nature has always been a great inspration for humans.\n\nIt goes without saying that optimization algorightms optimize for some kind of goal or target metric. Evolution is no different. Natural selection in the real world optimizes for survival on Earth. In fact, each and every life form on Earth is a solution generated by evolution\u2019s algorithm, which evolves a population of individuals over generations, optimizing for survival. Give it enough time (4.5 billion years), and you get humans, although we have yet to pass certain extinction tests that have wiped out other species in the past, but I\u2019m optimistic.\n\nIn this post we\u2019ll lossely draw inspiration from evolution to build a simple genetic algorithm.\n\nOur two main classes will be an Individual and a Population, and populations are made up of individuals. Our individuals will be made up of 10 initially random integers between 0 and 100. The goal will be to get the sum of those 10 numbers to equal 900, and that\u2019ll be the measure of how fit an is.\n\nWe\u2019ll initialize a with a size of 100 individuals, where each will be randomly initalized. Then we\u2019ll evolve the to find an optimal solution.\n\nHere are the basic steps for the evolving part:\n\nWhen we hit an average population fitness of 0 (or close to it) we\u2019ll stop evolving. Normally it\u2019s more intuitive to have a higher fitness be better than a lower fitness, in our case our convention is that lower fitness is better.\n\nThis of course is a super simplified version of a more complex program we could build. For example, we could have different species competing with each other, and we can have the chance of mutation be property of a species, etc. Having just the right mutation chance could make the difference between one species winning over another.\n\nOnce we have the main code nailed down, it\u2019s fun to play around with the parameters and look at how long it takes to achieve an optimal solution:\n\nTo run this code, do in your terminal:\n\nWe can use these kinds of genetic algorithms to build really cool things, such as evolving a neural network\u2019s hyperparameters leading to a very efficient search of optimal parameters.\n\nThanks for reading and let me know if you have any feedback :)"
    }
]