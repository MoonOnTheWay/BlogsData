[
    {
        "url": "https://towardsdatascience.com/deep-learning-best-practices-1-weight-initialization-14e5c0295b94?source=user_profile---------1----------------",
        "title": "Deep Learning Best Practices (1) \u2014 Weight Initialization",
        "text": "Here\u2019s a quick look at steps 2 , 3 and 4 for a network with 2 layers, i.e. one hidden layer. (Note that I haven\u2019t added the bias terms here for simplicity):\n\nConsider an L layer neural network, which has L-1 hidden layers and 1 output layer. The parameters (weights and biases) of the layer l are represented as\n\nNote \u2014 Whenever I refer to layers of a neural network, it implies the layers of a simple neural network, i.e. the fully connected layers. Of course some of the methods I talk about apply to convolutional and recurrent neural networks as well. In this blog I am going to talk about the issues related to initialization of weight matrices and ways to mitigate them. Before that, let\u2019s just cover some basics and notations that we will be using going forward.\n\nWhile writing this blog, the assumption is that you have a basic idea of how neural networks are trained. An understanding of weights, biases, hidden layers, activations and activation functions will make the content clearer. I would recommend this course if you wish to build a basic foundation of deep learning.\n\nAs a beginner at deep learning, one of the things I realized is that there isn\u2019t much online documentation that covers all the deep learning tricks in one place. There are lots of small best practices, ranging from simple tricks like initializing weights, regularization to slightly complex techniques like cyclic learning rates that can make training and debugging neural nets easier and efficient. This inspired me to write this series of blogs where I will cover as many nuances as I can to make implementing deep learning simpler for you.\n\nOne of the starting points to take care of while building your network is to initialize your weight matrix correctly. Let us consider 2 scenarios that can cause issues while training the model:\n\nLet\u2019s just put it out there \u2014 this makes your model equivalent to a linear model. When you set all weight to 0, the derivative with respect to loss function is the same for every w in W^l, thus, all the weights have the same values in the subsequent iteration. This makes the hidden units symmetric and continues for all the n iterations you run. Thus setting weights to zero makes your network no better than a linear model. It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different.\n\nInitializing weights randomly, following standard normal distribution ( in Python) while working with a (deep) network can potentially lead to 2 issues \u2014 vanishing gradients or exploding gradients.\n\na) Vanishing gradients \u2014 In case of deep networks, for any activation function, abs(dW) will get smaller and smaller as we go backwards with every layer during back propagation. The earlier layers are the slowest to train in such a case.\n\nMore specifically, in case of sigmoid(z) and tanh(z), if your weights are large, then the gradient will be vanishingly small, effectively preventing the weights from changing their value. This is because abs(dW) will increase very slightly or possibly get smaller and smaller every iteration. With RELU(z) vanishing gradients are generally not a problem as the gradient is 0 for negative (and zero) inputs and 1 for positive inputs.\n\nb) Exploding gradients \u2014 This is the exact opposite of vanishing gradients. Consider you have non-negative and large weights and small activations A (as can be the case for sigmoid(z)). When these weights are multiplied along the layers, they cause a large change in the cost. Thus, the gradients are also going to be large. This means that the changes in W, by will be in huge steps, the downward moment will increase.\n\nAnother impact of exploding gradients is that huge values of the gradients may cause number overflow resulting in incorrect computations or introductions of NaN\u2019s. This might also lead to the loss taking the value NaN.\n\n1. Using RELU/ leaky RELU as the activation function, as it is relatively robust to the vanishing/exploding gradient issue (especially for networks that are not too deep). In the case of leaky RELU\u2019s, they never have 0 gradient. Thus they never die and training continues.\n\n2. For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function. Here, instead of drawing from standard normal distribution, we are drawing W from normal distribution with variance k/n, where k depends on the activation function. While these heuristics do not completely solve the exploding/vanishing gradients issue, they help mitigate it to a great extent. The most common are:\n\na) For RELU(z) \u2014 We multiply the randomly generated values of W by:\n\nb) For tanh(z) \u2014 The heuristic is called Xavier initialization. It is similar to the previous one, except that k is 1 instead of 2.\n\nc) Another commonly used heuristic is:\n\nThese serve as good starting points for initialization and mitigate the chances of exploding or vanishing gradients. They set the weights neither too much bigger that 1, nor too much less than 1. So, the gradients do not vanish or explode too quickly. They help avoid slow convergence, also ensuring that we do not keep oscillating off the minima. There exist other variants of the above, where the main objective again is to minimize the variance of the parameters.\n\n3. Gradient Clipping \u2014 This is another way of dealing with the exploding gradient problem. We set a threshold value, and if a chosen function of a gradient is larger than this threshold, we set it to another value. For example, normalize the gradients when the L2 norm exceeds a certain threshold \u2013\n\nAn important point to note is that we have talked about various initializations of W, but not the biases b. This is because the gradients with respect to bias depend only on the linear activation of that layer, and not on the gradients of the deeper layers. Thus there is no diminishing or explosion of gradients for the bias terms. As mentioned earlier, they can be safely initialized to 0.\n\nIn this blog, we\u2019ve covered weight initialization pitfalls and some mitigation techniques. If I have missed any other useful insights related to this topic, I would be happy to learn it from you! In the next blog, I will be talking about regularization methods to reduce overfitting and gradient checking \u2014 a trick to make debugging simpler!"
    },
    {
        "url": "https://towardsdatascience.com/how-to-get-more-likes-on-your-blogs-2-2-f8ef0be21771?source=user_profile---------2----------------",
        "title": "How to get more likes on your blogs (2/2) \u2013",
        "text": "How to get more likes on your blogs (2/2) Estimating the claps you get, the data science way Wondered how you can get your story to trend more?? Is it the title, the images, the quotes or the content that gets you more claps? In this series by Alvira Swalin and me, we have tried to explore the relationship between the features of a blog and the number of claps it gets. Part 1, talks about the feature extraction and preliminary Exploratory Data Analysis (EDA) while in Part 2, we build models to predict the claps a blog can get. The EDA in Part 1 was based on 600 Data Science blogs, but for further analysis I have used ~4000 Medium blogs. For uniformity of content and audience, we\u2019ve scraped these blogs from Data Science, Artificial Intelligence, Technology and Programming categories. Our features include the length of the blog, images/word, number of tags, sentiment score of the title, duration since the blog was published and the number of followers. From preliminary EDA, we can see a positive correlation of claps with the reading time and the number of tags. More tags seem to fetch more claps whereas the sentiment of the title does not seem to have much effect on the claps. To investigate this further, we first tried treating this problem as regression first and then classification. As expected, regression did not do a very good job as the range for prediction and variation was huge. Thus, we settled giving this data the classification treatment, and will be further discussing that approach. To see whether our preliminary analysis holds true for a broader range of blogs \u2014 3-label classification to see whether a blog gets low, medium or high claps 20-label classification to get more granular predictions In all, I have included 24 features which comprise features directly extracted as well as those related to the content, author and date. The claps range from 1\u201362,000 with a standard deviation of 2.8k! That\u2019s a huge range to predict for, so to deal with this variation we clip the data at the 90th percentile.\n\nFor this, I have binned the blogs into Low, Medium and High, based on the number of claps. The classifiers I have tried in both the methods are Logistic Regression and Random Forest, because both are interpretable. Random Forest outperformed the latter as it could capture the non-linearity and interactions between features that Logistic Regression could not. After some parameter tuning that I won\u2019t be going into here, we get the following \u2014 Our model is able to predict class 0 and 1 relatively more accurately that class 2. This is due to the imbalance in the distribution of observations across classes in the training data. For converting labels into classes, here I have binned claps based on their distribution. This ensures that the observations are evenly distributed into all the classes (unlike the 3-label approach) and class imbalance is dealt with. Below are a few ranges corresponding to the bins. As in the previous case, we built logistic regression and random forest models on the training data of ~3300 blogs using 5-fold cross validation for tuning the parameters. We have used log loss as the metric here. This model gave a log loss of 2.7 in predicting the bin of the claps. Since our classes are sequential we can compute the Mean Absolute Error as well. For our model we get MAE = 5.16. The predicted bin gives us the range of the claps that blog can get. Let\u2019s take a look at some of the actual and predicted ranges: While some predictions (the ones in red) are completely off the mark, there are some (in orange) that are actually very close but still misclassified. So what are the features which decide if your blog will trend or not? \n\nOn computing the feature importances, we see that the number of followers and the quality of the content are a sure shot indication of more number of claps, other than (of course) the days elapsed since the blog was published. The proportion of images to the length of the blog is another important factor. Surprisingly, the number of words of the title and reading time do not seem to be a very crucial factor which is contradictory to what we have observed in the initial EDA. Going further, let\u2019s take a couple of blogs and try to interpret whether the impact of these factors is positive or negative (and also how close our predictions are). As an example, we take two sample blogs with ~760 and 1800 claps respectively. You can find the first and second blogs here. Let\u2019s first look at their features and predictions:\n\nThe prediction for the first blog is 17 i.e. 754 \u20131000 claps (bang on!) and 14 for the second, i.e. 380\u2013490 claps. This is completely off the actual range of 1400\u20132000 claps. To see why our prediction went so wrong for the second blog, let\u2019s look at that blog again. For this blog, content is king which points towards a shortcoming of our model. Our model could not capture how engaging a blog is in terms of its content and writing style. For this, more analysis needs to be done on the article itself. At the moment, we can only judge blogs based on features like number of followers, images, sentiment, length etc. Other work that can be done on blogs could be \u2014 Analyse the purpose of the blog \u2014 whether it aims to educate, explore a problem, provide a solution to a problem, etc Determine the popularity of a topic based on how many claps it can get As mentioned earlier, more NLP for determining how popular a blog gets For example, we could check if a blog contains current buzzwords (eg. cryptocurrency,) if it provides solutions to a relevant topic etc. I hope you enjoyed reading this! Any ideas, suggestions or comments are most welcome! This really cool blog on writing good blogs by Quincy Larson!"
    },
    {
        "url": "https://towardsdatascience.com/augmentation-for-image-classification-24ffcbc38833?source=user_profile---------3----------------",
        "title": "Augmentation for Image Classification \u2013",
        "text": "One of the issues one comes across while dealing with image data is the inconsistency in images (some are either too big or small, some are rectangular instead of square, etc). Another frequently faced problem is the number of images in the training set which often results in overfitting. To deal with these issues, I outline a technique that uses augmentation transforms \u2014 the images in the training set are transformed so as to increase the ability of the model to recognize different versions of an image. This increases the breadth of information the model has. It now becomes better suited to recognize target objects in images of varied contrast, size, from changed angles and so on.\n\nTo show how augmentation works, we look at the Dogs vs Cats dataset and make use of the deep learning library fast.ai by Jeremy Howard and Rachel Thomas that is built on top of PyTorch. This post is inspired from fast.ai Deep Learning part 1 v2.\n\nTo classify images as either dog or cat, we use resnet34 to train the model (more about ResNet architecture in this awesome blog by Apil Tamang!). We first train the model without data augmentation using learning rate 0.03 and 1 epoch.\n\nWith this we see a validation accuracy of\n\nHere\u2019s a look at the confusion matrix:\n\nThus we see that 26 images \u2014 20 cats and 6 dogs out of 2000 have been misclassified.\n\nTo reduce this misclassification error, we now augment the train data and see if there is an improvement. We can either choose from top-down transformations or side-on transformations. Here\u2019s a quick look at what the types involved:\n\nHere we use a side-on transformation because given that we have pictures of dogs and cats that are taken from the side (as opposed to from the top), they possibly need to be just flipped horizontally, rather than vertically. Here\u2019s a look at 6 random side-on transformations on a cat image:\n\nTop down transformations are not appropriate here due to the nature of the images \u2014 upside down images of a cat or dog are rare!\n\nWhile training this network, the learning rate is kept the same in order to see the difference in accuracy only due to augmentation. When we use augmentation to train the network, for every epoch a new transformation of every image is generated. Thus the model sees the same number of images in every epoch (as many as there are in the original training data), albeit a new version of those images each time. Thus, the range of images the model has seen increases with every epoch.\n\nNetworks such as ResNets are pretrained, i.e., the architecture comes with its set of precomputed weights for every layer except the fully connected ones. While training the model earlier, we used ResNet34\u2019s precomputed weights. But this time, since we use new sets of training images, we set to ensure that the model computes activations from scratch for the new model. Without doing this, we will still be using the precomputed activations that correspond to the original training data, not giving us much improvement in accuracy.\n\nNow we get a validation accuracy of which is an improvement over the previous model.\n\nFrom the confusion matrix above, we see that we have misclassified 22 images this time; a reduction in error as compared to before. Thus, we have increased the prediction power of the model.\n\nWhile augmentation helped give us a better model, prediction accuracy can be further improved by what is called Test Time Augmentation(TTA). To understand why this is needed let us first take a look at some of the misclassified images:\n\nWe see here that a few of the images have been misclassified due to poor contrast, rectangular rather than square images, or because the dog/cat is in a very small portion of the image. Take a look, for example, at the rectangular image of the dog. When the model tries to predict for this image, it sees just the center of the image (cropping by default is center). Thus it cannot predict if the image is of a dog or a cat.\n\nTo mitigate errors such as these we use TTA wherein we predict class for the original test image along with 4 random tranforms of the same image. We then take an average of the predictions to determine which class the image belongs to.\n\nOn using TTA, we now get a validation accuracy of with just 16 misclassified images.\n\nWhile the baseline ResNet34 model fits the data well giving pretty good results, applying augmentation transforms and TTA reduce misclassification errors, improving the accuracy of the model. What we\u2019ve covered here are just basic augmentation techniques like flipping and rotating, but GANs can also be used generate images of different styles as suggested in this paper. This may probably prove to be even more effective.\n\nAside from augmentation on images, it will be interesting to see how augmentation can also be applied to structured data to boost performance, just like it does in case of unstructured data! Let me know your thoughts on this!!"
    }
]