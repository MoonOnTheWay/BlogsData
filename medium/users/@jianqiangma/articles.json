[
    {
        "url": "https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e?source=user_profile---------1----------------",
        "title": "All of Recurrent Neural Networks \u2013 Jianqiang Ma \u2013",
        "text": "\u2014 notes for the Deep Learning book, Chapter 10 Sequence Modeling: Recurrent and Recursive Nets.\n\nMeta info: I\u2019d like to thank the authors of the original book for their great work. For brevity, the figures and text from the original book are used without reference. Also, many thanks to colan and shi for their excellent blog posts on LSTM, from which we use some figures.\n\nRNNs share parameters across different positions/ index of time/ time steps of the sequence, which makes it possible to generalize well to examples of different sequence length. RNN is usually a better alternative to position-independent classifiers and sequential models that treat each position differently.\n\nHow does a RNN share parameters? Each member of the output is produced using the same update rule applied to the previous outputs. Such update rule is often a (same) NN layer, as the \u201cA\u201d in the figure below (fig from colan).\n\nNotation: We refer to RNNs as operating on a sequence that contains vectors x(t) with the time step index t ranging from 1 to \u03c4. Usually, there is also a hidden state vector h(t) for each time step t.\n\nBasic formula of RNN (10.4) is shown below:\n\nIt basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n\nUnfolding maps the left to the right in the figure below (both are computational graphs of a RNN without output o)\n\nwhere the black square indicates that an interaction takes place with a delay of 1 time step, from the state at time t to the state at time t + 1.\n\nUnfolding/parameter sharing is better than using different parameters per position: less parameters to estimate, generalize to various length.\n\nVariation 1 of RNN (basic form): hidden2hidden connections, sequence output. As in Fig 10.3.\n\nThe basic equations that defines the above RNN is shown in (10.6) below (on pp. 385 of the book)\n\nThe total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n\nof y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence as shown in (10.7):\n\nVariation 2 of RNN output2hidden, sequence output. As shown in Fig 10.4, it produces an output at each time step and have recurrent connections only from the output at one time step to the hidden units at the next time step\n\nTeacher forcing (Section 10.2.1, pp 385) can be used to train RNN as in Fig 10.4 (above), where only output2hidden connections exist, i.e hidden2hidden connections are absent.\n\nIn teach forcing, the model is trained to maximize the conditional probability of current output y(t), given both the x sequence so far and the previous output y(t-1), i.e. use the gold-standard output of previous time step in training.\n\nVariation 3 of RNN hidden2hidden, single output. As Fig 10.5 recurrent connections between hidden units, that read an entire sequence and then produce a single output\n\nHow? Use back-propagation through time (BPTT) algorithm on on the unrolled graph. Basically, it is the application of chain-rule on the unrolled graph for parameters of U, V , W, b and c as well as the sequence of nodes indexed by t for x(t), h(t), o(t) and L(t).\n\nHope you find the following derivations elementary\u2026.. If not, reading the book probably does NOT help, either\n\nThe derivations are w.r.t. the basic form of RNN, namely Fig 10.3 and Equation (10.6) . We copy Fig 10.3 again here:\n\nOnce the gradients on the internal nodes of the computational graph are obtained, we can obtain the gradients on the parameter nodes, which have descendents at all the time steps:\n\nNote: We move Section 10.2.3 and Sec 10.2.4, both of which are about graphical model interpretation of RNN, to the end of the notes, as they are not essential for the idea flow, in my opinion\u2026\n\nNote2: One may want to jump to Section 10.7 and read till the end before coming back to 10.3\u201310.6, as those sections are in parallel with Sections 10.7\u201310.13, which coherently centers on the long dependency problem.\n\nIn many applications we want to output a prediction of y (t) which may depend on the whole input sequence. E.g. co-articulation in speech recognition, right neighbors in POS tagging, etc.\n\nBidirectional RNNs combine an RNN that moves forward through time beginning from the start of the sequence with another RNN that moves backward through time beginning from the end of the sequence.\n\nFig. 10.11 (below) illustrates the typical bidirectional RNN, where h(t) and g(t) standing for the (hidden) state of the sub-RNN that moves forward and backward through time, respectively. This allows the output units o(t) to compute a representation that depends on both the past and the future but is most sensitive to the input values around time t\n\nFigure 10.11: Computation of a typical bidirectional recurrent neural network, meant to learn to map input sequences x to target sequences y, with loss L(t) at each step t.\n\nFootnote: This idea can be naturally extended to 2-dimensional input, such as images, by having four RNNs\u2026\n\n(1) an encoder or reader or input RNN processes the input sequence. The encoder emits the context C , usually as a simple function of its final hidden state.\n\n(2) a decoder or writer or output RNN is conditioned on that fixed-length vector to generate the output sequence Y = ( y(1) , . . . , y(ny ) ).\n\nhighlight: the lengths of input and output sequences can vary from each other. Now widely used in machine translation, question answering etc.\n\nTraining: two RNNs are trained jointly to maximize the average of logP(y(1),\u2026,y(ny) |x(1),\u2026,x(nx)) over all the pairs of x and y sequences in the training set.\n\nVariations: If the context C is a vector, then the decoder RNN is simply a vector-to- sequence RNN. As we have seen (in Sec. 10.2.4), there are at least two ways for a vector-to-sequence RNN to receive input. The input can be provided as the initial state of the RNN, or the input can be connected to the hidden units at each time step. These two ways can also be combined.\n\nThe computation in most RNNs can be decomposed into three blocks of parameters and associated transformations:\n\n1. from the input to the hidden state, x(t) \u2192 h(t)\n\n2. from the previous hidden state to the next hidden state, h(t-1) \u2192 h(t)\n\n3. from the hidden state to the output, h(t) \u2192 o(t)\n\nThese transformations are represented as a single layer within a deep MLP in the previous discussed models. However, we can use multiple layers for each of the above transformations, which results in deep recurrent networks.\n\nFig 10.13 (below) shows the resulting deep RNN, if we\n\n(b) introduce deeper architecture for all the 1,2,3 transformations above and\n\n(c) add \u201cskip connections\u201d for RNN that have deep hidden2hidden transformations.\n\nA recursive network has a computational graph that generalizes that of the recurrent network from a chain to a tree.\n\nPro: Compared with a RNN, for a sequence of the same length \u03c4, the depth (measured as the number of compositions of nonlinear operations) can be drastically reduced from \u03c4 to O(log\u03c4).\n\nCon: how to best structure the tree? Balanced binary tree is an optional but not optimal for many data. For natural sentences, one can use a parser to yield the tree structure, but this is both expensive and inaccurate. Thus recursive NN is NOT popular.\n\nThe long-term dependency challenge motivates various solutions such as Echo state network (Section 10.8), leaky units (Sec 10.9) and the infamous LSTM (Sec 10.10), as well as clipping gradient, neural turing machine (Sec 10.11).\n\nRecurrent networks involve the composition of the same function multiple times, once per time step. These compositions can result in extremely nonlinear behavior. But let\u2019s focus on a linear simplification of RNN, where all the non-linearity are removed, for an easier demonstration of why long-term dependency can be problematic.\n\nWithout non-linearity, the recurrent relation for h(t) w.r.t. h(t-1) is now simply matrix multiplication:\n\nIf we recurrently apply this until we reach h(0), we get:\n\nand if W admits an eigen-decomposition\n\nthe recurrence may be simplified further to:\n\nIn other words, the recurrence means that the eigenvalues are raised to the power of t. This means that eigenvalues with magnitude less than one to vanish to zero and eigenvalues with magnitude greater than one to explode. The above analysis shows the essence of the vanishing and exploding gradient problem for RNNs.\n\nComment: the trend of recurrence in matrix multiplication is similar in actual RNN, if we look back at 10.2.2 \u201cComputing the Gradient in a Recurrent Neural Network\u201d.\n\nBengio et al., (1993, 1994) shows that whenever the model is able to represent long term dependencies, the gradient of a long term interaction has exponentially smaller magnitude than the gradient of a short term interaction. It means that it can be time-consuming, if not impossible, to learn long-term dependencies. The following sections are all devoted to solving this problem.\n\nPractical tips: The maximum sequences length that SGD-trained traditional RNN can handle is only 10 ~ 20.\n\nNote: This approach seems to be non-salient in the literature, so knowing the concept is probably enough. The techniques are only explained at an abstract level in the book, anyway.\n\nBasic Idea: Since the recurrence causes all the vanishing/exploding problems, we can set the recurrent weights such that the recurrent hidden units do a good job of capturing the history of past inputs (thus \u201cecho\u201d), and only learn the output weights.\n\nSpecifics: The original idea was to make the eigenvalues of the Jacobian of the state-to-\n\nstate transition function be close to 1. But that is under the assumption of no non-linearity. So The modern strategy is simply to fix the weights to have some spectral radius such as 3, where information is carried forward through time but does not explode due to the stabilizing effect of saturating nonlinearities like tanh.\n\nA common idea shared by various methods in the following sections: design a model that operates on both fine time scale (handle small details) and coarse time scale (transfer information through long-time).\n\nIdea: each hidden state u(t) is now a \u201csummary of history\u201d, which is set to memorize both a coarse-grained immediate past summary of history u(t-1) and some \u201cnew stuff\u201d of present time v(t):\n\nwhere alpha is a parameter. Then this introduces a linear self-connections from u(t-1) \u2192 u(t), with a weight of alpha.\n\nIn this case, the alpha substitutes the matrix W\u2019 of the plain RNN (in the analysis of Sec 10.7 ). So if alpha ends up near 1, the several multiplications will not leads to zero or exploded number.\n\nNote: The notation seems to suggest alpha is a scalar, but it looks it will also work if alpha is a vector and the multiplications there is element-wise, which will resemble gate recurrent unit (GRU) as in the coming Section 10.10.\n\nThe most effective sequence models used in practice are called gated RNNs. These include the long short-term memory (LSTM) and networks based on the gated recurrent unit (GRU).\n\nKey insight of gated RNN: Gated RNNs allow the neural network to forget the old state, besides accumulating info (as leaky units can also do).\n\nLong short-term memory (LSTM) model (Hochreiter and Schmidhuber, 1997) weight the self-loop in RNN conditioned on the context, rather than a fixed way of self-looping.\n\nRecap of plain/vanilla RNN: The current hidden state h(t) of the vanilla RNN is generated from the previous hidden state h(t-1) and the current input x(t) by the basic equation of RNN (10.6), repeated as follows:\n\nDue to the properties of block matrix, this is equivalent to feeding the concatenation of [h(t-1), x(t), b] to a single linear (dense) layer being parameterized by a single matrix, which has a tahn non-linearity.\n\nBy contrast, the LSTM uses a group of layers to generate the current hidden state h(t). In short, the extra elements in LSTM include:\n\nAs mentioned earlier, such weighting is conditioned on the context. In LSTM, this is done by the following gates, those information (signal) flow out from yellow-colored sigmoid nodes in the following figure (Fig: LSTM), from left to right:\n\nThe actual control by the gate signal occurs at the blue-colored element-wise multiplication nodes, where the control signal from gates are element-wisely multiplied with the states to be weighted: the \u201cprevious history\u201d C(t-1), the \u201cnew stuff\u201d (output of the left tahn node) and the \u201ccurrent history\u201d C(t) after the right tahn non-linearity , respectively, in a left-to-right order.\n\nHere is another illustration of LSTM from colah, as follows, which is equivalent as the above illustration, although more compact (thus slightly more difficult to read):\n\nWhat\u2019s nice about the Colah figure and his blog post itself is that it shows how to break down LSTMs and how to related the figure with the equations. Here\u2019s my summary of the break-down (fig from Colah):\n\n4. Finally, how the current \u201chistory\u201d C(t) goes through a tahn non-linearity and is then weighted by the output gate o(t) (which is denoted as q(t) in the book).\n\nNote: the formula (10.25) \u2014 (10.28) in the book are written in a component-wise, non-vectorized form, thus the underscript i in each term.\n\nHopefully, how LSTM works is clear by now. If not, I recommend to read the colah blog post. And finally, here\u2019s my annotation of the Fig 10.16 in the book, which might make better sense now.\n\nIf you find LSTM too complicated, gated recurrent units or GRUs might be your cup of tea. The update rule of GRU can be described in a one-linear (10.29):\n\nNote: If reset gate were absent, GRU would look very similar to the leaky units, although (1) u(t) is a vector that can weight each dim separately, while alpha in the leaky unit is likely to be a scalar and (2) u(t) is context-dependent, a function of h(t-1) and x(t).\n\nPractical tips: LSTM is still the best-performing RNN so far. GRU preforms slightly worse than LSTM but better than plain RNN in many applications. It is often a good practice to set the bias of the forget gate of LSTM to 1 (another saying is 0.5 will do for initialization ).\n\nNotes: these techniques here are not quite useful nowadays, since most of the time using LSTM will solve the long-term dependency problem. Nevertheless, it is good to know the old tricks.\n\nTake-away: It is often much easier to design a model that is easy to optimize than it is to design a more powerful optimization algorithm.\n\nClipping gradient avoids gradient explode but NOT gradient vanish. One option is to clip the parameter gradient from a minibatch element-wise (Mikolov, 2012) just before the parameter update. Another is to clip the norm ||g|| of the gradient g (Pascanu et al., 2013a) just before the parameter update.\n\nRegularizing to encourage the information flow. Favor gradient vector being back-propagated to maintain its magnitude, i.e. penalize the L2 norm differences between such vectors.\n\nNeural networks excel at storing implicit knowledge. However, they struggle to memorize facts. So it is often helpful to introduce explicit memory component, not only to rapidly and \u201cintentionally\u201d store and retrieve specific facts but also to sequentially reason with them.\n\nNeural Turing machine (NTM) allows end-to-end training without external supervision signal, thanks the use of a content-based soft attention mechanism (Bahdanau et al., 2014)). It is difficult to optimize functions that produce exact, integer addresses. To alleviate this problem, NTMs actually read to or write from many memory cells simultaneously.\n\nNote: It might make better sense to read the original paper to understand NTM. Nevertheless, here\u2019s a illustrative graph:\n\n============ Now we visit 10.2.3 and 10.2.4 in detail\n\nThis section is only useful for understanding RNN from a probabilistic graphic model perspective. Can be ignored\u2026 We can jump this section and the next (10.2.4) pp390- pp398\n\nThe point of this section seems to show that RNN provides a very efficient parametrization of the joint distribution over the observations.\n\nE.g. the introduction of hidden state and hidden2hidden connections can be motivated as reduce Fig 10.7 to Fig 10.8, the latter of which has O(1)*\u03c4 parameters while the former has O(k^\u03c4 ) parameters. \u03c4 is the sequence length here. I feel that such imaginary reduction might be kind of factor graph technique.\n\nIncorporating the h(t) nodes in the graphical model decouples the past and the future, acting as an intermediate quantity between them. A variable y(i) in the distant past may influence a variable y(t) via its effect on h.\n\nContinuation of the graphical model view of RNN. Can be skipped first and revisit later\u2026\n\nThis section extends the graphical model view to represent not only a joint distribution over the y variables but also a conditional distribution over y given x.\n\nHow to take only a single vector x as input? When x is a fixed-size vector, we can simply make it an extra input of the RNN that generates the y sequence. Some common ways of providing an extra input to an RNN are:\n\nThat\u2019s the end of the notes."
    }
]