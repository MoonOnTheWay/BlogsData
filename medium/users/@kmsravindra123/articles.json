[
    {
        "url": "https://towardsdatascience.com/architecture-overview-of-a-conversational-ai-chat-bot-4ef3dfefd52e?source=user_profile---------1----------------",
        "title": "Conversational AI chat-bot \u2014 Architecture overview \u2013",
        "text": "3. It is a AI / ML driven architecture: The model learns the actions based on the training data provided (unlike a traditional state machine based architecture that is based on coding all the possible if-else conditions for each possible state of the conversation.)\n\n2. It is conversational! \u2014 What does this mean? The bot should somehow maintain the state of the conversation and respond to the user request in the current context (aka., it needs to be context aware). For eg., lets say the conversation flow goes something like this \u2014\n\nThe aim of this article is to give an overview of a typical architecture to build a conversational AI chat-bot. We will review the architecture and the respective components in detail (Note \u2014 The architecture and the terminology referenced in this article comes mostly from my understanding of rasa-core open source software).\n\nI will refer to the components in the above diagram, as we go through the flow. First, lets see what all things do we need to determine an appropriate response at any given moment of the conversational flow?\n\nThe intent and the entities together will help to make a corresponding API call to a weather service and retrieve the results, as we will see later.\n\nNow refer to the above figure, and the box that represents the NLU component (Natural Language Understanding) helps in extracting the intent and entities from the user request.\n\n3. Now, since ours is a conversational AI bot, we need to keep track of the conversations happened thus far, to predict an appropriate response. For this purpose, we need a dictionary object that can be persisted with information about the current intent, current entities, persisted information that user would have provided to bot\u2019s previous questions, bot\u2019s previous action, results of the API call (if any). This information will constitute our input X, the feature vector. The target y, that the dialogue model is going to be trained upon will be \u2018next_action\u2019 (The next_action can simply be a one-hot encoded vector corresponding to each actions that we define in our training data).\n\nThen, that brings us to the next question \u2014 how do we get the training values for our feature vector, input X?\n\nNote \u2014 If the plan is to build the sample conversations from the scratch, then one recommended way is to use an approach called interactive learning. We will not go into the details of the interactive learning here, but to put it in simple terms and as the name suggests, it is a user interface application that will prompt the user to input the user request and then the dialogue manager model will come up with its top choices for predicting the best next_action, prompting the user again to confirm on its priority of learned choices. The model uses this feedback to refine its predictions for next time (This is like a reinforcement learning technique wherein the model is rewarded for its correct predictions).\n\nI will not go into the details of extracting each feature value here. It can be referred from the documentation of rasa-core link that I provided above. So, assuming we extracted all the required feature values from the sample conversations in the required format, we can then train an AI model like LSTM followed by softmax to predict the next_action. Referring to the above figure, this is what the \u2018dialogue management\u2019 component does. Why LSTM is more appropriate? \u2014 As mentioned above, we want our model to be context aware and look back into the conversational history to predict the next_action. This is akin to a time-series model (pls see my other LSTM-Time series article) and hence can be best captured in the memory state of the LSTM model. The amount of conversational history we want to look back can be a configurable hyper-parameter to the model.\n\nNow, the predicted value of the next_action can be something like \u2014\n\nIf it happens to be an API call / data retrieval, then the control flow handle will remain within the \u2018dialogue management\u2019 component that will further use/persist this information to predict the next_action, once again. The dialogue manager will update its current state based on this action and the retrieved results to make the next prediction. Once the next_action corresponds to responding to the user, then the \u2018message generator\u2019 component takes over.\n\nMessage generator component consists of several user defined templates (templates are nothing but sentences with some placeholders, as appropriate) that map to the action names. So depending on the action predicted by the dialogue manager, the respective template message is invoked. If the template requires some placeholder values to be filled up, those values are also passed by the dialogue manager to the generator. Then the appropriate message is displayed to the user and the bot goes into a wait mode listening for the user input."
    },
    {
        "url": "https://towardsdatascience.com/using-lstms-to-forecast-time-series-4ab688386b1f?source=user_profile---------2----------------",
        "title": "Using LSTMs to forecast time-series \u2013",
        "text": "There are several time-series forecasting techniques like auto regression (AR) models, moving average (MA) models, Holt-winters, ARIMA etc., to name a few. So, what is the need for yet another model like LSTM-RNN to forecast time-series? This is quite a valid question to begin with and here are the reasons that I could come up with (respond below if you are aware of more, I will be curious to know)\u2014\n\nOn the other hand, there are the usual downsides that one needs to be careful about, while using LSTM\u2019s (or any DNN architectures for that matter) \u2014 requirement of lots of data, multiple hyper-parameters to be tuned etc., I also came across few articles that mentioned that LSTM\u2019s are not supposedly good at auto regression type of series. So take this with a pinch of salt.\n\nA simple sine-wave as a model data set to model time series forecasting is used. You can find my own implementation of this example here at my github profile. The core idea and the data for this example has been taken from this blog but have made my own changes to it for easy understanding.\n\nSo how does our given data look like? Below is the plot of the entire sine wave dataset.\n\nA brief about the overall approach before we dive deep into details \u2014\n\n2. use a two layered LSTM architecture coupled with a dense output layer to make a prediction.\n\n3. We will look at couple of approaches to predict the output \u2014 a.) Forecasting step by step on the test data set, b.) Feed the previous prediction back into the input window by moving it one step forward and then predict at the current time step.\n\nNow lets dive into the details \u2014\n\n2. Fix the moving window size to be 50. For this purpose we use pandas shift function that shifts the entire column by the number we specify. In the below code snippet, we shifted the column up by 1 (hence used -1. If we want to shift it down by 1, we will have to use +1) and then concatenate that to the original data.\n\nI tried to illustrate this on a toy data set below as to how the above for loop works for a window_size of 3.\n\nNote \u2014 we dropped all the rows that contain the Nan values in the above code snippet.\n\nIf you look at the toy data set closely, you can observe that this models the input data in the fashion we want to input into the LSTM. The last column in the above table becomes the target y and the first three columns become our input x1,x2 and x3 features. If you are familiar with using LSTM for NLP, then you can look at this as a fixed sequence of length 3 of sentence containing 3 words each and we are tasked with predicting the 4th word.\n\n3.Preparing the 3D input vector for the LSTM. Remember, the input vector for LSTM is 3D array: (num_samples, num_time_steps, num_features). In this case we have num of time steps = 50 and num_features = 1 (Extending the same analogy we saw in the previous point, that I found very useful in understanding why the input shape has to be like this \u2014 lets say, we have 50 words in one sentence and each word is represented by a word vector. So we need 50 time steps to go through each word vector in the sentence as an input to the LSTM at each time step. There is one sentence per observation and hence num_features = 1. Like this, we need to iterate over all the sentences in the train data to extract the pattern between the words in all sentences. This is exactly what we want here in the time series forecast as well \u2014 we want to identify all the patterns that exist between each of the previous values in the window to predict the current time step!)\n\nBelow is the model architecture used that is quite self-explanatory\u2014(Its a double stacked LSTM layers with the output from the first LSTM at each time step is being fed to the second LSTM)\n\nThe plot of predictions vs actuals almost overlap with each other to the extent that we cannot distinguish the blue curve and red curve in the below plot.\n\nHowever, the above is usually not a realistic way in which predictions are done, as we will not have all the future window sequences available with us.\n\n2. So, if we want to predict multiple time steps into the future, then a more realistic way is to predict one time step at a time into the future and feed that prediction back into the input window at the rear while popping out the first observation at the beginning of the window (so that the window size remains same). Refer to the below code snippet that does this part \u2014 (the comments in the code are self explanatory if you go through the code in my github link that I mentioned above ) \u2014\n\nUsing this prediction model, the results are plotted below \u2014\n\nAs can be seen, quite understandably, the farther we try to predict in time, more the error at each time-step that builds up on the previous predicted error. However, the function still behaves like a dampening sine-wave! As I said earlier, this is more realistic modelling of any time series problem since we would not have all the future sequences in hand with us.\n\nThis code can very well be extended to predicting any time series in general. Note that you may need to take care of other aspects of data preparation like de-trending the series, differencing to stationarize the data and so on before it is fed to LSTM to forecast.\n\nThats it! Hope this article provides a good understanding on using LSTM\u2019s to forecast time series. In case there are some takeaways from this article, please show your appreciation by clapping :)"
    },
    {
        "url": "https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74?source=user_profile---------3----------------",
        "title": "Neural Machine Translation \u2014 Using seq2seq with Keras",
        "text": "Below is the detailed network architecture used for training the seq2seq Encoder \u2014 Decoder network. We will refer this figure through out.\n\nBefore we start, it may help to go through my other post on LSTM that helps in understanding the fundamentals of LSTMs specifically in this context.\n\nThis article is motivated by this keras example and this paper on encoder-decoder network. The idea is to gain intuitive and detailed understanding from this example. My own implementation of this example referenced in this story is provided at my github link .\n\nFirstly we will go about training the network. Then we will look at the inference models on how to translate a given English sentence to French. Inference model (used for predicting on the input sequence) has a slightly different decoder architecture and we will discuss that in detail when we come there.\n\nSo how does the training data look?\n\nDetailed flow for training the network with code \u2014\n\nRefer to snippet 1 \u2014 Note that we have appended \u2018\\t\u2019 for start of the french sentence and \u2018\n\n\u2019 to signify end of the french sentence. These appended french sentences will be used as inputs to decoder. All the english characters and french characters are collected in separate sets. These sets are converted to character level dictionaries (useful for retrieving the index and character values later).\n\nRefer to snippet 2\u2014 Prepare the embeds for encoder input, decoder input and the target data embeds. We will create one-hot encoding for each character in English and French separately. These are called as tokenized_eng_sentences and tokenized_fra_sentences in the code snippet. These will be the inputs to encoder and decoder respectively. Note that the target_data french character embeds that we compare at the softmax layer output are offset by (t+1) compared to the decoder input embeds (because there is no start tag in target data \u2014 refer to the above architecture diagram for more clarity). Hence the target_data in the below code snippet is accordingly offset ( note k-1 in second dimension of the target_data array below)\n\nRefer to snippet 2 \u2014 As we noted in my other post on LSTM, the embeds (tokenized_eng_sentences and tokenized_fra_sentences and target_data) are 3D arrays. The first dimension corresponds to nb_samples ( =10,000 in this case). The second dimension corresponds to the maximum length of english / french sentence and the third dimension corresponds to total number of english / french characters.\n\nRefer to snippet 3: We will input character by character (of-course, their corresponding one hot embeds) into the encoder network. For the encoder_LSTM, we had set return_state = True . We did not do return_sequences = True (and by default this is set to False). This would mean that we obtain only the final encoded cell state and the encoded hidden state at the end of the input sequence and not the intermediate states at every time step. These will be the final encoded states that are used to initialize the state of the decoder.\n\nRefer to snippet 3 \u2014 Also note that the input shape has been specified as (None, len(eng_chars)). This means the encoder LSTM can dynamically unroll that many timesteps as the number of characters till it reaches the end of sequence for that sentence.\n\nRefer to snippet 4 \u2014 Inputs to the decoder will be the french character embeds (contained in tokenized_fra_sentences array) one by one at each time step along with the previous state values. The previous states for the first step of the decoder will be initialized with the final encoder states that we collected earlier in snippet 3. For this reason, note that the initial_state=encoder_states has been set in the below code snippet. From the subsequent step on wards the state inputs to decoder will be its cell state and its hidden state.\n\nAlso from the above code snippet, notice that the decoder is setup with return_sequences = True along with return_state = True. So we obtain decoder output and the two decoder states at every timestep. While return_state = True has been declared here, we are not going to use the decoder states while training the model. The reason for its presence is that they will be used while building the decoder inference model (that we will see later). The decoder output is passed through the softmax layer that will learn to classify the correct french character.\n\nRefer to snippet 5 \u2014 The loss function is categorical cross entropy that is obtained by comparing the predicted values from softmax layer with the target_data (one-hot french character embeds).\n\nNow the model is ready for training. Train the entire network for the specified number of epochs.\n\nBelow is the architecture used for inference models \u2014The inference model will leverage all the network parameters learnt during training but we define them separately because the inputs and outputs during inference are different from what they were during training the network.\n\nFrom the below figure, observe that there are no changes on the encoder side of the network. So we feed the new English sentence (one hot character embedded) vector as input sequence to the encoder model and obtain the final encoding states."
    },
    {
        "url": "https://towardsdatascience.com/lstm-nuggets-for-practical-applications-5beef5252092?source=user_profile---------4----------------",
        "title": "LSTM \u2014 nuggets for practical applications \u2013",
        "text": "The input array to be fed into the LSTM should be three dimensional. Lets look at this in the context of feeding several rows of sentences to be fed into the LSTM where each sentence is a collection of words and the size of the sentence can be either fixed / variable.\n\nBut when we define the input shape of LSTM, we only define it as 2D and not 3D \u2014 we do not specify the number of samples or the batch size. We only specify the number of time-steps and the number of features. In keras we define this as \u2014\n\nWhen the number of timesteps is None, then LSTM will dynamically unroll the timesteps till it reaches the end of the sequence. This is typical of Neural machine translation architectures involving encoder-decoder networks."
    },
    {
        "url": "https://towardsdatascience.com/support-vector-machines-intuitive-understanding-part-2-1046dd449c59?source=user_profile---------5----------------",
        "title": "Support vector machines ( intuitive understanding ) \u2014 Part#2",
        "text": "This is a continuation to the Part#1 on this topic. Please refer to the above part, if you would like to.\n\nHere we will discuss the intuition behind maximum margin classification that is inherent in SVM. The problem statement is as follows \u2014\n\nWe ideally want out our classification line to look like the bold line in the above figure. Note that the bold line is farther from its nearest data points compared to the dashed line. So how does SVM achieve this?\n\nFirst, lets brush up some basics to understand this. We know that a simple linear equation is given by a*x+b*y+c = 0. We will be using this simple equation for 2D and this can be easily extended to visualize for nD.\n\nThen we can rewrite our simple linear equation as w1*x1 + w2*x2 + w0 = 0 \u2014 Just substituted : a = w1, b = w2, c = w0, x = x1, y = x2 ; where w1, w2 and w0 are the weights that our optimization algorithm will eventually figure out.\n\nThen, for any observation say, (x1i, x2i) \u2014\n\nCombine both equations into a one: y*(w1*x1i+w2*x2i+w0) > 0, that works for both the above cases when y =-1 or +1 (Just multiplying on either sides by the target value, y (+1 / -1), so that the equation generalizes for classifying both positive and negative classes)\n\nFrom geometry, we know that the perpendicular distance (lets call this as \u2018m\u2019 ) from point (x1i, x2i) to the line w1*x1+w2*x2+w0 = 0 is given by\n\nrefer the below link for a simple derivation on why this is so \u2014\n\nConsidering normalized weights (w1 \u00b2 + w2 \u00b2 = 1), we can get rid of the denominator in the previous equation and it just reduces to the below \u2014\n\nBy multiplying on either sides by the target value, y (+1 / -1), the equation generalizes for classifying both positive and negative classes. Given this, then the following in-equation, y*(w1*x1i+w2*x2i+w0) \u2265 m , guarantees that every observation (x1i, x2i) lies on/beyond that distance \u2018m\u2019 on either sides of the classification line, for both negative / positive target class. Here, \u2018m\u2019, is referred to as the margin.\n\nSo now comes the next question, what causes SVM to maximize the margin \u2018m\u2019 ? The answer lies in optimizing the cost/ loss function that was discussed in Part #1.\n\nCombining the above, hinge-loss becomes 0 when w1*x1 + w2*x2 + w0 \u2265 1. Again, by multiplying this on either sides by the target value of y (+1 / -1) so that the equation generalizes for classifying both positive and negative classes. Doing so, we arrive at : y*(w1*x1 + w2*x2 + w0) \u2265 1. Comparing this to the previous in-equation (in bold), we see that the margin, m equals 1. Depending on the hinge-loss function we use, we can change this margin to the value that we want. The optimization algorithm will figure out the weights such that the above equation holds good in order to minimize the loss function.\n\nFrom the above plot, note that \u2014\n\nAn important thing to note here is that \u2014 the notion of \u2018predicted y\u2019 in Part #1 is nothing but the notion of margin value, m. If \u2018m\u2019 is high ( = \u2018Predicted y\u2019 is high) then the point is farther away from the classification line and hence we are more confident that the observation definitely belongs to the identified class. This is exactly the concept behind the maximum margin classifier.\n\nFinally, by controlling the regularization parameter of \u2018C\u2019 in the cost function, that we discussed in Part#1, we indirectly manipulate the margin \u2018m\u2019 for each observation ((x1i, x2i) and thereby tune the amount of cushion / slack, we agree to, in mis-classifying the data. If \u2018C\u2019 is chosen higher, then we allow more cushion for the data to get mis-classified and vice-versa. We can understand this intuitively, by considering a corner case where we make \u2018C\u2019 = 0, which means we do not allow any cushion. Then, note that the loss function just reduces to optimizing the cost =||w\u00b2||/2 ( which we discussed as the hard margin classifier in Part#1), which means we need to strike a balance to reduce the weights (for the above cost function to be minimal) and at the same time achieve the margin of 1 (as we want w1*x1 + w2*x2 + w0 \u2265 1). This nature of combinations will not allow any cushion for mis-classification and hence aptly called as hard margin classifier.\n\nIn the next and final part, we will look at how is feature transformation done using kernel trick."
    },
    {
        "url": "https://towardsdatascience.com/support-vector-machines-intuitive-understanding-part-1-3fb049df4ba1?source=user_profile---------6----------------",
        "title": "Support vector machines ( intuitive understanding ) \u2014 Part#1",
        "text": "Most of the material online covered on this topic was heavily treated with mathematics and lot of finer details, one can get easily lost understanding the broader concept. Here is an attempt to bring an intuitive understanding to most of the details in SVM with a very little mathematical treatment. The only basic assumption made is that the reader is already aware of some math fundamentals, logistic regression along with basic terms and concepts of machine learning. I plan to cover this topic \u201cSupport vector machines ( intuitive understanding)\u201d in 3 parts. In Part # 1, we will look at the loss function for SVM.\n\nLets start with what we understand about logistic regression and how SVM differs from logistic regression.\n\nIn logistic regression, a line L1 defines a probability distribution over the input space. A line L1 is said to be better than line L2, if the the distribution defined by L1 is low at class \u2018-1\u2019 points and high at class \u2018+1\u2019 points, on average, compared to the distribution defined by line L2.\n\nFew major things of SVM that are conceptually different from a logistic regression \u2014\n\nPart#2: Maximum margin classification \u2014 At a very fundamental level, in SVM, a line L1 is said to be a better classifier than line L2, if the \u201cmargin\u201d of L1 is larger i.e., L1 is farther from both classes.\n\nLets go over one by one \u2014\n\nLets take an example of a simple binary classification task. Then for the given input features \u201cX\u201d and target \u201cy\u201d, the goal of the SVM algorithm is to predict a value ( \u2018predicted y\u2019) close to the target (\u2018actual y\u2019) for each observation. To do this \u2014\n\nTotal error / cost = Sum of all losses for each observation in that iteration.\n\n4. The final weights which we arrive at, at the end of all iterations forms our final model used for predicting on unseen data.\n\nSo, for SVM, a loss function called as \u2018hinge loss\u2019 is used \u2014 refer to the below plot (from wikipedia).\n\nNote that from the above plot (blue line), it can be seen that the loss equals 0 , when\n\nBut when \u2018actual y\u2019 , \u2018predicted y\u2019 have opposite sign, the hinge loss increases linearly with y (one-sided error).\n\nSVM uses hinge loss where as logistic regression using logistic loss function for optimizing the cost function and arriving at the weights. The way the hinge loss is different from logistic loss can be understood from the plot below (from wikipedia \u2014 Purple is the hinge loss, Yellow is the logistic loss function).\n\nNote that the yellow line gradually curves downwards unlike purple line where the loss becomes 0 for values \u2018predicted y\u2019 \u22651. By looking at the plots above, this nature of curves brings out few major differences between logistic loss and hinge loss \u2014\n\nGiven this understanding of the hinge loss function for a SVM, lets add a regularization term (L2 norm) to the cost. The intuition behind the regularization term is that we increase the cost penalty if the values for the weights are high. So while trying to minimize the cost, we not only adjust the weights, we also try to minimize the value of the weights and thereby reduce over fitting to the training data and make the model less sensitive to outliers.\n\nSo with the added regularization term, the total cost function finally looks like:\n\nTotal cost = ||w\u00b2||/2 + C*(Sum of all losses for each observation)\n\nwhere \u2018C\u2019 is the hyper-parameter that controls the amount of regularization.\n\nDepending on the value of \u2018C\u2019 that we choose, we can have a hard margin classifier and a soft margin classifier.\n\nIn the next part we will look at what exactly causes SVM hyper-plane to distance itself (aka., maximize the margin) from the data points in two classes in order to achieve the notion of a maximum margin classifier. We will also look at what is the value of the margin."
    }
]