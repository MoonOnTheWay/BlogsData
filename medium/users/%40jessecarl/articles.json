[
    {
        "url": "https://medium.com/@jessecarl/jump-in-the-pool-6c0013385b51?source=user_profile---------1----------------",
        "title": "Jump in the Pool \u2013 Jesse Allen \u2013",
        "text": "If our operation sees heavy use, and the cost of a new resource is too high to justify allocating a new one when the pool is empty, one strategy we can employ is a worker pool. The following worker pool manages access to a . While the example is completely useless as it stands, writing to a is a useful stand in for a host of operations (encoding, zipping, etc.). We\u2019ll try to use this same basic idea for all of our examples.\n\nThis approach allows us to allocate a buffer for the lifetime of each worker instead of only the lifetime of the call. All of the expense of allocation (in this case the byte slice in the ) happens before the worker begins accepting work. We\u2019re also reusing the same variable. As we explore some of the other approaches to pooling resources, we\u2019ll see the same basic pattern we have in the worker: prepare a new resource, get a resource for use, use the resource, prepare the resource for reuse.\n\nOne important thing to note here is that, as this is implemented, the overall resource use is not static. Once all of the workers are occupied, new calls will begin to stack up, all blocked while waiting for an available worker. Play around with some ideas for solving that problem. What other issues come up as a result?\n\nIf our operation sees more sporadic use, where the idle time of a resource is likely to be quite a while, worker pools might be less appealing. Instead, we can keep our pre-allocated resources on hand in a free list.\n\nThis implementation and the worker pool implementation could probably be used interchangeably for most cases. Worker pools are more useful when used in concurrent pipelines, while free lists are more useful for managing expensive resources used sporadically. How would you solve the capacity problem from the worker pool example here?\n\nSo far, we\u2019ve been dealing exclusively with static pools of resources. The static resource pools make an important assumption: the cost of allocating a new resource is higher than the cost of waiting for one to become available. As a result, we have to pay special attention to what happens when we reach capacity. If we flip that assumption, and assert that the cost of waiting for a resource to become available (or holding on to enough resources to handle peak capacity) is higher than the cost of allocating a new resource, we arrive at a dynamic pool. Let\u2019s turn that fixed-size free list into a dynamic one.\n\nThis is not much different than the fixed-size free list. It could even be primed in the constructor instead of allocating on first use. With this solution, we can have a much smaller pool. Instead of having a pool capacity greater than the expected peak use, we only need a pool capacity greater than an expected jump in use: the effective size of the pool is the number of resources in use and the capacity instead of just the capacity. This solves the problem of blocking when we reach capacity, but it loses the predictable resource use we had with the fixed-size free list.\n\nThis approach is especially helpful if the resource we\u2019re using requires more than just garbage collection to destroy. A pool of open network connections might need to be closed when they are destroyed. A pool of temporary files might need to be closed and then deleted when destroyed. This kind of free list shows up in a lot of packages, and is described in Effective Go. Play around with this implementation.\n\nIn a small sliver of cases, we need a truly dynamic pool of resources that are likely to be reused before they would be collected as garbage. These resources are always temporary, but benefit from being reused a few times before they are destroyed. This is what was made for.\n\nThis is not drastically different from the dynamic free list. The major differences are that the dynamic free list can handle resources other than memory allocations (files, network connections, etc.), and the limits excess resource by time instead of count. When the garbage collector comes along, the excess resources can be reduced to nothing, but until that happens, there can be a massive number waiting to be reused or destroyed. Play around with this one too.\n\nWe noted that earlier, and while these examples don\u2019t appear much more complex than a solution without pools, there are a couple of common problems that will come up.\n\nThis one has bitten me a few times. Before returning a resource to the pool (or finishing your work for the worker pool), you must be sure that the resource is ready to be used exactly like a brand new one.\n\nI forgot to on the temporary files in a pool once \u2014 files that were reused were uploaded to S3 padded with as many zeros as the previous use had bytes.\n\nThe most common uses for these pools is to reuse byte slices either directly or indirectly through a or other similar type backed by a byte slice. Unless we do a of our byte slice, we cannot return it to the pool until we know that every use of that slice, and the array that backs it, is complete. If not, we are likely to start overwriting the contents of the backing array. It\u2019s best to avoid adding any additional layers of concurrency within the work here.\n\nOne more pitfall to look out for is most likely to occur when pooling network connections or other resources that can be lost. A fixed-size free list or a worker pool could be severely impacted by such an event. We\u2019ll have to check that the resource is still viable before we use it, and if it\u2019s not, we may have to replace it.\n\nThere are several ways we can reuse resources. They all have distinct tradeoffs. They all share one common tradeoff: adding performance adds complexity. We are going to be tempted to throw pools at our problems, but we should avoid doing so. Instead, by familiarizing ourselves with the basic concepts behind these resource pools, we can build our simple solutions with a bit more forethought. Expensive resources almost always share the same basic lifecycle with or without pooling: prepare, use, cleanup. Keeping that lifecycle in mind as we build our simple solution helps us avoid common pitfalls and makes implementing a resource pool much easier and less error-prone.\n\nA few more takes on resource pools:"
    }
]