[
    {
        "url": "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0?source=---------0",
        "title": "Understanding Activation Functions in Neural Networks",
        "text": "Recently, a colleague of mine asked me a few questions like \u201cwhy do we have so many activation functions?\u201d, \u201cwhy is that one works better than the other?\u201d, \u201dhow do we know which one to use?\u201d, \u201cis it hardcore maths?\u201d and so on. So I thought, why not write an article on it for those who are familiar with neural network only at a basic level and is therefore, wondering about activation functions and their \u201cwhy-how-mathematics!\u201d.\n\nNOTE: This article assumes that you have a basic knowledge of an artificial \u201cneuron\u201d. I would recommend reading up on the basics of neural networks before reading this article for better understanding.\n\nSo what does an artificial neuron do? Simply put, it calculates a \u201cweighted sum\u201d of its input, adds a bias and then decides whether it should be \u201cfired\u201d or not ( yeah right, an activation function does this, but let\u2019s go with the flow for a moment ).\n\nNow, the value of Y can be anything ranging from -inf to +inf. The neuron really doesn\u2019t know the bounds of the value. So how do we decide whether the neuron should fire or not ( why this firing pattern? Because we learnt it from biology that\u2019s the way brain works and brain is a working testimony of an awesome and intelligent system ).\n\nWe decided to add \u201cactivation functions\u201d for this purpose. To check the Y value produced by a neuron and decide whether outside connections should consider this neuron as \u201cfired\u201d or not. Or rather let\u2019s say \u2014 \u201cactivated\u201d or not.\n\nThe first thing that comes to our minds is how about a threshold based activation function? If the value of Y is above a certain value, declare it activated. If it\u2019s less than the threshold, then say it\u2019s not. Hmm great. This could work!\n\nActivation function A = \u201cactivated\u201d if Y > threshold else not\n\nWell, what we just did is a \u201cstep function\u201d, see the below figure.\n\nIts output is 1 ( activated) when value > 0 (threshold) and outputs a 0 ( not activated) otherwise.\n\nGreat. So this makes an activation function for a neuron. No confusions. However, there are certain drawbacks with this. To understand it better, think about the following.\n\nSuppose you are creating a binary classifier. Something which should say a \u201cyes\u201d or \u201cno\u201d ( activate or not activate ). A Step function could do that for you! That\u2019s exactly what it does, say a 1 or 0. Now, think about the use case where you would want multiple such neurons to be connected to bring in more classes. Class1, class2, class3 etc. What will happen if more than 1 neuron is \u201cactivated\u201d. All neurons will output a 1 ( from step function). Now what would you decide? Which class is it? Hmm hard, complicated.\n\nYou would want the network to activate only 1 neuron and others should be 0 ( only then would you be able to say it classified properly/identified the class ). Ah! This is harder to train and converge this way. It would have been better if the activation was not binary and it instead would say \u201c50% activated\u201d or \u201c20% activated\u201d and so on. And then if more than 1 neuron activates, you could find which neuron has the \u201chighest activation\u201d and so on ( better than max, a softmax, but let\u2019s leave that for now ).\n\nIn this case as well, if more than 1 neuron says \u201c100% activated\u201d, the problem still persists.I know! But..since there are intermediate activation values for the output, learning can be smoother and easier ( less wiggly ) and chances of more than 1 neuron being 100% activated is lesser when compared to step function while training ( also depending on what you are training and the data ).\n\nOk, so we want something to give us intermediate ( analog ) activation values rather than saying \u201cactivated\u201d or not ( binary ).\n\nThe first thing that comes to our minds would be Linear function.\n\nA straight line function where activation is proportional to input ( which is the weighted sum from neuron ).\n\nThis way, it gives a range of activations, so it is not binary activation. We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that. So that is ok too. Then what is the problem with this?\n\nIf you are familiar with gradient descent for training, you would notice that for this function, derivative is a constant.\n\nA = cx, derivative with respect to x is c. That means, the gradient has no relationship with X. It is a constant gradient and the descent is going to be on constant gradient. If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !!!\n\nThis is not that good! ( not always, but bear with me ). There is another problem too. Think about connected layers. Each layer is activated by a linear function. That activation in turn goes into the next level as input and the second layer calculates weighted sum on that input and it in turn, fires based on another linear activation function.\n\nNo matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer! Pause for a bit and think about it.\n\nThat means these two layers ( or N layers ) can be replaced by a single layer. Ah! We just lost the ability of stacking layers this way. No matter how we stack, the whole network is still equivalent to a single layer with linear activation ( a combination of linear functions in a linear manner is still another linear function ).\n\nLet\u2019s move on, shall we?\n\nWell, this looks smooth and \u201cstep function like\u201d. What are the benefits of this? Think about it for a moment. First things first, it is nonlinear in nature. Combinations of this function are also nonlinear! Great. Now we can stack layers. What about non binary activations? Yes, that too!. It will give an analog activation unlike step function. It has a smooth gradient too.\n\nAnd if you notice, between X values -2 to 2, Y values are very steep. Which means, any small changes in the values of X in that region will cause values of Y to change significantly. Ah, that means this function has a tendency to bring the Y values to either end of the curve.\n\nLooks like it\u2019s good for a classifier considering its property? Yes ! It indeed is. It tends to bring the activations to either side of the curve ( above x = 2 and below x = -2 for example). Making clear distinctions on prediction.\n\nAnother advantage of this activation function is, unlike linear function, the output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. Nice, it won\u2019t blow up the activations then.\n\nThis is great. Sigmoid functions are one of the most widely used activation functions today. Then what are the problems with this?\n\nIf you notice, towards either end of the sigmoid function, the Y values tend to respond very less to changes in X. What does that mean? The gradient at that region is going to be small. It gives rise to a problem of \u201cvanishing gradients\u201d. Hmm. So what happens when the activations reach near the \u201cnear-horizontal\u201d part of the curve on either sides?\n\nGradient is small or has vanished ( cannot make significant change because of the extremely small value ). The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits ). There are ways to work around this problem and sigmoid is still very popular in classification problems.\n\nAnother activation function that is used is the tanh function.\n\nHm. This looks very similar to sigmoid. In fact, it is a scaled sigmoid function!\n\nOk, now this has characteristics similar to sigmoid that we discussed above. It is nonlinear in nature, so great we can stack layers! It is bound to range (-1, 1) so no worries of activations blowing up. One point to mention is that the gradient is stronger for tanh than sigmoid ( derivatives are steeper). Deciding between the sigmoid or tanh will depend on your requirement of gradient strength. Like sigmoid, tanh also has the vanishing gradient problem.\n\nTanh is also a very popular and widely used activation function.\n\nLater, comes the ReLu function,\n\nThe ReLu function is as shown above. It gives an output x if x is positive and 0 otherwise.\n\nAt first look this would look like having the same problems of linear function, as it is linear in positive axis. First of all, ReLu is nonlinear in nature. And combinations of ReLu are also non linear! ( in fact it is a good approximator. Any function can be approximated with combinations of ReLu). Great, so this means we can stack layers. It is not bound though. The range of ReLu is [0, inf). This means it can blow up the activation.\n\nAnother point that I would like to discuss here is the sparsity of the activation. Imagine a big neural network with a lot of neurons. Using a sigmoid or tanh will cause almost all neurons to fire in an analog way ( remember? ). That means almost all activations will be processed to describe the output of a network. In other words the activation is dense. This is costly. We would ideally want a few neurons in the network to not activate and thereby making the activations sparse and efficient.\n\nReLu give us this benefit. Imagine a network with random initialized weights ( or normalised ) and almost 50% of the network yields 0 activation because of the characteristic of ReLu ( output 0 for negative values of x ). This means a fewer neurons are firing ( sparse activation ) and the network is lighter. Woah, nice! ReLu seems to be awesome! Yes it is, but nothing is flawless.. Not even ReLu.\n\nBecause of the horizontal line in ReLu( for negative X ), the gradient can go towards 0. For activations in that region of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input ( simply because gradient is 0, nothing changes ). This is called dying ReLu problem. This problem can cause several neurons to just die and not respond making a substantial part of the network passive. There are variations in ReLu to mitigate this issue by simply making the horizontal line into non-horizontal component . for example y = 0.01x for x<0 will make it a slightly inclined line rather than horizontal line. This is leaky ReLu. There are other variations too. The main idea is to let the gradient be non zero and recover during training eventually.\n\nReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. That is a good point to consider when we are designing deep neural nets.\n\nNow, which activation functions to use. Does that mean we just use ReLu for everything we do? Or sigmoid or tanh? Well, yes and no. When you know the function you are trying to approximate has certain characteristics, you can choose an activation function which will approximate the function faster leading to faster training process. For example, a sigmoid works well for a classifier ( see the graph of sigmoid, doesn\u2019t it show the properties of an ideal classifier? ) because approximating a classifier function as combinations of sigmoid is easier than maybe ReLu, for example. Which will lead to faster training process and convergence. You can use your own custom functions too!. If you don\u2019t know the nature of the function you are trying to learn, then maybe i would suggest start with ReLu, and then work backwards. ReLu works most of the time as a general approximator!\n\nIn this article, I tried to describe a few activation functions used commonly. There are other activation functions too, but the general idea remains the same. Research for better activation functions is still ongoing. Hope you got the idea behind activation function, why they are used and how do we decide which one to use."
    },
    {
        "url": "https://medium.com/the-theory-of-everything/intuition-behind-machine-learning-supervised-a-primer-7b4c0101183?source=---------1",
        "title": "Intuition Behind Machine Learning (Supervised) \u2014 A Primer",
        "text": "Recently, a few people asked me about machine learning. How the hell on earth does it work?! Their chief issue was they could not get the idea behind machine learning. Simply put, they failed to understand the concept at the core.\n\nThe general idea of \u201cif you have data, you can train a machine\u201d is widespread, but without knowing the concept in its core, without getting the \u201cintuition\u201d behind the concept, people will hardly ask the questions like \u201cwhat kind of data\u201d, \u201cwhy would it work for this data\u201d and so on. Through this article, I intend to write a few words to let readers understand the concept \u2014 why it works, why won\u2019t it work and so on. That way, they\u2019ll develop an understanding of machine learning and grasp it better when they try to learn/read online.\n\nAh, I am not going to give you theoretical explanations here, you can read it pretty much anywhere on the Internet( Wikipedia would be a good place to start). So, a general question is \u201cif I write a code with so many \u2018if\u2019s and \u2018else\u2019s and it refers to some auxiliary data it stores and improves over time, is it learning?\u201d\n\nThe answer \u2014 yes!!! It is indeed learning. Anything that improves over time can be considered theoretically a learning algorithm (all though in machine learning, that is not the prime area of focus). \u201cThen why not write code like that with so many ifs and elses and other constructs? Why do we need to train an algorithm\u201d? Well, there are a lot of use cases where the programmer doesn\u2019t know which algorithm can solve the given problem, or he cannot come up with an algorithm to solve a problem ( if and else hell! ) or simply because he is lazy :p.\n\nConsider this, a problem statement of implementing a cache. You are to implement a cache algorithm to serve pages for better user experience, and you have limits on how much you can cache. Now, a very naive approach would be to have an \u201cLRU cache\u201d. In an LRU cache, you will have a data structure to store the data about recently used pages and their \u201crecentness\u201d. Consider this as your knowledge base. Then you have an algorithm to look at this and react appropriately given better user experience ( that\u2019s the LRU algorithm, which does nothing but removes least frequently used stuff from cache ).\n\nNow, this would be ideal if the pages visited by users were not random and users had an affinity towards certain pages. LRU will keep caching the \u201chot\u201d pages thereby improving its quality over time, giving out great user experience ( let\u2019s just say ). So, this is definitely an algorithm that gets better with more usage and gives better results ( user experience through caching ) over time. But there is a problem. With LRU cache, the developer had to decide the \u201calgorithm\u201d, the quality of the algorithm and how it could solve the problem. With machine learning, we try to solve this problem ( 1. Because we are lazy :p, 2. Because all algorithms cannot be found and hand coded ). There are a lot of problems for which the \u201cmathematics\u201d to solve the problem is not worth finding out by ourselves. Example, object recognition. Imagine having to find the mathematics of identifying an object based on its colour, size, an equation of its shape etc, come up with an equation to do that and then programming. We would rather try to find it automatically if possible.\n\nMachine learning is the rescue squad for this kind of problem. Using machine learning, you can let a computer figure out the mathematics, if any, behind the problem. In more formal words, \u201cfind y = f(x) function which will yield the y we are expecting for the x we have \u201c ( think of supervised learning, for those who are well read on the subject )\n\nSounds cool? So basically, I can let computers find the equation by themselves if I tell them what I want from what input? Yes!, you can use machine learning to find a function which can produce the output that you are looking for, to the given input.\n\nWell, it\u2019s pretty much mathematics, so \u201cif there exists a function to solve the use case mathematically, then you can find it given you have quality data to learn it.\u201d\n\nHow much data? Well, it\u2019s not about how much data ( though more data may certainly help in practical cases ), it\u2019s more about quality data. What does this mean? Well let\u2019s explain this through an example\n\nConsider the two functions y = x and y = |x|\n\nClearly, mathematically, these two are different.\n\nNow, if I tell you to find the function from the following data set-\n\nAnd then I ask you what will be the output for -1, you would say -1. You found y =x function\n\nWould you be able to find y=|x| from the above? You would probably end up assuming that its y=x. But I gave u a lot of data! A lot till infinity!. The point here is that the function shows different behaviour when the input is negative, to define that characteristics of the function you also need some samples of negative inputs. That\u2019s what I meant by \u201cquality\u201d data. And another point to note is, you don\u2019t need \u201ca lot of data\u201d if you have \u201cquality data\u201d capturing the complete characteristics of the function ( forget deep learning and stuff, let\u2019s focus hand-coded machine learning ).\n\nIf I tell you,\n\nThen you would be probably able to reach a conclusion that the function is y=|x|, but I didn\u2019t give you a lot of data ( definitely, not till infinity ), but I gave you \u201cdefining\u201d data which captured the characteristic of the function I am looking for.\n\nIn real world cases , when you have a lot of data, and you don\u2019t know the function you are looking for, you certainly don\u2019t know which subset of the data will help you find the function (or define the ) characteristics better ( there are techniques for all that, but let\u2019s just stay a beginner for the moment ). And you certainly don\u2019t know if you have good enough quality data to find the function ( think y=x and y=|x| example ).\n\nHmm, so I have data, I want to find the function, will I be able to do that with machine learning? Just train train train!!?\n\nWell, yes, and no!. Yes \u2014 if the data you have is good enough to define the characteristics of the function you are looking for, No \u2014 if it is not. (again, let\u2019s stay a beginner for this argument).\n\nThere can be cases when you are not able to find any relationship between the inputs and outputs you have. What?! Why!? Probably you are missing something. Imagine trying to find out the amount of water you can fill in a cylindrical shaped vessel by only knowing the radius of its base!. Yes! That\u2019s what indirectly is happening in the above-mentioned case.\n\nOk so, by machine learning ( supervised ), we are trying to find out a mathematical equation to solve something that we are looking for.\n\nA lot of data is less important than \u201cright\u201d set of data.\n\nThere are a lot of algorithms, why a lot? If it\u2019s all about data, then we don\u2019t need a lot ( hmm, nice, you are thinking like what the founders of deep learning did ). Well yes, if it\u2019s all about data, the algorithm shouldn\u2019t matter ( dont get me wrong here, i am not talking about using same algorithm for everything! ). There should be a way to find out what we need using every algorithm (devised for a problem, for example classification algorithms for classification) we use. At the same time this is true, in practical machine learning, because of the facts that \u201cdata quality is unknown\u201d ,\u201d there is no enough data\u201d, \u201cthere is no enough computer power\u201d, \u201cwe need it to be trained faster\u201d etc, we use specialised hand-coded algorithms which will get us to the required function faster, with probably lesser data. But in theory, choice of algorithms (from the given set of algorithms for a problem) matter very less compared to the effectiveness of the data ( every algorithm for a kind of problem will solve it given enough data and time, but the amount of data/time may vary ). Well, think about humans! We have 1 algorithm to do everything we do!!! ( well its not like we have already discovered what that 1 great algorithm is , though deep learning has great advancements. Will talk about it in another article in future) I\u2019ll probably explain more on this in an another article comparing regression and classification algorithms and the difference.\n\nThough the above write-up is more inclined towards supervised learning, it should get you a feel of what machine learning is all about, and that is not magic, but mathematics. Hm, let\u2019s say Magematics."
    },
    {
        "url": "https://medium.com/the-theory-of-everything/evolution-life-and-god-a-neural-network-thought-process-3ee81a2546fb?source=---------2",
        "title": "Evolution, Life and God, a Neural Network thought process",
        "text": "Questions like \u201cwho is God?\u201d, \u201cdoes God exist?\u201d, \u201cwhat is the purpose of life?\u201d, \u201cwhat is evolution?\u201d etc have been puzzling a lot of thinkers out there with no productive outcome. Some people say \u201cwhy find out? It doesn\u2019t change anything\u201d and so on. Though it is very much true, my curiosity doesn\u2019t let me leave it at that. Here are a few thoughts I had when I tried to analyse all of it through an AI/Neural net point of view.\n\nDISCLAIMER: This is neither questioning anybody\u2019s faith /love/belief in God nor is this suggesting anything with regard to God\u2019s existence. Readers discretion is advised.\n\nOne of the many reasons why people believe in the existence of God is the awesome design humans have. From the top of the head till the tip of the toe, human anatomy is awesome ( yeah, other organisms too for that matter. Let\u2019s focus on humans now, can we? :P). Now who designed it? How did it happen?.\n\nLet\u2019s go through my thought process\n\nEverything around us happened randomly. What does that mean? The fact that humans have hands, legs, a head and other systems is a virtue of chance. Randomly, this design was formed. Humans could have formed with hands on their head, legs on chest and all sorts of combinations. It is just that this design formed based on probability/randomness. That doesn\u2019t make sense? If everything is random, there should have been equiprobable configurations of humans that should have existed today, probably breathing hydrogen and speaking with light and hearing through light and all sort of stuff. Well, where are they!?\n\nThat\u2019s where probability played a role. My guess is that all sorts of equiprobable configurations that could have formed, had formed. Over a period of time, they disappeared. Think at an atomic level. The fact that atom has the design it has today is also by virtue of chance. There could have been a billion other types of atoms which formed and over a period disappeared. Who knows?\n\nEverything happening in this universe possesses a mathematical stability and is completely predictable! Why do I say so? Well, before we go deep into it, let\u2019s explain a coin toss. (Yeah the well known coin toss in the world of probability.)\n\nIf I toss a coin, can you estimate/predict ( and not guess ) correctly whether it will be heads/ tails ? Well no, you would say it\u2019s random? Now, what if i tell you the following-\n\nSuddenly it becomes a predictable problem. It is now just mathematics. We can accurately tell whether it will end up being \u201ca heads\u201d or \u201ca tails\u201d.\n\nWoah! What just happened? We just turned something which we considered probabilistic/random into a completely predictable mathematical system. How did we do it? We just captured all those dimensions/finer details we need to make it a mathematical system. ( But remember such fine details were not created from nothing, they existed and we simply captured it ). The same could be true for every damn thing in the universe.\n\nFor instance, if I am given the exact configuration of your brain, if I know how the brain works, if I know the minutest chemical reactions happening in brain, your memory and other things, I can specifically predict what you are going to do the very next moment (provided I have resources that I need ). Think about it. It\u2019s just there, as a mathematical process, but we have not uncovered/tapped into the data. That\u2019s all.\n\nIt is the same for everything happening in the universe. The more data we have, the more predictable it becomes.\n\nWell, that means everything in the universe is following some sort of mathematical system. There is nothing unpredictable ( right now, it may be unpredictable, but that\u2019s because we don\u2019t have the necessary data ). So, even evolution is a mathematical process. There is nothing random. The fact that some species should go extinct is also coded ( in the mathematics of it ). Some day when the universe goes into some configuration, humans may become extinct too! Who knows? It\u2019s all planned ahead ( not by God ) but by the maths behind it (or you can call it God maybe!).\n\nIt then means that nothing is random? Oh shit! Ok, maybe nothing is random. Everything is following a predictable mathematical process (well, we know this, right? What we call as random in one dimension is a process in another , it\u2019s just a matter of getting more data )\n\nOk, so the whole damn thing is a random process/ predictable mathematical process and so on. If you notice, every species is trying to evolve. Trying to become better than what they were before in the same environment. Why so? Why the hell do they have to evolve? Where in the system is it programmed that \u201cevery life form should try to evolve\u201d. Why!! It is puzzling.\n\nI thought of neural networks here. What is a neural network essentially trying to do? It is trying to converge. What is getting better? Weights!!! Woah! Ok . So does that mean we are part of a huge neural network which is always trying to converge ( for some reason ) and we are the components which are getting better? Interesting topic! Now, the fact that we don\u2019t know \u201cwhy we are evolving\u201d led to controversial Gods. Let me add one more to the list \u2014 a \u201cneural network\u201d.\n\nBut.. wait.. The fact that neural networks should converge is coded in its algorithm. What about our system? Where the hell in the universe is it coded that the huge network it is, should try to converge. Where is it coded that \u201clife form\u201d should try to improve, evolve and get better over time. Why!!! And how? Does that mean there is a creator? Whoever created the algorithm to converge? Or does that mean the algorithm itself is a result of many probable algorithms that could have come up in universe by chance?\n\nHmm or maybe aliens made humans. Aliens planted life on earth to let it \u201cevolve\u201d and \u201clearn\u201d and get better just like how we program a \u201cneural network\u201d. Will we ever know?\n\nI started thinking about Neural Networks. No matter how much they learn, how much stuff happens inside, unless we equip them with the ability to make changes in the physical world that we live, a neural network cannot find/reach his creator. It is a parallel universe. The first one, the one we are living in, and the second, the cyber universe where the neural network is living.\n\nThat\u2019s when it struck me, that the intelligence that the neural network develops is not good enough to know/probe the external universe that we human live in. The intelligence that the neural network possesses is by virtue of its algorithm and training process. Same goes for humans. There are dimensions of universe which cannot be uncovered with human intelligence simply because those dimensions had no effect on evolution of human intelligence in the training process that we went through. This could also mean that the \u201cdiscoverable science\u201d of humans has a limit!\n\nBecause discovered science is a manifestation of human intelligence. Science may never reach those dimensions of universe through the way of \u201cintelligence\u201d. Just like a neural network\u2019s intelligence cannot teach it about the existence of human universe."
    }
]