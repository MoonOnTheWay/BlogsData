[
    {
        "url": "https://medium.com/@tomgrek/gliders-over-the-us-conways-game-of-life-with-zip-codes-20b7b32189?source=user_profile---------1----------------",
        "title": "Gliders over the US: Conway\u2019s Game of Life with zip codes",
        "text": "It\u2019d be really interesting to model the success and failure over time of US zip codes. I\u2019ve been thinking about how to use multi-agent simulation to model how population groups evolve over time.\n\nThis weekend, instead of doing that, I decided to make something pretty instead: if each zip code could be \u2018alive\u2019 or \u2018dead\u2019, how would the US evolve?. There are about 33,000 zip codes in the United States, my majestic (and really, ridiculously large) adoptive home. That makes an interesting grid for Conway\u2019s Game of Life.\n\nHere\u2019s how the grid looks:\n\nAnd here\u2019s a bit of how it evolves when it\u2019s alive:"
    },
    {
        "url": "https://medium.com/@tomgrek/i-modeled-my-co-workers-using-our-office-slack-chats-the-results-were-awesomenian-3c5e205b6604?source=user_profile---------2----------------",
        "title": "I modeled my co-workers using our office Slack chats. The results were awesomenian.",
        "text": "Like many applications of current-generation AI this is a bit silly and far-fetched (but does it nonetheless work? Find the code on my Github here and try it yourself).\n\nIt makes a great pitch though. After going off to register a futuristic-yet-anodyne domain ending in .ai, I\u2019d hit the streets from South Park to Sand Hill, exclaiming:\n\nIt\u2019s plausible I guess, and wonderful to think about. There are still enough investors out there worried about missing the hype train that Series A would come long before the necessary pivot to something less ambitious and more real.\n\nThe first step is to get hold of your organization\u2019s Slack history. This is easy to do through their web app, but you\u2019ll have to ask the organization admin to get it. The first bit of my code takes all the chat, cleans it up a bit, and merges it (from many JSON files) into a single file/string. I didn\u2019t bother to try separating out different channels and that will hurt accuracy, but with a big enough dataset it doesn\u2019t matter too much.\n\nThere are two types of net that can predict or generate text, character level (predicting a character given a preceding character plus some context) and word level (predicting the next word given a preceding word plus context). Character level is much more interesting since it allows for new combinations of letters, but word-level trains faster and is able to capture the semantic meaning of words much easier. (Character level can too but will require a much bigger network and more training time; a hybrid might work best of all right now). I chose to go with a character model.\n\nIn addition, my hypothesis was that who is speaking at a given time must influence the predicted text. Rather than force the neural net to learn who is speaking just by following the characters input \u2014 the way I parse the Slack data prefixes somebody\u2019s chat message with their name, like <__tom_grek__>everybody be cool this is a robbery \u2014 I added another vector to the neural net\u2019s input so that it consists of a vector made up of something representing the current character, plus something representing the current speaker. That \u2018something\u2019 is an embedding.\n\nThen, somewhat arbitrarily, I added a linear layer with ReLU non-linearity, a recurrent layer (which builds up a hidden state by having feedback \u2014 I used GRU or Gated Recurrent Unit instead of LSTM as in my experience they perform better), and another linear layer to collect the output. The overall model looks like this:\n\nSome of the more basic theory and practice is covered in my previous article here.\n\nThe code for the model is very straightforward. I build with PyTorch, and you can find my final code here. My main learnings while building this were:"
    },
    {
        "url": "https://medium.com/sap-io/evolving-an-enterprise-ready-startup-stack-at-sap-io-atlas-c8e4db8dca23?source=user_profile---------3----------------",
        "title": "Evolving an enterprise-ready startup stack \u2013 SAP.iO \u2013",
        "text": "As an app that runs in a browser, choices are obviously pretty limited for front-end: HTML, CSS, JavaScript. (I mean, theoretically, we could draw everything to a canvas using WebAssembly compiled from Lisp, but we are not that sort of team).\n\nNothing beats a front-end framework for convenience, and we chose Vue. It\u2019s not the most immediately obvious choice, but it has turned out to be a good one. Firstly, nobody on the team really likes Angular, so that\u2019s easy. Secondly, React was, at the time we started building the application, unavailable to us because of the notorious Facebook patent license. At the time we began, Alibaba was already using Vue which was a reasonable guarantee of its longevity \u2014 in fact, Vue adoption has grown exponentially since then. Although the React license has now changed, we\u2019re too invested in Vue to go back (not that we particularly would want to since Vue\u2019s been great).\n\nOf course, by using Vue we lose the ability to easily migrate to a native mobile application that React Native provides, but the sales team has already determined that \u201cgeospatial analytics on-the-go\u201d is not much of a selling point to the average business user, or indeed, anyone. The fact that the app works on mobile is mostly a happy coincidence.\n\nIt\u2019s also probably a coincidence if the app works on any browser other than the last two Chrome and Safari releases. Business users are not always on cutting-edge hardware and software, but sadly it\u2019d be too much of a strain on the team to support Firefox, Edge, and IE9 (only kidding, I\u2019m not sad about the last one at all). This takes away much of the pain of troubleshooting CSS rendering differences, and also allows us to natively take advantage of newer features, specifically, ES2017\u2019s now-standard async/await which is a huge timesaver versus bare-bones promises and especially callback hell.\n\nIn truth, part of the browser requirement also comes from the fact that we\u2019re using Mapbox GL to do the map rendering, which as the name suggests requires WebGL. We\u2019ve looked into running our own tile server using data from OpenStreetMap, but that would definitely be the kind of interesting academic exercise that suicides startups as we get further away from building core product.\n\nOn a side note: in general, we try to minimize the amount of building, and use open source libraries and third-party SaaS wherever possible (also read Why Open Source Isn\u2019t an Open-and-Shut Case for Startups). So far, we haven\u2019t encountered any case where the amount vendors charge for support or additional services (such as Mapbox Studio) exceeds the time cost of building it ourselves."
    },
    {
        "url": "https://medium.com/@tomgrek/from-first-principles-make-an-ai-customer-service-bot-in-pytorch-d1f4c9575d35?source=user_profile---------4----------------",
        "title": "The making of an AI customer service bot in PyTorch",
        "text": "There\u2019s no point spending hours training a dope model before knowing that the basics are right. Considering we\u2019re still at PoC stage right now, let\u2019s choose some simple training data:\n\nFundamentally, neural networks operate on numbers, so we have to turn each word into a numeric token. This isn\u2019t cheating: I\u2019m still building a recurrent network here that\u2019s aware of the past, it\u2019s not some bag-of-words model. I guess if you think about it, the human brain doesn\u2019t operate on words either, but translates them to electricity and neurotransmitters that it can process.\n\nWe can build a dictionary that converts words to integers, and at the same time another dict that converts back (ints to words):\n\nThere are more sophisticated ways to do this conversion: commonly, words that occur in the corpus with a frequency < N might be replaced with the catch-all token <unk>, we might similarly replace proper nouns, we should probably mark the start and end of sentences, we might stem words such that writing and written are both replaced with write. There are lots of NLP techniques that compensate for how bad AI traditionally was.\n\nAt this point there\u2019s no reason to use them, but I will bring in spaCy in the second article of this series, a state of the art NLP library I\u2019ve found easy to use and had success with before.\n\nBased on the very simple four training sentences above, the dictionary that\u2019s built up looks like this:\n\nThe idea behind this conversational bot is that, given an input word, it should be able to predict the next word, and that it should have some memory of the conversation in predicting that next word. In the example above you can see I triggered the net with a starting token <unk>. From that, the net\u2019s next word was \u2018may\u2019. Next, from [<unk>, may] it predicted \u2018i\u2019. And so on.\n\nThe design step is, ahem, highly iterative (aka finger in the air). Certainly for language we need some kind of recurrent network, because it needs to keep track of previous words, as well as the overall context of the conversation, in order to form a sentence. (One of the unstated design requirements here is that we want the network to figure out its own best way to keep track of the context; we don\u2019t want to have to do that ourselves).\n\nThe idea behind a recurrent network is simple:\n\nThe net\u2019s input consists of a word, plus the previous predicted word (which may or may not have been accurate, and also changes continuously as it aggregates past inputs \u2014 so it can be thought of as a kind of hidden state rather than an actual word). From these two things it outputs its next prediction.\n\nThe problem here is that there\u2019s a feedback loop, and positive feedback is inherently unstable (think of the amplified screeching when a microphone\u2019s too close to a speaker). Errors get magnified over time and training cycles, leading to the well known exploding gradient problem. This might happen if neurons have for example a relu non-linearity. Alternatively, if neurons\u2019 outputs are clipped to between -1 and 1 as with a sigmoid or tanh output function, well, when quantities less than zero are multiplied repeatedly, they quickly asymptote to zero, especially in computers\u2019 limited binary representation. That\u2019s the vanishing gradient problem.\n\nThe good news is that these problems were largely solved by LSTM neurons and more recently in a different way by GRU. The implementation details are beyond the scope of this article; besides, they are basic building blocks of PyTorch.\n\nThe network architecture \u2014 how many cells per layer, how many layers, do we use dropout \u2014 are not critical implementation details, just things we can tweak as we find necessary.\n\nThat\u2019s the main body of the network, but still we have to settle on what the input and output look like. First, the input.\n\nIt\u2019s been known for ages that networks work better given sparse categorical inputs rather than dense ones. For example, with the vocabulary of 13 words listed above, it\u2019s easier to train the network with 13 inputs that are all zeroes except for one, than 1 input that varies between 0\u20131 in increments of 1/13. That\u2019s one-hot encoding.\n\nNobody really uses one-hot encoding any more. With 13 inputs, why waste 12 of them on zeroes and one on a one, when via a simple lookup we could choose a vector formed out of any 13 scalars? Better still, treat those 13 scalars as trainable parameters, too. That\u2019s what embeddings do, and an embedding layer is treated as any other, getting parameter updates via backpropagation.\n\nSo that\u2019s an embedding, going into a recurrent layer, and at the output we\u2019ll need a layer with the number of outputs equal to the length of our vocabulary (so, 13). Framing this as a kind of \u2018classification\u2019 problem, if neuron 6 had the highest activation of the 13 output neurons, we\u2019d say that word 6 was the net\u2019s output, and so on. That\u2019s argmax.\n\nBut there are a few problems with that approach, including most notably that it\u2019s not differentiable, and also that outputs could start to go arbitrarily high. Thus, along came softmax which is a simple, differentiable formula to squish each output between 0\u20131 (and the sum of all outputs is 1, so it can be interpreted as a probability):\n\nFor classification problems like this where there\u2019s a single right answer and we don\u2019t care about less likely classes, taking the log of the softmax helps the network train faster. If the net is confident in an output, that is, softmax tends to 1, log softmax will tend to 0, leading to a smaller gradient and smaller weight updates. Likewise, where the pseudo-probabilities are smaller, PyTorch\u2019s log_softmax will ensure bigger gradients.\n\nSimilarly for the loss function: we want more-wrong probabilities to yield higher errors so the gradient changes faster, and less-wrong probabilities to yield smaller errors. Those are nice effects of using Cross Entropy Loss, which also happens to be the standard error function for a problem like this. This function, unlike something like mean squared error, never goes to zero; the network should never get stuck (and also never achieve \u2018perfection\u2019, which probably would mean overfitting anyway).\n\nThis is just a standard iterator made from the tokenized training data in step 2. About the only interesting thing here is that X, the training samples that are being fed in to the network, have requires_grad=False. Normally the training inputs will require us to initialize them to have a gradient, so that everything after them in the network also gets a gradient and can learn.\n\nBut since the network\u2019s first layer, the embedding, is a simple lookup: that doesn\u2019t make sense. The embedding\u2019s parameters and output automatically get given a gradient by PyTorch.\n\nAlso, since this article is supposed to stop at a working PoC \u2014 for simplicity I\u2019ve omitted batching and CUDA at this point. I also haven\u2019t bothered to include test/validation splits.\n\nBuilding a neural network in PyTorch is very easy:\n\nTo clarify some of the numbers:\n\nThis model got down to a loss after 300 epochs of 0.27. (It\u2019ll be different for yours, but by all means run this cell many times, lessening the learning rate over time). That seems pretty good, but is hard to evaluate except by trying it:\n\nNotice how in using the model, all you have to do is take the max of the outputs, and that reflects the dictionary position of the word. I kind of like the chaotic entropy in the output right now; we could do better for sure, but it\u2019s clear we\u2019re doing this via some kind of AI, rather than a rules engine:\n\nThe complete notebook for this Part 1 of the series is here. Next time I\u2019ll show how I made this model much better and more interactive, and then, in the final part, how it got to production."
    },
    {
        "url": "https://towardsdatascience.com/four-fails-and-a-win-at-a-big-data-stack-for-realtime-analytics-4f82f651d476?source=user_profile---------5----------------",
        "title": "Four fails and a win at a big data stack for realtime analytics",
        "text": "First, I\u2019d say don\u2019t modify your stack until you have to. This is probably the point at which data should be considered big \u2014 when your existing infrastructure is struggling to cope. We operated happily doing everything in SQL for a long time. Postgres is an amazing database, and my experience with it has been that, with proper indexing and well constructed queries, tables of up to about 80 million rows can be aggregated and queried in realtime.\n\nUnfortunately, it can be rather painful to carry out all the necessary DB maintenance, figure out the proper indexes, and write and rewrite queries. You also have to get the data stored just right, in neatly normalized rows and columns.\n\nOr do you? In general it\u2019s good to keep SQL data normalized and have a single source of truth, but complex and frequent table joins were killing performance. As we could no longer meet users\u2019 needs, we had to move on from dumb queries and denormalize some of our bigger tables. So, for example,\n\nThis needs us to do a bit more in the ETL step, and ensure good data governance, but it improved speed a lot. To offset the additional time whilst the team size remained constant, we moved from self-hosting Postgres to AWS Aurora Postgres. Performance didn\u2019t change, but it did take away some database administration headaches.\n\nExcept that it also created new performance headaches. One of the biggest features of our app is that it can score and cluster, according to user-specified criteria, tiny hexagon-shaped areas of cities. One city might consist of half a million hexes; for each hex the user might want to rate it on the median incomes and psychographic profiles of its nighttime dwellers and daytime workers, as well as how far it is from the nearest Starbucks. This requires a lot of data and computation.\n\nMoving to Aurora cost us the ability to keep data in the memory cache of our own machine. As different users try to score different cities, sequentially or even worse simultaneously, data was being pulled into (presumably from S3) and ejected from our virtual machine. This data load time, like some kind of page thrashing, vastly outweighed the computation time: the end result was that a city scoring which took 3 minutes when the data is available \u2014 itself not a fantastic result for something intended to be realtime \u2014 mushroomed to ten minutes or just timed out.\n\nNow from what I can tell in the big data world that I haven\u2019t been part of for too long, is that the current \u201cnobody got fired for buying IBM\u201d is Apache Spark. I diligently set about creating a cluster of Spark machines to see if it could help.\n\nSince we were now dealing with tables that had hundreds of millions of rows, and Postgres was creaking, partitioning that data and dividing computation over multiple nodes must be the way forward if our app is to scale.\n\nMaking a cluster on AWS turned out to not be that hard thanks to Kubernetes and Kops.\n\nAt this point you\u2019ll get a bunch of new AWS resources including instances for the cluster master and the computation nodes. Docker containers can be deployed into those instances and thanks to Kubernetes magic they will be able to communicate with each other, with other machines on the same VPC, and if you choose with the outside world via exposed load balancers.\n\nWith Helm, I was able to quickly get a bunch of Spark nodes up and running, together with some Hadoop nodes for their HDFS file storage.\n\nI found Spark hard to administer, difficult to work with (it\u2019s all Java and is designed to work with Java applications, despite what the promise of PySpark may have you believe), and overall just not that fast.\n\nI\u2019m sure Spark can function in some people\u2019s workflows for batch processing, and I\u2019m aware it has streaming extensions too. However, it takes several minutes to spin up the machinery for even a simple job, rendering it useless for a realtime use case.\n\nAdditionally, HDFS was not fast and seems kind of irrelevant now we have S3.\n\nBesides, if you come from the Python ecosystem as I do, it\u2019s impossible not to miss the fantastic array of tools we have at our disposal. If you\u2019re that person and you\u2019re starting or evolving a big data project today, I learned there\u2019s a much better way.\n\nPandas is about as fast as it gets for working with numeric data. Array computations happen at native speed (thanks to Cython) and often leverage vector instructions (thanks NumPy).\n\nDespite this, I\u2019d always thought Pandas was really for the ETL and analytic parts of working with data; more of a data science tool than actually a library that can be deployed into production. I was wrong:\n\nDask, a distributed computation library made by the same people behind Anaconda, partitions large dataframes over different nodes, schedules parallel computations on those nodes (building a graph and lazily deferring computation til the last minute, like Spark does), and gathers results and manages the distributed data. It\u2019s integrated with and largely interoperable with Pandas, so a joy to work with.\n\n(Not 100%; some things like e.g. indexing a dataframe on multiple columns don\u2019t make sense with a partitioned dataframe).\n\nAnd it\u2019s fast: each node leverages Pandas and its Cythonized, vectorized code. Distributed computing isn\u2019t free; it takes a bit of time and network latency for Dask to build the graph and schedule each node\u2019s computations, as well as to combine/reduce data at the end. Basic Pandas would be faster, if it could fit the whole dataframe into memory \u2014 and it could, if we scaled up our base machine, but that\u2019s not going to last forever: we\u2019d still need to horizontally scale our platform at some point.\n\nCoding with Dask is largely familiar. For example, here\u2019s a replacement for the SQL SELECT MAX(col + col2) FROM table WHERE zip_code IN ('94105', '94106') AND day_of_week IN (1,2,3)\n\nIt was again fairly easy to deploy Dask into the Kubernetes cluster I\u2019d made earlier: they have their own Helm charts documented here. Our infrastructure now looks like this:\n\nThe analytics that took several minutes in Postgres, and often much longer if data needed to be read in from persistent storage, now returns to the client in under 30 seconds.\n\nIf you have the money to spend, and especially if you are not still proving out product/market fit, go for a vendor solution. A fast, column-oriented in-memory SQL database would not have required much change to our process and code.\n\nOtherwise, for processing big data in realtime as part of a SaaS, I do recommend looking to see if Dask could meet your needs: it\u2019s fast, it scales horizontally, it lets you write code in the same way using the same libraries you\u2019re used to, and it\u2019s being used live in production today (*well, by us at least).\n\nObviously, I think Dask kills Spark, but the author of Dask does give a more nuanced view here if you\u2019re interested."
    },
    {
        "url": "https://medium.com/@tomgrek/wtf-is-going-on-with-fast-ai-db59741b5da2?source=user_profile---------6----------------",
        "title": "WTF is going on with fast.ai? \u2013 Tom Grek \u2013",
        "text": "fast.ai\u2019s notebook generates some Nietzsche-like text, and it begins by getting some input data \u2014 Mr Existential Angst\u2019s complete works \u2014 with:\n\nWrapping a basic urllib method seems rather unnecessary to me. They do it primarily to put a tqdm progress bar on the download, but I think that\u2019s just arcana that obscures the simple purpose of the code. Instead, let\u2019s KISS, and keep things light with a bit of Shakespeare.\n\nThe rest of fast.ai\u2019s pre-processing just converts that text to numbers that can be input to a neural network (and provides for the reverse transformation). It\u2019s not particularly interesting, and looks like this:\n\nThe theory behind this network is that 3 preceding characters can predict the 4th character. fast.ai, quite sensibly, creates a training set of X and y as follows (slightly simplified by me):\n\nHere, they are using np.stack as a quick way to turn a list into a numpy array, but let\u2019s briefly examine that function as it crops up later too. It takes a keyword argument, axis, and stacks columns along that axis. Stacking along the first axis makes no difference, but if you stack along the second (axis=1):\n\nThe actual models in fast.ai are really well explained and are not the focus of this article, but here\u2019s their simple code for taking those 3 input characters and predicting the next. One simplification I made: fast.ai\u2019s code uses a method called as short hand for \u2014 useful if you\u2019re writing a ton of models to teach people, but opaque for the learner, so I switched it back.\n\nNote in particular how the model takes three separate tensors as input, instead of a single input tensor of higher rank. Prof Jeremy\u2019s original notebook does move to that as it progresses from the basic model above to one with an LSTM layer. But for now, splitting the inputs is easier to comprehend so let\u2019s stick with it, even if it adds complexity to feeding training data to the model as we will see.\n\nIn other places the fast.ai code uses as shorthand for the (wrapped) function (in this case our tensor must index into the embeddings matrix with integers, so the particular tensor type here is ). That\u2019s especially confusing when T is also numpy shorthand for transpose. I\u2019ve reverted to the long form.\n\nThis is my particular bugbear with the fast.ai code. Take a look at the original:\n\nThis class was glossed over on the course and indeed trying to dig into it with:\n\nPlumbing the source code reveals that this object inherits from or uses the following:\n\nis another undocumented class while simply says:\n\nCoincidentally this is the same exact docstring as for the class. Well, we can at least see that whatever is, it returns an object that has a property which is an iterator. In fact, each iteration yields a batch of inputs, a simple idea when shown visually:\n\nI replaced this with my own iterator:\n\nThen you have all the * weirdness that fast.ai uses together with the iterator. I\u2019d never come across that syntax before, and I specifically remember in class Prof Jeremy saying, \u201cIf you don\u2019t know what this means, look it up\u201d. Turns out it\u2019s called the unpack operator and does this:\n\nArmed with this knowledge, we can train the model in the regular PyTorch way without needing any wrapper classes; especially not the fit magic method that fast.ai\u2019s notebook uses:\n\nThis actually trains faster than fast.ai\u2019s model, while achieving similar accuracy, for what that\u2019s worth (which is not much: remember this was their rubbishy, basic version). And while outcomes are not really the purpose of this article, it\u2019d be disappointing to end without giving a sample output after training on the Bard for a couple of epochs:\n\nWell, we retained the good stuff from fast.ai\u2019s lesson 6, without needing to use their library. Of course, if you haven\u2019t done the course yet have a look; Google allows you to run Jupyter notebooks with PyTorch on GPU for free at their colaboratory; and you can also find my complete pared-down notebook on Github here."
    },
    {
        "url": "https://becominghuman.ai/pytorch-from-first-principles-part-ii-d37529c57a62?source=user_profile---------7----------------",
        "title": "PyTorch From First Principles: Part II \u2013",
        "text": "In the first part of this article, we built a multi-layer perceptron from scratch in order to learn an arbitrary function, utilizing some conveniences of PyTorch. In this article, we\u2019ll ditch the conveniences; after which developing any kind of neural network becomes easy!\n\nAs usual, we need some basics:\n\nPreviously, the neural network looked something like this:\n\nLet\u2019s say that to really understand neural nets, we need to understand what nn.Linear is. This is their basic building block; depending on our understanding of and belief in neurobiology we might want to develop something else.\n\nRecall that the basic neuron multiplies inputs by weights, sums them, and performs some non-linear function on the result. Here\u2019s an example, where non-linear function is ReLU, and where there is no bias term:\n\nWritten in the familiar linear algebra notation, what you see here is:\n\nThis calculation extends to any size of neural network, where each layer builds on the last: it\u2019s a series of matrix multiplications.\n\nHere\u2019s another example, this time with two inputs and two neurons.\n\nSay the mini-batch size was 2: a training sample of [1,2] and another training sample of [3,4]. The outputs of this layer can be computed simultaneously, making better use of a GPU:\n\nYou can confirm this in PyTorch, as follows:\n\nSeries of operations like this can be chained to make a deep network; somewhere internally the framework is compiling a graph of computations to figure out what can be done simultaneously vs sequentially. Additionally, inputs do not have to be fed in to the network in a long line \u2014 as tensors, this basic calculation extends into addditional dimensions (e.g. the three RGB channels of an image) which preserves spatial data that would be lost if the inputs to a network were just a long vector.\n\nBuilding a fully connected layer of neurons is a doddle. Inherit the utility class nn.Module, set inputs and outputs, create the parameters and initialize them, and define a forward method (PyTorch takes care of the corresponding backward method and neurons\u2019 gradients, and indeed recommends not trying to override it yourself).\n\nI mentioned in the first article that this one wouldn\u2019t just build a neuron, but would also make a new learning method to compete with SGD. Here it is! Zero error after a single epoch (the function we are learning is to triple a positive number: therefore set the weight to 3 and the bias to 0). This is just to demonstrate how to work with the network\u2019s parameters.\n\nAs a class, it just needs to be initialized with a generator of the network\u2019s weights. We can process them as a list, then convert the list back to a generator at the end. All you need to do is define a step method:\n\nAnd use named_parameters instead of parameters:\n\nI\u2019ve taken a few shortcuts here:\n\nNevertheless: this is a neural network from scratch. To go from here to state-of-the-art only requires adding more of the same. Let\u2019s go play! As usual you can find the final notebook from this article on my Github."
    },
    {
        "url": "https://medium.com/@tomgrek/building-your-first-neural-net-from-scratch-with-pytorch-56b0e9c84d54?source=user_profile---------8----------------",
        "title": "Building Your First Neural Net From Scratch With PyTorch",
        "text": "I\u2019m going to assume that you know what a neuron is. I mean, in biology, yeah it\u2019s some part of the brain, errr, we have a gazillion of them, something like that? In comp.sci terms: a unit that takes weighted inputs and sums them to produce some output; more useful ones have a non-linear function applied to the summation before producing an output.\n\nNow, I will also assume you have PyTorch installed, and ideally also Python, and even better Jupyter Notebook. If not, Google is your friend; I understand that PyTorch even works in Windows 10 now too which is cool. Import the requirements:\n\nIt\u2019s a fully connected layer, that has one input and one output. Now, that layer (technically neuron/weight combo) will have a weight that\u2019s applied to its input; its output will be Ax + b ([weight x input] + bias) since it\u2019s just entirely linear, with no activation function.\n\nTake a moment and look just how dope that network definition is: pure Python (pretty much), and you only need to define how the input signal is processed, without worrying about backprop. You just specify how the neuron layer modifies its input signal. That\u2019s because of the package that exports , and a keeps track of gradients. As you know, it\u2019s important to keep track of gradients otherwise gradient descent \u2014 estimate how wrong our network was for a given set of inputs, figure out the direction and scale of the error with respect to the weights, and scale that by its contribution \u2014 wouldn\u2019t work.\n\nI probably don\u2019t need to specify this, but a is a tensor: that is, a matrix with at least a third dimension. Go ahead and create a network according to how you specified it, and take a look inside:\n\nIt\u2019s possible then to take a look at the parameters of the network. Parameters are automatically optimized by the network; hyperparameters such as learning rate require tuning by a human (so far, at least).\n\nOK, so our network was initialized with some random weight of -0.4 and bias of -0.4478. Let\u2019s consider how to work this network.\n\nYeah, we just created a random number with PyTorch. It\u2019s a tensor with a single dimension (alternatively, would create an equivalent tensor with the value 1). Setting means it\u2019s an optimizable variable. Then you can chuck this number through the unlearned network:\n\nWell, that makes sense if you think about it. Next up, define a loss function and an optimizer using stochastic gradient descent:\n\nIn this case, we\u2019ve defined our own loss function: least squares. It has to be squared to give the magnitude of the error; the gradient itself will say whether to move the parameter up or down. For each training example in a randomly shuffled set, SGD \u2014 behind the scenes with the magic of PyTorch \u2014 adjusts the available parameters based on how they affected the gradient of the error (chain rule shizz, scaled by learning rate and momentum, which are chosen arbitrarily), and backpropagates gradients and updates through the network.\n\nThen, define a training dataset. For this article, we\u2019re just going to teach the network how to treble a number: our goal for the single perceptron of Ax + b will be that A=3 and b=0. A simple training dataset (who says neural nets need big data!?) is:\n\nThen, the training loop looks like:\n\nLoss begins to converge on zero after a while:\n\nDid we get to Ax + b (3x + 0)? Almost:\n\nClose enough. To a point Gary Marcus makes in his article linked above:- the network doesn\u2019t know to extrapolate to integers, but I think this is a good result for something with vastly fewer brain cells than a nematode."
    },
    {
        "url": "https://medium.com/@tomgrek/reactive-python-javascript-communication-in-jupyter-notebook-e2a879e25906?source=user_profile---------9----------------",
        "title": "Reactive Python-Javascript communication in Jupyter Notebook",
        "text": "This post details my first baby steps in combining Python\u2019s back-end ecosystem and expressiveness, Jupyter\u2019s intuitiveness, and Javascript\u2019s client-side interactivity to build \u201cthe ultimate app for everything\u201d.\n\nLet me first talk a bit about the specific motivation for what I\u2019m attempting. At work I use MapboxGL a fair bit for visualizing big data on a map. It\u2019s an awesome Javascript library for displaying client-side, GPU-accelerated maps with overlayed data.\n\nThe aim of our web app basically is to make geospatial data science accessible to regular people. Whilst our target analysts might be Excel experts, it\u2019s my personal goal to get them using Jupyter Notebook and to that end I\u2019m aiming to integrate MapboxGL into Jupyter for interactive map goodness: design data with Python/Pandas and easily display and animate it.\n\nRyan Baumann and contributors have made the mapboxgl-jupyter library, but it\u2019s limited in that the map displayed in Jupyter is static, just a cell that renders an with all the data for the map right there in the iframe source! My vision is that as the user alters data in a Pandas data frame, the map should update reactively, allowing proper investigative analysis and visualization of trends and movement.\n\nHere\u2019s what the library\u2019s output looks like: an iframe, with data embedded:\n\nWhat I\u2019m attempting to do here isn\u2019t limited to working with MapboxGL, however. It would be equally applicable to finally building a decent d3.js plugin for Jupyter (can you imagine matplotlib ever being displaced though!) Or a side project I\u2019m going to take on next: WebAudio synthesis powered by Python\u2019s number crunching, with Jupyter\u2019s friendly and intuitive interface (similarly, if you know any audio stuff, can you imagine that displacing the powerful but complex SuperCollider!)\n\nI love Jupyter, but while researching how to solve this problem, I was sad to discover that it uses JQuery and Backbone.js (really!)\n\nMy first approach was to look into creating a notebook extension, but it turned out they are more for adding to the UI of the notebook. I couldn\u2019t find an easy way to communicate with the Python kernel that the notebook was actually executing.\n\nThat\u2019s when I came across widgets. First, you\u2019ll need to enable the widgets notebook extension:\n\nIf the extension\u2019s not already installed, should fix it."
    }
]