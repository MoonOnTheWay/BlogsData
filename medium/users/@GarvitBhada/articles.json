[
    {
        "url": "https://medium.com/@GarvitBhada/deep-learning-part-2-ccc2753a343a?source=user_profile---------1----------------",
        "title": "Deep Learning ( Part 2 ) \u2013 Garvit Bhada \u2013",
        "text": "In this part of the blog, I will focus more on the Deep Learning tools which can help in performing Econometrics\u2019 tasks. Let\u2019s start with Recurrent Neural Networks or RNNs.\n\nAn RNN can be used for time series analysis. An RNN takes the output of a layer and feeds it back into the same layer. We usually have only a single layer in the entire network, however, since the feeding process goes on in different time periods, we can consider the single layer as a composition of multiple layers over different time periods. Let\u2019s clear the concept using a figure.\n\nIn the above figure, h (at different time periods) represents the output while X (at different time periods) shows the inputs. Now, the progression we see from left to right is for different time periods, but the layer that is being used is only one. The output at any time period is used as the input in the next time period and into the same layer.\n\nA major benefit of RNNs is that it can take a sequence of values as inputs and can output a sequence of values. If we stack a lot of RNNs together, we encounter the problem of vanishing gradient once again. A solution to that is the Gating technique, which we will not discuss here.\n\nAnother Neural Network which can be used for econometric methods includes Autoencoders, which is a type of RBM( Restricted Boltzmann Machine). Autoencoders help figure out the underlying structure of the data set.\n\nAutoencoders encodes the unlabeled inputs which can be decoded to form an accurate output. We try to minimize the nodes ( see figure below ) in the hidden layer and at the same time we want to preserve the information available in the inputs.\n\nAutoencoders have proven themselves in the application of Principal Component Analysis and studies have proven that they perform better than the traditional methods."
    },
    {
        "url": "https://medium.com/@GarvitBhada/deep-learning-part-1-569de4053bbb?source=user_profile---------2----------------",
        "title": "Deep Learning ( Part 1 ) \u2013 Garvit Bhada \u2013",
        "text": "Each day we hear about some machine learning innovation which blows our mind and enthralls us. A lot of these innovations use deep learning concepts. In this post, I will try to jot down a few useful basics. Let\u2019s start with neural networks (say NN).\n\nAn NN can outperform other traditional classification algorithms like Logistic regression, naive bayes etc. in complex tasks which involves large amount of variables and data. However, since NN uses nodes in layers to train and detect pattern, the number of nodes required increases exponentially for too complex tasks. Thereby, making NN unusable. Here comes the application of Deep Nets (DN), which break down complex patterns into simple patterns.\n\nDeep Nets can be used for both supervised and unsupervised learning problems. Now, we still need to train the NNs. For this we use the technique of backpropagation (I wouldn\u2019t go into much detail about how backpropagation works). Backpropagation causes the problem of gradient vanishing. Explained concisely in this link \u2014 https://www.quora.com/What-is-the-vanishing-gradient-problem/answer/Nikhil-Garg .\n\nFor dealing with the problem of vanishing gradient we resort to a type of deep net called the RBMs or the Restricted Boltzmann Machines. RBMs reconstruct the inputs (by encoding it into numbers and working backwards to check if it encodes properly, similar to NN process) to find patterns in the data. The term \u2018Restricted\u2019 comes from the fact that it typically uses only two layers. The functioning is similar to an NN. (Note \u2014 each node is connected to every other node).\n\nRBMs are a part of the feature extraction NN. But, the question is how does this help with the vanishing gradient problem.\n\nFor solving the vanishing gradient problem, we use a technique in which we stack the RBMs together and just like the perceptrons where stacking multiple perceptrons increased the power of the nets, stacking the RBMs too yield better results. This type of deep net is called Deep Belief Net or DBN. However, unlike the other NNs, each RBMs layer tries to learn the entire output.\n\nDBNs require us to introduce small set of labelled data in order to label the other data. Hence, DBNs are used for supervised learning problems.\n\nDBNs are the solution to the vanishing gradient problem.\n\n(My primary interest in writing this blog is to explore the different domains in which the field of Machine Learning can be applied to the field of Economics. In the next few posts, I will try to compare the techniques of Econometrics and explore how can Machine learning can be linked to that. Stay tuned !)."
    }
]