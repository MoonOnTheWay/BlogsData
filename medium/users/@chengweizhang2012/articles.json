[
    {
        "url": "https://heartbeat.fritz.ai/how-to-train-a-keras-model-to-generate-colors-3bc79e54971b?source=user_profile---------1----------------",
        "title": "How to train a Keras model to generate colors \u2013",
        "text": "How to train a Keras model to generate colors\n\nEver wonder how paint colors are named? \u201cPrincess ivory\u201d, \u201cBull cream.\u201d And what about \u201cKeras red\u201d? It turns out that people are making a living naming those colors. In this post, I\u2019m going to show you how to build a simple deep learning model to do something similar \u2014 give the model a color name as input, and have the model propose the name of the color.\n\nThis post is beginner friendly. I will introduce you to the basic concepts of processing text data with deep learning.\n\nLet\u2019s take a look at the big picture we\u2019re going to build,\n\nThere are two general options for language modeling: word level models and character level models. Each has its own advantages and disadvantages. Let\u2019s go through them now.\n\nThe word level language model can handle relatively long and clean sentences. By \u201cclean\u201d, I mean the words in the text datasets are free from typos and have few words outside of English vocabulary. The word level language model encodes each unique word into a corresponding integer, and there\u2019s a predefined fixed-sized vocabulary dictionary to look up the word to integer mapping. One major benefit of the word level language model is its ability to leverage pre-trained word embeddings such as Word2Vec or GLOVE. These embeddings represent words as vectors with useful properties. Words close in context are close in Euclidean distance and can be used to understand analogies like \u201cman is to women, as king is to queen\u201d. Using these ideas, you can train a word level model with relatively small labeled training sets.\n\nBut there\u2019s an even simpler language model, one that splits a text string into characters and associates a unique integer to every single character. There are some reasons you might choose to use the character level language model over the more popular word-level model:\n\nYou may also be aware of the limitation that came with adopting character level language:\n\nFortunately, these limitations won\u2019t pose a threat to our color generation task. We\u2019re limiting our color names to 25 characters in length and we only have 14157 training samples.\n\nWe mentioned that we\u2019re limiting our color names to 25 characters. To arrive at this number we checked the distribution of the length of color names across all training samples and visualize it to make sure the length limit we pick makes sense.\n\nThat gives us this plot, and you can clearly see that the majority of the color name strings has lengths less or equal to 25, even though the max length goes up to 30.\n\nWe could in our case pick the max length of 30, but the model we\u2019re going to build will also need to be trained on longer sequences for an extended time. Our trade-off to pick shorter sequence length reduces the model training complexity while not compromising the integrity of the training data.\n\nWith the tough decision of max length being made, the next step in the character level data pre-processing is to transform each color name string to a list of 25 integer values, and this was made easy with the Keras text tokenization utility.\n\nRight now padded_names will have the shape of (14157, 25), where 14157 is the number of total training samples and 25 being the max sequence length. If a string has less than 25 characters, it will be padded with the value 0s from the beginning of the sequence.\n\nYou might be thinking, all inputs are now in the form of integers, and our model should be able to process it. But there is one more step we can take to make later model training more effective.\n\nWe can view the character to integer mapping by inspecting the t.word_index property of the instance of Keras\u2019 Tokenizer.\n\nThe integer values have no natural ordered relationship between each other and our model may not be able to harness any benefit from it. What\u2019s worse, our model will initially assume such an ordering relationship among those characters (i.e. \u201ca\u201d is 2 and \u201ce\u201d is 1 but that should not signify a relationship), which can lead to an unwanted result. We will use one-hot encoding to represent the input sequence.\n\nEach integer will be represented by a boolean array where only one element in the array will have a value of 1. The max integer value will determine the length of the boolean array in the character dictionary.\n\nIn our case, the max integer value is \u2018x\u2019: 27, so the length of a one-hot boolean array will be 28(considering the lowest value starts with 0, which is the padding).\n\nFor example, instead of using the integer value 2 to represent character \u2018a\u2019, we\u2019re going to use one-hot array [0, 0, 1, 0 \u2026\u2026.. 0].\n\nOne-hot encoding is also accessible in Keras.\n\nThe resulting one_hot_names has the shape (14157, 25, 28), which stands for (# of training samples, max sequence length, # of unique tokens)\n\nRemember we\u2019re predicting 3 color channel values, each value ranging between 0\u2013255. There is no golden rule for data normalization. Data normalization is purely practical because in practice it could take a model forever to converge if the training data values are spread out too much. A common normalization technique is to scale values to [-1, 1]. In our model we are using a ReLu activation function in the last layer. Since ReLu outputs non-negative numbers, we\u2019ll normalize the values to [0, 1].\n\nTo build our model we\u2019re going to use two types of neural networks, a feed forward neural network and a recurrent neural network. The feed forward neural network is by far the most common type of neural network. In this neural network, the information comes into the input units and flows in one direction through hidden layers until each reaches the output units.\n\nIn recurrent neural networks information can flow around in cycles. These networks can remember information for a long time. Recurrent networks are a very natural way to model sequential data. In our specific model, we\u2019re using one of the most powerful recurrent networks named long short term memory(LSTM).\n\nThe easiest way to build up a deep learning model in Keras is to use its sequential API, and we simply connect each of the neural network layers by calling its model.add() function like connecting LEGO bricks.\n\nTraining a model cannot be any easier by calling model.fit() function. Notice that we\u2019re reserving 10% of the samples for validation purpose. If it turns out the model is achieving great accuracy on the training set but much lower on the validation set, it\u2019s likely the model is overfitting. You can get more information about dealing with overfitting on my other blog: Two Simple Recipes for Over Fitted Model.\n\nLet\u2019s define some functions to generate and show the color predicted.\n\nFor a color name input, we need to transform it into the same one-hot representation. To achieve this, we tokenize characters to integers with the same tokenizer with which we processed the training data, pad it to the max sequence length of 25, then apply the one-hot encoding to the integer sequence.\n\nAnd for the output RGB values, we need to scale it back to 0\u2013255, so we can display them correctly.\n\n\u201ckeras red\u201d looks a bit darker than one we\u2019re familiar with, but anyway, that was the model proposed.\n\nIn this post, we talked about how to build a Keras model that can take any color name and come up with an RGB color value. More specifically, we looked at how to apply the one-hot encoding to character level language models, building a neural network model with a feed forward neural network and recurrent neural network.\n\nHere\u2019s a diagram to summarize what we have built in the post, starting from the bottom and showing every step of the data flow.\n\nIf you\u2019re new to deep learning or the Keras library, there are some great resources that are easy and fun to read or experiment with.\n\nTensorFlow playground: an interactive visualization of neural networks run on your browser.\n\nCoursera deep learning course: learn the foundations of deep learning and lots of practical advice.\n\nKeras get started guide: the official guide for the user-friendly, modular deep Python deep learning library.\n\nAlso, check out the source code for this post in my GitHub repo."
    },
    {
        "url": "https://hackernoon.com/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier-2ecc6c73115a?source=user_profile---------2----------------",
        "title": "Simple guide on how to generate ROC plot for Keras classifier",
        "text": "After reading the guide, you will know how to evaluate a Keras classifier by ROC and AUC:\n\nWhat are they?\n\nFrom Wikipedia: Receiver operating characteristic curve a.k.a ROC is a graphic plot illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The critical point here is \u201cbinary classifier\u201d and \u201cvarying threshold\u201d. I will show you how to plot ROC for multi-label classifier by the one-vs-all approach as well.\n\nArea Under the Curve, a.k.a. AUC is the percentage of this area that is under this ROC curve, ranging between 0~1. It\n\nWhat can they do?\n\nROC is a great way to visualize the performance of a binary classifier, and AUC is one single number to summarize a classifier\u2019s performance by assessing the ranking regarding separation of the two classes. The higher, the better.\n\nIn the following two sections, I will show you how to plot the ROC and calculate the AUC for Keras classifiers, both binary and multi-label ones.\n\nFirst, let\u2019s use Sklearn\u2019s make_classification() function to generate some train/test data.\n\nNext, let\u2019s build and train a Keras classifier model as usual.\n\nWe then call model.predict on the reserved test data to generate the probability values. After that, use the probabilities and ground true labels to generate two data array pairs necessary to plot ROC curve:\n\nWe can call sklearn\u2019s roc_curve() function to generate the two. Here is the code to make them happen.\n\nAUC value can also be calculated like this.\n\nTo make the plot looks more meaningful, let\u2019s train another binary classifier and compare it with our Keras classifier later in the same plot.\n\nNow, let\u2019s plot the ROC for the two classifiers.\n\nHere is the result:\n\nAs you can see, given the AUC metric, Keras classifier outperforms the other classifier.\n\nROC curve extends to problems with three or more classes with what is known as the one-vs-all approach.\n\nFor instance, if we have three classes, we will create three ROC curves,\n\nFor each class, we take it as the positive class and group the rest classes jointly as the negative class.\n\nLet\u2019s started by creating some train/test data with 3 class outputs.\n\nThen we build and train a categorical Keras classifier like before.\n\nAfter training the model we can use it to make predictions for test inputs and plot ROC for each of the 3 classes.\n\nBefore doing that, let\u2019s define the metric to evaluate the overall performance across all classes. There are two slightly different metrics, micro and macro averaging.\n\nIn \u201cmicro averaging\u201d, we\u2019d calculate the performance, e.g., precision, from the individual true positives, true negatives, false positives, and false negatives of the k-class model:\n\nAnd in macro-averaging, we average the performances of each individual class:\n\nHere is the code to plot those ROC curves along with AUC values.\n\nHere is the result, the second plot is a zoom-in view of the upper left corner of the graph.\n\nYou can see for each class, their ROC and AUC values are slightly different, that gives us a good indication of how good our model is at classifying individual class.\n\nIn this tutorial, we walked through how to evaluate binary and categorical Keras classifiers with ROC curve and AUC value.\n\nThe ROC curve visualizes the quality of the ranker or probabilistic model on a test set, without committing to a classification threshold. We also learned how to compute the AUC value to help us access the performance of a classifier.\n\nIf you want to know more about ROC, you can read its Wikipedia page, Receiver operating characteristic, it shows you how the curve is plotted by iterating different thresholds.\n\nAlso, it is helpful to check out Sklearn\u2019s API document on computing ROC to further understand how to use that function.\n\nYou can find the source code for this tutorial in my GitHub repo."
    },
    {
        "url": "https://hackernoon.com/gentle-guide-to-setup-keras-deep-learning-framework-and-build-a-travel-recommendation-engine-part-9c805b325a2d?source=user_profile---------3----------------",
        "title": "Gentle guide to setup Keras deep learning framework and build a travel recommendation engine (Part\u2026",
        "text": "Let\u2019s continue our journey to build a travel recommendation engine. You can find the part 1 of the series on my blog. After reading this post, you will know how to turn a model trained for classification into one that extracts image feature vectors. Then we\u2019ll walk through how to compute the similarity between two images with their feature vectors. Finally, we will generate the travel recommendation with the most similar image.\n\nFor the best learning experience, I suggest opening the Colab Notebook while reading this tutorial.\n\nThe engine we are going to build is a content-based recommendation engine. If a user likes a destination photo, then the system will show him/her a similar travel destination image.\n\nIn our previous post, the model was built to classify an input image as one of the 365 place/scene names.\n\nWe are going to remove the last 4 layers responsible for place logits generation and only keep the \u201cfeature extractor\u201d part of the network.\n\nIn Keras we can pop out the last 4 layers like this.\n\nThe last line above saves the model weight to a file for later use. Next time we just need to define the model without the 4 classifier layers and initialize the network with the saved weights.\n\nIf you run the again, you will notice that the output is no longer a vector of 365 floating point numbers. Instead, it is now a vector of 4096 floating point numbers. This is the feature vector, an abstract representation for the input image.\n\nOur recommending engine takes a query image liked by a user and recommends a similar place.\n\nThe similarity between two images is computed by measuring the distance between the two feature vectors.\n\nYou can imagine measuring the distance between two feature vectors in a 4096-dimensional space. The smaller the distance, the more similar two images to each other.\n\nWe can compute all known images feature vectors at runtime and compare with the queried image\u2019s feature vector. But this will be ineffective since we are basically computing those values again and again. An alternatively faster approach is to pre-compute those feature vectors and store them in memory. During the run-time, we only need to compute the query image\u2019s feature vector if it has not been computed before, that saves a lot of time especially when you have lots of images to compare with.\n\nHere is the function to compute an image\u2019s feature vector by calling the feature extractor model\u2019s predict function.\n\nAnd we have another function to pre-compute all known images feature vectors and store them into the memory, this only needs to be done once.\n\nHere is the function we execute during runtime to search and display the most similar image to a new query image.\n\nLet\u2019s give it a try by running the following line.\n\nAnd here is the result, our model recommends a similar photo to our queried image.\n\nIf you play with the recommending engine, you may notice it generates wrong recommendations once a while.\n\nThere are two reasons,\n\n1. The model was trained for classification and the image feature extractor part of the network was optimized for classifying images to 356 classes, not for distinguishing similar images.\n\n2. The model was trained on image datasets distributed among 365 classes of places. The training set might not have enough images for a particular type of beaches or one place at different seasons.\n\nOne solution to the first problem is to use a siamese network with triplet-loss, which is popular in face verification task. The model will be trained to identify if two images are from the same place. You can check out the video introduction on Coursera about this concept, I find it very helpful.\n\nThe solution to the second problem is to apply transfer learning to our model by \u201cfreezing\u201d some earlier convolutional layers and train the rest of the model parameters with our custom image datasets. Transfer learning is a great way to leverage the general features learned from large image datasets when training a new image model.\n\nNow, you got a taste and likely impressed by the unlimited potential of deep learning as well as getting hands-on building and running a Keras model. The journey to master any technology is not easy, deep learning is no exception. And that is what initially motivates me to create this blog site by sharing and teaching what I have learned along the way to become better at applying deep learning to real-life problems. Don\u2019t hesitate to reach out to me personally if you are looking for a solution or simply saying hello.\n\nFind me on GitHub, LinkedIn, WeChat, Twitter or Facebook."
    },
    {
        "url": "https://hackernoon.com/gentle-guide-to-setup-keras-deep-learning-framework-and-build-a-travel-recommendation-engine-part-88d2ed4274d9?source=user_profile---------4----------------",
        "title": "Gentle guide to setup Keras deep learning framework and build a travel recommendation engine (Part\u2026",
        "text": "TL; DR. This post will get you started with the Keras deep learning framework without installation hassles. I will show you how easy it is to run your code on the cloud for free.\n\nI know there are lots of tutorials out there to get you up and running with deep learning in Keras. They normally go with an image classifier for the MNIST handwritten digits or cat/dog classification. Here I wanted to take a different but maybe a more interesting approach by showing you how to build a model that can recommend a place you might be interested in given a source image you like.\n\nIf you come from a background of programming in general, you might have once be suffered from the pain when you were first starting something new.\n\nInstalling an IDE, library dependencies, hardware driver support\u2026 And they might have cost you a lot of time before the first successful run of \u201cHello World!\u201d.\n\nDeep learning is the same, it depends on lots of things to make the model working. For example, in order to have a deep learning model to train and run faster, you need a graphics card. Which could easily take several hours for beginners to setup, let alone that you would have to choose and purchase the graphics card itself which can be quite costly.\n\nToday you can eliminate the initial learning curve of deep learning. It is now possible to run your code entirely in the cloud with all necessary dependencies pre-installed for you. More importantly, you can run your model faster on a graphics card for free.\n\nAt this point, I\u2019d like to introduce Google Colab since I found it very useful to share my deep learning code with others where they can reproduce the result in a matter of seconds.\n\nAll you need is a Gmail account and an internet connection. The heavy lifting computation will be handled by Google Colab servers.\n\nYou will need to get comfortable with Jupyter notebook environment on Colab. Which is quite easy, you tap the play button at the left side of a cell to run the code inside. You can run a cell multiple times if you want.\n\nLet\u2019s supercharge the running speed of a deep learning model by activating the GPU on colab.\n\nClick on the \u201cRuntime\u201d menu button, then \u201cChange runtime type\u201d, choose GPU in the \u201cHardware accelerator\u201d dropdown list.\n\nWe are ready for the journey! Now buckle up since we are going to enter the wild west of the deep learning world.\n\nThe model we are introducing can tell which places an image contains.\n\nOr described more formally, the input of the model is the image data and the output will be a list of places with different probabilities. The higher the probability, the more likely the image contains the corresponding scene/place.\n\nThe model can classify 365 different places, including coffee shop, museum, outdoor etc.\n\nHere is Colab notebook for this tutorial, you can experiment with it while reading this article. Keras_travel_place_recommendation-part1.ipynb\n\nThe most important building block of our model is the convolutional network which will play the role of extracting image features. From more general low-level features like edges/corners to more domain specific high-level features like patterns and parts.\n\nThe model will have several blocks of convolutional networks stacked one over another. The deeper the convolutional layer, the more abstract and higher level features it extracts.\n\nHere are an images showing the idea.\n\nNow enough with the intuition of convolutional network. Let\u2019s get our hands dirty by building a model to make it happen.\n\nIt is really easy to build a custom deep learning model with Keras framework. Keras is designed for human beings, not machines. It is also an official high-level API for the most popular deep learning library \u2014 TensorFlow. If you just get started and look for a deep learning framework. Keras is the right choice.\n\nDon\u2019t panic if it is your first time seeing a Keras model code below. Actually, it is quite simple to understand. The model has several blocks of convolutional layers. Each block as we explained earlier extract different levels of image features. For example \u201cBlock 1\u201d being at the input level, it extracts entry-level features like edges and corners. The deeper it goes, the more abstract features each block extracts. You also noticed the final classification block formed by two fully connected Dense layers, they are responsible for making a final prediction.\n\nLet\u2019s have the model predict labels for an image.\n\nThe model expects a fixed shape of image input which is 244 x 244 pixels with three color channels(RGB). But what if we have another image with different resolution? Keras has some helper functions come in handy.\n\nThe code below turns an image into the data array, followed by some data value normalization before feeding to the model.\n\nThen we feed the processed array of shape (3, 244, 244) to the model, the \u2018preds\u2019 variable is a list of 365 floating point numbers corresponding to 365 places/scenes.\n\nTake the top 5 predictions and map their indexes to the actual names of places/scenes.\n\nAnd here is the result.\n\nFeel free to try with other images.\n\nWe have learned how easy it is to get a deep learning model that predicts places/scenes up and running quickly with Google Colab. Read the second part of the tutorial, I am going to show you how to extract raw features from images and use that to build a travel recommendation engine.\n\nAt the meanwhile check out some resources that might be helpful.\n\nIf you want to upload your custom images to Colab, read the section \u201cPredict with Custom Images\u201d in one of my previous posts."
    },
    {
        "url": "https://hackernoon.com/how-to-deal-with-vanishing-exploding-gradients-in-keras-b4ab6e5f3a0a?source=user_profile---------5----------------",
        "title": "How to deal with Vanishing/Exploding gradients in Keras",
        "text": "If you have trained your deep learning model for a while and its accuracy is still quite low. You might want to check if it is suffering from vanishing or exploding gradients.\n\nBackprop has difficult changing weights in earlier layers in a very deep neural network. During gradient descent, as it backprop from the final layer back to the first layer, gradient values are multiplied by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero. As a result, the network cannot learn the parameters effectively.\n\nUsing a very deep network can represent very complex functions. It can learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). For example, earlier ImageNet model like VGG16 and VGG19 are striving to achieve higher image classification accuracy by adding more layers. But the deeper the network becomes, the harder it is to update earlier layers\u2019 parameters.\n\nVanishing gradients also appear in the sequential model with the recurrent neural network. Causing them ineffective in capturing long-range dependencies.\n\nTake this sentence as an example. The model is trained to generate a sentence.\n\nThe RNN needs to remember the word \u2018cats\u2019 as a plural in order to generate the word \u2018they\u2019 in the later sentence.\n\nHere is an unrolled recurrent network showing the idea.\n\nCompared to vanishing gradients, exploding gradients is more easy to realize. As the name \u2018exploding\u2019 suggests. During training, it causes the model\u2019s parameter to grow so large so that even a very small amount change in the input can cause a great update in later layers\u2019 output. We can spot the issue by simply observing the value of layer weights. Sometimes it overflows and the value becomes NaN.\n\nIn Keras you can view a layer\u2019s weights as a list of Numpy arrays.\n\nWith the understanding how vanishing/exploding gradients might happen. Here are some simple solutions you can apply in Keras framework.\n\nThe vanilla recurrent neural network doesn\u2019t have a sophisticated mechanism to \u2018trap\u2019 long-term dependencies. On the contrary, modern RNN like LSTM/GRU introduced the concepts of \u201cgates\u201d to artificially retain those long-term memories.\n\nTo put it simply, in GRU(Gated Recurrent Unit), there are two \u201cgates\u201d.\n\nOne gate called the update gate decides whether to update current memory cell with the candidate value. The candidate value is computed by previous memory cell output and current input. As compared in vanilla RNN which this candidate value will be used directly to replace the memory cell value.\n\nThe second gate is the relevant gate tells how relevant previous memory cell output is to compute the current candidate value.\n\nIn Keras, it is very trivial to apply LSTM/GRU layer to your network.\n\nHere is a minimal model contains an LSTM layer can be applied to sentiment analysis.\n\nAnd if you are struggling to choose LSTM or GRU. LSTM is more powerful to capture long-range relations but computationally more expensive than GRU. In most case, GRU should be enough for the sequential processing. For example, if you just want to quickly train a model as a proof of concept, GRU is the right choice. While you want to improve an existing model\u2019s accuracy, you can then replace the existing RNN with LSTM and train for a longer time.\n\nThe idea of the residual network is to allow direct backprop to earlier layers through a \u201cshortcut\u201d or \u201cskip connection\u201d.\n\nThe detailed implementation of ResNet block is beyond the scope of this article but I am going to show you how easy to implement an \u201cidentity block\u201d in Keras. \u201cIdentity\u201d means the block input activation has the same dimension as the output activation.\n\nAnd here is the Keras code for this identity block.\n\nThere is another ResNet block called convolution block, you use it when the input and output dimensions don\u2019t match up.\n\nWith the necessary ResNet blocks ready, we can stack them together to form a deep ResNet model like the ResNet50 you can easily load up with Keras.\n\nSigmoid function squeezes the activation value between 0~1. And Tanh function squeezes the activation value between -1~1.\n\nAs you can see, as the absolute value of the pre-activation gets big(x-axis), the output activation value won\u2019t change much. It will be either 0 or 1. If the layer gets stuck in that state, the model refuses to update its weights.\n\nOn the other hand here is the ReLu activation function.\n\nFor a randomly initialized network, only about 50% of hidden units are activated (having a non-zero output). This is known as sparse activation.\n\nYou might worry about the zero section of ReLu, it could shut down a neural entirely. However, experimental results tend to contradict that hypothesis, suggesting that hard zeros can actually help supervised training. We hypothesize that the hard non-linearities do not hurt so long as the gradient can propagate along some paths.\n\nAnother benefit of ReLu is it is easy to implement, only comparison, addition and multiplication are needed. So it is more computationally effective.\n\nTo apply a ReLu in Keras is also very easy.\n\nKeras default weight initializer is glorot_uniform aka. Xavier uniform initializer. Default bias initializer is \u201czeros\u201d. So we should be good to go by default.\n\nAs this name suggests, gradient clipping clips parameters\u2019 gradients during backprop by a maximum value or maximum norm.\n\nBoth ways are supported by Keras.\n\nL2 norm applies \u201cweight decay\u201d in the cost function of the network. Its effect is controlled by the parameter \u03bb, as \u03bb gets bigger, weights of lots of neurons are very small, effectively making them less effective, as a result making the model more linear.\n\nUse Tanh activation function example, when the activation value is small, the activation will be almost linear\n\nIn Keras, usage of regularizers can be as easy as this,\n\nIn this article, we start by understanding what is vanishing/exploding gradients followed by the solutions to handle the two issues with Keras API code snippets.\n\nOther Keras weight Initializers to take a look. https://keras.io/initializers/"
    },
    {
        "url": "https://heartbeat.fritz.ai/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2-65fe59ac12d?source=user_profile---------6----------------",
        "title": "Gentle guide on how YOLO Object Localization works with Keras (Part 2)",
        "text": "Editor\u2019s Note: Part 1 of this series was published in Hacker Noon. Check it out here.\n\nWelcome back to the second part of this series. In this section, we\u2019ll dive into the YOLO object localization model.\n\nAlthough you\u2019ve probably heard the acronym YOLO before, this one\u2019s different. For the purposes of this post, YOLO stands for \u201cYou Only Look Once\u201d.\n\nWhy \u201clook once\u201d you may wonder? Because there are other object location models that look \u201cmore than once,\u201d as we will talk about later. The \u201clook once\u201d feature of YOLO, as you already expected, makes the model run super fast.\n\nIn this section, we\u2019ll introduce a few concepts: some are unique to the YOLO algorithm and some are shared with other object location models.\n\nThe concept of breaking down the images to grid cells is unique in YOLO, as compared to other object localization solutions. For example, in reality, one image can be cut to 19 x 19 grid cells. But for the purpose of this explanation, we\u2019re going to use a 3 x 3 grid.\n\nIn the image above we have two cars, and we marked their bounding boxes in red.\n\nNext, for each grid cell, we have the following labels for training. Same as we showed earlier in Part 1 of the series.\n\nSo how do we associate objects to individual cells?\n\nFor the rightmost car, it\u2019s easy. It belongs to the middle right cell since its bounding box is inside that grid cell.\n\nFor the truck in the middle of the image, its bounding box intersects with several grid cells. The YOLO algorithm takes the middle point of the bounding box and associates it to the grid cell containing it.\n\nAs a result, here are the output labels for each grid cell.\n\nNotice that for those grid cells with no object detected, it\u2019s pc = 0 and we don\u2019t care about the rest of the other values. That\u2019s what the \u201c?\u201d means in the graph.\n\nAnd the definition of the bounding box parameter is defined as follows:\n\nFor the class labels, there are 3 types of targets we\u2019re detecting,\n\nWith \u201ccar\u201d belonging to the second class, so c2 = 1 and other classes = 0.\n\nIn reality, we may be detecting 80 different types of targets. As a result, each grid cell output y will have 5 + 80 = 85 labels instead of 8 as shown here.\n\nWith that in mind, the target output combining all grid cells have the size of 3 x 3 x 8.\n\nBut there\u2019s a limitation with only having grid cells.\n\nSay we have multiple objects in the same grid cell. For instance, there\u2019s a person standing in front of a car and their bounding box centers are so close. Shall we choose the person or the car?\n\nTo solve the problem, we\u2019ll introduce the concept of anchor box.\n\nAnchor box makes it possible for the YOLO algorithm to detect multiple objects centered in one grid cell.\n\nNotice that, in the image above, both the car and the pedestrian are centered in the middle grid cell.\n\nThe idea of anchor box adds one more \u201cdimension\u201d to the output labels by pre-defining a number of anchor boxes. So we\u2019ll be able to assign one object to each anchor box. For illustration purposes, we\u2019ll choose two anchor boxes of two shapes.\n\nEach grid cell now has two anchor boxes, where each anchor box acts like a container. Meaning now each grid cell can predict up to 2 objects.\n\nBut why choose two anchor boxes with two different shapes \u2014 does that really matter? The intuition is that when we make a decision as to which object is put in which anchor box, we look at their shapes, noting how similar one object\u2019s bounding box shape is to the shape of the anchor box. For the above example, the person will be associated with the tall anchor box since their shape is more similar.\n\nAs a result, the output of one grid cell will be extended to contain information for two anchor boxes.\n\nFor example, the center grid cell in the image above now has 8 x 2 output labels in total, as shown below.\n\nAnother reason for choosing a variety of anchor box shapes is to allow the model to specialize better. Some of the output will be trained to detect a wide object like a car, another output trained to detect a tall and skinny object like a pedestrian, and so on.\n\nIn order to have a more formal understanding of the intuition of \u201csimilar shape\u201d, we need to understand how it\u2019s evaluated. That is where the Intersection over Union \u2014 comes into play.\n\nThe concept of Intersection over Union (IoU) is quite simple. It\u2019s frequently used as an evaluation metric to measure the accuracy of an object localizer.\n\nBut implementing it may be a bit nerve-racking. So let\u2019s walk through a simple IoU implementation in detail.\n\nInstead of defining a box by its center point, width and height, let\u2019s define it using its two corners (upper left and lower right): (x1, y1, x2, y2)\n\nTo compute the intersection of two boxes, we start off by finding the intersection area\u2019s two corners. Here\u2019s the idea:\n\nThen, to compute the area of the intersection, we multiply its height by its width.\n\nTo get the union of two boxes, we use the following equation:\n\nHere is a function to compute IoU:\n\nThe usefulness of Intersection over Union(IoU) is not only limited to when we\u2019re assigning anchor boxes when preparing training datasets. It\u2019s also very important during the prediction. Let\u2019s take a look at the popular non-max suppression algorithm in the object localization task.\n\nNon-max suppression is a common algorithm used for cleaning up when multiple boxes are predicted for the same object.\n\nIn our previous illustration, we use 3 x 3 bounding boxes. In reality, 19 x 19 bounding boxes are used to achieve a more accurate prediction. As a result, it\u2019s more likely to have multiple boxes predicted for the same object.\n\nFor the example below, the model outputs three predictions for the truck in the center. There are three bounding boxes, but we only need one. The thicker the predicted bounding box, the more confident the prediction is \u2014 that means a higher pc value.\n\nOur goal is to remove those \u201cshadow\u201d boxes surrounding the main predicted box.\n\nThat is what non-max suppression does in 3 steps:\n\nAnd finally, the cleaned up prediction looks like this:\n\nThe YOLO model should now be ready to be trained with lots of images and lots of labeled outputs. Like the COCO dataset. But you won\u2019t want to do that, since there\u2019s a pre-trained model ready for us to play with!\n\nBefore we get into the fun part, let\u2019s look at how the YOLO model makes predictions.\n\nGiven an image, the YOLO model will generate an output matrix of shape (3, 3, 2, 8). Which means each of the grid cells will have two predictions, even for those grid cells that don\u2019t have any object inside.\n\nBefore applying non-max Suppression to clean up the predictions, there is another step to reduce the final output boxes by filtering with a threshold by \u201cclass scores\u201d.\n\nThe class scores are computed by multiplying pc with the individual class output (C1, C2, C3).\n\nSo here is the graph illustrating the prediction process. Note that before \u201cfilter by class scores\u201d, each grid cell has 2 predicted bounding boxes.\n\nWith the previous concepts in mind, you\u2019ll feel confident reading the YOLO model code.\n\nThe full source code is available in my GitHub repo. Let\u2019s take a look at some parts worth mentioning.\n\nInstead of implementing our own IoU and non-max suppression, TensorFlow has its ready for use.\n\nBut wait, are we using a Keras model?\n\nNot to worry. This time we\u2019re using Keras backend API, which allows Keras modules you write to be compatible with TensorFlow API, so all TensorFlow operators are at our disposal.\n\nThe TensorFlow+Keras implementation of non-max suppression can look like this. Note we are using a Keras variable and a TensorFlow operator here.\n\nThe input and output of the function are mostly input and output tensors. The output tensors can become input for another similar function, flowing to the downstream of the pipeline. Guess what? That\u2019s where TensorFlow gets its name from.\n\nAnd here is the code to construct the computation graph for prediction:\n\nWe can then run the TensorFlow session to compute the output and finally draw predicted bounding boxes on the image.\n\nHere\u2019s the output generated with a photo I took a while ago:\n\nIn this article, we walked through some key concepts that make the YOLO object localization algorithm work fast and accurately. Then we went through some highlights in the YOLO output pipeline implementation in Keras+TensorFlow.\n\nBefore wrapping up, I want to bring up 2 limitations of the YOLO algorithm.\n\nYOLO: Real-Time Object Detection. This page contains a downloadable pre-trained YOLO model weights file. I will also include instructions on how to use it in my GitHub repo.\n\nAllan Zelener \u2014 YAD2K: Yet Another Darknet 2 Keras. The Keras+TensorFlow implementation was inspired largely by this repo.\n\nThere are other competitive object localization algorithms like Faster-CNN and SSD. They share some key concepts, as explained in this post. Stay tuned for another article to compare these algorithms side by side."
    },
    {
        "url": "https://hackernoon.com/how-to-run-object-detection-and-segmentation-on-a-video-fast-for-free-d3291076af76?source=user_profile---------7----------------",
        "title": "How to run Object Detection and Segmentation on a Video Fast for Free",
        "text": "TL;DR. After reading this post, you will learn how to run state of the art object detection and segmentation on a video file Fast. Even on an old laptop with an integrated graphics card, old CPU, and only 2G of RAM.\n\nSo here is the catch. This will only work if you have an internet connection and own a Google Gmail account. Since you are reading this, it is very likely you are already qualified.\n\nAll the code in the post runs entirely in the cloud. Thanks to Google\u2019s Colaboratory a.k.a. Google Colab!\n\nI am going to show you how to run our code on Colab with a server-grade CPU , > 10 GB of RAM and a powerful GPU for FREE! Yes, you hear me right.\n\nColab was build to facilitate machine learning professionals collaborating with each other more seamlessly. I have shared my Python notebook for this post, click to open it.\n\nLog in to your Google Gmail account on the upper right corner if you haven\u2019t done so. It will ask you to open it with Colab at the top of the screen. Then you are going to make a copy so you can edit it.\n\nNow you should be able to click on the \u201cRuntime\u201d menu button to choose the version of Python and whether to use GPU/CPU to accelerate the computation.\n\nThe environment is all set. So easy isn\u2019t it? No hassles like installing Cuda and cudnn to make GPU working on a local machine.\n\nRun this code to confirm TensorFlow can see the GPU.\n\nGreat, We are good to go!\n\nIf you are curious about what the GPU model you are using. It is a Nvidia Tesla K80 with 24G of memory. Quite powerful.\n\nRun this code to find out yourself.\n\nYou will see the 24G graphics memory does help later. It makes possible to process more frames at a time to accelerate the video processing.\n\nThe demo is based on the Mask R-CNN GitHub repo. It is an implementation of Mask R-CNN on Keras+TensorFlow. It not only generates the bounding box for a detected object but also generate a mask over the object area.\n\nMask R-CNN has some dependencies to install before we can run the demo. Colab allows you to install Python packages through pip, and general Linux package/library through apt-get.\n\nIn case you don\u2019t know yet. Your current instance of Google Colab is running on an Ubuntu virtual machine. You can run almost every Linux command you normally do on a Linux machine.\n\nMask R-CNN depends on pycocotools, we are installing it with the following cell.\n\nIt clones the coco repository from GitHub. Install build dependencies. Finally, build and install the coco API library.\n\nAll this happens in the cloud virtual machine, and quite fast.\n\nWe are now ready to clone the Mask_RCNN repo from GitHub and cd into the directory.\n\nNotice how we change directory with Python script instead of running a shell \u2018cd\u2019 command. Since we are running Python in current notebook.\n\nNow you should be able to run the Mask R-CNN demo on colab like you would on a local machine. So go ahead and run it in your Colab notebook.\n\nSo far those sample images came from the GitHub repo. But how do you predict with custom images?\n\nIn order to upload a image to Colab notebook, there are three options that I think of.\n\n1. Use a Free image hosting provider like the imgbb.\n\n2.Create a GitHub repo, then download the image link from colab.\n\nAfter uploading images by either of those two options, you will get a link to the image. Which can be downloaded to your colab VM with Linux wget command. It downloads one image to the ./images folder.\n\nThe first two options will be ideal if you just want to upload 1 or 2 images and don\u2019t care other people on the internet also be able to see it given the link.\n\nThe option is ideal if you have private images/videos/other files to be uploaded to colab.\n\nRun this block to authenticate the VM to connect to your Google Drive.\n\nIt will ask for two verification code during the run.\n\nThen execute this cell to mount the Drive to the directory \u2018drive\u2019\n\nYou can now access your Google drive content in directory ./drive\n\nHope you are having fun so far, why not try this on a video file?\n\nProcessing a video file will take three steps.\n\nIn our previous demo, we ask the model to process just one image at a time. As configured in the .\n\nIf we are going to process the whole video one frame at a time, it will take a long time. So instead we are going to leverage GPU to process multiple frames in parallel.\n\nThe pipeline of Mask R-CNN is quite computationally intensive and takes a lot of GPU memory. I find the Tesla K80 GPU on Colab with 24G of memory can safely process 3 images at a time. If you go beyond that the notebook might crash in the middle of processing the video.\n\nSo in the code below, we set the to 3 and use cv2 library to stage 3 images at a time before processing them with the model.\n\nAfter running this code, you should now have all processed image files in one folder .\n\nThe next step is easy, we just need to generate the new video from those images. We are going to use cv2\u2019s VideoWriter to accomplish this.\n\nBut two things you want to make sure:\n\n1. The frames need to be ordered in the same way as they are extracted from the original video. (Or backward if you prefer to watch the video that way)\n\n2.The frame rate matches the original video. You can use the following code to check the frame rate of a video or just open the file property.\n\nFinally here is the code to generate the video from processed image frames.\n\nIf you have gone this far, the processed video should now be ready to be downloaded to your local machine.\n\nFree free to try your favorite video clip. Maybe intentionally decrease the frame rate when reconstructing the video to watch it in slow motion.\n\nIn the post, we walked through how to run your model on Google Colab with GPU acceleration.\n\nYou have learned how to do object detection and segmentation on a video. Thanks to the powerful GPU on Colab, made it possible to process multiple frames in parallel to speed up the process.\n\nIf you want to learn more about the technology behind the object detection and segmentation algorithm.\n\nHere is the original paper of Mask R-CNN goes through the detail of the model.\n\nOr if you just get started with objection detection, check out my object detection/localization guide series goes through important basics shared between many models.\n\nHere again the Python notebook for this post, and GitHub repo for your convenience.\n\nLike this article? Consider \ud83d\udc4f and share with more people."
    },
    {
        "url": "https://hackernoon.com/can-you-trust-a-keras-model-to-distinguish-african-elephant-from-asian-elephant-cd1cd3ed58a6?source=user_profile---------8----------------",
        "title": "Can you trust a Keras model to distinguish African elephant from Asian elephant?",
        "text": "I cannot lie but I just learned to distinguish an African elephant from an Asian elephant not long ago.\n\nOn the other hand, the state of the art ImageNet classification model can detect 1000 classes of objects at an accuracy of 82.7% including those two types of elephants of course. ImageNet models are trained over 14 millions of images and can figure out the differences between objects.\n\nHave you wondered where does the model focusing on when looking at images, or shall we ask should we trust the model instead?\n\nThere are two approaches we can take to solve the puzzle.\n\nThe hard way. Crack open the state of the art ImageNet model by studying the paper, figuring out the math, implementing the model and hopefully, in the end, understand how it works.\n\nThe easy way. Become model agnostic, we treat the model as a black box. We have control of the input image, so we tweak it. We change or hide some parts of the image that make sense to us. Then we feed the tweaked image to the model and see what it think of it.\n\nThe second approach is what we will be experimenting with and it has been made easy by this wonderful Python library \u2014 LIME, short for Local Interpretable Model-Agnostic Explanations.\n\nInstall it with pip as usual,\n\nThere are many Keras models for image classification with weights pre-trained on ImageNet. You can pick one here at Available models.\n\nI am going to try my luck with InceptionV3. It might take a while to download the pre-trained weight for the first time.\n\nWe choose the photo with two elephants walking side by side, it\u2019s a great example to test our model with.\n\nThe code pre-processes the image for the model and the model does the prediction.\n\nThe output is not so surprising, the Asian a.k.a the India elephant is standing in front, taking quite a lot of space of the image, no wonder it gets the highest score.\n\nThe model is making the right prediction, now let\u2019s ask it to give us an explanation.\n\nWe do this by first creating a LIME explainer, it only asks for our test image and function.\n\nLet\u2019s see what the model is looking at to predict the Indian elephant. The function will output probabilities for each class indexes ranging from 0~999.\n\nAnd the LIME explainer need to know which class by class index we want an explanation from.\n\nIt turns out the class index of \u201cIndian_elephant\u201d equals to 385. Let\u2019s ask the explainer to show the magic where the model is forcing on.\n\nThis is interesting, the model is also paying attention to the small ear of the Indian elephant.\n\nWhat about the African elephant?\n\nCool, the model is also looking at the big ear of the African elephant when predicting it. It is also looking at part of the text \u201cAFRICAN ELEPHANT\u201d. Could it be a coincidence or the model is smart enough to figure out clue by reading annotations on the image?\n\nAnd Finally, let\u2019s take a look at what are the \u2018pros and cons\u2019 when the model is predicting an Indian elephant.\n\nLooks like the model is focusing on what we do as well.\n\nSo far, you might be still agnostic about how the model works, but at least developed some kind of trust towards it. That might not be a bad thing since now you have one more tool to help you distinguish a good model from a bad one. An example bad model might be focusing on the non-sense background when predicting an object.\n\nI am bearly scratching the surface of what LIME can do, I encourage you to explore on other applications like a text model where LIME will tell you what part of the text the model is focusing on when making a decision.\n\nNow go ahead, re-exam some deep learning models before they betray you.\n\nMy full source code for this experiment is available here in my GitHub repository."
    },
    {
        "url": "https://hackernoon.com/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-1-aec99277f56f?source=user_profile---------9----------------",
        "title": "Gentle guide on how YOLO Object Localization works with Keras (Part 1)",
        "text": "TL: DR, We will dive a little deeper and understand how the YOLO object localization algorithm works.\n\nI have seen some impressive real-time demos for object localization. One of them is with TensorFlow Object Detection API, you can customize it to detect your cute pet \u2014 a raccoon.\n\nHaving played around with the API for a while, I began to wonder why the object localization works so well.\n\nFew online resources explained it to me in a definitive and easy to understand way. So I decided to write this post my own for anyone who is curious about how object localization algorithm works.\n\nThis post might contain some advanced topics but I will try to explain it as beginner friendly as possible.\n\nYou might have heard of ImageNet models, they are doing really well on classifying images. One model is trained to tell if there is a specific object such as a car in a given image.\n\nAn object localization model is similar to a classification model. But the trained localization model also predicts where the object is located in the image by drawing a bounding box around it. For example, a car is located in the image below. The information of the bounding box, center point coordinate, width and, height is also included in the model output.\n\nLet\u2019s see we have 3 types of targets to detect\n\nFor the classification model, the output will be a list of 3 numbers representing the probability for each class. For the image above with only a car inside the output may look like . The second class which is the car has the largest probability.\n\nThe output of the localization model will include the information of the bounding box, so the output will look like\n\nAt this point, you may have come up an easy way to do object localization by applying a sliding window across the entire input image. Kindly like we use a magnifier to look one region of a map at a time and find if that region contains something that interests us. This method is easy to implement and don\u2019t even need us to train another localization model. Since we can just use a popular image classifier model and have it look at every selected region of the image and output the probability for each class of target.\n\nBut this method is very slow since we have to predict on lots of regions and try lots of box sizes to in order to have a more accurate result. It is computationally intensive so it is hard to achieve good real-time object localization performance as required in an application like self-driving cars.\n\nHere is a trick to make it a little bit faster.\n\nIf you are familiar with how convolutional network works, it can resemble the sliding window effect by holding the virtual magnifier for you. As a result, it generates all prediction for a given bounding box size in one forward pass of the network. Which is more computationally efficient. But still, the position of the bounding box is not going to be very accurate based on how we choose the stride and how many different sizes of bounding boxes to try.\n\nIn the image above, we have the input image with shape 16 x 16 pixels and 3 color channels(RGB). Then the convolutionalonal sliding window shown in the upper left blue square with size 14 x 14 and stride of 2. Meaning the window slide vertically or horizontally 2 pixels at a time. The upper left corner in the output gives you the upper left 14 x 14 image result. If any of the 4 types of target objects are detected in the 14 x 14 section.\n\nIf you are not completely sure what I just talked about the convolutional implementation of the sliding window, no problem. Because the YOLO algorithm we explain later will handle them all.\n\nOne apparently application, self-driving car, real-time detecting and localizing other cars, road signs, bikes are critical.\n\nWhat else can it do?\n\nWhat about a security camera to track and predict the movement of a suspected person entering your property?\n\nOr in a fruit packaging and distribution center. We can build an image based volume sensing system. It may even use the size of the bounding box to proximate the size of an orange on the conveyer belt and do some smart sorting.\n\nCan you think of some other useful application for object localization? Please share your cool ideas below!\n\nThe second part in the series \u201cGentle guide on how YOLO Object Localization works with Keras (Part 2)\u201d."
    },
    {
        "url": "https://hackernoon.com/how-to-do-real-time-trigger-word-detection-with-keras-b8a56ab106b7?source=user_profile---------10----------------",
        "title": "How to do Real Time Trigger Word Detection with Keras",
        "text": "I just finished the Coursera deep learning online program this week. The last programming assignment is about trigger word detection, aka. wake/hot word detection. Like when you yell at Amazon Alexa or Google Home to wake them up.\n\nWill it be cool to build one yourself and run it in Real-time?\n\nIn this post, I am going to show you exactly how to build a Keras model to do the same thing from scratch. No third party voice API or network connection required to make it functional.\n\nA lot of background information is shown in the Coursera course. Don\u2019t worry if you are new to this, I am going to have an overview just enough for you to understand what is happening next.\n\nFor the sake of simplicity, let\u2019s take the word \u201cActivate\u201d as our trigger word.\n\nThe training dataset needs to be as similar to the real test environment as possible. For example, the model needs to be exposed to non-trigger words and background noise in the speech during training so it will not generate the trigger signal when we say other words or there is only background noise.\n\nAs you may expect training a good speech model requires a lot of labeled training samples. Do we just have to record each audio and label where the trigger words were spoken? Here is a simple trick to solve this problem.\n\nFirst, we have 3 types of audio recordings,\n\n1. Recordings of different backgrounds audios. They might just as simple as two clips of background noise, 10 seconds each, coffee shop, and living room.\n\n2. Recordings of the trigger word \u201cactivate\u201d. They might be just you speaking the word 10 times in different tones, 1 second each.\n\n3. Recordings of the negative words. They might be you speaking other words like \u201cbaby\u201d, \u201ccoffee\u201d, 1 second for each recording.\n\nHere is the step to generate the training input audio clips,\n\nWe choose overlay since we want to mix the spoken words with the background noise to sounds more realistic.\n\nFor the output labels, we want it to represent whether or not someone has just finished saying \u201cactivate\u201d.\n\nWe first initialize all timesteps of the output labels to \u201c0\u201ds. Then for each \u201cactivate\u201d we overlayed, we also update the target labels by assigning the subsequent 50 timesteps to \u201c1\u201ds.\n\nWhy we have 50 timesteps \u201c1\u201ds?\n\nBecause if we only set 1 timestep after the \u201cactivate\u201d to \u201c1\u201d, there will be too many 0s in the target labels. It creates a very imbalanced training set.\n\nIt is a little bit of a hack to have 50 \u201c1\u201d but could make them a little bit easy to train the model. Here is an illustration to show you the idea.\n\nFor a clip which we have inserted \u201cactivate\u201d, \u201cinnocent\u201d, activate\u201d, \u201cbaby.\u201d Note that the positive labels \u201c1\u201d are associated only with the positive words.\n\nThe green/blueish plot is the spectrogram, which is the frequency representation of the audio wave over time. The x-axis is the time and y-axis is frequencies. The more yellow/bright the color is the more certain frequency is active (loud).\n\nOur input data will be the spectrogram data for each generated audio. And the target will be the labels we created earlier.\n\nWithout further due, let\u2019s take a look at the model structure.\n\nThe 1D convolutional step inputs 5511 timesteps of the spectrogram (10 seconds), outputs a 1375 step output. It extracts low-level audio features similar to how 2D convolutions extract image features. Also helps speed up the model by reducing the number of timesteps.\n\nThe two GRU layers read the sequence of inputs from left to right, then ultimately uses a dense+sigmoid layer to make a prediction. Sigmoid make the range of each label between 0~1. Being 1, corresponding to the user having just said \u201cactivate\u201d.\n\nHere is the code written in Keras\u2019 functional API.\n\nTrigger word detection takes a long time to train. To save time, Coursera\u2019ve already trained a model for about 3 hours on a GPU using the architecture shown above, and a large training set of about 4000 examples. Let\u2019s load the model.\n\nSo far our model can only take a static 10 seconds audio clip and make the prediction of the trigger word location.\n\nHere is the fun part, let\u2019s replace with the live audio stream instead!\n\nThe model we have build expect 10 seconds audio clips as input. While training another model that takes shorter audio clips is possible. But needs us retraining the model on a GPU for several hours.\n\nWe also don\u2019t want to wait for 10-second for the model tells us the trigger word is detected. So one solution is to have a moving 10 seconds audio stream window with a step size of 0.5 second. Which means we ask the model to predict every 0.5 seconds, that reduce the delay and make it responsive.\n\nWe also add the silence detection mechanism to skip making a prediction if the loudness is below a threshold. This can save some computing power.\n\nLet\u2019s see how to build it,\n\nThe input 10 seconds audio is updated every 0.5 second. Meaning for every 0.5 second, the oldest 0.5 second chunk of audio will be discarded and the fresh 0.5 second audio will be shifted in. The job of the model is to tell if there is a new trigger word detected in the fresh 0.5 second audio chunk.\n\nAnd here is the code to make it happen.\n\nTo get the audio stream, we use the pyaudio library. Which has an option to read the audio stream asynchronously. That means the audio stream recording happens in another thread and when a new fixed length of audio data is available, it notifies our model to process it in the main thread.\n\nYou may ask why not just read a fixed length of audio and just process it in one function?\n\nSince for the model to generate the prediction, it takes quite some time, sometimes measured in tens of milliseconds. Doing so we are risking creating gaps in the audio stream while we are doing the computation.\n\nHere is the code for the pyaudio library\u2019s callback, in the callback function we send a queue to notify the model to process the data in the main thread.\n\nWhen you run it, it outputs one of the 3 characters every 0.5 second.\n\n\u201c.\u201d means not silence and no trigger word,\n\nFeel free to replace printing the \u201c1\u201d character with anything you want to happen when a trigger word is detected. Launch an app, play a sound etc.\n\nHere is the demo on YouTube\n\nThis article demonstrates how to build a real-time trigger word detector from scratch with Keras deep learning framework.\n\nHere\u2019s what you should remember:\n\nNow, grab the full source code from my GitHub repo and build an awesome trigger word application."
    },
    {
        "url": "https://hackernoon.com/how-to-generate-realistic-yelp-restaurant-reviews-with-keras-c1167c05e86d?source=user_profile---------11----------------",
        "title": "How to generate realistic yelp restaurant reviews with Keras",
        "text": "TL; DR. After reading this article. You will be able to build a model to generate 5-star Yelp reviews like those.\n\nI will show you how to,\n\nTraining the model could easily take up a couple of days even on GPU. Luckily the pre-trained model weights are available. So we could jump directly to the fun part to generate reviews.\n\nThe Yelp Dataset is freely available in JSON format.\n\nAfter downloading and extracting, you will find 2 files we need in the dataset folder,\n\nThose two files are quite large, especially the review.json file (3.7 GB).\n\nEach line of the review.json file is a review of JSON string. The two files do not have the JSON start and end square brackets \u201c[ ]\u201d. So the content of the JSON file as a whole is not a valid JSON string. Plus it might be difficult to fit the whole review.json file content to the memory. So, let\u2019s first convert them to CSV format line by line with our helper script.\n\nAfter that, you will find those two files in dataset folder,\n\nThose two are valid CSV files we can open by pandas library.\n\nHere is what we are going to do. We only extract 5-stars review texts from the businesses that have \u2018Restaurant\u2019 tag in their categories.\n\nNext, let\u2019s remove the new line characters in reviews and any duplicated reviews.\n\nTo show the model where is the start and end of a review. We need to add special markers to our review texts.\n\nSo one line in the finally prepared review will look like this as you expected.\n\nThe model we are building here is a character-level language model, meaning the minimum distinguishable symbol is a character. You may also come across the word- level model where the input is the word tokens.\n\nThere are some pros and cons for the character-level language model.\n\nThe model is quite similar to the official lstm_text_generation.py demo code, except we are stacking RNN cells allows storing more information throughout the hidden states between the input and output layer. It generates more realistic Yelp reviews.\n\nBefore showing the code for the model, let\u2019s peek a little deeper on how stacking RNN works.\n\nYou may have seen in the standard neural network.(That is the Dense layers in Keras)\n\nThe first layer takes the input x to compute the activation value a[1], that stack next layer to compute the next activation value a[2].\n\nStacking RNN is a bit like the standard neural network and \u201cunrolling in time\u201d.\n\nFor notation a[l]<t> means activation asslocation for layer l, and <t> means timestep t.\n\nLet\u2019s take a look how an activation value is computed\n\nTo compute a[2]<3>, there are two input, a[2]<2> and a[1]<3>\n\ng is the activation function, wa[2] and ba[2] are the layer 2 parameters.\n\nAs we can see, to stack RNNs. The previous RNN need to return all the timesteps a<t>to the subsequent RNN.\n\nBy default, an RNN layer such as LSTM in Keras only returns the last timestep activation value a<T>. In order to return all timesteps\u2019 activation values, we set the parameter to .\n\nSo here is how we build the model in Keras. Each input sample is a one-hot representation of 60 characters, there are total 95 possible characters.\n\nEach output is a list of 95 predicted probabilities for each character.\n\nAnd here is the graphical model structure to help you visualize it.\n\nThe idea to train the model is simple, we train it with the input/output pair. Each input is 60 characters, and the corresponding output is the immediately following character.\n\nIn the data preparing step, we created a list of clean 5-star reviews text. Total 1,214,016 lines of reviews. To simplify the training, we are only going to train on reviews equal or less than 250 characters long. Which end up with 418,955 lines of reviews.\n\nThen we shuffle the order of the reviews so we don\u2019t train on 100 reviews for the same restaurant in a row.\n\nWe read all reviews as a long text string. Then create a python dictionary (i.e., a hash table) to map each character to an index from 0\u201394 (total 95 unique characters).\n\nThe text corpus has a total of 72,662,807 characters. It is hard to process it as a whole. So let\u2019s break it down into chunks of 90k characters each.\n\nFor each chunk of a corpus, we are going to generate pairs of inputs and outputs. By shifting the pointer from beginning to end of the chunk, one character at a time if step set to 1.\n\nTraining one chunk for one epoch takes 219 seconds on GPU (GTX1070), so training the full corpus will take about 2 days.\n\nTwo Keras callbacks come handy, ModelCheckpoint and ReduceLROnPlateau.\n\nModelCheckpoint helps us save the weights everytime it improves.\n\nReduceLROnPlateau callback automatically reduces learning rate when the loss metric stops decreasing. The main benefit of it is that we don\u2019t need to manually tune the learning Rate. Its main weakness is that its learning rate is always decreasing and decaying.\n\nCode to train the model for 20 epochs looks like this.\n\nIt will take one month or so as you might guess. But training for about 2 hours already produces some promising results in my case. Feel free to give it a try.\n\nWhether you jump right to this section or you have read through the previous ones. Here is the fun part!\n\nWith the pre-trained model weights or one you trained by yourself, we can generate some interesting yelp reviews.\n\nHere is the idea, we \u201cseed\u201d the model with initial 60 characters and ask the model to predict the very next character.\n\nThe \u201csampling index\u201d process will add some variety to the final result by generating some randomness with the given prediction.\n\nIf the temperature is very small, it will always pick the index with highest predicted probability.\n\nTo generate 300 characters with following code\n\nIn this post, you know how to build and train a character-level text generation model from beginning to end. The source code is available on my GitHub repo as well as the pre-train model to play with.\n\nThe model shown here is trained in a many to one fashion. There is also another optional implementation in many to many fashion. Consider the input sequence as characters of length 7 \u201cThe cak\u201d and the expected output is \u201che cake\u201d. You can check it out here, char_rnn_karpathy_keras."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/how-to-multi-task-learning-with-missing-labels-in-keras-4a8df0896620?source=user_profile---------12----------------",
        "title": "How to Multi-task learning with missing labels in Keras",
        "text": "Multi-task learning enables us to train a model to simultaneously do several tasks.\n\nFor example, given a photo was taken by a self-driving car, we want to detect different things in the image. Stop sign, traffic lights, cars etc.\n\nWithout multi-task learning, we have to train model for each object we want to detect and with one output either the target object is detected or not.\n\nBut with multi-task learning, we can have one model trained only once to detect if any of the target objects are detected by having 3 output labels.\n\nThe model input is an image and the output has 3 labels, with 1 meaning a specific object is detected.\n\nFor model trained on dataset like images, training one model to do multiple tasks performs better than models trained separately to detect objects separately since lower level images features learned during training could be shared between all objects types.\n\nAnother benefit of multi-tasking learning is it allows the training data output to be partially labeled. Let\u2019s say instead of labeling previous 3 objects, we want the human labeler to labels 3 additional different objects in all given images, pedestrians, cyclists, roadblocks. He/She may eventually get tired and didn\u2019t bother to label whether or not there\u2019s a stop sign or whether or not there\u2019s a roadblock.\n\nSo the labeled training output could look like this, where we indicate unlabeled as \u201c-1\u201d.\n\nSo how can we train our model with the dataset like this?\n\nThe key is the loss function we want to \u201cmask\u201d labeled data. Meaning for unlabeled output, we don\u2019t consider when computing of the loss function.\n\nLet\u2019s walk through a concrete example to train a Keras model that can do multi-tasking. For demo purpose, we build our own toy datasets since it is simple to train and visualize the result.\n\nHere we randomly generate 100,000 data points in 2D space. Each axis is in the range between 0 to 1.\n\nFor the output Y, we have 3 labels in the following logic\n\nWe will build a model to discover such relation between X and Y,\n\nTo make the problem more complicated, we will simulate the labeler to drop some of the output labels.\n\nHere is the important part, where we define our custom loss function to \u201cmask\u201d only labeled data.\n\nThe mask will be a tensor to store 3 values for each training sample whether the label is not equal to our mask_value (-1),\n\nThen during computing the categorical cross-entropy loss, we only compute those masked losses.\n\nTraining is simple, let\u2019s first reserve the last 3000 generated data for the final evaluation test.\n\nAnd split the rest data into 90% for train and 10% for dev during training.\n\nAfter training for 2000 epochs, let\u2019s check the model performance with our reserved evaluation test data.\n\nTo help visualize what the model is thinking, let\u2019s plot its decision boundary for each of our 3 labels.\n\nLooks like our model figured out the logic between X and Y ;)\n\nIf you are not convinced of the effectiveness of our custom loss function. Let\u2019s compare them side by side.\n\nTo disable our custom loss function, simply change the loss function back to the default like this.\n\nThen run the model training and evaluation again.\n\nIt finally evaluated accuracy is only around 0.527 which is much worse than our previous model with custom loss function.\n\nCheck out the source code on my GitHub repo.\n\nWith Multi-task learning, we can train the model on a set of tasks what could benefit from having shared lower-level features.\n\nUsually, the amount of data you have for each task is quite similar.\n\nSome degree of missing labels in training data is not a problem, which can be dealt with a custom loss function to mask only labeled data.\n\nWe can do so much more, one potential dataset that came to my mind is the (MBTI) Myers-Briggs Personality Type Dataset.\n\nThe Input is the text a given person posted.\n\nWe can treat each one as a binary label.\n\nWe can allow the labeler to leave any personality type unlabeled for a given person\u2019s post.\n\nThe model should still be able to figure out the relationship between the input and output with our custom loss function.\n\nIf you have tried this, leave a comment below and let us know if it works."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/quick-notes-on-how-to-choose-optimizer-in-keras-9d3d12d09039?source=user_profile---------13----------------",
        "title": "Quick Notes on How to choose Optimizer In Keras \u2013 Chengwei Zhang \u2013",
        "text": "Use SGD+Nesterov for shallow networks, and either Adam or RMSprop for deepnets.\n\nI was taking the Course 2 Improving Deep Neural Networks from Coursera earlier this week. Week #2 for this course was about Optimization algorithms. I find it helpful to develop better intuition about how different optimization algorithms work even we are only interested in APPLY deep learning to the real-life problems.\n\nHere are some takeaways and things I have learned with some research.\n\nIn Keras, we can define it like this.\n\nMomentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.\n\nIn Keras, we can do this to have SGD + Nesterov enabled, it works well for shallow networks.\n\nIntuition how it works to accelerate gradient descent.\n\nWe\u2019d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.\n\nHere is an animated gradient descent with multiple optimizers.\n\nNotice the two momentum based optimizers (Green-Momentum, Purple-NAG) has overshooting behavior, similar to a ball rolling down the hill.\n\nNesterov momentum has slightly less overshooting compare to standard momentum since it takes the \u201cgamble->correction\u201d approach has shown below.\n\nIt makes big updates for infrequent parameters and small updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.\n\nThe main benefit of Adagrad is that we don\u2019t need to manually tune the learning Rate. Most implementations use a default value of 0.01 and leave it at that.\n\nIts main weakness is that its learning rate is always Decreasing and decaying.\n\nIt is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it.\n\nAnother thing with AdaDelta is that we don\u2019t even need to set a default learning rate."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/two-simple-recipes-for-over-fitted-model-f35e10c494d7?source=user_profile---------14----------------",
        "title": "Two Simple Recipes for Over Fitted Model \u2013 Chengwei Zhang \u2013",
        "text": "Overfitting can be a serious problem, especially with small training dataset. The model might achieve great training accuracy but when goes to the real world with new data it has never seen, it doesn\u2019t generalize the new examples very well.\n\nThe first and most intuitive solution is sure to train the model with larger and comprehensive dataset or apply data augmentation to the existing dataset, especially for images. but what if that\u2019s all the data we have?\n\nIn this post, we will explore two simple technique to deal with this issue using regularization in the deep learning models.\n\nSupposed you have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France\u2019s goalkeeper should kick the ball so that the French team\u2019s players can then hit it with their head.\n\nWe have the following 2D dataset from France\u2019s past 10 games.\n\nYour goal is to build a deep learning model to find the positions on the field where the goalkeeper should kick the ball.\n\nThis dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well.\n\nLet\u2019s train and validate our model with training and test dataset for 1000 epochs.\n\nIt achieved a final training and validation accuracy shown below. Looks like the model performs better during training than validation.\n\nAs we can see after around 600 epoch the validation loss stopped decreasing and begin to increase instead. It is a normal sign of a model begin to overfit the training datasets.\n\nTo get a clear idea of what the final trained model is \u201cthinking\u201d, let\u2019s plot its decision boundary.\n\nAs shown in the graph, the boundary is not clean and the model is trying too hard to fit those outlier samples. It could become really obvious for deep neural networks as it is capable to learn the complete relationship between data points but at the same time, if we are only training it with a small dataset, it turns to overfit on those noisy outliers.\n\nThe standard way to avoid overfitting is called L2 regularization. It consists of applying penalties on layer weights. Then the penalties are applied to the loss function.\n\nSo the finally regularized loss function will contain both the cross-entropy cost as well as the L2 regularization cost.\n\nFor example, we can calculate the L2 regularization cost for layer \u201c2\u201d as\n\nWhere \u201cW2\u201d is the weight matrix for Dense layer \u201c2\u201d. We have to do this for W2, W3, then sum them up and multiply by regularization factor which controls how strong the regularization is.\n\nIn Keras, it is very easy to apply the L2 regularization to kernel weights.\n\nWe choose the factor 0.003 for our Keras model, achieved finally train and validation accuracy of\n\nNote that the final validation accuracy is very close to the training accuracy, this is a good sign that tour model is not likely overfitting the training data.\n\nThe decision boundary is also quite clear compared to the previous model without regularization.\n\nDropout is a popularly used regularization technique in deep learning. It randomly shuts down some neurons in each iteration.\n\nThe probability of any neuron being shut down is controled by the dropout rate parameter. The idea of drop-out is that at each iteration, the model only uses a subset of the neurons, as a result, the model becomes less sensitive to any specific neuron.\n\nOne thing to keep in mind. We only apply dropout at training time since we want to use all neurons\u2019 weights learned previously for testing or inferencing. Don\u2019t worry, this is handled automatically in Keras when we are either calling , or .\n\nBut how do we choose the dropout rate parameter?\n\nShort answer: if you are not sure, 0.5 is a good starting point, since it provides the maximum amount of regularization.\n\nIt is also feasible to use different drop out rate by layer. If the preceding layer has larger weight matrix, we can apply larger dropout to it. In our example, we apply larger dropout after dense_3 layer since it has the largest weight matrix 40x40. We can apply smaller dropout after dense_2 layer, say dropout rate 0.4, since it has smaller weight matrix 20x40.\n\nLet\u2019s take a look at the result after applying the dropout to our model.\n\nThe decision is also quite smooth.\n\nWe explored two simple regularization recipes to solve deep learning model suffering from overfitting issue when training with small datasets. Regularization will drive weights to lower values. L2 regularization and Dropout are two effective regularization techniques. Check out the full source code for this post in my GitHub repo. Enjoy!"
    },
    {
        "url": "https://medium.com/@chengweizhang2012/teach-old-dog-new-tricks-train-facial-identification-model-to-understand-facial-emotion-e23969eaaa96?source=user_profile---------15----------------",
        "title": "Teach Old Dog New Tricks \u2014 Train Facial identification model to understand Facial Emotion",
        "text": "Last week, we explored using the pre-trained VGG-Face2 model to identify a person by face. Since we know the model is trained to identify 8631 celebrities and athletes.\n\nThe original dataset is large enough, then the spatial feature hierarchy learned by the pre-trained network can effectively act as a generic model for our facial identification task. Even though our new tasks might involve completely different persons\u2019 faces to identify.\n\nThere are two ways to leverage a pre-trained network: feature extraction and fine-tuning.\n\nWhat we did last week is an example of feature extraction since we choose to use the extracted features of a face to calculate the distance to another face. If the distance is small than a threshold, we identify they are from the same person.\n\nThis week, we are going to leverage the same pre-trained model to identify the emotion shown on a face, they are 7 emotions we will classify.\n\nYou can download the fer2013 dataset from Kaggle. Each picture is 48x48 pixel grayscale images of faces.\n\nThe easiest way I can think of if take the CNN feature extraction layer output and stack it on a classifier.\n\nThe classifier will be two Dense full connected layers, with the last Dense layers having output shape of 7 denoting the 7 emotions\u2019 probabilities.\n\nAnd we freeze the weight of CNN layers making them non-trainable. Since we want to keep the representations that were previously learned by the convolutional base.\n\nWhat turns out the model cannot generalize well and always predict a \u201chappy\u201d face. Why \u201chappy\u201d face, is the model like happy faces more than other faces like we do? No really.\n\nIf we take a closer look at the fer2013 training dataset. There are more happy faces for the model to train than other emotions. So if the model can only pick one emotion to predict, of course, it picks the happy face.\n\nBut why the model always pick only one face? It all comes from to the pre-train model. Since the pre-trained model was trained to identify a person no matter what emotion he/she is wearing. So the finally feature extraction layer will contain abstract information to tell different persons. That abstract information contains the size and location of a person\u2019s eye, skin color and something else. But one thing for sure, the pre-trained model don\u2019t care about is what emotion the person has.\n\nSo what we can do, can we still leverage the pre-trained model\u2019s weights to do something for us?\n\nThe answer is YES, we have to \u201cFine Tune\u201d the model.\n\nFine-tuning consists in unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in our case, the fully-connected classifier) and these top layers. This is called \u201cfine-tuning\u201d because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant our task at hand.\n\nBig convolutional network like the pre-trained resnet50 face model here has many \u201cConv blocks\u201d stacks one over another.\n\nAn example residual block is shown in the figure below.\n\nEarlier layers in the convolutional base encode more generic, reusable features. In our case, the first conv block can extract the edges, some image patterns like curves and simple shapes. Those are very generic features for different image processing tasks. Go a little deeper, conv block can extract a person\u2019s eyes and mouth which is more abstract and less generic features for different image processing tasks.\n\nThe last conv block can represent more abstract information associated with the pre-trained model task to identify a person. With this in mind let\u2019s unfreeze the weights of it and update it as we train with our face emotion images.\n\nWe unfreeze the model weights by locating the layer to start with. The layer named \u201cactivation_46\u201d in our case as shown below.\n\nWe are given 35887 48x48 pixel grayscale images of faces. And our pre-trained model is expecting 224x224 color input image.\n\nConverting all 35887 images to 224x224 size and store to RAM will take a significant amount of space. My solution is to convert and store one image at a time to a TFRecord file which we can load up later with TensorFlow with little headache.\n\nWith TFRecord as the training dataset format, it also trains faster. You can check out my previous experiment.\n\nAnd here is the code to make the conversation happen.\n\nIn the above code train_data[0] contains the list of face image arrays each with shape (48, 48) and train_data[1] is the list of actual emotion labels in one-hot format.\n\nFor example, one emotion is encoded as\n\nWith 1 on index 2, and index 2 in our mapping is emotion \u201cfear\u201d.\n\nIn order to train our Keras model with TFRecord dataset, we first need to turn it into a TF Estimator with method.\n\nWe have introduced how to write an image input function for TF Estimator in my previous post for the binary classification task. Here we have 7 categories of emotions to classify. So the input function looks a little different.\n\nThe following snippet has shown the major difference.\n\nNow we are ready to train and evaluate our model altogether by calling .\n\nIt took 2 minutes total and achieve a validation accuracy of 0.55. Which is not bad considering one face might actual consists different emotions at the same time. Being both surprised and happy for example.\n\nIn this post, we tried two different approaches on the pre-train VGG-Face2 model for emotion classification task. Feature extraction and fine-tuning.\n\nFeature extraction approach is unable to generalize the facial emotion since the original model is trained to identify different persons instead of different emotions.\n\nFine-tuning the last conv block achieved the desired result.\n\nYou might be wondering what if we fine-tuning more conv blocks, could that improve the model performance?\n\nThe more parameters we are training, the more we are at risk of overfitting. So it would be risky to attempt to train it on our small dataset.\n\nThe source code is available on my GitHub repo.\n\nI am also looking into making this a live demo. So stay tuned."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/live-face-identification-with-pre-trained-vggface2-model-a6dea3aeee62?source=user_profile---------16----------------",
        "title": "Live Face Identification with pre-trained VGGFace2 model",
        "text": "One challenge of face identification is that when you want to add a new person to the existing list. Do you retrain your network with tons of this new person\u2019s face images along with others\u2019? If we build a classification model, how can the model classify an unknown face?\n\nIn this demo, we tackle the challenge by computing the similarity of two faces, one in our database, one face image we captured on webcam.\n\nThe VGGFace model \u201cencodes\u201d a face into a representation of 2048 numbers.\n\nWe then compute the Euclidean distance between two \u201cencoded\u201d faces. If they are the same person, the distance value will be low, if they are from two different persons, the value will be high.\n\nDuring the face identification time, if the value is below a threshold, we would predict that those two pictures are the same person.\n\nThe model itself is based on RESNET50 architecture, which is popular in processing image data.\n\nLet\u2019s first take a look at the demo.\n\nThe demo source code contains two files. The first file will precompute the \u201cencoded\u201d faces\u2019 features and save the results alongside with the persons\u2019 names.\n\nThe second will be the live demo to capture frames of images from a webcam and identify if any known faces.\n\nOne standard way to add a new person to the model is to call the one-shot learning. In the one-shot learning problem, you have to learn from just one example to recognize the person again.\n\nI might be risky since this one photo could be badly lighted or the pose of the face is really bad. So my approach is by extracting faces for from a short video clip taking only contains this person and calculate the \u201cmean features\u201d by averaging all computed features for each image.\n\nYou can find my full source precompute_features.py. But here is the important parts that make the magic happen.\n\nWe have one or more video files for each person. 's method takes a video file, read it frame by frame.\n\nFor each frame, it crops the face area, then saves the face to an image file into the .\n\nIn the method, we call the VGGFace feature extractor to generate face features like this,\n\nWe do this for all people\u2019s videos. Then we extract features for those face images and calculate the \u201cmean face features\u201d for each person. Then save it to file for the demo part.\n\nSince we have already pre-computed the face features of each person in the live demo part. It only needs to load up the features file we just saved.\n\nExtract the faces, compute the features, compare them with our precomputed features to find if any matches. If we found any matching face, we draw the person\u2019s name in the frame overlay.\n\nThe method below takes the features computed from a face in webcam image and compare with each of our known faces\u2019 features\n\nIf the person\u2019s face feature is \u201cfar away\u201d from all of our known face features, we show the \u201c?\u201d sign on the final image overlay to indicate this is an unknown face.\n\nI have only included 3 people in this demo. As you can imagining, as the number of people grows, the model will likely to confuse with two similar faces.\n\nIf that happens, you could consider exploring the Siamese Network with Triplet Loss as shown in the Coursera course.\n\nFor those interested. The full source code is listed in my GitHub repo. Enjoy!\n\nClap and follow if you like this post."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/easy-real-time-gender-age-prediction-from-webcam-video-with-keras-8b12dc7c1543?source=user_profile---------17----------------",
        "title": "Easy Real time gender age prediction from webcam video with Keras",
        "text": "Have you ever being in a situation to guess another person\u2019s age? Maybe This simple neural network model can do the work for you.\n\nThe demo you will be running in a second will take a live video stream from the WebCam and tag each face it found with the age and gender. Guess how cool it could be to place one such WebCam, let\u2019s say at your front door to get an overview of all visitors\u2019 age/gender statics.\n\nI ran this model on my Windows PC with Python 3.5. It\u2019s possible to run on other OS as well.\n\nLet\u2019s have an overview how it works in general.\n\nFirst, the photo is taken from the webcam stream live by the module.\n\nSecond, we turn the image to grayscale and use the module's class to detect faces in the image\n\nThe variable faces return by the method is a list of detected face coordinates [x, y, w, h].\n\nAfter known the faces\u2019 coordinates, we need to crop those faces before feeding to the neural network model.\n\nWe add the 40% margin to the face area so that the full head is included.\n\nThen we are ready to feed those cropped faces to the model, it\u2019s as simple as calling the method.\n\nFor the age prediction, the output of the model is a list of 101 values associated with age probabilities ranging from 0~100, and all the 101 values add up to 1 (or what we call softmax). So we multiply each value with its associated age and sum them up resulting final predicted age.\n\nLast but not least we draw the result and render the image.\n\nThe gender prediction is a binary classification task. The model outputs value between 0~1, where the higher the value, the more confidence the model think the face is a male.\n\nMy complete source code as well as the link to download the pre-trained model weights is available in my GitHub repo.\n\nFor those not satisfied with the demo and have more understanding how the model is built and trained. This section is for you.\n\nThe datasets came from IMDB-WIKI \u2014 500k+ face images with age and gender labels. Each image before feeding into the model we did the same preprocessing step shown above, detect the face and add margin.\n\nThe feature extraction part of the neural network uses the WideResNet architecture, short for Wide Residual Networks. It leverages the power of Convolutional Neural Networks (or ConvNets for short) to learn the features of the face. From less abstract features like edges and corners to more abstract features like eyes and mouth.\n\nWhat unique of the WideResNet architecture is that the author decreased the depth and increased the width of original residual networks so it trained several times faster. Link to the paper here.\n\nThe possibility of the model is endless, it really depends on what data you feed into it. Say you have lots of photos labeled by attractiveness, you can teach the model to tell the hotness of a person from the webcam live stream.\n\nHere is a list of related projects, datasets for those curious."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/how-to-teach-ai-to-suggest-product-prices-to-online-sellers-6724a6e0c5be?source=user_profile---------18----------------",
        "title": "How to teach AI to suggest product prices to online sellers",
        "text": "Given the product brand, name, category and one sentence or two for short descriptions, then we can predict its price. Could it be that simple?\n\nIn this Kaggle competition, we are doing exactly the same thing. Developers around the world are fighting for the \u201cMercari Prize: Price Suggestion Challenge\u201d, and the prize money is The total amount is $ 100,000 (first place: $ 60,000, second place: $ 30,000, third place: $ 10,000).\n\nIn this post, I will walk you through building a simple model to tackle the challenge in the deep learning library Keras.\n\nIf you are new to Kaggle, in order to download the datasets, you need to register an account, totally pain-free. Once you have the account, go to the \u201cData\u201d tab in the Mercari Price Suggestion Challenge.\n\nDownload all three files to your local computer, extract and save them to a folder named \u201cinput\u201d, and at the root folder create a folder named \u201cscripts\u201d where we will start coding.\n\nRight now you should have your directories structured similarly to this.\n\nFirst, let\u2019s take some time to understand the datasets at hand\n\nThe files consist of a list of product listings. These files are tab-delimited.\n\nFor the price column, it would be problematic to feed into neural network values that all take wildly different ranges. The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult.\n\nA widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix) we apply to it.\n\nLet\u2019s take a look at the distribution of the new \u2018target\u2019 column.\n\nWe will replace contractions pair like those below, the purpose is to unify the vocabulary to make it easier to train the model.\n\nBefore we are doing this let\u2019s count how many rows contain any of the contractions in the \u201citem_description\u201d column.\n\n5 top listed below, which is no surprise.\n\nHere is the code to remove contractions for both \u2018item_description\u2019 and \u2018name\u2019 columns in train and test datasets.\n\nThe concept of missing values is important to understand in order to successfully manage data. If the missing values are not handled properly by the researcher, then he/she may end up drawing an inaccurate inference about the data.\n\nFirst take a look at how many data is missing, and which column.\n\nIn the output we know there are 42% brand_name are missing, \u201ccategory_name\u201d and \u201citem_description\u201d columns are also missing less than 1% data.\n\nSuppose the number of cases of missing values is extremely small; then, an expert researcher may drop or omit those values from the analysis. In statistical language, if the number of the cases is less than 5% of the sample, then the researcher can drop them. In our case, we can drop rows with \u201ccategory_name\u201d or \u201citem_description\u201d column missing value.\n\nBut for simplicity, let\u2019s replace all missing text values with the string \u201cmissing\u201d.\n\nThere are two text columns has special meanings,\n\nDifferent products could have same category name or brand name, so it will be helpful to create categorical columns from them.\n\nWe will use sklearn\u2019s LabelEncoder for this purpose. After the transform, we will have two more new columns \u201ccategory\u201d and \u201cbrand\u201d with an integer type.\n\nFor each unique word in the vocabulary, we will turn it to one integer to represent it. So one sentence will become a list of integers.\n\nFirst, we need to gather the vocabulary list from our text columns we are going to tokenize. i.e. those 3 columns,\n\nAnd we will use Keras\u2019 text processing Tokenizer class.\n\nThe method will train the Tokenizer to generate the vocabulary word index mapping. And the method will actually make the sequences from texts.\n\nThe words sequences generated in the previous step are in different lengths. Since the first layer in our network for those sequences is the layers. Each Embedding layer takes as input a 2D tensor of integers, of shape\n\nAll sequences in a batch must have the same length since we need to pack them into a single tensor. So sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.\n\nTo begin with, we need to choose the for each of our sequences columns. If it is too long, the model training will take forever. If it is too short, we are at the risk of truncating important information. It will be great to visualize the sequence length distribution before we make this decision.\n\nThis line of code will plot sequence length distribution for the \u201cseq_item_description\u201d column in a histogram.\n\nLet\u2019s pick the number 60 for the max sequence length since it covers up the majority sequences.\n\nIn the code below, we are using Keras\u2019 sequence processing method to pad sequences to be the same length for each column.\n\nThis will be a multi-input model for those inputs\n\nAll Inputs except \u201cshipping\u201d will first go to embedding layer\n\nFor those sequences inputs, we need to feed them to layers. layers turn integer indices (which stand for specific words) to dense vectors. It takes as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup.\n\nThe embedded sequence will then feed to the layer, like other types of recurrent networks, it is good at learning patterns in sequences of data.\n\nNon-sequential data embedded layers will just be flattened to 2 dimensions by the layer.\n\nAll layers including the \u201cshipping\u201d will then be concatenated to a big two-dimensional tensor.\n\nFollowed by several Dense layers, final output Dense layer takes \u201clinear\u201d activation regression to arbitrary price values, same as specifying None for the parameter.\n\nI like visualization, so I plot the model structure as well.\n\nIt can be done with those two lines of code if you are curious.\n\nYou need to install Graphviz executable. pip install graphviz and pydot packages before trying to plot.\n\nTraining the model is easy, let\u2019s train it for 2 epochs, is the dictionary we created earlier, mapping input names to Numpy arrays.\n\nThe Kaggle challenge page has chosen \u201cRoot Mean Squared Logarithmic Error\u201d as the loss function.\n\nThe following code will take our trained model and compute the loss value given the validation data.\n\nIf you are planning on generating the actual prices for the test datasets and try your luck on Kaggle. This block of code will reverse the feature normalization process we discussed previously and write the prices to a CSV file.\n\nWe walked through how to predict prices give multiple input features. How to preprocessing the text data, dealing with missing data and finally build, train and evaluate the model."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/top-10-deep-learning-experiences-run-on-your-browser-458a64c9625f?source=user_profile---------19----------------",
        "title": "Top 10 Deep Learning experiences run on your Browser",
        "text": "Originally published at www.dlology.com. For more practical deep learning experiences.\n\nNo coding tutorial today. Just sharing my top 10 deep learning experiences found recently that run on browsers.\n\nA game where you are challenged to draw a something, e.g. a donut, and let the neural net guess what you are drawing. My first attempt gets 4 out of 6 right. Let\u2019s see how well you can draw.\n\nUnder the hood, the model takes sequences of strokes your draw on the canvas and feed into a combination of convolution layer and recurrent network. Finally, the class digits will be generated from the softmax output layer.\n\nAnd here is the illustration of the structure of the model.\n\nYou may wonder where are those training data came from. You guessed it! It is a collection of 50 million drawings across 345 categories coming from people plays the game.\n\nAnother AI Experients from google build on the deeplearn.js library.\n\nTeach machine to recognize your gesture and trigger events, whether it is a sound or gif. Be sure to train on enough samples and different angles, otherwise, the model will likely find it hard to generalize your gestures.\n\nThis experience built on TensorFire runs neural networks in the browser using WebGL which means it is GPU-accelerated. It allows you to compete with a computer in real time through your webcam.\n\nAnother demo from TensorFilre, GPU accelerated style transfer. It takes one of your photos and turns it into an astonishing piece of art.\n\nIf you are familiar with Keras library, you may already come across its demo for style transfer which computes two losses \u201ccontent\u201d and \u201cstyle\u201d when training the model. It takes really long to generate a decent image.\n\nWhile this one running on your browser takes less than 10 seconds, make it even possible for videos.\n\nThis demo based on Google Cloud Vision API which means the browser sends a request to Google\u2019s server. Kindly like when you use Google Image search to search for similar images.\n\nThis demo uses LSTM recurrent neural networks for handwriting synthesis. the source code is available.\n\nYou sketch a cat, the model will generate the photo for you, very likely a creepy cat.\n\nThe demo talks to the backend server running TensorFlow model, the backend server run by itself or forward to Cloud ML hosted TensorFlow service run by Google.\n\nThe cat model is trained on 2k cat photo and automatically generated edges from cat photos. So it is \u201cedge\u201d to \u201cphoto\u201d. The model itself uses a conditional generative adversarial network (cGAN).\n\nCheck out the original paper if you are interested in implementation detail, it shows more example usages for cGAN like \u201cmap to aerial\u201d, \u201cday to night\u201d etc.\n\nAnother in-browser experience builds on the hardware accelerated TensorFire library let you auto-complete your sentences like Taylor Swift, Shakespeare and many more.\n\nThe original author\u2019s goal is not to make the result \u201cbetter\u201d, but to make it \u201cweirder\u201d.\n\nThe model is built on rnn-writter, to learn more, you can read the author\u2019s page.\n\nThis model creates music in a language similar to MIDI itself, but with note-on and note-off events instead of explicit durations. So the model is capable of generating performances with more natural timing and dynamics.\n\nTinker with a neural network in your browser. Tweak the model by using different learning rate, activation function and more. Visualize the model as its training.\n\nLet\u2019s have a sneak peek into the brain of the neural network.\n\nSo that\u2019s it. Do you have any cool deep learning experience you want to share? Please leave a comment and let\u2019s continue the journey.\n\nI have one last secret experience at the bottom of my original post at www.dlology.com."
    },
    {
        "url": "https://hackernoon.com/one-simple-trick-to-train-keras-model-faster-with-batch-normalization-baa7787bf923?source=user_profile---------20----------------",
        "title": "One simple trick to train Keras model faster with Batch Normalization",
        "text": "Disclaimer: Batch Normalization is really an optimization to help train faster, so you shouldn\u2019t think of it as a way to make your network better.\n\nFirst introduced in the paper: Accelerating Deep Network Training by Reducing Internal Covariate Shift.\n\nAs the data flows through a deep network, the weights and parameters adjust those values, sometimes making the data too big or too small again \u2014 a problem the authors refer to as \u201cinternal covariate shift\u201d. By normalizing the data in each mini-batch, this problem is largely avoided. Batch Normalization normalizes each batch by both mean and variance reference.\n\nA normal Dense fully connected layer looks like this\n\nTo make it Batch normalization enabled, we have to tell the Dense layer not using bias since it is not needed, it can save some calculation. Also, put the Activation layer after the layer\n\nA normal Keras Conv2D layer can be defined as\n\nTuring it to Batch normalized Conv2D layer, we add the layer similar to Dense layer above\n\nNormally the model needs to be complicated enough so that the training could get noticeable benefit from batch normalization.\n\nFor demo purpose, we choose the MNIST handwritten digits datasets since\n\nHere is the simple model structure with 3 stacked Conv2D layers to extract features from handwritten digits image. Flatten the data from 3 dimensions to 1 dimension, followed by two Dense layers to generate the final classification results.\n\nWe will apply batch normalization for all Dense and Conv2D layers and compare the results with the original model.\n\nWe are training models with different parameters and compare two side by side\n\nAs we can see the validation accuracy curve for the model with batch normalization is slightly above original model without batch normalization.\n\nLet\u2019s try training both models with 10 times larger learning rate,\n\nThe original model without batch normalization was not able to learn at all with this learning rate.\n\nWhat if we use the non-linear relu activation function instead with the same x10 learning rate,\n\nFinally, the original model without batch normalization is able to train, while our model with batch normalization is superior with higher validation accuracy during training.\n\nThis post demonstrates how easy it is to apply batch normalization to an existing Keras model and showed some training results comparing two models with and without batch normalization. Remarkably, the batch normalization works well with relative larger learning rate.\n\nOne final note, the batch normalization treats training and testing differently but it is handled automatically in Keras so you don\u2019t have to worry about it.\n\nCheck out the source code for this post on my GitHub repo.\n\nFor Tensorflow demo \u2014 it shows you the training and testing difference"
    },
    {
        "url": "https://medium.com/@chengweizhang2012/how-to-leverage-tensorflows-tfrecord-to-train-keras-model-b0553a91f335?source=user_profile---------21----------------",
        "title": "How to leverage TensorFlow\u2019s TFRecord to train Keras model",
        "text": "In our previous post, we discovered how to build new TensorFlow Datasets and Estimator with Keras Model for latest TensorFlow 1.4.0. The input function takes raw image files as input. In this post, we will continue our journey to leverage Tensorflow TFRecord to reduce the training time by 21%.\n\nBefore reading on, if you haven\u2019t checkout out our previous post, it is suggested to do so. So that you are familiar with the process to turn a Keras model to a TensorFlow Estimator, and the basics of Datasets API.\n\nOnce we have a list of image files and associated labels(0-Cat, 1-Dog).\n\nWe can write the function for reading images from disk and writing them along with the class-labels to a TFRecord file.\n\nNote that in the function, we resize all image to (150,150) so we don't have to do it in the training process like in the previous post, it also makes the generated TFRecord files' size smaller.\n\nIf we call the function, it will generate the train and test TFRecord Files for us.\n\nOur Estimator needs a new input function that read the TFRecord Dataset file, we call the function to read the TFRecord file we created earlier.\n\nNotice that since the image data is serialized, so we will need to turn it back to its original shape(150, 150, 3) with .\n\nSimilar to the previous post, the function takes the path to the TFRecord files, and there is no parameter for the labels since they are already included in the TFRecord files.\n\nTo show you the result of the training speed boost, we timed the execution of the call.\n\nThis post with TFRecord file as Datasets.\n\nThe cat vs dog datasets we are using here is relatively small. If we are working on larger datasets that do not fit into our memory. The same class we used in the post also enables us to stream the contents of more than one TFRecord file as part of an input pipeline.\n\nCheck out the full source code in my GitHub repo for this post"
    },
    {
        "url": "https://medium.com/@chengweizhang2012/an-easy-guide-to-build-new-tensorflow-datasets-and-estimator-with-keras-model-9b0f6b4c1b0d?source=user_profile---------22----------------",
        "title": "An Easy Guide to build new TensorFlow Datasets and Estimator with Keras Model",
        "text": "If you haven\u2019t updated yet,\n\nIn this post, I will show you how to turn a Keras image classification model to TensorFlow estimator and train it using the Dataset API to create input pipelines.\n\nIf you haven\u2019t read TensorFlow team\u2019s Introduction to TensorFlow Datasets and Estimators post. Read it now to have an idea why we do what we do here.\n\nSee you just happen to be in a region where you do not have access to any Google\u2019s websites, which kindly sucks, so I summarized it here for you.\n\nYou should use Dataset API to create input pipelines for TensorFlow models. It is the best practice way because:\n\nEstimators is a high-level API that reduces much of the boilerplate code you previously needed to write when training a TensorFlow model.\n\nTwo possible way to create Estimators: Pre-made Estimators to generate a specific type of model, and the other one is to create your own with its base class.\n\nKeras integrates smoothly with other core TensorFlow functionality, including the Estimator API\n\nAll right, enough for the intros, let\u2019s get to the point to build our Keras Estimator.\n\nFor simplicity reason, let\u2019s build a classifier the famous dog vs cat image classification.\n\nThe cats vs. dogs dataset was made available by Kaggle.com as part of a computer vision competition in late 2013. You can download the original dataset at https://www.kaggle.com/c/dogs-vs-cats/download/train.zip\n\nWe are only using a small portion of the training data.\n\nAfter downloading and uncompressing it, we will create a new dataset containing three subsets: a training set with 1000 samples of each class, and a test set with 500 samples of each class. This part of the code is omitted here, check out my GitHub to grab it.\n\nWe are leveraging the pre-trained VGG16 model\u2019s convolution layers. aka the \u201cconvolutional base\u201d of the model. Then we add our own classifier fully connected layers to do binary classification(cat vs dog).\n\nNote that since we don\u2019t want to touch the parameters pre-trained in the \u201cconvolutional base\u201d, so we set them as not trainable. Want to go deeper how this model works? Check out this great jupyter notebook by the creator of Keras.\n\nmodel_dir will be our location to store trained tensorflow models. Training progress can be viewed by TensorBoard.\n\nI found that I have to specify the full path, otherwise, Tensorflow will complain about it later during training.\n\nWhen we train our model, we\u2019ll need a function that reads the input image files/labels and returns the image data and labels. Estimators require that you create a function of the following format:\n\nThe return value must be a two-element tuple organized as follows: :\n\n- The first element must be a dictionary in which each input feature is a key. We have only one \u2018input_1\u2019 here which is the input layer name for the model that took processed image data as input for the training batch.\n\n- The second element is a list of labels for the training batch.\n\nSo here is important code that makes the input function for our model.\n\nAs a sanity check, let\u2019s dry run the imgs_input_fn() and look at its output.\n\nIt output the shape of our image and image itself\n\nLooks like color channels \u2018RGB\u2019 has changed to \u2018BGR\u2019 and shape resized to (150, 150) for our model. That is the correct input format the VGG16\u2019s \u201cconvolutional base\u201d is expecting.\n\nTensorFlow release 1.4 also introduces the utility function which simplifies training, evaluation and exporting Estimator models.\n\nThis function enables distributed execution for training and evaluation, while still supporting local execution.\n\nThe model training result will be saved to ./models/catvsdog directory. If you are interested, you can take a look at the summary in TensorBoard\n\nHere we only predict the first 10 images in the test_files.\n\nTo predict we can set the labels to None because that is what we will be predicting. \u2018dense_2\u2019 is our model\u2019s output layer name, will be one single float number between 0~1 where 0 means a cat image and 1 is a dog image.\n\nWe build a Keras Image classifier, turn it into a TensorFlow Estimator, build the input function for the Datasets pipeline. Finally, train and estimate the model. Go ahead and check out the full source code in my GitHub repo for this post."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/how-to-do-multi-class-multi-label-classification-for-news-categories-1445bdd906c5?source=user_profile---------23----------------",
        "title": "How to do multi-class multi-label classification for news categories",
        "text": "My previous post shows how to choose last layer activation and loss functions for different tasks. This post we focus on the multi-class multi-label classification.\n\nWe are going to use the Reuters-21578 news dataset. With a given news, our task is to give it one or multiple tags. The dataset is divided into five main categories:\n\nFor example, one given news could have those 3 tags belonging two categories\n\nIn previous step, we read the news contents and stored in a list\n\nOne news looks like this\n\nWe start up the cleaning up by\n\nAfter this our news will looks much \u201cfriendly\u201d to our model, each word is seperated by space.\n\nSince a small portation of news are quite long even after the cleanup, let\u2019s set a limit to the maximum input sequence to 88 words, this will cover up 70% of all news in full length. We could have set a larger input sequence limit to cover more news but that will also increase the model training time.\n\nLastly, we will turn words into the form of ids and pad the sequence to input limit (88) if it is shorter.\n\nThe same news will look like this, each number represents a unique word in the vocabulary.\n\nAfter training our model for 10 epochs in about 5 minutes, we have achieved the following result.\n\nThe following code will generate a nice graph to visualize the progress of each training epochs.\n\nTake one cleaned up news (each word is separated by space) to the same input tokenizer turning it to ids.\n\nCall the model predict method, the output will be a list of 20 float numbers representing probabilities to those 20 tags. For demo purpose, lets take any tags will probability larger than 0.2.\n\nThe model got 2 out of 3 right for the given news.\n\nWe start with cleaning up the raw news data for the model input. Built a Keras model to do multi-class multi-label classification. Visualize the training result and make a prediction. Further improvements could be made\n\nThe source code for the jupyter notebook is available on my GitHub repo if you are interested."
    },
    {
        "url": "https://medium.com/@chengweizhang2012/how-to-choose-last-layer-activation-and-loss-function-98dfea718dcb?source=user_profile---------24----------------",
        "title": "How to choose Last-layer activation and loss function",
        "text": "This competition on Kaggle is where you write an algorithm to classify whether images contain either a dog or a cat. It is a binary classification task where the output of the model is a single number range from 0~1 where the lower value indicates the image is more \u201cCat\u201d like, and higher value if the model thing the image is more \u201cDog\u201d like.\n\nHere are the code for the last fully connected layer and the loss function used for the model\n\nIf you are interested in the full source code for this dog vs cat task, take a look at this awesome tutorial on GitHub.\n\nThe task is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset came with Keras package so it\u2019s very easy to have a try.\n\nLast layer use \u201csoftmax\u201d activation, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.\n\nAgain the full source code for MNIST classification is provided on GitHub.\n\nReuters-21578 is a collection of about 20K news-lines and categorized with 672 labels. They are divided into five main categories:\n\nFor example, one news can have 3 tags\n\nYou can take a look at the source code for this task on my GitHub.\n\nI also wrote another blog for this task in detail as well, check out if you are interested.\n\nThe goal is to predict a single continuous value instead of a discrete label of the house price with given data.\n\nThe network ends with a Dense without any activation because applying any activation function like sigmoid will constrain the value to 0~1 and we don\u2019t want that to happen.\n\nThe mse loss function, it computes the square of the difference between the predictions and the targets, a widely used loss function for regression tasks.\n\nFull source code can be found in the same GitHub repo.\n\nFor a task like making an assessment of the health condition of a jet engine providing several sensors recordings. We want the output to be a continuous value from 0~1 where 0 means the engine needs to be replaced and 1 means it is in perfect condition, whereas the value between 0 and 1 may mean some degree of maintenance is needed. Compare to previous regression problem we are applying the \u201csigmoid\u201d activation to the last dense layer to constrain the value between 0 to 1.\n\nLeave a comment if you have any questions."
    }
]