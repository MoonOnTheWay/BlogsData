[
    "https://medium.com/@aerinykim/why-do-we-subtract-the-slope-a-in-gradient-descent-73c7368644fa?source=user_profile---------1----------------", 
    "https://towardsdatascience.com/big4-tech-interview-question-derive-the-linear-regression-c45ccdd213e3?source=user_profile---------2----------------", 
    "https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1?source=user_profile---------3----------------", 
    "https://medium.com/@aerinykim/the-gradients-of-linear-regression-cost-function-1a42b98ab0ef?source=user_profile---------4----------------", 
    "https://medium.com/@aerinykim/how-to-derive-chi-squared-pdf-from-normal-gaussian-c48d6d19b3d4?source=user_profile---------5----------------", 
    "https://medium.com/@aerinykim/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d?source=user_profile---------6----------------", 
    "https://medium.com/@aerinykim/numpy-sum-axis-intuition-6eb94926a5d1?source=user_profile---------7----------------", 
    "https://medium.com/@aerinykim/why-the-normal-gaussian-pdf-looks-the-way-it-does-1cbcef8faf0a?source=user_profile---------8----------------", 
    "https://medium.com/@aerinykim/why-is-the-second-principal-component-orthogonal-to-the-first-one-d453c9fd97ca?source=user_profile---------9----------------", 
    "https://medium.com/@aerinykim/derive-the-gradients-w-r-t-the-inputs-to-an-one-hidden-layer-neural-network-fb24ed1ed05f?source=user_profile---------10----------------", 
    "https://medium.com/@aerinykim/draft1-5c1f76d4833a?source=user_profile---------11----------------"
]