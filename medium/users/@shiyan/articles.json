[
    {
        "url": "https://medium.com/@shiyan/some-cuda-concepts-explained-12ecc390d10f?source=user_profile---------1----------------",
        "title": "Some CUDA concepts explained \u2013 Shi Yan \u2013",
        "text": "I bet you can easily accelerate your program by 10x by adopting CUDA. But that 10x is far from the end of the story. A fully optimized CUDA code could give you a 100x boost. To write highly optimized CUDA kernels, one needs to understand some GPU concepts well. However, I found some of the concepts are not being explained well on the internet and can easily get people confused.\n\nThose concepts are confusing because\n\nI hope I could clarify some of the CUDA concepts with this post.\n\nA GPU is formed by multiple units named SM (Streaming Multiprocessors). As a concrete example, the GPU Titan V has 80 SMs. Each SM can execute many threads concurrently. In the case of Titan V, that maximum concurrent thread count for a singleTitan V SM is 2048. But these threads are not exactly the same as the threads run by a CPU.\n\nThese threads are grouped. And a thread group is called a warp, which contains 32 threads. So a Titan V SM can execute 2048 threads, but those 2048 threads are grouped into 2048 / 32 = 64 warps.\n\nWhat these threads are different from a CPU thread is that CPU threads can each execute different tasks at the same time. But GPU threads in a single warp can only execute one same task. For example, if you want to perform 2 operations, c = a + b and d = a * b. And you need to perform these 2 calculations on lots of data. You can\u2019t assign one warp to perform both calculations at the same time. All 32 threads must work on the same calculation before moving onto the next calculation, although the data the threads process can be different, for example, 1 + 2 and 45 + 17, they have to all work on the addition calculation before moving onto the multiplication.\n\nThis is different from CPU threads, because if you have a powerful enough CPU to support 32 threads simultaneously, each of them can work on a separate calculation. Therefore, the concept of the GPU thread is akin to the SIMD (Single Instruction, Multiple Data) feature of CPU.\n\nThe best real life analogy of GPU threads in a warp I found is the above. I don\u2019t know if you have had similar experience before. In high school, when we got punished to copy words for not being able to finish homework, for example, we tended to bound a group of pens together in a vertical row so that we can write multiple copies of the same content at the same time. That helped us finish it quicker.\n\nAs powerful as the school trick is, holding multiple pens isn\u2019t turning you into a mighty octopus who has many tentacles and can perform multiple tasks at the same time. This is the major difference between CPU threads and GPU threads.\n\nWhy is this important? Because when you launch a GPU program, you need to specify the thread organization you want. And a careless configuration can easily impact the performance or waste GPU resources.\n\nFrom the software\u2019s point of view, GPU threads are organized into blocks. Block is a pure software concept that doesn\u2019t exist in the hardware design. Unlike the physical thread organization, the warp. Blocks don\u2019t have a fixed number of threads. You can specify any number of threads up to 1024 within a block, but that doesn\u2019t mean any thread number will perform the same.\n\nConsider we want to perform 330 calculations. One natural way is launching 10 blocks, and each block works on 33 calculations with 33 threads. But because every 32 threads are grouped into a warp. To finish the 33 calculations, 2 warps == 64 threads are involved. In total, we will be using 640 threads.\n\nAnother way is launching 11 blocks of 32 threads. This time, each block can fit into a single warp. So in total, 11 warps == 352 threads will be launched. There will be some waste, but it won\u2019t be as much as the first option.\n\nAnother thing that needs to be considered is the number of SMs, because each block can only be executed within one SM. A block can\u2019t be processed by more than one SM. If the workload is very large, i.e. we have lots of blocks, we could use up all available SMs and we still have remaining work to do. In this case, we will have to launch part of the work as the first batch and then finish the remaining work in following batches. For example, in the case of Titan V, there are 80 SMs. And suppose we have a complex work that requires 90 SMs to finish. We will have to launch a batch of 80 SMs first and launch the remaining work with 10 SMs as the second batch. But in this case, during the second batch, 70 SMs are idle. A better way is adjust the workload for each SM, so that they can do less work each and finish sooner. But in total, you need 160 SMs this time. Although you still need to launch 2 batches of calculations, but because each batch can finish quicker, the overall run time reduces.\n\nLastly, if you are familiar NVIDIA\u2019s marketing terms, a GPU\u2019s powerfulness is often measured by the number of CUDA cores. But when you learn CUDA programming, you probably seldom see it as a programming concept. Well, a CUDA core is actually a warp. So again in the Titan V case, it has 80 (SMs) * (2048) Threads / 32 (Threads / Warp) = 5120 CUDA cores."
    },
    {
        "url": "https://medium.com/@shiyan/how-to-explain-the-spectre-bug-to-non-techies-90a73f8f150b?source=user_profile---------2----------------",
        "title": "How to explain the spectre bug to non-techies? \u2013 Shi Yan \u2013",
        "text": "My colleague asked me to explain the recent CPU spectre bug and speculative execution to a non-tech person. It\u2019s like a challenge.\n\nFor speculative execution, I immediately thought about this real-life example:\n\nNewsweek, as well as other magazines, pre-made different issues for both presidential candidates. This is their strategy to deliver the election result timely. Of course only one candidate would win, so part of the labor would be wasted.\n\nThis is speculative execution. Before you even know that the prerequisites for an action or a response will fulfill, you carry out the action anyway. If the conditions your action responses to has indeed happened, you end up delivering the results faster. Otherwise, you will need to throw away your work and restore everything to its initial state, as if nothing has changed.\n\nHow to explain the spectre bug then? The core idea of the spectre bug is that you can make use of the above speculative execution feature to acquire data you have no permission to access.\n\nLet me use an analogy. Assume you are a private investigator. Your client asked you to find out if a person, who is a politician, was actually born in Hawaii. You don\u2019t have the resource to carry out this investigation and you have few ways to get access to this information. What do you do?\n\nOne way (the spectre way) is starting a rumor that the politician will run for office. And you spread this rumor to a large newspaper which has tremendous resources to dig any information about this politician. The newspaper is very interested in making a dedicated issue to introduce this politician, so they formed a whole team to check on him, including where the politician was born. This is kinda of speculative execution, because the action is based on a rumor which will be proven wrong eventually. When the newspaper has found out that the rumor was just a rumor, they have to throw away their effort on making this special issue.\n\nOne journalist from the above newspaper was also your high school best friend. The journalist happened to be part of the investigation team. He has learned a lot about the politician. You can just buy your journalist friend a beer and ask anything you are interested pretending that it was just a casual chatting about work.\n\nThis is roughly the idea of spectre. You basically trick the cpu to fetch some information that you normally don\u2019t have access to. And based on the information, you perform another data access, but to different memory locations you are allowed to access this time. When the cpu realizes that it has been tricked, it will put back what you have accessed and restore the values it has changed. But you can still access the information you want indirectly by measuring time, because if a data has been fetched by the second data access, it will be faster to fetch next time. In the above case, the newspaper is the cpu. And the information of interest is where the politician was born. You tricked the cpu to load the information by starting the rumor. The rumor turned out to be just a rumor. So the newspaper ceases the effort on writing articles on the politician. But what has been learned during the process is cached, you can simply access it from your journalist friend.\n\nI don\u2019t know if you are satisfied with my attempt on explaining the spectre bug with the above analogy. I think the most ingenious part of the spectre bug is that people can guess the content of a piece of inaccessible data by looking at the access time of some other accessible data. Who would know that accessing time could imply the content of some data!\n\nThis reminds me another bug I met while working at NVIDIA, my favorite one. It was a driver crash happened only on a prototype notebook after running a graphics stress test for days. Eventually the root cause was identified as a bit flip in system memory. A bit at a random location would flip from 1 to 0, or from 0 to 1, for no reason. We concluded that this bug was caused by memory quality issues. My colleague even joked that it was due to stronger sun spot activities, which was indeed happening at the time. (Because cosmos radiation could affect the stability of memory chips.)\n\nAnyway, being trained as a pure software engineer, I am often under the illusion that computer hardware is trustworthy, reliable and rigid. You tell it 0, it won\u2019t remember it as 1. This is the benefit of abstraction, we are hid from unnecessary details. But sometimes those details are useful, like in the above cases. The spectre POC code looks logically correct from a pure software point of view. But when you run it on hardware, that\u2019s a different story.\n\nI guess the more you work with computers, the more you can feel about the organic side of machines. When I\u2019m frustrated with machines on problems or bugs, I often feel that my computers are not in a good temper, I have to conciliate them somehow as if they were just pets."
    },
    {
        "url": "https://medium.com/@shiyan/xavier-initialization-and-batch-normalization-my-understanding-b5b91268c25c?source=user_profile---------3----------------",
        "title": "Xavier initialization and batch normalization, my understanding",
        "text": "Mr. Ali Rahimi\u2019s recent talk put the batch normalization paper and the term \u201cinternal covariate shift\u201d under the spotlight. I kinda agree with Mr. Rahimi on this one, I too don\u2019t understand the necessity and the benefit of using this term. In this post, I\u2019d like to explain my understanding of batch normalization and also Xavier initialization, which I think is related to batch normalization.\n\nRecall the structure of an artificial neuron, the building block of a deep learning model:\n\nIt performs the following 2 steps of computation.\n\nYou usually choose from few common activation functions for step 2: TanH, Sigmoid or ReLU.\n\nAs shown in the above image, both TanH and Sigmoid have a similar shape that resembles a stretched/squashed \u201cS\u201d, whereas ReLU\u2019s shape is different and artificial. It is so in order to overcome some problems of TanH and Sigmoid.\n\nYou might wonder why TanH and Sigmoid look like the way they are? My friend recently pointed me to a paper explaining that. I haven\u2019t finished that paper yet, so I won\u2019t go into details here. But to summarize, the \u201cS\u201d shape has some biological roots, it resembles the way a real neuron reacts to input signals.\n\nIf we put the activation functions (TanH and Sigmoid) under the microscope, you can see that an activation function is mostly flat (at both ends). It only curves within a very small range. If you random an input x and feed it into an activation, such as the Sigmoid function, the return is most likely 0 or 1. Only when your x is sampled within the small range close to 0, the activation function can return a value between 0 and 1.\n\nAs you may have noticed that deep learning models are very sensitive to weight initialization. Without a careful initialization, the same neuron network trained with the same data, that converges rapidly sometimes, may not converge at all.\n\nAnd this phenomena is largely due to the above characteristic of activation functions. Recall that an input of an activation function is a weighted sum of inputs Xi. If the weighted sum is out of the sweet range of the activation function, then the output of the neuron is almost fixed at the maximum or the minimum. From the backpropagation\u2019s point of view, the \u201cadjust-ability\u201d of this neuron is very poor in this situation, because the activation function is almost flat at the input point and the gradient evaluated at that point is close to zero. Being close to zero means that you don\u2019t know the direction to adjust the input.\n\nIn this case, the neuron is being trapped (often called \u201csaturated\u201d). It will be difficult for the neuron to get out of the trapped situation due to the lack of \u201cadjust-ability\u201d. (ReLU is better in that it has only one flat end.) Therefore, one needs to be careful when initializing a neuron network\u2019s weights. If the initial weights are too large or too small, you will likely start with saturated neurons. This will lead to slow or no convergence.\n\nAn initialization method called Xavier was therefore introduced to save the day. The idea is randomizing the initial weights, so that the inputs of each activation function fall within the sweet range of the activation function. Ideally, none of the neurons should start with a trapped situation.\n\nThis post is a great material on Xavier initialization. Basically it tries to make sure the distribution of the inputs to each activation function is zero mean and unit variance. To do this, it assumes that the input data has been normalized to the same distribution. The more number of inputs a neuron has, the smaller the initial weights should be, in order to compensate the number of inputs. In a word, the Xavier initialization method tries to initialize weights with a smarter value, such that neurons won\u2019t start training in saturation.\n\nHowever there is another factor, other than the weights, that may cause a neuron to be trapped in saturation. That\u2019s the inputs X. And you only have some control over the inputs X for the first layer of the network, because X of the first layer is the input data, you can always normalize it. But after you have trained the network for few iterations, the internal weights change. And the the weight changes will result in value changes of the inputs X to the internal layers, for which, you don\u2019t have direct control. It\u2019s very likely that those internal inputs cause neurons to trap at a saturation point, which slows down the training process. This is the so called internal covariate shift.\n\nWhat can we do about it? Can we modify the weights similar to what Xavier initialization does? No, we can\u2019t, because our weights are learnt from the gradient descent procedure. We can\u2019t artificially change them, as that will mess up the training process. Therefore, we have to alter the inputs X (of a batch), so that after some transformation, the inputs X of a neuron can still fall into the sweet range of the neuron\u2019s activation function.\n\nAnother way to think of this is that we squeeze and shift the activation function, so that it can adapt to the inputs X automatically. This is the core idea of batch normalization.\n\nAs a concrete example, assuming we have a group of inputs X to an activation function:\n\nAs you can see, the inputs (red dots) are trapped in saturation. Assuming our activation function is y = f(x), we can squeeze and shift this function by doing this modification: y = f(x/gamma-beta), where gamma defines how much you squeeze this function and beta is a shift factor.\n\nAfter this modification, or normalization, the new activation function will look like (red line):\n\nAs you can see, inputs X now fall into the sweet range again. So really, batch normalization is to improve the adjust-ability of the neurons. In the training process, both gamma and beta are part of the variables to be trained by gradient descent. The network will learn the best gamma and beta (both variables are vectors) for each neuron. They add more flexibility to the network. For mathematical details, I recommend this article.\n\nAs an analogy, suppose you are a gym owner. You recently installed some showers in your gym, because your sweaty customers could really use a shower after a workout. However, after certain time, you started to receive complains. According to the customers, the shower water is too cold.\n\nAs you should have noticed, shower valve is usually a knob like the above. If you rotate the knob towards one direction, the water should become warmer, and the opposite direction will give you colder water. But there is a temperature range; you can\u2019t adjust the temperature to very hot or very cold. After certain point, you are trapped at a saturation point where your temperature can\u2019t change further towards the extreme end.\n\nYour customers complained because the current adjustable range of the water temperature doesn\u2019t overlap much with the temperature range people prefer. Your customers think the water is too cold. And the shower valve is very sensitive, a tiny rotation will turn the water into the hottest or the coldest. They can\u2019t have a fine control over the water temperature.\n\nHere, in this analogy, the angles of the valve people prefer during shower are like the inputs X to an activation function. And the valve is the activation function. Given an angle (an input), the activation function (the valve) translates it into a value of water temperature. The water temperature can be viewed as the output of the activation function. Clearly, we are now trapped at a saturation point where most people prefer a water temperature that is even hotter than the maximum temperature the shower head can provide. No matter how much customers turn the shower knob, they cannot achieve a satisfyingly warm temperature.\n\nHow do you fix this? Well, for the cold water problem, you basically need to add another hot water pipe to the original water pipe to raise the default or the base water temperature. This is like the shift operation of batch normalization, which basically adjusts the mean of the inputs X. For the second issue where the valve is too sensitive, all you need to do is requesting more rotation to achieve the same adjustment (scaling). This is the \u201csquashing/stretching\u201d part of batch normalization. After these modifications, your customers should be able to find perfect temperatures in a conformable temperature range."
    },
    {
        "url": "https://medium.com/freewill-ai/on-the-alchemical-side-of-deep-learning-d68d09f3e275?source=user_profile---------4----------------",
        "title": "On the alchemical side of deep learning \u2013 FreeWill.ai \u2013",
        "text": "Mr. Ali Rahimi recently gave a talk at NIPS, in which he described the current deep learning as alchemy, for its lack of theoretical explanation and being pragmatically oriented. If you haven\u2019t seen it and are interested, here is the link:\n\nI feel that what Rahimi described is something that I, as a deep learning student, can relate. Few years back when we first studied deep learning, we called ourselves \u201cold Chinese medicine practitioners \u8001\u4e2d\u533b\u201d. We grew up in an eastern culture, alchemy isn\u2019t a term that\u2019s familiar to us. But an old Chinese medicine practitioner is sort of the eastern counterpart of an alchemist. By calling ourselves that, we were referring to the fact that when designing a deep learning model, we had no idea how many layers or how wide our model should be and how many convolution layers vs. fully connected layers. There is no methodology to guide us. We, as well as many deep learning practitioners, have to spawn many models at once, and pick the best performing one. And normally there is no explanation as for why this one model is better than the rest. It could even be due to a better random starting point.\n\nDoing blind experiments is the methodology traditional Chinese medicine have been used for thousands of years. Whereas less efforts have been spent on developing concrete theories or trying to explain the mechanisms of action of herbs or the principles of treatments. It\u2019s unfair to say that traditional Chinese medicine has not developed any theory at all, but its theories have always been escalated to a philosophy level, very nebulous in my opinion (sometimes, this hides its problem of lacking understanding). Although it won China a Nobel prize, from time to time, some traditional treatments had been found to be harmful. And when an unexplainable deep learning model is applied to autonomous driving, we will have some fun!\n\nI consider doing experiments a scientific approach. After all, it is being used actively and successfully in the fields of Chemistry and Physics, though not so much in the fields of computer science and mathematics. There is nothing wrong with it. Also, I think Mr. Rahimi\u2019s talk wasn\u2019t really criticizing the status quo of deep learning, but was reminding us that we shouldn\u2019t throw away our curiosity and stop pursuing the truth and rules underneath the surface. Deep learning is currently a very pragmatic field. You see more professors in this field having close ties to the industry. They are the CTOs, VPs, VCs, CEOs of other companies. It may not be a bad thing, after all industry can provide huge amount of resources, and guide research to needed areas. But hopefully being money driven won\u2019t make people short sighted.\n\nA while back, I was curious about distributed deep learning training. As you know, deep learning models have densely interconnected parts, which by nature are hard to separate. There was this paper by a top AI company claiming that if you let two models (started from the same random point) converge separately for a few steps and then average them, you can still make the training procedure converge. I found it very mysterious, because if gradient descent is to find a good local minimum, averaging 2 local minimum points shouldn\u2019t guarantee an even better local minimum point. Say, there are 2 people currently standing at the bottoms of 2 abysses, will their averaged location be at an even deeper abyss? All I was curious to know, before reading the paper, was the mathematical theory to back the above claim. And this, in my opinion, is the core sentence of the paper, hiding behind some thick introduction and related works:\n\nMy translation: \u201cWe tried, it worked (on MNIST), and deal with it!\u201d Again, I consider experimenting a scientific approach. But is the result conclusive given only a limited number of experiments?\n\nIn my opinion, It\u2019s ok that a work is based on unexplained blind experiments, but I hope its paper could be utterly honest about it. These days, you see too much decoration, marketing effort and pretentiousness in academic papers. That\u2019s what frustrates me more. I get confused at what the purpose of academic papers is. Is it for spreading knowledge, promoting understanding? Or is it for scholars to declare victory? A lot of times, it feels like the later case."
    },
    {
        "url": "https://medium.com/@shiyan/orchid-protocol-and-micro-payment-on-blockchain-b408a1770756?source=user_profile---------5----------------",
        "title": "Orchid protocol and micro-payment on blockchain \u2013 Shi Yan \u2013",
        "text": "My friends recently got interested in censorship fighting systems based on blockchain, so we read about the orchid protocol. What interested me more, however, is the micro-payment ideas being explored by blockchain systems like orchid.\n\nAs we have all noticed, current generation blockchains suffer from scalability issues . A transaction, for example, needs to take several hours to settle. And this will only get worse, as transaction history accumulates and will become the burden that everyone will carry forever. This is a fundamental trade-off blockchain designers played in order to achieve decentralization. If we don\u2019t do anything about it, it seems our only hope is that the efficiency of our network/compute infrastructures can outgrow the blockchain, but it doesn\u2019t seem to be the case.\n\nSo naturally, people are thinking how to cope with this by moving some data off the blockchain. Afterall, after a while, it makes little sense to carry very ancient transaction history. One proposed idea is having off chain payment channels. High frequency private transactions between a stable group of users can be carried out offline. Only the final results will be written back to the blockchain and thus reduce the size of historical data. For example, A gave B $5 first, and then B gave A $4. These transactions are equivalent to A gives B $1. If A and B do these payments a lot, then they don\u2019t need to tell others about the transaction details, they just need to tell others about the final result of all transactions between them. One such a technique is called lightning network. Its protocol is quite complex, I don\u2019t fully understand it at the moment.\n\nI knew about payment channels since a while back. Every time, I read articles talking about how significant payment channels are, I always saw the same kind of example being used: Say you need to pay your landlord monthly rent, you can do this with your landlord off the blockchain, as you only need to settle your transactions once a while. But how many real life transactions are like this? Other than the monthly rent case, all I can think of is the membership card of, say, Starbucks or certain gym. On what scale can payment channels solve the scalability issues of blockchain? I failed to understand the value of it until I read about the orchid protocol.\n\nImagine the typical movie scene where a person wants to hire an assassinator to carry out an assassination. The conversation must go like this: \u201cI will pay half upfront. And once the job is done, I will pay the rest.\u201d Why will it be always like this? Because assassination is of high risk. To reduce the risk, you want to break the trade down into smaller granularity. In case shit happens, you lose only half of your money right? And in fact, if efficiency allows, you would want to break the trade into even smaller pieces, rather than dividing the payment into just 2 parts. For example, if I want to sell a bag of rice to an untrusted friend. I\u2019d like to sell the bag one grain at a time, and he will pay me only a small amount of money for each grain. We repeat this till we are done with the whole bag. At anytime, if one of us becomes dishonest, one of us will only lose either one grain of rice or a small amount of money that is worth one grain of rice. However it would be too tiresome to trade this way in real life. But in the virtual world, computers can do millions of transactions quickly, so it is feasible.\n\nFor example, many cryptocurrency exchanges have been hacked. People are now trying to create unhackable p2p exchanges. Such an exchange won\u2019t hold your money as a middle man. Instead, it will only find you a match who is willing to trade with you, and you and your match will trade in a p2p fashion. But this match isn\u2019t necessarily honest. Therefore to guarantee safety, it\u2019s wise to break down the transaction into micropayments. Hence the real value of micropayment channels is serving as a safety mechanism in an untrusted setting.\n\nOrchid protocol tries to solve the censorship problem by creating an anonymous routing system, somewhat similar to Tor, the onion router. In case you are not familiar with Tor, basically when you visit a website via Tor, your network requests and responses will hop between few Tor users before reaching the server. The censors will have harder time to figure out which website you are visiting. The more users, who can access the internet freely, participate in the Tor network to provide traffic relay for other people, the more robust the system will become. But currently, Tor relies on volunteers in the free world to become relay nodes, they are not paid for what they do. And people are less likely to become a node without a financial incentive.\n\nOrchid tries to provide such an incentive by adopting blockchain, so that you get proper cryptocurrency reward proportional to the amount of network traffic you have helped relay. Accounting is the biggest challenge of this system, as millions of network packets can be sent in a small period of time. And charging money for each packet isn\u2019t practical with blockchain.\n\nThe micropayment idea they use came from this paper, which turns high frequency small payments into a series of raffle tickets. Taking the case of selling a bag of rice as an example, suppose for each grain of rice, you want to charge $0.01. And there are 100 grains of rice in total. So in the end, 100 transactions of $0.01 are needed for this deal to close. But with this micropayment idea, instead of paying 100 bills of $0.01, the buyer would give $1 to a third party, a computer program, beforehand. And then during the trade, the buyer will issue a raffle ticket for each grain of rice he receives. As a seller, you gather those raffle tickets. Only one of them will be the lucky ticket and that ticket will be able to claim the reward of $1 from the third party. Because you will have to gather all tickets to claim the reward, you have to close the deal first. As soon as you have all tickets, you give them to the third party who holds the prepaid $1 and that money will be yours if the third party can verify the authenticity of your raffle tickets. Again, this third party is a trustworthy computer program, not a dishonest human. This way, only 2 transactions will be written to the blockchain, the first being the buyer prepay money to the escrow and the second being you, the seller receive the money from the escrow when the deal is closed. Therefore relatively less data is needed to be kept on blockchain, versus 50 times more data if the micropayments were carried out naively.\n\nRelated to this, another blockchain platform, called Theta tries to create a distributed twitch. It has this idea of letting advertisers to bribe content viewers for watching their commercials. It would be to too costly to bribe all content viewers, so they designed this raffle system. Raffle tickets are distributed with the video streams. Viewers receive those raffle tickets while watching the advertisements. And they need to outrun each other to claim their rewards. Only one of them will be the lucky guy.\n\nTo sum up, micropayment is a very useful tool to ensure safety and reduce risk in an untrusted environment. Therefore, having efficient micropayment methods on blockchain is of importance. Blockchain, however, wasn\u2019t designed to handle micropayment well, it prefers large infrequent transactions for they generate less data. One simple but smart idea of turning small frequent transactions into a large infrequent transaction, as used by the Orchid protocol, is turning transactions into raffle tickets."
    },
    {
        "url": "https://medium.com/@shiyan/game-of-thorns-a-brief-history-of-my-last-name-e8d0a17027ab?source=user_profile---------6----------------",
        "title": "Game of Thorns \u2014 a brief history of my last name \u2013 Shi Yan \u2013",
        "text": "I got really curious about my origin and did some research. This is the result of it.\n\nMy last name Yan \u4e25 is probably one of the few lucky Chinese last names that still have a clear path that traces me back to 2500 years ago, because the name is not very large by population size and it is not very small either. If a last name is too rare, it may be poorly documented, crucial information might be lost throughout the years. And if a name is too common, the sources of origins may not be unique, one can\u2019t tell for sure which branch is his/her origin. For example, there are many periods in Chinese history, migrants and minorities took Chinese last name as a way to blend in to avoid being harmed during political movements. And there are people who were given royal family names as a reward for their contributions. They have lost their origin as a result. The name of Yan, on the other hand, is about 0.3% of the Chinese population and ranks the 107th by population size.\n\nLike every other history, the Chinese history started with a mythology. Unfortunately, the Chinese mythology is fragmented into pieces, unlike the Greek one or the Norse mythology. Stories of the Chinese mythology are loosely connected, if not unconnected. One explanation is that in ancient times, China was formed by different tribes or sub-cultures. They each has their own mythology. During the process of culture mixing, those mythologies merged in a not so harmonic way.\n\nAccording to the mythology, the world began in an eggshell (or a Calabash). Within the shell, there was only chaos at first and nothing else. A giant then grew out of this chaos and soon outgrew the internal volume of the shell. So he broke the shell into two pieces, with the bottom piece sinking down as the ground and the top piece floating above and became the sky.\n\nTo prevent the two pieces from falling back to each other, he kept growing and used his body to support the two pieces, till the sky and the ground were too far apart to fall back. At this point, the giant was exhausted and died. His body became mountains and his blood became rivers and oceans. His eyes became the sun and the moon.\n\nMany years after that (many fragmented stories skipped), China became under the ruling of three major tribes. One is led by Emperor Yellow. Another is led by Emperor Flame. They are actually brothers and enemies at the same time. The third tribe was formed by wildlings and was led by Chiyou. War broke out between the three, with Yellow won eventually and unified all three tribes. About two third Chinese last names rooted from Emperor Yellow, as well as most of the ruling houses of the first few Chinese dynasties.\n\nAfter Yellow, the tribe was led by a series of 5 emperors. One of the emperors, a grandson of Emperor Yellow is said to be my ancestor, and after him, one of his grand grand grand sons was granted the title the lord of flame (but he was not an emperor), and then his youngest grandson named Jilian, who was also not an emperor but married to the daughter of one of the emperors of the Shang dynasty.\n\nThis is when mythology turned into history. According to ancient documents, the first 3 dynasties were Xia, Shang(1600\u20131046 BC) and Zhou (1046\u2013256 BC). Xia started by a son of the last of the 5 emperors. But there lacks concrete proof of the existence of Xia. There are few archaeological sites which are candidates of potential Xia civilization, but none of them have provided text evidences. On the other hand, the second dynasty Shang preserved lots of text documents carved into bones and turtle shells. So the Xia dynasty is considered the separator of mythology and serious history.\n\nBoth of the first 2 dynasties were overthrown by their sub kingdoms, much like the Game of Thrones. Around the time when the 3rd dynasty Zhou was about to overthrow the 2rd dynasty Shang, one of the descendants of Jilian became the teacher of the lord of Zhou and helped Zhou overthrow the Shang dynasty. He was given a viscountship (the lowest title of nobility) as a reward, and was allowed to rule the southern land of China.\n\nThis started the State of Chu, which had once grown into the largest country of China, the strongest of the seven kingdoms and had lasted for 800 years. On the surface, being granted a noble title and a territory seemed to be a reward, but in reality, Chu people were chased away from the center of the empire. At the time, the political center of the Zhou dynasty was also at the geological center of China, and the area was also referred as the \u201cmiddle land\u201d. People lived in the middle land, the capital of the kingdoms thought themselves the most civilized people and they despised others who lived on the corners, including Chu.\n\nThe character Chu \u695a, literally means thorns, hence the title of this article. The writing of the character depicts a person walking in the woods and breaking bushes, as the top part of the character \u6797 means forest, and the lower part \u8d70 means walking. The character also has the meaning of suffering and pain. Based on these, people guessed that my ancestors named their kingdom so because they had suffered an arduous journey while they were chased away from the middle land.\n\nBut several years ago, archeologist uncovered an ancient manuscript which contains the earliest written history of the state of Chu. According to the manuscript, while my ancestors migrated from the middle land to the south, a woman (who seemed to be the common mother of the family) gave birth to a baby by Caesarean and died. A priest wrapped her body with thorns as a way to seal up her wound. And to memorize her, my ancestors started to call themselves Chu (thorn). Because the people of Chu are descendents of the lord of flame, they worshiped flame and the sun with a phoenix as their sigil and they liked to wear red clothes.\n\nFor long, Chu wasn\u2019t taken seriously by the Zhou dynasty. Even invited to its feast, Chu royals could only be Zhou\u2019s servants. This all had changed when Chu had its strongest king Zhuang (613\u2013591 BC), who is the direct ancestor of mine from 2500 years ago. Unfortunately none has left regarding his early life, all stories about him start after he had ascended the throne. During the first three years of his ruling, Zhuang was said to fool around every day, bedding harlots and getting drunk. One of his servants once entreated him to mind his ruling. Zhuang told the servant that there was a bird which hadn\u2019t cried and flied for three years, but he would amaze the world with just one cry and with a single wing flapping he would reach the sky. He eventually led the country to become the largest and the strongest at the time. In fact, during the first few years, it was said that his position was challenged by other royalties, so Zhuang was really playing dumb at first.\n\nDuring his strongest years, Zhuang had marched his army right outside the capital of Zhou Dynasty and challenged the emperor by asking him the weight of The Nine Tripod Cauldrons. In ancient China, The Nine Tripod Cauldrons was the symbol for ultimate power and the ownership of the world. As powerful as Zhuang was, he sought kindness over ruthlessness and gave up the idea of using military power to conquer other countries. He never abused his power and he was a good king. There are lots of interesting things happened around him.\n\nMy last name came directly from king Zhuang of Chu. Speaking of names, I need to cover some ancient Chinese name cultures. 2000 years ago, we had more complex naming system than we have today. At the time, each person had two family names. The first is the called Xing \u59d3. The character \u59d3 is formed by two parts, \u5973 means woman and \u751f means birth. And most early first family names have this woman \u5973 part. Some think this is a sign of early matriarchal society. I guess if two share the same first family name Xing, it means they have the same common mother? And the second family name was a branch name. Someone who was proud of his hometown, for example, could use the location name as his/her second family name (like Brienne of Tarth). But about 2000 years ago (around the time of Qin Dynasty), people started to merge their family names. Lots of information had been lost. And of course, there was also the given name. But it\u2019s considered offensive to call others by their given names in ancient China, so people often used nicknames instead.\n\nZhuang \u5e84 wasn\u2019t his family name, nor was his given name. His first family name was \u8288 according to history. This character isn\u2019t used anymore. It looks like a sheep and is pronounced a bit like sheep bleating. I\u2019m pretty sure it means sheep. The second family name is Xiong \u718a, which means bear. But both characters shouldn\u2019t be the real names the house of Chu ever used. Evidences suggest they are disparaging names given by Chu\u2019s enemy (most likely by Qin after Qin conquered Chu). The real names used by the house of Chu were \u5b2d and \u9153 according to archeological evidences. \u5b2d means mother (or breast), whereas \u9153 means leader. As I mentioned, ancient surnames usually share the same part \u5973, means woman.\n\nZhuang, on the other hand, was the king\u2019s posthumous name. Emperors and kings were given this name after their death, as a conclusion of their life. I heard, the posthumous names come from a dictionary of around 50 characters. Most of the characters are positive, there are negative ones, and also neutral ones, such as \u201csad\u201d, to describe certain young king who died early. My ancestor was given the posthumous name Zhuang \u5e84 to praise his military achievements. To memorize him, his offsprings took the character Zhuang \u5e84 as their surname.\n\nThe Zhou Dynasty became very weak in controlling other kingdoms during its second half. Kingdoms then started tangled warfare. Unfortunately, Chu wasn\u2019t the one that stood till the end. Qin did and started the Qin Dynasty. Chu was conquered by Qin, but its people refused to accept it and there was a prophecy circling around saying that the person who will overthrow Qin must be from Chu.\n\nQin was a short lived dynasty that lasted for only few years and collapsed due to its tyranny. The person who eventually replaced Qin was indeed from Chu. He started the new dynasty called Han. At the time, one of my ancestors (37 BC -43 AD) who bore the family name Zhuang \u5e84 was a good friend of the emperor of Han and was also a famous hermit. But the fourth son of the emperor, who eventually inherited the crown had the same character Zhuang \u5e84 in his given name. Remember I mentioned that calling others with given names was considered offensive in ancient China. Sharing the same name with an emperor was also forbidden. So my ancestor had to change his family name to Yan \u4e25, which is similar to Zhuang in meaning.\n\nAnd many generations after him in Song Dynasty (1265\u20131274 AD), an ancestor of mine had two sons. They lived close to a river. Their offsprings formed two villages in today\u2019s Zhejiang province, named upper Yan village and lower Yan village. This is where I tell people my last name came from. But I have never been there before, my family had left generations ago.\n\nIn the past, many Chinese families kept a family tree book. However, 60 years ago during the socialism movement, they were forced to burn the book (the idea was discarding your little family to embrace the community as your new family). History went lost as a result. Surprisingly, several years ago, a distant relative of mine still kept a copy of the family tree book. I skimmed a digital copy of the book, it is very difficult to read. I faintly remember it traces the family back to the above ancestors (the two sons) from 800 years ago, but I can\u2019t be sure. What\u2019s certain is that it says I originated from one of the above villages."
    },
    {
        "url": "https://medium.com/@shiyan/ethereum-summary-a53c4a8e66d?source=user_profile---------7----------------",
        "title": "Ethereum summary \u2013 Shi Yan \u2013",
        "text": "I tried to understand Ethereum a while back, but I was discouraged by some irresponsible document and video, describing it as a \u201cTuring complete world computer\u201d. Turing complete is true, but it\u2019s definitely not a world computer, not in the sense of High Performance Computing, as Ethereum is not combining the computation power of all joined computers. On the contrary, it asks everybody to execute everybody else\u2019s programs in an inefficient and redundant way. And what does Turing Complete mean for program developers? What program can you write under the limitation that the program has to be deterministic? You can\u2019t have I/O, can\u2019t have good random generator, timer and threading. Therefore, I felt that the term \u201cTuring Complete World Computer\u201d is a bit misleading marketing bullshit. There was also another article trying to explain Ethereum as a \u201cblockchain that runs code\u201d. Blockchain is a data structure, a fancier version of linked list. A data structure stores data, it doesn\u2019t run code!\n\nRecently, Ether, the money of Ethereum, skyrocketed to a point that I didn\u2019t want to stay ignorant about it, so I have been reading Ethereum\u2019s official yellow paper this week. The experience was painful. There are many things I hate about the yellow paper.\n\nFirst, I dislike the use of mathematical symbols. In the world of programming, you get yourself spanked, if you dare to name variables and functions with a single character, such as \u201cint \u03c9 = \u03f4(\u03c8);\u201d. Somehow, in the world of paper writing, people do this and think this is better! Though the yellow paper does a good job explaining its symbol usage, it imposes an extra decoding effort for me to understand the paper.\n\nThe second thing I hate is lacking intuition and rationale, ideas are presented in their final form, without a trace of how the ideas were developed, and why the inventors made certain decisions. Nobody comes up with a complex idea all at once. It must be developed from a naive form, and evolved into its mature form. Knowing how an idea was developed helps you understand it. But documents just don\u2019t bother talking about history.\n\nHowever, both issues are shared among almost all documents and papers, not unique to this paper. It\u2019s a culture thing. I just feel frustrated every time I read papers. The yellow paper is a well written document, certainly better than the irresponsible ones I read few years ago. And I understand document like the yellow paper is not optimized for understanding, but for accuracy.\n\nAnyway, enough complaining, I want to summarize what I have learned. Hopefully this could ease your pain of understanding Ethereum. The idea of Ethereum was evolved from Bitcoin. But I will not start from Bitcoin. My previous posts [1] and [2] should give you brief introductions to Bitcoin. At its core, Bitcoin is built based on blockchain. People often describe blockchain as a decentralized database, but I found this description not so accurate. What a blockchain really is, in my opinion, is the journal of a decentralized database, not the database itself. As on the blockchain, only the history of updates are kept. In order to recover the database, you will have to scan through the entire chain and perform each update one by one.\n\nI found blockchain resembles Git, the source code management system a lot. Git is decentralized, each programer has a local duplication. What\u2019s saved by Git is a journal of updates to source files (commits). To get the latest file, you apply a series of patches onto the initial commit. You can generate local git commits. These commits will not be written to other people\u2019s Git repository yet, until you perform a \u201cgit push\u201d. This is like having a local Bitcoin block, but only when you have won a lottery (mined a lucky number), you have the opportunity to write your local block to the blockchain. You also cannot write random data to the blockchain. Your change has to be verified by others, just like doing code reviews before a \u201cgit push\u201d. Or in Bitcoin, a commit (a data write to the blockchain) has several recent transactions. Each of them needs to be cross verified by others.\n\nA transaction is a movement of value, which can be formalized with a program like below:\n\nNow if you build a cryptocurrency network and let people execute this program every time they receive a transaction, and cross check other people\u2019s results of running the same program, you end up creating a cryptocurrency network similar to Bitcoin.\n\nA natural question to ask is \u201cCan we run different programs this way?\u201d. This is what Ethereum has added on top of Bitcoin.\n\nEthereum lets you publish a piece of program in the form of a programing languages defined by Ethereum to the Ethereum network. You can trigger this program at anytime. Once triggered, everybody in the network will run this program within a virtual machine. Upon finishing the program, the states changed by the program, such as changes to account balances, virtual machine\u2019s memory, storage and many other things will be cross checked by everybody and will be written back to the blockchain by a lucky miner.\n\nIn detail, Ethereum has 2 types of accounts and 3 types of transactions. The first account type is the human account. Similar to the account of Bitcoin, this account is generated randomly at the first run of the Ethereum program. The second type of account is the program account, or the smart contract account. This type of accounts are created by a human with a program (smart contract) attached to it.\n\n3 types of transactions, or commands, include \u201cCreate Program (Contract)\u201d, \u201cExecute Program\u201d and \u201cSend Money\u201d. Sending money is similar to that of Bitcoin. The other 2 kinds of transactions are added by Ethereum. Create Program transaction allows you to publish your program. A miner will write your program to the block chain. Then at any time, you could send the \u201cExecute Program\u201d transaction. People will then execute the program you wrote and cross verify the results and state changes your program has caused and write the results back to the blockchain.\n\nThe program you can write is Turing Complete, but because there is no way to pre-determine if your program can finish in finite time (i.e. no infinite loop.), you need to attach some money whenever you want to execute a program. Execution of every line of the program (and among other things) will cause you money. The program will halt when the attached money run out. The lucky miner who mined the current block will get the attached money as a reward.\n\nThe amount of money you pay to run a program is measured in \u201cgas\u201d, as if your program consumes fuel when executing. But, in my opinion, the use of the word \u201cgas\u201d to describe this concept is very misleading, because this word implies that it was another currency, or a virtual object, or virtual liquid in the network, same as Ether. As a lot of people, including myself, would ask questions like, \u201chow gas is mined\u201d, \u201chow gas can be transferred/trade and stored/accumulated\u201d, when they first heard about \u201cgas\u201d. And in the official document, they would say \u201cpurchase gas\u201d, but in reality, gas is not purchased.\n\nWhat best and accurately describes \u201cgas\u201d is basically \u201cprice tag\u201d. It is the price tag you attached to your program, a measurement of the amount of money (Ether) you would like to pay for the execution of your program. It\u2019s not a virtual object or another currency you need to trade. It\u2019s like a tag labelled to a piece of meat chop, saying \u201c8 pound for $3 / pound\u201d.\n\nPeople execute a program independently on their own computer. They only compare the results afterwards. Therefore, the program that can be run by Ethereum has to be deterministic. To fulfill this requirement, Ethereum builds a sealed virtual machine environment to run the programs. Within it, there can\u2019t be any I/O, timing, threading and \u201cquasi real\u201d random number generator and any factors that could potentially cause different execution results. This means perhaps 90% programs that are of practical use can\u2019t be ported to Ethereum. And because program execution on Ethereum network is inefficient, it is impractical to port most of the rest 10% programs to Ethereum. That\u2019s why I think calling it a \u201cTuring Complete World Computer\u201d is much exaggerated.\n\nThen what\u2019s good of Ethereum if it can\u2019t run many kinds of programs. I think the best use of Ethereum is implementing another cryptocurrency. Suppose you want to implement a cryptocurrency today. Starting from scratch is more of a marketing challenge than an engineering challenge. You will have to convince a lot of people to use your cryptocurrency to jump start the currency\u2019s network. The less people who use your currency, the more vulnerable your currency\u2019s network is exposed to attacks, like the 51% attack. If the currency is implemented on Ethereum in the form of Ethereum program, you could benefit from the existing users and infrastructure of Ethereum. As everyone will cross check the correctness of your currency\u2019s transactions among all other programs. It\u2019s harder to perform attacks on your currency as the Ethereum network is already big and secure. Simply put, Ethereum is a great framework to implement cryptocurrencies."
    },
    {
        "url": "https://medium.com/@shiyan/rejuvenate-my-old-opengl-gui-with-webassembly-1d46a6baa52e?source=user_profile---------8----------------",
        "title": "Rejuvenate my old OpenGL GUI with WebAssembly, or why WebAssembly is awesome!",
        "text": "Ever since the 90s, I have been trying to master javascript, html and later css. I have failed. I rarely complain about the difficulty of using C++ pointer and template, but I\u2019m a hater of the web frontend stack.\n\nAlthough the landscape keeps changing, from using the <table> tag everywhere to the pervasiveness of Angular.js and React.js, the fundamentals feel the same. To me, each update to the web development technology is just a little patch to cover a blast wound.\n\nHtml was first invented as a document format, like the .docx file of Microsoft office. Today, if you are asked to use Microsoft word to develop an app, you would think it is nothing but purely ridiculous. However, that ridiculousness had for long become normal for web app development.\n\nWhen we design a framework, a platform or a library, we\u2019d like to design it into different levels of abstractions. For example, we have Assembly language for low level, speedy logics, we have C++ for midlevel driver and operating system development and we have Python for productivity and cross platform development. We define tcp/ip to lay the ground, and then http on top. We build low level graphics APIs, like OpenGL and on top of it, we create game engines. This is a common and reasonable design pattern. The benefit of this is, as a user, you get to choose the level of functions. If you want finer control, you choose the low level stuff. If you just want to use the ready made solutions and don\u2019t need customization, you select the high level APIs. You can enjoy the security of being able to change everything if you want to.\n\nTake the iOS UI library as an example. It is built on top of a low level graphics element called CALayer. CALayer is nothing, but a rectangular patch, where you can draw your graphics on. On top of this, we have UIView, which uses CALayer to do graphics, and also has event handling and so on. You can always choose to use the default UI elements, like a UIButton. But whenever you want to have a fancier UI component that is not part of the defaults, you know you can always go lower level, draw whatever you like.\n\nDesigning in levels is a concept you seldom find from web standards. Every tag is essentially treated equally. Can you customize a <a> tag, for example? You can, only to certain degrees, like decorating it with css. Or you fake the customization by combining many tags, which is inefficient to load and display (for example, using <div> tags to form a progress bar).\n\nAnother example is the web video standards, like WebRTC and Media Source API. If I were to design these standards, I would do it level by level. First, I will make a standard to do nothing but video decoding. Then, I add another level to improve the quality of a video stream, such as adding jittering buffer to improve smoothness. Then finally, on top of this, I solve connectivity, adding P2P for example.\n\nUsers would get to choose and customize each level if they want to. A game or desktop streaming use case would prefer low latency over smoothness, in this case, developers can customize the buffering logic. A video broadcasting case, however, would prefer smoothness, they care less about timely delivery.\n\nHowever, both WebRTC and Media Source API were designed as a big monster that does only one specific thing. You have no way to configure it to fulfill your needs. Have you wondered why the quality of chrome remote desktop is so bad? Because it uses WebRTC designed for P2P chatting.\n\nAs a metaphor, writing C++ is like using pencils. The quality of your art work depends on your capability of using them. It takes time to master, but theoretically, if you want to and are good enough, you can draw whatever you like with pencils. Writing web apps, however, is like filling coloring books. You can only do what\u2019s confined by a coloring book\u2019s line work. It\u2019s difficult for you to do anything out of line.\n\nAnd yet, javascript, html and css are so popular, as we have no choice. We certainly have demand for web apps, as it is so convenient to run them. There is no need to install, maintain and back up files. But do web apps really need javascript, html and css? I think what they really depend on is a way to load them without installation, a cross platform binary standard that is accepted broadly and a sandbox execution environment. There is no need for the slow document tree really, it only makes sense for a \u201cdocument\u201d.\n\nBut I think things could be changing with the introduction of WebAssembly. I, as a C++ programmer, who had felt being left out from frontend development, finally feel fitting in. Although, in my ideal world, WebAssembly would come out first and become the fundamental and then we built html and document tree on top of it. It\u2019s now the other way around, but still an improvement for sure.\n\nIn 2007, while working on my 3D modeler project, I developed an OpenGL GUI library in C++, called AssortedWidgets. I recently ported this code into WebAssembly with little effort. It\u2019s really amazing to see it running on the web!"
    },
    {
        "url": "https://medium.com/@shiyan/caffe-c-helloworld-example-with-memorydata-input-20c692a82a22?source=user_profile---------9----------------",
        "title": "Caffe c++ helloworld example with MemoryData input \u2013 Shi Yan \u2013",
        "text": "So for this blog post, I\u2019d like to provide a simple example of using Caffe with a MemoryData layer as the input to solve the xor problem. Hopefully, this is a better helloworld for those who want to learn Caffe.\n\nNeural Network is known to be able to solve the XORproblem well, even the XOR operation is nonlinear. The XOR operation can be summarized in the following table. So our goal is creating a neural network, with two binary numbers a and b as the inputs and one binary number c as its output, c should be equal to a xor b if the network works as expected.\n\nThere are many materials explaining this problem in more detail. For example:\n\nThe model I\u2019m using is similar to that of the above link:\n\nThe only difference is that my model has biases. This one doesn\u2019t.\n\nA Caffe model or a Caffe neural network is formed by connecting a set of blobs and layers. A blob is a chunk of data. And a layer is an operation applied on a blob (data). A layer itself could have a blob too, which is the weight. So a Caffe model will look like a chain of alternating blobs and layers connecting with each other, because a layer needs blobs as its input and it generates new blobs to become the inputs for the next layer.\n\nOverall, my model looks like this (model.prototxt):\n\nThe first 2 layers are the input layers. I use 2 input layers, because one is for training, the other is for interference. As you can see, they are MemoryData layers, because we want to provide the training and testing data directly from memory as they are generated. The only difference between these two layers is the batch size. For training, I use 64 batch size. For testing, the batch size is 4. Because I only need to test these 4 cases: 0 xor 0, 0 xor 1, 1 xor 0, 1 xor 1.\n\nNotice that this MemoryData doesn\u2019t allow you to specify the size of you labels. It has to be 1. I think this is another shitty thing about Caffe (What\u2019s worse is that they don\u2019t document this. You have to debug Caffe\u2019s source code to find out.). While 1 is indeed the label size for the xor problem, for other problems, you will have to put all your data and labels into the same piece of memory and use a SplitLayer to cut them into data and labels. I may show an example later of how to use SplitLayer.\n\nThe next layer is the first hidden layer, corresponding to the yellow neurons in the above diagram. The filters, according to Caffe document, can randomize the initial neural network, otherwise the initial weights will be zeros. Since the model is a fully connected network, the layer type is InnerProduct here. In my previous experiment with my own Neural network implementation, sigmoid activations gave good results. So here I\u2019m just using sigmoid.\n\nThe next layer is corresponding to the green output neuron in the diagram, which is also an InnerProduct:\n\nBut for activation, I created two layers, one for training and one for testing.\n\nThe layer for training is a SigmoidCrossEntropy layer, where sigmoid is the activation and cross entropy is the cost function. The reason for combining sigmoid and cross entropy together as a single layer is because calculating their derivatives is easier this way. Since for testing, there is no need to produce the loss, using just the sigmoid layer should be fine.\n\nAnd then, we have the solver config file (solver.prototxt):\n\nThe learning rate is 0.02 to start with and decreases by 50% for every 500000 steps. The overall iteration count is 5000000.\n\nNow comes the C++ program.\n\nFirst I generate 400 sets of training data. Each training data has the batch size of 64.\n\nBasically, I just random 2 binary numbers a and b and calculate their xor value c. And I save both a and b saved together as the input data and c saved into a separate array as the label.\n\nAnd then I create a solver parameter object and load solver.prototxt into it:\n\nNext, I create the solver out of the solver parameter:\n\nThen I need to obtain the input MemoryData layer from the solver\u2019s neural network and feed in my training data:\n\nThe reset function of MemoryData allows you to provide pointers to the memory of data and labels. Again, the size of each label can only be 1, whereas the size of data is specified in the model.prototxt file. 25600 is the count of training data. It has to be a multiply of 64, the batch size. 25600 is 400 * 64. Basically we generated 400 training data with batch size of 64.\n\nNow, call this line, the network will be trained:\n\nOnce it is trained, we need to test it. Create another network, with the same model, but pass TEST as the phase. And load the trained weights cached inside XOR_iter_5000000.caffemodel:\n\nSimilar to training, we need to obtain the input MemoryData layer and pass the input to it for testing:\n\nNotice that the name for this input layer is test_inputdata, whereas the input layer for training is inputdata. Remember we created 2 input layers in the model file. These names are corresponding to the 2 layers. Their difference is the batch size.\n\nThen we do the following to calculate the neural network output:\n\nOnce this is done, we need to obtain the result by accessing the output blob:\n\nWe know the output size is 4, and we save the outputs into the result vector.\n\nIn the end we just print the results:\n\nThe complete source code is here:\n\nHere are the test results for this simple neural network:\n\nSo given 0 and 0, the expected output should be 0, and the neural network generated 0.0005, which is very close. Given 0 and 1, while the expected output is 1, the neural network gave 0.99368, which is also good enough."
    },
    {
        "url": "https://medium.com/@shiyan/bitcoin-in-comic-ded553a57dfb?source=user_profile---------10----------------",
        "title": "Bitcoin in Comic \u2013 Shi Yan \u2013",
        "text": "I have always wanted to draw a comic about bitcoin, but never started. This one is overly simplified, an overview at best. I missed lots of details.\n\nTwo days ago I found the work of Pablo Stanley. I really liked his style. This is kinda inspired by his work, need to credit him."
    },
    {
        "url": "https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a?source=user_profile---------11----------------",
        "title": "L1 Norm Regularization and Sparsity Explained for Dummies",
        "text": "Well, I think I\u2019m just dumb. When understanding an abstract/mathematical idea, I have to really put it into images, I have to see and touch it in my head. I need the geometry, the object, the intuition behind the idea and better with vivid metaphors in real life.\n\nSometimes when I found people don\u2019t think or at least don\u2019t explain things this way, pointing me to equations and papers, saying there are no simple explanations, I got angry. And often after I thought stuff through, I could find silly intuitive explanations to those ideas. One such an experience was yesterday when I tried to understand L1 norm regularization applied to machine learning. Thus, I\u2019d like to make this silly but intuitive piece to explain this idea to fellow dummies like myself.\n\nWhen performing a machine learning task on a small dataset, one often suffers from the over-fitting problem, where the model accurately remembers all training data, including noise and unrelated features. Such a model often performs badly on new test or real data that have not been seen before. Because the model treats the training data too seriously, it failed to learn any meaningful pattern out of it, but simply memorizing everything it has seen.\n\nNow, one solution to solve this issue is called regularization. The idea is applying an L1 norm to the solution vector of your machine learning problem (In case of deep learning, it\u2019s the neural network weights.), and trying to make it as small as possible. So if your initial goal is finding the best vector x to minimize a loss function f(x), your new task should incorporate the L1 norm of x into the formula, finding the minimum (f(x) + L1norm(x)). The big claim they often throw at you is this: An x with small L1 norm tends to be a sparse solution. Being sparse means that the majority of x\u2019s components (weights) are zeros, only few are non-zeros. And a sparse solution could avoid over-fitting.\n\nThat\u2019s it, that\u2019s how they explain it in most of the articles, textbooks, materials. Giving an idea without explanation feels like pushing a spear through the back of my head.\n\nNot sure about you guys, but the reason for using an L1 norm to ensure sparsity and therefore avoid over-fitting wasn\u2019t so obvious to me. It took me some time to figure out why. Essentially, I had these questions:\n\nMy initial confusion came from the fact that I only looked at the L1 norm and only thought about what it means for L1 norm to be small. What I should really do, however, is thinking the loss function and the L1 norm penalty as a whole.\n\nLet me explain it from the beginning, the over-fitting problem. I\u2019d like to use a concrete example. Suppose you purchased a robot and you want to teach him to classify the Chinese characters by looking at the following example:\n\nThe first 5 characters belong to the first category, the last 5 are the second category. And these 10 characters are the only training data you have.\n\nNow, unfortunately, the robot is too smart for the task. It has large enough memory to remember 5 characters. After seeing all the 10 characters, the robot learned a way to categorize them: It remembers all the first 5 characters exactly. As long as a character is not one of those 5, the robot will put the character into the second category.\n\nOf course, this method will work very well on the 10 training characters, as the robot can achieve 100% accuracy. However, you provide a new character:\n\nThis character should belong to the first category. But because it never appeared in the training data, the robot hasn\u2019t seen it before. Based on its algorithm, the robot will put this character into the second category, which is wrong.\n\nIt should be pretty obvious for us human to see the pattern here. All characters that belong to the first category have a common part. The robot failed the task because it\u2019s too smart and the training data is too small.\n\nThis is the problem of over-fitting. But what is regularization and why can sparsity avoid over-fitting?\n\nNow suppose you got angry at your robot. You banged the head of the robot with a hammer, and while doing it, you shook some of its memory chips off his head. You essentially have made the robot dumber. Now, instead of being able to memorize 5 characters, the robot can only remember a character part.\n\nYou let the robot do the training again by looking at all 10 characters and still force him to achieve the same accuracy. Because he can\u2019t remember all 5 characters this time, you essentially force him to look for a simpler pattern. Now he discovers the common part of all the category A characters!\n\nThis is exactly what L1 norm regularization does. It bangs on your machine (model) to make it \u201cdumber\u201d. So instead of simply memorizing stuff, it has to look for simpler patterns from the data. In the case of the robot, when he could remember 5 characters, his \u201cbrain\u201d has a vector of size 5: [\u628a, \u6253, \u6252, \u6355, \u62c9]. Now after regularization (banging), 4 slots of his memory became unusable. Therefore the newly learned vector is: [\u624c, 0, 0, 0, 0] and clearly, this is a sparse vector.\n\nMore formally, when you are solving a large vector x with less training data. The solutions to x could be a lot.\n\nHere A is a matrix that contains all the training data. x is the solution vector you are looking for. b is the label vector.\n\nWhen data is not enough and your model\u2019s parameter size is large, your matrix A will not be \u201ctall\u201d enough and your x is very long. So the above equation will look like this:\n\nFor a system like this, the solutions to x could be infinite. To find a good one out of those solutions, you want to make sure each component of your selected solution x captures a useful feature of your data. By L1 regularization, you essentially make the vector x smaller (sparse), as most of its components are useless (zeros), and at the same time, the remaining non-zero components are very \u201cuseful\u201d.\n\nAnother metaphor I can think of is this: Suppose you are the king of a kingdom that has a large population and an OK overall GDP, but the per capita is very low. Each one of your citizens is lazy and unproductive and you are mad. Therefore you command \u201cbe productive, strong and hard working, or you die!\u201d And you enforce the same GDP as before. As a result, many people died due to your harshness, those who survived your tyranny became really capable and productive. You can think the population here is the size of your solution vector x, and commanding people to be productive or die is essentially regularization. In the regularized sparse solution, you ensure that each component of the vector x is very capable. Each component must capture some useful feature or pattern of the data.\n\nAnother way of regularization in deep learning is dropout. The idea is simple, removing some random neural connections from the neural network while training and adding them back after a while. Essentially this is still trying to make your model \u201cdumber\u201d by reducing the size of the neural network and put more responsibilities and pressure on the remaining weights to learn something useful. Once those weights have learned good features, then adding back other connections to embrace new data. I\u2019d like to think this adding back connection thing as \u201cintroducing immigrants to your kingdom when your are in short hands\u201d in the above metaphor.\n\nBased on this \u201cmaking model dumber\u201d idea, I guess we can come up with other similar ways to avoid over-fitting, such as starting with a small network and gradually adding new neurons and connections to the network when more data is available. Or performing a pruning while training to get rid of connections that are close to zero.\n\nSo far we have demonstrated why sparsity can avoid over-fitting. But why adding an L1 norm to the loss function and forcing the L1 norm of the solution to be small can produce sparsity?\n\nYesterday when I first thought about this, I used two example vectors [0.1, 0.1] and [1000, 0]. The first vector is obviously not sparse, but it has the smaller L1 norm. That\u2019s why I was confused, because looking at the L1 norm alone won\u2019t make this idea understandable. I have to consider the entire loss function as a whole.\n\nLet\u2019s go back to the problem of Ax = b, with a simple and concrete example. Suppose we want to find a line that matches a set of points in 2D space. We all know that you need at least 2 points to fix a line. But what if the training data has only one point? Then you will have infinite solutions: every line that passes through the point is a solution. Suppose the point is at [10, 5], and a line is defined as a function y = a * x + b. Then the problem is finding a solution to this equation:\n\nSince b = 5 \u2013 10 * a, all points on this following line b = 5 \u2013 10 * a should be a solution:\n\nBut how to find the sparse one with L1 norm?\n\nL1 norm is defined as the summation of absolute values of a vector\u2019s all components. For example, if a vector is [x, y], its L1 norm is |x| + |y|.\n\nNow if we draw all points that have a L1 norm equals to a constant c, those points should form something (in red) like this:\n\nThis shape looks like a tilted square. In high dimension space, it will be an octahedron. Notice that on this red shape, not all points are sparse. Only on the tips, points are sparse. That is, either x or y component of a point is zero. Now the way to find a sparse solution is enlarging this red shape from the origin by giving an ever-growing c to \u201ctouch\u201d the blue solution line. The intuition is that the touch point is most likely at a tip of the shape. Since the tip is a sparse point, the solution defined by the touch point is also a sparse solution.\n\nAs an example, in this graph, the red shape grows 3 times till it touches the blue line b = 5\u201310 * a. The touch point, as you can see, is at a tip of the red shape. The touch point [0.5, 0] is a sparse vector. Therefore we say, by finding the solution point with the smallest L1 norm (0.5) out of all possible solutions (points on the blue line), we find a sparse solution [0.5, 0] to our problem. At the touch point, the constant c is the smallest L1 norm you could find within all possible solutions.\n\nThe intuition of using L1 norm is that the shape formed by all points whose L1 norm equals to a constant c has many tips (spikes) that happen to be sparse (lays on one of the axises of the coordinate system). Now we grow this shape to touch the solutions we find for our problem (usually a surface or a cross-section in high dimension). The probability that the touch point of the 2 shapes is at one of the \u201ctips\u201d or \u201cspikes\u201d of the L1 norm shape is very high. That\u2019s why you want to put L1 norm into your loss function formula so that you can keep looking for a solution with a smaller c (at the \u201csparse\u201d tip of the L1 norm). (So in the real loss function case, you are essentially shrinking the red shape to find a touch point, not enlarging it from the origin.)\n\nDoes L1 norm always touch the solution at a tip and find us a sparse solution? Not necessarily. Suppose we still want to find a line out of 2D points, but this time, the only training data is a point [1, 1000]. In this case, the solution line b = 1000 -a is in parallel to one of the edges of the L1 norm shape:\n\nEventually, they touch on an edge, not by a tip. Not only you can\u2019t have a unique solution this time, most of your regularized solutions are still not sparse (other than the two tip points.)\n\nBut again, the probability of touching a tip is very high. I guess this is even more true for high dimension, real-world problems. As when your coordinate system has more axises, your L1 norm shape should have more spikes or tips. It must look like a cactus or a hedgehog! I can\u2019t imagine.\n\nIf you push a person towards a cactus, the probability of he being pricked by the needles is pretty high. That\u2019s also why they invented this pervert weapon and that\u2019s why they want to use L1 norm.\n\nBut is the L1 norm the best kind of norm to find a sparse solution? Well, it turns out that the Lp norm when 0 <= p < 1 gives the best result. This can be explained by looking at the shapes of different norms:\n\nAs you can see, when p < 1, the shape is more \u201cscary\u201d, with more sharpen, outbreaking spikes. Whereas when p = 2, the shape becomes a smooth, non-threatening ball. Then why not letting p < 1? That\u2019s because when p < 1, there are calculation difficulties.\n\nIn conclusion, over-fitting is a problem you see when your machine learning model is too large (has too many parameters) comparing to your available training data. In this case, the model tends to remember all training cases including noisy to achieve better training score. To avoid this, regularization is applied to the model to (essentially) reduce its size. One way of regularization is making sure the trained model is sparse so that the majority of it\u2019s components are zeros. Those zeros are essentially useless, and your model size is in fact reduced.\n\nThe reason for using L1 norm to find a sparse solution is due to its special shape. It has spikes that happen to be at sparse points. Using it to touch the solution surface will very likely to find a touch point on a spike tip and thus a sparse solution.\n\nJust think about this:\n\nWhen there is a zombie outbreak, which one should be the weapon of choice?"
    },
    {
        "url": "https://medium.com/@shiyan/use-intel-spmd-program-compiler-to-accelerate-my-neural-network-800ea311a509?source=user_profile---------12----------------",
        "title": "Use Intel SPMD Compiler to accelerate my neural network",
        "text": "Today, there was this new thing called ISPC posted on HackerNews. It is an intel compiler that allows you to write c like programs to make use of SIMD automatically.\n\nI thought this was new, but apparently it\u2019s been around for a while. I must have lived under a rock!\n\nI immediately thought that this could accelerate my neural network library. I have been always wanted to use SIMD. I didn\u2019t because, first, there doesn\u2019t seem to be any cross-compiler SIMD solution. Compiler intrinsics seem to be the suggested way, but I hate writing different versions for different compilers. Second, there are so many different SIMD \u201clevels\u201d, from SSE2 to AVX2. Writing different code to fully use SIMD on different platforms is also difficult.\n\nThis ISPC seems to be a good solution to these two problems. I quickly tried it to replace my neural network forward kernel. I got 4.5x acceleration. Not bad!"
    },
    {
        "url": "https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714?source=user_profile---------13----------------",
        "title": "Understanding LSTM and its diagrams \u2013 ML Review \u2013",
        "text": "Although we don\u2019t know how brain functions yet, we have the feeling that it must have a logic unit and a memory unit. We make decisions by reasoning and by experience. So do computers, we have the logic units, CPUs and GPUs and we also have memories.\n\nBut when you look at a neural network, it functions like a black box. You feed in some inputs from one side, you receive some outputs from the other side. The decision it makes is mostly based on the current inputs.\n\nI think it\u2019s unfair to say that neural network has no memory at all. After all, those learnt weights are some kind of memory of the training data. But this memory is more static. Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.\n\nThe naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.\n\nA lot of times, you need to process data that has periodic patterns. As a silly example, suppose you want to predict christmas tree sales. This is a very seasonal thing and likely to peak only once a year. So a good strategy to predict christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.\n\nTheoretically the naively connected neural network, so called recurrent neural network, can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.\n\nThen later, LSTM (long short term memory) was invented to solve this issue by explicitly introducing a memory unit, called the cell into the network. This is the diagram of a LSTM building block.\n\nAt a first sight, this looks intimidating. Let\u2019s ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. X_t is the input of the current time step. h_t-1 is the output from the previous LSTM unit and C_t-1 is the \u201cmemory\u201d of the previous unit, which I think is the most important input. As for outputs, h_t is the output of the current network. C_t is the memory of the current unit.\n\nTherefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.\n\nThe way its internal memory C_t changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves.\n\nThe first valve is called the forget valve. If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through.\n\nThe second valve is the new memory valve. New memory will come in through a T shaped joint like above and merge with the old memory. Exactly how much new memory should come in is controlled by the second valve.\n\nOn the LSTM diagram, the top \u201cpipe\u201d is the memory pipe. The input is the old memory (a vector). The first cross \u2716 it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.\n\nThen the second operation the memory flow will go through is this + operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the \u2716 below the + sign.\n\nAfter these two operations, you have the old memory C_t-1 changed to the new memory C_t.\n\nNow lets look at the valves. The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it\u2019s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.\n\nNow the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory.\n\nThe new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.\n\nThese two \u2716 signs are the forget valve and the new memory valve.\n\nAnd finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit.\n\nThe above diagram is inspired by Christopher\u2019s blog post. But most of the time, you will see a diagram like below. The major difference between the two variations is that the following diagram doesn\u2019t treat the memory unit C as an input to the unit. Instead, it treats it as an internal thing \u201cCell\u201d.\n\nI like the Christopher\u2019s diagram, in that it explicitly shows how this memory C gets passed from the previous unit to the next. But in the following image, you can\u2019t easily see that C_t-1 is actually from the previous unit. and C_t is part of the output.\n\nThe second reason I don\u2019t like the following diagram is that the computation you perform within the unit should be ordered, but you can\u2019t see it clearly from the following diagram. For example to calculate the output of this unit, you need to have C_t, the new memory ready. Therefore, the first step should be evaluating C_t.\n\nThe following diagram tries to represent this \u201cdelay\u201d or \u201corder\u201d with dash lines and solid lines (there are errors in this picture). Dash lines means the old memory, which is available at the beginning. Some solid lines means the new memory. Operations require the new memory have to wait until C_t is available.\n\nBut these two diagrams are essentially the same. Here, I want to use the same symbols and colors of the first diagram to redraw the above diagram:\n\nThis is the forget gate (valve) that shuts the old memory:\n\nThis is the new memory valve and the new memory:\n\nThese are the two valves and the element-wise summation to merge the old memory and the new memory to form C_t (in green, flows back to the big \u201cCell\u201d):\n\nThis is the output valve and output of the LSTM unit:"
    },
    {
        "url": "https://medium.com/@shiyan/materials-to-understand-lstm-34387d6454c1?source=user_profile---------14----------------",
        "title": "Materials to understand LSTM \u2013 Shi Yan \u2013",
        "text": "People never judge an academic paper by those user experience standards that they apply to software. If the purpose of a paper were really promoting understanding, then most of them suck. A while ago, I read this article talking about academic pretentiousness and it speaks my heart out. My feeling is, papers are not for better understanding but rather for self-promotion. It\u2019s a way for scholars to declare achievements and make others admire. Therefore the golden standard for an academic paper has always been letting others acknowledge the greatness, but never understand enough to surpass itself.\n\nWhen it comes to LSTM, for example, there aren\u2019t many good materials. Most likely what they would show you is this bullshit image:\n\nThere are many things on this image pisses me off.\n\nFirst of all, they use these f shapes (with a footnote \u201cf\u201d that looks like a \u201ct\u201d)to denote non-linearity and at the same time, they have f_t in their equations. And they are not the same thing!\n\nSecond, they have these dash lines and solid lines to represent time delay. So solid lines are C_t, and dash lines are C_t-1. There are 5 arrows coming out of the \u201cCell\u201d, but only 4 are labelled. One C_t is incorrectly labelled with dash line. One solid line is supposed to be a dash line and C_t-1.\n\nThird, what the hell are black dots? You have to look at the equation to figure out that they are element-wise multiplications of two vectors.\n\nAnd finally, C_t is supposed to be calculated as a summation of f_t * C_t-1 and i_t * tanh(W_xc * x_t + W_hc*h_t-1 +b_c) (the third equation). But the image completely misses the plus sign.\n\nCompared to this shitty image, the following version is slightly better:\n\nThe 3 C_t-1 are correctly represented as dash lines. The plus sign is there too. Sigmoid functions are written with the proper sigma sign, not some freaking f_f.\n\nSee, things don\u2019t have to be so difficult. And understanding shouldn\u2019t be a painful experience. But unfortunately, academic writing makes it so almost all the time.\n\nPerhaps the best learning material is really this excellent blog post:\n\nAnd its diagram is simply beautiful:\n\nI think the most evil thing about the first two versions are that they treat C (the cell, or the memory) not as an input to the LSTM block, but rather a thing that is internal to the block. And they adopt this complex dash line, solid line thing to represent delay. This is really confusing.\n\nBut I do noticed one difference between the first two diagrams and the last one. The first two diagrams sum up the inputs with the outputs from the previous layer to calculate the gates (for example, f_t and i_t). But in the third version, the author concatenate them. I don\u2019t know if it is because the third version is a variation or this doesn\u2019t matter in terms of correctness. (But I don\u2019t think it should be concatenation because the vector size h shouldn\u2019t be changed. If it is really concatenation, the vector size of h will change to the size of x+h?)\n\nI drew a new diagram, which I think is better:"
    },
    {
        "url": "https://medium.com/@shiyan/get-a-taste-of-reinforcement-learning-implement-a-tic-tac-toe-agent-deda5617b2e4?source=user_profile---------15----------------",
        "title": "Get a taste of reinforcement learning \u2014 implement a tic tac toe agent",
        "text": "I got really interested into reinforcement learning after seeing the Berkeley robotic arm demo, as this could be one of the ways AI interacts with the real world. I dreamed of making robots doing kitchen chores, such as peeling potatoes and this method looks like a potential solution.\n\nThe idea behind this is called deep reinforcement learning. Similar idea has been applied to computer AI that plays video games automatically. The most famous paper in this area is this:\n\nHere is a good introductory material about the reinforcement learning method used in the above paper:\n\nI think the basic idea of reinforcement learning can be summarized as the follows:\n\nTo accomplish a goal, one often needs to perform several steps. Each step will receive an immediate feedback (a score for example) measuring how much this step helps achieve the final goal. In addition, this step also changes the state of the system.\n\nTo make a smart move, however, one needs to consider not only the immediate reward, but also future probability of winning.\n\nWhile it is easy to calculate the immediate gain, it\u2019s difficult to know the future probability of winning. This is where machine learning comes in. This future probability of winning is predicted by a quality function (the q function). The traditional way of getting the q function is an iterative process, where you start with a random q function, and gradually it converges to a good enough quality function.\n\nMore formally, suppose at anytime, the status of the system, be it a game or a robot, can be represented by a state S. For example, if the system you are looking at is a Super Mario game, the state S can be the lives you have, the scores, the speed and coordinates of Mario and those enemies. If the system is a robotic arm, then the state S might contain the angle and force of the joints.\n\nNow suppose we have a function R(S), which means the total reward from the state S till the end of the game. R(S) can be represented by the following\n\nWhere S means the current game state. The reward equals to an immediate reward r plus the reward of the next state S+1, and all the following rewards of state S+2, S+3 \u2026\n\nNow to make a good decision, your action at the state S should maximize the R(S). This decision takes not only the short term reward r into consideration but also the long term rewards, so that your move is not shortsighted.\n\nIdeally, you should look at the scores of following states all the way to the end of the game. But in practice, because the system is very dynamic and noisy, looking too far ahead is unnecessary. Therefore a decay factor d should be added:\n\nThis decay factor means that the further a state is away from the current state, the less it has impact on the current decision making.\n\nNow suppose we call the maximized R(S) as Q(S), with the above decay factor, the equation can be simplified to:\n\nThis function equation is called the Bellman equation. The way to solve this equation is often an iterative process, where a random Q function, which is defined as a look-up table, is used at first. The function will converge if you repeatedly apply the above equation on different states and actions. There are not many materials about why it converges. So I don\u2019t understand it at the moment. But my gut feeling is that it has something to do with back induction, where the steps that are close to the end of a game is corrected first, and the model gradually propagates to the beginning steps.\n\nTo be more clear, suppose you want to train an AI to play chess. It is very difficult to figure out which first move can improve your chance of winning. Instead, one should learn the strategy in a backward manner by looking at the last step first, for example, at a check situation. When one side is in check, one has to seek defense otherwise he immediately loses the game. Since checking gives you less options, you can easily make good decisions. Once these decision strategies are learnt, the AI will look at the second last steps to maybe avoid checking or conduct checking. And similarly it will learn the third last steps, and the forth last steps, till the beginning position. Numerically, you will see the Q function converges as it learns the strategies.\n\nThis is called reinforcement learning. Deep reinforcement learning is similar but uses a neural network to represent the Q function instead of a look-up table. The benefit of deep reinforcement learning is that it can handle high dimension data, i.e. when you have too many properties for each state and the actions you can perform are also numerous."
    },
    {
        "url": "https://medium.com/@shiyan/on-facebook-s-video-improvements-for-vr-162f9dc25d41?source=user_profile---------16----------------",
        "title": "On Facebook\u2019s video improvements for VR \u2013 Shi Yan \u2013",
        "text": "Today I saw this Facebook post on Hackernews. I think it\u2019s super genius but simple as hell. My friend actually did something similar to optimize pixel shader for VR. I want to summarize what I\u2019ve learnt.\n\nThe computation complexity of an image processing algorithm, be it a piece of pixel shader or a video compression algorithm can be simply measured by the count of pixels or the area of the image. So when you want to improve the performance of your image processing algorithm, one idea is reducing the pixel count of the input image. But you need to consider two things.\n\nSo the Facebook optimization as well as the shader trick my friend did are all based on these two ideas. I\u2019d like to start with the Facebook idea for VR video streaming.\n\nVR is getting more and more popular these days thanks to the availability of VR hardwares. As a result, it is of interest to stream 360 videos. But the current 360 video streaming is built with the current video technology that is mainly designed for 2D content. When the content is a 360 video on a sphere, a common method is remapping the sphere onto a 2D image the same way you would map a globe on a 2D map.\n\nThe problem of this \u201cmap\u201d is that the pole areas are stretched, but those areas contain the same amount of information and should not receive special attention. One can easily see the problem by checking the true size of Greenland. The reason for stretching them is just for mapping a sphere to 2D, but it increases computation and data size.\n\nSo the first optimization Facebook did was using Cube map. Cube map stretches the image less than the map solution and it stretches the image uniformly. This immediately saves bandwidth.\n\nBut when people use VR, the area that is currently visible is just a small portion of the sphere. Does it make sense to treat each pixel with the same amount of attention? One may argue that we could simply discard the facets of the cube map that are currently unseen. But this naive approach causes trouble in the streaming case, because the user might rotate the viewing angle rapidly, it\u2019s hard to deliver a previous discarded facet timely when the user turns into a new direction. So we better still stream the entire sphere, but treat those pixels that are unseen with less attention.\n\nThe way they do it is putting the original sphere inside a pyramid and project it onto the facets of the pyramid. The area that is most visible is projected onto the bottom of the pyramid, which has the largest size among the facets. And the rest areas which are either on the side or behind the user are projected on the four triangles. Pixels on the area that is opposite to the viewing angle is projected on the tips of the 4 triangles, so they will receive less attention when being compressed as a video.\n\nThe apparent trade-off is that if the user suddenly turns into a different direction, he/she will experience a slightly blurry image, but this will soon be corrected by adjusting the viewing angle of the pyramid projection algorithm.\n\nIn reality, instead of adjusting the viewing angle of the pyramid projection method, they pre-generate a set of videos for several angles and jump among those videos when the viewing angle changes.\n\nNow, I want to talk about the method my friend did to optimize pixel shader. When you render a real-time scene for VR, the pixels that are apart from the viewing angle should be calculated less. So he projected the curved surface in front of one of your eyes onto a homogenous coordinates, this is essentially the same idea as cube map."
    },
    {
        "url": "https://medium.com/@shiyan/tried-lstm-music-generation-51b2f2213bcf?source=user_profile---------17----------------",
        "title": "Tried LSTM music generation \u2013 Shi Yan \u2013",
        "text": "My friend trained a LSTM network to generate ancient style Chinese poems. If you could read Chinese, you can tell that the generated results are pretty plausible:\n\nAlthough the contents of the poems are arguably meaningless, it would take an innocent person a while to figure that out, as ancient Tang poems are meant to be obscure. And immediately we had noticed two things from the results. A, the format of the poems are exactly correct! They are formed by either 4 or 8 short half-sentences with each sentence formed by either 5 or 7 characters. B, the sentences rhythm, even the pronunciation of characters wasn\u2019t part of the input. More than that, Tang poems and many other Chinese literature forms tend to use word pairs with similar or opposite meanings in adjacent sentences. And this had been learnt also by the network. In the above result, for example, the first half of the second sentence ends with \u201cSnow (\u96ea)\u201d and in the following half, the corresponding character is \u201cFrost(\u971c)\u201d.\n\nSimilar experiments are easy to set up thanks to the char-rnn project. I tried to apply it to the novels of Jin Yong, who I think is the Chinese counterpart of George R. R. Martin. I had the hope that a well-trained network could generate some funny fictional pieces. But the network didn\u2019t converge well. I couldn\u2019t find long phrases with more than 5 characters make sense. But it did get the punctuation marks correctly. So if you stand 5 meters away from the text, it will still look plausible:\n\nIn the above result, you can easily find names from Jin Yong\u2019s works, such as \u59d1\u82cf\u6155\u5bb9\u6c0f, \u5b8c\u989c\u6d2a\u70c8, \u6210\u6606 and \u6bb7\u7d20\u7d20. This means that the network recognized them as short terms. But it never understands that they can be abstracted as the concept of name. And when generating a fake novel, names can be made up; they don\u2019t have to come from the original works. This made me wonder how far away we are from the real artificial intelligence.\n\nAs a second try, I trained a network to generate music. Because the character size for music note is way smaller than Chinese and judging the quality of a music is not as easy as judging a piece of text, I hoped for more exciting results.\n\nI still haven\u2019t found the optimized parameters to train the network. I had converging issues and overfitting. But some short generated music pieces are actually pretty good.\n\nThis is a raw output of my network.\n\nAnd this is a piece I composed with several outputs.\n\nI\u2019m still experimenting the network for better results."
    },
    {
        "url": "https://medium.com/@shiyan/how-i-usually-explain-cryptocurrency-1d06d1a46980?source=user_profile---------23----------------",
        "title": "How I usually explain cryptocurrency. \u2013 Shi Yan \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@shiyan/calligraphr-6f30859f8d0e?source=user_profile---------24----------------",
        "title": "Calligraphr \u2013 Shi Yan \u2013",
        "text": "Characters are the major barriers for Chinese language learners. As an extreme case, the single character biang has 56 strokes. Learning characters is not only about memorizing how characters look, but also how they should be written, how different parts of a character should be proportioned to each others. Needless to say, there are hundreds of writing styles. For this matter, stroke order plays an important role.\n\nAlthough learning curve is very steep for the language, effective learning methods seem to be rear. In elementary school, for example, Chinese students practice writing on a special paper with guide lines. This learning experience is not ideal, because it\u2019s very repetitive, not interactive and just boring.\n\nIntroducing calligraphr, our hack for the google glass education hackathon to improve this learning experience. We use augmented reality to project an interactive character animation on a real paper. Students can use the traditional brush and ink to trace down the character stroke by stroke. Once finished, our program can automatically judge the work and give a score. Like games, characters are organized by difficulties. Users can start with simple characters and proceed to further levels.\n\nTechnology is under the blame of ruining traditions. We forget how to write because of the keyboard. We don\u2019t know how to spell due to auto completion. But really they shouldn\u2019t be a pair of enemies. With wearable technologies like google glass, we can bridge something old and classic with something new."
    }
]