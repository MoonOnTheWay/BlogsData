[
    {
        "url": "https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59?source=user_profile---------1----------------",
        "title": "Visualizing parts of Convolutional Neural Networks using Keras and Cats",
        "text": "It is well known that convolutional neural networks (CNNs or ConvNets) have been the source of many major breakthroughs in the field of Deep learning in the last few years, but they are rather unintuitive to reason about for most people. I\u2019ve always wanted to break down the parts of a ConvNet and see what an image looks like after each stage, and in this post I do just that!\n\nFirst off, what are ConvNets good at? ConvNets are used primarily to look for patterns in an image. You did that by convoluting over an image and looking for patterns. In the first few layers of CNNs the network can identify lines and corners, but we can then pass these patterns down through our neural net and start recognizing more complex features as we get deeper. This property makes CNNs really good at identifying objects in images.\n\nA CNN is a neural network that typically contains several types of layers, one of which is a convolutional layer, as well as pooling, and activation layers.\n\nTo understand what a CNN is, you need to understand how convolutions work. Imagine you have an image represented as a 5x5 matrix of values, and you take a 3x3 matrix and slide that 3x3 window around the image. At each position the 3x3 visits, you matrix multiply the values of your 3x3 window by the values in the image that are currently being covered by the window. This results in a single number the represents all the values in that window of the image. Here\u2019s a pretty gif for clarity:\n\nAs you can see, each item in the feature matrix corresponds to a section of the image. Note that the value of the kernel matrix is the red number in the corner of the gif.\n\nThe \u201cwindow\u201d that moves over the image is called a kernel. Kernels are typically square and 3x3 is a fairly common kernel size for small-ish images. The distance the window moves each time is called the stride. Additionally of note, images are sometimes padded with zeros around the perimeter when performing convolutions, which dampens the value of the convolutions around the edges of the image (the idea being typically the center of photos matter more).\n\nThe goal of a convolutional layer is filtering. As we move over an image we effective check for patterns in that section of the image. This works because of filters, stacks of weights represented as a vector, which are multiplied by the values outputed by the convolution.When training an image, these weights change, and so when it is time to evaluate an image, these weights return high values if it thinks it is seeing a pattern it has seen before. The combinations of high weights from various filters let the network predict the content of an image. This is why in CNN architecture diagrams, the convolution step is represented by a box, not by a rectangle; the third dimension represents the filters.\n\nPooling works very much like convoluting, where we take a kernel and move the kernel over the image, the only difference is the function that is applied to the kernel and the image window isn\u2019t linear.\n\nMax pooling and Average pooling are the most common pooling functions. Max pooling takes the largest value from the window of the image currently covered by the kernel, while average pooling takes the average of all values in the window.\n\nActivation layers work exactly as in other neural networks, a value is passed through a function that squashes the value into a range. Here\u2019s a bunch of common ones:\n\nThe most used activation function in CNNs is the relu (Rectified Linear Unit). There are a bunch of reason that people like relus, but a big one is because they are really cheap to perform, if the number is negative: zero, else: the number. Being cheap makes it faster to train networks.\n\nBefore we get into what a CNN looks like, a little bit of background. The first successful applications of ConvNets was by Yann LeCun in the 90\u2019s, he created something called LeNet, that could be used to read hand written numbers. Since then, computing advancements and powerful GPUs have allowed researchers to be more ambitious. In 2010 the Stanford Vision Lab released ImageNet. Image net is data set of 14 million images with labels detailing the contents of the images. It has become one of the research world\u2019s standards for comparing CNN models, with current best models will successfully detect the objects in 94+% of the images. Every so often someone comes in and beats the all time high score on imagenet and its a pretty big deal. In 2014 it was GoogLeNet and VGGNet, before that it was ZF Net. The first viable example of a CNN applied to imagenet was AlexNet in 2012, before that researches attempted to use traditional computer vision techiques, but AlexNet outperformed everything else up to that point by ~15%.\n\nAnyway, lets look at LeNet:\n\nThis diagram doesn\u2019t show the activation functions, but the architecture is:\n\nHere is an image of a cat:\n\nOur picture of the cat has a height 320px, a width of 400px, and 3 channels of color (RGB).\n\nSo what does he look like after one layer of convolution?\n\nHere is the cat with a kernel size of 3x3 and 3 filters (if we have more than 3 filter layers we cant plot a 2d image of the cat. Higher dimensional cats are notoriously tricky to deal with.).\n\nAs you can see the cat is really noisy because all of our weights are randomly initialized and we haven\u2019t trained the network. Oh and they\u2019re all on top of each other so even if there was detail on each layer we wouldn\u2019t be able to see it. But we can make out areas of the cat that were the same color like the eyes and the background. What happens if we increase the kernel size to 10x10?\n\nAs we can see, we lost some of the detail because the kernel was too big. Also note the shape of the image is slightly smaller because of the larger kernel, and because math governs stuff.\n\nWhat happens if we squish it down a bit so we can see the color channels better?\n\nMuch better! Now we can see some of the things our filter is seeing. It looks like red is really liking the black bits of the nose an eyes, and blue is digging the light grey that outlines the cat. We can start to see how the layer captures some of the more important details in the photo.\n\nIf we increase the kernel size its far more obvious now that we get less detail, but the image is also smaller than the other two.\n\nWe get rid of of a lot of the not blue-ness by adding a relu.\n\nWe add a pooling layer (getting rid of the activation just max it a bit easier to show)\n\nAs expected, the cat is blockier, but we can go even blockyier!\n\nNotice how the image is now about a third the size of the original.\n\nWhat do the cats look like if we put them through the convolutional and pools sections of LeNet?\n\nConvNets are powerful due to their ability to extract the core features of an image and use these features to identify images that contain features like them. Even with our two layer CNN we can start to see the network is paying a lot of attention to regions like the whiskers, nose, and eyes of the cat. These are the types of features that would allow the CNN to differentiate a cat from a bird for example.\n\nCNNs are remarkably powerful, and while these visualizations aren\u2019t perfect, I hope they can help people like myself who are still learning to reason about ConvNets a little better.\n\nAll code is on Github: https://github.com/erikreppel/visualizing_cnns\n\nFollow me on Twitter, I\u2019m @programmer (yes, seriously).\n\nA guide to convolution arithmetic for deep learning by Vincent Dumoulin and Francesco Visin"
    }
]