[
    {
        "url": "https://medium.com/@tyreeostevenson/logistic-regression-and-pokemon-945e954d84a3?source=user_profile---------1----------------",
        "title": "Logistic Regression and Pokemon \u2013 Tyree Stevenson \u2013",
        "text": "In this post you will learn how to use Logistic Regression to successfully predict whether a Pokemon is a fire type.\n\nThe act of classifying is determining what category an object belongs to. Let\u2019s say you have a bouncy circular object, and you want to know if it is a rock or not. By observing the features of this object, you can quickly distinguish whether it is a rock or not. We can assume rocks are not bouncy. Unfortunately for you, the object is bouncy thus it is not a rock. The human brain is complex enough to do task like this almost instantly, so this example seems trivial. Thus, classification is determining what category an object belongs to.\n\nFor now let\u2019s talk about computers classifying objects in the simplest way possible, which is determining an objects category. Computers operate in binary, meaning every decision a computer makes is choosing between 1 and 0. Let\u2019s say the value of a rock is 1 and if not a rock then it is 0. So if we can give a computer data, it would determine whether an object is a rock or not by seeing if the value of the object is 1.\n\nLogistic Regression is a supervised binary classification technique. What does that mean? That means you can give your Logistic Regression model labeled data, and the models decides whether or not the data belongs to a specific group.\n\nRecall your rock object. Let\u2019s say the features of this object are bounciness level, volume, and buoyancy. We can represent those features mathematically as a feature matrix.\n\nIf you do not understand feature matrices you can click here. Each of the columns within this feature matrix are independent x values. Great, so how do we calculate a classification?\n\nOnce again if you do not understand this notation for the theta and x\u2019s click here.\n\nThe sigmoid function is perfect because it will output a value between 0 and 1\n\nThis is perfect for binary classification because it\u2019s just like computers, we can determine an object classification by whether it is a 0 or 1. However, we seldom get an exact value majority of times our output is a decimal. So, we usually set a cut off point typically at 0.5. So, if it is above or equal to 0.5 you can interpret the value has 1 or if below 0.5 you can interpret the value as 0. Back to our rock example. If our circular object value has 0.6 then it can be classified as a rock; however, if our circular object was 0.3 then we can classify it as not being a rock.\n\nWhen you graph the sigmoid function it appears as\n\nThe graph represents how the function output cannot surpass 0 or 1. Also, showing how 0.5 is the cut off point between a classification.\n\nGreat question! Later in this post, we will discuss calculating the accuracy of a model. For now, let\u2019s focus on your loss. What is the loss? The loss is the difference between the value of your model\u2019s prediction and the objects actually value. For example, if the object is a rock then it\u2019s value is 1. If your model predicted 0.5, while it is still in the ballpark for a correct estimate since you did not estimate 1, your loss on the prediction was 0.5. The function we can use to calculate this loss is\n\nHowever, this function can be written as\n\nNotice that depending on our y value it will cancel out one of the addends.\n\nIn your Logistic Regression model, your theta\u2019s are essential to the model. Our end goal is to find the perfect theta values so that given any x we can accurately predict its classification. So you essentially want to treat your thetas with love and care. How does one love their thetas? Training! Training is machine learning jargon for placing your thetas through a series of iterations in order to find their optimal value, this can be done by using Gradient Descent.\n\nGradient descent is an optimization technique used to find the minimum value of a function. You can imagine Gradient Descent as a ball rolling down a bowl. The goal of this ball is to find the bottom of the bowl if you want a more in-depth explanation click here. In our case, this will result in us finding the optimal thetas. In our quest to find the perfect thetas we will use\n\nPokemon come in several types with some being multiple types. For the sake of this project, let\u2019s assume every Pokemon has one type. You can think of a Pokemon type as it\u2019s classification. So you will either have a water, fire, grass, fighting, bug, dragon, rock, or ect type Pokemon. A Pokemon's type will affect the stats of that Pokemon. Meaning given a Pokemon\u2019s stats aka features we can use Logistic Regression to predict the Pokemon\u2019s type.\n\nThe dataset we are using is pokemon_alopez247.csv. You can use it by clicking here. If you have the Kaggle API installed on your machine you can download the dataset in your terminal with this command:\n\nLibraries you will be using are:\n\nNow you can use Pandas to setup your dataframe:\n\nGreat, next you will need to clean up your data. Remember that we are predicting values between 0 and 1. So in our Type_1 column. Every Pokemon that is a fire type will receive a score of 1 and all other types will be 0.\n\nwe find that there is a 14% correlation between Special Attack and a Pokemon\u2019s type. Also, there is a 17% correlation between Probability of a Pokemon being male and a Pokemon\u2019s type.\n\nGreat! We are going to use train_test_split from the sklearn.model_selection. This model,\n\nSo we are going to use 30% of our data as training data and 70% as testing data.\n\nNow we can set our theta\n\nYou can finally implement the sigmoid function!\n\nNow you can write your cost function:\n\nLet\u2019s implement your optimization function. Recall we are using Gradient Descent because it will allow us to find the optimal theta values.\n\nCongrats, you can now run your model!\n\nFor classification problems we can measure accuracy by:\n\nNow you can code this:\n\nWow! You were able to predict whether a Pokemon is a fire type with 85% accuracy.\n\nThanks for reading! If you have any questions or thoughts please comment below."
    },
    {
        "url": "https://medium.com/@tyreeostevenson/pokemon-stats-and-gradient-descent-for-multiple-variables-c9c077bbf9bd?source=user_profile---------2----------------",
        "title": "Pokemon Stats and Gradient Descent For Multiple Variables",
        "text": "Yes! Gradient Descent can be used to minimize multiple parameters of a function. If you are not familiar with Gradient Descent click here. In this post, you will use Python to implement Multivariable Gradient Descent for a Multivariate Linear Model. This will all be done from scratch! We will use this model to predict the catch rate of a Pokemon given it\u2019s Total and Special-Attack stats.\n\nLet\u2019s talk about Multivariate Linear Regression. If you are not familiar with Linear Regression click here. Recall that Linear Regression allows you to predict dependent y values using independent x values. For example, by using temperature outside you can predict the sales of ice cream.\n\nHowever, Multivariate Linear Regression allows you to use multiple x\u2019s (features) to predict a y value. A great example, of this, would be using temperature, distance from ice cream shop, and distances from smoothie shops to predict ice cream sales.\n\nGreat question! Once again features are Machine Learning terminology for variables. So how can you graph this model? Well, each feature of our dataset represents a dimension. Including the y variable! You are using features x to predict feature y.\n\nThis model contains 3 features Radio, Sales, and TV. Each feature correlates to a dimension that results in this model having 3 dimensions.\n\nSo we can easily assume the function for Multivariate Linear Regression is:\n\nHowever, the most efficient representation is:\n\nWhat are these hieroglyphics on the screen?\n\nA matrix is a rectangular array that holds values and we discuss the dimension of an array by its Rows x Columns.\n\nFor example, this is a 2 x 4 matrix. For the sake of this post, you can define a vector as an Amount of Rows x 1 matrix.\n\nGreat Question! In Machine Learning we can represent a group of data in a matrix. Each row of the matrix is an individual data point and the columns of that row are the data points\u2019 features. For example, if we had a matrix of Pokemon data\n\nOur matrix has 3 rows(aka 3 data points). The features of those data points are Name, HP, Attack, and Defense. What are the numbers on the side? You can think of those numbers as unique indexes this allows us to distinguish between data points. We can call the matrix above pokeMatrix now we can utilize our indexes. If you were to type\n\nyou would get back that data point\n\nRecall that we defined a vector as a Amount of Rows x 1 matrix. Lets visualize this\n\nBefore scrolling down, on a sheet of paper write the vector for the HP feature\n\nGreat! These values are on a separate line, because they each belong to their own rows.\n\nWhats is the difference between indexing the pokeMatrix\n\nWhen you index the pokeMatrix, you are grabbing a specific data point. However, when you are grabbing the HP feature you are getting every HP value in the matrix, thus resulting in you getting a vector containing those values. You can think of a matrix as a collection of vectors, so you are essentially grabbing one of those vectors within the pokeMatrix.\n\nData Points are also known as training examples. Our pokeMatrix has four rows thus it has four data points which means it has 4 training examples.\n\nLet\u2019s dive deep into the value of x. The x in our equation is an independent variable or independent feature we are using this independent feature to predict the dependent feature y. Recall our pokeMatrix\n\nLet\u2019s say we have a pokemon HP feature that depends on the pokemon\u2019s Defense feature. So we could use Vector4 to predict Vector2. Now let\u2019s imagine that a pokemon\u2019s HP depends on its Attack, Defense, and some number of other features. Instead of writing this pesky function\n\nwe can now put all of our feature vectors into a vector.\n\nAnd just as we can place our features in a vector we can do the same for our theta variables\n\nLet\u2019s reimagine our x vector from above, each entry within this x vector is a data point, and each data point will be an 1 x N vector. In our case we can imagine this as the amount of features. So let's make an x vector containing the HP, Attack, and Defense for each of our data points within the pokeMatrix.\n\nDoes this look familiar? Yes, our vectors within vectors have transformed into a feature matrix.\n\nIn this section, I will have to recommend Basic Linear Algebra for Deep Learning by Niklas Donges. To complete these mathematical operations you will need to understand vector/matrix multiplication, vector/matrix transpose, and vector/matrix addition and subtraction. Unfortunately, those topics are so dense I would need to write an entire medium post just explaining how to complete these operations. If you are not familiar with those operations please click on the suggested blog and re-join us! Due to time constraints, I must continue on to Gradient Descent.\n\nGreat, now that we understand matrix/vector operations let\u2019s talk about gradient descent! Recall that Gradient Descent is an optimization technique which allows you to find the minimum value of a function. In our case, we are looking for the minimum theta values that will give our Multivariate Linear Model the smallest loss value. Remember that loss is the difference between our predicted value and the actual value. So if we predicted a Pokemon\u2019s attack power was 76 when in actuality the attack power was 81 then we have a loss of 5.\n\nWe can use the function below to calculate Gradient Descent for multiple thetas:\n\nHowever, this is not scalable at all and would require us to hardcode in each theta. Instead we use the implementation bellow, which is scalable and easier to code:\n\nBy the power of vector operations, we can compute each theta simultaneously.\n\nWe are in search of the perfect thetas! Our goal in Gradient Descent is to find the optimal data values that for any given x we can perfectly predict y. This is a point I see glossed over in many tutorials! I want to stress the point that we are training our thetas and with these trained thetas we should be able to predict any y given, a x value that we have never seen before. In conclusion, we are training our theta values to accurately predict y when given an x value we have never before seen.\n\nNow that you understand the math we can talk about the more important things in life such as pokemon! In this post we will use Special Attack and Total stats to predict a Pokemon\u2019s Catch rate. The dataset we are using is pokemon_alopez247.csv you can use it by clicking here. If you have the Kaggle API installed on your machine, you can download the dataset in your terminal with this command:\n\nSince your data is already clean we can automatically plot it. Now you can import these libraries\n\nWe can grab our desired features from the data variable:\n\nUsing matplotlib.pyplot we can build a 3d figure\n\nNow we can graph our initial thetas\n\nPretty close, right? Our end goal is to have those red circles covering as much of our data points as possible.\n\nYou can now set your learning rate.\n\nNow that we have our newly trained thetas lets graph the results,\n\nWow, your model is working like a charm!\n\nRemember your thetas are your model! You went through this elaborate process to find the optimal thetas that, when paired with an unseen x value, you can accurately predict a y value.\n\nGreat, through this tutorial you have learned about feature matrices and the importance of theta. Thank you for reading and please comment if you have any questions."
    },
    {
        "url": "https://medium.com/@tyreeostevenson/anime-ratings-and-gradient-descent-eaf935a87c9b?source=user_profile---------3----------------",
        "title": "Anime ratings and Gradient Descent \u2013 Tyree Stevenson \u2013",
        "text": "Gradient descent is an optimization technique used to find the minimum value of a function. So what does that mean in English? So image you are sliding down a bowl water slide,\n\nyou will continue to slide down until you reach the bottom of the slide. That is exactly what Gradient Descent is doing for a function! Essentially you are a point on the slide, the slide is a function, your sliding motion is gradient descent, and the minimum point of the function is the bottom of the slide. In conclusion, Gradient Descent allows you to find the minimum value of a function aka the bottom of the water slide.\n\nMachine Learning and Deep Learning! Machine Learning and Deep Learning is essentially one big optimization task. You give the computer some data and expect it to make the most accurate prediction, classification, categorization, etc. So how do you ensure your predictions are accurate? You do this by minimizing the error in your prediction. This is known as the loss. The loss is the difference between your prediction and the actual value. In machine learning, you will use lose functions to assess the accuracy and fine tune your model. What is the best way to minimize this function? Gradient Descent of course.\n\nIn this blog post, we will be using the number of members in an Anime\u2019s fan club to predict the rating of that Anime. The predictive model we will be using is Linear Regression.\n\nLinear Regression is a statistical model that allows you to predict continuous values, you can do this by using some independent variable x to predict some dependent variable y. A real-world example is predicting ice cream sales based on the temperature outside.\n\nYour x value is the temperature and your y value is ice cream sales. As the temperature increases, the ice cream sales increases, thus your x and y are positively correlated.\n\nThe magic line of predictions, formerly known as the Line of Best Fit, is used to predict values in your dataset. The Line of Best Fit is the bread and butter of Linear Regression, you can use this line to predict ice cream sales based on the temperature. We can calculate Linear Regression with this formula\n\nh(x) is your prediction and x is your input. Using these points(x, h(x)) we can generate the Line of Best Fit. Why is this line not going through every point? That is because we can never have 100% accuracy in our predictions so we strive to get close enough. The space between our points on and Line of Best Fit is known as the error.\n\nOnce again, error, or loss, is the difference between our prediction value and the actual value. We can calculate this using our cost function.\n\nThe cost function tells you the cost of every prediction you make and the cost of a prediction is the loss. So how will you ever minimize this function? With Gradient Descent of course! So what exactly are we minimizing? Theta! The smaller your theta values, the lower your cost will be.\n\nEverything is moving along in one big circle!\n\nRemember our water slide example, for every iteration or amount we move down our water slide, theta is updated This goes on until we reach the bottom.\n\nThe squiggly circle in your formula is known as alpha, alpha is the learning rate of your formula. You can think of the learning rate as the speed at which you are going down the water slide. If you are going to fast, then you will fly right over the bottom of the water slide. If you are going to slow, you may never reach the bottom of the water slide. Same goes for a function. If the learning rate is too high, you\u2019ll overshoot the minimum or if the learning rate is too low, then you will never reach the minimum.\n\nYes! Now that you are a black belt in the art of Gradient Descent you can implement it in code. The problem we will be solving is determining the rating of an Anime based on the size of its fan club.\n\nLet\u2019s use the Anime Recommendation Database. Within this directory, you are going to use the anime.csv file. You can get this dataset by clicking here. If you have the Kaggle API installed on your machine you can download the dataset in your terminal with this command:\n\nNow you can set up your project! The libraries you will be using are math, matplotlib, numpy, and pandas.\n\nNext, we will set up a data frame. A data frame is an object similar to a CSV file that allows you easily access your data in a structured manner. Think of each row as being its own entity with the columns being the rows features. Pandas is an extremely popular data analysis library in Python and it lets you easily create data frames.\n\nA common coding convention for pandas is using pd. You can find the correlation between fan club size and rating by running\n\nNow lets graph your data. To avoid any issues of missing values let\u2019s check for nulls\n\nSince, there are null values in the rating column. You must clean your data. So for now let's just remove any rows without a rating.\n\nNow every element in the members array has a corresponding element in the ratings array, you can think of them as ordered pairs. Also, by making them numpy arrays they are given some extra features like being able to multiply every element in the array by a number.\n\nYou can set your initial theta values\n\nSince you are dealing with very large numbers, choosing smaller thetas will be easier to graph. Also, the error variable will be used to track the overall error of our model.\n\nYou can finally graph your points!\n\nYou can generate the line of best fit by implementing your hypothesis function.\n\nNow you can draw the line of best fit through your graph.\n\nOur final goal is to find the best fitting line. As explained earlier this can be done through Gradient Descent. Let\u2019s implement our cost function first, this function will be used to track our model\u2019s error.\n\nChoosing your alpha, aka learning rate, will be the toughest part of this code. However, since this dataset deals with extremely large numbers, it would be wise to choose an extremely low alpha.\n\nGreat! We, have now fitted our model and have an error rate of 19%.\n\nThanks to Gradient Descent you are now able to find the most optimal line of best fit, allowing your model to produce even more accurate predictions!"
    },
    {
        "url": "https://medium.com/@tyreeostevenson/teaching-a-computer-to-classify-anime-8c77bc89b881?source=user_profile---------4----------------",
        "title": "Teaching a Computer to Classify Anime \u2013 Tyree Stevenson \u2013",
        "text": "In this post, I will show you how to teach a computer the difference between Pokemon and Digimon. During, this process you will also learn how to build a world-class image classifier, that can consistently achieve accuracy scores of 90%. A special thanks go out to FastAI Course which taught me these skills.\n\nThis blog post will mainly focus on deep learning techniques, in regards to building image classifiers. However, collecting the data was a pretty big hurdle for me, so I am sharing these steps to help others.\n\nWhere can I find a dataset of pokemon images? Luckily, there\u2019s a Kaggle dataset for it named pokemon-images-dataset. If you have the Kaggle API installed on your computer you can download the dataset in your terminal with one command:\n\nLet\u2019s get our dataset of Digimon images! I came across a GitHub account which used a dataset of Digimon images to train a GAN click here for the full project and if you are only interested in the dataset click here.\n\nNext, I wrote a python script to move 30% of my training images into the test images directory\n\nNow our new directory should appear as:\n\nGreat, we have successfully organized our data!"
    },
    {
        "url": "https://medium.com/@tyreeostevenson/how-to-analyze-the-sentiment-of-tweets-c8b5aece6962?source=user_profile---------5----------------",
        "title": "How to analyze the sentiment of Tweets \u2013 Tyree Stevenson \u2013",
        "text": "Today, we will show you how to make a simple Twitter Sentiment Analyzer.\n\nLet\u2019s start by explaining sentiment analysis. Wikipedia states this as the basic definition of sentiment analysis:\n\nIn layman\u2019s terms we can think of sentiment analysis as using machines to interpret the underlying meaning of a body of text. For example, let\u2019s say you are writing a letter to your boss. If you were recently promoted and this letter was written to thank your boss, then the sentiment would be labeled as \u2018positive\u2019. However, if you feel as if you were wrongly demoted and wrote a letter to your boss detailing how you truly feel about the situation, the resulting sentiment would most likely be labeled \u2018negative\u2019.\n\nPositive sentiment is usually correlated with sentences or words that express happiness, enthusiasm, kindness, and feelings of that nature.\n\nOn the other hand, negative sentiment is usually correlated with sentences or words that express sadness, hate, violence, discrimination, etc.\n\nNeutral sentiment then implies that there is no significant emotion shown/detected within the text.\n\nGreat, now that we understand sentiment analysis, we will move on to using twitter\u2019s api to pull our tweets!\n\nGreat Question! API\u2019s are tools on the web which we can call to send and receive data. We normally send data and receive a response which is what makes these API\u2019s so useful. Usually the data received is from huge applications/libraries which we either lack the time, resources, or motivation to build for ourselves. Thus the ability to leverage someone else\u2019s application saves us valuable development time. Just imagine, how long this medium post would be if we had to build a library to pull the tweets? If you are interested in a more in depth analysis of API\u2019s check out, \u201cWhat is an API? In English, please.\u201d by Petr Gazarov\n\nTo get your API keys (consumer and access), you have to create an app in Twitter via\n\nYou\u2019ll see a page like this. We don\u2019t have a website in place, so your description and website can be whatever you want.\n\nYou\u2019ll be given your consumer keys and there is a button below to generate access tokens. These tokens are the way for you program to send requests to a server. Provided the key is received as legitimate, the request is completed and the server returns a response matching your query. Place the keys at the beginning of your code and try to print out some of your results. Don\u2019t call the API too much, you have a limit as to how many calls you can make on the twitter API without a premium account! To quote Twitter:\n\nBefore we can pull tweets we must first use our access keys. To gain access to these keys we must first use go to apps.twitter.com and choose our application.\n\nNow we see our dashboard\n\nIf this is your first time using this project, choose under Access Token choose generate API keys.\n\nLet\u2019s create a class. This class will be used to pull tweets and analyze their sentiment.\n\nA class uses some combination of functions and data to represent abstract or real objects.\n\nNext we will attempt authentication for our requests against the API.\n\nThe try keyword allows you to run code that may crash. What really makes this keyword popular is the keyword that it works with. An except will prevent an error from crashing your code and instead of the error stopping your code another action will happen. For example, if you are not authenticated to access an API, the code will throw a runtime error and the except will trigger the program to print, \u201cError: Authentication Failed\u201d.\n\nThe next function will be used to get our tweet\u2019s sentiments:\n\nPolarity represents the degree of emotion expressed in the text. Polarity is represented as a decimal number that ranges from -1 to 1. In our code, we defined text with negative sentiments to have polarity less than zero, neutral sentiments have polarity equal to zero, and positive sentiments have a polarity greater than 0.\n\nA main is not necessary to run code (this is only in Python and a few newer languages!). It is, however, absolutely necessary for readability and creating libraries. The advantage to having a main function is that you can now import this script and use the Twitter Client library without having your code call the other functionalities of the program. For example, importing the Twitter Client class will not trigger your code to print Donald Trump tweets.\n\nThank you for following along with the tutorial. I would also like to thank Geeks for Geeks who made the majority of this code available."
    }
]