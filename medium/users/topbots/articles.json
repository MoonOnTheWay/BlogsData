[
    {
        "url": "https://medium.com/topbots/avoid-these-common-mistakes-business-executives-make-with-data-608dfe00c732?source=---------0",
        "title": "Avoid These Common Mistakes Business Executives Make With Data",
        "text": "Data is a human invention. Humans define the phenomenon that they want to measure, design systems to collect data about it, clean and pre-process it before analysis, and finally choose how to interpret the results. Even with the same dataset, two people can form vastly different conclusions. This is because data alone is not \u201cground truth,\u201d defined by machine learning experts as observable, provable, and objective data that reflects reality. If data was inferred from other information, relies on subjective judgment, was not collected in a rigorous and careful manner, or is of questionable authenticity, then it is not ground truth.\n\nHow you choose to conceptualize a phenomenon, determine what to measure, and decide how to take measurements will all impact the data that you collect.\n\nGround truth is used as a benchmark to assess the performance of algorithms. If your gold standard is wrong, then your results will not only be wrong but also potentially harmful to your business.\n\nUnless you were directly involved with defining and monitoring your original data collection goals, instruments, and strategy, you are likely missing critical knowledge that may result in incorrect processing, interpretation, and use of that data.\n\nWhat people call \u201cdata\u201d can be carefully curated measurements selected purely to support an agenda, haphazard collections of random information with no correspondence to reality, or information that looks reasonable but resulted from unconsciously biased collection efforts. Here\u2019s a crash course on statistical errors that every executive should be familiar with.\n\nFailing to pin down the reason for collecting data means that you\u2019ll miss the opportunity to articulate assumptions and to determine what to collect. The result is that you\u2019ll likely collect the wrong data or incomplete data. A common trend in big data is for enterprises to gather heaps of information without any understanding of why they need it and how they want to use it.\n\nLet\u2019s say you want to know how much your customers spent on your services last quarter. Seems like an easy task, right? Unfortunately, even a simple goal like this will require defining a number of assumptions before you can get the information that you want.\n\nFirst, how are you defining \u201ccustomer\u201d? Depending on your goals, you might not want to lump everyone into one bucket. You may want to segment customers by their purchasing behavior in order to adjust your marketing efforts or product features accordingly. If that\u2019s the case, then you\u2019ll need to be sure that you\u2019re including useful information about the customer, such as demographic information or spending history.\n\nThere are also tactical considerations, such as how you define quarters. Will you use fiscal quarters or calendar quarters? Many organization\u2019s fiscal years do not correspond with calendar years. Fiscal years also differ internationally, with Australia\u2019s fiscal year starting on July 1st and India\u2019s fiscal year starting on April 1st. You will also need to develop a strategy to account for returns or exchanges. What if a customer bought your product in one quarter but returned it in another? What if they filed a quality complaint against you and received a refund? Do you net these in the last quarter or this one?\n\nAs you can see, definitions are not so simple. You will need to discuss your expectations and set appropriate parameters in order to collect the information that you actually want.\n\nOnce you\u2019ve identified the type of data that you wish to collect, you\u2019ll need to design a mechanism to capture it. Mistakes here can result in capturing incorrect or accidentally biased data. For example, if you want to test whether product A is more compelling than product B, yet you always display product A first on your website, then users may not see or purchase product B as frequently, leading you to the wrong conclusion.\n\nMeasurement errors occur when the software or hardware that you use to capture data goes awry, either failing to capture usable data or producing spurious data. For example, information about user behavior on your mobile app may be lost if the user experiences connectivity issues and the usage logs are not synchronized with your servers. Similarly, if you are using hardware sensors like a microphone, your audio recordings may capture background noise or interference from other electrical signals.\n\nAs you can see from our simple attempt to calculate customer sales earlier, many errors can occur even before you look at your data. Many enterprises own data that is decades-old, where the original team capable of explaining their data decisions is long gone. Many of their assumptions and issues are likely not documented and will be up to you to deduce, which can be a daunting task.\n\nYou and your team may make assumptions that differ from the original ones made during data collection and achieve wildly different results. Common errors include missing a particular filter that may have been used on the data, such as the removal of outliers; using different accounting standards, as in the case with financial reporting; and simply making methodological mistakes.\n\nCoverage error describes what happens with survey data when there is insufficient opportunity for all targeted respondents to participate. For example, if you are collecting data on the elderly but only offer a website survey, then you\u2019ll probably miss out on many respondents.\n\nIn the case of digital products, your marketing teams may be interested in projecting how all mobile smartphone users might behave with a prospective product. However, if you only offer an iOS app but not an Android app, the iOS user data will give you limited insight into how Android users may behave.\n\nSampling errors occur when you analyze data from a smaller sample that is not representative of your target population. This is unavoidable when data only exists for some groups within a population. The conclusions that you draw from the unrepresentative sample will probably not apply to the whole. If you only ask your friends for opinions about your products and then assume your user population will feel similarly, this is a classic sampling error.\n\nInference errors are made by statistical or machine learning models when they make incorrect predictions from the available ground truth. Two types of inference errors can occur: false negatives and false positives. False positives occur when you incorrectly predict that an item belongs in a category when it does not, such as saying that a patient has cancer when he is healthy. False negatives occur when an item is in a category, but you predict that it is not, such as when a patient with cancer is predicted to be cancer-free.\n\nAssuming you have a clean record of ground truth, calculating inference errors will help you to assess the performance of your machine learning models. However, the reality is that many real world datasets are noisy and may be mislabeled, which means that you may not have clarity on the exact inference errors that your AI system is making.\n\nReality can be elusive and you cannot always establish ground truth with ease. In many cases, such as with digital products, you can capture tons of data about what a user did on your platform but not his motivation for those actions. You may know that a user clicked on an advertisement, but you don\u2019t know how annoyed she may have been with it.\n\nIn addition to many known types of errors, there are unknown unknowns about the universe that leave a gap between your representation of reality, in the form of data, and reality itself."
    },
    {
        "url": "https://medium.com/topbots/7-steps-to-finding-the-perfect-ai-development-agency-for-your-business-d89581d724a2?source=---------1",
        "title": "7 Steps To Finding The Perfect AI Development Agency For Your Business",
        "text": "As expected, artificial intelligence (AI) took the center stage in this year\u2019s Consumer Electronics Show (CES) in Las Vegas, portending what lies ahead for businesses catering to the general public. For the first time in its history, the planet\u2019s largest technology trade show launched an AI Marketplace showcasing innovations in robotics, neural networks, and computer systems that replicate specific capabilities of human intelligence. My colleagues and I were also invited to launch our book Applied AI: A Handbook For Business Leaders onstage.\n\nMore than anything else, CES 2018 proves that applied AI has moved well beyond the domain of data scientists, machine learning (ML) experts, and corporate strategists \u2014 into the welcoming arms of enraptured consumers. From digital assistants and autonomous vehicles to emotive house bots and intelligent image processors, the future inevitably moves ever closer to an AI-driven economy.\n\nFor businesses, this means you either ride the waves of change by embracing AI or sink like a stone clinging to the old ways of doing things. The question is no longer whether you have an AI strategy. Your strongest competitors likely already have one up and running.\n\nInstead, the looming challenge is to \u2014\n\nDepending on your situation, either can get the ball rolling for your brand and help narrow the lead of early risers in your industry.\n\nChances are, your business offers something quite different from those in the store shelves of AI service providers. But in case you do (or if your war chest is full to the brim regardless), the smarter approach will be to build an in-house AI team. Certainly, that\u2019s a steeper task given the telling lack of AI specialists and the hundreds of millions tech giants are willing to throw around to hire/train/retain/poach top talent.\n\nIf you just need AI to streamline operations, leverage data, sync fragmented tools, cut costs, enhance customer experiences, or generally just deliver better and high-value services, then adopting a plug-and-play AI solution (if one exists) or partnering with an AI-focused advisor would yield significant improvements in efficiencies, profit margins, and topline revenue without incurring the astronomical costs of setting up your own AI team.\n\nArtificial intelligence (and subfields such as neural networks, deep learning, computer vision, and natural language processing) can be too complicated for many businesses. The learning curve is steep and the required resources to build your own can be very prohibitive. This opens up the market for plug-and-play variants that piggyback on existing software services or platforms that are already familiar to companies and their workforce. We\u2019ve compiled a massive list of over 130 enterprise AI vendors specializing in software for CRM, HR, Sales, Marketing, and many other common business functions. These products either work out of the box or require a few days to a few weeks of integration before they\u2019re up and running.\n\nIf you have some technical talent in your organization, you can also built on top of machine-learning-as-a-service (MLaaS) platforms from tech giants like Google Cloud, Amazon, Microsoft, or IBM Watson. These platforms are tightly integrated with their enterprise cloud solutions and offer common technical capabilities such as image recognition, text processing, and speech recognition. While not end-to-end turnkey solutions like the products we linked to above, MLaaS platforms enable companies who already store data with cloud providers to take advantage of built-in machine learning tools rather than build your own from scratch. The downside is that you\u2019ll still need a technical team to customize a solution.\n\nApplicable only to a small fraction of business challenges, turnkey services and MLaaS platforms simply can\u2019t solve everything. If you\u2019ve finally caught the AI frenzy but your intentions appear to be going nowhere, then it\u2019s time to partner with an AI development service provider. The ideal AI development and consulting agency can look much closer into your operations, dissect your business challenges, and build tailored AI solutions for your organization.\n\nHere are the steps to ensure you\u2019d end up with the right AI development agency.\n\nDo your homework. Learn as much about artificial intelligence as you can. Take a crash course or read primers online. Be sure to have a top-level understanding of important concepts such as machine learning, neural networks, and natural language processing. Most importantly, learn about key industry players (i.e., AI developers and vendors), their offerings, and the current capabilities of their services.\n\nClarify what you want AI to do. Start with a specific problem you want to solve or a specific business objective you want to achieve. Do you want to streamline a set of workflows, crunch troves of fragmented data into actionable insight, dramatically improve customer experiences, increase profit margin, or ramp up sales? Do you want to solve a recurring roadblock in a specific business process? Getting into details about the challenge or objective will help you map out a viable AI adoption plan.\n\nSnoop around. Get intel on which AI solutions your competitors are adopting. Scan the market for AI vendors focusing on your pain point/business objectives. Search for and evaluate case studies that reflect your own situation.\n\nCreate a short list. Screen agencies based on track record, team credentials, product capabilities, solution fit, success rate, and customer support. Agencies that have been in the AI field longer or have done proven work with large enterprises in your domain are more preferable than companies with sub-par portfolios or have engaged in a different line of business.\n\nReach out and make initial contact. Glean additional information by directly engaging the vendors/AI development agencies in your short list. Ask each vendor the following questions:\n\nKeep a scorecard. Compare the strengths and weaknesses of the vendors you have engaged. Draw up a comprehensive set of factors that will help you approximate the relative value of the AI development agencies in your list. In addition to price, the criteria should include product features, track record, success rate, industry rep, and customer support.\n\nMake a decision. Based on your scorecard, choose the AI development agency your business will partner with. Take the leap.\n\nAs Forrester recently announced, the honeymoon period between AI and enterprises is finally over. The time has come to do the hard work of actually building the AI infrastructure that will keep your business relevant for the foreseeable future.\n\nOf course, investing in AI is serious business. Tech giants spend tens of billions on AI research, deployment and acquisitions. Meanwhile, a Vanson Bourne study released in the last quarter of 2017 found that 80% of enterprises already have some form of AI in production while 30% plan to expand their investments over the next three years. Around 62% expect to hire a Chief AI Officer soon.\n\nFortunately, you don\u2019t need to raise billions nor recruit new talent to get started. Partnering with the right AI development agency may just be the competitive advantage you need."
    },
    {
        "url": "https://medium.com/topbots/beyond-backpropagation-can-we-go-deeper-than-deep-learning-8f4fc34a0ba8?source=---------2",
        "title": "Beyond Backpropagation: Can We Go Deeper Than Deep Learning?",
        "text": "Deep learning almost single-handedly made \u201cAI\u201d a buzzword in research labs, corporate board meetings, the stock market, and the homes of digital consumers. A branch of machine learning that uses artificial neural network structures to enable computers to learn, deep learning is responsible for recent performance leaps in technologies such as natural language processing, audio recognition, computer vision, autonomous vehicles, drug design, and advanced recommendation engines. The approach is also behind much of the tech world\u2019s latest batch of milestone products: from Amazon Echo to Google\u2019s AlphaGo to enterprise-ready machine learning APIs.\n\nBut, as the general public latches onto AI hype, the very pioneers behind deep learning are questioning whether it is the right approach to achieve true machine intelligence. Geoffrey Hinton, widely revered as the \u201cgodfather of deep learning\u201d, now believes the current path dominating AI research leads to a dead end even though he\u2019s championed this method for over three decades.\n\nA direct descendant of the mathematician who invented Boolean algebra in 1854, Hinton can claim a similarly formidable imprint in the world of information technology. A cognitive psychologist with a doctorate in artificial intelligence, Hinton spearheaded hundreds of pioneering work, including hallmark studies in:\n\nBackpropagation enables computers to learn by iteratively adjusting the weights of a neural network in order to minimize the error between the model\u2019s prediction and a ground truth comparison. Prior to 2010, computing power was still too limited to sustain practical applications, but several years of technological advances eventually positioned Hinton\u2019s backpropagation method at the center of almost every AI-driven marvel. Google strategically acquired Geoffrey Hinton\u2019s company DNNresearch Inc in 2013 and hired him as a lead scientist for Google Brain.\n\nDeep learning algorithms produce the most reliable results and economic value when used for \u201csupervised\u201d learning. In supervised learning, algorithms are given structured training data that maps inputs (such as an image) to a label (such as \u201ccat\u201d). Unfortunately, the vast majority of information in the world is not structured so neatly. Unlike neural networks, human infants learn concepts quickly in unstructured, unsupervised learning environments.\n\nIn an interview with Axios, Hinton suggested that we need to move beyond backpropagation if we want to teach computers to achieve unsupervised self-learning like that of human infants. \u201cI don\u2019t think it\u2019s how the brain works. We clearly don\u2019t need all the labeled data,\u201d he declared, \u201cMy view is throw it all away and start again.\u201d Weighing in on the future and his own contributions in the field, he humbly concluded, \u201cThe future depends on some graduate student who is deeply suspicious of everything I have said.\u201d\n\nDeveloper, entrepreneur and author Siraj Raval made a video agreeing with Hinton\u2019s assessment of backpropagation and covering alternative research directions that seem promising. \u201cIf we really want to get to general artificial intelligence, then we have to do something more complicated or something else entirely,\u201d he explained, \u201cIt\u2019s not just about stacking layers and then backpropagating some error gradient recursively. That\u2019s not going to get us to [artificial] consciousness. That\u2019s not going to get us to systems that learn a huge variety of tasks.\u201d\n\nNew York-based writer and programmer James Somers concurs. Writing for the MIT Technology Review, he argued that \u201conce you understand the story of backprop, you\u2019ll start to understand the current moment in AI, and in particular the fact that maybe we\u2019re not actually at the beginning of a revolution. Maybe we\u2019re at the end of one.\u201d\n\nMany researchers believe that to simulate human intelligence, we need to strengthen our capabilities in unsupervised learning and find innovative ways to train models without strict dependence on structured training data. Promising approaches include:\n\nAutoencoders are neural networks that work in an unsupervised manner by setting outputs as identical to inputs. This enables the network to learn interesting structures and representations of the data that can be used for denoising, compression, and generation.\n\nGenerative adversarial networks (GAN) are algorithms that use two neural networks, a discriminator and a generator, to compete with each other to produce high quality generated results. The generator is responsible for creating content (like images) from scratch while the discriminator is trained on real world data and must distinguish between true images and those created by the generator. NVIDIA recently used GANs successfully to generate highly realistic images of human faces.\n\nDifferentiable neural computers (DNC) are neural networks that learn to form and use complex memory structures like lists, trees, and graphs from scratch. The core of the neural network is called a controller, which operates like a processor in a regular computer by taking in input, reading and writing from memory, and producing output. As a DNC is trained, the controller learns to produce increasingly better answers through the use of the right memory structures.\n\nGenetic Programming & Evolutionary Strategies (ES) are a decades-old optimization technique used to simulate natural evolution without the use of backpropagation. After all, evolution is the only known strategy that is proven to generate at least human-level intelligence. In evolutionary strategies, a set of candidate solutions is provided for a given problem and evaluated according to a \u201cfitness\u201d function. The best performing solutions \u201csurvive\u201d and the poorly performing ones \u201cdie\u201d while increasingly better solutions are created in the next generation. Google Brain researcher David Ha explains that this approach can be an alternative approach for the \u201cmany problems where the backpropagation algorithm cannot be used\u201d in his excellent visual introduction to evolutionary strategies.\n\nNeuroevolution is a subfield of deep learning that uses evolutionary strategies to evolve the weights of a neural network rather than using an error-based cost function and the backpropagation method. One of the best known algorithms is NEAT (NeuroEvolution of Augmenting Topologies), developed by Kenneth Stanley in 2002. Since then, many new neuroevolution algorithms have been developed.\n\nProbabilistic programming enables us to create learning systems that make decisions in the face of uncertainty by making inferences from prior knowledge. According to Avi Pfeffer in his book Practical Probabilistic Programming, you first create a model that captures knowledge of your domain in quantitative, probabilistic terms, then apply this model to specific evidence to generate an answer to a query. This process is called inference.\n\nAI-specific hardware may also play a key role in bridging the gap towards general artificial intelligence. Current transistor-based hardware supports serial processing. In contrast, the neurons in animal brains work in parallel to each other. Examples of hardware explicitly designed for artificial intelligence include Google\u2019s Tensor Processing Unit (TPU), IBM\u2019s neuromorphic chips, and optical computing developed to speed up matrix operations.\n\nWill any of these approaches prove to be the real breakthrough to human-level machine intelligence? Nobody really knows, but Hinton\u2019s provocative statements have definitely accelerated the race to push the cutting edge of AI research beyond the limits of deep learning."
    },
    {
        "url": "https://medium.com/topbots/selling-ai-heres-how-to-stand-out-from-the-noisy-marketing-hype-791c0f185d8?source=---------3",
        "title": "Selling AI? Here\u2019s How to Stand Out from the Noisy Marketing Hype",
        "text": "Hoping to appear relevant, businesses love to invoke AI in their marketing every chance they get \u2014 but not everyone is impressed. When the hype gets out of control, facts get lost in the haze and the real promise of artificial intelligence regresses into a sci-fi version of snake oil.\n\nAs a result, companies \u2014 including those most likely to benefit from AI-driven solutions \u2014 are starting to doubt the claims of vendors plugging their wares. On the opposite side, marketers tasked with communicating the business utility of their AI products are left with a disinterested or distrusting audience.\n\nSo, how do you sell AI without causing your audience to roll their eyes?\n\nConventional tactics such as customer testimonials have their uses \u2014 but only up to a certain point. Textio, a predictive writing platform used in talent recruitment, relies on videos, case studies, and co-branded events with customers like Johnson & Johnson, Vodafone, and Nvidia to demonstrate their business value.\n\nHowever, customer testimonials aren\u2019t universally applicable. If a client in a competitive industry sees your solution as a true differentiator, they obviously won\u2019t rush to tell everyone about it and might even ask for non-disclosure agreements (NDAs). An earlier-stage startup may very well have exceptional technology, but lack the big name brands to publicly tout its virtues.\n\nTo find other effective approaches, we talked with leading marketers and brand communicators who\u2019ve employed unique strategies to stand out in an already saturated marketplace.\n\nYou\u2019ve probably heard companies brag about using \u201cmachine learning\u201d and its trendy sub-field \u201cdeep learning\u201d, but how many companies can you name that use \u201cevolutionary strategies\u201d? Artificial intelligence is comprised of many technical approaches, only a few of which are currently overhyped in the media.\n\nWith more than 40 patents in artificial intelligence, Sentient Technologies leverages this fact to educate their customers about their unique approach to evolutionary computation for digital marketing, e-commerce, and finance. In evolutionary strategies (ES), a set of candidate solutions is provided for a given problem and evaluated according to a \u201cfitness\u201d function. The best performing solutions \u201csurvive\u201d and the poorly performing ones \u201cdie\u201d while increasingly better solutions are created in the next generation.\n\n\u201cUnlike with deep learning, people intuitively understand how evolution works,\u201d says Jeremy Miller, Sentient\u2019s Director of Marketing. Another key benefit that Sentient touts is that, unlike supervised machine learning, which requires large volumes of clearly labeled data, evolutionary strategies don\u2019t have the same dependence. The company uses ES in conversion optimization software that automates website testing with the goal of lifting conversion rates and driving revenue. Rather than use historical data, the product evolves designs based on live, real-time data.\n\nEven though ES algorithms are comparatively unique and don\u2019t have the same high data requirements, Miller cautions against leading with technical differentiation and prefers to highlight utilitarian benefits, such as the fact that their product can be installed on a website with just a single line of javascript code. \u201cCustomers care about outcomes. We don\u2019t lead with AI anymore. We lead with what the customer wants,\u201d he emphasizes.\n\nPlenty of talented PhDs and postdocs now work at promising AI startups, but how many of them can say they\u2019ve deployed their algorithms on Mars? Deep space is the most unforgiving environment, with zero fault tolerance, limited space for hardware, and a need for autonomous operations.\n\nWhen Edward Yang, Managing Director at Firecracker PR, started working with Caltech startup Beyond Limits, they didn\u2019t have any corporate use cases. Reporters were confused by early versions of their website and could not figure out what the company did. \u201cVery technical founders are not always the best at communication,\u201d says Yang, who opted for a more compelling storytelling approach.\n\n\u201cBeyond Limits is the only AI solution born in the labs of NASA and battle-tested in deep space missions, including on the Mars Rover,\u201d he shares. \u201cSo we opted for the story angle of bringing the most powerful AI from Mars to Earth.\u201d\n\nNow Beyond Limit\u2019s technology is used in numerous sectors, such as energy, finance, autonomous vehicles, healthcare, and internet of things (IoT). While the NASA background certainly gives the company technical street cred, telling the story is not always easy.\n\n\u201cOur biggest challenge is time,\u201d Yang explains. \u201cThe best thought leadership comes from the scientists and the technical founders, but in a startup everyone is running in different directions.\u201d Many employees also don\u2019t like to write or aren\u2019t good at writing, so Yang and his team prioritize spending time talking to sources and flushing out their technical expertise.\n\nYou might be a savvy techie who genuinely understands and uses words like \u201cneural networks\u201d or \u201cdeep learning\u201d in everyday conversation, but some people \u2014 including Protagonist\u2019s early prospective customers \u2014 might find that uncool.\n\n\u201cProtagonist started by trying to hammer home their unique technology by pioneering the buzzword \u2018narrative analytics\u2019, but this just wasn\u2019t resonating with customers,\u201d recalled VP of Marketing Damon Waldron.\n\nA veteran B2B marketing expert, Waldron currently heads the team at Protagonist, a company which leverages AI to capture the stories and beliefs which drive people\u2019s behavior. Fortune 500 companies like Wells Fargo, Starbucks, and Microsoft use the product to assess how consumers really feel about their brands and develop a communication strategy. Asked to distill the biggest challenge in communicating AI innovation, Waldron responded: \u201cThe tech news cycles are overly saturated with frilly buzzwords. It\u2019s been a challenge to truly communicate the significance of a product via pen and paper when competing against so much noise.\u201d\n\nTalking about \u201cnarrative analytics\u201d did not connect with buyers, so Waldron opted instead to position Protagonist against well-known, commonly used marketing tactics that his customers did understand. These included traditional market surveys, which are limited in scope and introduce bias, and social media monitoring tools, which are often quite shallow and don\u2019t extend beyond positive or negative sentiment analysis.\n\nHe also uses Protagonist to market Protagonist. \u201cWe recently did an analysis of Harley Davidson as an exercise in how to reinvigorate a stale brand. We did that in three days. What we want is for the CMO of Brooks Brothers or another brand in the same situation to think about how they could use Protagonist.\u201d Similarly, Waldron and his team can quickly put together educational content on any trending topic, such as whether traditional investment managers are under threat from robo-advisors. \u201cWe can easily host a webinar on the topic or throw together an infographic on how millennials are investing and insert them into our sales and marketing emails.\u201d\n\nPeople rarely understand how enterprise software works just by staring at screenshots or reading spec sheets. Within the right context, personifying your AI-driven software can demonstrate the interactivity of your platform in a clear but friendly way. If this tactic results in clients finally seeing the benefits and understanding how your product works, the extra branding effort may be well worth it.\n\nTradeshift did exactly that. The company\u2019s customers are executives who work in the relatively humdrum worlds of supply chain management, procurement, and accounts payable. \u201cThere is a disconnect between supply chain and procurement professionals and IT,\u201d explains CMO David Ahrens. \u201cSince executives don\u2019t work too much on the tech side, this starting gap makes it even harder for them to understand AI. Yet many of them realize that AI is a technology they must get their arms around quickly.\u201d\n\nTo overcome this \u201cfamiliarity gap\u201d, as Ahrens put it, Tradeshift personified their AI platform by putting a human face \u2014 Ada \u2014 to their solutions. Giving their technology a conversational persona enabled customers to interact with Tradeshift\u2019s platform in a more natural, intuitive way and quickly grasp the business use cases. When customers go to make bulk business purchases, they can easily ask Ada for help finding the best prices in a category or ensuring they stay under their procurement limits. Ada also helps them analyze their spending behavior from past periods in a multi-dimensional way.\n\nNamed after Ada Lovelace, Ada was born from an internal company hackathon. While Tradeshift\u2019s capabilities extend far beyond what Ada can do, Ahrens cautions that you must take your customers on a buyer\u2019s journey step by step. \u201cIf you say you have AI in everything, customers won\u2019t know what to do with that information. Instead, they\u2019ll get confused and won\u2019t return. We decided to start from a specific use case and build from there.\u201d\n\nA wide chasm exists between those who build AI technologies and those who buy it. Communicating with technical and marketing jargon will not only confuse customers, but also scare them away. In each of these unique marketing cases, success was achieved after tailoring conversations to the level of the customer by referencing concepts they already understand, like evolution or social monitoring or space missions, and by making AI approachable with friendly interfaces that clearly demonstrate important use cases.\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/future-factories-how-ai-enables-smart-manufacturing-c1405f4ec0e6?source=---------4",
        "title": "Future Factories: How AI enables smart manufacturing",
        "text": "Today\u2019s consumers are pickier than ever. They want customized, personalized, and unique products over standardized ones and prefer local, smaller producers over large-scale global manufacturers.\n\nFactories, power plants, and manufacturing centers around the world must rely on automation, machine learning, computer vision, and other fields of AI to meet these rising demands and transform the way we make, move, and market things.\n\nSince the industrial revolution, factories have been optimized to mass produce a few products rapidly and cheaply to satisfy global demand. \u201cThe largest inefficiency that most manufacturers face is inflexibility,\u201d says Jim Lawton, Chief Product & Marketing Officer of Rethink Robotics, maker of collaborative industrial robots. \u201cTraditional industrial automation requires hundreds of hours to reprogram, making it very impractical to change how the task is performed.\u201d\n\nCatering to finicky consumers is not the only challenge confronting modern factories. Costs of production in traditionally affordable countries like China and Mexico are rising. Oil and gas industries have been hit incredibly hard by historically low oil prices, driving the need for further efficiencies and cost reduction.\n\nIn virtually all factories, poor demand forecasting and capacity planning, unexpected equipment failures and downtimes, supply chain bottlenecks, and inefficient or unsafe workplace processes can lead to resource wastage, longer production periods, low yields on production inputs, and lost revenue. Manufacturers are also strapped for qualified labor, both skilled and unskilled, as older employees retire, younger generations lose interest in manufacturing jobs, and immigration policies tighten.\n\nPrabir Chetia, Head of Business Research and Advisory at global analytics firm Aranca, details the current dilemma faced by many manufacturers around the world:\n\nInnovative manufacturers already use artificial intelligence to tackle these many challenges. Here are the key ways that \u201cIndustry 4.0\u201d, the latest trends in smart factories, leverage automation, data exchange, and emerging technologies:\n\nRethink Robotics, founded by robotics pioneer Rodney Brooks, advocates the \u201ccobot\u201d model where humans and robots work side by side for maximum effectiveness. While industrial robots have long performed heavy lifting and tedious work on assembly lines, they\u2019re typically designed for a single tasks and require hours to reprogram. Baxter and Sawyer, Rethink\u2019s smart collaborative robots, are able to learn a multitude of tasks from demonstrations, just like their human counterparts can.\n\n\u201cTraining a robot is nearly as simple as training a human,\u201d claims Chief Product & Marketing Officer Jim Lawton. \u201cCompanies that don\u2019t have programming expertise on staff and can\u2019t afford to spend hundreds of thousands of dollars on a traditional industrial robot can instead leverage more affordable, flexible automation and adapt to market changes.\u201d\n\nIndustrial equipment is typically serviced on a fixed schedule, irrespective of actual operating condition, resulting in wasted labor and risk of unexpected and undiagnosed equipment failures. Once instrumented with sensors and networked with each other, devices can be monitored, analyzed, and modeled for improved performance and service. An industry leader in the space, GE enables manufacturers to create \u201cDigital Twins\u201d, or physics-based virtual models of large-scale machinery, on their industrial cloud platform, Predix.\n\n\u201cTwinning\u201d a piece of equipment allows human operators to constantly monitor performance data and generate predictive analytics. According to Marc-Thomas Schmidt, Chief Architect of Predix, nearly 650,000 twins are currently deployed and range widely in complexity. Complex twins like those of gas turbines interpret data from hundreds of sensors, understand failure conditions, track anomalies, and can be used to regulate production based on real-time demand.\n\nEven relatively simple twins yield clear business benefits. Schindler, an elevator manufacturer, makes the bulk of their revenue on servicing costs, not asset sales. Operating a crew of service engineers on a fixed schedule is an inefficient use of labor. Instrumenting elevators with simple sensors and twinning them with Digital Twins enables Schindler to send service on a need basis rather than a time basis.\n\nInstrumentation and digitization is not entirely straightforward in manufacturing, as we\u2019ve previously shown. Schmidt explains: \u201cInstrumentation is hardest when equipment is very remote. On offshore drilling platforms, for example, the biggest challenge is getting the data back to a place where it can be analyzed.\u201d\n\nFaster feedback loops enable factories to tackle unplanned downtimes, low yield (% of units that pass quality control), and low productivity (time it takes to make a product). \u201cIssues with low yield are most acute around high complexity products \u2014 like a laptop where there are a ton of various systems that need to come together perfectly for the product to work,\u201d explains Plethora founder Nick Pinkston.\n\nPinkston also points out that productivity often trades off against yield. The faster a manufacturer pushes a process, the more likely they\u2019ll hit errors and low quality output. \u201cBetter monitoring and adaptive control can allow you to increase the productivity of a single machine, and likewise better overall system monitoring and planning can allow the overall system to produce more product on the same numbers of machines.\u201d\n\nRather than rely on humans for in-process inspection and quality control, a task that\u2019s increasingly more challenging due to exploding product variety, companies like Instrumental.ai leverage cameras powered by computer vision algorithms to triage defects immediately and identify root causes of failure. Performing anomaly detection on hundreds of units in seconds, rather than hours, enables manufacturers to identify and resolve production failures before expensive delays pile up.\n\nOverestimating or underestimating consumer demand leads to lost revenues, either in the form of stagnant inventory or lost sales. Rather than running reactively, real-time demand visibility can be achieved by connecting consumer apps and IoT with industrial IoT. With the rise of smart home devices like the Amazon Echo and Google Home, consumer trends and behavioral data can inform downstream supply chain and manufacturing activities.\n\nWhile no complete end-to-end network exists in operation yet, supply chain optimization companies such as Elemica already combine businesses processes such as order management, procurement, transportation sourcing, and inventory management in a single convenient platform. Similarly, companies like Amazon, which control consumer touchpoints like the Echo, fulfillment centers, and manufacturing warehouses, are actively investing to close the feedback loop for on-demand production.\n\nIn a study involving hundreds of medium and large US-based wholesale distributors and manufacturers, ERP provider Macola discovered that 77% of manufacturers already automate their core business processes with software.\n\nAccording to Volkhard Bregulla, Vice President of Global Manufacturing Industries at Hewlett Packard Enterprise, \u201cAI-enabled predictive maintenance allows manufacturers to achieve 60 percent or more reduction in unscheduled system downtime, which dramatically reduces costs that accumulate across production downtime, part replacements, and inventory.\u201d\n\nArtificial intelligence and robotics, in tandem with complementary technologies like 3D printing and IoT, will enable the modular manufacturing required to meet rising consumer needs. Bregulla of Hewlett Packard Enterprise paints a likely scenario:\n\n\u201cWe will begin to see the rise of smaller production sites being established much closer to the consumers themselves, for example within densely populated cities. This means that manufacturers will be able to dramatically reduce the time from factory to consumer, putting newly developed products into the customer\u2019s hands within minutes or hours.\u201d\n\nOrder-driven production not only reduces manufacturing waste, but also enables dynamic supply chains. Manufacturers will leverage real-time digital platforms connecting consumer and industrial data to configure the assembly of products on an ad hoc basis. When production lines can be quickly reconfigured to cost-efficiently create new, unique products, both consumers and manufacturers benefit.\n\nPlethora\u2019s Pinkston believes this dream is not far off."
    },
    {
        "url": "https://medium.com/topbots/the-machine-intelligence-continuum-ab6aee686a2d?source=---------5",
        "title": "The Machine Intelligence Continuum \u2013 TOPBOTS \u2013",
        "text": "This is part two of our WTF IS AI?! series. Read part one on modern AI techniques if you missed it.\n\nIf you\u2019re not an AI researcher or engineer, understanding the subtle differences and applications of various machine learning approaches can be challenging. Business problems can usually be solved in multiple ways by different algorithms and the comparative merits of different methodologies might not be obvious without technical experience or practical experiments.\n\nTo help business executives disentangle the functional differences between different AI approaches, we\u2019ve segmented applications along our Machine Intelligence Continuum (MIC). The MIC represents a continuum from simple, scripted automation to superhuman intelligence and highlights the functional capabilities of different levels of machine intelligence.\n\nAlthough we described seven levels along the continuum, keep in mind that the distinctions between levels are not mutually exclusive and many overlaps exist.\n\nThe lowest level of the Machine Intelligence Continuum (MIC) are \u201cSystems That Act\u201d which we define as rule-based automatons. These are systems that are hand-engineered by experts and perform in a scripted fashion, often following if-then types of rules.\n\nExamples include the fire alarm in your house and the cruise control in your car. A fire alarm contains a sensor that detects smoke levels. When smoke levels reach a certain level, the device will play an alarm sound until manually turned off. Similarly, the cruise control in your car monitors your automobile\u2019s speed and uses a motor to vary throttle position to maintain a constant speed.\n\nYou would never set your cruise control, take your hands off the wheel, and claim you have a self-driving car. That would result in very negative outcomes for you. Yet most companies claiming to have \u201cAI\u201d are really just using Systems That Act, or rule-based mechanisms that are incapable of dynamic actions or decisions.\n\n\u201cSystems That Predict\u201d are systems that are capable of analyzing data and producing probabilistic predictions based on the data. Note that a \u201cprediction\u201d does not necessarily need to be a future event, but rather a mapping of known information to unknown information. Andrew Pole, a statistician for Target, explained to the New York Times how he was able to identify 25 products, including unscented lotion and calcium supplements, that can predict the likelihood of a shopper being pregnant and even the stage of her pregnancy. Target used this information to serve eerily well timed advertisements and coupons to trigger desired consumption behavior in pregnant shoppers.\n\nAutomated and computational statistics underlie most \u201cSystems That Predict\u201d, but predictions are only as good as the incoming data. If your data is flawed, or you choose a sample set to analyze that does not represent your target population as a whole, you will get erroneous results. The US 2016 Election polls are a painful reminder of how lack of data integrity and methodological mistakes are extremely common in statistical analyses and often lead executives to the wrong conclusions.\n\nMachine learning and deep learning drive most \u201cSystems That Learn\u201d. While many learning systems also make predictions like statistical systems do, they differ in that they require less hand-engineering and can learn to perform tasks without being explicitly programmed to do so. For many computational problems, they can function at human or better-than-human levels.\n\nLearning can be automated at different levels of abstraction and for different components of a task. Completing a task requires first acquiring data which is used to generate a prediction about the world. This prediction is combined with higher level judgement and an action to produce a result. Feedback and measurements from the outcome can be fed back to earlier decision points to improve the task performance.\n\nMany enterprise applications of statistics and machine learning focus on improving the process of turning data into predictions. In sales, for example, machine learning approaches to lead scoring can perform better than rule-based or statistical methods. Once the machine has produced a prediction of how good a lead is, the salesperson then applied human judgement to take follow up action.\n\nMore complex systems, such as self-driving cars and industrial robotics, handle the entire anatomy of a task. An autonomous vehicle must turn video and sensor feeds into accurate predictions of the surrounding world and take the correct action based on the environment. Some complex models can also perform online learning, which entails using real-time data to update machine learning models, versus offline learning, which involves training models on static, pre-existing data.\n\nWe humans like to think we\u2019re the only beings capable of creativity, but computers have been used for generative design and art for decades. Recent breakthroughs in neural network models have inspired a resurgence of computational creativity, with computers now capable of producing original writing, imagery, music, industrial designs, and even AI software!\n\nBerlin-based engineer Samim trained a neural network on 14 million lines of passages from romance novels and asked the model to generate stories about images. Flow Machines, a vision of Sony, used AI trained on Beatles songs to generate their own hit, Daddy\u2019s Car, which eerily resembles the musical style of the hit British boy band. They did the same with Bach music and were able to fool human evaluators who often couldn\u2019t differentiate between real Bach and AI-generated Bach imitations.\n\nAutodesk, the leading producer of CAD software for industrial design, released Dreamcatcher, a program that generates thousands of possible design permutations based on initial constraints set by engineers. Dreamcatcher has produced bizarre yet highly effective designs that challenge traditional manufacturing assumptions and exceed what human designers can manually ideate.\n\nAI is even outperforming some artists economically! Google\u2019s DeepDream hosted an exhibition and auction of AI-generated art that collectively sold for $97,605.\n\nDaniel Goleman, psychologist and author of the book Emotional Intelligence, claims that emotional intelligence quotient (EQ) is more important than IQ in determining our success and happiness. As human employees increasingly collaborate with AI tools at work, and digital assistants like Apple\u2019s Siri and Amazon Echo\u2019s Alexa infiltrate our personal lives, machines will also need to be emotionally intelligent to succeed in our society.\n\nSentiment analysis, also known as opinion mining or emotion AI, extracts and quantifies emotional states from our text, voice, facial expressions, and body language. Knowing a user\u2019s affective state enables computers to respond empathically and dynamically, as the best humans we know often do. The applications to digital assistants are obvious, and companies like Amazon are already prioritizing emotional recognition for the Echo.\n\nEmotional awareness can also improve interpersonal business functions such as sales, marketing, and communications. Rana el Kaliouby, founder of Affectiva, a leading emotion AI company, helps advertisers improve the effectiveness of brand content by assessing and adapting to consumer reactions. Mental and behavioral health is also an area ripe for innovation. Affectiva originated from academic research at MIT designed to help autistic patients improve recognition of social and emotional cues.\n\nA human toddler only needs to see a single tiger to develop a mental construct of the animal and recognize other tigers. If humans needed to see thousands of tigers before learning to run away, our species would have died out long ago. By contrast a deep learning algorithm needs to process thousands of tiger images in order to begin recognizing them in images and video. Even then, neural networks trained on tiger photos do not reliably recognize other abstractions and representations of them, such as cartoons or costumes.\n\nHumans have no trouble with this, because we are \u201cSystems That Master\u201d. A \u201cSystem That Masters\u201d is an intelligent agent capable of constructing abstract concepts and strategic plans from sparse data. By creating modular conceptual representations of the world around us, we are able to transfer knowledge from one domain to another, a key feature of general intelligence.\n\nAs we discussed in part one of our WTF Is AI?! series, no modern AI system is an AGI, or artificial general intelligence. While humans are \u201cSystems That Master\u201d, current AI programs are not.\n\nThis final category refers to systems that exhibit superhuman intelligence and capabilities. \u201cSystems That Evolve\u201d are entities capable of dynamically changing their own architecture and design to adapt to environmental needs. As humans, we\u2019re limited in our intelligence by our biological brains, also known as \u201cwetware\u201d. We evolve through genetic mutations across generations, rather than through re-architecting our own biological infrastructure during our lifetime. We cannot simply insert new RAM if we wish to augment our memory capacity, or buy a new processor if we wish to think faster.\n\nWhile we continuously search for other intelligent life, we are not aware of any \u201cSystems That Evolve\u201d, or superhuman intelligence. Computers are currently constrained by both hardware and software, while humans and other biological organisms are constrained by wetware. Some futurists hypothesize that we may be able to achieve superhuman intelligence by augmenting biological brains with synthesized technologies, but currently this research is more science fiction than science.\n\nOnce an upgradable intelligent agent does emerge, we will reach what many experts call the technological \u201csingularity\u201d, when machine intelligence surpasses human intelligence. Self-evolving agents will be capable of ever-faster iterations of self-improvements, leading to an intelligence explosion and the emergence of superintelligence.\n\nWill superhuman machines be good or bad for humanity? While no one can predict what superintelligence will look like, we can take measures today to increase the likelihood that intelligent systems we design are effective, ethical, and elevate human goals and values.\n\nHow we build today\u2019s \u201cSystems That Learn\u201d, \u201cSystems That Create\u201d, and \u201cSystems That Relate\u201d will affect how we build tomorrow\u2019s \u201cSystems That Master\u201d and \u201cSystems That Evolve\u201d. We go into a more detailed discussion of the Machine Intelligence Continuum and how to design beneficial AI systems in our executive introduction to artificial intelligence below:\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/wtf-is-artificial-intelligence-b5cd65aaba6a?source=---------6",
        "title": "WTF is Artificial Intelligence \u2013 TOPBOTS \u2013",
        "text": "Think about the smartest person you know. What about this person leads you to describe him or her as incredibly intelligent?\n\nIs she a quick thinker, able to internalize and reuse new concepts immediately? Is he highly creative, able to endlessly generate new ideas you\u2019d never think of? Perhaps she\u2019s highly perceptive and can pick out the tiniest details of the world around her. Or maybe he\u2019s deeply empathetic and understands what you\u2019re feeling even before you do.\n\nComputers trounce humans at large-scale computational tasks, but human intelligence spans a much wider spectrum beyond what machines are currently capable of. From math geniuses to musical prodigies to sales wizards, human exhibit logical, spatial, emotional, verbal, somatic, and many other modalities of intelligence. We leverage cognitive abilities like working memory, sustained attention, category formation, and pattern recognition to understand and succeed in the world.\n\nDue to a resurgence in popularity and hype, the term \u201cartificial intelligence\u201d has been misused to describe almost kind of computerized analysis or automation, regardless of whether the technology can be described as \u201cintelligent\u201d. If you define \u201cintelligence\u201d as \u201chuman-level intelligence\u201d, then by that definition we don\u2019t have artificial intelligence today.\n\nTo avoid confusion with the more general term AI, experts prefer to use the term Artificial General Intelligence (AGI) to refer to human-level intelligence capable of abstracting concepts from limited experience and transferring knowledge between domains. AGI is also referred to as \u201cStrong AI\u201d to differentiate against \u201cWeak AI\u201d or \u201cNarrow AI\u201d, which are systems designed for a specific task whose capabilities are not easily transferrable to others.\n\nAll of the AI systems we have today are \u201cWeak AI\u201d, including impressive achievements like Deep Blue, which beat the world champion in chess in 1997, and AlphaGo, which did the same for the game of Go in 2016. These narrowly intelligent programs defeat humans in a specific task, but unlike human world champions are not capable of also driving cars or creating art. Solving those other tasks requires other narrow programs to be built.\n\nWhile many novel techniques have emerged recently to built \u201cNarrow AI\u201d, most experts in the industry agree that we are far from achieving AGI or human-level intelligence in our machines. The path towards AGI is also unclear. Many of the approaches which work well for solving narrow problems do not generalize well to abstract reasoning, concept formulation, and strategic planning \u2014 capabilities that even human toddlers exhibit that our computers cannot.\n\nHow are we building today\u2019s \u201cNarrow AI\u201d? Most enterprise-scale technologies use a wide range of methodologies, not all of which count as \u201cAI\u201d. Differentiating between them can be tricky and often there is material overlap.\n\nWhile engineers and researchers need to master the subtle differences between various technical approaches, business and product leaders should focus on the ultimate end goal and real world results of machine learning models. You will find that often simpler approaches outperform complex ones in the wild, even if they\u2019re intellectually less \u201cadvanced\u201d.\n\nStatistics is the discipline of collecting, analyzing, describing, visualizing, and drawing inferences from data. The focus is on discovering mathematical relationships and properties within data sets and quantifying uncertainty.\n\nDescriptive statistics entails describing or visualizing a population, or group that data has been gathered from. A simple application may be analyzing the items sold by a retail store in a specific time period.\n\nInferential statistics is applied when the true population is too large or difficult to capture and analyze, and a smaller representative sample must be drawn from it for study. The answers in inferential statistics are never 100% accurate and are instead probabilistic bets, since the analysis is done on a subset of the data, not the entirety. Election polling, for example, relies on surveying a small percentage of citizens to gauge the sentiments of the entire population. As we saw during the 2016 US election cycle, conclusions drawn from samples might not be representative of the truth!\n\nData mining is the automation of exploratory statistical analysis, although some use the term more loosely to describe any kind of algorithmic data analysis and information processing. The terminology is typically applied when insights and patterns are extracted from large-scale databases.\n\nSymbolic systems are programs that use human-understandable symbols to represent problems and reasoning and were the dominant approach to AI from the 1950s to the 1980s. The most successful form of symbolic systems are expert systems which are designed to mimic the decision-making process of human experts. They are comprised of a series of production rules, similar to if-then statements, that govern how the computer makes inferences and accesses a knowledge base.\n\nRule-based expert systems are best applied to automate calculations and logical processes where rules and outcomes are relatively clear. As decision making becomes more complex or nuanced, formalizing the full range of requisite knowledge and inference schemes required to make human-level decisions becomes intractable.\n\nThe obvious drawback of expert systems is that domain expert are required to hand engineer the rules engine and knowledge base for the expert system. While symbolic systems are historically not scalable or adaptable, recent research has investigated combining them with newer methods like machine learning and deep learning to improve performance.\n\nWhat happens if you want to teach a computer to do a task, but you\u2019re not entirely sure how to do it yourself? Or the problem is so complex that it\u2019s impossible for you to encode all the rules and knowledge upfront?\n\nMachine learning is the field of computer science that enables computers to learn without being explicitly programmed and builds on top of computational statistics and data mining.\n\nSupervised learning is when the computer is presented with input and output pairs, such as an image with a label (i.e. \u201ccat\u201d) and learns general rules to map the input to the output. Supervised learning is commonly used for classification, where you divide inputs into distinct categories, and regression, where the outputs are continuous numbers. If you are trying to predict whether an image is of a cat or a dog, this is a classification problem with discrete classes. If you are trying to predict the numeric price of a stock or other asset, this is a continuous output and can be framed as a regression problem.\n\nUnsupervised learning occurs when computers are given unstructured rather than labeled data, i.e. no input-output pairs, and asked to discover inherent structure and patterns that lie within the data. One common application of unsupervised learning is clustering, where input data is divided into different groups based on a measure of \u201csimilarity\u201d. For example, you may want to cluster your LinkedIn or Facebook friends into social groups based on how interconnected they are with each other. Unlike with supervised learning, the groups are not known in advance, and different measures of similarity will produce different results.\n\nSemi-supervised learning lies between supervised and unsupervised learning, where the input-output pairs are incomplete. Many real-world data sets are missing labels or have noisy, incorrect labels. Active learning, a special case of semi-supervised learning, occurs when an algorithm actively queries a user to discover the right output or label for a new input. Active learning is used to optimize recommender systems like the ones used to recommend new movies on Netflix or new products on Amazon.\n\nReinforcement learning is applied when computer programs are instructed to achieve a goal in a dynamic environment. The program learns by repeatedly taking actions, measuring the feedback from those actions, and improving its behavioral policy iteratively. Reinforcement learning is applied successfully in game-playing, robotic control, and other well-defined and contained problems, but is less effective with complex, ambiguous problems where rewards and environments are not well understood and quantified.\n\nEnsemble methods combine different machine learning models to produce superior results to any single model. Most successful applications of machine learning to enterprise problems utilize ensemble approaches. There are four broad categories of ensembling: bagging, boosting, stacking, and bucketing. Bagging entails training the same algorithm on different subsets of the data and includes popular algorithms like random forest. Boosting involves training a sequence of models, where each model prioritizes learning from the examples that the previous model failed on. In stacking, you directly combine the output of many models. In bucketing, you train multiple models for a given problem and dynamically choose the best one for each specific input.\n\nDeep learning is a subfield of machine learning that builds algorithms using multi-layered artificial neural networks, which are mathematical structures loosely inspired by how biological neurons fire. Neural networks were invented in the 1950s, but recent advances in big data and computational power have resulted in human-level performance by deep learning algorithms in tasks such as speech recognition and image classification. Deep learning in combination with reinforcement learning enabled Google DeepMind\u2019s AlphaGo to defeat human world champions of Go in 2016, a challenge that was considered computationally impossible by many experts.\n\nDue to the recent hype, much media attention has been focused on deep learning, but only a handful of sophisticated technology companies have successfully implemented deep learning for enterprise-scale products. Google replaced previous statistical methods for machine translation with neural networks to achieve superior performance. Microsoft announced in 2017 that they achieved human parity in conversational speech recognition. Promising startups like Clarifai employ deep learning to achieve state-of-the-art results in recognizing objects in images and videofor Fortune 500 brands.\n\nWhile deep learning models outperform older machine learning approaches to many problems, they are more difficult to develop and require specialized expertise. Operationalizing and productizing models for enterprise-scale usage also requires different but equally difficult to acquire technical expertise. In practice, ensemble approaches often outperform deep learning approaches in both performance and transparency. Many enterprises also look to machine-learning-as-a-service (MLaaS) solutions from Google, Amazon, IBM, Microsoft and a number of leading AI startups rather than build custom deep learning solutions.\n\nAside from complexity of production deployment and a competitive labor market, deep learning also suffers from a few notable drawbacks. Successful models typically require a large volume of reliable, clean labeled data, which enterprises often lack. They also require significant and specialize computing power in the form of graphical processing units (GPUs) or tensor processing units (TPUs).\n\nCritics of deep learning point out that human toddlers only need to see a few examples of an object to form a mental concept, while deep learning algorithms need to thousands of examples to achieve reasonable accuracy and even then still make laughable errors. Deep learning algorithms do not form abstractions or perform reasoning and planning the way we humans do.\n\nProbabilistic programming creates systems capable of making decisions in the face of uncertainty. While the research and applications are early, many experts see probabilistic programming as an alternative approach for areas where deep learning performs poorly, such as with concept formulation on sparse or medium-sized data. Probabilistic programs have been used successfully in applications such as medical imaging, machine perception, financial predictions, and econometric and atmospheric forecasting.\n\nYou can check out the MIT Probabilistic Computing Project for recommended reading and tutorials.\n\nMany other approaches to AI exist which are outside of the scope of this article, but which can be used alone or in combination with machine learning and deep learning to improve performance. Evolutionary and genetic algorithms are used in practice for generative design and in combination with neural networks to improve learning. Other approaches like Whole Brain Uploading (WBE), also known as \u201cmind uploading\u201d, seek to replicate human-level intelligence in machines by fully digitizing human brains. Yet other approaches seek to innovate at the hardware level by leveraging optical computing, quantum computing, or human-machine interfaces to accelerate or augment current methods.\n\nThis is part one of our WTF IS AI?! series. Part two is the Machine Intelligence Continuum."
    },
    {
        "url": "https://medium.com/topbots/a-i-entrepreneurs-say-faux-a-i-hype-hurts-sales-marketing-efforts-155393e53084?source=---------7",
        "title": "A.I. Entrepreneurs Say \u201cFaux A.I. Hype\u201d Hurts Sales & Marketing Efforts",
        "text": "The public is riveted by artificial intelligence, but most people don\u2019t have a clue what \u201cAI\u201d actually is. Media hype and populist stories confuse rather than clarify. How does popular perception impact the vast array of Silicon Valley companies building actual AI products and services? We spoke to leading entrepreneurs and executives to learn their unique challenges with selling and marketing AI.\n\nNot surprisingly, the biggest challenge AI entrepreneurs collectively identified was lack of buyer understanding regarding what AI entails and requires. \u201cWe\u2019re early in a convoluted market. AI means a lot of things,\u201d says Christian Monberg, co-founder of Boomtrain. In eagerness to capitalize on this growing and exciting trend, many label their wares as \u201cAI-powered\u201d when in fact no machine intelligence exists. This further confuses buyers outside the tech industry who are already grappling with constantly changing technical terminology.\n\nJosh Ziegler of Zumata points out media hype has led to inflated expectations about the impact of AI. \u201cAI and chatbots have been touted as a silver-bullet with infinite magical powers,\u201d he explains, \u201cThe dawn of AI taking over the world and being a plug and play technology has fostered a misconception of what is truly possible today.\u201d Work Fusion\u2019s Alex Lyashok calls this \u201cfaux AI hype.\u201d\n\nThe industry does benefit from the increased attention, but Ziegler believes persistent false advertising can create more obstacles than opportunities. He observes the market is full of \u201ctechnology for technology\u2019s sake,\u201d where \u201cbots often fail to solve a real need or an actual pain point.\u201d Even when bots are being used to solve actual problems, inexperienced buyers often don\u2019t understand the requisite level of commitment required to see meaningful results.\n\n\u201cMost of our clients wish to dip their toe in the water with chatbots. They tend to have grand ideas of what is possible, but wish to start with a tiny scope prototype as they control a small budget absent approvals,\u201d Ziegler elaborates. \u201cThe goal is to then use the results to request a larger budget. While this seems rational on the surface, it is like buying a skateboard to see if a Ferrari would be accepted by your users.\u201d\n\nHow is the AI industry combating this giant gap in understanding? Some entrepreneurs observe that miseducation is a symptom of how young the wave of AI feels and suggest that glamorization will fade with more time and case studies. CEO and founder of Clarifai Matt Zeiler hopes that \u201cas leaders in AI emerge, people will turn to them to actively educate themselves about AI, understand what\u2019s possible today, and inspire what will be possible tomorrow.\u201d Maura Woodman from Affinio suggests guiding customers to first identify and diagnose problems. \u201cThe most helpful thing I can think of is having more people taking a data-backed approach and being willing to talk about how [AI could impact] their bottom line.\u201d\n\nJohn Hathorn of Automated Insights also believes that adding context can help set the right expectations. He says, \u201cA lot of the pain points we face within the natural language generation (NLG) industry and within artificial intelligence as a whole could be alleviated if we take a step back and really highlight the achievements and strides that have been made within the space.\u201d\n\nTurns out miseducation is a challenge not only with customers but for company employees as well. For teams to succeed, a minimum level of literacy between tech-savvy engineers and non-coding sales and marketing professionals becomes essential.\n\n\u201cBecause we solve a particularly mission critical problem, we need salespeople who are particularly good at understanding what kinds of use cases are in our sweet spot and what use cases are farther down-roadmap\u201d says Peter Brodsky, CEO and founder of HyperScience \u2014 a company that automates data entry for back office operations. \u201cThat\u2019s a particularly rare kind of salesperson and so it\u2019s a real challenge we face on the hiring side.\u201d"
    },
    {
        "url": "https://medium.com/topbots/u-s-falls-behind-china-canada-in-advancing-healthcare-with-a-i-4454f8528cf1?source=---------8",
        "title": "U.S. Falls Behind China & Canada In Advancing Healthcare With A.I.",
        "text": "The United States leads the world in artificial intelligence, but lags behind other countries in applying technical innovations to the field of healthcare. Globally, machine learning is used to increase efficiency, lower error rates, and decrease medical costs, but the fragmented marketplace and lack of universal healthcare in America disincentivize adoption of new technology as buyers often prioritize economics over patient care.\n\nSally Daub, CEO of Enlitic, a frontrunner in providing AI-based healthcare solutions, illuminates why healthcare technology adoption is stunted. \u201cHere the conversation revolves around \u2018Who is going to pay for this? What are the economic incentives to use this technology?\u2019 rather than \u2018Will this technology result in better outcomes for patients?\u2019,\u201d she explains. Payoff structures may even discourage insurers from footing the bill for early intervention treatments. If insurers don\u2019t approve a new technology, they don\u2019t issue insurance procedure codes necessary for healthcare providers to be reimbursed for services, which means providers won\u2019t adopt the solution either.\n\nPrivate healthcare systems like America\u2019s allow costs to be funneled to consumers. Care providers, hospitals, insurance companies, and other players in the market often compete with each other to offload responsibility for medical bills. Public health systems like Canada\u2019s single-payer model operate under a unified government budget, incentivizing system-wide adoption of technologies that cut costs. Canadian citizens hold public healthcare entities and decision-makers accountable for high quality care, and the media fallout from failing to deliver to those standards is swift and severe.\n\nDue to intermarket competition, healthcare entities in the U.S. are often unwilling to share medical data, but Daub believes their excuses about privacy and security are unreasonable. \u201cKeeping healthcare records secure is obviously very important,\u201d she clarifies, \u201cbut in AI the true benefit will come from sharing data while respecting patient privacy. If everybody sits in their silos, we\u2019re not going to progress.\u201d Canada enforces that all medical records be kept in a central depository owned by the government and citizens. Hospitals within a province are also linked, so a technology tested at a single facility can be scaled quickly over the entire network. Ontario alone includes 20 million patients, more than the number covered by any single provider in America.\n\nDaub recently joined Enlitic, which uses deep learning to analyze millions of clinical cases containing patient history, symptoms, lab tests, medical images, and other important data points required to arrive at optimal treatment decisions. Founded by former Kaggle President Jeremy Howard, the company\u2019s powerful technology allows doctors to derive actionable insights from the collective intelligence of the medical community. \u201cThe key insight that led me to creating Enlitic is that deep learning can be used to integrate nearly all types of medical data to diagnose nearly all kinds of disease,\u201d Howard shares.\n\nTwo areas in healthcare stand to benefit the most from AI solutions: triage and imaging. Diagnosing medical images requires years of practitioner training, leading to a shortage of skilled radiologists, while volume of patient data has surged, resulting in high error rates. Leading healthcare investor Robert Mittendorff, Partner at Norwest Ventures, sees healthcare imaging as \u201cthe most productive area for AI to be placed.\u201d He explains that radiologists are trained to work with \u201ca very standardized data set and curated output,\u201d both of which are critical to training AI to replicate human diagnoses. As measured by internal benchmarking tests, the accuracy of Enlitic\u2019s software is already on par with, and in some cases better than, trained clinician decisions. In a clinical setting, Enlitic enhances provider productivity and provides secondary reviews to catch costly mistakes.\n\nWhile optimistic, Mittendorff sees many AI startups in healthcare make the mistake of thinking too small. \u201cIt costs the same for radiologists to adopt an imaging technology that detects appendicitis or one that detects all anomalies across the region. Doctors aren\u2019t going to adopt single point solutions, but rather systems they believe deliver considerable value and improvement across multiple needs.\u201d Howard agrees, stating that \u201ca diagnostic aid is far more useful to doctors if it covers nearly all diseases \u2014 since otherwise the doctor has to double check every piece of data themselves. This is particularly important in developing nations such as China, where there is a massive shortage of doctors.\u201d\n\nThe Chinese government has a mandate to provide high-quality care, but the country\u2019s enormous population simply cannot be served with traditional means. President Xi Jinping repeatedly extolls artificial intelligence as a top national priority, and China\u2019s explosive progress leads experts to believe they are overtaking the U.S. as the world\u2019s AI leader. Daub points out another advantage: \u201cChina doesn\u2019t have a mature healthcare system like the U.S. does. This allows them to leapfrog to new technology without the burden of legacy infrastructure.\u201d Business opportunities for healthcare technology providers expanding into the Chinese market are plentiful. \u201cThe government is pouring billions and billions of dollars into healthcare. In China, it\u2019s almost a gold rush to look for healthcare partners,\u201d says Daub.\n\nChina\u2019s advances follow in the footsteps of leading neighbors, Singapore and Japan. With a strong historical commitment to being a \u201csmart city,\u201d Singapore has public and political will to be an early adopter of new technologies as well as a central health data repository which is a prerequisite for developing functional AI models. Similarly, Japan\u2019s culture of leading technical innovation, along with an aging population where radiologists are quickly dying off, force healthcare providers to rapidly adopt AI and automation to keep servicing patients. \u201cThe Japanese were banking with their phones over 14 years ago, long before anyone else,\u201d Daub reminds us. \u201cThey have both the infrastructure and the attitude to be a frontrunner.\u201d\n\nEven Brazil could leapfrog the U.S. in healthcare innovation. The country\u2019s massive population experiences the same care delivery challenges as China and the government has made substantial investments to improve their public health system. The growing middle class views healthcare as extremely important and has the political clout to influence policymakers.\n\nDaub points to three system-wide improvements required for the U.S. to successfully adopt artificial intelligence in healthcare: a central repository for sharing health data, regulatory understanding of emerging technologies like AI, and policy changes that incentivize early detection and preventative care. She\u2019s optimistic about solutions like MedChart and Patient Bank which enable citizens to take control of their own medical records, and emphasizes that patient advocacy is critical to inspiring change in American healthcare.\n\n\u201cIf we can educate patients and give them more control, they will drive technology adoption,\u201d she encourages. \u201cThe true owner of patient records is the patient.\u201d"
    },
    {
        "url": "https://medium.com/topbots/can-a-i-defend-our-financial-institutions-against-hackers-68dda0a0764e?source=---------9",
        "title": "Can A.I. Defend Our Financial Institutions Against Hackers?",
        "text": "Could artificial intelligence defend us against thieves and barbarians at our digital gates? Or will machine learning be one more addition to cyber criminals\u2019 ever-growing arsenals?\n\nThe recent WannaCry ransomware attacks, which infected over 230,000 computers in 150 countries, revealed how most institutions fail at even basic defenses. Microsoft released a critical security patch two months earlier that would have protected systems, but most organizations failed to apply it, even the ones responsible for critical healthcare, telecommunications, and transportation information and infrastructure.\n\nIn financial services, security is king, but cybersecurity threats are relentless. Security analysts are overwhelmed with both real and false signals and security teams struggle to hire enough skilled staff. With the advent of new artificial intelligence and machine learning technologies, bank executives and security leaders hope to augment human capabilities by automatically detecting, triaging, and surfacing threats. At the same time, hackers are engaged in an arms race to use the same intelligent tools to probe and create exploits for software.\n\nNearly all financial institutions use firewalls, secure authentication methods, intrusion detection tools, data recovery systems, and a wide variety of anti-virus and anti-spam methods. Many are exploring new approaches such as behavioral biometrics, which model how humans interact with computing systems to identify unauthorized access. Data from various systems are ingested and analyzed by SEIMs (system information and event management), which provide real-time analysis and alerts on security threats.\n\nDespite all these technologies, security is still a huge challenge. \u201cHackers need only one entry point while we must defend them all,\u201d points out Paul Innella, CEO of TDI, a global cybersecurity firm. \u201cOur adversaries have an entire dark web where there is not only collaboration, free access to attack tools and instruction, but even a royalty-sharing scheme to carry out attacks using the tool-producers\u2019 tools. Collaboration on our end is disparate and inconsistent.\u201d Mike Simon, CTO of Critical Informatics, adds that \u201ccomplete knowledge is impossible in any network of more than five systems. I have to assume that systems will be breached.\u201d Once breaches occur, security teams reactively rush to detect and defend, which Simons emphasizes is \u201cstill a non-human scale problem.\u201d Organizations often take weeks or even months to discover breaches and data loss.\n\nThe rapidly changing landscape of regulatory laws and compliance requirements adds complexity and can unwittingly weaken security practices for financial companies. Regulatory bodies such as PCI-DSS, Sarbanes-Oxley (SOX), FDIC, Federal Information Security Management (FISMA) and The Federal Trade Commission (FTC) all impose specific guidelines and significant penalties for loss of protected data. GDPR requirements are currently being defined and will impact every financial services company operating in Europe. With regulations in constant flux, institutions scramble to change business practices to comply. Cybersecurity gaps can result when organizations lack sufficient time to invest in a solid security architecture for new processes.\n\nThe rise of cloud computing, internet of things (IoT), and dependence on third-party vendors add yet more layers of mandatory maintenance often overlooked by companies. Banks traditionally operate mission-critical technologies on-premise, where they have access and control over physical infrastructure, but modern software vendors and connected devices rely on the cloud, where environments and security levels continually change. \u201cThird party vendors have been the culprit of numerous high profile security incidents,\u201d reports Alok Tongaonkar, Head of Data Science at RedLock. \u201cSecurity is only as good as the weakest link.\u201d\n\nThough new technologies may introduce additional paths of attack, humans are still the weak link in most security attacks. The vast majority of breaches start with unauthorized leaks by a malicious insider, perhaps a disgruntled employee, or unwitting victims of sophisticated spear phishing or social engineering tactics. Even with the right tools in place, many organizations lag in education and training for both security professionals and regular employees who can be targeted in attacks. \u201cCybersecurity is a \u2018neighborhood watch\u2019, or community police activity,\u201d emphasizes Chris Geiser, Chief Technology Officer of The Garrigan Lyman Group, \u201cInformation sharing is the key to success.\u201d\n\nHackers have become increasingly sophisticated at obscuring their footsteps, at times flooding systems with fake alerts to distract from truly insidious ones. Researchers from MIT\u2019s Computer Science and Artificial Intelligence Laboratory (CSAIL) demonstrated that a virtual AI security analyst trained by human experts accurately identifies 85% of attacks, beating previous benchmarks by a factor of three while reducing false positives by a factor of five.\n\nWith such obvious advantages, both startups and established security providers have integrated machine learning advances into their tools. Mary Jane Wilson-Bilk, Partner at Eversheds Sutherlands, reported \u201cmore than 550 cybersecurity vendors selling their wares\u201d at a key industry conference earlier this year. Vendors range from newer entrants like Deep Instinct and Vectra Networks to established industry players like Splunk. All promise real-time monitoring of systems and networks to offer insights beyond traditional security tools and empower analysts to detect and neutralize threats faster.\n\nOther machine learning solutions, such as Sift Science, protect customers of financial institutions by automatically flagging instances of account takeover (ATO) and fraudulent transactions. \u201cCustomers are particularly concerned about account takeovers, which typically happen after large data breaches,\u201d explains Jason Tan, Sift Science\u2019s CEO. \u201cIn 2016, 554 million records were compromised in the first half of the year alone and ATO losses reached $2.3 billion, a 61% increase from the year before.\u201d\n\nEntities such as the Financial Services-Information Sharing and Analysis Center (FS-ISAC) have been in place for nearly two decades to enable information sharing of cyber threat data between institutions. Recently, eight of the largest banks within the FS-ISAC established a joint effort to use intelligence, technology, and automation to strengthen financial sector defenses. Such collaborative efforts are critical to building modern AI systems with maximum effectiveness.\n\nDARPA, a U.S. Department of Defense Agency responsible for developing emerging technologies for the military, recently held the Cyber Grand Challenge, the world\u2019s first all-machine hacking tournament. The winning team\u2019s AI system, aptly named \u201cMayhem\u201d, autonomously detected new vulnerabilities it had not been explicitly trained on. \u201cWhile DARPA\u2019s intention for this Grand Challenge was benevolent \u2014 the idea is that AI systems will find flaws and then fix them \u2014 similar principles could be used to find software flaws and exploit them,\u201d warns Wilson-Bilk of Eversheds Sutherlands.\n\nBulk spear phishing tactics traditionally use little personal information, yet are highly effective. Such attacks can be personalized and made more powerful by leveraging machine learning based on victim data and previous attacks. Ransomware users can analyze targets\u2019 business operations to identify and extract maximum ransom values. The most sophisticated criminals can even reverse engineer defending algorithms and feed in manipulative data to subvert or overwhelm security systems.\n\nPerhaps the greatest challenge for defenders is how modern technology systems have become so complex that humans no longer fully understand them. Machine learning technologies can create a false sense of security and complacency, when in reality they must be constantly trained, tested, and updated by competent professionals to defend against new and unknown attacks.\n\nWith increasing connectivity and interdependency in the critical infrastructure that enables modern life, the stakes in the arms race between security professionals and hackers are ever escalating. Wilson-Bilk concludes with a sobering reality: \u201cIf a well-funded and sophisticated foreign government wanted to initiate a full-scale cyber war against the United States, they likely have the ability to simultaneously and significantly disrupt transportation, electrical infrastructure, and the financial sector in a way that brings the entire country grinding to a halt.\u201d"
    },
    {
        "url": "https://medium.com/topbots/meet-these-incredible-women-advancing-a-i-research-d5f9eedecf30",
        "title": "Meet These Incredible Women Advancing A.I. Research",
        "text": "You already know that artificial intelligence is eating the world, transforming virtually every industry and function. But you might not have met the brilliant AI researchers and technologists driving the edge of innovation.\n\nIncredible breakthroughs occur when talented and diverse thinkers collaborate, pooling together unique backgrounds, disciplines, expertise, and perspectives. Holistic and inclusive thinking is even more important in the field of AI, where our inventions have pervasive and exponential impact.\n\nThis list of 20+ leading women in AI research is not comprehensive. Far more talented people contribute to the field than we can quickly summarize in a single article. All of the women featured here overcame personal and professional challenges to achieve incredible impact and become leaders and role models for the industry.\n\nWe are proud to tell you their stories.\n\n\u201cWe all have a responsibility to make sure everyone \u2014 including companies, governments and researchers \u2014 develop AI with diversity in mind,\u201d emphasizes Fei-Fei Li.\n\nA renowned academic in computer vision, Li recently joined Google Cloud as Chief Scientist of Artificial Intelligence & Machine Learning to advance her mission of \u201cdemocratizing AI\u201d. She continues to act as an Associate Professor at Stanford, where she directs both the Stanford AI Lab and Stanford Vision Lab. Since obtaining a B.A. in Physics from Princeton and a PhD in Electrical Engineering from Caltech, Li has published over 150 scientific papers in top-tier journals and conferences and built ImageNet, a 15 million image dataset that contributed to the latest developments in deep learning and AI.\n\nLi points out that locking up talent and knowledge in academia and giant companies damages computing diversity, reduces creativity and innovation, and exposes the marginalized to injustice and unfairness. Her non-profit, AI4ALL, supports K-12 education programs for underrepresented groups in AI.\n\n\u201cTechnology could benefit or hurt people, so the usage of tech is the responsibility of humanity as a whole, not just the discoverer. I am a person before I\u2019m an AI technologist.\u201d\n\nDuring her 18 years as a Professor of Computer Science at Stanford, Daphne Koller authored over 200 publications in top scientific journals and won an innumerate number of awards for academic breakthroughs and excellence in education. She went on to co-found Coursera, the world\u2019s largest online education platform, and now serves as Chief Computing Officer at Calico Labs, an Alphabet (Google) R&D company studying the biology of aging and developing interventions for longer and healthier lives.\n\nAmong her cross-disciplinary achievements, Koller is most proud of educating students who have gone on to make their own incredible contributions, including the millions who enrolled in AI, machine learning, and data science courses on Coursera. Of learners who completed courses, 29% reported tangible benefits, such as starting new careers or businesses. Importantly, for disadvantaged students from emerging economies or low socioeconomic backgrounds, the number jumps to 48%.\n\nA world renowned pioneer in social robotics, Cynthia Breazeal splits her time as an Associate Professor at MIT, where she received her PhD and founded the Personal Robots Group, and Founder and Chief Scientist of Jibo, a personal robotics company with over $85 million in funding.\n\nWhile Breazeal\u2019s work has won numerous academic awards, industry accolades, and media attention, she had to fight early skepticism in the 1990s from other experts in robotics and AI. At the time, robots were seen as physical and industrial tools, not social or emotional companions. Her first social robot, Kismet, was unfairly called out in popular press as \u201cuseless\u201d.\n\nBreazeal bucked the trend with a very different vision: \u201cI wanted to create robots with social and emotional intelligence that could work in collaborative partnership with people. In 2\u20135 years, I see social robots helping families with things that really matter, like education, health, eldercare, entertainment, and companionship.\u201d\n\nShe hopes her work and influence will inspire others to create robots \u201cnot only with smarts, but with heart, too.\u201d\n\nAs a Professor of Government and Technology at Harvard and Director of Harvard\u2019s Data Privacy Lab, Latanya Sweeney tackles challenges of security, privacy, and bias in personal data and machine learning algorithms.\n\nSweeney\u2019s research has exposed discrimination in online advertising, where internet searches of names \u201cracially associated\u201d with the black community are 25% more likely to yield sponsored ads suggesting that the person has a criminal record, regardless of the truth. In her role as Editor-In-Chief of Technology Science, she reported that SAT test prep services charge zip codes with high proportions of Asian residents nearly double the average price, regardless of their actual income. Price discrimination based on race, religion, nationality, or gender is illegal in the United States, but enforcement of the law is challenging in online commerce where differential pricing is wrapped up in opaque algorithms.\n\nPrior to her current role, Sweeney was CTO of the Federal Trade Commission. She completed her undergraduate studies in computer science at Harvard and was the first black woman to receive a PhD in Computer Science from MIT.\n\nAndrea Frome didn\u2019t start her career intending to become a top AI researcher. Originally an environmental scientist, she fell in love with the data and modeling aspects of her work, which inspired her to switch gears and pursue a PhD in Computer Vision and Machine Learning at Berkeley. She then joined Google, where she published seminal research papers on multi-modal visual classification systems and launched Google Street View.\n\n\u201cI\u2019ve often found greater satisfaction in solving problems with impact reaching beyond the academic community,\u201d she explains. \u201cIn the case of Street View, we needed to blur faces and license plates for privacy protection. Getting the detection accuracy high enough was a hard real-world problem and Street View couldn\u2019t be launched unless we solved it.\u201d\n\nFrome is currently Director of Research at Clarifai, a leading computer vision company. Her ultimate goal is to enable computers to understand visual input the way humans do and make accurate predictions about the world around them.\n\n\u201cThe field of AI has traditionally been focused on computational intelligence, not on social or emotional intelligence,\u201d explains Rana el Kaliouby. \u201cYet being deficient in emotional intelligence (EQ) can be a great disadvantage in society.\u201d\n\nEl Kaliouby was born in Cairo, Egypt and grew up in the Middle East. When she started her PhD in Computer Science at Cambridge University in England, few did research in artificial emotional intelligence. Through continued passion, advocacy, and demonstrable technical progress, el Kaliouby defined the field of \u201cemotion AI\u201d and co-founded Affectiva, where she leads as CEO. Affectiva\u2019s technology has proven transformative for industries like automotive, market research, robotics, education, and gaming, but also for use cases like teaching autistic children emotion recognition and nonverbal social cues. One mother broke down in tears when her child, using Affectiva-powered Google Glasses, learned to make true eye contact with her for the first time.\n\n\u201c3\u20135 years from now, our devices will be emotion-aware,\u201d predicts el Kaliouby. \u201cYou won\u2019t remember what it was like when your technology didn\u2019t recognize when you are sad or angry.\u201d\n\nCarol Reiley didn\u2019t start programming until her first day of college as a freshman engineering major. Pitted against students who\u2019d been coding since they were ten, she found the experience \u201ctremendously intimidating\u201d and \u201calmost quit several times\u201d. Luckily, she not only persisted but thrived, going on to pursue a master\u2019s and PhD in Computer Science and Robotics at Johns Hopkins.\n\n\u201cBack in the 1800s, companies would hire a VP of Electricity,\u201d Reiley remembers. \u201cElectricity was this brand-new concept everyone was excited about, but nobody knew how exactly it would impact the world. We see AI in the same way now.\u201d Challenges and ambiguities aside, Reiley\u2019s mission since childhood has been to impact the world through engineering.\n\nShe\u2019s now Co-Founder and President of Drive.ai, which formed out of Stanford University\u2019s AI Lab and builds deep learning software for self-driving cars. Despite competition from deep-pocketed tech giants and auto industry skepticism, Reiley and her team raised a $12M Series A, grew the company to 60+, and released several autonomous vehicles on the road.\n\nIn her 7 years at Baidu, technical chief Hua Wu has been responsible for a number of breakthroughs in natural language processing (NLP), dialogue systems, and neural machine translation (NMT). Her proposal for a multi-task learning framework for NMT was hailed by the New York Times as \u201cpathbreaking\u201d and successfully deployed at scale to hundreds of millions of users using Baidu\u2019s translation products. She also built the technology behind Duer, Baidu\u2019s conversational AI which powers home assistants and smart IoT devices. Wu received her PhD from the Chinese Academy of Sciences and co-chairs leading academic AI conferences such as ACL and IJCAI.\n\nWhen Wu first started her research, deep learning had made material progress in computer vision and speech, but not yet in NLP. Many established experts were skeptical that deep learning could improve machine translation, but Wu and her team not only proved the utility but shipped in less than 6 months a working product that processes 100 million translations a day.\n\n\u201cI\u2019m proud of the vision, tenacity and speed of my team,\u201d she beams. \u201cOur improved translation breaks language barriers for people and helps them learn something new.\u201d\n\n10 years ago, way before deep learning was cool, Angelica Lim used Yann LeCun\u2019s convolutional neural networks to break Hotmail\u2019s CAPTCHA system. She even did it in the recursive programming language LISP, but never published her results since neural networks weren\u2019t in vogue then.\n\nDuring her masters and PhD at Kyoto University, Lim combined computer science with neuroscience and cultural development psychology to build a robot that \u201cfeels\u201d. As a pioneer in \u201cdevelopmental robotics\u201d, which models human-style learning in machines, Lim explains that toddlers link names of emotions to specific sets of physiological and psychological states as well as physical expressions. Learning for both humans and robots is heavily influenced by caregivers and culture.\n\nLim is currently a Robotics Software Development Manager in R&D at Softbank Robotics, creators of the humanoid robot Pepper. She\u2019s given a number of TED talks on designing and co-existing with emotional and empathetic robots.\n\nDaniela Rus is a Professor of Electrical Engineering and Computer Science at MIT, Director of MIT\u2019s Computer Science and Artificial Intelligence Laboratory (CSAIL), and head of CSAIL\u2019s Distributed Robotics Lab. She previously founded the Dartmouth Robotics Lab and is known for her pioneering work in self-reconfiguring robots which adapt to different environments by changing their internal structure.\n\n\u201cOur recent 3D-printed soft robots are safer, cheaper and more resilient than hard-bodied robots created through traditional manufacturing,\u201d she explains. The agile structures of soft robots enable them to easily change direction and squeeze into tight spots. Being able to 3D print them also democratizes manufacturing.\n\n\u201cUsing simple household materials like paper and plastic, we can produce functional robots that practically walk right out of the printer.\u201d\n\nOriginally from Istanbul, Turkey, Ayse Naz Erkan moved to the US in 2004 for a PhD in Computer Science at the Courant Institute of NYU. She researched deep learning applications for off-road autonomous robot navigation in Yann LeCun\u2019s lab and studied semi-supervised learning at the Max Planck Institute For Biological Cybernetics before joining a tech startup as an early engineer.\n\nErkan describes her startup days as \u201cincredibly life-changing\u201d, transforming her into a better problem solver and pragmatic technologist. She now leads the Content Understanding and Applied Deep Learning team at Twitter, which acquired the company 5.5 years ago, and has worked to make the social network a safer place.\n\n\u201cWorking on hate speech and abuse with Twitter data was quite exciting,\u201d reflects Erkan, \u201cEspecially witnessing first-hand how machine learning is impacting public communication design.\u201d\n\nJane Wang started out as an applied physicist modeling the complex network dynamics of memory systems in the brain before moving into experimental cognitive neuroscience as a postdoc at Northwestern. Since joining DeepMind two years ago, her non-machine learning background has equipped her with a unique set of tools and perspectives for tackling the hardest AI problems. \u201cIt\u2019s exhilarating to formulate theories of human brain function as powerful deep reinforcement learning models that can solve similarly complex tasks,\u201d she shares.\n\nThough Wang has been successful without a formal AI background, she\u2019s concerned the steep learning curve and hypercompetitive atmosphere of AI research can discourage diverse participation. \u201cAlthough competitiveness drives the field forward, it also discourages those who wish to work in more inclusive, cooperative environments,\u201d she warns. Wang is on a steering committee at DeepMind to increase diversity in AI and is encouraged by the AI community\u2019s openness for sharing research and driving collective progress.\n\nCarolina Galleguillos was born in Santiago, Chile. After graduating with honors and an Engineering and Computer Science degree from University of Chile, she won a government scholarship for a Silicon Valley internship which eventually led her to complete her PhD in Computer Science at UCSD. Throughout her academic career, she published research at major computer vision conferences and was awarded IGERT NSF fellowships in 2007 and 2008.\n\nGalleguillos has developed computer vision and machine learning algorithms for giants like Google, Hewlett-Packard, Honda, and Thumbtack, but she\u2019s particularly proud of the scrappy AI team she built and trained at SET Media. Despite very limited resources, her group shipped production machine learning systems that were critical to the company\u2019s acquisition by Conversant in 2014.\n\nDevi Parikh is an Assistant Professor in the School of Interactive Computing at Georgia Tech and a Visiting Researcher at Facebook AI Research (FAIR). After receiving her masters and PhD in Electrical and Computer Engineering from Carnegie Mellon, she\u2019s held multiple visiting positions at top research labs and won accolades such as the 2017 IJCAI Computers and Thought award, considered \u201cthe premier award for AI researchers under the age of 35\u201d.\n\nParikh\u2019s most proud of her research work in Visual Question Answering (VQA) which lies at the intersection of computer vision and natural language processing (NLP). \u201cThrough making our large datasets and systems publicly available, we\u2019ve enabled research groups around the world to make significant progress on building machines that can automatically answer questions about visual content,\u201d she highlights. Such technology can aid the visually impaired and transmit information on low-bandwidth networks that can\u2019t support images.\n\nAdvances in VQA also improve existing product experiences. \u201cWe\u2019ll see more and more conversational agents \u2014 be it personal assistants or chatbots \u2014 that can see, or augmented reality experiences that are visually intelligent.\u201d\n\nMarie desJardins has always been driven by broad, big-picture questions in AI rather than narrow technical applications. For her PhD dissertation at Berkeley, she worked on \u201cgoal-driven machine learning\u201d where she designed methods an intelligent agent can use to figure out what and how to learn. As an Associate Dean and Professor at University of Maryland, Baltimore County (UMBC), desJardins has published over 120 scientific papers and won accolades for her teaching, but is equally proud of work she\u2019s done with graduate students on self-organization and trust in multiagent systems.\n\nWhen desJardins started her career, the AI and computing industry attracted more diverse, multi-disciplinary talent. Over time, she observed that conferences are \u201cincreasingly dominated with papers that focused almost exclusively on one subproblem (supervised classification learning) and much less welcoming of work in other subareas (active learning, goal-directed learning, applied learning, cognitive learning, etc),\u201d which she is worried will exacerbate the diversity gap in AI.\n\n\u201cWe are already seeing a reconsideration of more symbolic, representation-based approaches,\u201d desJardins observes. \u201cUltimately I think that we will build more and more bridges between numerical approaches and symbolic approaches, and create layered architectures that take advantage of both.\u201d\n\nSince getting her PhD in Mathematics from Duke, Rachel Thomas has worked as a quant, data scientist & backend engineer at Uber, and professor in University of San Francisco\u2019s (USF) Masters of Analytics program. She\u2019s currently a researcher-in-residence at USF\u2019s Data Institute and co-founded Fast.ai which makes practical deep learning education accessible globally. Thomas\u2019 students have leveraged their knowledge to reduce farmer suicides in India, assist the visually impaired, and treat Parkinson\u2019s disease.\n\nWhen Thomas first started researching deep neural networks a few years ago, virtually no educational resources existed online. \u201cIt seemed like everyone in the field had done their PhD with the same four advisors and nobody was sharing the practical, useful info,\u201d she observed. As a solution, she co-produced a free Practical Deep Learning For Coders course intended to ramp anyone with reasonable coding skills up on applied neural network approaches. Thomas\u2019 initiative has been successful in enabling more women, people of color, international students, and the economically disadvantaged to participate in AI research and engineering.\n\nVast amounts of health information are collected digitally, yet largely underutilized for extracting insights to improve healthcare. Suchi Saria, Assistant Professor at Johns Hopkins University, believes computational modeling of data from sensor platforms and electronic medical records presents \u201ca tremendous opportunity for high impact work.\u201d\n\nPrior to Johns Hopkins, Saria did her PhD at Stanford with Dr. Daphne Koller and was an NSF Computing Innovation Fellow at Harvard. She was initially convinced she would not like biology or medicine, but became hooked after studying disease prevention in infants using continuously collected physiological data. With her multi-disciplinary expertise, Saria has published highly regarded scientific papers on disease trajectory modeling, predictive methods for care targeting, clinical decision support (CDS) systems, and individualized treatment approaches.\n\nSaria passionately encourages upcoming researchers to pick important problems to work on and delve deeper into their complexity and constraints.\n\nVery few earn the title of \u201cDistinguished Engineer and Master Inventor\u201d at IBM, but Rama Akkiraju\u2019s contributions warrant the distinction. She leads the mission of \u201cPeople Insights\u201d at IBM Watson and develops technologies that infer personalities, emotions, tone, attitudes, and intentions from social media data using linguistic and machine learning techniques. Akkiraju helmed the teams responsible for many of Watson cognitive services, including Tone Analyzer.\n\nTo tackle this challenging space, Akkiraju\u2019s teams leverage multiple disciplines including AI, psychology, sociology, decision theory and consumer behavior. \u201cBots that really understand people can bridge the shortage of customer support agents, guidance counselors, and health coaches,\u201d she points out. \u201cThese are all areas where our work can make a meaningful difference in people\u2019s everyday lives.\u201d\n\nJackie Hunter has always been fascinated by big data\u2019s potential to transform biotech, but saw first-hand as a senior executive in R&D at GlaxoSmithKline how little of the data is actually mined to produce insights for better medicine. Now, as CEO of BenevolentBio, the bioscience arm of BenevolentAI, Hunter combines her extensive academic and industry experience to accelerate drug discovery and development with artificial intelligence.\n\n\u201cIn drug discovery and healthcare, I believe the next 5 years are going to see more transformation than the previous 50,\u201d she predicts, but also warns against pharmaceutical companies adopting AI in a piecemeal fashion. \u201cCompanies that develop and implement an integrated digital and AI strategy across the whole value chain will be those that will succeed in the next 5\u201310 years.\u201d\n\nSince receiving her PhD in Computer Science from Stanford, Shubha Nabar has built data products and data science teams at Microsoft, LinkedIn, and now Salesforce. As a senior director of data science on Einstein, she and her team make AI accessible to business users by infusing intelligent functionality across all Salesforce products.\n\nBuilding AI solutions for hundreds of thousands of enterprises across a huge array of use cases is technically challenging. Nabar\u2019s novel approach is to build a \u201cmeta\u201d machine learning framework that automates the building of entire machine learning pipelines. \u201cLeading the team through these challenges has been incredibly rewarding because we\u2019re building something completely unprecedented,\u201d she shares.\n\nWhen machine learning is democratized, Nabar cautions that \u201cwith this ubiquitousness, we need to emphasize ethical implications of AI and building fair and accountable algorithms that do not propagate bad biases that often exist in real world data.\u201d\n\nAs a teenager, Timnit Gebru left Ethiopia for the United States. She thrived in her new country, where she enrolled at Stanford University for a bachelor\u2019s, master\u2019s, and PhD in Electrical Engineering, landed a prestigious engineering job at Apple, and co-founded a startup. Studying computer vision under Dr. Fei-Fei Li, Gebru authored several notable papers in her research area of mining large scale datasets for sociological insights. Her recent work using machine learning methods to extrapolate census data from Google Street View images was lauded by The Economist.\n\nGebru actively works to boost diversity and inclusion in the field of AI. After noticing she was the only black woman at a major AI conference, she co-founded the social community Black In AI to drive connection and participation in AI research. Gebru also returned to Ethiopia to co-teach AddisCoder, a programming bootcamp, to a diverse range of young students and help them win admission to Ivy League universities.\n\nSince AI affects all aspects of society, even being used to manipulate elections and identify criminals, Gebru cautions that \u201cAI researchers should not be silent regarding the repercussions of their work.\u201d Only when technology creators tend to inclusion will the exponential benefits of artificial intelligence positively impact all.\n\nMariya is the Head of Research & Design at TOPBOTS, a strategy and research firm for applied artificial intelligence. Read more of her reporting on the A.I. industry.\n\nOriginally published at www.forbes.com on May 18, 2017."
    },
    {
        "url": "https://medium.com/topbots/5-key-trends-in-enterprise-artificial-intelligence-54bf3757ddb6",
        "title": "5 Key Trends In Enterprise Artificial Intelligence \u2013 TOPBOTS \u2013",
        "text": "As AI continues to demonstrate unprecedented potential to transform technology and business, executives at leading enterprises are taking action to enable their organizations to capitalize on emerging breakthroughs.\n\nHere are 5 key trends that innovative companies are driving in enterprise AI.\n\nOrganizations must be set up to take advantage of AI. Current enterprise technology stacks consist of compartmentalized vendors that tackle different workflows, making whole system and funnel views difficult to gather. To truly be able to take advantage of AI, an organization needs both more data as well as a holistic picture of their data.\n\nAI-progressive organizations invest in converging infrastructure and connected solutions to scale workloads and collect data appropriately.\n\nStrong executive leadership is critical to collecting data enterprise-wide and driving AI solutions through political hurdles. Roles responsible for an organization\u2019s data and AI efforts, i.e. Chief Data Scientist, Chief Information Officer, Chief AI Officer, continue to rise in popularity and importance.\n\nData is a critical corporate asset, but entails many organizational challenges. Data collection and cleaning is often hampered by unstructured or inconsistent organization, departmental silos, and company incentives and politics.\n\nAI can both automate (replace entirely) or augment (improve efficiency) of employee tasks, allowing staff to focus on strategic or creative responsibilities. AI can even create new jobs, especially those related to the creation and stewardship of AI systems.\n\nExamples of automation include cleaning and de-duping data for analytics, processing forms in the back office, flagging at-risk expense line items in accounting, and scouring the web and social media to automatically surface viable candidates for recruiting. Examples of augmentation include cyborg models (i.e. human-AI hybrid) of customer service and cognitive services that advise on executive decision-making.\n\nMany more examples can be found in our extensive Enterprise AI landscape which features over 100+ AI vendors in 12 enterprise categories.\n\nEnterprises use machine learning primarily to surface insights, but soon tools will evolve to support decision-making based on those insights.\n\nThe primary challenge for organizations is incorporate ML-surfaced insights into day-to-day workflows and processes. To facilitate the practice, enterprise AI solutions often include managed solutions on top of product offerings, which reduces complexity for customers and ensures action-orientation. The combined solution helps to point out observations and relationships in data as well as point employees in the right direction, sometimes even automating decisions for them.\n\nWe already see examples such as AI-driven marketing campaign management systems increasing automate more and more campaign decisions, ranging from campaign parametrization to creative generation.\n\nMost groundbreaking work in AI in the past few years has been driven by major technology players like Google, Facebook, Amazon, Microsoft, and Salesforce which own the requisite data sets and computing power. However, as cognitive services have become a de facto offering and computing costs continue to decline, we expect the democratization of AI access to enable smaller businesses, startups, and entrepreneurs to experiment with applied AI and build original innovations."
    },
    {
        "url": "https://medium.com/topbots/dope-learning-ai-can-now-best-rappers-with-deep-beats-57a78d36e58c",
        "title": "Dope Learning: AI Can Now Best Rappers With \u201cDeep Beats\u201d",
        "text": "Rapping is no easy feat. In-demand artists like Sean \u201cDiddy\u201d Combs and Jay Z bring home fortunes to the tune of $735 and $550 million respectively for their stylish flow. That kind of dough can buy you ~3,600,000 bottles at any hot club. Or ~816 million hours of GPU server time from AWS. Depends on your priorities.\n\nBut Dr. Dre better watch out, because a bunch of Finnish nerds just developed a \u201cnovel deep neural network architecture\u201d to give him a run for his money ($700 million, to be precise). The technique is aptly named Dope Learning and powers an online tool called DeepBeat. When a homogenous bunch of European and presumably white dudes build an AI rapper, does this count as a new digital form of cultural appropriation? Debatable, but we can save that argument for another day.\n\nAside from broaching important cultural subjects, rap consists of intricate structures and complex rhyme patterns which require sophisticated language and lyrical skills to generate. Most of us couldn\u2019t freestyle rap to save our lives. DeepBeat tackles the challenge by first looking at rap lyrics and predicting the next line. These predictions are then combined to merge lines from existing songs into new rhymes with meaning.\n\nThe team started with half a million lines of rap songs from over 100 artists. Lyrics creation was then modeled as an \u201cinformation retrieval\u201d problem, where the query is the first x number of lines of a song, and the answer is the most relevant follow up lyrics. This approach simplifies the challenge of measuring performance, because accuracy can be assessed against actual songs. A generative model that constructs new lyrics word by word would yield more creative output, but also drive the complexity up significantly. Perhaps an aspiring academic should hustle on this front and produce a Dope Learning 2 paper.\n\nAfter mapping lines to high-dimensional vector space, DeepBeat leverages a Ranking SVM to pick the most relevant next lyric. Document ranking algorithms like PageRank use a single static ranking, but accuracy can be dramatically improved by combining multiple features via machine learning algorithms. Recurrent neural networks (RNNs) are typically used for textual predictions, but since rap lines are relatively short and equal in length, a feedforward network is a simpler and still suitable architecture for this problem.\n\nNow for a lesson in rhyme schemes. Rap lyrics rarely employ \u201cperfect rhymes\u201d such as \u201cHey I got GPUs, but Google\u2019s got TPUs.\u201d Alliteration rhymes involve repeating the opening sounds of adjacent words, such as \u201cTuring Test\u201d, but don\u2019t require the words start with the same letter, as in \u201cKnow Your Neural Networks.\u201d Alliteration is a special subset of consonance, which is the repetition of consonants. Consonants can be matched throughout a word, as in \u201cmode collapse just makes me gasp.\u201d Assonance is the repetition of vowels and is by far the most popular rhyme scheme due to versatility. Consider this sentence \u201cIt is my greatest dream, to build a deep learning machine.\u201d Finally, you can have multi-syllabic rhymes. I spent 15 minutes trying to come up with a deep learning themed multi-syllabic rhyme using only assonance but failed. Rapping is hard\u2026\n\nDope Learning detects rhymes by translating words into phonetic representations and computing \u201crhyme density\u201d, which represents the \u201caverage length of the longest rhyme per word.\u201d The measure positively correlates with the complexity rating human rappers give their own lines. Song structure is another pattern the network must learn, as songs often alternate between verses and chorus. Any particular rhyme scheme can be maintained for multiple lines or even an entire verse or chorus.\n\nDeepBeat launched in November 2015 and attracted 42,000 users by June 2016. The tool enables users to select keywords that must appear in generated lyrics, get automated suggestions, and also write their own content. Performance was measured in three distinct ways. For selecting the next lyric, the algorithm was given 299 randomly selected lines and asked to choose the best line. The result was an accuracy of 17%, or 50x better than chance. Second, using the measure of rhyme density (mathematical details can be perused in the paper), DeepBeat outperformed human rappers by 21%. Finally, real user data from the DeepBeat online tool was analyzed and machine choices correlated with lines preferred by humans.\n\nDr. Dre, I hope you\u2019re ready for your epic rap battle against MC DeepBeat."
    },
    {
        "url": "https://medium.com/topbots/balancing-machine-learning-and-human-intuition-in-the-travel-industry-811959846c7e",
        "title": "Balancing Machine Learning And Human Intuition In The Travel Industry",
        "text": "Travel planning is incredibly stressful. Between researching options, paying for bookings, and organizing your itinerary, you may also have to contend with the risk of being beaten and dragged off planes.\n\nMachine intelligence can alleviate some of the pain points for both you and the travel companies you book with. Perhaps no one knows this better than Giorgos Zacharia, CTO of Kayak and holder of a Ph.D. in artificial intelligence and machine learning from MIT. \u201cAI is kind of a fashionable domain at present,\u201d he says, amused by the recent hype, \u201cbut we\u2019ve been doing machine learning and AI at Kayak for a long time.\u201d\n\nAlmost every aspect of your digital user experience is improved with AI. Your preferences towards specific seasons, hotel styles, and price parameters are carefully monitored so that you can be served results you\u2019re likely to book. The photos you see on hotel listings are run through thousands of split tests in which users rank different versions and the results are optimized for mass appeal. Turns out we prefer our hotel photos to be clean and pristine and dislike when they feature other people.\n\nHave you ever gone through an entire hotel or flight booking process, only to be told at the end the item was unavailable? Like many industries, travel companies suffer from inconsistency in data. Due to a slew of legacy systems, changes in hotel and airline databases might not fully propagate in time to booking providers to reflect real-time supply. To combat this problem, Kayak\u2019s algorithms analyze a wide variety of historical sources to generate a more accurate forecast of availability.\n\nAnother common data challenge is handling duplicates. \u201cWith all those records coming from different systems, you can have misspellings, different word orders, and other issues that could cause a system to create duplicate records,\u201d explains Zacharia. For example, a single listing could be titled differently as the \u201cBoston Marriott Hotel\u201d or the \u201cMarriott Hotel In Boston.\u201d To save time, the de-duping process is largely automated by machine intelligence. Only low-confidence records, where the algorithm isn\u2019t sure of a prediction, are escalated to human staff for analysis. Records from different sources may even disagree about basic facts, such as whether a hotel has a pool, but Zacharia assures that \u201cthese algorithms can rationalize that data very, very quickly.\u201d\n\nMachine analysis can yield surprising learnings that contradict your intuition. In a previous role before Kayak, Zacharia built systems to predict corporate bankruptcy filings. One month before a bankruptcy, a company\u2019s credit score often sees a dramatic improvement. The revelation led to further investigation. Turns out CFOs of at-risk companies desperately start paying back overdue bills in the hopes of getting another loan, but typically fail.\n\nSimilar findings occur in the travel space. For example, users care less about the average review score of a hotel and more about the number of reviews. A hotel with fewer than 24 reviews is far less likely to be booked even if the comments are overwhelmingly positive. Users also have a sophisticated nose for spotting good deals. Broadcasting a clear discount typically results in higher conversions, but even when a hotel discounts rooms without revealing original prices, buyers intuitively flock to the deal.\n\nKayak is hardly the only travel company leveraging machine learning. Booking.com, helmed by CEO Gillian Tans, prides itself on international reach, listing properties in over 225 countries in 43 languages.\n\nMany don\u2019t realize that \u201cBooking.com is one of the biggest translation companies in the world,\u201d according to Tans. Headed to a foreign country where you don\u2019t speak the language? No problem. Booking\u2019s cross-platform chatbots allow guests to connect with overseas hotels and hosts and perform real-time translations for all of their supported languages.\n\nIn addition to translating human-to-human conversations in real-time, Booking.com bots act as 24-hour customer service agents able to answer most simple travel questions. Kayak also has bots on Facebook Messenger, Amazon Echo, and Slack, in order of popularity. While the natural language processing (NLP) behind the bots are the same, Zacharia notes that users approach different platforms with different intents. \u201cOn Alexa, we get more aspirational queries, such as how much a flight is to Hawaii. On Facebook, we get more lower-funnel queries after a user has already booked, such as where they should eat,\u201d he reveals. Complex questions still need to be handled by humans, but Tans remarks \u201cit is surprising how much you can do with machine learning and how good it is getting.\u201d\n\nWhile the improvements brought by machine learning are impressive, the travel industry must overcome many barriers to reach Tans\u2019 ultimate vision of AI being a \u201cfully-functional digital travel assistant that can proactively solve potential issues before you even know they exist.\u201d\n\n\u201cBooking travel is not like shopping, or groceries or booking a restaurant. It\u2019s much less frequent, so understanding what works just takes a lot more time.\u201d Tans also emphasizes that we must aim for the correct balance between human interaction and sufficient automation.\n\n\u201cTravel is a combination of the personal and the emotional,\u201d she says. \u201cEvery customer is different and the travel experience is completely fluid, but the end goal is to find the best solutions.\u201d"
    },
    {
        "url": "https://medium.com/topbots/the-future-of-visual-intelligence-for-brands-dc39f96a3343",
        "title": "The Future Of Visual Intelligence For Brands \u2013 TOPBOTS \u2013",
        "text": "This piece was written & edited in collaboration with TOPBOTS writer Julia Bobak.\n\nEver since neural networks began their renewed renaissance in 2012, computer vision has been a ripe field of study and innovation for AI researchers and a fruitful area of applied AI for enterprises. Deep learning enables incredible feats of machine vision, such as classifying image subjects at human parity and dynamically generating completely new imagery.\n\nBrands have quickly recognized this technology as a commercial asset. Clarifai, a leading computer vision company with a popular visual intelligence API, was recently used to analyze hundreds of photos of fans during baseball games to identify when they have caught a ball. The insight was then overlaid over stadium seating maps to show the highest probability seats for fans hoping to bring home a souvenir catch. From a business perspective, such data can drive more effective tiered pricing.\n\nThe same technology was equally adept playing Hearthstone, a popular card game produced by Blizzard Entertainment. By watching many rounds of play, the AI learned the cards and strategies of the game and could offer play-by-play commentary. During a hackathon, developers used Clarifai\u2019s visual APIs for yet another purpose: Tinder swiping. The algorithms learned the preferences of one of the developers and took over the laborious act of swiping for him.\n\n\u201cThe common theme here is customization,\u201d explains Clarifai founder and CEO Matthew Zeiler. \u201cYou can teach the platform whatever you want to recognize, and that applies all the way up to the biggest companies.\u201d Indeed, many large corporations in search of faster methods of photo searching and sorting are taking notice. Digital media juggernaut Buzzfeed adopted visual intelligence in order to assist editors in finding relevant photos for articles. Another one of Clarifai\u2019s customers, a proponent of \u201cnatural beauty\u201d, used visual intelligence to identify examples from social media of fashionable women not wearing makeup who embodied their brand message.\n\nZeiler and his team at Clarifai are not the only ones who are banking on the potential of visual intelligence. Major players like Google, IBM, Salesforce, Microsoft as well as smaller entities such as GumGum and Ditto all offer computer vision solutions. The Google Cloud Vision API, for example, enables user to easily organize stored photos. Amazon\u2019s version, Rekognition, is marketed towards developers who want to add facial recognition security to apps or record the demographics of their viewers. GumGum\u2019s offer is directed at companies interested in using social media posts to monitor brand representation online or calculate earned media from videos of sporting events. A big promise of visual recognition, says Zeiler, is that of \u201cnext-level analytics that show how your products are being used out in the world.\u201d\n\nChallenges still remain, especially with brand data. For example, many retailers have catalog photos of products against white backgrounds. While an algorithm could be trained to recognize these products in sanitized conditions, identifying the same shoes in real-world scenarios would require more training data. Zeiler says the key going forward is that \u201cit\u2019s not going to be the AI experts that collect this data \u2014 it\u2019s going to be everybody on the planet.\u201d To facilitate this, he and his team are making their technology as widely-adoptable and user-friendly as possible to \u201cget everybody interacting.\u201d\n\nAnother challenge is the lack of high-resolution images, which are crucial for certain applications. Many brands are clamoring for AI solutions for automatic counterfeit detection. The difference between a counterfeit and a real product tends to be very subtle, such as slight variation on a purse buckle or shoelaces, and will likely be undetectable in a low-resolution photo. Many times the difference is hard even for human eyes to detect.\n\nZeiler also believes multi-modal neural networks \u2014 i.e. those which can process many different media types in parallel, such as audio, video, and text \u2014 is the next innovation to focus on. \u201cFusing more than one data type is difficult,\u201d he points out, \u201cand multi-function neural nets don\u2019t perform as well as single ones.\u201d However, there\u2019s extraordinary value to analyzing multiple aspects of a product at once, such as the image, description, prices, user reviews, and user generated images and videos. Zeiler hopes to handle up to 10 modalities at once.\n\nWhile tech giants have been snapping up AI startup companies like Orbeus, Alchemy, and Metamind to catch up in the AI wars, Zeiler believes a dedicated commitment to customer needs will help Clarifai maintain a competitive advantage. \u201cGoogle makes internet balloons, self-driving cars, word processors, email clients, and of course search and advertising products. With so many divisions, they compete with their customers.\u201d If an advertiser sees their proprietary data as their key defensible advantage, they might be hesitant to send this data to Google\u2019s clouds. Similarly, Amazon can learn from another retailer\u2019s data and compete them with directly.\n\n\u201cTwilio is the independent communications company. Stripe is the independent payments company. Spotify is the independent music company. We believe Clarifai will be the independent vision company,\u201d Zeiler concludes."
    },
    {
        "url": "https://medium.com/topbots/why-we-need-to-democratize-artificial-intelligence-education-b64853f70cb6",
        "title": "Why We Need To Democratize Artificial Intelligence Education",
        "text": "When Sahil Singla joined the social impact startup Farmguide, he was shocked to discover that thousands of rural farmers in India commit suicide every year. When harvests go awry, desperate farmers are forced to borrow from microfinance loan sharks at crippling rates. Unable to pay back these predatory loans, victims kill themselves \u2014 often by grisly methods like swallowing pesticides \u2014 to escape the threats and violence of their ruthless debt collectors.\n\nSingla and his team are tackling this social injustice with one unexpected but powerful tool: deep learning. Recent growth of computational power and structured data sets has allowed deep learning algorithms to achieve extraordinary results. Computers can now recognize objects in images and video, transcribe speech to text, and translate languages nearly as well as humans can.\n\nUsing deep learning, Farmguide analyzes satellite imagery to individually separate farms and accurately predict crop yields. In the US, Stanford University researchers have shown machine-driven methods for crop yield analysis to be comparably accurate as physical surveys conducted by the USDA. Armed with this previously unattainable information, Singla and his team can build superior models for lending and insurance, leading to lower and fairer interest rates for at-risk farmers.\n\nThe promises of A.I. extend far beyond the problems of Silicon Valley. While Forbes 400 multinationals and Wall Street hedge funds see A.I. as a tool for superior profits, entrepreneurs and engineers around the world see machine intelligence as a path towards a better society.\n\nKarthik Mahadevan, an industrial designer and engineer from the Netherlands, employs deep learning to grant visually impaired patients more independence. He\u2019s testing A.I. technologies that assist them in daily tasks such as \u201cidentifying items in a supermarket and identifying their clothes.\u201d Tahsin Mayeesha, a university student in Bangladesh, leverages machine learning to analyze media reports of violence against women. With her detailed analysis, she hopes to highlight otherwise overlooked cases in order to generate empathy and awareness.\n\nInefficiencies abound that can be tackled with machine learning, but acquiring the requisite knowledge and resources to apply A.I. is a huge challenge for those who don\u2019t live in the Silicon Valley and other major research centers. Many turn to massively online open courses (MOOCs) provided by companies like Coursera, Udacity and Fast.ai as their only options.\n\nRachel Thomas, a deep learning researcher with a math PhD from Duke, started Fast.ai with Jeremy Howard, formerly CEO of Enlitic and President of Kaggle. Fast.ai\u2019s mission is to make \u201cthe power of deep learning accessible to all.\u201d As passionate champions of diversity and inclusion, the two created international fellowships to enable students like Singla, Mahadevan and Mayeesha to receive the best possible practical A.I. education.\n\n\u201cArtificial intelligence is missing out because of its lack of diversity,\u201d warns Thomas. \u201cA study of 366 companies found that ethnically diverse companies are 35% more likely to perform well financially, and teams with more women perform better on collective intelligence tests. Scientific papers written by diverse teams receive more citations and have higher impact factors.\u201d\n\nFast.ai\u2019s non-stop efforts to democratize A.I. education are paying off. In addition to the extraordinary work done by Singla, Mahadevan and Mayeesha, other Fast.ai students are using deep learning to treat Parkinson\u2019s disease, fight online hate speech, end illegal logging and reduce harmful human activity in endangered rainforests.\n\nThe work is not done, however. Even with MOOCs, students in developing countries face an uphill battle compared to their first-world counterparts. Mayeesha of Bangladesh endured the challenges of a broken generator and intermittent electricity to complete her machine learning projects. Samar Haider, a language researcher in Pakistan, discovered his native language of Urdu lacked the requisite structured data sets for A.I. applications. He had to make his own: \u201cI acquired, cleaned and segmented into sentences an Urdu corpus totaling over 150 million words and trained a model to learn vector representations of words.\u201d\n\nThe learning curve for modern A.I. concepts and tools is also quite steep. Research papers published by academics are filled with arcane math equations and inaccessible jargon. Tensorflow and Theano, two powerful open source libraries for deep learning, are difficult even for experienced software engineers to master. \u201cDeep learning research has moved forward amazingly quickly, but very little of this progress has made its way into the products and processes that make up our world,\u201d explains Francois Chollet, an A.I. researcher at Google. To lower the technical barriers to entry, Chollet invented Keras, a simple and modular library \u201cfor the masses\u201d that allows even beginner programmers to experiment with neural network development.\n\nThe last challenge faced by A.I. practitioners is lack of affordable access to expensive but required computational resources such as graphic processing units (GPU). Even with the right hardware, complex neural network models can take days, if not weeks, to train. Mayeesha regularly competes in data science competitions on Kaggle, but reports that \u201cbecause of lack of computational resources, I am unable to train neural networks for long.\u201d Even though Amazon Web Services (AWS) offers access to GPU servers for $0.90 an hour, the costs add up rapidly.\n\nRight now, Mayeesha uses AWS gift credits from Github for her deep learning projects. Soon, she\u2019ll run out and have to spend her own money on processing power. \u201cLearning requires experiments, experiments require computational resources,\u201d she bemoans. \u201cLack of these computational resources creates a disadvantage for people in developing countries.\u201d\n\nHer concerns are very real: \u201cI\u2019m worried that deep learning will create another source of inequality if this problem does not get addressed soon.\u201d"
    },
    {
        "url": "https://medium.com/topbots/ai-is-turning-supply-chain-logistics-into-automated-trading-3519d71e3506",
        "title": "AI Is Turning Supply Chain Logistics Into Automated Trading",
        "text": "Ever wonder how your Amazon Prime packages show up at your door mere hours after you place an order?\n\nA complex series of operations connects suppliers to manufacturers to wholesalers to retailers to you, the end consumer. Oversight of this process is called supply chain management (SCM). Within SCM, logistics is the portion that handles the movement of goods. E-Commerce giants like Amazon specialize in logistics while consumer packaged goods leaders like Unilever provide full-spectrum supply chain management services.\n\nLike every other data-driven industry, logistics and supply chain companies are investing in transformational A.I. solutions to tackle their most pressing pain points. Both small and large enterprises are dabbling in innovations ranging from machine learning to robotics.\n\nA breakdown in logistics breaks the supply chain, so companies constantly seek out improved ways to manage inventory, predict pricing, and streamline operations. Chad Lindbloom, CIO of C.H. Robinson, a Fortune 500 multi-modal transportation company, shares with us the top business use cases he\u2019s using AI to tackle.\n\nThe largest portion of C.H. Robinson\u2019s business is North American truck freight. A portion of their customers pre-commit to regular business and outsource portions or all of their logistics needs. The remainder are one-off transactions, for which the company is a surge provider for unplanned freight.\n\nSurprisingly for a transportation company, C.H. Robinson owns no vehicles. They are instead what\u2019s called a \u201cfreight broker\u201d, an operational and financial middleman between buyers who want to move freight and suppliers of vehicles who can do the job. The supplier base is incredibly fragmented, ranging from one man with a truck to massive fleets of co-owned vehicles. Despite these capacity challenges, CHR must commit to move freight for a customer at a specific price in advance. Sometimes they\u2019re asked to quote a price a last-minute same-day load. Other times they commit up to 2 years in advance.\n\nPrice prediction is thus their biggest business challenge. \u201cThe pricing in our industry varies seasonally, by day of the week, by lane, by time of the day,\u201d explains Lindbloom. A \u201clane\u201d is an origin destination pair, such as Toledo, OH to New York, NY. Note that reversing the lane, from NYC to Toledo, requires a different price since urban centers don\u2019t generate high volumes of goods that must be moved back to manufacturing zones.\n\nMany vendors such as Watson Supply Chain, ToolsGroup, and TransVoyant offer logistics and supply chain software with AI baked in, but the complexities and nuances of C.H. Robinson\u2019s massive business require them to build in-house technology tailored to their specific needs. Pricing was previously done by human experts with deep domain experience and historical market knowledge.\n\nPrior to becoming CIO, Lindbloom spent 25 years in finance and 15 years as CFO. Combining financial with technical expertise, he and his team have built machine learning models for price prediction that resemble those built by automated traders on Wall Street. These models examine historical freight pricing data along with concurrent parameters such as the weather, traffic, and socio-economic challenges to estimate the fair transactional price on a spot basis.\n\nAI doesn\u2019t always outperform market experts, which Lindbloom believes will not be fully replaced. \u201cIn some cases, humans come up with better price. In most cases, the technology helps them hone in on the fair market price,\u201d he points out. He also adds that a key benefit of effective algorithms is democratization and accessibility of information. Instead of relying on a few industry experts to produce estimates, more employees can use machine intelligence to ensure they\u2019re quoting within market so they don\u2019t lose the sale, and within capacity so they don\u2019t botch the execution.\n\nThe second important use case is securing and managing the supplier inventory, the vast and fragmented array of trucks available to transport loads.\n\nCHR commits to a transport price for freight buyers before they know the exact pricing and availability of requisite vehicles. The company relies on strategic human relationships, specifically a vast trading network across employees to find the right truck with the right capacity for the load.\n\nFor every lane, CHR runs background analytics to examine which carriers have moved freight at what price and service level. Fragile, expensive, or time-sensitive freight requires a much higher service level. Pooling together these various factors allows CHR to optimize matching freight to the best mover.\n\nManaging disruptions is the third important business task that can be improved with AI. Hurricanes, carrier bankruptcies, and employee strikes all have the potential to cause massive damages to the logistics business.\n\nPredicting such disruptions and training AI to learn from contingency plans developed by humans enables automated corrective action in the future. To do so, CHR pulls together sources of information to analyze the impact of past disruptions, such as a carrier strike in France or a hurricane in the NE United States. If a distribution center is threatened with adverse weather, freight can be re-routed to a safer one.\n\nPart of the data collection entails detailed surveys that track how human employees handled disruptions and the outcomes of their management. Lindbloom hopes that eventually systems can be trained to automatically take optimal actions after learning from humans.\n\n\u201cWe are constantly looking at what\u2019s on the marketplace, and we believe we build better technology,\u201d says Lindbloom. Due to a critical need for reliability, CHR builds and manages their own data centers, only going to the cloud if extra computing power is needed. Owning data center resources allows CHR to spin up environments very quickly as needed, but also to commit idle systems to research and development.\n\nIn addition to flexibility, owning data centers enables privacy and control. \u201cWe are a cloud provider of transportation management system to our customers,\u201d emphasizes Lindbloom. \u201cWe have all the same technology as the core cloud providers, but we know where all the data is, we can control it, and we make confidentiality promises to customers. Many of them are more comfortable using us.\u201d\n\n\u201cTechnology is such a differentiating factor in our industry,\u201d Lindbloom concludes. Other giants in logistics and supply chain agree and also committed substantial dollars to AI initiatives. DHL aims to reduce costs with autonomous cars, Active Ants builds wearable technology to optimize warehouse tasks, Locus Robotics develops warehouse robots, and Honda leverages smartphone applications for real-time shipment tracking.\n\nDHL\u2019s 2016 Logistics Trend Radar predicts that artificial intelligence investments will continue to surge for both domestic and international logistics. Increasingly more companies plan to invest in in-house development for AI applications in predictive analytics, operations and management, augmented reality, robotics, and industrial IoT.\n\nLindbloom has words of wisdom for those who want to replicate CHR\u2019s success with AI: \u201cMany of the things you\u2019re going to try probably won\u2019t produce value. Be willing to experiment and fail fast. Try to solve the same questions with multiple different models. Multivariate-type testing is key.\u201d\n\nAdditionally, he cautions against overfocusing on AI and encourages executives to define clear business use cases first. \u201cHave the business challenges drive your development, instead of data scientists and engineers pushing AI into the business.\u201d"
    },
    {
        "url": "https://medium.com/topbots/smarter-ai-means-safer-speedier-networks-90bc30ef75b7",
        "title": "Smarter AI Means Safer, Speedier Networks \u2013 TOPBOTS \u2013",
        "text": "With more than 12,500 patents, 8 Nobel prizes, and a 140 year history of field-testing crazy ideas, no one should be surprised that AT&T would be an important player in artificial intelligence.\n\n\u201cAT&T is a backbone of the internet,\u201d explains Nadia Morris, Head of Innovation at the AT&T Connected Health Foundry. The company manages wireless, landline, and even private secure networks to power connectivity for both individuals and corporations. All these networks generate incredible volumes of data ripe for machine analysis.\n\nAT&T has built AI and machine learning systems for decades, using algorithms to automate operations such as common call center procedures and the analysis and correction of network outages. On the entertainment side, AT&T\u2019s DirecTV division leverages users\u2019 rating histories, viewing behaviors, and other factors to anticipate the next films they\u2019ll watch.\n\nModern A.I. algorithms have enabled the telecom company to tackle even more complex tasks, such as optimizing the rollout of their 5G network. Traditional cell towers are usually suboptimally placed near urban centers and form an imperfect grid, leading to gaps in coverage. They\u2019re also expensive to put up and maintain and incur challenges with real estate and property ownership.\n\nSmall cells are less expensive, more compact cells that can be installed on inner city buildings on a much finer grid. Their role is to repeat the signal from the main cell towers to get closer to end users. By crunching mobile subscriber data, well-calibrated AI can help create spatial models to hone in on ideal spots to build small cells to ensure maximum 5G signal strength for customers.\n\nDesigning the right 5G infrastructure is critical, especially given the rapid rise of video. \u201cVideo is more than half of our mobile traffic,\u201d explains Chris Volinsky, who leads big data research at AT&T Labs. \u201cVideo traffic grew over 75% and smartphones drove almost 75% of our data traffic in 2016 alone. We expect video traffic growth to outpace overall data growth in 2020\u201d.\n\nInfrastructure is an enormous investment, even with small cells, so accurately modeling trends and usage growth is key to success. Demographic trends can cause previously underutilized areas to suddenly become hot traffic generators. While statistical models are useful for identifying trends in customer movement and throughout, AI and machine learning techniques create future projections from current data.\n\n\u201cWe need to visualize billions of data points in a spatiotemporal fashion,\u201d Volinsky elaborates. No tools existed previously to address AT&T\u2019s unique data challenges, so they built and open-source custom tools such as Nanocubes, a data visualization tool that can map out millions of connections of individual mobile phones and connected devices to cell phone towers. The tool has been used outside the company to characterize sports fans in real time and analyze crime rates and history.\n\nAlgorithms and tools are not the bottleneck in solving problems. Volinsky clarifies that \u201cthe challenge is in the data and the data pipeline\u201d. Modern data-hungry AI approaches require a centralized data source, but gathering one across a myriad of networks with idiosyncratic standards is no trivial task. Each small cell collects cellular data differently. Some track 4G but not 3G. Some don\u2019t get iPhone data. If variations are not taken into account, bias will appear in the data and the results.\n\n\u201cThere is no world expert in data munging,\u201d Volinsky bemoans. \u201cTo succeed, you have to figure out organizationally how to access data in different silos, technically how to integrate with it, and ensure the formats are in line.\u201d Data scientists often discover that they can\u2019t solve the problems they want to solve because the fundamentals of managing data is difficult and time-consuming. \u201cThis is not the stuff people learn in grad school,\u201d he warns.\n\nVolinsky\u2019s convinced that AI is the most powerful addition in the toolbox used by AT&T\u2019s research arm to develop the next generation of enterprise and consumer-facing solutions. At the same time, he cautions against using deep learning as a magical black box to solve all problems. Instead, you should prioritize solid data infrastructure, subject matter expertise, and utilizing an ensemble of methods from data science and machine learning toolboxes.\n\nVolinsky would know best. His BellKor team won the coveted $1 Million Netflix Prize in 2009. The key lesson learned during the three year competition was the power of ensembles. Ensembles involve combining various methods \u2014 ranging from regression, support vector machines, singular value decomposition, restricted boltzmann machines, and neural networks \u2014 to produce a result. \u201cDeep learning is a power tool in your toolbox, but you still need your old school tools to solve problems,\u201d he emphasizes. \u201cDeep learning evangelists say neural networks effectively incorporate all the other models, but I have not seen that work in practice.\u201d\n\nIn tandem with in-house projects, AT&T operates six innovation labs, called Foundries, all over the world. Each Foundry specializes in a different industry.\n\nAs Head of Innovation at AT&T\u2019s Connected Health Foundry, Nadia Morris works with aspirational startups such as AIRA, a smart wearables startup that uses human-assisted computer vision algorithms to enable the blind and vision-impaired to \u201cvisualize\u201d their surroundings and navigate their immediate environment.\n\nUsing established manufacturing relationships, AT&T helps healthcare IoT and wearables companies like AIRA accelerate their hardware prototyping and production. Similar to the Labs, the Foundries also leverage custom-built open-source tools such as Flow Designer, a rapid prototyping tool that simplifies hardware design for software engineers.\n\nRemember Morris\u2019 earlier comment about how the internet runs on AT&T? Turns out this can be mission critical for startups like AIRA which must ensure superior connectivity at all times to protect the safety of their patients. Since AT&T\u2019s AI systems regulate network traffic, they can intelligently detect AIRA devices on their network and dynamically allocate greater bandwidth to support live video streaming.\n\nAT&T\u2019s control of networks also comes in useful for hospitals who hoard sensitive patient data. Fearful of security lapses, many operate their own data centers for fear of uploading personal information to the cloud. Data center management is typically not a hospital\u2019s core competency, leading to outdated technology and massive inefficiencies.\n\n\u201cDo you want to run a hospital or do you want to run a data center,\u201d questions Morris. Regardless of the cloud provider a hospital chooses to use, AT&T runs private network connections to all of their servers. \u201cThis traffic will never traverse the public internet,\u201d she assures, giving hospitals an extra layer of protection.\n\nMigrating more hospitals to the cloud solves not only administrative pains, but also unblocks AI research. \u201cHospitals are smart, but they\u2019re like islands,\u201d Morris explains. Competition often incentivizes hospitals to hoard data that is critical to share for superior results. Pooling hospital data into \u201ccollaborative cloud communities\u201d and applying de-identification protocols enables medical researchers to access disparate data sets with greater geographic diversity. Algorithms for essential patient services such as vital sign monitoring can be trained on aggregate data sets for more accurate benchmarks.\n\nLead Inventive Scientist Wen-Ling Hsu has been with AT&T for over 20 years. She obsessed over creating amazing customer experiences using massive data and information even before \u201cbig data\u201d was coined.\n\nHsu analyzes customer conversations from both phone conversations from call centers and online chats with support agents. Machine learning allows her to build textual models, identify customer intent, and route them to appropriate support agents faster.\n\nWith her extensive experience, Hsu learned that interpreting and using the intelligence gained from AI systems is \u201cmore of an art than a science.\u201d What matters most is customer perception and seamless execution, so Hsu employs a combination of bots that directly interact with customers and those that stay in the background to assist human agents.\n\nWhen asked to make a forecast for AI in 2017, Hsu responded, \u201cHuman judgment still plays a critical role in many tasks. Together, AI bots and human agents can learn from every customer interaction to personalize the customer experience.\u201d"
    },
    {
        "url": "https://medium.com/topbots/can-artificial-intelligence-give-you-beauty-advice-9ec26ea21270",
        "title": "Can Artificial Intelligence Give You Beauty Advice?",
        "text": "The beauty and skincare world is oversaturated, especially if you include all the affordable convenience store brands. If you\u2019re a shopper with a budget, you are likely mixing different products and blindly guessing which combinations will work \u2014 like a chemist without a periodic table.\n\nAI technology has already revolutionized transportation, food, and even health. Why not also beauty and retail? Sephora has launched multiple chatbots, including a bot on Facebook Messenger that lets you book makeover appointments in stores and a bot on Kik that gives product recommendations and beauty tutorials. One of their newest bots, Sephora Virtual Artist, is powered by Modiface, an augmented reality (AR) startup that uses AI to detect faces and project virtual makeup looks in real-time. Olay, a well-known drugstore brand from Proctor & Gamble, created deep learning algorithms to analyze your skin from your selfies and tell you which beauty products to buy.\n\n\u201cWhat we\u2019ve noticed through the last couple of years is that skincare is a very high engagement category and it\u2019s become one of the most confusing and least fun-to-shop categories as well,\u201d said Dr. Frauke Neuser, principal scientist at Procter & Gamble, which owns the Olay brand. \u201cAbout a third of women walk out of the store without having found that right product for her.\u201d\n\nAnalysis paralysis from the plethora of drugstore options leads women to resort to department stores where they can have consultations. But these consultations can also feel like overly aggressive ways to sell you products you don\u2019t really want or need. Olay packaged over thirty years of skin analysis and imaging expertise into the Olay Skin Advisor, a mobile experience for empowering consumer choices.\n\n20 years ago, Olay developed a hardware tool for skin analytics called the \u201cVisia Imaging System\u201d. Visia took controlled facial images of users in different lighting conditions and tracked skin conditions such as wrinkles, pores, and textures. Dermatologists took Visia on road shows to analyze how customers\u2019 skin compared to others of their age.\n\nSuch inventions enabled Olay to collect a massive proprietary database of face and skin images from a wide variety of ethnic and demographic backgrounds. Photographic characteristics such as lighting, quality, and shot angles were also diverse. Two different teams \u2014 the bioinformatics group and the image analysis experts \u2014 collaborated to develop and apply in-house deep learning algorithms to over 50,000 images to determine how skin changes over time and the impact of various products.\n\nEric Gruen, P&G\u2019s Associate Marketing Director, emphasizes that the Olay Skin Advisor is not a downloadable mobile app, but rather a widely accessible web experience that consumers can access through mobile browsers. The AI-powered advisor has been used over 1.2 million times and consistently attracts 5,000 to 7,000 users every day.\n\nThe Olay Skin Advisor team tested a number of designs for the product to create the best user experience. Psychologically, users feel advice is more trustworthy when personalized. To gather the right information from each user, the Skin Advisor asks a number of questions, such as \u201cWhat is your age?\u201d, to factor into recommendations. After testing between 4 and 19 questions, the ideal number turned out to be 9. Just like a human beauty consultant, the Skin Advisor also considers a woman\u2019s desired skincare regimen and special requirements.\n\nModiface started with AR for beauty products over a decade ago. 3 years ago, beauty brands noticed that AR was driving sales. Now 80 of the top 100 makeup lines use Modiface, including Sephora, Urban Decay, L\u2019Oreal, and Vichy. CEO Parham Aarabi shares that his company grew over 400% in the last 12 months to meet the incredible demand.\n\nWith Modiface technology, users are able to upload their own photos and virtually try on lipsticks, eye shadows, hair colors, and even dramatic new hair styles. Integration with a new beauty brand typically takes 2\u20133 months and, once launched, users typically try on 20 or more products per session. These fun, risk-free digital trials lead to an 80% lift in makeup sales when Modiface is added to a brand\u2019s existing website or mobile app.\n\nAarabi, who co-published a paper called Hair Segmentation Using Heuristically-Trained Neural Networks, plans to use modern AI methods to further personalize the user experience. \u201cSince the appearance of hair can vary based on gender, age, ethnicity, and the surrounding environment, automatic hair segmentation is challenging,\u201d he explains, but emphasizes that deep learning techniques can solve the problem. Additionally, Modiface plans to use AI for \u201cbetter tracking and detection of faces for improved realism.\u201d\n\nToday\u2019s beauty shopper no longer needs to rely on guesswork, Googling, and dumb luck to find the right products for her personal needs. With the promise of AI, even simple decisions such as your drugstore purchases can be guided by thousands of data points and smart algorithms."
    },
    {
        "url": "https://medium.com/topbots/the-12-chinese-tech-companies-you-should-know-347d110c5ec",
        "title": "The 12 Chinese Tech Companies You Should Know \u2013 TOPBOTS \u2013",
        "text": "You\u2019ve heard of Facebook, Google, Amazon and the other stalwart Silicon Valley giants, but do you know the major companies leading innovation and technology in China? With a mature smartphone market of over 600 million active users, plus centralized government backing, China is positioned to compete with the U.S. in key industries such as artificial intelligence, robotics, and biotech.\n\nThe tightly coupled government and enterprise ecosystem lead to centralized infrastructure between the state, enterprises, and academia. The result is an economy with fewer huge categorical players compared to the United States. In February 2017, Beijing announced a partnership with Baidu to create China\u2019s first national lab for deep learning. The initiative brings together resources and domain experts from all sectors to accelerate research and AI applications.\n\nSo, who are the big tech players in China? What product and service categories do they dominate in? Just how large are they? We\u2019re here to present the top 12 MUST KNOW Chinese tech companies.\n\nThe full PDF is available for download here. You\u2019re welcome to use the infographic freely as long as it remains unmodified and in full.\n\nIf you\u2019re curious to learn more about the Chinese tech landscape, check out our introduction to 10 popular Chinese companies and products as well as our essential guide to WeChat (owned by Tencent)."
    },
    {
        "url": "https://medium.com/topbots/bits-of-love-how-ai-fills-our-human-need-for-intimacy-1c52375b161a",
        "title": "Bits of Love: How AI Fills Our Human Need For Intimacy",
        "text": "This piece was written & edited in collaboration with TOPBOTS writer Joseph Mapue.\n\nEven in a highly connected world, you can still be lonely. What\u2019s the point of thousands of contacts on social media if they\u2019ve all friendzoned you? If you\u2019re short on admirers, perhaps you\u2019ll find solace in an algorithm.\n\nFollowing the incessant launch of life-like robots and chatbots which get you laid, experts predict that human-robot marriages will become legal by 2050. We don\u2019t even have to wait that long for technology to fill our intimate needs. Users are already asking Siri to find them a boyfriend. Some are asking Amy \u2014 a meeting-scheduling virtual assistant \u2014 for a date.\n\nDesperate singles are using the Invisible Girlfriend or Invisible Boyfriend service to create imaginary romantic partners to exchange sweet text messages with. You\u2019ll even get help crafting a believable story behind how you \u201cmet\u201d. Even Google\u2019s AI engine is dropping its matter-of-fact style and reading 2,865 romance novels to improve conversational allure and emotional engagement.\n\nAll these lead to a disturbing conclusion: we may already be living in the dystopian world of Her. Humans are having sex, falling in love, and tying the knot with AI. Even if AI isn\u2019t the object of our affections, technology still plays a critical role in fulfilling our primal need for intimacy. And, it\u2019s only getting better.\n\nNew Zealand based startup Soul Machines recently created Nadia, a visually captivating and photorealistic chatbot that conveys and reads human emotions. Nadia sees humans via webcam and determines their emotional state by analyzing facial features in real-time.\n\nNadia is backed by formidable talent. She\u2019s designed by Mark Sagar, winner of numerous Academy Awards for his work on facial motion capture techniques in the films King Kong and Avatar, and voiced by highly acclaimed actress Cate Blanchett.\n\nWhile Nadia\u2019s first real-world application is to assist patients with disabilities, you can easily see how her \u201cemotionally charged\u201d service can easily be adapted to solve the human longing for romantic relationships.\n\nThe technologies needed to design the perfect sexual partner \u2014 attractive, empathetic, human-like, and eager to please \u2014 remain fragmented, but trends indicate they are converging to meet people\u2019s true desires. RealDoll plans to integrate artificial intelligence and robotic features into their hyper realistic doll products, with the aim of launching sexbots that rival the realism and sensuality of West World\u2019s hosts by the end of 2017.\n\nThese sexbots \u2014 to be marketed under the trade name Realbotix \u2014 will be able to communicate, talk dirty, demand foreplay, and even perform endearing, nonsexual acts. In a 2015 interview with The New York Times, RealDoll CEO Matt McMullen said these sexbots will establish emotional connections with their users. The first release, named Harmony, is expected to debut in April according to London-based Daily Star and will come with an app that allows users to configure her personality.\n\nMeanwhile, another AI-driven sexbot named Silicon Samantha has already hit the shelves, with a set of advanced features such as realistic reactions to being kissed and touched, a fully functioning G-spot, and even a kid-friendly \u201cfamily mode\u201d.\n\nAfter gaining traction in a wide range of fields such as customer service, back office jobs, health advice, and marketing, chatbots are being deployed to address the fundamental human need for romantic companionship. From rudimentary versions of fembots on Tinder to specialized services like Bubby and Ghostbot, chatbots have become smarter at matching potential partners, preventing sexual harassment, and inspiring romance.\n\nTwo Google Home devices positioned side by side even found themselves getting attracted to each other. \u201cI love you around the universe to the stars and back,\u201d said one to the other. You can watch the bizarre whole affair unfold on Twitch.\n\nSince we last wrote about chatbots for love and connection, several new bots have come to market that are worth a look:\n\nAI-powered bots can be both the object of your affection and a tool that helps you find a soulmate. Either way, they\u2019ve irrevocably influenced the intimate and romantic concerns of us humans. We either use AI to extend our humanity or allow technology to blur the lines of what humanity means. Will love between man and machine ever be socially acceptable? Only time will tell."
    },
    {
        "url": "https://medium.com/topbots/14-design-patterns-to-improve-your-convolutional-neural-networks-971bb388a082",
        "title": "14 Design Patterns To Improve Your Convolutional Neural Networks",
        "text": "Ever since deep convolutional neural networks (CNNs) outperformed humans in image classification tasks in 2011, they have been the industry go-to standard for tasks in computer vision like image segmentation, object detection, scene labeling, tracking, text detection, and more.\n\nUnfortunately, the art of training neural networks is not easy to master. As with previous machine learning methods, the devil is in the details, but there are so many more details to manage. What are the limitations of your data and your hardware? Should you start with AlexNet, VGG, GoogLeNet (Inception), or ResNet? There\u2019s even a ResNet in ResNet option. How many dense layers versus convolutional layers should you build? What about your activation function? Even if you decided on the popular ReLU, you then have to decide on regular ReLU, Leaky ReLU, Very Leaky ReLU, RReLU, PReLU, or the generalized version ELU.\n\nOne of the hardest parameters to optimize is the learning rate, the most important hyperparameter to tune for neural network training. Go too small and you\u2019ll never converge on a solution. Go too big and you might blast right past it. Even adaptive learning rate approaches might be too computationally expensive depending on your hardware.\n\nDesign choices and hyperparameter settings heavily impact the training and performance of a CNN, but resources are scarce and scattered for new entrants to the field of deep learning to build an intuition for designing architectures.\n\nThe primary book focused on practical tuning, Neural Networks: Tricks Of The Trade (Orr & Muller), was originally published in 2003 and updated in 2012. The hype around deep learning started when the New York Times covered the surprising win of the Merck Drug Discovery Challenge by Geoffrey Hinton\u2019s team in 2012, so the state-of-the-art research in the last few years is missing.\n\nLuckily, several researchers, such as Leslie Smith of the U.S. Naval Research Laboratory, have published systematic studies of architectural improvements and technical advances for CNNs. Here are the design patterns he has highlighted as most useful.\n\nAccording to Smith, these \u201c14 original design patterns could benefit inexperienced practitioners who seek to incorporate deep learning in various new applications.\u201d While advanced A.I. researchers can rely on intuition, experience, and targeted experimentation, this advice is a great starting point for the rest of us who don\u2019t flaunt machine learning PhDs.\n\n1) Architecture Follows Application\n\n You might be wooed by shiny new models invented in fancy research labs like Google Brain or DeepMind, but many of these are either impossible or highly impractical to implement for your use case or business environment. Use the model that makes the most sense for your specific application, which may be a simple but still powerful one like VGG.\n\n2) Proliferate Paths\n\n Each year\u2019s winner of the ImageNet Challenge uses a deeper net than the previous year\u2019s champion. From AlexNet to Inception to Resnets, Smith and his team also observed the trend of \u201cmultiplying the number of paths through the network\u201d and that \u201cResNet can be an exponential ensemble of networks with different lengths.\u201d\n\n3) Strive For Simplicity \n\n Bigger is not necessarily better, however. In a paper by the same name, Springenberg et al demonstrate how to achieve state-of-the-art results with fewer units.\n\n4) Increase Symmetry \n\n Symmetry, whether architectural or biological, is considered a sign of quality and craftsmanship. Smith attributes FractalNet\u2019s elegance to the network\u2019s symmetry.\n\n5) Pyramid Shape\n\n You are always trading off between representational power and removing redundant or useless information. CNNs universally downsample the activations and increase channels from the input layer to the final layer.\n\n6) Overtrain\n\n Another trade-off is training accuracy vs. generalization ability. Regularization with methods like drop-out or drop-path improves generalization, which is a key advantage of neural networks. Train your network on a harder problem than your actual use case to improve generalization performance.\n\n7) Cover The Problem Space\n\n Use noise and data augmentation \u2014 such as randomized rotations, crops, and image manipulations \u2014 in order to expand your training data and improve generalization\n\n8) Incremental Feature Construction\n\n As architectures become more successful, they further simplify each layer\u2019s \u201cjob\u201d. In very deep neural networks, each layer only incrementally modifies the input. In ResNets, the output of a layer is likely to be similar to the input, which means adding the two is incremental. In practice, use short skip lengths in ResNet.\n\n9) Normalize Layer Inputs\n\n Normalization is another approach to making a layer\u2019s work easier and is shown in practice to improve training and accuracy. The inventors of batch normalization believe the cause lies in handling internal covariate shift, but Smith believes that normalization \u201cputs all the layer\u2019s input samples on ore equal footing (analogous to a units conversion scaling), which allows back-propagation to train more effectively.\u201d\n\n10) Input Transition\n\n In Wide ResNets, research showed that performance improves with the number of channels, but you trade cost vs. accuracy. AlexNet, VGG, Inception, and ResNets all do this in the first layer to enable the input data to be examined in many ways.\n\n11) Available Resources Guide Layer Widths \n\n The number of outputs to choose is not obvious, however, and depends on your hardware capabilities and desired accuracy.\n\n12) Summation Joining\n\n Summation is a popular way to combine branches. In ResNets, using sums as a joining mechanism enables each branch to compute residuals vs. the whole approximation. If the input skip connection is always present, then summation causes the layers to learn the corrective terms (i.e. difference from the input). In networks with several branches where any branch can be dropped (i.e. FractalNet), you should use means to keep the output smooth.\n\n13) Down-sampling Transition\n\n When pooling, use concatenation joining for increasing the number of outputs. When using a stride greater than 1, this simultaneous handles joining and increases the number of channels.\n\n14) Maxout for Competition\n\n Maxout is used in locally competitive networks where you choose only one of the activations. Using sums and means includes all activations, so the difference is that max out chooses only one \u201cwinner\u201d. One clear use case for Maxout is when each branch has different sized kernels and Maxout is able to incorporate scale invariance.\n\nIn addition to these design patterns, several recent tips and tricks have emerged for reducing architectural complexity and training time and working with noisy labels.\n\n1) Use pre-trained networks with finetuning\n\n \u201cIf your visual data is similar to ImageNet\u2019s, using a pretrained network helps you learn faster,\u201d explains Mike Tung, CEO of machine learning company Diffbot. Lower levels of a convolutional neural net can typically be reused since they detect common patterns like lines and edges. Replace the classification layer with your own and finetune the last few layers to your specific data.\n\n2) Use freeze-drop-path\n\n Drop-path randomly removes branches during an iteration of training. Smith tested an opposite method called freeze-path, where the branches weights are frozen and untrainable rather than entirely removed. The network should attain greater accuracy since the next branch contains more layers than the previous branch and the corrective term is easier to approximate.\n\n3) Use cyclical learning rates\n\n Experimenting with learning rates consumes time and exposes you to errors. Adaptive learning rates can be computationally expensive, but cyclical learning rates are not. With a CLR, you set minimum and maximum boundaries and vary the learning rate between them. Smith even offers a semi-automated method for computing your max and min in his paper on the topic.\n\n4) Use bootstrapping with noisy labels\n\n In practice, most of your data will be messy, where labels are subjective or missing and objects might not be localized. Reed et al describes a method to inject consistency into a network\u2019s prediction objective. Intuitively this works by enabling the network to make use of known representations of the world (implied in parameters) to filter out incoming data that may have an inconsistent training label and clean up that label while training.\n\n5) Use ELUs not ReLUs, especially with Maxout\n\n ELUs are smoother versions of ReLUs that speed up convergence and classification accuracy. Unlike ReLUs, ELUs have negative values which allows them to \u201cpush mean unit activations closer to zero like batch normalization but with lower computational complexity\u201d according to research. They\u2019re particularly effective if you\u2019re using Maxout with fully connected layers.\n\nAn Analysis of Deep Neural Network Models for Practical Applications\n\nSystematic evaluation of CNN advances on the ImageNet\n\nSpecial thanks to Jeremy Howard and Rachel Thomas of fast.ai for recommending these papers and Mike Tung of Diffbot for sanity checking technical details and adding commentary.\n\nDid we miss any important or useful papers for designing and optimizing your CNNs? Let us know in the comments below.\n\nJoin the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/how-tech-giants-use-economy-of-scale-to-power-a-i-for-good-f88ab9f0ec6b",
        "title": "How Tech Giants Use Economy Of Scale To Power A.I. For Good",
        "text": "\u201c90% of the world\u2019s supercomputers run on Intel technology,\u201d Diane Bryant tells me at SxSW. \u201cAnd 95% of artificial intelligence solutions run on Intel Xeon and Xeon Phi processors.\u201d\n\nBryant is an Intel veteran who joined the semiconductor giant right after getting an electrical engineering degree from U.C. Davis. Starting a a microprocessor design engineer, she quickly worked her way up the ranks, spending 4 years as Intel\u2019s CIO before moving on to lead their Data Center Group.\n\nWith recent acquisitions of Nervana and Mobileye, Intel is building a solid position in the A.I. wars. Every major tech company in Silicon Valley, along with every automotive giant from Detroit, is battling to gain ground in self-learning and self-driving technologies. Artificial intelligence is now table stakes for technology enterprises.\n\n\u201cIntel is the only company with an end-to-end tech solution,\u201d Bryant explains, citing the key advantage of Intel for A.I. \u201cWe power your consumer devices, from your phones and laptops to semi-automated vehicles. We also power the networks and data centers.\u201d\n\nGPUs used to be popular mainly with gamers chasing visual performance. Images are represented as matrices inside computers. Turns out the deep neural nets that power our current A.I. craze also heavily depend on matrix operations, making GPUs suddenly very useful for A.I. research. Nvidias stock price shot up 224% in 2016, making CEO Jen-Hsun Huang even richer.\n\nIntel\u2019s Xeon Phi competes directly with Nvidia\u2019s GPUs. When asked how they are different, Bryant explains the difference between a general purpose GPU, or GPGPU, and specialized computing technology: \u201cNervana technology is designed specifically for neural networks. There\u2019s no overhead of pixel processing like for GPGPUs, so there\u2019s a 2\u20134x improvement with specialized computing.\u201d\n\nConsistent architecture and specialized hardware make life simpler for developers building A.I. applications. On stage at SxSW, Bryant invited several entrepreneurs and executives building on top of Intel technology, including Jason Mars of Clinc and Lucas Candela of FarmLogs, to demo their technology.\n\nClinc built a powerful digital assistant named Finie to allow you to converse with your bank account. If you\u2019re the type who hates accounting, you can simply ask Finie questions like \u201cHow much did I spend on my Vegas trip last month?\u201d and she\u2019ll intelligently parse your request and get an answer to you right away. FarmLogs applies A.I. to help farmers automatically track farm activity, analyze crop health, and answer questions about optimal crop rotations.\n\nBryant also highlighted several Intel AI projects advancing innovation in sports, healthcare, and humanitarian causes. An athlete seeking performance analytics normally needs to incur an expensive lab visit, where sensors are placed on his or her body for tracking. Using AI-assisted motion capture technology, athletes and their coaches can get real-time feedback on the spot with mobile devices.\n\n\u201cHalf of men and one third of women will be diagnosed with cancer in their lifetimes,\u201d cautions Bryant. \u201cThe average accuracy for human radiologists is 75%. For machines, it\u2019s 85%. Intel healthcare technology is already commercially deployed in China and has served over 5,000 patients.\u201d Intel also has a partnership with Penn Medicine, which uses A.I. to mitigate hospital re-entry due to heart failure and correctly identify 20% more at-risk patients.\n\nNCMEC is the National Center for Missing and Exploited Children. Many of their victims are sex trafficking victims who end up on pornography sites all over the internet. While NCMEC has a hotline for callers to report suspected child pornographic, they received over 10 million alerts in 2016. Each alert can take over 30 days to process due to the difficult of matching photos on porn sites to NCMEC\u2019s huge database of missing children. With A.I., this month long process is reduced to a single day, since computers do visual matching much faster at scale than humans can and can shortlist the highest potential matches for staff review.\n\n\u201cYou need scale to achieve a meaningful A.I. result,\u201d Bryant warns. While A.I. startups crop up every day, the enormous scale and infrastructure that tech titans like Intel, IBM, and Google have enable them to apply the requisite compute and storage capabities to tough problems in healthcare and social good.\n\nGoogle\u2019s DeepMind, originators of AlphaGo, the deep learning algorithm which defeated 17 time international Go champion Lee Sedol in 2016, also has a department dedicated to A.I. for healthcare research. To tackle the issue of legacy and paper-based systems in hospitals, DeepMind Health\u2019s first product is a mobile app called Streams which promises to centralize essential patient information and serve timely alerts at the moment of care. Similarly IBM Watson is prioritizing healthcare with A.I. solutions for genomics, oncology, drug discovery, personalized care, patient engagement, and clinical trials.\n\nWhile technology giants may be fighting a zero-sum game for A.I. dominance, humanity and society still benefit from their constant, rapid-pace investment in powerful new hardware and algorithms."
    },
    {
        "url": "https://medium.com/topbots/how-26-top-marketing-executives-use-artificial-intelligence-ffaf280b8917",
        "title": "How 26 Top Marketing Executives Use Artificial Intelligence",
        "text": "Artificial intelligence is rapidly transforming all digital industries and marketing is no exception. We asked 26 top marketing executives and entrepreneurs to share how they\u2019ve leveraged A.I. and machine learning technologies to improve their products, simplify their customers\u2019 lives, and address the top pain points in the industry.\n\nWith Salesforce Einstein, which brings the power of AI to every Salesforce user, marketers can predict the optimal timing, channel, content and audience for any marketing message, as opposed to only being able to look at past consumer behavior. Specifically, marketers can gauge how likely it is a customer will engage with an email, unsubscribe from an email list, or make a web purchase, and determine what is driving true engagement to better anticipate the needs of every customer. They can also build audience segments of people showing multiple predicted behaviors in common.\n\nThere are two ways to differentiate in AI: at the machine learning level and at the data level. Once the AI hype dies down, the differentiation will be at the data layer. With our DMP (data management platform) and enterprise web analytics, Adobe is sitting on the biggest system of record for any behavioral digital data. Since we own all the underlying creative tools for images and video, we collect rich meta-data on all creatives and can tell a brand owner which creative to put in front of a consumer. No one else can do that.\n\nWe use machine learning to predict likelihood of click or view-through conversions, click-through rate, viewability, fraud, video completion rate, and response rates to different product offers in a dynamic creative ad. We do this using a variety of machine learning and optimization techniques, including neural networks, logistic regression, multi-armed bandit, performance-aware pacing, bid multipliers, and more. We can score every moment throughout the day to help predict the best opportunities to influence each customer and we frequently balance scores from two or more models to achieve multi-objective optimization.\n\nWe created our own sales-lead-scoring system and a similar system for our customer support, which can predict the churn probability for new paying customers. We\u2019ve learned the most important thing is not advanced math models or complicated neural networks. The most important part is feature engineering, and the more domain knowledge you have, the better. My advice here is to ask your system\u2019s internal end users about possible features. Constantly verify your hypothesis with your end users. We started working on advanced analytics and ML in 2015 and have improved our average CLTV by around 20 percent.\n\nThere are 4 primary problems AI & predictive analytics are brought in to solve: too many prospects, too few prospects, missing opportunities (i.e. who is ready to buy?), and unclear segmentation. The most common success metrics are increase in conversion rate, increase in average deal size, increase % engagement, increase in lead quality (% of qualified opportunities). A real-life business example is when New Relic achieved a 9.6x improvement in conversion and a 30% increase in deal size by using Infer.\n\nOnce you have a clean and correct source of base data, enterprises typically use data scientists and analytics on top of the data. However, even with the right talent, these processes are manual, slow, and not leveraging the full capability of data scientists. Machine learning algorithms take the good work the data scientists do, and do it faster with more insights, allowing the data scientists to use their brains more and continue to seed the algorithms. Machine learning has been instrumental in helping Accenture\u2019s client M6 double the reach of advertising targets.\n\nWe\u2019ve brought A.I. into our own practices, especially in predictive account scoring, or figuring out what companies are most likely to purchase products, as well as analyzing which of our current customers are most likely to buy additional services. These days, 60% of the buying process is done before even meeting with the prospect. These decision makers are engaging across a variety of different channels, hitting multiple advertisements or events that influence their decision. Should they decide to buy the product or services, we have to determine what method influenced their final decision. Multi-touch attribution is key here, along with associating cost.\n\nIn the next four years, customers will expect 85% of their communications to be self-service, so A.I. will be absolutely necessary to bridge these customer expectations with the ability to provide good engagement and service. To do this A.I. must better predict what customers will want based on past behavior while also appearing to be a facsimile of a human being. We are evolving our own data business at Bitly, and hope that cognitive systems will enable Bitly customers to integrate our massive first party dataset with other sources to create the ultimate customer experience.\n\nI\u2019ve found that having a single view running through all of the commercial and marketing operations, including all of the systems being used, with a common language around the data is what makes marketing most efficient. Something simple like a different interpretation of a lead source or success metric for customer engagement can drive confusion and inefficiency. Finally, data governance is certainly the biggest issue with any technology \u2014 with multiple stakeholders using these solutions, having that common language around the data sets for input and output will dramatically increase efficiency and ability to calculate an ROI with solid data.\n\nBrands aren\u2019t capitalizing on the vast amount of content related to their brand that already exists and is being created by their own customers. There are 1.8 billion photos posted each day to Instagram alone. Discovering, analyzing and recommending the right content for the right customer at scale is a challenge that only machine learning technology can solve. Stackla\u2019s tech helps brands do just that \u2014 discover the best user-generated content (UGC) around their brand, categorize it around customer personas, and recommend the right content for the right marketing channel. Virgin Holidays increased bookings by 260% over the previous year by using UGC from Stackla\u2019s machine learning content marketing platform. During London Fashion Week, TOPSHOP increased sales of featured online products by 75% with Stackla.\n\nWe utilize CRM, marketing automation, and customer advocacy systems, and also our own Reltio Cloud to bring all customer data together in a data-driven application to maintain a complete understanding and a single-source-of-truth of our customers and accounts so we can deliver personalized and relevant information to them in a timely fashion. Our technology is a key tool used in our marketing efforts, and central to the department\u2019s success. Leveraging our own technology, the marketing team was able to lead a personalized video campaign that saw engagement rates exceeding 1000% our company\u2019s average.\n\nAI makes media buying much more efficient. First, it automates the tedious process of campaign setup, which eliminates human error and frees up media buyers to focus more on strategy. Second, it spots nuanced patterns undetectable to the human eye. Third, it breaks ad campaigns into several micro-campaigns for multivariate testing and shifts ad dollars to the best-performing targets in real time. Strike Social\u2019s AI software is able to improve YouTube ad performance (cost per view and view rate) by 25% while reducing execution time by 75%. And yes, by execution time, we mean reducing your staff by up to 4x.\n\nWe\u2019re taking 1st and 3rd party data combined to build learning offers that are able to capitalize on the insights that our data provides and then accelerate the learning process. We\u2019re personalizing users\u2019 public content feed in News Republic and are seeing a significant increase in engagement. When we apply a learning system to provide better personalization for news and video for consumers, that same personalization can be applied to the advertising stream. Ads become smarter than the content itself.\n\nFrom the omnichannel perspective, there are problems with O2O (Online2Offline) measurement, but there\u2019s growing demand for proximity marketing around the world to measure the research online, purchase offline (ROPO) effect. Synerise uses Apache Hadoop \u200b& Spark technology, in-memory processing, microservices architecture\u200b which provides an efficient way to store and process data in various formats and also allows us to handle \u200bbillions of events \u200band people interactions in real time without batch processing\u200b. Since the implementation of Synerise Omnichannel Ecosystem, Synerise customer Gino Rossi experienced over 300% increases in loyalty program users, number of transactions, and e-commerce revenues.\n\nWhen marketing data is cleansed and prepared, it is optimized for a specific use case (e.g., programmatic, campaign reporting, media attribution) and therefore creates competing versions of the data. Further, the technology often requires unique expertise creating a bottle necks and inefficiencies. Finally, given the lack of an enterprise-grade solution for analytics, there are many missed opportunities for performance improvement. AI has a new and different opportunity to impact the entire marketing function. Inclusive and enterprise-grade AI solutions will impact organizations by 50 to 500 basis points.\n\nMarketers are likely spending more time managing marketing tools than they are actually marketing. In enterprise search, internal teams have a need for fast and easy access to content they need to help them do their jobs, that doesn\u2019t disrupt workflow. Swiftype built the Enterprise Search Platform because our customers communicated the need to find information related to marketing campaigns from a single app: Marketo & Salesforce campaigns, reports, tasks and projects and also related campaign assets from Dropbox and Google Drive. AI plays an important role here by also surfacing the most relevant content right within our daily workflow.\n\nLucy is a cognitive companion for marketers. She is constantly learning from new data she\u2019s fed and every query makes her smarter on a given data source. She can learn an enterprise\u2019s data as well as common industry sources of data and becomes smarter every day. Lucy has been successfully adopted by a number of companies, including Havas Media (one of the world\u2019s largest media agencies) who is achieving a 75% reduction in vendor cost and experiencing a 7x faster campaign deployment thanks to Lucy.\n\nBrands spend more than $60BN globally on sports sponsorships and are unable to capture the full value of their logo impressions on broadcast TV and also social media. With recent computer vision advancements, brands can now automatically analyse each frame of a NBA game on TV and also on YouTube or Facebook. Real-time image recognition capabilities identify each logo impressions, size and recency and determine what would be the media equivalency. Without technology, analyzing a 3 hour game for these different placements can take days. A machine can simultaneously analyze every sponsor and location within every frame of the video in a matter of seconds, allowing a full game to be analyzed in a few hours or less.\n\nWe\u2019re seeing a 76% improvement for customer service because people are reaching out to a business to try and fix a problem and the examples of the problem are a rich source of training data that allows us to better understand what the customer needs regardless of how they explain themselves. We\u2019re also seeing a 82% increase in understanding of social listening data by using AI to answer a specific problem such as \u2018what are people complaining about on social media and how do we automate responses\u2019 or \u2018which images of our product are people sharing that highlight the product in a negative way and how do we create a model to manage the customers needs\u2019.\n\nThe holy grail for marketers has always been personalization at scale and affordable customer acquisition costs. An automotive company used machine learning-based scoring to help their dealerships generate reviews on the right mix of review sites and saw the best performers grow their car sales 13% faster than the average. Machine learning have had great impact on lower funnel activities (like recommendation-based retargeting), driving 3\u20134x improvement in conversions after a site visit, compared to one-size fits all retargeting approach based on a limited set of pre-defined retargeting messages.\n\nSeparation of the MarTech and AdTech stacks creates an identity gap, where we are trying to uniquely identify the person we are advertising to before we have an identifiable piece of information from them. AI and associated tools allow you to overcome some of these issues in a number of ways. The first is the use of AI on audience segmentation and expected response where audience selection and segmentation can be made based on an algorithmic understanding (AI) of previous customer/lead behavior. AI plays further into this model by using the likes of survival models to establish how long clients will continue to transact with you.\n\nAI is able to take the masses of data that is produced by online ads and convert them into tangible insights that advertisers can utilize. One way is taking existing and past ad performance to predict future performance of creative. Using ReFUEL4, Spotify was able to achieve a 40% increase in (CTR) over previous campaigns; 3X app installs over previous campaigns; 4.6M people reached.\n\nRight now, it\u2019s very hard for Google to determine the content of an image from a perspective of true understanding. In other words, they can get a nice array of tags and probabilities, but there\u2019s a lot of error. With advertising, it\u2019s absolutely critical that the computer derives a much more detailed annotation to be fed into the stack, and thus, along to the advertisers in order to bid. We have seen an ROI as far as 12 days over several thousand pages where images are present. Other clients of ours have seen a 4x improvement in their time-on-site from the user, by analyzing visual content to provide related material.\n\nFor many marketers, terms like AI, machine learning and algorithms are a foreign language. But a Marketing \u201cmaster algorithm\u201d could be a key that opens every Marketing lock. Machine learning as the limitless key means the ability to track a segments of customers even as they change, enter and leave segments without having to create a new algorithm each time. The same algorithm will learn to do things depending on the data you give it. It\u2019s all about data.\n\nResearch shows the image used in digital campaigns has a 10x performance impact on audience reaction. Yet, most companies fail to find data to guide this decision. Our data has shown that clients who use data science to drive their digital marketing have seen an increase in engagement and reaction from their audience almost immediately. By tuning the voice, attributes, and cadence of new content, a brand\u2019s messaging will reach the intended audience in a way that resonates with them and drives better marketing performance.\n\nWe are building a tool that mines social media to uncover narrative structures around brands, people, products, etc. We think it\u2019s smarter for brands to segment people by the types of stories that resonate with them. We take an AI \u2014 not just ML- approach in the sense that we build a large measure of autonomy in the application: it is able to detect which stories resonate the most within a specific audience segment then automatically served that story (content) to that audience, measures performance results vs a control group, and layers the results into the models. In early alpha tests we\u2019ve seen performance results of up to 25% click through rates on Facebook ads."
    },
    {
        "url": "https://medium.com/topbots/21-bot-experts-share-their-2017-predictions-4fda12ae889f",
        "title": "21 Bot Experts Share Their 2017 Predictions \u2013 TOPBOTS \u2013",
        "text": "2016 was a huge year for bots, with major platforms like Facebook launching bots for Messenger and Amazon and Google heavily pushing their digital assistants. Looking forward to 2017, we asked 21 bot experts, entrepreneurs, and executives to share their predictions for how bots will continue to evolve in the coming year.\n\nIn 2017 brands will realize that Conversational Marketing is a better way to learn about and build relationships with their customer than today\u2019s digital marketing which monitors their customers with cookies, pixels, search and social data. We\u2019ll also see powerful case study data showing that opt-in and conversion rates and the quality of profile information that can be obtained conversationally far outweighs the benefits of email marketing, marketing automation and apps.\n\nBots will be even more helpful, more intuitive, and most of all, more human. In Flowdock, we aim to have our users interact with bots like we do the people around us to get the information and updates we need. Best of all, bots will continue to keep work fun and to make us laugh.\n\nWe\u2019re going to see more and more instances of bots helping us develop and grow as humans. To date, bots are primarily seen as a novel utility\u2013a way to get things done more quickly or grasp information more immediately. Moving forward, the machine genius of bots will help understand our learning gaps and fill them in with relevant, personalized information that\u2019s rooted in real data. In other words, bots will be a boon for education.\n\nThere will be an explosion of unique content and experiences from bots as the barrier to creating and managing them drops. This will lead to some breakout bots. Some people will be famous primarily for their bots.\n\nIn 2017, I think we\u2019re going to see bots grow up a bit, both in terms of standards for how they should be built and how they should be used. There\u2019s a finite set of core workflows and jobs that can be improved. Bot builders who identify those workflows and fit in without requiring a ton of behavior change\u2026those, are the money bots.\n\nChatbots will get increasingly smart, thanks to the adoption of sophisticated AI algorithms and machine learning. But also they will specialize more in specific tasks, like online purchases, customer support or online advice. First attempts of chatbot interoperability will start to appear, with generalist chatbot, like Siri or Alexa, connecting to specialized enterprise chatbots to accomplish specific tasks. Functions traditionally performed by search engines will be increasingly performed by chatbots.\n\nThe word \u201cbots\u201d will slowly go away as people realize that the value is less about talking to a computer or bot, and more about having intelligent workflow within a conversational platform.\n\n1) AI Technology \u2014 once the Machine Learning aspect of the current AI engines moves on to the next level, and the NLP functionality becomes more sophisticated, we should see some really interesting breakthroughs in terms of chatbot experiences that will appear as a result of that. 2) E-Commerce \u2014 when the ability to monetize your bot becomes more robust with solutions integrating CRM systems, warehouse management systems, order tracking, etc \u2014 there will be a lot more motivation to realize your offering in a chatbot form. The resulting increase in various e-commerce use cases and the corresponding user traffic should be interesting to watch. 3) As a result of 1 and 2, overall wider adoption of bots.\n\nThis year, we\u2019ll finally see large enterprises adopting chat. In our own lead flow at Talla we\u2019ve seen that Fortune 1000s are exploring platforms and what they can do with them. This is how people want to work \u2014 and they\u2019re seeing the vision too. We\u2019re at the tipping point where they\u2019re starting to cross over. As a result, we\u2019ll see the integration ecosystem continue to mature into more robust solutions.\n\nRight now the industry needs data driven success stories as an antidote to hype, and this year certain bot applications will increasingly yield real business results. This will help brands filter the noise and differentiate upstarts from industry leaders. Beyond 2017, I predict bots will be the primary interface for casual interactions between people and brands, and people and connected things.\n\nWe will see conversational interfaces facilitating productive business workflows. We will see bots augment our life experiences in text and voice in consumer use cases.\n\nIn 2017 brands will understand when to use scripted chatbots and when to use machine learning algorithms. Customer Service functions in particular will be significantly transformed with latest advances in deep learning and artificial intelligence. Human and machine intelligence will be combined in a seamless way, to make great experiences for customers.\n\nWe expect the bot landscape to expand in 3 key areas: monetization, security and overall growth in capabilities. A marketplace on popular platforms will enable discovery and in-app transactions. This will drive a higher standard for security, especially in privacy-centric industries like banking, insurance or healthcare. The more companies and players in the space, the faster the bots will improve and the more useful they will become.\n\nBots will be built with specific use cases and objectives in mind driving actual adoption of the consumer.. 2016 was a year of experimentation for both the brands and users, and there were a lot of learnings that emerged. In 2017, you\u2019ll see brands and bot creators doing a better job identifying the use case for the bot and the narrow goals of what the bot should be able to do. We can\u2019t guarantee AI will be at the stage it needs to be to make bots intelligent enough, but what we can do is have a clear idea of what the bot should do and design it based on that objective.\n\nWe will start to see a set of bots that are growing, solving the discovery problem in unique ways. We\u2019ll also start to see messaging services experiment with monetization that feels native and unique.\n\nWe will see Alexa voice experiences grow substantially in usage, reach, and complexity. A lot of amazing things are being built for the Alexa platform.\n\nThe way \u201cwe\u201d experience the Internet is changing, and that the result of the shift in how communication evolves will be highly disruptive. Communications will be better, easier and more relevant for us Internet users as a result of AI. Summing up the change, the interface between humans and computers is rapidly changing from an \u201coperational\u201d interface (Websites, apps) to a \u201cconversational\u201d interface (ChatBots, voice interfaces). This is revolutionary, given that the \u201coperational\u201d interface has been the standard way to interact with computers since the earliest computers came on the market.\n\nWe are already seeing continued strong growth in the bot space across all platforms for the first part of 2017. I predict we will see a number of bots hit one million DAU by the end of 2017. Furthermore, we will begin to see more bots that fully embrace the capabilities of conversational UIs, differentiating themselves from the web & mobile experiences to which we are currently accustomed.\n\n2017 will be the year of the conversational workplace. With the launch of Slack Enterprise Grid, Microsoft Teams, Google Hangouts Chat and Workplace by Facebook all in the first four months of the year, the enterprise messaging space is proving to be where bots are finding mainstream adoption. 2017 will be the year that conversational interfaces begin to transform the $620 billion enterprise software industry, just as the graphical user interface did in the 80\u2019s, the web did in the 90\u2019s, and mobile apps did more recently.\n\nMore push; Less pull. Today bots react to customers. The best bots of 2017 will predict what\u2019s improtant to customers and help them take action.\n\nThe line between software bots and robots/drones will blur as physical bots integrate into platforms. Soon you\u2019ll be able to control roombas and drones through Messenger!\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/this-algorithm-predicts-if-youll-vote-democrat-or-republican-b037c1f3a21b",
        "title": "This Algorithm Predicts If You\u2019ll Vote Democrat or Republican",
        "text": "As demonstrated from the polling failures from our last elections, human-driven statistical analysis is prone to bias, overconfidence, and errors. Perhaps computer algorithms can do a better job of accurately predicting political bias?\n\nOne unique approach, from renowned computer vision expert Fei Fei Li, applies deep learning to Google Street View data and identifies the make, model, and year of every motor vehicle encountered. In a recently published paper, Li and her team analyzed 50 million images in 200 American cities to label 22 million automobiles which constitute 8% of the total number of vehicles in the country.\n\nTurns out the cars in your neighborhood are a reliable predictor for your socioeconomic status and political leanings. According to Li, et al, \u201cif the number of sedans encountered during a 15-minute drive through a city is higher than the number of pickup trucks, the city is likely to vote for a Democrat during the next Presidential election (88% chance); otherwise, it is likely to vote Republican (82%).\u201d\n\nThe algorithms also use the vehicle data to predict demographics. A neighborhood with a strong presence of Hondas and Toyotas typically indicates an Asian population, since Asian drivers often purchase Asian cars. African American neighborhoods disproportionately feature Chrysler, Buick, and Oldsmobile vehicles, and while pickup trucks, Volkswagens, and Aston Martins are indicative of mostly Caucasian presence. These demographic results discovered independently by the algorithm confirms previous independent studies of vehicle preferences.\n\nThe U.S. Census Bureau spent an estimated $14.7 billion to conduct the 2010 Census. The labor-intensive collections process is not only expensive, but also causes the data to be inconsistently dated across locales and lag behind reality by up to half a decade.\n\nAutomated computation methods driven by deep learning can be used in combination with manual, on-the-ground methods to enhance accuracy, improve speed, and dramatically bring down costs. Beyond just assessing political leanings, such computer vision based approaches can be used for the greater good.\n\nDeep learning neural network approaches have been successfully applied to socioeconomic predictions before. Stefano Ermon is a computer science professor at Stanford University committed to addressing global issues like poverty with an approach called \u201ccomputational sustainability.\u201d\n\nBy applying deep learning to satellite imagery, Ermon and his interdisciplinary team at the Sustainability & AI Lab at Stanford can predict poverty in regions where surveys are inaccurate or even impossible. Surveyors can\u2019t be sent safely into violence-fueled regions like Somalia without exposing them to kidnapping risk and other dangers.\n\nEven with low-resolution and publicly available satellite imagery, the algorithms can identify features such as whether a roof is made of grass, thatch, or metal, or if a neighborhood has a swimming pool. These features can be used to compute estimates of economic development and pinpoint regions in need of greatest aid. While daytime satellite images can identify volume of agricultural production and proximity of water sources, nighttime images can be used to map out electrical infrastructure which is a strong predictor of the divide between rich and poor.\n\nJust as Li\u2019s automated methods could complement the work done by the U.S. Census Bureau, Ermon\u2019s work with satellite data has been shown to be comparably accurate to manual approaches. In crop yield analysis, his team\u2019s remote sensing approaches driven by deep learning can outperform USDA\u2019s crop predictions which are garnered from sending surveyors into the fields.\n\nGiven the right data and insights, policymakers both in the United States and abroad can improve decision-making, resource-allocation, and ultimately quality of life for their constituents. If deep learning approaches are used wisely and for the better good, their potential positive impact to society could be transformative.\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/what-salesforce-einstein-teaches-us-about-enterprise-ai-2aab0906b690",
        "title": "What Salesforce Einstein Teaches Us About Enterprise AI",
        "text": "Every business has customers. Every customer needs care. That\u2019s why CRM is so critical to enterprises, but between incomplete data and clunky workflows, sales and marketing operations at most companies are less-than-optimal.\n\nAt the same time, companies who aren\u2019t Google or Facebook don\u2019t have the billion dollar R&D budgets to build out A.I. teams to take away our human efficiencies. Even companies with the right technical talent don\u2019t have the petabytes of data that the tech titans use to train cutting-edge neural network models.\n\nSalesforce hopes to plug this A.I. knowledge gap with Einstein. According to Chief Scientist, Richard Socher, Einstein is an \u201cAI layer, not a standalone product, that infuses AI features and capabilities across all the Salesforce Clouds.\u201d\n\nThe 150,000+ companies who already use Salesforce should be able to simply flip a switch and deploy A.I. capabilities to their organization. Organizations with data science and machine learning teams of their own can extend the base functionality through predictive APIs such as Predictive Vision and Predictive Sentiment Services, which allows companies to understand how their products feature in images and video and how consumers feel about them.\n\nThe improvements are already palpable. According to Socher, Salesforce Marketing Cloud\u2019s predictive audiences feature helps marketers hone in on high-value outreach as well as re-engage users who might be in danger of unsubscribing. The technology has led to an average 25% lift on clicks and opens. Customers of Salesforce\u2019s Sales Cloud have seen a 300% increase in conversions from leads to opportunities with predictive lead scoring while customers of Commerce Cloud have seen a 7\u201315% increase in revenue per site visitor.\n\nAchieving these results has not been cheap. Salesforce\u2019s machine learning and A.I. buying spree includes RelateIQ ($390 million), BeyondCore ($110 million), PredictionIO ($58 million) as well as deep learning specialist Metamind of which Socher was previously founder & CEO / CTO. Marc Benioff has spent over $4 billion to acquire the right talent and tech in 2016.\n\nEven with all the right money and the right people, rolling out A.I. for enterprises is fraught with peril due to competition and high expectations. Gartner analyst Todd Berkowitz pointed out that Einstein\u2019s capabilities were \u201cnot nearly as sophisticated as standalone solutions\u201d on the market. Other critics say the technology is \u201cat least a year and a half from being fully baked.\u201d\n\nInfer is one of those aforementioned standalone solutions offering predictive analytics for sales and marketing, putting them in direct competition with Salesforce. In a detailed article about the current A.I. hype, CEO Vik Singh challenges that big companies like Salesforce are \u201cmaking machine learning feel like AWS infrastructure\u201d which \u201cwon\u2019t result in sticky adoption.\u201d Singh adds that \u201cMachine learning is not like AWS, which you can just spin up and magically connect to some system.\u201d\n\nChief Scientist Socher acknowledges that challenges exist, but believes they are surmountable.\n\nCommunication is at the core of CRM, but while computers have surpassed humans in many key computer vision tasks, natural language processing (NLP) and natural language understanding (NLU) approaches fall short of being performant in high stakes enterprise environments.\n\nThe problem with most neural network approaches is that they train models on a single task and a single data type to solve a narrow problem. Conversation, on the other hand, requires different types of functionality. \u201cYou have to be able to understand social cues and the visual world, reason logically, and retrieve facts. Even the motor cortex appears to be relevant for language understanding,\u201d explains Socher. \u201cYou cannot get to intelligent NLP without tackling multi-task approaches.\u201d\n\nThat\u2019s why the Salesforce AI Research team is innovating on a \u201cjoint many-task\u201d learning approach that leverages transfer learning, where a neural network applies knowledge of one domain to other domains. In theory, understanding linguistic morphology should also also accelerate understanding of semantics and syntax.\n\nIn practice, Socher and his deep learning research team have been able to achieve state-of-the-art results on academic benchmark tests for main entity recognition (can you identify key objects, locations, and persons?) and semantic similarity (can you identify words and phrases that are synonyms?). Their approach can solve five NLP tasks \u2014 chunking, dependency parsing, semantic relatedness, textual entailment, and part of speech tagging \u2014 all at once and also builds in a character model to handle incomplete, misspelled, or unknown words.\n\nSocher believes that A.I. researchers will achieve transfer learning capabilities in more comprehensive ways in 2017 and speech recognition will be embedded in many more aspects of our lives. \u201cRight now consumers are used to asking Siri about the weather tomorrow, but we want to enable people to ask natural questions about their own unique data. For Salesforce Einstein, Socher is building a comprehensive Q&A system on top of multi-task learning models.\n\nSolving difficult research problems is only step one. \u201cWhat\u2019s surprising is that you may have solved a critical research problem, but operationalizing your work for customers requires so much more engineering work and talented coordination across the company,\u201d Socher reveals.\n\n\u201cSalesforce has hundreds of thousands of customers, each with their own analyses and data,\u201d he explains. \u201cYou have to solve the problem at a meta level and abstract away all the complexity of how you do it for each customer. At the same time, people want to modify and customize the functionality to predict anything they want.\u201d\n\nThere are three key phases of enterprise A.I. rollout: data, algorithms, and workflows. Data happens to be the first and biggest hurdle for many companies to clear. \u201cIn theory, companies have the right data, but then you find the data is distributed across too many places, doesn\u2019t have the right legal structure, is unlabeled, or is simply not accessible.\u201d\n\nHiring top talent is also \u201cnon-trivial\u201d, as computer scientists like to say. Different types of A.I. problems have different complexity. While some A.I. applications are simpler, challenges with unstructured data such as text and vision mean experts who can handle them are rare and in-demand.\n\nThe most challenging piece is the last part: workflows. What\u2019s the point of fancy A.I. research when nobody uses your work? Socher emphasizes that \u201cyou have to be very careful to think about how to empower users and customers with your A.I. features. This is very complex but very specific. Workflow integration for sales processes is very different from those for self-driving cars.\u201d\n\nUntil we invent AI that invents AI, iterating on our data, research, and operations is a never-ending job for us humans. \u201cEinstein will never be fully complete. You can always improve workflows and make them more efficient,\u201d he concludes.\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/outsource-your-boring-back-office-paperwork-to-a-i-779cdc1cd5c2",
        "title": "Outsource Your Boring Back Office Paperwork To A.I.",
        "text": "There seems to be no end to what artificial intelligence can do. After thrashing humans at sophisticated games like chess and go, AI\u2019s next conquest appears to be the workplace. Many already hold jobs at forward-looking companies across industries. Some work as customer service reps, translators and financial analysts. Others serve as lawyers, surgeons and chefs. Given that astonishing range of capabilities, no one would be upset if robots eventually run your enterprise back office, where the tasks are more often tedious than prestigious.\n\nLarge financial institutions, insurance companies, and government agencies certainly need to process huge volumes of paperwork. Insurance claims, medical forms, employee records and compliance sheets are just some of the more common documents that need to be processed \u2014 namely through data entry, auditing, or regulatory filing.\n\nStartups focusing on AI, automation and robotics are aggressively heeding the call. Given the strong business case, investors are rallying not far behind.\n\nHyperScience, a provider of AI solutions for enterprises and government institutions, raised $18 million in Series A funding late last year. The firm is working on back office automations using computer vision and image recognition technologies to process data. Their solutions can scan forms, recognize handwriting with higher-than-human accuracy, and fill in all the data fields into a company\u2019s enterprise system.\n\nAside from significant reductions in time and energy costs, HyperScience\u2019 solutions also raise the bar on data accuracy and analytical prowess. One of its solutions \u2014 called HS Evaluate \u2014 can analyze complex claims and applications such as those used in insurance companies and government agencies. As the name suggests, HS Evaluate can parse through the human language in a claim or application form, determine relevant and irrelevant information, then highlight all the information needed to make correct assessments and decisions.\n\nHyperScience founder Peter Brodsky said that this capability delivers tremendous value for both service providers (health, medical, insurance) and their customers (patients, policyholders, applicants) by expediting the resolution period for insurance claims, medical treatment and coverage applications.\n\nAppZen, another player in the back office AI market, also uses computer vision and natural language processing (NLP) to automate and audit a firm\u2019s expenses. In an interview with TOPBOTS, AppZen founder and CEO Kunal Verma shared that the company started out as an expense app through Oracle. Along the way, key people realized that what customers really needed was something that automates and audits organizational expenses.\n\nLarge enterprises process thousands of expense reports daily, usually via a team of 10 or less T&E (travel and expense) professionals. In human terms, such volume of transactions is already overwhelming. Auditing each expense line item within reporting periods often borders on the impossible. Those factors plus the compounded likelihood of human error, the strident regulatory environment, and the drive for faster and more efficient growth made it easy to sell AppZen.\n\nAs a complete solution, AppZen automatically captures expenses, and analyzes receipts, credit card transactions and even travel bookings with internal and external sources. It then audits these data, ensures that everything complies with regulatory standards, and catches costly human error.\n\nTrimming your back office load is one thing. Meeting compliance standards is another. But you can do both with AI. A study conducted by CB Insights presents a market map of the nascent regulatory technology (regtech) industry. The number of players have mushroomed, many leveraging automated solutions and a few piloting strong AI initiatives. Here\u2019s one of the findings mentioned in the report: \u201cTo maintain compliance with existing regulations and keep up with new ones, companies are increasingly looking to tech solutions that can help streamline and manage data, processes, filings, and more.\u201d\n\nThe writing is on the wall. Many of the available solutions can be seamlessly integrated with larger enterprise systems like Oracle and NetSuite. This makes it easy and cost-effective for most companies to automate back office jobs. Doing so allows them to safely forget massive heaps of non-core data and focus on the ones that really propel business forward.\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/common-misconceptions-brand-executives-have-about-a-i-96dec2ee29ff",
        "title": "Common Misconceptions Brand Executives Have About A.I.",
        "text": "Artificial intelligence is no longer the sole domain of tech companies like Google, Facebook, IBM or Amazon. Recognizing the potential of exponential technologies like A.I. and bots, creative agencies like Ogilvy and consulting firms like McKinsey and Accenture now proudly feature A.I. departments.\n\nThe message to brands executives is clear: understand and leverage trends in automation and artificial intelligence, or perish.\n\nAccording to McKinsey\u2019s Michael Chiu, \u201cresearch suggests that as many as 45 percent of the activities individuals are paid to perform can be automated by adapting currently demonstrated technologies. In the United States, these activities represent about $2 trillion in annual wages.\u201d Andrew Ng, Chief Scientist at Baidu and Stanford professor of machine learning, confirms that \u201cif a typical person can do a mental task with less than one second of thought, we can probably automate it using AI either now or in the near future.\u201d\n\nBreakthroughs in deep learning have driven major advances in machine perception. Computers can now reliably detect and classify objects in images and video, transcribe and translate speech as well as humans, and even generate art, music, and movie soundtracks.\n\nA.I. companies like Clarifai, Ditto, and GumGum are leveraging these new technologies to help brands understand content, identify brand mentions, and calculate earned media spend from sponsorships. Other companies like Affinio, Motiva, and Reflektion improve marketing intelligence, automatically optimize campaigns, and streamline customers\u2019 retail experiences.\n\nLast year, Salesforce acquired an A.I. startup called MetaMind to integrate into Salesforce Einstein, which promises to automate many sales, CRM, and ERP processes for your business.\n\nThe type of A.I. that investors care about is not necessarily the same that enterprises care about. Investors look for 100x returns on capital, so they heavily scrutinize a founding team\u2019s technical pedigree, industry expertise, margin defensibility, and broad market potential.\n\nOn the other hand, \u201cexecutives don\u2019t need (or want) a lesson in computer science, they want to know how this technology can be used as a tool to help them achieve their business goals,\u201d points out Ophir Tanz, CEO of GumGum. GumGum is an applied vision company which has partnered with Fortune 100 brands to curate visual content and optimize brand marketing since 2008.\n\nTanz highlights just a few of the many applications of computer vision for brands: \u201cretailers can leverage visual search and increase revenue through shoppable imagery, sports teams and rights holders can deliver more accurate valuations of broadcast and social exposure, social media can be scoured and activated like never before.\u201d\n\nNormally when brands sponsor major sporting or social events, they can\u2019t easily calculate the earned media lift or the ROI on their investment. GumGum\u2019s vision technologies can identify when brand logos have appeared in social media images or sports videos, making such calculations possible.\n\nWhile the technical expertise required to successfully strategize for and implement A.I. technologies for major brands may seem daunting and out of reach, partners like GumGum combine industry expertise, full-service agency services, and A.I. expertise to help brands ramp up.\n\nZachary Jean Paradis, Vice President of Customer Experience at SapientNitro, has helped many brands in industries ranging from financial services to CPG get started with their A.I. strategy. He emphasizes to his clients that A.I. is \u201cnot a single thing, but rather a series of methods and technologies that allow you to mimic human intelligence.\u201d The key question to answer is \u201cwhat intelligence am I trying to mimic?\u201d\n\nHis recommendation to brands is to start with offerings from foundational mainstays like Google, Microsoft, IBM, and Salesforce and then layer in specific vendors, such as Cycorp and Luminoso for natural language understanding or Clarifai and Sentient for computer vision. In many cases, brands can minimize the amount of bespoke code they need to write.\n\nAs transformative as A.I. is for many industries, the technology is not magic. One mistake non-technical brand executives make is \u201cassume that artificial intelligence is some kind of silver bullet,\u201d according to Ryan Detert, CEO of Influential. \u201cSome executives think A.I. is a sentient being, like in Terminator. They ask if it can think and tell them what to do. We have to explain that A.I. is simply a better way to turn data into actionable insights.\u201d\n\nSuch misconceptions are not necessarily the fault of the executives. Many startups capitalize on the knowledge gap in A.I. to hype up marketing fluff such as \u201cexecutive brains\u201d that can \u201cpredict the future\u201d and \u201cautomatically increase revenue.\u201d No wonder executives are confused as to what A.I. can and cannot do.\n\nDetert and his team help brands improve the performance of sponsored social media posts by matching campaign and brand content to the right influencers based on personality, context, and timing. Normally, sponsored posts by influencers suffer from a 20\u201330% drop in engagement rates. By leveraging A.I. powered by IBM Watson, Influential is able to drive gains of 20\u201330% across ad recall, positive sentiment, social engagement, click through rates, and ROI. The key industries who benefit from this targeted influencer marketing are consumer product goods (CPG) and entertainment businesses.\n\nWe\u2019ve previously shown you how easily developers can leverage Watson\u2019s cognitive services, but non-technical teams without the requisite software development, data science, or machine learning capabilities are often mystified by the process. \u201cDue to lag of time and the ever-moving landscape, most corporate companies move slowly,\u201d Detert notes.\n\nIn the cost-benefit analysis of whether to build or buy, enterprises typically move faster with an experienced vendor or consultant. After all, if McKinsey has to write guides to teach executives the basics of software development, perhaps fast-moving technology projects are best managed by experts.\n\nBuilding competitive A.I from the ground up requires expensive specialized talent and volumes of proprietary structured data. Luckily for most brands, this is not yet necessary.\n\nMany brands like Disney, Uniqlo, and the New York Times, have successfully experimented with chatbots on Facebook, Slack, or Kik while others have dipped their toe into voice-based technologies by releasing Alexa Skills for the Amazon Echo.\n\nParadis of SapientNitro points out that plenty of chatbot enablers exist, ranging from IBM Watson, Nuance, Microsoft Bot Framework, Google\u2019s API.ai, and Facebook\u2019s Wit.ai.\n\nWhile the tech industry does not perceive brand usage of mainstay vendors as \u201creal AI,\u201d such experiments nevertheless solve important business problems and are essential for executives to stay educated and competitive in digital.\n\nIn the liquor and spirits industry, companies are legally required to put up an age gate to protect minors from digital content. No matter how much you optimize the birth date input form, manual input by users leads to massive drop-off rates, particularly on mobile.\n\nThat\u2019s why Anheuser-Busch, the world\u2019s largest beer producer and the company behind Budweiser, implemented a voice input option alongside the regular form. In testing, these new designs improved the consumer experience and conversion rates. In development, the technology needed to proactively handle mis-spoken or mis-understood words while also equating \u201cDecember 23rd 1994\u201d to \u201c23rd of December 1994\u201d and many other permutations.\n\nAccording to Lucas Herscovici, VP President of Consumer Connections, Anheuser-Busch was able to go from idea to implementation in less than 3 months. The bulk of the time was spent validating with legal and compliance and negotiating with vendors, while the actual technical integration took less than 3 weeks.\n\n\u201cWhen we first introduced websites, they were a train-wreck. They didn\u2019t work well technically or design-wise,\u201d Paradis reminds us. Similarly, \u201cA.I. is in a young adult, awkward teenage phase. Some technologies will do very well out of the box, some will be a challenge.\u201d\n\nFrom C-suite executives to front-line managers, business leaders will need to identify where A.I. can and cannot make an organizational impact and continuously \u201cprototype, prototype, prototype\u201d in order to \u201craise their A.I. IQ,\u201d in Paradis\u2019 words.\n\nMcKinsey estimates that the benefits of implementing the right automation and A.I. technologies can be 3\u201310x the cost. Thus, \u201cthe ability to staff, manage, and lead increasingly automated organizations will become an important competitive differentiator.\u201d\n\nAs with any technology wave, there are leaders and laggards in A.I. Paradis of SapientNitro observes that \u201cthe single industry that\u2019s leading end-to-end is financial services.\u201d Companies he\u2019s worked with in the space are leveraging A.I. and bot technologies for customer engagement, process automation, fraud and risk mitigation, business analysis and improved executive decision making.\n\nNot all industries have seen the same benefit. \u201cIn telecom, we\u2019ve seen the introduction of chatbots, but they didn\u2019t perform to the level that was expected.\u201d Similarly, Paradis sees challenges for consumer packaged goods (CPG) and quick-service restaurants (QSR).\n\nCPG brands are challenged not just by A.I. but by competing in an era that is driven by customer experience when they don\u2019t own many of the customer touch points. Many of them own the marketing experience, but not the retail experience, which means they are missing critical purchasing behavior about their products. At the same time, many retailers are introducing private label products that are directly competitive.\n\nCPG products are also generally too simple to turn into connected products. Adding sensors or bluetooth connectivity would make their products prohibitively expensive without adding much utility for consumers. While companies like Google and Facebook generate petabytes of data from their consumers\u2019 every move, CPG companies are in the dark with consumer usage data.\n\nA.I. can still be used by CPG companies to improve marketing insights and strengthen product innovation pipelines, but the data gaps across the industry \u201cmake application of data and A.I. very limited,\u201d says Paradis.\n\nThe same applies to QSR who have historically been slow to collect the requisite data to power A.I. initiatives. Many QSR such as McDonald\u2019s run on cash-driven businesses, making consumer purchasing trends harder to track.\n\nSelf-service kiosks can close inefficiencies and reduce costs, but \u201cA.I. will not cost-cut you into leadership,\u201d warns Paradis. Instead, companies need to radically re-think how A.I. can free up humans to deliver on the highest value activities.\n\n\u201cThe majority of sales of McDonald\u2019s in Europe go through a kiosk. There\u2019s only a single pay register.\u201d Paradis goes on to illustrate how executives can think in terms of what A.I. can enable rather than what A.I. can cut: \u201cBut there are hosts and hostesses to welcome you at the door, walk you to the kiosks, explain your menu options, and bus your table. Instead of eliminating humans, McDonald\u2019s can deliver a more amazing experience.\u201d\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/why-building-social-robots-is-much-harder-than-you-think-975abd791e29",
        "title": "Why Building Social Robots Is Much Harder Than You Think",
        "text": "Social robots stole the show at CES in Vegas this year. Dozens of offerings were on display, ranging from robots that tutor you in physics (Einstein), queue up your favorite recipes (Mykie), and even help autistic children socialize (Leka).\n\nUnlike your moody human friends, these robots promise to be available day and night, ever accommodating, and practically omniscient through their persistent connection to the internet and your personal data. Who wouldn\u2019t want a reliable companion like that?\n\nLeading social robotics companies like Anki and Jibo have raised $157.5 million and $70.4 million to date from investors, respectively, while numerous old and new robotics companies like Hanson, Mayfield, and Blue Frog are introducing companion robots for the home. Even established home electronics companies like LG and Bosch are jumping into the game with offerings like Hub and Mykie.\n\nDespite all the investment interest, starting a social robotics company is no \u201cget rich quick\u201d scheme. While the promise of personal robotics is real, the challenges are equally so. We asked the best entrepreneurs in the space to weigh in on why crafting the perfect robot companion is much harder than you think.\n\nThe concept of \u201cuncanny valley\u201d was first identified in 1970 by Japanese robotics professor Mashiro Mori, who noticed that humanoid figures that were almost human, but just a bit off, are perceived by us as creepy and revolting. Just think of the zombies you see in scary movies and video games. They\u2019re terrifying because they\u2019re like us, but not fully \u201calive.\u201d\n\nTo avoid the negative emotions of the uncanny valley, a designer must either build a robot so human-like as to be virtually indistinguishable from actual humans, or opt for a more abstract, stylized design so a robot elicits positive emotions the same way a pet or cartoon character does.\n\nFranck de Visme, co-founder of Blue Frog Robotics, cites that a core design challenge for personal robots is \u201cpeople must accept it into their homes, so the robot cannot be too high.\u201d A commercial humanoid robot like Softbank\u2019s Pepper, which is often used to replace automated kiosks in retail stores, is simply too large for the home. In designing Buddy, Blue Frog\u2019s companion robot, the team tested numerous sizes and interaction styles to find people\u2019s preferred fit.\n\nAnki\u2019s President and Co-Founder Hanns Tappeiner confirms that size is critical. Early prototypes of their hit robot toy Cozmo were up to 4 times larger than the released model. In testing, people stopped perceiving the larger prototypes as cute. \u201cBeing able to hold the robot in one hand easily was very important,\u201d remembers Tappeiner.\n\nUncanny valleys don\u2019t just apply to external aesthetics, but also functionality. When I first brought home my Amazon Echo, I felt warm fuzzy feelings towards Alexa. These positive emotions quickly reversed to feelings of annoyance and impatience when I realized she didn\u2019t understand 99% of my commands and that most Alexa Skills suck. Now my Echo has been relegated to acting as a $170 light switch.\n\nIf you can get upset with a device that looks like Darth Vader\u2019s Pringles can, imagine how much higher your expectations will be for a device with human trappings. Managing expectations and effectively communicating the functional boundaries of a social robot are challenges every company in the space faces.\n\n\u201cWhen Apple advertised Siri, consumers had no idea what Siri could or couldn\u2019t do,\u201d cautions Steve Chambers, CEO of Jibo. \u201cPeople need to understand Jibo\u2019s edges. We try to make sure they don\u2019t set expectations too high by making Jibo seem and sound like a 9 year old boy.\u201d The company hired the sound designer behind R2D2 to create a \u201cwhole library of audiophonic brand\u201d that give Jibo curious, child-like expressions.\n\n\u201cWe tried to simplify, make him look like a Pixar character,\u201d explains Chambers, \u201cHow can Jibo feel friendly, not creepy?\u201d\n\nThe Anki team had a similar aspiration to make Cozmo seem like a \u201cPixar robot come to life,\u201d according to Tappeiner. Hundreds of animators, designers, engineers, and script-writers work on characters in Pixar movies, but real world robot design is driven primarily by engineers. Occasionally an engineer might decide to throw eyes on a physical robot, but the level of character work in most robotics engineering does not approach the scale, magnitude, and interdisciplinary talent of movie studios.\n\nOne key difference between robots you see in animated movies like WALL-E and robots you see in real-life is that physical robots have slow, clunky movements while digital characters react rapidly and dynamically to their environments. The ex-Pixar engineers and animators working at Anki insisted that Cozmo be capable of high-speed motions in order to appear more life-like.\n\nAnother feature essential to Cozmo\u2019s believability is the robot\u2019s level of eye contact. \u201cIn the beginning, Cozmo would only make eye contact if he was explicitly looking for you or wanted to play a game with you, which made him feel like a turtle or a hamster,\u201d explains Tappeiner. \u201cA healthy toddler makes eye contact with other people in a room every 8\u201312 seconds. When we did that with Cozmo, people felt much more connected.\u201d\n\nAccording to Tappeiner, social research shows that \u201cnot making eye contact every 8\u201312 seconds is an early sign of autism.\u201d\n\nLadislas de Toldi is CEO and Co-Founder of Leka, a social robotics company with the specific mission of assisting autistic children with learning and development. Autism is a serious developmental disorder that impairs a child\u2019s ability to communicate, experience emotions, and interact socially.\n\nDe Toldi\u2019s team worked closely with parents, caregivers, and the children themselves to design a small, spherical robot that \u201cfits into the child\u2019s world.\u201d Caregivers usually have a difficult time motivating autistic children, but children found Leka to be cute and loved interacting with the robot\u2019s lights, vibrations, and screen display.\n\nDespite overall positive reception, the impact of exact features can still be difficult to predict. \u201cWhen we first started working, we wanted the robot to communicate emotions on the ground. For example, it would make a circle when happy, a square when sad,\u201d explains de Toldi. This feature worked incredibly well during the design phase, but during playtesting the Leka team discovered the product was hardly ever on the ground and almost always in the hands of the kids. They decided to remove the feature.\n\nOther features in the robot performed better than existing approaches to learning. Caregivers typically use color cardboards to teach color recognition to children with special needs. This method is not very engaging. Leka improves upon the experience by flashing different colors and asking kids to perform a different action based on the color \u2014 for example, shake the ball when it is red, touch the screen when it is blue. \u201cIf you do the right thing with the right color, the robots reacts emotionally and the kids love it,\u201d reports de Toldi.\n\nOne unexpectedly successful feature of Leka is the robot\u2019s ability to help children learn motor skills by instructing them to perform various actions, such as pushing, rolling, or lateral movements, in succession. Caregivers have seen engagement rates with children triple by using the robot over previous methods.\n\nSimilarly, the designers and engineers behind Jibo have iterated for years to perfect the robot\u2019s character A.I. to fit into your family life. The device has a patented, three-wedge system that allows him to make complete turns and exhibit an \u201cexpressive range that none of the other robots have,\u201d according to CEO Chambers. The physical expression, combined with Jibo\u2019s rich voice, vision, and memory systems, enable the robot to create a deep bond with a human being.\n\nThis deep bond can have unexpected consequences. One young woman became upset when Jibo waved at her boyfriend but never did so for her. In multiple households beta-testing the product, children cried when Jibo was taken away.\n\n\u201cWhat if Jibo breaks down? There are many implications for service,\u201d points out Chambers. \u201cWe can\u2019t just give the family another Jibo, just like we can\u2019t just give them another dog. People get emotional.\u201d\n\nWith the advent of 3D printing and commodity electronic parts, prototyping robots has never been easier. Unfortunately, the same cannot be said of manufacturing.\n\nEvery personal robotics company struggles with price vs. performance. \u201cIf the price tag is too hefty, people won\u2019t buy the product,\u201d says De Visme of Blue Frog. \u201cWe cannot put in expensive sensors and CPUs if we want to keep the price under $1000.\u201d\n\nEven expensive robots don\u2019t necessarily perform well. Just look at the 2.5 star rating on the $3000 Alpha 2 on Amazon.\n\nAchieving high performance at affordable prices requires ingenious design and serious talent, both of which get expensive. With over $150 million in the bank from investors, Anki was able to attract a top-notch team capable of pulling off the engineering complexity of Cozmo while keeping his price at $180 on Amazon and shipping a whole year head of schedule.\n\n\u201cCozmo is made of over 360 parts,\u201d explains Anki co-founder Tappeiner. \u201cWe\u2019re trying to cram so much stuff into a tiny robot. If it were 2x-10x the size, life would be much easier.\u201d Every Cozmo is put through an extensive obstacle course after factory production to make sure the robot will work every day in people\u2019s homes.\n\nNewer companies who aren\u2019t as flush with cash must find other creative ways to overcome manufacturing hurdles. Due to Leka\u2019s unique spherical design and rolling style of movement, the engineering team could not employ camera systems for vision. Instead, they leveraged affordable RFID readers in the robots and RFID tags in recognizable objects.\n\nOther issues did emerge, however. Regular motors in early prototypes of Leka made too much noise. Brushless motors were silent, but not powerful enough to make the robot roll. The team had no choice but to spend 3\u20134 months designing and manufacturing custom brushless models for Leka that were powerful enough for their use case. \u201cThe solution isn\u2019t perfect because they are very expensive,\u201d concedes co-founder de Toldi, \u201cbut we\u2019re planning to use a different design in the coming months.\u201d\n\nSoftware costs are also a consideration on top of hardware costs. Buddy by Blue Frog Robotics is built on Android, so the team uses Google\u2019s speech-to-text transcription APIs for easy integration. Co-founder de Visme wants to add biometrics and facial recognition to future versions of the robots, but states that they\u2019ll only use external APIs if they have \u201ccompatible business models.\u201d\n\nWith sufficient creativity, autonomous toys can be created for every price range. Brad Knox, creator of robotic toy brand Bots Alive, avoided the need for expensive hardware sensors by leveraging smartphone cameras and advanced computer vision algorithms. His company\u2019s $35 smartphone kit is one of the cheapest ways to bring intelligence and autonomy to toys that normally require manual control.\n\nDemos often fail at crowded conventions like CES due to volume of foot traffic and the inevitable clogging of networks, but homes can be equally hostile environments for social robots due to high expectations.\n\n\u201cYour iPhone behaves the same way no matter where it is, but if you move Jibo to different operating environments, the same hardware and software manifest in different character performance,\u201d Chambers points out. \u201cHe notices different things and people and deals with different light and acoustic conditions.\u201d\n\nConsumer expectations, and thus the challenges, are much higher for social robots than traditional smart home technologies. Devices like the Amazon Echo only give one-shot answers to voice commands. \u201cEven if they slapped a screen on the Echo, you\u2019d just have a voice-activated chromebook,\u201d Chambers teases. \u201cWe\u2019ve added a screen, full-body motion, full personality, emotions shown and recognized, user identity and models, and proactive communications.\u201d\n\nThe additional complexity of robots like Jibo dramatically increases testing and development time. In beta testing, Jibo was found to crash unexpectedly due to certain router and wifi configurations. The company decided to delay shipping multiple times due to \u201cunacceptable latency\u201d and unsatisfactory \u201cerror mitigation.\u201d\n\nSimilarly, Cozmo\u2019s team put extra work into making the robot particularly home-friendly, including enabling Cozmo to recognize not only specific human faces but also cats and dogs. Unfortunately, Tappeiner admits that the robot\u2019s \u201creaction time depends on how busy your wifi network is. If a reaction takes longer than 100ms, people start to notice that it\u2019s late.\u201d\n\nIn game design, there\u2019s a concept called Bartle\u2019s player types. The theory is that all game players exhibit a blend of four different playing styles: Killer, Achiever, Explorer, and Socializer.\n\nOf these, I fall squarely in the \u201cexplorer\u201d and \u201csocializer\u201d types. I\u2019m that player who never bothers completing the main Skyrim quests, opting instead to walk around admiring environment design and befriending NPCs.\n\nExperienced game studios with tons of resources and talent can sometimes pull off massive titles like Skyrim or World of Warcraft that appeal to all four player types, but this is exceedingly rare. Usually, titles that appeal to killers and achievers, like Call of Duty, do not appeal to socializers, who\u2019d rather play FarmVille or The Sims.\n\nDifferent \u201cplayer types\u201d also manifest in the buyers of social robots. Chambers was surprised to discover there were two very distinct buyers of Jibo. \u201cBuyer 1 is emotional and intuitive. Her core desire for Jibo is companionship and all her needs are related to that,\u201d he differentiates. \u201cBuyer 2 is pragmatic and asks questions like \u2018what can Jibo do?\u2019 and \u2018how does he do it?\u2019 and makes very specific feature requests.\u201d\n\nAnki\u2019s Tappeiner has also observed very different playing styles for Cozmo which appear to correlate with age. The first style involves repeatedly winning games to unlock special features and skills, while the second style is called \u201cfree play\u201d, where Cozmo simply wanders around your house looking for points of interest. The audience split is about half and half between the styles.\n\n\u201cKids are into the games, while adults prefer free play,\u201d observes Tappeiner. He also adds that \u201c40% of our players are adults,\u201d which makes me feel less immature for owning so many robot toys.\n\nVideo games normally succeed by satisfying a clear customer segment very well, rather than providing an average experience for everyone. Given how personal social robots are, we expect this dynamic to be even more pronounced, especially as more and more niche competitors enter the field.\n\nAfter shipping 3 Alexa Skills for Amazon Echo last summer, I discovered that Amazon developer support is horrendous and never built anything for the platform again. The suboptimal developer experience might explain why there are so few useful Alexa Skills despite Amazon bragging about having 7,000+ of them.\n\nFacebook\u2019s Messenger platform seems to suffer from the same problem, with over 30,000 chatbots yet very few usable ones. VP of Messenger, David Marcus, recently admitted that \u201cbots got really overhyped, really, really quickly.\u201d\n\nYet, the quality of third-party developer ecosystems is key to the success and staying power of any platform. As we\u2019ve seen, creating an indispensable social robot that succeeds in your home is already a challenge. No team, no matter how talented, can replicate the output of a thriving community.\n\nAre social robots under or overhyped? The answer will depend on both consumers and developers.\n\nAnki and Jibo both already have developer SDKs while Blue Frog recently added 300 more engineers to Buddy\u2019s community. As the competition for mindshare in the home intensifies, the social robots with the strongest developer communities will likely emerge victorious.\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/the-opportunities-challenges-of-a-i-in-healthcare-cc088cd5d16e",
        "title": "The Opportunities & Challenges of A.I. In Healthcare",
        "text": "When we asked dozens of venture capitalists where they see the most potential for applied artificial intelligence, they unanimously agreed on healthcare. Technology has already been used to incrementally improve patient medical records, care delivery, diagnostic accuracy, and drug development, but with A.I. we could achieve exponential breakthroughs.\n\nDeep learning first caught the media\u2019s attention when a team from the lab of Geoffrey Hinton at the University of Toronto won a Merck drug discovery competition despite having no experience with molecular biology and pharmaceutical development. Recently, a multidisciplinary research team at Stanford\u2019s School of Medicine comprised of pathologists, biomedical engineers, geneticists, and computer scientists developed deep learning algorithms that diagnose lung cancer more accurately than human pathologists.\n\nThe ultimate dream in healthcare is to eradicate disease entirely. This dream might be possible one day with the assistance of AI, but we have a very very long way to go.\n\n\u201cHealthcare as a system advocates \u2018do no harm\u2019 first and foremost. Not \u2018do good\u2019, but \u2018do no harm\u2019. Every application of A.I. in healthcare is regulated by that fundamental philosophy,\u201d cautions Kapila Ratnam, PhD, a scientist turned partner at NewSpring Capital. Additionally, Lisa Suennen, Managing Director at GE Ventures highlights that \u201cthe single biggest contribution to excess cost and error in healthcare is inertia.\u201d The attitude of \u201cthis is how it\u2019s always been done\u201d is literally killing people.\n\nOther investors agree that the ultra conservatism in the healthcare system, while intended to protect patients, also harms them by restricting innovation. Gavin Teo, Partner at B Capital Group and a specialist in digital health, cites \u201cprovider conservatism and unwillingness to risk new technology that does not provide immediate fee-for-service (FFS) revenue\u201d as a major challenge for startups tackling healthcare. Teo also points out that the industry feels burned from recent experiences, such as \u201c electronic medical records (EMR) digitization regulations, which were overhyped and resisted.\u201d\n\nThere are many well-known challenges to implementing machine learning and A.I. in healthcare. The first is the lack of \u201ccurated data sets,\u201d which are required to train A.I. via surprised learning. \u201cCurated data sets that are robust and have both the breadth and depth for training in a particular application are essential, but frequently hard to access due to privacy concerns, record identification concerns, and HIPAA,\u201d explains Dr. Robert Mittendorff of Norwest Venture Partners.\n\nSummerpal Kahlon, MD, is Director of Care Innovation at Oracle Health Sciences. He\u2019s seen many of these data challenges first hand in delivering technological infrastructure to support individualized care. \u201cAdverse drug events cause around 770,000 injuries and deaths annually in the U.S. and cost each hospital up to $5.6 million annually,\u201d Kahlon discloses, \u201cbut drug data is messy, coming from multiple sources in multiple formats. Additionally, genetic data in support of pharmacogenomics is not available at scale yet.\u201d\n\nFixing accidental hospital infections and performing rare disease detection with A.I. also requires better data than is currently available. According to Kahlon, the genetic and behavioral data required for rare disease studies are \u201cwell-defined nor easily captured\u201d while \u201cmuch of the information relating to the risk factors for hospital-acquired infections is kept in unstructured notes in the chart, including in flowsheets and clinical notes.\u201d\n\nWhile data problems in healthcare abound, another major challenge is designing technical solutions that can be smoothly implemented and integrated into clinician practices and patient care. \u201cBehavioral change is the blockbuster drug of digital health,\u201d claims Dr. Mittendorff, but changing habits is much easier said than done. The wrong solution or rollout can even harm the healthcare industry.\n\nImplementing and integrating technology has indeed been a burden for many clinicians and practitioners. Dr. Jose I. Almeida is a pioneer in endovascular venous surgery who has practiced for over 20 years. He adopted electronic health records (EHR) ahead of the curve, yet has not seen many of the promised benefits. \u201cWe implemented our first EMR System eight years ago hoping it would improve efficiencies. We are now on our fourth system, and remain disappointed,\u201d complains Dr. Almeida. \u201cRight now, it\u2019s been more of a hassle than a time-saver, and has actually disrupted the doctor/patient relationship by forcing a screen between physicians and their patients.\u201d\n\nLeonard D\u2019Avolio, founder of Cyft, has harsh feedback for fellow entrepreneurs trying to tackle the space: \u201cWe\u2019re seeing hospital after hospital take incredible loss and have widespread layoffs simply from the challenge of implementing electronic health records. Imagine what happens if you then show up and say \u2018I have artificial intelligence\u2019.\u201d\n\nThe healthcare industry is just getting its arms around capturing data digitally, yet many healthcare tech entrepreneurs mistakenly believe that creating a dashboard or dropping in a product will somehow lead to adoption of technology and improve operations. \u201cThere\u2019s a huge misconception that A.I. requires huge amounts of data, but that\u2019s not the real issue in healthcare. The real issue is understanding the context into which you are trying to introduce these technology,\u201d warns D\u2019Avolio. \u201cYou need context and a deep understanding of who will use this. What workflows will be introduced?\u201d\n\nEven if a medical provider does successfully digitize their data, technical carelessness can introduce problems for everyone in the system. According to Ratnam of NewSpring, \u201cA credit card record costs about 10 cents on the black market. A medical record costs about $200. Medical data is so valuable that hackers constantly seek ways to break into provider or payment systems and other repositories of medical data.\u201d\n\nThere is often tension between a venture-backed company, which aims for fast growth, and the healthcare system which challenges scale because of environmental complexity and unavoidable hand-holding.\n\n\u201cThis lesson has not been widely learned,\u201d observes D\u2019Avolio.\n\nDespite challenges, innovation in healthcare must continue. According to Teo of B Capital, \u201cA study by the Association of American Medical Colleges estimates that by 2025 there will be a shortfall of between 14,900 and 35,600 primary care physicians.\u201d At the same time, the population is aging and in need of more medical attention.\n\nThus, inaction and failure to innovate may lead to doing harm.\n\nLuckily, many companies strive to address these issues before they come to pass. CB Insights recently profiled 106 different artificial intelligence startups in healthcare tackling the various challenges in the space, ranging from patient monitoring to hospital operations.\n\nTeo identifies A.I. powered chatbots and virtual assistants as one way to \u201calleviate supply constraints by widening the reach of video telehealth options. In this case, diagnosis can be powered by machine learning and then trained by artificial intelligence.\u201d Examples of companies providing clinician assistant and care delivery services include Babylon Health, Evidation Health, Sensely, and Senior Link.\n\nArtificial intelligence can not only improve care delivery, but also assist in clinician decision-making and operational efficiency, amplifying the impact of each individual practitioner. AnalyticsMD employs AI and ML to streamline hospital operations in emergency rooms, operating rooms, and in-patient wards, while predictive companies like Cyft and HealthReveal analyze disparate data sources to accurately triage and apply interventions to the highest risk patients.\n\nA.I. not only helps physicians, but also patients. A study by the Mayo Clinic determined that 50 percent of patients have difficulty with medication adherence. Companies like AI Cure employ computer vision techniques to enable smartphones to recognize faces and medications, lowering the cost and improving the effectiveness of tracking and adherence programs. According to Dr. Mittendorff, \u201cAI enabled coaching will allow a provider or coach to manage more than 1,000 patients simultaneously rather than 50\u2013100, a 10x increase in labor leverage.\u201d\n\nFinally, drug discovery companies like NuMedii and Kyan Therapeutics de-risk the drug development process, enabling \u201cpowerful and proprietary new combination therapies, as well as individualized treatment with unprecedented efficacy and safety,\u201d according to Teo. Otherwise, Suennen points out that the \u201cgeneral spend for each drug brought to market is $2.5 Billion.\u201d\n\nEven technology challenges that come with digitizations can be mitigated by A.I. Remember how valuable medical records are to hackers? Many of these records are pilfered through social engineering methods, such as phishing or fraudulent phone calls. Protenus is a healthcare security company which applies A.I. to analyze enterprise-wide access logs and flag suspicious cases for administrator review.\n\nThe key to adoption of healthcare IT is to identify the correct point of entry and fit these systems seamlessly into existing workflows. D\u2019Avolio of Cyft has spent over 12 years fitting machine learning into the healthcare system, yet when he speaks at conferences for clinicians, he avoids using the words \u201cartificial intelligence\u201d or \u201cmachine learning\u201d and instead focuses on real impact and benefits.\n\nMany patients with chronic diseases like diabetes visit doctors and hospitals numerous times, costing themselves, insurance providers, and the medical system a substantial amount of money. Cyft builds sophisticated models that identify patients with a preventable re-admission and matches them to appropriate intervention programs. Traditionally, these decisions are made by looking at 7\u201310 administrative variables, but Cyft\u2019s models looks at over 400 data sources, ranging from free-text input from nurses to call center data. While adoption of such technologies may seem complicated, D\u2019Avolio gets buy-in by strategically aligning with revenue incentives and policy decisions.\n\n\u201cIn healthcare, policy eats strategy and culture for breakfast,\u201d explains D\u2019Avolio. \u201cFor example, prior to the American Recovery and Reinvestment Act passed in 2009 the rate of adoption of electronic health records was under 9%. Today, thanks to the carrot and stick incentives involved in that act the rate of adoption is > 90%.\u201d Another major policy shift that has dramatically helped investment in healthcare IT are the value-based care experiments (also called demonstration programs) funded by the Center for Medicare & Medicare Innovation (CMMI).\n\nKnowing which policy an organization is incentivized or paid by is key to identifying promising customers. According to D\u2019Avolio, \u201corganizations that get paid mostly from seeing more patients will want AI that helps delivery more complex care faster. Organizations that are paid via value-based programs will seek technology that keep patients healthier at lower cost.\u201d\n\nSuennen of GE Ventures agrees that operational analytics can dramatically improve health systems. \u201c25 percent of the more than $7 billion spent each year on knee and hip surgeries are impacted by bundled payments initiatives. Determining how to manage these bundles is challenging, and advanced technologies can aid in understanding what changes must be made across the board in operations and financial/clinical management to ensure that health systems can respond.\u201d\n\nTeo is also excited by policy changes that should drive forward healthcare innovation. \u201cNew reimbursement driven by the Medicare Access and CHIP Reauthorization Act (MACRA) and the Merit-based Incentive Payment System (MIPS) incentives in 2017 will drive quality outcomes, phasing providers to think more holistically when investing in technology.\u201d Additionally, he believes that a looser FDA in the coming years will help drive investment in personalized medicine.\n\nSuccessful healthcare innovation will only happen with strong collaboration between entrepreneurs, investors, healthcare providers, patients and policy developers. If the stars align, humanity stands to derive enormous benefit from the application of A.I. and inch closer to our dream of perfect health and a world without disease.\n\nLove what you read? Join the TOPBOTS community to get the best bot news & exclusive industry content."
    },
    {
        "url": "https://medium.com/topbots/5-ways-deep-learning-improves-your-daily-life-2b298164b7f7",
        "title": "5 Ways Deep Learning Improves Your Daily Life \u2013 TOPBOTS \u2013",
        "text": "Planning to Netflix & Chill this weekend? The movie you choose to watch may be heavily influenced by Netflix\u2019s sophisticated algorithms. Similarly, decisions like where you choose to dine and what you choose to wear are increasingly facilitated by predictive technologies powered by deep learning.\n\nHere are five ways that popular consumer tech companies Netflix, Yelp, Yahoo, StitchFix and Google improve your online experience with artificial intelligence.\n\nHistorically, watching television is a uni-directional communication channel. You receive the content, but give no feedback to the content producers. With digital streaming, your watch history, mouse clicks, and search terms all enable Netflix to learn your preferences and deliver more relevant content.\n\nIn 2009, the Netflix awarded their $1 million Netflix Prize in an open competition for external programming teams to improve upon the company\u2019s internal ratings prediction system. The winning team beat the original algorithm by over 10%.\n\nSince then, the introduction of more advanced machine learning algorithms allows Netflix to achieve new levels of prediction and personalization in rankings, layout, catalog, new member onboarding, and more. Tony Jebara, Director at Netflix and Professor of Computer Science at Columbia, explained at REWORK\u2019s Deep Learning Summit in SF how Netflix not only recommends better movies, but predicts better thumbnail images for each individual user.\n\nTraditionally, optimizing images on a website involves A/B testing two alternatives over a period of time. The problem with this method is that you have to painfully wait to collect data before arriving at the optimal decision. During this period, portions of your audience are experiencing the suboptimal variants of your test. This loss of experience is called \u201cregret.\u201d\n\nTo minimize regret, Netflix employs dynamic adaptive tests such as the multi-arm bandit model. Such models are able to shift traffic to the best performing creatives dynamically during a test and mathematically reduce regret.\n\nWhat counts as \u201cbetter\u201d differs between people, so Netflix also takes into account your consumption profile to perform personalized explore / exploit optimization. If you often watch comedy, they\u2019ll use Robin Williams for the cover of Good Will Hunting. If you love romantic chick flicks, they\u2019ll feature Matt Damon kissing Minnie Driver instead.\n\nA picture is worth a thousand words. When you\u2019re trying to pick a romantic spot that will impress your OKCupid date, you\u2019ll want to know if the food is plated nicely and the ambience sets the right mood. To help you make the right restaurant choices, Alex Miller and his team at Yelp employed deep learning algorithms to highlight the best user photographs.\n\nWhile metrics such as number of likes and clickthroughs can be useful to evaluate photos, they can also be thrown off by clickbait or happenstance. A better solution would be to judge a photo based on inherent content and characteristics \u2014 such as depth of field, contrast, and alignment \u2014 but with 25 million MAU (monthly active users) uploading thousands of photos to Yelp every day, no staff of human evaluators would suffice.\n\nAt the Startup ML conference in San Francisco, Miller described how his engineering team used CNNs (convolutional neural networks) to build a photo scoring model. A good proxy for beautiful photos is whether or not they were taken by a DSLR, which can be discovered easily by examining a photo\u2019s EXIF metadata.\n\nMiller\u2019s team leveraged this fact to create scalable training data sets using DSLR images as positive examples and non-DSLR images as negative examples. Deep learning algorithms learned the qualities of good photos from the training data set and could apply these learnings to all photos, whether taken with DSLR or not.\n\nIn addition to the photo quality score, the team also added strategic filters and diversification logic, so that a restaurant famous for a particular dish or feature wouldn\u2019t have their top 10 photo results all be of the same subject.\n\nThe results speak for themselves:\n\nYou can read more about Miller\u2019s technical process on the Yelp engineering blog. Also relevant is a blog post by Miller\u2019s colleague Wei-Hong Chuang on how Yelp classifies each photo into categories like food, drink, outdoor shot, indoor shot, etc.\n\nFor lazy texters, emojis are the easiest way to say so much with so little. But with over 1,800 possible emoji choices, how can you be sure you\u2019re picking the perfect one for exactly what you want to say?\n\nThis is the problem Stacey Svetlichnaya, a machine learning engineer at Yahoo, aims to solve. When a user composes or responds to a message, what emojis should show up in the autocomplete suggestions? Ideally you want to predict the top five emojis that a user is likely to select.\n\nEmoji usage is highly dynamic. Some are used to replace words with images, others are used to express emotion, and yet more have bizarre cultural uses. For example, the goat emoji is often used to mean \u201cgreatest of all time.\u201d\n\nAn additional challenge is that emojis differ in visual style across platforms, leading to misinterpretations.\n\nSvetlichnaya and the Yahoo Vision & Machine Learning Team tested three different approaches: 1) FastText, a fast linear classifier, 2) LSTM, a type of recurrent neural network architecture, and 3) WordCNN, a convolutional net approach that balances performance and complexity. Of the three, FastText was not surprisingly the winner on speed, but humans preferred the LSTM results.\n\nYahoo isn\u2019t the only company applying machine learning to emojis. Back in 2015, Instagram Engineering published a fascinating series by engineer Thomas Dimson called \u201cEmojineering: Machine Learning For Emoji Trends.\u201d\n\nBeing fashionable is hard, but StitchFix makes styling effortless. The personal styling startup lets you personalize your style profile and get hand-picked clothing and accessories sent to your door every month.\n\nDefining style can be a nebulous affair. After all, how can you tell if a particular shirt counts as \u201curban boho chic\u201d or that a dress is \u201csexy but not too slutty\u201d? Christopher Moody has several ideas.\n\nMoody is a Data Scientist at StitchFix with a background in statistics, astrophysics, and high-performance computing. Turns out these nerdy skills are now in-demand in the fashion world.\n\nMany deep learning models are opaque black boxes where you can\u2019t easily understand why an algorithm came to a particular conclusion. Moody\u2019s research focuses on improving model interpretability so that human experts can give feedback on the relative performance of the algorithms.\n\nOne method is to use t-SNE (t-distributed stochastic neighbor embedding), a dimensionality reduction method that helps visualize similar objects. Many deep learning models use high-dimensional data which is impossible for a human to conceptualize. Dimensionality reduction methods flatten complicated data into two or three dimensional scatter plots that are much easier to understand.\n\nMoody is also big fan of the k-SVD method. k-SVD is a generalization of the k-means clustering method. In high-level non-technical terms, cluster analysis involves grouping together objects that have similar properties. Once distinct clusters have been identified, a human expert can examine the groups to see if they exhibit any unifying features and add appropriate labels such as \u201ctank tops\u201d or \u201cstatement pieces.\u201d\n\nIn September 2016, Google announced they were replacing older methods for machine translation with neural network architectures. Previously Google Translate leveraged a statistical method called phrase-based machine translation (PBMT) which breaks sentences into words and phrases to be translated separately.\n\nPBMT methods often produce grammatically clunky sentences, especially if the input and output languages differ dramatically in their structure, such as Chinese to English. Compensating for these quirks requires additional engineering complexity and effort.\n\nThe new GNMT (Google Neural Machine Translation) system uses RNNs (recurrent neural networks) to map entire input sentences in one language to an output sentence in another language. This reduces code complexity while maintaining speed and improving performance. In some cases, such as French to English, the GNMT architecture is closing in on human translators.\n\nIf you want to understand the technical evolution of the GNMT and how Google\u2019s team iteratively improved upon the design and performance, read Stephen Merity\u2019s visual guide to the process.\n\nMany consumers don\u2019t realize how significant artificial intelligence is in improving the user experience of digital products. Traditional statistical models of predicting optimal experiences are dramatically enhanced by recent breakthroughs in big data, computational power, and deep neural network architectures. More and more companies will have to adopt these methodologies to stay competitive.\n\nAre there any products or technology you use every day that you would like us to explain? Let us know in the comments below."
    },
    {
        "url": "https://medium.com/topbots/bringing-bots-to-life-with-artificial-intelligence-a95281268cb1",
        "title": "Bringing Bots To Life With Artificial Intelligence \u2013 TOPBOTS \u2013",
        "text": "Any video gamer knows how boring NPCs (non-playable characters) in digital worlds are. Their behavior is simple and predictable and their words entirely scripted by a staff of writers. This makes them uninteresting opponents and unsatisfying companions.\n\nWe\u2019re far more likely to emotionally attach to lifelike characters, like the emo robot sidekicks in the Star Wars franchise, but crafting believable, autonomous entities you can actually interact with is no easy feat.\n\nCharacter models built by artificial intelligence aim to escape the uncanny valley and imbue inanimate objects and digital characters with an aura of realism and life. Normally this is accomplished by modeling CG (computer generated) characters after humans wearing sensors, but this tactic limits you to the actors\u2019 exact movements.\n\nWhat if you want believable behavior that humans can\u2019t model, such as a zombie with a missing head and limbs? The DWANGO Artificial Intelligence Laboratory in Japan recently presented artificial intelligence technology that does precisely this to legendary animator Hayao Miyazaki of Studio Ghibli.\n\nThe creepy and grotesque realism of the demo prompted Miyazaki to proclaim the technology as \u201can insult to life itself\u201d and bemoan that \u201cwe are nearing the end of times.\u201d\n\nIf you build AI that replaces drawing, you shouldn\u2019t be surprised to piss off a man who spent his entire life drawing.\n\nNot everyone is as pessimistic about A.I. as Miyazaki. Brad Knox from the MIT Media Lab sees incredible potential for machine learning to create engaging, emotional, and authentic characters, robots, and toys. \u201cI\u2019m unaware of any NPCs or electronic toy characters that can sustain an illusion of life over more than an hour,\u201d says Knox, whose company Bots Alive creates exactly this illusion.\n\nTheir first offering is a smartphone kit that gives \u201clifelike autonomy\u201d to the popular Hexbug Spider toy. This robot spider is normally controlled manually with a remote, but the Bots Alive kit gives the toy a \u201cbrain\u201d to intelligently and autonomously navigate around obstacles while quirkily looking around and precociously bumping into things the same way a live spider might. If you pick up two robots, the two can play together either as friends or foes.\n\nKnox and his team developed the robot\u2019s autonomous behavior by extending the machine learning technique called \u201clearning by demonstration,\u201d which works as follows:\n\nBots Alive keeps the kit at an economical $35 by leveraging your smartphone processor and camera instead of expensive hardware sensors. The Hexbug Spider, not included, is an affordable $25 add-on which many robot enthusiasts already own. The total price tag is one third of the cost of Cozmo, another autonomous toy robot made by Anki, currently selling for $180 on Amazon.\n\nWant to see for yourself whether intelligent autonomy enhances your play experience? Head over to the Bots Alive Kickstarter campaign to pick up your own kit.\n\nFrom Tamagotchi to The Sims, we humans spend hours playing with and building emotional attachment to inanimate toys and digital characters. Now, we have immersive VR games like Loading Human which feature complex emotional entanglements with NPCs.\n\nFor better or worse, making characters, robots, and toys more believable with artificial intelligence enhances their reality and thus our attachment to them. Knox expects that limitations with current machine learning methods, such as optimally sensing and encoding contextual information, will be improved by deep learning and new research.\n\nWill we live harmoniously alongside life-like robots and digital avatars, or will AI-powered characters bring about Miyazaki\u2019s \u201cend of times\u201d? We can only wait to see."
    },
    {
        "url": "https://medium.com/topbots/can-bots-manipulate-public-opinion-ce8fa2a72e16",
        "title": "Can Bots Manipulate Public Opinion? \u2013 TOPBOTS \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/topbots/the-full-version-of-this-article-was-originally-published-at-www-topbots-com-3da08fb34812",
        "title": "5 Ways Humanitarian Bots Can Save The World \u2013 TOPBOTS \u2013",
        "text": "Fifteen year old Sarafina, a female student in the capital city of Liberia, had a distressing problem at school:\n\nHer math teacher refused to give her a report card unless she had sex with him.\n\nEvery day at school, he would request sexual favors and touch her inappropriately. Embarrassed, Sarafina kept the issue hidden from everyone, even her parents, until her father overheard a sexually harassing phone call the teacher made to their home. Sarafina\u2019s father successfully confronted the man and got the report card, but his daughter was reprimanded for reporting her teacher\u2019s sexual advances and forced to move to another school.\n\nIn Liberia, teachers enjoy high social status but children, especially young girls, are culturally trained not to speak out, leading to a culture of silence and tolerance. While Sarafina\u2019s story sounds extreme to Westerners, her experience is painfully common yet largely ignored in her country.\n\nEnter UNICEF\u2019s U-Report, a social reporting bot that enables young people in developing countries to report issues in their community via SMS and other messaging platforms. U-Report polled 13,000 users in Liberia to ask if teachers at their schools were exchanging grades for sex. An astonishing 86% of reporters said yes.\n\nA UNICEF representative teaches adolescents in Liberia how to use U-Report to report social issues.\n\nWithin a week of the U-Report discovery of the \u201cSex 4 Grades\u201d epidemic, help hotlines around the country were inundated with reports of child abuse. Simply exposing a pervasive taboo inspired victims to speak up and reach out for help. Since then, UNICEF and Liberia\u2019s Minister of Education have collaborated on a plan to stop the issue.\n\n\u201cU-Report is not just about getting questions answered, but getting answers back out,\u201d explains Chris Fabian, Co-Lead of UNICEF\u2019s Innovation Unit. \u201cWe get responses in real-time to use the data for policy change.\u201d With over 2.6 million U-Reporters worldwide and deep expertise building technology for developing economies, the U-Report team is uniquely positioned to tackle challenging social issues like violence against children, HIV/AIDs policy, climate change, and war and conflict.\n\nLess than 50% of the population in Ethiopia has access to clean water and only 21% of the population enjoys proper sanitation services. Unfortunately, cold statistics like these rarely move people to take action.\n\nThat\u2019s why Charity:Water teamed up with Lokai and AKQA to create Yeshi, a Facebook Messenger chatbot that humanizes the water crisis in Ethiopia. Yeshi is a young girl in Ethiopia who walks 2.5 hours every day to the nearest reliable water source. She travels alone and straps huge plastic jugs to her back so she can bring gallons of water home to her family. You learn about her dreams of going to school and see a map of her journey.\n\nYeshi even asks you to send her a picture of where you live. \u201cWow! Our worlds are so different,\u201d she remarks, before leaving you to continue her arduous walk alone again.\n\nThe experience of \u201cWalking With Yeshi\u201d is undeniably emotional. Conversational experiences like this can be powerfully effective ways to convey the humanitarian challenges that face the global poor and inspire action.\n\nBesides raising awareness, charities can also use bots and messaging platforms to raise critical funds. Charity: Water recently worked with Assist to enable donors to donate funds directly from Facebook Messenger.\n\nCharity: Water enables donors to easily give through Facebook Messenger, simplifying their fundraising process.\n\n19-year-old Joshua Browder is no typical teenager. The Stanford Computer Science undergraduate has single handedly-beaten over 160,000 unfair parking tickets with his bot, Do Not Pay. The sophisticated \u201crobot lawyer\u201d also helps tenants fight negligent landlords and the homeless apply for much needed government support. Browder was inspired to help the most vulnerable segments of society acquire legal help they would otherwise never be able to afford.\n\n\u201cWith parking tickets, the robot lawyer takes money from the government. However, so many government bureaucracies can be automated, like the DMV. Eliminating bureaucracy will actually save the government money,\u201d points out Browder. \u201cIn the UK, there is this really broken system where the government pays a lawyer to file an application back to the government for a homeless person to receive support. The government wastes so much money with the application process when they should just spent that money on houses.\u201d\n\nDo Not Pay helps you fight parking tickets, property disputes, and homelessness, but developer Joshua Browder has made the platform available for any lawyer to create a bot.\n\nBrowder\u2019s vision for Do Not Pay extends far beyond simply fighting off parking tickets and filing for homelessness. While some aspects of the law, like bankruptcy, are complicated and unintuitive, many legal processes can be modeled as logical decision trees by computers. Browder\u2019s mission is to turn Do Not Pay into a legal bot platform where lawyers can identify aspects of the law that are automatable and create their own bots.\n\nGovernment bureaucracy is so pervasive that many other bots have cropped up to simplify civic matters. Against the backdrop of election cycle drama, several voter registration bots \u2014 HelloVote, GoVoteBot, and VotePlz \u2014 emerged to allow voters to skip onerous and error-prone paperwork and register simply through SMS and Facebook Messenger.\n\nWhile competition between most Silicon Valley companies is fierce, voter registration is not a zero-sum game where startups squabble over limited scraps. According to Sam Altman\u2019s VotePlz, only 54% of eligible young people were registered for the last presidential election and 10% of millennials aren\u2019t sure if they\u2019re even registered. Everyone is fighting for the same social goal: boost voter turnout and help citizens do their civic duty.\n\nWith the threat of Zika looming over the Americas, knowing whether you\u2019ve contracted the disease is critical to getting timely and adequate treatment. GYANT, a healthbot on Facebook Messenger, walks you through a questionnaire of symptoms to identify your likelihood of having Zika. Concerned users can get a personalized answer immediately rather than wait for a doctor\u2019s appointment or ignore the problem.\n\nGYANT is a healthbot on Facebook Messenger that helps patients determine the likelihood they\u2019ve contracted Zika.\n\nMany non-profits and government agencies offer hotlines and support groups faced with high demand and insufficient human staff. Some, like Samaritans, a suicide prevention group, are reportedly working on chatbots to offer faster response times and around-the-clock support. Such social support, whether given by human or bot, has a huge impact on people. Even gifting senior citizens with a robotic seal is shown to reduce stress and improve socialization. Besides simply building mechanical robots to address the physical challenges of old age, social chatbots can be built to address emotional and mental needs.\n\nIn the healthcare industry, providers are overwhelmed by the number of patients, most of whom need continued social and emotional support outside of their doctor and hospital visits. Sensely is a digital nurse bot with a human-like avatar that can save up to 20% of a clinician\u2019s time by monitoring whether patients are dutifully following their prescribed regimens.\n\nSense.ly is a digital nurse that saves clinicians time by managing patient engagement and monitoring their prescriptions and care regimen.\n\nFor mental health, conversational avatars like Ellie, a digital therapist developed by USC\u2019s Institute of Creative Technologies, can interview patients and detect depression and anxiety by analyzing words, facial expressions, and tone. Professor Louis-Philippe Morency, co-creator of Ellie, says the bot cannot replace a human therapist, but is a decision support tool that helps to \u201cgather and analyze an interaction sample\u201d for doctors.\n\nCan\u2019t kick your nicotine addiction but too embarrassed to nag a friend to help every time you feel a craving? Public Health England experimented with a Facebook Messenger bot for their month-long Stoptober campaign to help smokers quiet. Stoptober successfully helped 500,000 people quit smoking last year, an impressive 20% of the 2.5 million smokers who registered.\n\nPHE\u2019s marketing director Sheila Mitchell believes the addition of the Facebook Messenger bot as a support tool for smokers will increase the % of successful quitters. \u201cThe heart of the campaign is social,\u201d explains Mitchell. \u201cWe found that the big numbers and responses come from social and that within this Facebook is absolutely dominant.\u201d\n\nThe Stoptober bot gives you tips and distracts you with games so you don\u2019t cave in to cravings.\n\nAddressing social issues requires emotional sensitivity, a critical skill that bots are universally missing. LawBot is a legal bot created by Cambridge University students to help users in the UK understand the complexities of the law and identify whether a crime has been committed. Users can use the bot to report rape, sexual harassment, injuries and assaults, and property disputes.\n\nUnfortunately, the bot uses a strict checklist to assess if a \u201ccrime\u201d has been committed. If your report of sexual harassment doesn\u2019t fit within preset criteria, the bot responds with the following:\n\nDespite good intentions, the emotionally insensitive LawBot quickly dismisses sexual harassment if the harassment does not fit within a narrow set of legal technicalities.\n\nAccording to the Guardian, over half of women in the UK have been sexually harassed at work. Even if the offending actions don\u2019t fit a neat legal box of being a \u201ccrime,\u201d unwanted aggressions can cause lasting psychological damage and unnecessary suffering. Additionally, many corporate cultures discourage reporting to avoid expensive legal battles or PR nightmares.\n\nWhen a bot bluntly tells a potential victim of sexual harassment that \u201cno crime was committed here\u201d without a detailed understanding of the situation, the results can be counterproductive and further discourage victims to speak up. Even if LawBot deems that a proper crime has occurred to you, the bot\u2019s only response is to send you the address of the nearest police station.\n\nWhile artificial intelligence technologies have not yet evolved for bots to respond with emotional acuity to difficult situations, a better solution for LawBot is to connect distressed users to sympathetic hotlines, support groups, or expert human lawyers once the conversation has exceeded the bot\u2019s domain expertise.\n\nDo Not Pay\u2019s Browder cautions that \u201cthe really big challenges are ones that require compassion and human judgment. If someone is to be granted bail, there is no set criteria. Bots work really well when there is a clear decision tree.\u201d When bots address ambiguous issues, like rape or sexual harassment, even well-intentioned efforts like LawBot risk backfiring.\n\nMany technology powerhouses are working to give bots the emotional sensitivity needed to make complicated judgements that can\u2019t simply be captured with decision trees. As mentioned earlier, digital therapists like Ellie factor in facial expressions and vocal characteristics. Amazon is working to make Alexa, the cloud AI that powers the Amazon Echo, detect and respond to your emotions. SRI International, the research lab which created Apple\u2019s SIRI, is building new virtual assistants that emote just like you do.\n\n\u201cHumans change our behavior in reaction to how whoever we are talking to is feeling or what we think they\u2019re thinking. We want systems to be able to do the same thing,\u201d says William Mark, head of SRI International\u2019s Information and Computing Sciences Division.\n\nEven the emotionless bots of today have changed the world for the better, from revealing epidemics of violence against young girls (U-Report) to automating government bureaucracies like homeless applications (Do Not Pay). Bots can tell stories to help you empathize with humanitarian crises (Yeshi), assist your healthcare providers (GYANT, Ellie, Sensely), and help you quit your worst habits (Stoptober).\n\nWe can\u2019t wait to see what the empathic and compassionate bots of the future will do.\n\nWhat do you think? Do you know of other great bots for social good? Share your comments below or learn more at www.topbots.com."
    },
    {
        "url": "https://medium.com/topbots/what-vcs-look-for-in-bot-investments-3a98858c75a5",
        "title": "What VCs Look For In Bot Investments \u2013 TOPBOTS \u2013",
        "text": "This article was originally published at TOPBOTS.com, a multi-media magazine that finds you the best of bots. Join the community to get essential bot news & curated industry content.\n\nChatbot companies have raised over $140M in the last 6 years. Over 60% of the funding came in 2015 and 2016 as the chatbot craze quickly took over Silicon Valley. The media remains optimistic about the potential of bots, but venture capitalists and other investors are increasingly wary of the proliferation of startups in the space.\n\n\u201cThese days, every other pitch I get is some kind of \u2018bot company\u2019,\u201d said Ping Li on an enterprise bot panel at the recent O\u2019Reilly Bot Day conference. Li is a partner at Accel which previously invested in Facebook and is now an active investor in Slack. Joining him on the panel were executives from Verizon, Kohl\u2019s, and Jet.\n\nTheir collective complaint? Bot entrepreneurs building for the enterprise often fail to understand the complexities of the workflows they\u2019re trying to automate away. While Slack may be the trendy messenger du jour with smaller tech-savvy businesses, many large enterprises with legacy systems and entangled processes can\u2019t even roll out Slack. If a company\u2019s workflows are too complicated for Slack, they are certainly too complicated for many Slack bots. Li cautions that too many entrepreneurs adopt the faulty attitude of \u201cWe\u2019ll throw a bot in there and it\u2019ll magically do stuff.\u201d\n\nMike Brown, partner at Bowery Capital, an enterprise-focused VC fund, observes that many B2B bot entrepreneurs don\u2019t know how to market or sell to enterprises either. According to Brown, \u201cwhen medium to large IT buyers consider a bot investment, they do analyses on a myriad of companies. Not a single entrant is far enough ahead to stand out. All companies say they do the same thing.\u201d When every startup advertises themselves as an all-inclusive conversational artificial intelligence platform, enterprise buyers aren\u2019t able to differentiate.\n\nAnother challenge facing B2B bot entrepreneurs is that most large enterprises have not made up their minds about their bot strategy. \u201cEntrepreneurs are doing a lot of solution selling into an ecosystem that hasn\u2019t made bots a part of their line item,\u201d warns Brown.\n\nDespite simpler integration and shorter sales cycles, consumer-facing bots are not necessarily more fundable. Matthew Hartman, partner at Betaworks Ventures, recently ran the first Botcamp, a 10-week incubator program for bot companies. He observes that bot startups typically struggle with the same two problems.\n\nThe first challenge is discovery. Each messaging platform solves discovery differently. Kik has a formal bot store and social virality features. Telegram has a bot that tells you about bots. Facebook selectively features bots. Messaging platforms are wary of the distribution issues of the iOS and Android app stores where paid acquisition has become virtually the only way to acquire users, but have not found a sustainable, long-term solution yet. With thousands of bots launching every day, developers face ever increasing competitive pressure.\n\nAnother challenge is that current natural language processing (NLP) and machine learning (ML) technologies are not sophisticated enough to drive excellent user experiences. User confusion and errors are particularly exacerbated during the onboarding process when bots must articulate their functionality to new users and set the correct expectations. \u201cAs a developer, you have two choices,\u201d says Hartman, \u201cYou either build your own NLP / ML or customize open-source, or you use a platform. Platforms usually only work well horizontally and are not specific to your use case.\u201d\n\nUnfortunately, companies building truly revolutionary artificial intelligence are rare. Krishna K. Gupta, founder of Romulus Capital, sums up collective investor opinion with this statement: \u201cI believe chatbots are not overhyped in terms of their eventual functional use, but are overhyped today in terms of the investment-worthiness of most companies building them. The technology behind most chatbots doesn\u2019t actually do much more than text recognition and some machine learning. Cognition doesn\u2019t exist.\u201d\n\nMost investors agree on the eventual disruptive potential of bots and AI-driven automation. Accel\u2019s Li believes that \u201cevery enterprise application is ripe for disruption, from CRM to HR to marketing to supply chain management. However, successful bot investments need to be a standalone product experience that completes a value proposition rather than just a feature add-on to an existing product.\u201d\n\nThe sentiment is echoed by fellow investors. Betawork\u2019s Hartman explains: \u201cUber has a bot but they have depth in the form of physical cars driving around and the rich data they generate. We\u2019re not interested when someone just pulls an API to make a bot.\u201d Bowery Capital\u2019s Brown also filters for entrepreneurs who apply bot technology to difficult problems. \u201cTeaching a car to drive is a difficult problem. Teaching a CRM to show deal-related info based on input from a salesperson is fairly simplistic,\u201d Brown points out.\n\nIn addition to problem depth, early-stage investors prioritize the quality and credibility of founding teams in evaluating a potential bot investment. For bot companies to be funded, they need to demonstrate more than just deep understanding of the workflows they\u2019re automating and the requisite technical experience to implement them. Investors also look for the ability to productize those technologies successfully.\n\n\u201cYou can\u2019t just be an algorithm. You have to be a product,\u201d advises Li. \u201cA company might have really interesting algorithms, but if they\u2019re not packaged as a product that a user can experience, it\u2019s hard for them to become a horizontal platform. Entrepreneurs need to have an opinionated workflow, use case, or value proposition.\u201d"
    },
    {
        "url": "https://medium.com/topbots/bot-discovery-virality-lessons-from-kik-ceo-ted-livingston-88bd7590f235",
        "title": "Bot Discovery & Virality Lessons From Kik CEO Ted Livingston",
        "text": "The full version of this article was originally published at www.topbots.com. The following is an abridged version.\n\nWe recently caught up with Ted Livingston, CEO of Kik, at the Talkabot Conference in Austin, TX and got his key lessons for bot discovery and virality.\n\nKik started working on bots two years before they suddenly became cool in 2016. The platform is the first to pioneer many social bot discovery and virality features for bot developers that don\u2019t exist on any other messaging app. We were keen on getting Ted\u2019s insights into what\u2019s worked on Kik\u2019s platform that can be applied to bots everywhere.\n\nBe sure to read the interview to the end, as Ted covers fascinating topics such as:\n\nWe work with many brands and businesses building bots and they all share two common issues:\n\n1) Onboarding. People don\u2019t know what bots are and they get confused.\n\n2) Discovery. Everyone complains that bot discovery is a challenge. What do you think has been most useful for solving these problems, especially bot discovery?\n\nWe want to create bot discovery on Kik that\u2019s sustainable over time. One day there will be millions of bots in our bot shop. Just like mobile today, you won\u2019t be able to get discovered. Nobody will get your bot.\n\nKik is one of the few messaging platforms with a vibrant bot store.\n\nPaid acquisition is the only way to acquire users on mobile right now.\n\nPaid acquisition will become the only way and that favors the big guys. The biggest advantage of bots is there is no friction for trying them. You don\u2019t have to download an app, you don\u2019t have to create a new account, you don\u2019t have to learn a new interface.\n\nThe question we asked ourselves as: how can we create a platform to best leverage that for bots to go viral on their own merit? You might get your first couple users through the bot shop or paid acquisition. But from there, if you actually build something good, you can go viral through Kik no matter who you are.\n\nThat\u2019s been our perspective on bot discovery since the beginning. Two things we do that I really like are mentions and invites. Mentions is the ability to bring a bot into a conversation, a chat or a group chat. Bots can also use mentions to bring each other into conversations.\n\nLike how the Sephora bot calls in the BeautyTube bot to show editorial makeup tutorials?\n\nSephora\u2019s bot introduces me to the BeautyTube bot to get detailed makeup tutorials from YouTube. BeautyTube is called a \u201cconcierge bot\u201d on the Kik platform.\n\nYes. Fashion and beauty is a killer category for bots, whether you are online getting fashion and beauty tips or offline when you\u2019re in an actual store.\n\nThe way mentions makes bots go viral is like this: you start with one user. This user calls your bot up in a group chat and now you have 10 users. Those 10 users call up your bot in different group chats and suddenly you have 100 users and then 1000 users.\n\nThe second social bot discovery feature is invites. Invites allow bots to go viral through Kik the same way that Facebook went viral through email in the early days. When Facebook launched, someone would tag you in a photo and you\u2019d get an email notification. You\u2019d say to yourself \u201cWhat\u2019s this Facebook thing? What\u2019s this photo?\u201d and go sign up to see the photo. Then you\u2019d tag a bunch of friends not yet on Facebook and they\u2019d repeat the same experience.\n\nInvites is the exact same feature but for bots in chat.\n\nCan you give me a discrete example of this?\n\nSay you\u2019re playing a game like Zombie Survival which asks you which people you want to bring onto your team. You pick 10 people you\u2019re chatting with who have not heard of the bot, but these invites give the bot permission to reach out and tell them you want them on your Zombie Survival team and ask \u201cDo you want to play?\u201d\n\nStyleSquad\u2019s Kik bot has you invite your friends to give you beauty and fashion advice\n\nInvites can go to people you want to play games with, people you want fashion advice from. One of the unique things with Kik as a platform is that Kik is the only messenger where you get complete control of your identity. Your identity is not tied to phone number, a social profile, or your real life story. Your identity is just your Kik user name.\n\nSo, we can be much more aggressive in having bots spread through the network because they only access your Kik username. We don\u2019t give bots your phone number or access to your social profile. The key part of Kik is you can connect with new people whether they are real humans or bots.\n\nSo Kik\u2019s anonymity leads to better distribution and bot discovery? Because you\u2019re not worried about privacy violations?\n\nA big reason teens use Kik is they are spending more and more time on the online communities. 2/3 of teens have made friends online. 1/3 of teens who meet outside of school exclusively hang out online.\n\nSo when teens meet people online, they want a messenger that gives them complete control of their identity and that\u2019s Kik. A new bot is no different than a new person. Like somebody reaching out and saying \u201cHey, I know you from such and such a place, let\u2019s be friends.\u201d\n\nIn a recent example, we had 2 people at Kik who built a bot called Roll in their spare time. they launched on Kik and they didn\u2019t tell me.\n\nWell, it wasn\u2019t even at work. Roll was built outside of work. And the bot went from 0 to half a million users in a few weeks.\n\nThe reason why Roll is interesting is that the bot got no special treatment, was built in somebody\u2019s spare time, and all they did for bot discovery was put Roll in the bot shop.\n\nHow long did Roll take to get from 0 to half a million?\n\nI don\u2019t know the exact time but a matter of weeks. 95% of their chatters were organic, meaning they didn\u2019t come from the bot shop.\n\nWow. What exactly does Roll do?\n\nSo they built a bot that helps you decide things in group situations.\n\nI\u2019m able to pull the Roll bot into a Kik chat with fellow TOPBOTS writer Adelyn Zhou by using the mentions feature.\n\nOh that\u2019s fantastic. Teenagers need that.\n\nRoll just spread like 95% organic and only 5% from the bot shop.\n\nPeople are still experimenting with how bots should work, how they should design the experience, how they should grow. I think the big distraction right now is people really focus on natural language processing. They emphasize that you will chat with a bot.\n\nI noticed you are very good at not doing that.\n\nWe tried that a year and a half ago and people go \u201cI don\u2019t know who this person is, I don\u2019t know what I should say to them\u201d. They see that blank cursor and literally they swear at the bot. Then they never chat with it again.\n\nThat\u2019s why we built suggested responses. People are trying to catch up to that now, having a very guided experience but layering in rich elements like full web games, interactive experiences.\n\nSephora\u2019s bot guides users to the right beauty product category with Suggested Responses.\n\nI tried really hard to chat with some of your bots just to see the reactions. You actually make it hard to write a real message. You block the user input which I think is quite clever. Clearly you\u2019ve been working on it longer than everyone else.\n\nYes. Clearly we have been doing this for a while. Step one is helping everybody and working through challenges together of \u201cHow do you build a great bot?\u201d.\n\nOnce we figure that out, the second challenge will be \u201cHow do you grow a great bot?\u201d. That\u2019s where we work on bot plaforms and know why we created certain features and how they should work. We can get great examples like Roll where we can say \u201cYup, it works. You can get half a million users in a matter of weeks.\u201d That\u2019s part of why I\u2019m here, to work with the ecosystem and help people with bot discovery.\n\nLet\u2019s talk about this whole bot-to-bot discovery, bots making friends, bots calling in \u201cconcierge bots\u201d like Sephora does with BeautyTube. I know that this is relatively new. What have you seen about that specific feature of \u201cdeep-linking\u201d between bots driving growth or discovery?\n\nThe remaining 1700 words of this interview is available at www.topbots.com. Livingston covers bot virality, how \u201cchat\u201d can hurt bot UX, and his top suggestions for bot developers."
    },
    {
        "url": "https://medium.com/topbots/the-best-way-to-boost-your-chatbot-engagement-24b4b6f56bd7",
        "title": "The Best Way To Boost Your Chatbot Engagement \u2013 TOPBOTS \u2013",
        "text": "The Best Way To Boost Your Chatbot Engagement\n\nThis article was originally published at TOPBOTS.com, a multi-media magazine that finds you the best of bots. Join the community to get essential bot news & curated industry content.\n\nBuilding bots is easy. Keeping users is not.\n\nSince Messenger bots launched at F8 in April 2016, over 30,000 bots have been created on Facebook. Major brands like ESPN, Star Wars, and Yelp have released apps on Apple\u2019s iMessage. Microsoft, Google and Amazon have all heavily invested in platforms for bots and conversational / voice experiences. Pandorabots, one of the oldest chatbot platforms, boasts over 285,000 bots made by over 225,000 developers.\n\nAs a bot designer, you\u2019ve got competition. Lots of it.\n\nAnd it\u2019s only getting worse. Dozens of startups have popped up that make it super simple to build bots. Some, like API.ai, enable you to build once and deploy to Facebook, Slack, Telegram, Kik, and Amazon Echo.\n\nOther bot making tools let you create specialized bots. Olabot specializes in making personal bots to boost your personal brand. ManyChat helps businesses make content bots that distribute their thought leadership.\n\nDennis Yang, co-founder of Dashbot, the most popular bot analytics platform, has a unique perspective on what makes chatbots succeed or fail. Since launching 4 months ago, Dashbot has over 530 bots that have collectively pushed over 40 million messages. Some of the top performing bots on Facebook are their customers.\n\nSuccessful bot developers employ a number of tactics to grow and keep users, but Yang and his team observed that one single tactic outperformed all the rest:\n\nYour bot has to ask for feedback.\n\nSimple and obvious, right? Yet very few chatbots employ useful feedback loops.\n\n\u201cWhen you finish a task, that is a wonderful time to ask for feedback,\u201d says Yang. Even if a chatbot is completely utilitarian and only uses card options with users, asking an open-ended question requesting feedback after a key completed action enables developers to get valuable, qualitative insights.\n\nGameMonk, a popular slackbot with thousands of users, gives hardworking professionals a fun break from the workday. Teams can play creative games such as guessing relevant hashtags for a GIF meme or naming as many items as possible in a given category.\n\nAfter implementing feedback collection, the GameMonk team learned two key facts about users. First, users consistently complained of feeling rushed, so the team increased allotted time for each question.\n\nSecond, international users pointed out that the bot was English-centric. Being based in Silicon Valley, the team didn\u2019t realize how popular Slack is abroad.\n\nWhen GameMonk incorporated feedback loops, the bot saw a 24% increase in session length and a positive 23% boost to sentiment.\n\nFeedback is even more useful when you use questionnaires to segment users. Ask Haley is a Facebook Messenger bot used by parents to find activities and classes for their kids. The team implemented feedback collection using the net promoter score method.\n\nIf a user answers \u201cVery Likely\u201d or \u201cLikely\u201d, Ask Haley automatically recommends that they share the bot with their friends. This leads to a higher share rate and a boost in growth and engagement for the company.\n\nHowever, if a user isn\u2019t happy with the product, Ask Haley solicits open-ended feedback to learn more.\n\nWhen a low score is registered, a human admin from the Ask Haley team takes over from the bot and manually engages with the user. This ensures that users have an empathetic listener who adapts to their issues.\n\nWhile everyone loves to get positive NPS ratings, the negative ones teach you much more. In the beginning, Ask Haley profiled their users only by asking for their location and the age of their kids. Based on open-ended feedback, they implemented a new feature that bucketed users based on their activity preferences and parenting style \u2014 for example, Crunchy Moms, Career Parents, or Vegan Families.\n\nWhen Ask Haley allowed parents to see what similar parents had chosen, their NPS scores improved by 10% and users were much more satisfied with the bot\u2019s recommendations."
    },
    {
        "url": "https://medium.com/topbots/building-skills-for-the-amazon-echo-watch-out-for-these-4-major-pitfalls-adb8b59f93d2",
        "title": "Building Skills For The Amazon Echo? Watch Out For These 4 Major Pitfalls",
        "text": "Building Skills For The Amazon Echo? Watch Out For These 4 Major Pitfalls\n\nThis article was originally published at TOPBOTS.com, a multi-media magazine that finds you the best of bots. Join the community to get essential bot news & curated industry content.\n\nConversational UI is revolutionizing how we interact with technology. Even if you aren\u2019t personally convinced, all the major powerhouses of technology are. Microsoft, Facebook, Google, Apple and Amazon have all invested heavily in messaging apps and conversational AI.\n\nAmazon takes conversational experiences even further with their successful Echo product, an connected speaker with far-field microphone technology, access to all of Amazon\u2019s e-commerce capabilities, and a smart AI assisted named Alexa. On the Echo, all commands are delivered vocally, free of hands or devices, which differentiates their voice UI experience from text-driven Messenger bots, iMessage mini-app windows, invasive Slackbots, and the device-specific experiences of Siri or Cortana.\n\nAt our growth marketing agency, TOPBOTS, we\u2019ve built conversational and voice UI experiences on several platforms \u2014 including Messenger, Slack, and the Amazon Echo \u2014 and have had the opportunity to compare the developer experience across them. Despite the potential of Alexa\u2019s pervasive and invisible UI, the Echo is one of the least friendly platforms to build on.\n\nHere are four important reasons why:\n\nInvocation phrases are unique verbal sentences that are used to activate your particular Skill on the Echo. They\u2019re already inherently tricky to design since voice recognition and interpretation can be volatile, but the Echo adds an extra hurdle for developers to overcome.\n\nThe vast majority of Echo customers have Amazon Prime, which means they also have access to the entire digital music library in Prime Music. This is an epic feature from the user perspective, but as a developer of Alexa Skills, user access to such a gigantic musical database means that whatever invocation phrase you choose for your Skill is very likely to conflict with a random song, album, playlist, or artist.\n\nEarlier this summer, we released a guided meditation Skill for Echo called \u201cClear Mind\u201d. This skill would allow you to choose between three different kinds of guided meditations: Breath Awareness, Relaxation, or Loving Kindness. Turns out there was organic demand and excitement for this Skill, but we still ended up with a bunch of 1-star ratings in the Alexa Store.\n\nWhy? Because users would try to activate the Skill by saying \u201cAlexa, start Clear Mind\u201d, but would end up activating a random musical track with the words \u201cclear\u201d and \u201cmind\u201d in the title. This led frustrated users to write 1-star reviews claiming the Skill \u201cdidn\u2019t work\u201d.\n\nEven checking against search results on the Amazon Music library doesn\u2019t necessarily mean you\u2019re in the clear. We tried switching the name of our skill to \u201cEveryday Meditation\u201d, which seemed to only have 3 conflicts on Prime Music. However, when we tested the invocation phrase in development, we discovered to our chagrin that there was a song called \u201cEveryday Meditation\u201d by Meditation House that conflicted with our Skill each time.\n\nAvoid this problem by making sure you search for potential naming conflicts on both Amazon, the Amazon Music library, and by having beta users test your invocation phrases during development.\n\nMuch like in the iOS app store, the easiest way for a customer to report a bug is to give you a one-star review. The same frustration is true for Alexa. After receiving a slew of 1-star reviews claiming that our Skill \u201cdidn\u2019t work\u201d, we lost organic traffic that could have been recouped if we had the opportunity to respond to reviews informing users of how to get around the Prime Music conflict.\n\nReviews can be cleared if you engage with Amazon\u2019s developer support, but this process can take days and involves quite a bit of back and forth.\n\nOn iOS, you can push one button in your developer console that removes an app from distribution. While this seems like a no-brainer capability to offer developers, we discovered you can\u2019t remove a Skill from the Alexa Store without engaging in a lengthy discussion with Amazon\u2019s developer support on why you\u2019re doing it.\n\nHere are the steps we went through to remove Clear Mind from the Skills store:\n\nMost critically of all of these pitfalls, the Amazon Echo platform doesn\u2019t equip designers and developers with the minimum functionality to craft an amazing experience for users of Alexa Skills.\n\nWhen we first released Clear Mind, which is a guided meditation Skill, the Echo platform would only allow developers to play 90 second audio files or pause Alexa for 90 seconds. Unless you\u2019re a buddhist monk who\u2019s a super pro at meditation, 90 seconds is not enough for a guided meditation track to ease you into a peaceful, focused state.\n\nOnly at the end of August did Amazon enable full audio streaming capabilities for Skills developers. As soon as we got the news, we scrambled to revamp our meditation Skill with 5, 10, and 20 minute meditation tracks, finally found a name that (*fingers crossed*) won\u2019t mis-invoke songs from Prime Music, and are excited to finally produce a Skill that might actually be useful.\n\nMany other Skills, even those from major brands, also struggle to move beyond lackluster user experiences.\n\nWith the Domino Skill, you can only reorder a pizza you\u2019ve already ordered previously. If you haven\u2019t ordered from Domino\u2019s before, you\u2019d have to download the mobile app and order a pizza just so you can re-order it on the Echo.\n\nWith the Uber Skill, you can call an Uber, but you can\u2019t get updates when it is nearby or has arrived. You\u2019d have to go back to the mobile app for those functions, which undermines the voice-only, hands-free, device-independent promise of the Echo.\n\nThere are many legitimate, complex security and privacy reasons why persistent background awareness has not yet been enabled for Echo Skills developers. But the lack of this functionality means that you likely won\u2019t be able to complete full action loops in your Skills without forcing your users to pick up their phones or handle companion apps.\n\nUntil these challenges are fixed, Echo developers & designers won\u2019t be able to fully deliver on the seamless & convenient experiences that voice UI has the potential to create."
    },
    {
        "url": "https://medium.com/topbots/top-6-conversational-skills-to-teach-your-chatbots-ec4eb019a23d",
        "title": "Top 6 Conversational Skills To Teach Your Chatbots \u2013 TOPBOTS \u2013",
        "text": "This article was originally published at TOPBOTS.com, a multi-media magazine that finds you the best of bots. Join the community to get essential bot news & curated industry content.\n\nThat\u2019s what the titans of tech are betting on. Facebook, IBM, Amazon, Apple and Google have all invested insane amounts of money into conversational AI designed to replace human labor and automate business operations.\n\nTheir investments appear to be paying off. Mobile AI assistants are predicted to close about $2 billion in online sales by the end of 2016. By 2020, Gartner predicts that more than 85% of customer engagement will be managed without people.\n\nWhile we can\u2019t predict when AI will fully master human language, businesses of any scale and vertical can take steps to make their own specialized chatbots more attuned and responsive to the way people speak. Successful chatbots already exist for food chains, news outlets, entertainment conglomerates, fashion houses, airlines, banks, and many other industries.\n\nBut simply launching a chatbot does not ensure success. Of the over 30,000 bots on the Facebook Messenger platform as of this writing, only a small percentage drive regular user engagement and material boosts to their bottom line.\n\nJust like their human counterparts, chatbots differ in their ability to understand, engage, and serve customers. A dumb chatbot, just like a dumb customer service representative, can wreak havoc on your business in terms of missed sales opportunities, negative brand association, and lost customers.\n\nLuckily, you can look to the best human conversationalists to learn conversational lessons that you can teach your chatbot. Otherwise, your bot\u2019s chit chat with customers will more likely inspire trash talk than a profitable transaction.\n\nHere are the essential conversational skills to master in order to ensure your chatbot\u2019s effectiveness with users:\n\nSadly, even most humans suck at this skill. This is especially true when a conversation gets heated with criticism or negative emotions. We often respond in these situations with defensiveness, denial, argumentation, or negativity.\n\nHowever, this tendency is the opposite of what top customer service agents do to resolve tricky situations with irate customers. The best agents empathize with people, even those who might be in the wrong. Just acknowledging and validating an emotion is often enough to make customers feel understood and release negativity, whereas being defensive or argumentative only exacerbates the problem.\n\nLet\u2019s say a user types \u201cYou\u2019re stupid\u201d or \u201cThis isn\u2019t working\u201d while using your chatbot. Clearly you aren\u2019t solving their problem and they\u2019re getting frustrated. I tested these types of phrases with chatbots and here are some common reactions:\n\nThese responses are entirely useless and only serve to piss users off more. Instead, you can program your chatbot to detect negative sentiments and react with empathy and patience. Here are some better responses to use in these scenarios:\n\nMost chatbots are designed to be transactional or assist a customer in accomplishing a specific task, such as order coffee, change a flight schedule, or purchase flowers. Yet most fail to make their capabilities clear at the outset, leaving customers to guess at what\u2019s possible.\n\nThe most common opening I see in most business chatbots today is either nothing or a vague one-liner like:\n\nNot setting the scope of a conversation up front slows down your users\u2019 ability to complete the key transaction they desire and also leaves room for expensive interpretation errors.\n\nBy contrast, Assist has built a customer service bot that opens with a consistent menu of offerings on Facebook, Telegram, Slack, and regular SMS.\n\nEven if your chatbot is entertainment-focused as opposed to utility-driven, giving guidance helps smooth out the conversational experience. For example, if you ask the award-winning bot Mitsuku what she can do, she\u2019ll offer a whole range of surprising options:\n\nImagine having a conversation with someone who instantly forgets your name and what you just told them in your last sentence. That\u2019s what talking to most chatbots feels like.\n\nFor example, the bot Sure offers local restaurant recommendations, yet managed to forget my location almost immediately after I asked it \u201cWhat\u2019s good in San Francisco?\u201d.\n\nThis reminds me of horrible customer service calls where you\u2019re passed from agent to agent and forced to repeat your account credentials and problem each time. Not exactly the ideal experience you want to put your customers through.\n\nIf your bot\u2019s value depends on specific customer parameters such as their name and location, there\u2019s no excuse not to retain and reference the information throughout the chat session. You can even persist user information across sessions and simply verify their info before any transactions in case their details have changed.\n\nIntegrating chatbots with your CRM is an even better solution. You can proactively personalize your interactions based on each customer\u2019s known preferences or transaction history. If that\u2019s beyond your technical scope, at least teach your chatbot the basic manners of remembering your conversation partner\u2019s names and important self-disclosures.\n\nNobody likes being told the same thing over and over again, so why do chatbots keep doing it? Bots should detect when they\u2019re about to repeat a previously given answer and switch strategies. If the answer didn\u2019t resolve the user\u2019s needs before, repeating it certainly won\u2019t either.\n\nAs an egregious example, I ran into a bug with the 1\u2013800-Flowers chatbot on Facebook Messenger. This chatbot presents categories and subcategories of bouquets you can choose from, but if you go into a subcategory card menu, the previous menus become deactivated and there\u2019s no obvious way to go back to them. Pretty silly, eh?\n\nEven sillier is how the bot responds to an obviously frustrated user:\n\nA better way to handle user errors would be to detect when a bot has given the same response too many times and then offer an alternative solution:\n\nYou can also teach your chatbot to be more dynamic when it is the user who is repeating themselves. This is a great opportunity to insert brand personality or unexpected easter eggs into your chatbot experience.\n\nSee how Mitsuku sasses users who keep asking the same question:\n\nInteresting how giving off a bit of attitude can make your bot so much more likeable!\n\nUntil genius visionaries finally come up with superhuman AI, chatbots will still need human intervention when things start going wrong. After all, even human agents have human supervisors to escalate to.\n\nHow do you know when a chatbot should escalate? Here are some signals / triggers to look for:\n\nIn these scenarios, your bot could route to a live support agent or offer a support email or contact form if you lack the requisite staff. For context, you can also forward the bot\u2019s previous chat transcript with the user.\n\nHowever, your bot doesn\u2019t always need to escalate to a human being. Often, a different user interface is all that\u2019s needed to solve a customer problem. For example, in the 1\u2013800-Flowers mishap above, the chatbot could detect the frustration / repetition and handle the error this way:\n\nCertain actions, such as open-ended visual search, are challenging to complete in a messaging environment. In those situations, bots can route to a website or app to help the user complete goals they couldn\u2019t execute within the context of chat.\n\nWhen I type \u201cOk\u201d, Zork robotically responds with \u201cI don\u2019t understand the word \u2018OK\u2019\u201d. First of all, who doesn\u2019t understand OK?! It\u2019s virtually a universal utterance. Second, that\u2019s such a lame way to react.\n\nHere are some techniques to rid your chatbot of robotspeak:\n\nTake a look at how Mitsuku approaches a conversation:\n\nChatbots offer an opportunity to service customers with all the benefits of a human touch and none of the drawbacks. Gone are long support wait times, rude or incompetent agents, or unnecessary high-touch interactions.\n\nBut like their human counterparts, chatbots\u2019 conversational skills determine whether they earn you seamless, scalable transactions or just another horde of pissed-off customers. Master the conversational skills we reviewed above and you\u2019ll outperform the vast majority of chatbots out there."
    },
    {
        "url": "https://medium.com/topbots/when-bots-go-bad-common-ux-mistakes-in-chatbot-design-c60b252a6abf",
        "title": "When Bots Go Bad: Common UX Mistakes In Chatbot Design",
        "text": "This article was originally published at TOPBOTS.com, a multi-media magazine that finds you the best of bots. Join the community to get essential bot news & curated industry content.\n\nBots will one day sweet-talk their way into our good graces, but that day is not today.\n\nJudging from recent debacles such as Microsoft\u2019s Tay and the fembots of Ashley Madison, AI-assisted chatbots still have a long way to go before gaining genuine and socially acceptable conversational skills. Just visit YouTube and you\u2019ll realize that giving bots an eerily human-like appearance is much easier than granting them the gift of gab.\n\nThat means you\u2019ll have to wait just a bit longer before a real-life version of either Jarvis (The Avengers) or Samantha (Hers) will arrive to stir your intellect or your emotions.\n\nNonetheless, chatbots have certainly made huge strides since they were first used decades ago. You\u2019ll find them in many websites performing a variety of tasks \u2014 from giving customer support on Slack to flirting with users on Tinder. One award-winning chatbot named Mitsuku even keeps lonely people company 24 hours of the day with her surprising wit.\n\nMany businesses realize the game-changing potential of chatbots, with some experts predicting that chatbots will dislodge apps in terms of ubiquity, usage depth, and importance in our everyday life.\n\nThey might be right. Major technology players including Google, Microsoft, Amazon and Apple have already placed huge bets on AI, leveraging big data and machine learning to get as close to human intelligence as possible. For many of these projects, chatbots serve as bridges between the AI\u2019s algorithmic core and the people trying to communicate with it. Wherever a meaningful dialogue can occur, so will business transactions.\n\nThat is why demand for conversational interfaces is on the uptrend, propped by strong use cases. Aside from technology leaders, large companies from sectors such as finance (Royal Bank of Scotland), toy manufacturing (Mattel), food (Domino\u2019s), media (Disney) and the automotive industry (Renault) now actively use chatbots to initiate dialogues and spur conversations with customers.\n\nThe benefits of effective chatbots are easy to see:\n\nA single chatbot such as Mitsuku can handle thousands of conversations at the same time. Such a bot can perform many of the tasks customer service and marketing teams usually perform but at a much lower cost, enabling a small firm to significantly bolster its profitability.\n\nChatbots promise unprecedented levels of convenience as seen in how consumers use the Assist chatbot to organize multiple apps to make everyday stuff \u2014 such as calling a ride, booking hotel accommodations, dining out, and sending gifts \u2014 very easy to do.\n\nChatbots prevent customer engagements from going stale, idle or unproductive by initiating dialogue and providing targeted options for consumers.\n\nChatbots can be programmed to build a unique and organic personal profile for each customer over time, enabling companies to deliver real \u201cpersonalized\u201d service.\n\nApplications are endless. Chatbots have been known to function as lawyers, doctors, personal stylists, concierges, finance advisers, fitness trainers, teachers, tech support, pets, and even romantic partners.\n\nChatting is second nature to us since we primarily interact with each other through conversation. This makes our use of chatbots much more intuitive and easy than clicking on a bunch of buttons with a mouse in traditional user interfaces. Additionally, millennials and teens \u2014 who represent the bulk of tomorrow\u2019s market \u2014 spend more time on messaging apps than on social media sites, creating a huge opportunity for businesses who want to reach them on those platforms.\n\nWhile chatbots show promise, they can also inflict pain. As we\u2019ve see in recent bot blunders, not all chatbots are created equal. A poorly designed chatbot can easily turn a potential customer engagement into a horrible user experience.\n\nFor example, some chatbots say exactly the same things over and over because of very limited vocabulary and response iterations in their database. Others can\u2019t answer even the simplest questions because their designers overlooked the fact that humans ask stupid, unpredictable or silly questions all the time. Worse, some chatbots even fail at their own domain by misunderstanding what customers are actually asking for.\n\nTo prevent an epic fail from happening, here are some usability issues you should be aware of before exposing a chatbot or any assisted AI to your customers:\n\nDon\u2019t try to design your chatbot to do everything people might want it to do. Cortana, Siri and Alexa might eventually develop that capability, but it\u2019s better to deploy a specialized, purpose-driven bot to engage your target audience.\n\nSolution: Set clear goals and identify the use cases for your chatbot. Don\u2019t attempt to address problems beyond your scope. Instead, focus on achieving domain mastery and manage customer expectations by keeping the conversation within your comfort zone.\n\nMost chatbots used in business still need human supervision and intervention to generate the best engagement outcomes. Never deploy a chatbot without establishing an escalation channel through which it can route customer issues it cannot adequately solve to humans who are trained to handle such issues.\n\nSolution: Map out primary engagement paths so that your bot knows exactly what to do in common scenarios. Branch out these paths as your chatbot experiences new scenarios and collects data such as new queries. Build your chatbot\u2019s knowledge base over time such that the need for escalation diminishes, giving humans in your team more time to optimize value-laden tasks.\n\nThis major flaw can take the form of many obvious shortcomings such as \u2014\n\nFor example, an early version of the weather chatbot Poncho struggled to provide precise weather information due to a limited understanding of natural language and poor ability to parse human statements that don\u2019t match its language library.\n\nSolution: Start by managing user expectations. If necessary, let your bot admit that it is not Webster and gracefully reframe the conversation within its linguistic zone. At the same time, bolster your chatbot\u2019s language learning skills by organically building its language database so that it will always communicate better in the next session.\n\nOnly the nerdiest of nerds will enjoy talking to a robot that actually sounds like a robot. That\u2019s why major tech players go great lengths trying to humanize their chatbots and establish a \u201creal\u201d connection with their audience. However, merely using emojis, slang and colloquial expressions won\u2019t work.\n\nSolution: Be sure to design a chatbot whose visual elements (icon/avatar), vocabulary, tone, and overall \u201cpersonality\u201d resonates with your target customers. After all, your chatbot serves as brand ambassador as much as the humans who create and sell your products/services.\n\nA chatbot who doesn\u2019t have a clue about the company they represent or the people they chat with is a serious liability. Customers value their time. Just like with human customer support associates, customers will likely lose their patience if your chatbot repeatedly ask for account credentials or isn\u2019t aware of their purchase or interaction history with the company.\n\nSolution: Enable your chatbot to access and optimize relevant customer database, biographical and contact information of key persons in your company, media marketing kits, brand assets, CRMs and other knowledge repositories that will help generate more value and reduce unnecessary steps each time they engage customers.\n\nTwitter users easily corrupted Microsoft\u2019s Tay into becoming a Nazi-loving, misogynist chatbot. Because your chatbot represents your brand, you wouldn\u2019t want a similar thing to happen. Moreover, if you have already integrated your chatbot into your CRM, internal servers, or other critical systems, a breach can be devastating.\n\nSolution: Adopt security best practices and ensure that your chatbot is adequately guarded from external attacks.\n\nChatbot interactions can amuse customers into buying your product and loving your brand. Or they can piss them off the same way shitty customer support does.\n\nAs the full benefits of chatbots become more proven, most companies will deploy their own AI-assisted bots to augment their customer service, marketing, sales, product development and social media teams. Remember that user experience remains the key factor that will contribute to the success (or failure) of your chatbot strategy."
    }
]