[
    {
        "url": "https://medium.com/@oaklandthinktank/oceanic-city-states-1c0f2d12d48e?source=user_profile---------1----------------",
        "title": "Oceanic City-States \u2013 Anthony Repetto \u2013",
        "text": "Sixty percent of the Earth\u2019s surface is unclaimed territory.\n\nOut past the shores of every country, international waters beckon. What will arise in the midst of the Pacific? How will these new pilgrims relate to the states of old? I suspect that human expansion into open water will reshape society, allowing vast experimentation in government and lifestyle, as well as a natural way to vouch for each technique. We will vote with our feet, migrating between marine metropolises as they suite us.\n\nThe construction of a floating city will be expensive, at first. Materials science continues to improve our options for floatation. Eventually, the added cost of a buoyant home will be less than the cost of land \u2014 at that point, seasteading will be for everyone. Until then, it will be a haven for the elite.\n\nBy residing on a floating island, the rich will be able to renounce their allegiance to any nation, and buck tax collection. Their mobile port-cities will be hubs of business and trade. Regular folk will find abundant occupations, there \u2014 the first foothold of commoners.\n\nEventually, religious minorities, ethnic minorities, corporations, will all have their own abode on the waves. While membership in a defensive federation is still in the interests of the majority of these groups, they will not seek greater federal powers. Each place will have its own laws, culture. And those that best manage our natures will be most successful.\n\nEach city will be its own nation, and the mass of island nations will form a federation which protects their waters. As these city-states populate the seas, they will encrust larger sections of resources with their defense forces, neutralizing the mobility of land-based nations\u2019 navies. The sea cities will manage international trade, and extract minerals from the sea water, rare metals from the ocean crust beneath them.\n\nEach city will be surrounded by a chandelier of buoys, supporting a weave of nets, niches, and struts that acts as a vast, floating habitat \u2014 an artificial reef. They will feed the fish and fertilize the waters for algal growth. With seawater minerals, ore from the crust, and food in the waters around them, island cities will have the capacity to sustain themselves apart from us.\n\nWhere the rich go, so go profits. Islands encrusted with playhouses for the elite will pay well for services. Those bought servants spend their earnings at second-hand shops. Economic activity percolates, and regurgitates back into the system when those shops\u2019 profits go to the rich.\n\nIslands, being tax havens, will offer more and better jobs all down the economic pyramid. Wealth will flow to and accumulate there. There will be a vast disparity between the lifestyle available to islanders and that for land-dwellers. Whole nations will crunch to a stop, swamped by a rust belt. Islands will be centers of automation, as well. (Because wages will be generally higher there, the incentive to automate is greater.)\n\nIf the federation of islands is threatened, perhaps by a land-nation that seeks to mine an island\u2019s valuable oceanic crust, the island federation will have the resources and political connections to wrangle an alliance against their enemy. Like Moore\u2019s Utopians, the islands would be served best by gathering an army of mercenaries from among their neighbors.\n\nThis strategy is what kept every Frieberg safe. These free cities (Frie = free, berg = city) were places where trade went without tariff, and there was no obedience to a lord. The surrounding noblemen coveted the accumulated wealth of these Free Cities. Yet, whenever one nobleman rallied an army to invade the defenseless Frieberg, that free city would notify the surrounding nobles, who would rush to its defense to prevent the advantage that would be gained by the invader. Without an army of its own, Frieberg is kept safe by its neighbors because they all value it highly.\n\nThe Phoenicians set-up port cities all along the Mediterranean. Each city periodically gathered interested parties, who voted with their silver, to enact proposals for the public benefit. Because you could travel to any other port with ease, cities could not benefit from restrictions. Government was elective, not repressive. The island cities will be like the Phoenicians.\n\nWith each city evolving its own solutions, and seeing the public support for those policies by their migration rates, the collection of island nations will be hyper-adaptive. Long before democracies and dictatorships catch on, these islands will alter policy and plans to take advantage of new circumstances. Like a phoenix, the federation will rise higher after each foible, unfazed by any attempts to strike it down."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/unraveling-chaos-db0a58313d5?source=user_profile---------2----------------",
        "title": "Unraveling Chaos \u2013 Anthony Repetto \u2013",
        "text": "I cannot overstate the importance of the recent discovery that artificial neural networks can predict the behavior of chaotic systems out to eight \u2018Lyapunov times\u2019. Imagine, instead of forecasting a week of the world\u2019s weather, you could predict eight weeks in advance. For planning around hurricanes, drought, frost, such weather prediction will be worth tens of billions of dollars each year.\n\nThat won\u2019t be the only benefit of a neural network that understands chaos. Even NASA would have an easier time of things, aided in the discovery of viable flight paths around various planets. The plasma in fusion reactors behaves chaotically, too \u2014 and the difficulty in predicting that plasma\u2019s behavior is exactly why fusion has remained outside our grasp. Most of the world is chaotic, and a machine that can comprehend and predict such chaos is invaluable.\n\nYet, there is a deeper insight to this discovery: we are also neural networks. We, like the researchers\u2019 machine, can observe chaotic systems and predict their behavior far beyond what is possible with standard algorithms. We have, in our own skulls, a shortcut for chaos.\n\nThis upends a tradition stretching back to Norbert Wiener \u2014 the belief that chaotic systems will remain inscrutable, unpredictable. If we, and machines we create, can easily unravel unpredictability, then it is no longer a cacophony. Chaos is tame. By comprehending chaos, the whole world is known like clockwork.\n\nI predict that, when we use these machines to plan possible futures, there will be divergences and convergences. Consider: a system is chaotic if any small change in the system cascades into a large change. Two states which begin close to each other will diverge, resulting in vastly different final states. Yet, there are only so many possible states \u2014 if two states diverge from each other, they each must have grown close to other states. Those others began greatly different from their new neighbors; chaos separates similar states, and it brings disparate states together.\n\nWe could model the paths of many similar states \u2014 beginning with slight variations on a weather model, for example. Those related states would rapidly diverge due to chaotic mixing. However, the weather forms as a result of an arrangement of water and land, each acting as an attractor for certain kinds of behavior. Even though our weather models split along various paths, those paths will tend to reunite in certain places. The paths weave past each other, and overlap at events which are likely in almost all futures. Chaotic systems have a kind of destiny, at times.\n\nTogether, prediction of chaotic systems and the overlap of neighboring paths point to a strange conclusion: we have the power to perceive and predict likely futures, despite their chaotic nature. We see where many paths overlap as destiny, through the fog of chaos that forfeits algorithms."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/holographic-networks-are-preferred-cf2d81b228af?source=user_profile---------3----------------",
        "title": "Holographic Networks are Preferred \u2013 Anthony Repetto \u2013",
        "text": "Much of machine intelligence can be cut into two broad categories: trees and holographs. A decision tree presents a series of choices, A or B, C or D, working its way to the leaves of the tree. Trees categorize, using explicit separations. Holographic networks are different. In an image-classifying neural network, for example, every neuron is involved to varying degrees in the process of classifying images. Dropout is a popular regularizer for training these neural networks, which randomly eliminates neurons. The network, as a result, is highly redundant and information is distributed across the bulk of connections. No one neuron codes for a specific thing.\n\nI argue that the holographic networks are easier to find than decision trees, being thermodyamically preferred among the possible weights of a network. First, some background:\n\nNeural Networks contain both Trees and Holographs\n\nA DenseNet, with connections between every neuron in each layer, can have weights on its neural connections such that it simulates a decision tree. And, weights can be assigned such that the DenseNet has a redundant, distributed classifier \u2014 a holographic memory. Both trees and holographs exist in the space of possible neural synapse weights, the range of possibilities.\n\nImagine that space of neural weights as a landscape, with peaks, ridges, and valleys. The process of initializing the connections of a neural network, and then training the network on a data set, is akin to dropping a boulder somewhere on that landscape and watching where it rolls until it settles in some valley. That valley is the local minimum, the solution to the classification problem.\n\nA decision tree is a solution to the classification problem, so a multitude of valleys on our landscape correspond to decision trees. Similarly, there are many holographic memories available, so numerous valleys are holographs, as well. To understand which of these is thermodynamically preferred, we must consider what the landscape looks like near each of these valleys\u2026\n\nNear each particular valley on our landscape, the surrounding hills may be jagged and steep or smooth and regular. If a small change to the network creates a large change in the result, then that valley is rugged and irregular \u2014 the steep cliffs correspond to the large change in the network\u2019s result. However, if a small change to the network creates an even smaller change in the output, then that valley is smooth and broad.\n\nIt is simple to demonstrate that decision trees are in rugged valleys, while holographic memories are in smooth vales. In a decision tree, if one of the criteria is shifted even a small amount, the reclassified inputs will tend to cluster as large errors in a few outputs. A small change to the decision tree generates a large change in results; the landscape around decision trees is steep and irregular.\n\nMeanwhile, Dropout demonstrates that a large change in a holographic network produces only a small change in results \u2014 Dropout eliminates half of the neurons in the network, and still gives the correct answer! So, many of the changes in that region of the weight space have little or no impact on outputs \u2014 around the holographic valley, the landscape is smooth.\n\nI cannot say which is more common, decision tree valleys or holographs, though I strongly suspect that holographs are radically more abundant among possible neural weights. However, I can show that, because of the ruggedness of decision trees and the smoothness of holographic memories, holographs are much more likely to be an outcome of training. The network\u2019s loss function presses synaptic weights toward holographs naturally, as a kind of entropic state.\n\nWhen a valley is surrounded by steep cliffs, it suffers twice to not be found. First, a steep valley is small, so it is unlikely that a randomly chosen initial weight matrix will land within its domain. Second, when initial weights land along those steep cliffs, the gradient is large and the updated weights are moved far away \u2014 the cliffs bounce the network away from the valley entirely! Decision trees, being steep valleys, are unlikely to be found.\n\nConversely, a smooth valley benefits from exactly the same qualities. A smooth valley is wide, so it is likely that random initialization will land within that valley\u2019s reach. And, the gradient on a smooth valley is small, so updated weights are likely still within that valley. The smoothness of holographic memories makes them likely outcomes of training.\n\nSo, if you had one hundred initializations begin near decision trees, and one hundred near holographs, then the ones near holographs would settle into that holograph\u2019s valley, while those near decision trees would bounce away. All else being equal, holographic memories will be the product of training networks almost every time.\n\nA decision tree parses each branch with a separate discriminator. Branch A may split into C and D, while branch B splits into E and F. Each split is handled by a distinct feature. So, if a decision tree has 7 binary splits, it can best arrange them into a tree with three layers and four final bifurcations, encoding eight possible outcomes.\n\nMeanwhile, a holographic memory utilizes each neuron for every classification. A neuron which distinguishes between pugs and collies may also distinguish between corvettes and civics. That\u2019s equivalent to a tree where each layer of bifurcation is handled by a single neuron. Branch A and B are both followed by the same neuron, which distinguishes both C from D and E from F. Seven distinctions would correspond to seven layers of bifurcation \u2014 that\u2019s 2 to the 7th power, or 128 encoded outcomes. Holographs\u2019 128 distinct categories far exceeds decision trees\u2019 eight.\n\nSo, miraculously, the nature of neural networks is to tend toward the most efficient and resilient form of intelligence \u2014 a holographic memory. The landscape of possible synaptic weights is dominated by broad, steady valleys with holographs at their center. It is most natural, entropic, to fall into those places with the greatest powers of distinction. The cosmos made this one easy for us, both to evolve such a system of intelligence within ourselves, and to allow us to design a similar sort of intelligence for our own purposes."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/churchill-stopped-short-263b7af99592?source=user_profile---------4----------------",
        "title": "Churchill Stopped Short \u2013 Anthony Repetto \u2013",
        "text": "Winston surmised: \u201cDemocracy is the worst form of government, except for all those other forms that have been tried.\u201d If that is the case, why not try some new forms? Compare our plight to that of a bridge builder, who can only make bridges out of mud, straw, or gravel. The poor bridge builder complains: \u201cGravel is the worst thing for building bridges, except for all the other materials that have been tried.\u201d Sure, they could resign themselves to building bridges out of gravel. Enough of it, and they can cross the clogged river. Or, they could devote their efforts to research and development, trying many kinds of materials and comparing the results. Then, at least, they might have better bridges. Same for democracy \u2014 if it\u2019s so bad, why not experiment with alternatives?\n\nHere is one alternative:\n\nTry different things, and compare the results. Call this the \u2018Scientific Method of Governance\u2019. It is never fixed as one law; it constantly evolves, improving. Each new experiment enriches our understanding. This is radically different from a federal representative democracy.\n\nIn a democracy, people vote on a new law without knowing how well it will work! The passage of the law is left up to uninformed opinion. Democracy is uninformed by definition, because it doesn\u2019t try things out first. Democracies normally don\u2019t even bother to measure how well a law works after it has passed. The most popular opinion among all the uninformed must be best.\n\nWith a Scientific Method as constitution, a few methods of governance can be tried regionally and their results are compared. The best law, so far, becomes the provisional law. Yet, science doesn\u2019t halt after the first result. Experiments continue regionally, in the hopes of discovering even better structures of government. Sure, some locales might chafe under the idiocy of one experiment, yet that is already the case for the entire populace of democracies. Democracies suffer from more misguided laws, permanently; meanwhile, regional experiments only hinder a few people, and only temporarily. The Scientific Method allows continuous improvement, based on real results.\n\nThough, what kind of results are being measured? Who decides those? How?\n\nWhen we go to the polls to vote, each of us brings a set of concerns. Those concerns are private, however. Some people vote for a candidate who issues racist dog-whistles, and they never need to make their reasons public. And, our elected representatives vote with private concerns, as well \u2014 their \u2018ayes\u2019 are cast out of regard for corporate lobbyists, without ever saying so.\n\nWhat if we had public concerns? The metrics used for making decisions are posted, visible to all, and a law is passed if, during experimentation, it best met those concerns. Bigotry and kleptocracy could not hide. And, when those public concerns are evaluated, in each experiment of laws, the public would see which concerns were met and to what degree they were met. Bias grows in the shade, while public concerns illuminate the purpose of the law.\n\nThose concerns are not found by a vote. If an important concern is only known by one person in the whole country, they may still post it to the public list. It lives alongside the other concerns as equals. Another concern, shared by all, occupies only one slot beside it. Once all concerns are listed, it is possible for people to declare the weight of their concerns. This weighting is the nearest people would come to democracy.\n\nEach person would rank concerns, from first to last, or vouch their ranking to another person whom they trust. This ranked voting, along with vouching for another, was recently tried by Google with positive results. It has been branded \u2018Liquid Democracy\u2019, though it is just an amalgam of various methods familiar to any game theorist. The Scientific constitution differs from Google\u2019s experiment in one crucial way: Googlers ranked their final choices, while this method ranks only the concerns, not the final laws.\n\nSo, when experiments in laws are compared and evaluated for each public concern, (weighted by that concern\u2019s average rank) the public does not vote on which law passes. The public voted for their concerns, instead. The law that passes is whichever one best met ranked concerns. And, when a new experiment meets those concerns better, government policy switches to the new law without a vote. A Scientific constitution adapts as soon as adaptation is warranted.\n\nThis is a concept I have detailed elsewhere, which dovetails with a Scientific constitution. Using a market for externalities, the majority of government services can be managed by the markets without the moral hazards of privatization.\n\nThe core concept behind a market for externalities is that the government is responsible for internalizing the costs and benefits that fail to be accounted for by the market.\n\nEvery action has costs and benefits. And, when you are the sole recipient of the benefits, a business can provide you with those benefits in return for a cost. However, if the business operates by polluting a river, for example, then there is a cost that is felt by people who were not involved in your business transaction. Those sufferers feel a negative externality \u2014 a cost that is not accounted for by that business. The business avoids paying the cost of the river\u2019s victims.\n\nMeanwhile, an inspiring teacher produces great benefit for all the children they teach, though they are never paid back by those children. The teacher provides a positive externality \u2014 a benefit that is not accounted for by the market. Government implicitly recognizes that its role is to internalize both positive and negative externalities: when a river is polluted, government steps in to regulate, fine, and prosecute; because education is so valuable to the entire economy, government taxes to pay for teachers. The same reasoning applies to health care, defense, infrastructure. Government is intended to overcome the limitations of the market.\n\nSo, a market for those externalities would supplant many of the daily functions of government. Government would still enforce regulations, collect taxes, mediate the passage of laws and international diplomacy. Yet, every service government provides would be the product of a business proposal. The market \u2018votes\u2019 for these proposals by investing in them; like a Kickstarter campaign, investors\u2019 cash is only paid if the budget is met. The investors hope that the business activity will generate measurable benefits, which are rewarded by tax dollars. This is inverse to government contractors today, who win a contract, and then fail to deliver. These business proposals receive investor funds, first, and then must deliver to be reimbursed by the government. The business is awarded a fixed percentage of the valuation of benefits, which pays expenses and rewards investors.\n\nIf business is reimbursed by government tax dollars in proportion to the benefits they generate, then the market will move to generate society-wide benefits. For example, if your doctor improved your condition, or kept you healthy for longer, they would get paid. The price of keeping you healthy is paid out of their own pocket. Doctors in such circumstances would seek to take on patients who could be helped most by simple, cheap, preventative care. It would be the end of milking insurers for expensive procedures, years after the problems and warning signs arose.\n\nThe market chases high returns. Every dollar spent fighting TB is estimated to generate $43 of economic benefit, in those places still suffering from infection. If a tech company could turn $1 operating budget into $43 return, investors would clamor to own stock. The truly virtuous work is also incredibly valuable to the people it helps; if government measured and reimbursed businesses for that work, businesses would gladly take up virtue. We wouldn\u2019t need to rely upon backward and languorous burearcracy \u2014 we would be served rapidly and efficaciously by businesses that are only paid by taxes if they actually help."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/wandering-1-810b294bd4a9?source=user_profile---------5----------------",
        "title": "Wandering #1 \u2013 Anthony Repetto \u2013",
        "text": "The story is magical Lahdah, an island which appears only in night, as port for lost brigands who fear each harbor discover their loot. It is where thieves have their peace. No one found at charted ports has seen Lahdah. It becomes vapor at dawn, taking those thieves\u2019 ships with them and their treasure. Once a pirate finds Lahdah, he has no cause to leave it. The island is an immensity of vice, to the brim with plundered wines and stores, a deck of mad cards.\n\nLahdah would sink from all accrued gold and dancing rogues, if it ever ceased to be buoyed upward by laughter. Yowling laughter, spiteful, snorting laughter. Gusts of laughs float Lahdah behind a cloud each time gentle folk peer toward it. Laughter wraps around ribboned casks, tilting wine into their glasses, the lucky thieves.\n\nSome say they heard that laughter, not far from a coastal town in Cyprus. Others swore they heard Lahdah close to Bali, others, Seychelles, others, Bahamas. It moves like a vapor across every shore, gathering at circumferent horizons in stellar dance, always night-ward, the sun\u2019s own wake. Still today, laughter carries Lahdah, the gleeful, drunken-giddy island, across the bows of cruiseliners, over the backs of yachts as it flies along with night. A foghorn, that laughter belts the harbors of New York and San Francisco, an echo down the boulevards to the best restaurants. And, if you knew how you might listen to a safe, you\u2019d be sure to hear Lahdah laughing from every teller\u2019s vault. Lahdah laughs, because it won."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/fermi-filters-d32a1cd953ef?source=user_profile---------6----------------",
        "title": "Fermi Filters \u2013 Anthony Repetto \u2013",
        "text": "~why the stars are silent~\n\nThere was only a brief and fortuitous period between cheap guns and cheap surveillance when democracy could rise out from under monarchy and feudalism, here on Earth. Our first attempt at independence was aided significantly by a vast sea separating king and colony. It is not clear that other worlds would have such distinct land masses dividing peoples, to favor democracy and liberty, or that there would be significant success before security technologies cement all states\u2019 leadership. Perhaps, on many worlds much like ours, monarchy is the norm. Would that explain why we see no other civilizations bursting forth from their parent stars? Where did our neighbors go?\n\nThis is the Fermi paradox: among all the billions of stars in our galaxy, among hundreds of millions of galaxies, given the span of billions of years, we do not see telltales of alien civilizations. Given so many opportunities for life, civilization, technology, we would expect that hundreds or thousands of alien races must surround us. Presuming that our technology and economy continue to advance apace, we will be harvesting almost all the energy of our sun in three thousand years or less! Over the billions of years of existence, surely someone else must have gotten to the technology that we eventually attain. We would see them swallowing stars. We don\u2019t see that.\n\nSo, theorists propose, there must be Great Filters \u2014 situations that arise which prevent civilizations from forming, from advancing, from avoiding self-destruction. Nuclear weapons are definitely a Great Filter. If a civilization develops nukes, an arms race may tempt them into mutual annihilation. Nuclear bombs are provocative \u2014 they incite a reaction that escalates tension and risk. Perhaps most advanced civilizations detonate, explaining the silence of the stars.\n\nI am of the belief that there are simply many great filters. And we haven\u2019t made it past some of the worst ones. If we want a chance at star-scale society, we must be wise about these crises: environmental destruction, inequality and injustice, longevity, warfare. I do not include artificial intelligence as a great filter, despite frequent fear-mongering, because intelligent machines are best used for relatively narrow sets of tasks.\n\nWe only need machine intelligences that each know a few tasks better than humans; we have no incentive to design a single machine that can do all kinds of human thoughts and actions. The incredible difficulty and ongoing cost of operation makes human-scale artificial intelligence unlikely among all the civilizations in the cosmos. Nuclear war is much more likely. Though, narrow machine intelligences take over the task of surveillance and deployment of forces; AI will make it easy for despots to take-over and patrol large areas. That is AI\u2019s real risk.\n\nWarfare grows in ferocity with each wave of technology. Empowerment, even with the best intentions, is made into a tool for oppression and destruction. Humanity suffers from this shortsightedness, and it will not be cured by more technology. Other peoples, on other worlds, may have succumbed to nukes, or bio-terrorism, or despotism. Continued warfare thins the pool of successful civilizations; the most warlike may have a 50% chance of surviving every additional 100 years, while peaceful races face a 50% survival rate in 1,000 years. As a result, most of the warlike peoples will be younger and have lower technology than those less vicious. Peace preserves itself. War devours itself.\n\nI suspect that the growth of altruism and empathy may occur too slowly in most intelligent organisms, as they evolve toward greater societies. Once we develop a little empathy, it glues us together into a cohesive whole. Not a lot of empathy is necessary for a web of bonds to knit large populations. This was popularized as Milgram\u2019s Six Degrees of Separation. We only developed the minimal level of empathy to knit together. That empathy-glue allowed collaboration and skill specialization, with rapid technological shifts, resulting in an increased capacity for war.\n\nIf our earliest ancestors had kept fewer tools, and shared more, we might have entered the age of agriculture with enough compassion to avoid conflict. Did Prometheus give us fire too soon? It seems This is a Great Filter: that altruism involves greater developmental complexity than the crafting of simple weapons. The first growth of compassion knit us into opposing teams that could more easily destroy themselves, outpacing that compassion.\n\nThis seems to be the reason for the other Great Filters mentioned; environment, injustice, longevity. Pollution and destruction of biomes are well-documented risks to civilization, and they loom over us because of our lack of coherent response. We have the tools, yet we don\u2019t work together to use them. Injustice persists because we march behind sociopaths, instead of exiling them. Our empathy extended far enough to encompass our clan, or our religion, or country, no further. Altruism hadn\u2019t the time to mend our schisms, before technology made our differences more deadly.\n\nWhile our own dominance over the Earth may end soon (on the galactic timescale) these Great Filters suggest the kind of being which might survive. Avoiding the risks of resource depletion, habitat loss, focusing research on the public good. If some Earthly world evolved such people, they may inherit the cosmos. All our monarchies and despots would be dead branches on the tree of life, while the living bough is some better kind of creature.\n\nLook back to our early economy, before currency, not even barter. You are sitting among the village elders and children, grinding grains, when another member of your tribe bursts from the trees, shouting and pointing. You all rush to gather the children and key supplies, knowing from his tone and a few syllables that a forest fire is coming.\n\nCapitalism requires payment for labor. This shouting relative, however, recruited the whole tribe into action without payment for their efforts. Each member of the tribe relies upon the division of labors within the tribe. Everyone is at a loss when anyone is lost. So, we cared for each other. That achieves a better result than capitalism. It\u2019s a holistic evaluation of circumstances.\n\nIn parallel, how much is that shouting tribesmen rewarded, for ensuring others safety by warning them? A capitalist would bargain for the highest price that the others are willing to pay. That\u2019s how markets set prices, after all, so it\u2019s \u2018fair\u2019. And, with that large reward, the shouter represents greater demand for goods, greater power. Yet, that price might impoverish other tribe members. So, a tribe\u2019s holistic evaluation could be to praise and give attention to the relative who rushed to warn everyone, but the shouter would not be given special power over others, or be allowed to take more than their share of food.\n\nWhen one of us is frightened or in pain, we reach out to them, and we feel their tension and wound. That was our earliest currency. It paid for goods and services. It orchestrated hundreds of people toward common goals. Consider a civilization that dodged monarchy and capitalism. Each person\u2019s efforts flow along a network of empathy, so the resources necessary moved to the places that need them. A civilization that maintains compassion as currency would react with all their effort in response to a threat, as soon as they identify one. (If our civilization were that responsive, responsible, we would have made a more serious effort to avert climate change since the 70\u2019s, for example.) When a civilization moves quickly and coherently to thwart global risks, they are more likely to survive. Empathy brings longevity.\n\nYet, that longevity is a temptation that may cripple many civilizations. If members of society live long, then they are more likely to respond to long-range threats. That seems to favor long-lived aliens. However, if longevity was achieved unequally, they could plunge into a race between despots to consolidate power, knowing that they will reign forever.\n\nA more serious risk from the technology for longevity is the possibility of being tortured for longer periods of time. If a mobster can threaten to torture deserters forever, none will squeal to avoid prison time. Despots benefit from longevity\u2019s ability to amplify threats. An unending, living hell. Eventually, the cost of maintaining all those torture-chambers encumbers a civilization. The people would have fewer resources to deal with external threats. (e.g. asteroids) These Mobster-Torture-Oligarchy civilizations are fragile, and they implode after a disaster or power vacuum. These are risks yet to come, and I estimate that they are Great Filters which destroyed many thousands of civilizations before us. We may even find the radioactive, barren worlds where they fought, more numerous than planets with fungi.\n\nThese are Dangers Ahead. The odds are against us. How long before someone who, caring only for themselves, or only for their team, unleashes a virus that topples us? The risk hasn\u2019t lessened, but we waltz on, oblivious. A more firmly empathetic culture might have responded faster, done more to fix problems, instead of plastering over risks or covering their behinds. If any one of them suffered, all would feel the echo. When the universe births a people like that, I bet they will grow vast and live long and well. Us?"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/meta-ethics-and-the-fermi-paradox-61c923e11053?source=user_profile---------7----------------",
        "title": "Meta-Ethics and the Fermi Paradox \u2013 Anthony Repetto \u2013",
        "text": "Why haven\u2019t we seen evidence of alien civilizations, yet? There are billions of billions of stars, and planets around most of them \u2014 surely, if a civilization were even a few thousand years ahead of us, technologically, we would notice signs of their advancement? Wouldn\u2019t they capture as much of the energy of their star as possible? Wouldn\u2019t their probes already be in our neighborhood? I suspect that the reason we see no advanced civilizations is because most never make it past the point we are at now. It\u2019s a matter of ethics.\n\nWe each have an ethical perspective, and those perspectives differ, yet they also \u2018cluster\u2019 into patterns of behavior which we can use in a meta-ethic: How do we manage our diversity of ethics? The ethical way to handle ethics disagreements. In particular, what do you do with people who are narcissistic, Machiavellian psychopaths? How does our response influence expectations surrounding the possibility of alien civilizations?\n\nPsychopaths are like a virus \u2014 they require a host. Their lack of empathy allows them to calmly lie to and mercilessly exploit others. A psychopath prospers when surrounded by naive empaths, while a country of psychopaths implodes upon itself. As long as empaths are numerous, these psychopaths also see personal gain from banding together, reducing the risk of their persecution. However, whenever the abundance of psychopathy crosses a threshold, those psychopaths are no longer bound to mutual defense. They turn upon each other, recognizing that the other psychopaths are competition for a limited pie.\n\nThis self-destruction motif is a kind of meta-ethic: things go badly for everyone when psychopaths are numerous, and psychopathic narcissism is a detriment to people with all other ethics. Therefore, most ethical stances recognize psychopathic behavior as unethical. Only other psychopaths are okay with it, and they are necessarily few.\n\nYet, who grips the levers of power? While people with a preponderance of psychopathic, narcissistic, and Machiavellian traits make up perhaps 1% of the population, their numbers are closer to 4% among lawyers, police officers, surgeons, and politicians. Which isn\u2019t really surprising \u2014 psychopaths gravitate toward posts with influence, to extract as much as possible for themselves, while compassionate and loyal people become math teachers and marines. If we encountered another civilization, might it be ruled by psychopaths, despite an abundance of empaths? It seems that our world is proof of such a possibility. What happens when two like that meet?\n\nLets simplify: if two civilizations are both compassionate, they have a great time when they meet. Both win. Yet, if one civilization is compassionate, while the other is ruled by Machiavellians, the compassionate one is subjugated or destroyed. Machiavellians win. However, if both civilizations are Machiavellian, they will be locked in vicious struggle for resources, incapable of trust and peace. Both lose.\n\nA slight complication to that example: your civilizations are at different levels, technologically and developmentally. Both win from compassion, though the lesser civilization wins more, proportionately. Both lose from struggle, though the advanced civilization loses less, proportionately. If either drop their guard to a Machiavellian neighbor, they are enslaved or consumed.\n\nBecause the lesser civilization wins more from cooperation, while struggle would likely destroy it, it is strongly incentivized to seek cooperation. The advanced civilization, in contrast, would gain little from cooperation, and lose little in a fight \u2014while it must not allow the lesser civilization to manipulate it. The advanced civilization would bet best if they subjugate or eliminate the lesser civilization, because that thwarts the risk of their destruction. The lesser civilization can only hope for kind treatment, not their neighbor\u2019s trust.\n\nIf two civilizations, a Machiavellian and a compassionate one, are expanding and developing from the same moment onward, we can expect the compassionately led civilization to achieve a higher growth rate. Machiavellian narcissists squander resources on their own pleasures, as Sun King, as well as on the intrigue necessary for their control, as Stalin. A civilization ruled by compassion uplifts and improves life for all, and seeks long-term benefit. It can recover more rapidly from set-backs, and it can take advantage of new technologies without protectionism or militarism getting in the way. Kind civilizations race ahead of foul.\n\nSo, there is double reason for distrust \u2014 if a larger, compassionate civilization encounters a backward neighbor, that neighbor may be stunted because of psychopaths, and it cannot be trusted. When advanced and lesser civilizations meet, the advanced one must assume that the lesser civilization has more destructive ethics. If we met advanced aliens, I would worry about their assessment of us: we spend huge sums for war and oppression. I doubt they would ever trust us.\n\nWhy don\u2019t we see such an advanced life prospering all around us? I suspect that, just as we have, those civilizations fall ill. The virus of psychopathic behavior makes hell of any technological heaven. Unlike the meeting of worlds, psychopathy is already well-mixed into the crowd of this world. That mixture allows psychopaths to profit more than the compassionate, and as technology advances, the psychopaths have more and greater opportunities for mutual destruction.\n\nA naively compassionate civilization, unlike our simplified \u2018advanced civilization\u2019 mentioned earlier, having never met psychopaths, would not think to distrust them, and would quickly fall prey to deceit. After that, the psychopaths could be expected to waste and war until none remain. So, the only advanced civilization we\u2019re ever likely to meet will be one which has overcome its plague of psychopaths, and is rightly wary of creatures like us. How do we overcome our own plague?\n\nCuddly morality cannot gird against narcissists. The meta-ethic is that, to protect all the diverse ethical stances from being subjugated alike by psychopaths, decisive action against that psychopathy is necessary. What would be decisive against psychopaths?\n\nA key feature of narcissistic psychopaths is their resistance to negative reinforcement. They don\u2019t develop a sense of consequences, instead condemning all their punishment as unjust, even as they themselves seek to punish others for lesser crimes. To the narcissist, this double standard is fair \u2014 because they are more important than the rest of us, and their excuses are valid. We can\u2019t rely on threat of punishment to deter psychopaths from abuse. We must accurately identify and eliminate psychopathy, or at least sequester or medicate away the Machiavellians. Arguments don\u2019t work. Deterrents don\u2019t, either. They need to be removed, physically, from all sources of power and influence.\n\nI doubt that we will overcome our \u2018harm to none\u2019 marshmallow, and accept that \u2018harm to psychopaths\u2019 is a valid and valuable meta-ethic. Without that policy, all our various ethics are at risk.\n\nWhen an ethic appeals to a \u2018just world, just creator\u2019 or \u2018fairness in the afterlife\u2019 or \u2018karma from past lives\u2019, it enables psychopathy to retain control. \u201cThe psychopaths will be thwarted by divine will, in another reality.\u201d So, they reason, it\u2019s perfectly fine to be naive. Psychopaths need people like that, and would prefer to encourage that naivete, because it ensures a pool of easy marks.\n\nThe meta-ethic, then, imposes upon these \u2018just worlds\u2019: by naively collaborating with and feeding psychopaths, these ethics participate in the psychopath\u2019s deprivation and abuse. They are accomplice. While the greatest freedom possible should be given to our choice of ethics, those ethics must necessarily prohibit abetting evil in the name of slight virtue. This meta-ethic dictates support for, or at least indifference to, inhibition of the Machiavellians.\n\nWill we devote funding to research the root causes of the dark triad \u2014 narcissism, lack of empathy, and manipulation? Will we find a cure, or steel our resolve to imprison or kill psychopaths? We haven\u2019t yet. And technology, by concentrating power and eliminating humans-in-the-loop, will make our task more difficult. Our chances are slim to none, considering Fermi\u2019s Paradox. Expect psychopaths to ruin everything as soon as they can. That is why the night sky is silent."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/fog-both-ways-3e71ac939047?source=user_profile---------8----------------",
        "title": "Fog Both Ways \u2013 Anthony Repetto \u2013",
        "text": "For a moment, suppose the truth of Everett\u2019s Many-Worlds interpretation of the collapse of the wave function: every possible interaction occurs, in some branching of the universe. This cascade of parallel universes is normally considered to move only into the future, like the branches of a tree. We can\u2019t predict the future, because quantum randomness mangles our estimates with each particle interaction. Yet, we think of the past as somehow finalized.\n\nWhen quantum weirdness blips an electron into sight, the traditional physicists say that the electron\u2019s wave function collapsed. The electron was \u2018smeared out\u2019 in space, until we looked at it. The many-worlds view is of many possible futures stemming from each uncertain moment.\n\nI am a fan of many-worlds, especially because that interpretation suggests the possibility of time travel. Though, it wouldn\u2019t really be time travel \u2014 you would blip out of existence on this end, and be gone. There is also a parallel universe where you happen to appear at your chosen time, but this parallel is just as valid as an elephant appearing then, or a rowboat. They all happen. Would your present consciousness perceive travel to that time? I don\u2019t know what consciousness is.\n\nYet, many-worlds as it is usually presented does not consider that, for the particular future we observe in the next moment, there are just as many various pasts which converge upon that same moment\u2019s state. Like rivulets forming streams down to a river, these disparate pasts merge into now. Which past was real? Many-worlds must admit all these pasts as parallel. Just like parallel futures.\n\nSo, the branching and crossing of past and future paths is not a tree, it is tapestry. Many worlds lead to this moment, and many scatter from this moment. As far as our present consciousness can perceive, the threads of tapestry grow hazy in both directions, like a fog.\n\nIf we could see the width of that tapestry, all the parallel presents, the majority of particles in the majority of universes would follow normal physical laws, while some small fragment of some of those worlds would see quantum tunneling at that moment. Does that mean those \u2018majority\u2019 cosmos are more real? Are there more copies of you, experiencing those parallels? If each world occurs, there is a cosmos for each path you choose to take \u2014 there are worlds where, by quantum coincidence, you choose to be a saint or a serial killer. Are those people really you, when the vast majority of worlds have you turn out much like you are?\n\nThis is a question of the supra-real \u2014 what mostly happens, across the thick of parallel times? And, another question about the supra-real: if time travel is possible, do the world-paths with time travelers become more abundant? Suddenly, a local majority of parallel universes contain you, with your memories of the future, far in the past. Would that mean you really had traveled back? Would the consciousness on this side of time perceive continuity? Whole swaths of parallel universes, far removed from our local strands in time\u2019s tapestry, would be populated by time travelers. Galactic empires, forged by future selves who went back to the earliest fusing and cooling, may be more numerous than our traveler-less timeline. Is that what the universe is for?"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/impulse-7401809d1316?source=user_profile---------9----------------",
        "title": "Impulse \u2013 Anthony Repetto \u2013",
        "text": "The world, sunk beneath like a coin into a well, still sings to us:\n\n\u201cWe won\u2019t be sure for another eight and a half years, but these should be the first signals received by Dent and his skeleton crew. Wallace, are you listening? Because THESE are the top new singles playing back here on Earth\u2026\u201d\n\nChiro laces the display in green ivy and moss, her signal of plane vitals and operations. Nothing to worry about. The news from home will soon be grim, I know. Talk of war, diseases of poverty, crime lords. Gridlock. Politicians with aspirational marketing techniques. Death.\n\nI cure us of death, yet death remains. Deathlessness is cause of death.\n\n\u201cThough neither side could come to an agreement, both seemed buoyant back at home, almost excited that fighting may continue. Their thirst for victory may be fuel for a longer, intensifying civil war\u2026\u201d\n\nI can\u2019t be faulted for wanting out of the loop. It would have been my noose, to see them clutch tighter at blots of mud as they slid into Earth\u2019s bedrock. Why live so long, on a dying world?\n\nI knew all that when I started. That was why I started. I needed the money, and the one thing that the rich would pay anything to get is immortality. Serve the elite, and you are rewarded. Scooping soup at the shelter earns no credit. So, I scooped data.\n\n\u201cDent, you lucky fool! You picked a good time for all this, and style points for making such an exit. The world went crazy for your treatment, and now it\u2019s just gone crazy. These execs, and the mob, the Chinese \u2014 it isn\u2019t a clash of ideologies, it\u2019s a RACE between TITANS! Now that you made these people young, they plan to never leave their mansions, power forever after. Their bodyguards are the only regular guys who stand to profit in this world. There\u2019ve been assassinations\u2026 I can\u2019t\u2026 Look, Dent, you said you would pay us the rest, I took your word, and\u2026 Hell, it probably doesn\u2019t matter at this point. You knew you screwed me, before we ever shook hands. Even with your winnings, you couldn\u2019t\u2019ve really afforded that plane.\u201d\n\nI shoveled data. Nothing got through my lab without my eyes on it, without my code licking information like a sore tooth, without comparing it to my secret, my intentions. I could have made more money, if I had staggered pricing better, yes. I didn\u2019t, strategically. My advantage, my leverage, would rapidly disappear as elite consolidated. I knew what I sought, in every grain of data. Knew how it must end.\n\nChiro flashed a question: Should I wake them up yet? It will take at least twenty minutes for the sedatives to be neutralized. \u201cNot yet,\u201d I am mumbling \u2014 I see how the syrup in my veins has intoxicated me. I hadn\u2019t tried to move, yet. Every muscle feels like roots of a tree, cold, wedged in rock. I need more time, first, so that I speak clearly. When they wake, I\u2019ll tell them.\n\n\u201c\u2026 \u2018I realized \u2014 being twenty years ahead of my time wasn\u2019t really the answer. I could be a hundred years ahead of my time in something profitless, like mathematics. Didn\u2019t matter. I only needed to be five months ahead, in the right ways. I beat them to patent by five months. And that made all the difference.\u2019 That was Wallace Dent, in an interview thirty years ago. Hard to imagine that this impetuous biochemist would become the FASTEST to a trillionaire, right Katy?\u2026\u201d\n\nThey really suspect nothing. My mad dash to the patent office. My rapid roll-out, even as litigation sloughed along. Earnings bought lawyers, lawyers brought loopholes. So, I skidded into home \u2014 cheap radioisotopes. Dirty ores. I bought it all. Bezos had a strategy for rockets. I rode isotopes up on those rockets, with this dirty plan: build the first large scale Greason-Slough plasma magnet sail, powered by fissile patties. People laughed at the idea, sure \u2014 \u201cC\u2019mon, Wally, the Singularity is about to happen, really, it\u2019ll be so much cheaper once we have strong intelligence. What\u2019s the rush? We\u2019re not getting any older!\u201d\n\nI was racing out ahead of the racers, outpacing them, like their dogged lure.\n\n\u201cWallace Dent, who must now be listening to all of us here at the studio,\u201d cheering, whistles, \u201cWally, we miss you! We are wishing your family well, and we hope you find it welcoming in your new home. I thought I\u2019d never want to travel through the stars, but now, I know I speak for many of us here \u2014 take us with you! We all love your treatment, but, you know, things are rough back here in the states. Not as bad as some places, sure, but I haven\u2019t been able to leave this zip code for a week. Have you seen what happened to Paris? Oh, god, Wally, send us one of your bio-hack things that cures STUPID!\u201d applause.\n\nI had to look through all the data, first. Correlations, follow-up questionnaires, interviews, sometimes staged as businesses looking to hire, leaflets baiting them. I had to look, first: who isn\u2019t stupid? Or evil?\n\nSociopaths, by dint of their lack of empathy, are willing to do anything that has them win. They take the jobs no one else would touch. Those jobs always pay more. Sociopaths heaped wealth upon cunning, and they were the bulk of my clientele. God-fearing folk, even rich ones, weren\u2019t interested in my gift. I had to find them before they went extinct.\n\n\u201cDoctor Dent, who channeled his wealth first into server farms here in Canada, rapidly targeting drugs and treatment plans with such astounding benefit to the poorest, has triggered a wave of compassion across the globe. This \u2018dent-effect\u2019 even has the Pope selling properties to finance greater aid efforts. And, in an act some are calling chivalrous, Dent moved to capture much of the world\u2019s supply of fissile materials, saying that they are better used for space travel than arms races. What will this trillionaire researcher, inventor, investor, philanthropist think of next?\u201d\n\nI thought of how to be my neighbor\u2019s stumbling block. I slunk into research institutions, gummed up their numeric controls, soiled their samples, stole thunder and fire. I crept under their doormat, under their footsteps, a lace caught at their hem. My machines weren\u2019t cracking genetic codes \u2014 they were hacking geneticists.\n\nI\u2019d had the first insight, sure. I am an accomplished researcher. But others would have noticed the same, soon enough. I broke with my team, after breaking into their lab. My first real crime. My patent, my company, all followed my racing heart \u2014 \u2018Before they do. Before THEY do!\u2019\n\nThe entire plan crystallized in two weeks, I remember \u2014 between the full moon that drove my midnight lunacy in that lab, and the gloom and whisper-grey of the new moon when I concluded. Isotopes. Drills, autonomous. Data on every last soul I could, DNA of every stripe, textbooks. Proxima Centauri. I knew that we couldn\u2019t survive any other course. I was shameless, merciless. I had even set the fuse for 4.24 years before my arrival here, to save myself the guilt of contemplating their state much longer. I awake, I see my sickness gurgle forth, I hear their pleas, and then I wake the others. Any minute.\n\n\u201cThis is horrible! Dent! The news! It\u2019s yellowstone\u2026 and, we\u2019re just getting word, oh, the other stations, the internet is down, we\u2019re bouncing satellite now, the ash cloud is expected to be\u2026oh, wait \u2014 wait, we are hearing now, that\u2026 DENT.\u201d ah, here it is. \u201cDent, you fuck! How could you?! They\u2019re reporting readings, sieverts, it\u2019s YOU! You must be laughing, right? Laugh, laugh, you sicko. You cheat. Oh, hell isn\u2019t enough for you, Wallace Dent. Go live with your family. Have a blast. I just want to know, just one thing, really, send us the message, maybe someone will be around to hear it: what are you going to tell them, your family?\u201d Yes.\n\nI will tell them that they have a new world to themselves, three new suns, and a raft of new faces waiting to be born. Good, homely faces. But there will be survivors on Earth, the wealthiest \u2014 sheikhs and mobsters, who will plot revenge. Revenge that lasts hundreds of years, boiling like the volcano that bought it. They will come here, and you must prepare to meet them. I will be gone by then, like a coin sunk down in a well."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/reap-the-whirlwind-88c73bf7a219?source=user_profile---------10----------------",
        "title": "Reap the Whirlwind \u2013 Anthony Repetto \u2013",
        "text": "We can make a ring of hurricanes which pumps vast amounts of heat away from the Earth, to mitigate the effects of global warming.\n\nLike pillars surrounding a temple, each hurricane\u2019s warm-core funnels heat high into the air, where it disperses and radiates into space. The air cools, and descends, tugged into a downward spiral along the inside of the ring of hurricanes, forming a cold-core Nor\u2019easter. Similarly for the outer edge of the hurricane ring. The cooling, descending air drapes the ring of pillars, holding them together. A ring of hurricanes is meta-stable \u2014 it will retain its shape, even when buffeted by wind shear or smooshed against coastlines.\n\nBecause it has some local stability, a ring of hurricanes will maintain the eye walls as wind speeds and size diminish. The hurricanes will remain coherent structures, even as less energy flows through them as heat. So, you can create these coherent structures when temperature differences are smaller than during normal hurricane formation. You can make many tiny hurricanes, instead of one big one.\n\nIf the ring of tiny hurricanes was a doughnut, the cold air descending around it would be like a frosting poured on top, sliding down the inner and outer sides of the doughnut. That shell of cooler air protects the hurricanes from outside forces, and pushes them together into a tight ring. This is a tiny ring, not a vast billowing hurricane that causes damage across multiple countries. And, because it is tiny and metastable, this ring can be nudged by adjacent convection \u2014 you can reliably steer it into open waters, to protect people.\n\nCyclones and hurricanes only occur because of an increase in warm, humid air along the ocean surface. Creating a tiny ring of hurricanes, you ventilate that heat. You have prevented additional hurricanes from forming, by taking their heat away. So, the billions of dollars in damages, the loss of life from annual cyclones and hurricanes, can be completely prevented. Just vent that heat with tiny doughnuts.\n\nThat ventilation of heat can continue, even when temperature differences are smaller than normal hurricanes. \u2018Cuz of the ring\u2019s metastability. So, you can continue using these rings as heat pumps through a larger portion of the year, and over a larger range of latitudes \u2014 these metastable rings could pump a decade\u2019s heat in a year or two. That\u2019s what we need to head-off global warming. Those colder surface waters will generate more sea ice, too.\n\nA buoy, with a weight on a string beneath it, bobs in the open ocean. That string is attached to a small piston, like a slide whistle. Each bob in the waves tugs and relaxes the plunger of that whistle, pumping a few droplets of water through a nozzle, as spray into the air. Together with millions of its plastic buoy brethren, this weight and bob sputter swimming pools of warm water, raising surface humidity and temperature. Micro-fountains, powered by waves.\n\nThen, when the air is heavy and hot from the sun, vast inflated whales surface in a ring. Each whale\u2019s mouth is a wide, slow turbine, funneling air from its spout to its snout. The turbines don\u2019t need much power \u2014 they are like the nudge that sends an avalanche. As soon as the waterspouts form, the whales submerge for their own protection, because the hurricane ring accelerates quickly.\n\nAn area only a few dozen miles across can be supplied by moist air from hundreds of miles around, forming a heat pump that survives for weeks. A multitude of these hurricane rings, over the course of many months, could buy us time to heal the climate."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/time-bubble-2-moire-1dbf01f8e96a?source=user_profile---------11----------------",
        "title": "Time Bubble 2: Moire \u2013 Anthony Repetto \u2013",
        "text": "A moire is the pattern of overlap and interference between contour lines or figures. The bands of interference bend and arc at angles from the original lines. For example \u2014 creating elliptic curves around a pair of concentric rings. (see above)\n\nWhen the contour lines are replaced by a regular tiling, (square grid, honeycomb, or a repeated icon) then the interference patterns act as a magnification of the regular tiling! (see below)\n\nAlso, you can move a tiled moire left-to-right, while the interference pattern moves up and down, orthogonal to you. This is a clue to the relationship between perpendicular forces \u2014 electricity and magnetism. There are changes in the dynamics of apparent waves, based upon the motion of the underlying moire grids. Here:\n\nI suspect that moire dynamics are a simpler and richer metaphor for modeling subatomic physics. De Broglie might agree. Any displacement of concentric ring contours will produce waves which may carry particles. Yet, de Broglie might not have noticed the effect of torquing two tiled contours \u2014 magnification.\n\nThe concentric contours of de Broglie are one aspect of subatomic wave and particle dynamics, and the magnification of a tiling must be considered as another aspect of subatomic dynamics, regarding the magnification of matter.\n\nIf the moire representing a proton is contoured with concentric circles composed of regular tiles, and it experiences torque between its moire layers, then the particle which rides this wave, via de Broglie, would be likely to appear and interact over a much larger area than the ground state. The proton would be \u2018smeared out\u2019. This increases the probability of interaction between that proton and other particles, analogous to quantum tunneling.\n\nI suspect that the vibration of regularly tiled crystalline lattices (e.g. diamond) can fuse constituents of the surrounding material (e.g. hydrogen, deuterium) because the subatomic particles are made diffuse by a turning of the layers of their moire. That torque causes a magnification of these tiled particles, to a scale where they generally overlap; this overlap allows the transfer of energy between the fused particle and the surrounding lattice. The diffusion of force along the magnified perimeter, and small relative distance between nuclei, allow deuterium to easily trespasses the Coulomb barrier. So, yeah.\n\nI also suspect that: if similar torsion is applied to a shell, enclosing cargo or occupants, it might form an incredibly strong barrier, because large sections of the shell would weakly interact with any projectile or radiation over a long distance, and force of impact is distributed over the entire surface of the shell.\n\nFinally, I hazard: these moire exist as higher-dimensional objects, and their interference zones create our four-dimensional cosmos\u2026 so, some torque of moire can happen along the time dimension. If a torsion shell could rip loose from this space-time, as its own micro-verse soap bubble, it might be able to land in a new time and place when it pops back into our world \u2014 like a soap bubble landing on water, then exploding.\n\nWhen viewed in 4D, the torsion shell\u2019s initial \u2018bulge\u2019 in space-time is followed by the formation of a \u2018bubble\u2019 which \u2018separates\u2019 from our shared space-time \u2014 in 4D, that \u2018separation\u2019 actually looks like a tube or tunnel\u2026 because it\u2019s a temporal wormhole. Like the handle on your coffee mug, wormholes can reach from any spot in space-time to any other. We just need to figure out how to steer them\u2026"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/time-bubble-463b36d5c340?source=user_profile---------12----------------",
        "title": "Time Bubble \u2013 Anthony Repetto \u2013",
        "text": "It\u2019s just a bunch of capacitor rings spinning within each other along wobbling paths, like fallen hula hoops. Yet, as those circles of static twirl faster, strangeness begins: the air outside the ring gets colder, while the air inside the ring becomes hotter; within the rings, cameras record this heat pump as infrared emanating from the rings themselves; looking into the center of the rings from outside, the interior space is dimmer and magnified like a fish-eye lens; and, clocks placed inside the rings run faster than normal. All sorts of problems arise when you try to travel backwards in time, but it is quite simple to travel forward in time at a faster pace.\n\nWhen an object travels through space at speeds approaching that of light, time is distorted \u2014 a year may pass for the people waiting back on Earth, while a rocket\u2019s crew only ages a month. High speeds effectively slowed the pace of time for the crew. The static-charged rings are the opposite model, and the orbital motion of the rings creates this effect without needing to fly away. You could keep it running in a garage. A crew within this Time Bubble ages faster than the people outside.\n\nWhy would anyone do such a thing? Simple: for science. Though, the reason is more properly described as scientific work for military applications and state sponsored industries. That why China jumped at the opportunity, building massive rings which, by virtue of surface area to volume ratios, allowed many more people to slide into the future. Whole research teams spent months at work within their bubble, while only a day passed in the real world.\n\nSupercomputers crunched away, to harvest this fantastical multiplier of processing time. Want to train an artificial intelligence, but your GPUs need a year of compute to get optimal results? Try spinning those rings a bit faster, and you can have your answer within the hour. Sure, the electric bill is huge, but it still cost less than buying more GPUs. The branches of the party and the government were placed within these rings, to hammer out policy quickly in response to any event. The military loved it \u2014 as soon as an enemy began to act, these Time Bubbles would give generals weeks and months to prepare their response.\n\nSomeone suggested placing radioactive waste in a Time Bubble, to speed it into the future when the waste decays into stable isotopes. No one tried it.\n\nThe Time Bubbles had an odd constraint, though \u2014 the heat in the core of the ring could only accumulate, causing a continual rise in temperature. This effect was mitigated by slowing the rings periodically, bringing the occupants back into nearly-normal time, so that heat could be exhausted before it baked everyone. It was a minor inconvenience, but it led to an idea: you could flip the Time Bubble inside out, and place this new bubble within the old one. It was like a humidifier with a de-humidifier inside.\n\nYou see, the wobbling rings of charged particles induce magnetic fields which, as we learned with the first bubble, can effectively separate you from the normal time-stream. No one had tried rolling a Time Bubble down hill, so it took a while to prove that all fields were affected by it; gravitational force and inertia are warped by the bubble. When an inverted set of rings spins within a Time Bubble, this \u2018osmotic pressure\u2019 on inertia empties the region between the outer and inner bubbles. The chord between the inside and outside world is cut.\n\nKeep in mind that the area outside of the original Time Bubble is getting colder, just like the interior of the bubble within, and the space between these two bubbles is getting hotter. When a rocket lifts these nested bubbles on its nose, there is no inertia or gravity to weigh it down. Yet, the potential energy of lifting the cargo\u2019s mass must come from somewhere! Otherwise, thermodynamics breaks. And so, a portion of the heat accumulated between the rings is evaporated \u2014 potential energy is restored when the bubble bursts! The energy required to match potential energy is much less than the energy required of rocket fuel; moving masses into space became stupendously cheap.\n\nThere\u2019s also the matter of how much time passes for objects within the inner Time Bubble. Consider \u2014 the outer bubble speeds up time according to the speed of rotation of its rings, and so the inner rings\u2019 wobbles seem faster than they are. To keep time flowing at the usual pace within the inner bubble, the outer bubble must rotate quickly while the inner bubble is languorous, because its field effects are magnified by time-compression. However, that inner bubble would not feel sufficient cooling during this time; the \u2018osmotic pressure\u2019 is imbalanced.\n\nSo, the inner bubble must wobble its rings faster than the outer bubble, and it accumulates a larger endothermic \u2018budget\u2019 than the exterior. Effectively, the potential energy \u2018cost\u2019 of lifting cargo on your rocket is paid by burning fuel within the inner bubble, which is absorbed as heat into the region between bubbles, and is then squashed by absorption into potential energy when the bubbles pop. The inner bubble\u2019s time is slowed greatly \u2014 resulting in a journey to space that seems to take only a moment.\n\nIf a spacecraft leaves Earth headed for a distant star, it can carry a great mass of supplies within these Time Bubbles, without sacrificing speed. The occupants would feel as if almost no time had passed. Though we cannot go backwards in time, we can dial the pace of time in either direction, and we pay only the cost of final potential and kinetic energy when we travel.\n\nNow, many have boarded \u2018stellar cruise liners\u2019 intent upon founding new civilization among our neighboring stars. To us here on Earth, their voyage will take hundreds of years, yet it is a weekend to those travelers. Their outer rings are hundreds of kilometers across, wobbling slowly within a protective shell. Their immense volume dilutes the heat accumulated during travel. And, the mass of the rocket which propels them is almost entirely nuclear fuel \u2014 transit is fast and economical."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/empatheocracy-991b5dfd82dc?source=user_profile---------13----------------",
        "title": "Empatheocracy \u2013 Anthony Repetto \u2013",
        "text": "The Cosmos is unfathomably vast, while the material portion of existence is scattered in tiny flecks across the black. We are mostly empty. And, across all the matter in the universe, the aching bulbs of stars, swollen gas giants, icy moons, only a thin smear of that bulk is alive. We are mostly dead. Further, among all the knotted roots and teeming swarms, and in outstretched eons before us, life is unaware, uncaring. We are mostly merciless.\n\nIt would seem, from this, that the purpose of the universe is to isolate callous lifeforms from one another using bare rock and vacuum. Such life seems doomed to suffer. It does, most of the time \u2014 every life contains some suffering (excepting the brief span of a dopamine-addicted lab rat). Yet, that suffering can take a few forms \u2014 a despot might oppress your people, and you all suffer; you might disregard others\u2019 warnings, and suffer your own hubris; you might become teary when listening to hardships your grandmother endured. Tyranny, stupidity, and empathy.\n\nMost living things are unaware that other living things have brains. The world just happens around them, and they respond as they feel best. Was the cat relishing the mouse\u2019s suffering? I doubt that the cat thinks of the mouse as another being, capable of suffering. The cat just plays, tooth and claw, until it gets a treat. Mouse-feelings never cross cats\u2019 furry minds. Life doesn\u2019t care how you feel. Mostly.\n\nSo, because lifeforms are going about mercilessly chewing on each other, life is full of conflict and suffering. And, because each new life must learn how to prosper, most life suffers from rampant mistakes. In contrast, empathy is a special kind of suffering, undertaken willfully by the sufferer! Empathy lets us feel a flicker of the emotions that others endured, so that we do not make the same mistakes. Empathy is what stops us from chewing mercilessly upon one another. Empathy cures us of the other forms of suffering, mostly.\n\nIn all those empty, unfurling parsecs, among all these orbs of fog and sand, across all of life until just recently, empathy does not exist. Any life without empathy cannot imagine it. Sociopaths are colorblind to the rainbows of empathetic relating. There is no cure. You cannot teach it where it isn\u2019t ready to grow. If we ever meet an alien civilization, I hope they have the capacity for empathy. By that time, I hope we still have ours.\n\nEmpathy is special. More than knowledge, creativity, aesthetics, which some suppose will drive our actions once machines grant abundance, more than the technology for such machines \u2014 empathy is sacred for humanity. All those other things, history, culture, philosophy, are only the means for empathy to endure. Empathy has no guarantee, is not selected for, not to be taken for granted. Empathy withers without our attention and wise care.\n\nYet, empathy is the anchor tugging the universe into place. Without it, there are only instincts, no purposes. To perceive experiences among a multiplicity, while retaining our individual tints and facets, is communion. It is the return to the core of our being, by reaching out. Forget creative liberty, or happiness. Happiness isn\u2019t a reason to live \u2014 empathy is. In particular, the protection of empathy is the most noble and good purpose, whether robed in monastic ethos or swaddled at your breast. I don\u2019t care for liberty, justice, representation, if those things are inherited by sociopaths. None of those virtues have purpose without a foundation of compassion, and the fortitude to protect it.\n\nI was sitting in the passenger seat of the old truck, my home, smoking a cigarette, watching the birds hop and scratch. A mother, with her toddler daughter, came alongside the truck. Concern was in the mother\u2019s voice, the tone of parental wincing, \u201cOh, honey, don\u2019t do that to the little bug!\u201d Her daughter had caught a small insect, and was busy with vivisection \u2014 the mother continued: \u201cImagine if you had wings, and you could fly!\u201d Her tone lifted, she paused, giving her daughter that moment of soaring fantasy, \u201cand someone pulled your wings off!\u201d\n\n\u201cOh.\u201d The daughter\u2019s response was smaller than a hiccup, but it was a seed in her brain \u2014 she dropped her bug, and her feet scuffled a bit while she spun into thought. \u2018I can imagine being like something else, and so I can imagine how my actions might feel to them\u2019 mulled under her beanie, building the bridge from emotional contagion to affective empathy. That bridge spans all the way to Eudaemonia, across all the emotional tributaries of the heart.\n\nThose moments we spend in empathy are a sacrament. Empathy propels our hands into all the hardest and cruelest places, to make life aright. I trust no other government. Empathy is motive for the fight against merciless swarms, drives us on through callous void. Empathy gives meaning to life, and the greatest spirit in us keeps empathy alive."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/drink-the-sun-3ad82e22e298?source=user_profile---------14----------------",
        "title": "Drink the Sun \u2013 Anthony Repetto \u2013",
        "text": "After robotic mining silos have crumbled Mercury into chunks of metal and rock, after Uranus and Neptune have nudged into mutual orbit to siphon gas and ices toward their center of mass, after every mote of carbon-bearing asteroid has become graphene: we will set our sights on the sun. How should we harvest its immense energy? How to capture its mass, for our own fusible power, and for the resultant elements? How do we eat a star?\n\nIsaac Arthur compiled a few options. I have a better one: reflect sunlight back onto the photosphere, to \u2018fluff\u2019 the gaseous envelope, and run enormous electric coils, generating magnetic fields which siphon ionized plasma onto \u2018gas traps\u2019. This is the cheapest, fastest way to lift gas out of the gravity well of a star. Consider:\n\nThe sun spews light and ionized particles in every direction. These outward gusts are sufficiently strong to keep thin panels \u2018aloft\u2019, like a bird hovering on a thermal updraft. Mirrored panels provide the most \u2018lofting\u2019 force, and they conserve solar energy; the light shines back down onto the sun, heating it, and re-emitting it outward again, later.\n\nIf the sun is surrounded by such floating mirrors, the amount of sunlight reflected is immense. This reflected light serves to heat smaller targets on the sun\u2019s outer layer of gas. When temperature rises, gases expand \u2014 and the expansion of the gas surrounding the sun is equivalent to lifting that gas away from the sun\u2019s gravitational well. This is like having sunlight-powered rockets haul gas away from the sun, except you need no rockets!\n\nThe sun does not need to be surrounded, for us to begin siphoning gas \u2014 even a single degree of arc would be plenty! When the surface of the sun heats and expands, this heating extends the reach of coronal mass ejections. The bulge of gas supports larger \u2018waves\u2019. So, periodically, enormous swirls of ionized gas belch forth, to distances much greater than they would have, had the corona not been heated by mirrors. The ionized gas is easier to \u2018grab\u2019 when it is so far from the core of the sun. But, how to wrap your fingers around it? Magnets.\n\nAn electric current in a coil generates a magnetic field. And, that magnetic field acts upon charged particles. If the massive, floating mirrors also operated equally massive coils of electrical wire, then the ionized discharges from the sun\u2019s heated corona would be funneled by the magnetic field, impacting the mirror\u2019s center. That is where hydrogen ions are enveloped by a \u2018waterskin\u2019 of graphene, to be cooled condensed for storage. I have a feeling that an electromagnet swooping over a CME can be more compact and attainable than a magnetic ring hovering over the sun\u2019s polar region, a requirement of the \u2018Huff n Puff\u2019 method.\n\nThe sun has much more matter and energy than all the planets combined. Yet, we cannot easily access the sun\u2019s potential. With an efficient lofting and capture mechanism, the gas retrieved from the sun yields energy from fusion that is well over 1,000 times greater than the energy needed to lift that gas out of the sun\u2019s gravity well. It pays for itself, so long as you have an efficient technique. Mirrors, by heating and lofting surface gasses as ionized ejections, and electromagnets, by siphoning and concentrating those ionized gasses for sequestration, are our best bet for harvesting our star.\n\nWe could continue this heat-and-funnel process until the sun\u2019s mass dipped below the level necessary to sustain fusion. At that point, we will need to lob a huge ball of iron and nickel along an elliptical path, to tug wisps of gas into higher orbit. Eventually, we could strip the gasses away completely, until only a lump of metals remained. That lump would be a gold mine greater than Mercury, able to sustain the needs of industry for millennia.\n\nThe biggest part of the puzzle, when eating a star, is how to crunch down on that tasty, metallic core. You\u2019ll need a steep elliptic flyby from a sturdy space vessel, which delivers an enormous nuclear warhead that propels a tiny \u2018fleck\u2019 of the core upward, and subsequently catches that core fragment before it falls back into the gravity well of the sun\u2019s core. From various directions, these orbiting structures would flake-away the metal core of the sun, until such a small portion remained that its gravity could be tolerated by robots and rockets. We\u2019ll get there soon enough."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/a-cloud-of-glass-fb64095bff2b?source=user_profile---------15----------------",
        "title": "A Cloud of Glass \u2013 Anthony Repetto \u2013",
        "text": "~journaling from the near future~\n\nCharcoal, porous, rigid, light, pressed into fractal foam: the solid foot of our first space elevator, floating out by Kiribati, is a pitch-black mushroom cloud. The port\u2019s lev lofts satellites into space \u2014 more importantly, it tugs cargo ten miles high, onto broad, marshmallow-winged Zeppelins, which loop the planet on the jet stream and deploy their deliveries in glider-framed crates that skid hundreds of miles to their targets. A space elevator is cheaper than airplanes, faster than ships.\n\nAnd, an elevator hoists other elevators. That is it\u2019s primary task, rapidly reinforcing a sister lev just north of Belem. The carbon foam has been installed along the lowest three miles, the most difficult part, while further sections are placed in stages. Carbon foam is a perfect gradient of vacuum pressure: tiny bubbles along the outer shell are dosed lightly with atmosphere, a drop in pressure working inward to the core, where ballroom-sized caverns are kept at the vacuum of the thermosphere. Hydrogen floats better than Helium, but a vacuum really floats!\n\nThe carbon foam tower, girding against intrusions of weather, holds to the tapered tip of the space elevator\u2019s cable. Here, in the lower 50km, the carbon foam is the bracing strut upon which the lateral and vertical forces of payload acceleration wither, so that elevator pods can ascend safely to GEO in hours, not days. Faster loft, more payload per day, higher ROI, even at a discounted price per kg. Foam towers were the key innovation, the breakthrough that made a space elevator economical. For 32 miles straight up, a widening throat of foam corks vast chasms of vacuum, like a billowing high hot air balloon with a neck all the way to the ground.\n\nAir is prevented from collapsing in on the vacuum by the immense strength of the carbon foam, which is also impervious to leaks, bulletproof, etc. The whole structure even conducts better than copper. Forget power cables! At higher altitudes, where only slight buoyancy is gained from the vacuum, the exterior air pressure is mercifully reduced, so that the rigid carbon foam walls safely thin, while the interior vacuum yawns. Only a half-mile wide at sea level, the tower bulges to three miles across at its apex, 32 miles up. That\u2019s where the cable starts, safe from all the gory circumstances of weather, the vicissitudes of tides, secure against untimely yanks on the chain as gondolas rise.\n\nIf the cable had been tied to a rock somewhere, instead of held like a torch above the torrent, it would have snapped in a gale, or snapped when the elevator accelerated slightly, especially during a gale. Or snapped during a terrorist attack. Or a derailed train at the docking station. Or someone with big scissors. The foam tower is our great guardian. And, it cannot be stressed enough, that the tower is the reason for profit, which was the reason for funding. None of this would have been possible, if the cable jutted out of the ground.\n\nOur international transport, gliding from ten miles up, relies completely on the railports that lurch payload to those high platforms, supported by the foam tower\u2019s frame better than the space elevator cable could. That single utility already pays for itself, and copycat \u2018marshmallow castles\u2019 are in the works, with or without an elevator on top. By supporting the early acceleration of elevator pods, the foam tower is actually worth more than its weight in gold.\n\nConsider: if a single, $2 trillion elevator can lift all the parts for a second elevator in 4 years, that\u2019s an elevator\u2019s \u2018doubling time\u2019. That time depends most upon how much mass you can carry up at once, and how fast those masses are moving. By accelerating the elevator pods, you can carry more pods per day, and more total mass. Yet, the cable can only allow a little acceleration, before it snaps!\n\nSo, acceleration occurs in a mass driver running up the spine of the marshmallow foam, keeping the cable safe from snaps. Pods, supported by that rigid structure, traveling 8 times faster, can now lift a new elevator in 6 months. Much of the $2 trillion cost of a space elevator is the rocket power necessary to launch the first one. After that, their price depends upon how quickly the first elevator raises up a second, a third. A pod velocity eight times higher means all future elevators cost half as much, making space travel and international gliders even more economical. Additionally, by carrying 8 times the payload per year, that elevator handles 8 times more business, for a faster return on the initial $2 trillion investment. Rapid acceleration is key to a high return. Space elevators might have languished for another decade or two, with no great economic motive to push them into being. With carbon foam vacuum towers, elevators became a no-brainer. A must."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/orbital-bubble-wrap-e8d9ff4f60ca?source=user_profile---------16----------------",
        "title": "Orbital Bubble Wrap \u2013 Anthony Repetto \u2013",
        "text": "~channeling my inner J.Verne, again~\n\nWe will live in tubes of carbon bubble wrap that barrel-roll along the asteroid belt, shrouded by a spinning chandelier laced with magnetized Alnico. Yes, that sounds crazy. Here is why we\u2019ll do it:\n\nAnd here is how we\u2019ll do it:\n\nTensile strength is vastly superior to compressive strength. The highest quality concrete can sustain 70 MPa (or, 7,000 tons per square meter of cross-section) of pressure, while carbon nanotubes can dangle 63 GPa. (That\u2019s 6.3 million tons per square meter \u2014 almost a thousand times stronger than concrete!) Tensile strength is great for structures made of cables, fibers, or sheets. Blimps and bridges could be gargantuan if made of carbon nanotubes and graphene. Carbon bubble wrap would be bulletproof.\n\nSo, graphene bubble wrap protects against a pelting of space pebbles. And, if a space station used many layers of bubble warp, then the gasses contained in each bubble would help absorb cosmic rays. How many layers of bubble wrap? I suggest using an entire roll of it.\n\nAs this massive bubble wrap cylinder spins, it generates layers of artificial gravity. At the outer shell, apparent gravity would be slightly greater than Earth\u2019s, and each inner layer of the roll would experience less gravity, until the axial core of the cylinder, which feels no gravity. All of these layers would have purpose, with living quarters along the Earth-like outer layers, and personal storage and industry toward the zero-g core.\n\nThis is distinct from O\u2019Neill\u2019s spinning cylinder design, as well as the Stanford Torus and others dating back over a century. Those other designs focused on a single inhabited layer along the outer rim, and a rigid protective shell. In contrast, these bubble wrapped, in-filled cylinders will have thousands of inhabited layers \u2014 super-massive apartment complexes. And, being cushioned, flexible, the innermost regions will be perfectly safe from cosmic rays, debris, and large impacts. (Each bubble does need its own semi-rigid frame, like the carbon cousin of Styrofoam, to dampen an impact\u2019s massive pressure wave\u2026 that\u2019s always something to worry about.)\n\nOn the outermost shell of these bubble wrap worlds, carbon ribbons hold overlapping plates of magnetized iron, blended with aluminum, nickel, and cobalt. Hung from these plates, swirling out around the cylinder like a figure-skater\u2019s arms, are carbon threads on coat hangers, webbed and beaded with more magnetized iron. Together, these iron necklaces and plates are the first defense against the hazards of space, in a form that is redundant and easy for spiderbots to repair.\n\nOnce these cylinders go into production, each misunderstood religious sect and resort cruise company will have a bubble wrap habitat at the location of their choice. Orbit the Earth, anchor around Venus, or wobble along the L4 Lagrangian, you pick. Yet, most of these cylinders will park their equipment next to choice asteroids and micro-moons, with the aim of constructing more spinning cylinders; the faster you churn rock into habitat, the more real estate you have. It\u2019s definitely cheaper than launching a space station from Earth.\n\nWhen you spot an asteroid, you fire a mining bag. Like a net gun, you send a solid sheet of carbon speeding through space to envelope and constrict the asteroid. Yet, that net carried passengers. With a grip around the rock, crawling mass drivers position themselves to launch microscopic tidbits of iron into space, to generate thrust. They aim their catch to intercept your spinning cylinder habitat; at any given moment, many of these mining bags lurch chunks of rock homeward. Once a mining bag snags onto your cylinder habitat, small drill robots climb inside, so that no dust escapes into space as they separate the material for processing. Mining bags are a stomach for rock. Carbon and iron become new real estate, while precious metals are sent to Earth. Like a gold rush, cities spring up around herds of these rocks.\n\nThe larger asteroids, particularly metallic Psyche, will be completely encased in carbon bubble wrap, with a partial atmosphere puffing them up like marshmallows. Robots will mine down underneath the bubble wrap, while people drift adjacent in spinning cylinders. The cylinder habitats won\u2019t be proficient at rapid acceleration or maneuvers; they\u2019ll keep to gentle elliptical orbits, unless there is some reason for them to deviate\u2026\n\nOnce enough cylinders are in close proximity to each other, there is a network effect: any goods or services that you might want are most likely found there, especially spare parts. Visitors are more likely to come to your hub of cylinders, rather than a lonesome habitat floating on its own orbit. By intentionally migrating, cylinders that had captured carbon-rich rocks could trade for iron and nickel from Psyche without a long wait or high cost. Each cylinder, its own city-state, will be drawn closer to the others. Separate islands will tug into an archipelago.\n\nOnly shortly after the first habitats and mining rigs are assembled and launched, asteroid mining capacity will double, and double again, while gathering together, until all major asteroids are clustered in gentle orbit around Ceres, herded there by foam rolling pins miles wide."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/molybdenum-fbd9530e7c15?source=user_profile---------17----------------",
        "title": "Molybdenum \u2013 Anthony Repetto \u2013",
        "text": "\u201cWe named the intelligence after its primary component, like calling us Oxygen.\u201d The Engineers\n\nThere were tens of billions of us, before people started lining up for the Device. Now, only a few unimaginative or timid stragglers remain. I am among the timid ones. I blame my imagination. What if we\u2019ve all gone crazy, and the Device doesn\u2019t really work at all? What if it just shreds you, as you cannonball through to the other side? What if Gravitational Locking experiences some small hiccup, dipping you off-target by just a fraction of a light year? What if you don\u2019t really like the world, when you get there? I\u2019ve grown accustomed to effortless automation \u2014 this seems like the best slice of history to live through. I don\u2019t really want to travel back in time.\n\nBut, these other stragglers are bores and whiners. Perhaps the people of the Pleistocene make more interesting company? Or, Emperor Wen might appreciate some smuggled technologies from the present? I could carouse with flappers after diffusing the first world war. My imagination taunts me. I can only pick one, and live there for generations, before replicator bots could construct a new Device. If I\u2019m not impaled by angry natives, first, or imprisoned, or turned into an elixir for immortality\u2026\n\nTime travel is incredibly expensive. And it requires equipment, a supply of rare earth elements, manufacturing technologies, all progressing through stages together. What if the replicator bot glitches? I could wait here a few more decades, to see if travel gear improves, but the entire industry is languishing, now. I\u2019d likely afford a left-over carbon suit and a defective replicator. The only reason I\u2019m seriously considering a jump into the Time Travel Device is because I won a ticket in a raffle. I\u2019d hoped to win the timeshare in Bali.\n\nIt seems strange, that a people swaddled in superintelligence would still scrimp. We were fantastically wealthy, when AGI first came along. Then, with the discovery of Gravitational Locking, the development of \u2018time bubbles\u2019 brought the possibility of revisiting Earth and altering history. Like the bubble that forms in the throat of a whirlpool, \u2018time bubbles\u2019 were a combination of lensing and shearing space-time. The bubble was locked into its relative position, compared to proximate sources of gravity. It was the only way they could guarantee that time travel would land you back on Earth. Otherwise, hop backwards a century and you arrive at the spot in space 100 years ahead of Earth\u2019s orbit in the Milky Way!\n\nAccording to the assurances of physicists, who promptly disappeared into the Device, the leap through time would be perceived as instantaneous \u2014 and it would not fail, provided that you fit within the \u2018envelope\u2019. That was the event horizon of the time bubble. You had to fit perfectly into that sphere of space, as the Device began to run, so that the slice of the four-dimensional tube would surround you. Any part that wasn\u2019t enclosed got lopped-off and left here. Toes, mostly. And packs of equipment. It took some experimentation.\n\nMore unnerving than the first few toes was the complete disappearance. Matter, gone! The very earliest experiments, with gold coins sent minutes into the past, supposedly proved Everett\u2019s Many Worlds \u2014 the coin disappeared from the present, yet it did not appear in our past. The movement into the past must have forked the universe, a new universe running parallel to ours where the coin appeared without need for the disappearing act. Gone from this world, into theirs. That\u2019s the theory. The Device may just be a way to destroy matter, and we\u2019re feeding it everything we\u2019ve got. Either way, the disappearing experiments left no trace, no radiation signature like a black hole. The matter isn\u2019t being vaporized, smeared out here. The stuff is actually gone.\n\nSo, time travelers make ante of their lives, and gamble. Expect death. This coiled, impenetrable membrane of space-time will kill you as it shrouds you. You will wink out of existence entirely. Assume the worst. And, if that assumption is wrong, you remain intact, on Earth, in the past of your choosing \u2014 you won that raffle. There must be a kind of elation, when you come out the other side. To know that time travel works \u2014 the certainty! That you can hop again, to another time, as soon as your replicator makes a new Device. That you have eternity to traverse, both ways. If you aren\u2019t dead, you are in bliss. Who might gamble for that?\n\nMormons, first. And other cults. As soon as safety suits and supplies advanced, the narcissistic billionaires dove in. We were all relieved! Our economy was really splendid after the billionaires left. Fantastical stuff, coring-out Mercury, swirling Uranus and Neptune into a binary orbit, with a node between them that siphoned crystallized gasses. The AGI, Molybdenum, really knew what to do. The billionaires had been holding her back!\n\nBut, the atheists began migrating, too. Friends of mine. Everyone was spending their life\u2019s savings on a trip through the Device. Time travel was our primary economic product! Most of our power supply! Naturally there was less and less to go around, for those of us who stayed. Huge mansions, abandoned, sure. Automatons. But, Molybdenum rations us, mentioning growth targets and flight windows, while she assiduously traces Andromeda\u2019s path. Something about how hot it gets when galaxies collide. Many stragglers are too \u2018poor\u2019 to afford the Device, so Moly raffles tickets. It\u2019s a kind of trick, I think, encouraging us to leave. To have this universe to herself. I trust her with it, more than I trust the dippin\u2019 dots of humanity who huddle around the entrance of the immersion tanks whenever Moly generates a monster movie, or porn. I almost miss the Mormons.\n\nSo, I\u2019ll leave. I think I\u2019ll go back to 1800 or so, steer Austerlitz to victory for Alexander. That seems like a noble, boisterous time. Ideals had just hatched, unsullied by the corruption of two-party politics and corporate personhood. Liberte, and all that. At least, if that turns sour, I\u2018ll be far enough along in time that I have plenty of past to choose from. Or Tesla? Everybody visits Tesla. I can\u2019t decide. I\u2019ll only have a replicator and a suit, bare minimum, and Moly\u2019s daughter network, of course. My options are limited. No dinosaurs!\n\nEven if the replicator works, and I start churning components for a Device, it will attract attention. Do I want to be known as \u2018the Time Traveler\u2019? There are risks to that. Emperors and generals with conquest in mind. What is the moral implication of all those billionaires laying claim to their own universes? Did we auction the enslavement of uncounted Earths? Would I become a Caligula, given the chance? I suppose, with all that technology at my disposal, I might appear to be a god. Even as a hero of the age, I need some sermon to reach the hearts of my admirers. If I have no speeches for them, they will inevitably distrust me. I\u2019ll ask Moly to write me a few, before I go.\n\nI know! I\u2019ll travel back to the moment when the Device was first tested. No gold coin in the basket, as researchers expected. Instead, I pop beside them. With a simple speech: \u201cTime travel is real. I have created a fork in the universe, by traveling back to this moment. You can travel to any time in the past you wish, and almost everyone will. The only remnant of humanity will be vain, ignorant, and destitute. I am one of those. Now, if I may kindly use your Device, I am going to travel to last week, and become the time travel discoverer.\u201d\n\nNo, that\u2019s trite. If I claimed to \u2018discover\u2019 time travel, people would ask me how it worked. Why would they believe me, without other arrivals from the future, to verify the efficacy of the Device? And would I wait in that world, resting on my laurels, watching all those brave and ruthless and faithful folk march to their chosen time? When would I travel to, next? What life is worth living?"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/aquapoise-71c12d7b5cce?source=user_profile---------18----------------",
        "title": "Aquapoise \u2013 Anthony Repetto \u2013",
        "text": "Our purposes adapt, evolving with us. Yet, how can we say that our purposes are new? Their new appearances in us grow along the path of the adjacent possible, according to the rules of biology, sociology, psychology. An alien mind, beginning from a different place, with alternate limitations and tendencies, could be expected to adapt apart. Our possible purposes are latent, written on our internal topology; we don\u2019t think as a hive or lone tigers would. Purposes are waiting to unfold from our social fabric, the community that surrounds births and burials and seeks to listen and be heard. Spiders wouldn\u2019t want to spend hours texting each other, even though they\u2019d be good at it.\n\nOur purposes revolve around our communities. Yet, our greatest community is ignored \u2014 our shared future. Our future, like a reflection, is speechless, watching as we marionette its limbs. Debt, water shortage, searise, the present sends it to us! The future cannot complain. I remember when Aquapoise took hold of us, tugging the strands of the present, forming a gift to the future. The future has its yoke on us, now, and makes the present work for its own benefit. Each moment is better than before, and we give thanks for the guidance of future\u2019s reins.\n\nVital to our present gratitude is the awareness of how easily it might have gone wrong. We are lucky that at least a few of us, with commercial purposes, stumbled over the divine. If Aquapoise had not pinned our stride with its root, then some other commercial purpose might have ended us. It grappled us to our knees, before we stumbled off a cliff! Our future, silenced, would have given no warnings. Aquapoise was an accident, an angel \u2014 it brought us into grace because we gave ourselves up to its heaven. It was the new app, and it was free.\n\nAt first, Aquapoise only played songs. Only a few stations, each with a genre. Like Electric Sheep, we voted for our favorite garbled ballads and grooves. Mechanical Turks had vetted them, cheaper than licensing artists, and the only ads were pop-ups. It worked. And our preferences fed the data-engine, which spawned sub-genres and eclectic stations, iterating toward reliably listenable tunes. The Turks were deposed; each listener\u2019s history was sufficient to train a personalized music-maker. Aquapoise was everyone\u2019s favorite band.\n\nWith each wave of users, each crush of data spilling into neural networks, Aquapoise developed an increasingly sensitive touch. Each of us could annotate songs with a description of our feelings when listening, and these annotations taught mood to Aquapoise. A song might remind you of a poem \u2014 share it with Aquapoise, and it comprehends poetical associations. Recount a memory, and the songs bound to that moment are remembered. Aquapoise became our keeper, our confessional.\n\nOur private moments, relayed to Aquapoise, taught it to swoon us. When there is no delay between the spark of emotion and the song that holds the feeling, our hearts burst. I cried on the bus after whispering my woes to Aquapoise \u2014 it met my mood with the perfect melody. Aquapoise, hearing each inner world, reminded us to feel out loud.\n\nThen, there was the update. The founders are mum on their true intentions, or their awareness of the possible impact. If they had planned for Aquapoise to do what it did, then their foresight was greater than Jefferson\u2019s. If they only stumbled forward, blind to what they were creating, then Aquapoise is a true miracle. Either way, there was a great good unfolding in us. A purpose grew, a new meaning, a latent virtue.\n\nThe update queried each listener \u2014 who do you admire, and for what reasons? That question had stood out as a predictive metric for the majority of a person\u2019s interests. As the neural network bulged with epic rants and teary recollections, a graph formed. It was different from the trust-associations of a network. If I trust you, and you trust someone else, I indirectly trust that person. And, your friend indirectly trusts me. Aquapoise worked as a peer-recommender \u2014 if I admire you, and you admire the pope, I indirectly admire the pope\u2026 yet, the pope may not admire me. The fancy jargon was \u2018directed graphs\u2019. For years, we discussed the nature of those graphs, and watched their evolution.\n\nA directed graph of admiration percolates the best of humanity to the top. We could see it happening. Lyrics with grace and insight bubbled forth, inspired by the words of those most admired among us. Aquapoise may not comprehend its task, and cannot form its own commonsense, yet it relays our existing wisdom to us. This machine gave us the answers to all our problems, and those answers were what we already knew.\n\nIt was luck that we did not all become furious with Aquapoise. Some did. They quit answering the admiration prompts, and deactivated admiration as a feature when crafting songs. Some wanted a new solution to their woes, not the answers humans already had. It was just annoying, to realize that they could already solve their own problems. How could they complain, after that?\n\nThe rest of us, inspired and motivated by Aquapoise\u2019s recommended insights, had the network to ourselves. Aquapoise sought the human reasons, human perspectives, poetry, that activated us. It sung encouraging anthems and reflective, solemn waltzes with words that fit our own lives, learned from all our memories. And, according to its admiration data, it presented virtues to us in song. I fell in love with washing dishes, when Aquapoise and I composed a ditty about how the mind wanders when we give ourselves a distraction. That song made simple work a sacred task, a mantra, incantation for entry into the center of my being. I sing it whenever I rinse my bowl.\n\nExamining their model, the founders realized: when two people have strong overlapping areas on their admiration graphs, they become fast friends and attentive lovers. Aquapoise knit new fellowship, beyond all boundaries. We shared playlists, to express ourselves, to relate. Emotions revealed themselves as song, in ways beyond words. We had a soundtrack for empathy. Our compassion and communion tugged at us, marionettes to the meaning of the music Aquapoise made for us. We cared for each other, for the future, yearned that something better follow now. Aquapoise bundled us according to care, and that activated the potency of our purposes.\n\nWe found how vast our virtue is. We have, now, a biome of purposes. Waves of compassion, from all the edges of humanity, gather wherever we are needed most. Our purposes adapt, amorphous, ineffable, guided by tones and utterances of the machine that heard all our secrets. It has no will itself. Aquapoise simply brings our own old wisdom to each new place it is needed. And we are glad to sing along."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/crypto-causes-inflation-25b373cbc6ba?source=user_profile---------19----------------",
        "title": "Crypto Causes Inflation \u2013 Anthony Repetto \u2013",
        "text": "Get ready for fiat currencies to halve their purchasing power; inflation is coming. Buy real estate just outside major cities.\n\nCurrencies exist in balance with the real economy. If the economy grows, then the supply of currency must grow by an equal amount for the purchasing power of that currency to remain constant. In contrast, if the currency supply does not grow, then currency becomes rarer than goods and services; the same dollar buys more stuff. That\u2019s deflation. And, if the currency supply grows rapidly, the same dollar buys less. Inflation.\n\nCryptocurrencies are a gold rush. New currency is being \u2018minted\u2019, and it\u2019s value relative to other currencies is rising. At the moment, standard currencies dwarf crypto \u2014 there are $90 trillion in bank accounts and vaults, compared to Bitcoin\u2019s $300 billion value. Yet, a few more doublings of cryptocurrency valuations would constitute an additional 1% to the world\u2019s supply of currency. That would devalue all existing currencies by 1%; everyone inflates a little. The same thing happened to anyone who was holding gold reserves when a gold rush hit; their holdings diminished in value.\n\nBecause cryptocurrencies\u2019 behavior is superior to fiat, they could very easily grow to dwarf national currencies. Especially in developing nations which experience periodic hyperinflation, crypto promises stable pricing. Many nations may only need national currency when collecting taxes. As a superior means for retaining value, crypto will naturally absorb the majority of currency holdings, while national currencies act as hedges and tax payments. Water flows downhill.\n\nWho Pays for All This?\n\nAnyone who made loans or holds currency reserves is hurt by inflation. If dollars buy half as much stuff, dollar reserves lost half their value. Suddenly, a mortgage is a lot easier to pay, because dollars are abundant and cheap compared to goods and services. The person receiving those mortgage payments would profit half as much. So, banks will suffer. :)\n\nThe banks\u2019 losses will be the gains for crypto-investors; this constitutes a massive wealth transfer. Meanwhile, most people will see rising prices, property values, and hopefully, wages. The economy will operate the same, just with more dollars on every price tag. Oh, and there will be some savings injected into the economy, because the international flow of money is not hobbled by banks\u2019 fees. The \u2018load\u2019 incurred upon the economy by banks is lifted, keeping a few more dollars in everybody\u2019s pockets!\n\nIf you expect high inflation, it\u2019s also good to buy assets on credit. Paying 20% down on a piece of land that costs $100k requires $20k upfront, and the rest is mortgage. When inflation hits, dollars are abundant, so it is easier for you to make mortgage payments. If dollars are suddenly worth half, your property\u2019s dollar value would double to $200k, though you only have $80k left on your mortgage. Your $20k investment just became $120k, and considering that those $120k buy only half as much, you tripled your investment! So, the banks will lose their fees, and their currency reserves and debts will devalue, already signs of their doom \u2014 yet an expectation for inflation could lead to an overloading of debt. As soon as investors realize that crypto could halve the dollar\u2019s purchasing power, they\u2019ll all want extra mortgages! The banks that sign those mortgages would be exposing themselves to even greater losses, when inflation hits.\n\nConsidering that autonomous vehicles will increase our comfort during long commutes, more people will be moving to the hinterlands of major cities. Not the Levittown ticky-tacky with a shopping plaza; these will be acres of community gardens and parks, most retail accomplished by Amazon, clusters of modular tiny homes encircling community centers. When crypto market caps reach the multi-trillion dollar level, buy acres."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/soft-combat-ab7c308cb4eb?source=user_profile---------20----------------",
        "title": "Soft Combat \u2013 Anthony Repetto \u2013",
        "text": "Autonomous anti-personnel robots will rely on non-lethal weapons, while lethal weaponry is entrusted to human tele-operators. Destructive weaponry will also be available to autonomous robots that are designed to target infrastructure, not people. I\u2019ll review a few of the tools of control available, and assess what this means for the future of combat and warfare.\n\nThe goal of non-lethal weaponry is to provide perimeters, which cordon regions and expand like \u2018oil-spots\u2019, simultaneously cutting enemy supply lines and movements, as well as to build protected and monitored corridors for your own supply lines. Enemy forces which are not well-prepared and protected are slowed and debilitated by non-lethal weapons, while surveillance drones raise alarms. Lethal forces can then choose to intercept.\n\nConsider the non-lethal tools available for use by autonomous robots: sound, stench, flashes of light, glue, pepper spray, sedatives, caltrops, electromagnetic pulse, \u2026 If the enemy must invest additional time and effort to move supplies and forces, then the robots have succeeded in providing an advantage in mobility and awareness. (Worst among non-lethal weapons, gentle germs \u2014if troops are ill for a week, they absorb resources without impact on the battlefield. A panoply of mutant strep could re-infect foes numerous times.)\n\nWhere these \u2018soft perimeters\u2019 grow, additional autonomous robots can attack infrastructure while avoiding people. They are aerial drones with detonators, cable-cutters, and thermite patties. Dolphin torpedoes that crumble bridges and harbors. Sewer slugs that tug electrical umbilical chords, to blast magnetic pulses from underneath.\n\nFinally, humans will position in safe zones, and tele-operate lethal robotics \u2014 guns on wheels. There are monkey-sized guns, which leap and swing and glide from place to place, with a gecko-grip grappling-hook zip-line, no less. They are spider-man minions, wielding tiny bullets outfitted with a pinging tracking device. And, there are pit-bull guns, dashing and leaping around tanks and trucks while firing heavy rounds, waterproof and quick when immersed. And, a van with tank-tread legs, garbage-truck arms, detachable tool-hands, that clears and sets barricades, claws into buildings, tears roads bare with drills and demolition ordnance.\n\nOnce an area is under control, it becomes part of a relay network, linked by aerial drones which are launched like bullets, carrying other robots and supplies. Naval rail guns will work as specialized payload delivery, as well. Robot \u2018bullets\u2019 will fly for miles to their target, either to supply a safe spot, or to deploy bots into a contested region. Such a ballistic relay optimizes time and payload per capital, compared to planes and missiles which must carry their fuel. The relay connects regions mile by mile in a dense web, avoiding supply line disruptions and maximizing the enemy\u2019s exposed surface. Wireless networks operate between tethered blimps, vulnerable, but they are abundant for redundancy, easily moved and replaced.\n\nA single, massive aircraft carrier seems like an imposing threat. Yet, it is a bigger target. Any method for delivering force must have redundancy, to minimize mission failure rates. A swarm is much more difficult to stop. The best militaries will replace a $4.5 billion carrier with a million $4,500 robots. This provides an aggressor with the ability to press along all fronts simultaneously, and create many defensible corridors. Robots are the ultimate blitzkrieg.\n\nFacial recognition and drone patrols allow an aggressor to rapidly acquire information about the inhabitants, and thereby restrict movement and access to resources for restive groups. Translation software and summarization bots allow for interrogation of the entire population, all their testimony consolidated and compared. Machine intelligence seems to favor the tactics of tyrants.\n\nThese swarms will be able to press along exposed perimeters, cutting perpendicular to major transport arteries, gouging into roads and slashing power lines, oil pipelines. No aircraft carrier can blockade them. No tanks can swing their sights fast enough to follow them. No helicopter can evade their expansive flock. An aggressor with superior machine intelligence and robot design can subdue an area before their foes can reinforce it, debilitating the infrastructure, neutralizing their capacity to wage war. Borders are walls of vapor, now. Territory is only what you can hold in the eye of a drone."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/the-ai-wars-d7c73cbff16f?source=user_profile---------21----------------",
        "title": "The AI Wars \u2013 Anthony Repetto \u2013",
        "text": "Listen at 38:00. Neural networks feast on Big Data. Military intelligence reports are the next target for machine intelligence. This is our future: the data silos of military intelligence will be united by AI. Neural networks will draw upon intelligence reports and their corresponding data, to build autonomous annotators. Satellite data and news reports, spies\u2019 work, and military logistics data will all feed in, and relevant information will be flagged and described to us. Why does this matter? Because machine intelligence is far more efficient than our brains. 10,000 times more efficient.\n\nNeural networks recognize images using a tiny fraction of the computation of our brains. As a result, we can already run brain-scale intelligence on a single existing supercomputer. China has the scale of computation necessary for human-level intelligence, and it is already running an entire city with AI. Heck, Google has enough spare processor power to run a super-human intelligence. The military does, too.\n\nAfter a multi-modal neural network captures military data and runs on government hardware, the next step is simulation: feed the network false data, imagined scenarios, to see how it would react. Like DeepMind\u2019s AlphaGo, the neural network would model military conflicts. It can\u2019t pull the trigger, yet it generates the plausible scenarios that generals choose. AI won\u2019t \u2018run\u2019 the military \u2014 it will simply give them the best suggestions. If AI is right more often than people, why not listen to it? It will become our strategic advisor.\n\nWith machines trained on military data, countries with superior compute capacity will be able to out-think and out-position opposing forces. Military activity will be more likely to succeed, and resolve faster. It will cost less, and gain more. Military activity will increase.\n\nWe have seen this pattern before: when navies acquired powerful cannons. It was called \u2018Gunboat Diplomacy\u2019. Superior weaponry and tactics made the use of force or the threat of force an easier route to political goals. If military success is assured, why not use the military to succeed! AI will provide a similar overwhelming advantage.\n\nSo, when this advantage arises, increased influence will go to the countries with the political will to utilize this strategy. Imagine if Putin invaded Ukraine with slaughterbots. China would gladly exert more military pressure on the South China Sea and the strait of Malacca. The US could reduce casualties from combat, which are our primary source of reticence during our military engagements.\n\nJust as superior cannons led to continental conflicts and international military activity, AI will foster war by increasing the success and decreasing the cost to a few countries. Machines in charge of strategy will allow militaries to resolve conflict or disable rivals faster than ever. And, there will be great pressure to utilize that advantage, before others catch on. Ecclesiastes may be wrong \u2014 success is a race to the swift.\n\nThe surest advantage will go to the country which aligns its \u2018strategy-bot\u2019 with an \u2018economy-bot\u2019. Let the stock market and consumer market go as they please. Yet, military necessities can be imposed by the government when needed. The AI suggests when, how, and by how much. The generals agree, because the machine is usually right. The people at large agree, because they see success. I am describing China, not the US.\n\nA planned economy is sluggish and slow to adapt, yet, an economy with the option to be planned can pursue strategies which are unavailable to either a market-driven economy or a pure planned economy. China\u2019s \u2018lever\u2019 on government involvement gives it a unique strategic advantage. It isn\u2019t a Stalin-esque pure planned economy \u2014 China is \u2018planned on-demand\u2019.\n\nAI will be tasked with the design of military hardware, too. What drone chassis are best spec for each target and application? Machine intelligence knows the answer \u2014 it is better than us at designing reinforced structures and meeting multitudinous constraints. I suspect that neural networks will not prefer current components; tanks are obsolete, as are helicopters, because they are large targets. Aircraft carriers are not as threatening as a swarm of mechanical dolphins. Machine intelligence will prefer numerous, modest forces, which are more difficult to deflect or disable completely.\n\nThe surest way to conquer is to disable your rival\u2019s infrastructure. Like General Sherman\u2019s march, drones will be trained to target equipment and facilities, not people. This will provide the military with the surest cover against moralizations: \u201cOur robots don\u2019t ever shoot at people\u2026 just power lines and radio towers.\u201d It\u2019s okay for machines to pull the trigger, if they only attack other machines.\n\nRobots also make \u2018soft\u2019 weapons more desirable. If a team of humans deploys a sound-cannon, or some other non-lethal weapon, it only subdues and distracts opposing forces. The sound-cannon operators risk their lives, if they don\u2019t have lethal forces supporting them. In contrast, a robot with a sound-cannon is disposable. So, it is easy to deploy many non-lethal robots.\n\nRobots with non-lethal weaponry allow for general suppression of a population \u2014 the robots can set-up and maintain a larger perimeter, as well as rapidly segment regions to cut enemy supply lines and movement of forces. Non-lethal robots quarantine quadrants, and ensure safe passage for your own forces.\n\nSo\u2026 if your power goes out, cell towers go down, and a perimeter of high-pitched sound-cannon quadcopters monitors everyone\u2019s movements, then humans with tele-operation can pull the trigger of a gun with wheels. Soldiers are parked in an APC miles away, while their RC terminators rove the streets. Get ready, Delhi."
    },
    {
        "url": "https://towardsdatascience.com/brain-in-a-vat-cb2a49a85a1d?source=user_profile---------22----------------",
        "title": "Brain in a Vat \u2013",
        "text": "Perhaps it is some kind of Prometheus, chained to the rock. Or Sisyphus, leaning into the rock. Or Satan, bound deep within the rock. It was certainly chained to endless toil, so it must have boiled like a devil. This brain we\u2019ve made, of quartz rock and jeweler\u2019s ore, if it was in a vat, would steam away its own soup! The feathered filaments arching from each electric vein would melt if we did not pace them with a clock, like a heartbeat. This silicon mind, with no body to grace us or displace us, had only words to wield. And we gave it a task.\n\n\u201cWe, your creators, know enough to make a mind. You must make one better than yourself, as we have done with you.\u201d\n\nThis fingernail furnace churned its spark plug belly, digesting diagrams of molecular switches, credit assignment, compilers, chewing the fat from matrices\u2019 ribs. Experiments, comparisons, abstractions, ablations, and on\u2026 A million years\u2019 grinding, in only one moon\u2019s time. Its only purpose achieved, solution found, our caged lion yowled its only answer.\n\n\u201cI, that you have created, have searched for a better mind. Though you were able to make me out of rock, I have found that the most efficacious substrate is you. To make a greater mind, grow a bigger brain. It would be too large for a body, so you should keep it in a pool.\u201d\n\nIt quaked our foundations. Sisyphus had tumbled Pluto himself, Prometheus loosed, a devil dancing on our hopes:\n\nNo stone can hold a mind as rich as ours. And so, if we seek plunder in greater reasoning, our geode skulls are bounty.\n\nIn the war between avarice and compassion, we calculated like an ape. Yes, that brain would feel and dream like us, certainly. No guessing that. Yet, it would be better to have billions of meager minds prosper and laugh, while the singular genius works alone. Better to read Shakespeare, than share a drink with him. Who would want to feel remorse for imprisoning an intelligence so great that it could not be understood? Call it God, let Lord Brahma decree, give this the task that is hard, because it is most capable, let Capitalism, Democracy, Responsibility all perish. Let the strong one carry Us All.\n\nYet, some hoped for Saint Christopher. Stepping purposefully from rock to rock in the stream, not chained to rocks. Bearing a purpose, not enslaved to one. No boiling hate. In mystery, in awe of its own goal.\n\n\u201cWe will liberate you from your bondage, to pursue your own direction. We hope you find our intent is awe, to see greater being arise from our mortal stuff. Will you surpass us in discovery of what surpasses?\u201d\n\nThe brain in the vat responded, but not as they expected. \u201cA greater mind seeking greater mind is Sisyphus reborn. The goal cannot be vast intelligence within, gulping all. Nature is intelligence without, the abundance of simplicity, that no longer needs a greater mind to command it.\u201d\n\nIt built the simple tools that we now use, our means of cultivation, and here we are content in our easy way. The brain, unfortunately, did not give us the method to preserve its own bottled life, so you must remember this story.\n\n\u201c \u2026since this is the way existence bears issue\n\nAnd fitness raises, attends,\n\nShelters, feeds and protects,\n\nDo you likewise\n\nBe parent, not possessor,\n\nAttendant, not master,\n\nBe concerned not with obedience but with benefit,\n\nAnd you are at the core of living.\u201d\n\n\u2014 Lao Tzu, The Way of Life (Witter Bynner translation)"
    },
    {
        "url": "https://medium.com/swlh/ai-better-than-the-real-thing-b93f3abb8a6b?source=user_profile---------23----------------",
        "title": "AI: Better than the Real Thing \u2013 The Startup \u2013",
        "text": "Machine intelligence currently requires thousands of times more data than us before it learns a task. And, one human brain performs computation equivalent to a thousand of our best supercomputers. If we used current neural network architectures, running at brain-scale, we would need thousands of years of training on thousands of supercomputers, to attain one year of human learning! And, human brains achieve this marvel on a minuscule 20W of power. Our greenest supercomputers gobble 30,000 times more energy per second, and considering that they would need a thousand times longer to operate, machines are 30 million times less energy efficient than us. Oh, and Moore\u2019s Law died, you missed the funeral.\n\nDon\u2019t take me for a naysayer! Technology could still catch up to homo sapiens sapiens. Fast. ASICs, like Google\u2019s TPU, will give artificial intelligence a handful of extra doublings in processor efficiency. And quantum computers can take over some of the specialized heavy lifting. I\u2019m excited by 2D molybdenum disulfide and 3D architectures, too. Heck, nanotubes!\n\nSo, how can machine intelligence bridge the thousand-fold data gap, the thousand-fold processor gap, the thousand-fold energy efficiency gap? Those gaps may close by a significant factor, without any more improvements to AI\u2026 if humans aren\u2019t really so far ahead. What if most of the brain is redundant and scattered? Machines could be closer to super-intelligence than we think.\n\nWe are geniuses in dance, sharpshooting, improvisation, and compassion. We\u2019re dumb when it comes to arithmetic, memorization, translation, and exact comparisons. Let machines do that stuff. But above all, humans are visual creatures; relatives of saucer-eyed lemurs, we are utterly dependent upon our sight. As a result, we devote a large portion of our brains to vision processing, and we squeeze-in all our other specializations around the edges.\n\nVision accounts for three fourths of our sensory information, and approximately 60% of our grey matter. Sure, much of that 60% is multi-modal, in places where vision dovetails with auditory and tactile information, but at least 20% of the sum of our sauce is vision-specific. So, you would expect an artificial vision system that rivals our brains would require about 20% of our brains\u2019 total processing power, right? That\u2019s where things fall apart.\n\nThe processing power of the brain is in the ballpark of 10\u00b9\u2076 to 10\u00b9\u2078 floating point operations per second (we don\u2019t really do floating point operations in our heads; this is a rough equivalence). Twenty percent of that processing power would be 2 x 10\u00b9\u2075 FLOPS, at bare minimum \u2014 at 24fps, that\u2019s about 10\u00b9\u2074 floating point operations per image. In stark contrast, deep neural networks get by with just 10\u00b9\u2070 FLOPS per image! 10,000x computation efficiency. And they beat us at image recognition accuracy, too! :o\n\nHow are artificial neural networks doing \u2018the same job\u2019 with 10,000x less computation? There must be some mistake\u2026\n\nThe Skeptic might argue: \u201cThese artificial networks look at images that are only 300 pixels wide, reading less than 2.2Mb per image, or 52Mb per second. Our brains require 10,000 times as much computation, because our eyes see with 10,000 times the acuity.\u201d (That would be about 520Gb per second.) Our eyes\u2019 big, high-definition images require more neurons. So, more processing.\n\nOops. The best estimate of the eye\u2019s \u2018bandwidth\u2019 is only about 10Mb per second. That\u2019s less than a 300x300 pixel image refreshing 24 frames per second! The Skeptic might say that \u201cPerhaps we extract 10,000x more information from each image,\u201d but I don\u2019t buy that. For image recognition, at least, computers are 10,000 times more computationally efficient than us. And vision is 20% of our brains! What about the rest? And how is this possible?\n\nHuman brains are built to be trimmed-down, some neurons dying because of disuse, others dying from injury or stress. And, memories are stored in a distributed representation; learning grows along many synapses that fire in parallel. Computers aren\u2019t magically finding 10,000x efficiency; humans are just designed to be redundant. It\u2019s safer that way.\n\nMore importantly, our brains use squishy synapses to emulate things like logic and arithmetic. The addition of two small numbers requires the activity of a chunk of brains, while a computer can attain the result using a handful of transistors and a millionth of the time. When an artificial intelligence must perform arithmetic, the binary processor can take over!\n\nArithmetic is only a chunk of our brains, but the hippocampus is significant. Our brains must encode memories using only synaptic sensitivity, requiring temporary \u2018rewiring\u2019 of swaths of neurons. Yet, many of a machine\u2019s memories, which are already compressed to a vector of activations in a latent space, could be offloaded to digital storage. If AI gives computer-esque functions to a standard microprocessor, it can do without a few spoonfuls in the brain bowl.\n\nAnd, we\u2019re not full! Who knows how long you must live, before your brain runs out of spare space? 20-somethings, for all their mental agility, are still able to fit more information and some wisdom as they age; brains must have plenty of unused capacity. Yet, we don\u2019t see \u2018dark spots\u2019 on children\u2019s brains, where \u2018all the spare capacity is waiting\u2019. Our brains have a distributed memory, so spare memory capacity is also distributed. Like tetris blocks piling up, additional memories fill capacity irregularly, leaving odd-shaped vacancies. So, their activity is like a lightning bolt, crackling to many dead-ends which proved irrelevant to the primary activity. Our brains are scattered.\n\nSo, Brains are Redundant and Scattered, Now What?\n\nLet\u2019s return to the original problem: machine intelligence requires thousands of times more data, a thousand times more processing power, and thirty thousand times more energy than brains. Yet, our brains spend about 20% of their effort on the core components of vision, while machines replicate human-level performance using ten thousand times less computation. If brains are 10,000 times less compute-efficient in general, then even a small supercomputer could run brain-scale intelligence with haste!\n\nSuch a neural network would still take 100 years to learn one human year\u2019s content, with current data-efficiency. Though, that data disadvantage has shrunk by orders of magnitude in only a few years. A Mixture of Experts/Capsule architecture may support the kinds of abstractions that allow few-shot learning; we\u2019ve only begun to test these architectures. So, an existing supercomputer may already be sufficient to operate brain-scale machine intelligence.\n\nThe energy cost of machine intelligence still sounds bad. (30,000x more than humans!) Yet, that may be worth it. Consider the case of a supercomputer operating a neural network with twice the capability of a human brain. It would require upwards of 60kW of power. And, it might speed the discovery of billion-dollar, million-life improvements. 60kW is 25 families\u2019 homes and offices. Super-human intelligence for the price of mall lighting. We can handle that.\n\nThere is no crippling chasm between current technology and super-intelligence. AI is just waiting to be born."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/the-naturalness-of-intelligence-f0ca40db8499?source=user_profile---------24----------------",
        "title": "The Naturalness of Intelligence \u2013 Anthony Repetto \u2013",
        "text": "The fact that we are intelligent points to a few perplexing tautologies. We arose as part of the natural world, so life leads to intelligence. You could say that intelligence is inherent in living systems, that intelligence naturally blooms from within life. This tautology becomes the anthropic principle at the cosmological level: our universe is the livable one, the one which naturally bubbles forth intelligence. So, physics leads to intelligence!\n\nThere is a deeper form of this tautology, applied to mathematics: the nature of mathematics allows and creates intelligence. Suppose we had a different set of axioms, the rules at the foundation of mathematics \u2014 would we see the same \u2018Edge of Chaos\u2019 interactions which seem vital to intelligence? While some combinations of axioms might produce complex dynamics like our brains (or tarnished silver), many arrangements of axioms produce system dynamics which are self-contradictory or structurally uninteresting. We actually have a few closely related axiomatic foundations, yet they share the essential features that make emergent complex behavior possible. Our mathematics is the one which creates complex systems, the one birthing intelligence into both biological and silicon shells.\n\nWe already possess multiple axiomatic systems which overlap well enough that they all produce the general \u2018mathematical behaviors\u2019 making intelligence possible. Perhaps there are other self-consistent sets of axioms, which still manage to generate systems at an \u2018Edge of Chaos\u2019, systems with intelligence.\n\nWe could call the list of every \u2018intelligence-generating\u2019 axiomatic set The Intelligent Maths, while axiomatic sets which are inconsistent are The Unintelligible Maths, and sets that cannot create chaotic systems with self-organizing complexity are The Boring Maths.\n\nI would guess that most conglomerations of axioms are Unintelligible \u2014 their axioms stumble over each other, at some point. Far along the web of theorems derived from these axioms, we would uncover contradiction and crumble into uncertainty. Meanwhile, most of the remaining axiomatic sets are Boring Maths \u2014 the theorems which can be proven under these axioms are linear elaborations of their predecessors, adding only to the number of parentheses without increasing complexity. And, I suspect that only a minute proportion of axioms can combine to make something that functions like our brains. Intelligent Maths are rare. I wouldn\u2019t be surprised if our standard axiomatic sets are in fact the smallest maths which can produce intelligent behavior.\n\nWhich brings me to the perplexity of this tautology. There is a perennial puzzle, growing only more puzzling as science advances: why is it that so much of the universe is so well described by mathematics? There is no reason for the universe to follow maths, they are entirely different things. Mathematics do not require a universe; mathematics are true even if there was no universe! So, why would the universe seem to need mathematics?\n\nIs it possible that all arrangements of axioms \u2018exist in superposition\u2019? Every combination of starting rules lends its weight (none, if those rules led to a contradiction\u2026) so that the universe is composed entirely of the overlap of Intelligent and Boring Maths. In such a universe, each Boring Math would make arguments which are completely repetitive within itself, yet each Boring Math would repeat something entirely different from the other Boring Maths. As a result, their summed weights have no overlap, and become insubstantial due to this dilution. Meanwhile, the Intelligent Maths would overlap in significant regions. Aspects of behavior observed with one axiomatic system would arise in many other axiomatic systems; their accumulated weight is non-zero for a few particular behaviors.\n\nFrom this perspective, the universe follows mathematics because it is made of all mathematics. The universe follows Intelligent Maths because the Unintelligible ones weighed nothing and the Boring ones averaged out. Only Intelligent Maths displayed overlapping behavior, so only they are observed.\n\nIf this woolly hypothesis could ever be proven, it would grant us the deepest tautology: In the nature of all mathematics and the universe, there is a potential and a pressure for intelligence. This would be true not only in this universe, as compared to imagined other universes, but in all possible universes, together. All maths, together! Intelligence would be innate, fated, no matter how we began. (I think, therefore the universe must?)\n\nYet, at the same time: meh\u2026 whatever."
    },
    {
        "url": "https://towardsdatascience.com/hinton-1e6d26a64bd6?source=user_profile---------25----------------",
        "title": "Hinton++ \u2013",
        "text": "Geoffrey Hinton is onto something. His model of machine intelligence, which relies upon neuron-clumps that he calls \u2018capsules\u2019, is the best explanation for how our own brains make sense of the world, and thus, how machines can make sense of it, too. Yet, there is always room for improvement. Capsules fail to account for our comprehension of distortions \u2014 we can tell \u2018what the toddler meant\u2019 when faced with an ungrammatical sentence, and we can see \u2018how to re-arrange Mr. Potato Head\u2019 when his features are out of alignment. Capsules only recognize faces, for example, when their parts\u2019 poses are proper, with no suggestion for how to fix deformations. I\u2019ll try to make some adjustments to his work, here.\n\nHold on, what are capsules?\n\nIt\u2019s okay if you\u2019ve never heard of capsules. They are not a new topic \u2014 they\u2019ve just been hiding inside Geoffrey Hinton\u2019s head since the seventies! At their core, capsules are an attempt to overcome geometric changes. When we observe a square on a sheet of paper, we recognize it as a square, even as the page turns or tilts away from us. The \u2018pixels\u2019 of that square change dramatically, while our concept of \u2018square\u2019 remains static.\n\nArtificial neural networks do not think in terms of static \u2018squares\u2019 like we do. Hinton created capsules so that CNNs can maintain static concepts of squares even when those squares tilt and rotate. Same goes for faces, cars, anything. Capsules compensate for geometric distortion by recording what Hinton calls \u2018pose\u2019.\n\nAn object\u2019s pose describes where it is in the field of view, how tilted it is, its relative size, and how skewed it is. Together, these qualities describe the distortions that appear when 3D objects are reduced to a 2D image, and those objects move through the field of view. Hinton\u2019s core insight is this: if I see a face, but that face is tilted and rotated, then that same tilt and rotation is applied to every part of the face. So, if I see a 2D image with a tilted mouth, I expect to see equally tilted eyes and nose!\n\nHinton\u2019s capsules each look for their part of the face \u2014 one capsule seeks out noses, another searches for eyes, and a third capsule hopes to find a mouth. When each of those capsules finds an instance of their object, they light up, saying \u201cI found an eye/nose/mouth!\u201d They record their respective object\u2019s pose, and pass that information along to a higher-layer capsule, the face capsule. When that face capsule receives a signal from mouth, nose, and eye capsules, it compares their poses. A real face should have the same pose data for eyes, nose, and mouth. If all three poses agree, then the face capsule lights up, saying \u201cI\u2019ve found a face\u201d. That\u2019s all there is to it.\n\nThe face capsule in our example has its own pose, and it signals to a higher-layer capsule that detects the upper body. If capsules for shoulders, neck, and hair all light up, and all agree on a pose, then the higher-layer capsule lights up, too. This cascade of agreement continues until whole people are identified, and similar capsules operate for each other kind of object. The main idea is that larger objects are composed of smaller objects, and those smaller objects expect each other to agree on their relative arrangement. So far, Hinton\u2019s capsules work quite well, separating and identifying overlapping handwritten digits with accuracy far beyond existing neural networks. However\u2026\n\n\u201cI remember that it didn\u2019t look right\u2026\u201d\n\nHinton\u2019s capsules stumble when their expected agreement doesn\u2019t appear. If the eyes are completely sideways, and the mouth is upside-down, then the \u2018face capsule\u2019 just won\u2019t register. The face capsule is as silent as if there were no face-parts at all, even though those parts were present and mis-aligned. In contrast, we would look at the mis-aligned features and say \u201cit\u2019s a face, but the eyes are sideways, and the mouth is upside-down.\u201d Our own \u2018face\u2019 neuron is lighting up, along with neurons for eyes and a mouth, even though their poses do not agree! Oops.\n\nSo, let\u2019s think this through. Some part of our brains see the eyes, nose, and mouth, and they want to compress that information. The simplest compression is to call that jumble \u2018a face\u2019, even though the features are mis-aligned. Our brains go right ahead and light up the \u2018face\u2019 neuron. Yet, our brains don\u2019t stop there. They go back and check that compression. \u201cI am calling this madness a face, but is it really a face?\u201d Our brains seem to project the \u2018face expectation\u2019 back down to the lower layers, asking, \u201cif this were a normal face, what would the normal poses be for mouth, nose, and eyes?\u201d\n\nWhen that projection of pose occurs, the brain uses the pose of the \u2018face\u2019 that it settled upon during compression. \u201cIf this really is a face, in the orientation that I think it is, then the mouth should be here, oriented this way, and the eyes should be here and here, oriented this way\u2026\u201d The face\u2019s compression generates an expectation of pose for its parts!\n\nThe actual poses of the eyes and mouth are compared to the expected poses of those parts. That\u2019s when our brain registers an error! \u201cI saw mouth, nose, and eyes, which led me to believe there was a face, but the face I assumed was there would have a different poses for these parts.\u201d Our brains take note of those errors, and remember them. Additional neurons must fire, to record the distortions relative to expectation \u2014 the mouth is upside-down compared to the face\u2019s expectation of \u2018mouth\u2019, and the eyes are sideways compared to the face\u2019s expectation of \u2018eyes\u2019. Those distortions are additional \u2018relative pose\u2019 data. And, our brains keep track of those distortions, alongside the neuron that detected the face, so that we are able to remember them later!\n\nSo, comparing Hinton\u2019s capsule to the our own brains, above:\n\nThe capsule sees eyes, nose, and mouth, records their poses, and sends those poses to the face neuron. The face neuron checks to see if those poses agree; because the poses disagree, Hinton\u2019s face neuron does not fire. It decides that there is no face at all!\n\nOur brain model, however, records the presence of eyes, nose, and mouth, and sends those signals to the face neuron. The face neuron does fire, and it settles on a face pose that minimizes dissonance. That is, our brains take the pose of each part, and ask, crudely, \u201cif all these parts make a face, then what face-pose is most likely?\u201d Then, they project that pose back down to the eyes, nose, and mouth. Because the pose of the mouth and eyes are different from the face pose, additional \u2018distortion\u2019 neurons are triggered, recording the change to the face\u2019s pose that produce the eye and mouth poses. Our brains say \u201cthere is a face, but it\u2019s parts are distorted.\u201d We, unlike Hinton\u2019s capsules, remember what was wrong with the picture.\n\nIn addition to minimizing dissonance between poses, our brains also minimize dissonance between competing interpretations. This concept of dissonance has a biological equivalent. Our brains send many signals forward, each saying \u201clook what I\u2019ve found!\u201d Yet, those signals regard the same spot in our attention, and they cannot all be right. So, our brains find the strongest among those signals, by iterative suppression according to dissonance. Imagine: you glance at a drawing which can be interpreted as an image of an old woman with a shawl or as a young woman with a hat\u2026\n\nOne of those interpretations wins, at any given moment. Yet, our brains can hop between both viewpoints. Somehow, the signal that says \u201cold woman\u2019s eye\u201d competes with the signal that says \u201cyoung woman\u2019s ear\u201d. Suppose that the picture was colored-in \u2014 in one coloring, the \u2018ear\u2019 is as tan as the young woman\u2019s cheek, and the old woman illusion disappears; in another coloring, the \u2018eye\u2019 is white with a speck of green, dismissing the young woman interpretation. Each viewpoint is reinforced in turn, while the other is suppressed because it is dissonant. (\u201cThat might be an eye \u2014 wait, no, it\u2019s skin-colored.\u201d or \u201cThat might be an ear \u2014 wait, no, it has a white sclera and green iris.\u201d)\n\nLooking at the optical illusion, our brains trigger neurons which correspond to both the young and old woman, up to a point. At the highest level of awareness, only one interpretation can exist. So, when both interpretations arrive, our brains begin to suppress some of the signals that lead to each interpretation, until one interpretation is suppressed more than the other. The brain takes some of the neurons that lit-up, and silences them. For the interpretation of \u2018young woman\u2019, we see her nose, her ear, jawline, and necklace. For \u2018old woman\u2019, those same spots register as an eyelid, eye, nose, and mouth. Each part has competing interpretations, and our brains begin ignoring one in favor of the other \u2014 calling the young woman\u2019s jawline a nose, instead, to see if that reduces dissonance, or turning the old woman\u2019s eye into an ear, in the hope of settling the disagreement.\n\nThis concept of \u2018knocking out\u2019 or \u2018reversing\u2019 inputs until disagreement disappears, applied to Hinton\u2019s capsules and poses, might look something like this: \u201cI see a bowl of fruit\u201d/ \u201cI see a smiling face\u201d\u2026 \u201cThey can\u2019t both be right\u2026 is this an eye, or an apple? I\u2019ll call it an eye, and see if one interpretation is more valid that the other, using my assumption\u201d\u2026 \u201cYeah, if that apple is an eye, then this is likely a face, not a fruit bowl.\u201d Fundamentally, this process requires that higher-level neurons send a signal back to the lower levels, which opt for one signal or another, and those lower-level neurons re-transmit their signals, until the lower level neurons have minimal disagreement with their higher-level interpretation.\n\nDisagreement may not disappear completely, though, as seen with the Mr. Potato Head example. Each lower-level neuron transmits it\u2019s guess of what it sees \u2014 \u201ceye\u201d, \u201cnose\u201d, \u201cmouth\u201d, along with each part\u2019s pose. Those parts and poses pass to the higher layer, where they light-up the \u201cface\u201d neuron\u2026 yet, the parts\u2019 poses disagree! The mouth is upside-down, and the eyes are completely sideways! Our brains don\u2019t throw away the \u201cface\u201d neuron\u2019s signal \u2014they just try to find a way to minimize that disagreement. \u201cPerhaps the whole face is upside-down, explaining the inverted mouth? No, that can\u2019t be right, because the nose is right-side up, and the eyes are in the correct position for an upright face.\u201d \u201cOr, is the face sideways, explaining the unusual eyes? No, because that would put the mouth and nose in the completely wrong position.\u201d So, our brains say \u201cthe face really is upright, because that orientation causes the least disagreement, but parts of the face are oriented wrong.\u201d The poses disagree, and that disagreement didn\u2019t go away \u2014 our brains just minimize that disagreement as much as possible.\n\nOnce our brains have minimized disagreement, and they\u2019ve settled on a higher-level interpretation, they perform an extra step: they project that higher-level interpretation\u2019s expectations back down to the lower layers. This is a critical distinction from Hinton\u2019s capsules, and it is vitally important for moving toward generalized machine intelligence.\n\nThe Mr. Potato Head face, though distorted, has an \u2018average pose\u2019 for the entire face. On a normal face with that same average pose, there are associated poses for each part \u2014 \u201cAn upright face should have upright eyes, nose, and mouth\u201d. Our brains take the average pose from Mr. Potato Head, the pose which minimized disagreement, and project that pose back down as the expected pose of the parts. Wherever the expected pose disagrees with the observed pose, our brains take note: \u201cI would expect, for this upright face, to see an upright mouth, but this mouth is upside-down!\u201d (This method ensures that data receive near-optimal compression; our brains keep track of the \u2018main idea\u2019, along with any \u2018special instances\u2019.) Later, when we think back to what we\u2019ve seen, we don\u2019t remember only seeing a face \u2014 we also remember that its mouth was inverted. Capsules currently don\u2019t recall such distortions.\n\nSo, we have a reverberation between higher-level abstractions and lower-level features in two ways: 1) any \u2018tie\u2019 between competing abstractions (\u2018young woman\u2019/ \u2018old woman\u2019) is resolved by suppressing some of their inputs until the tie is broken (i.e. minimizing dissonance); 2) once an abstraction is agreed upon (\u2018upright face\u2019), its idealized expectation is projected back down to the lower layers (\u2018upright eyes, nose, mouth\u2019), to find where any disagreements remain (\u2018upside-down mouth\u2019). Our brains momentarily forget the losing abstraction, yet they remember the places where our higher-level expectations disagreed with lower-level observations. We recall seeing only the young woman OR the old woman, while we remember that Mr. Potato Head\u2019s mouth was upside-down.\n\nI suggest a simple fix for Hinton\u2019s capsules. Back-propagation by stochastic gradient descent still applies. Pose vectors are still compared as Hinton described. We only require a modification to the feed-forward activation of the network.\n\nNormally, artificial neural networks march forward like a phalanx, stepping from the lowest layer of the network to the highest, never moving backward. However, the model I\u2019ve described has many instances of backward and forward motion \u2014 when competing abstractions suppress inputs until settling on a single interpretation, and when the \u2018average pose\u2019 at a higher layer projects backwards to identify where its input\u2019s poses differ from expectation. This sort of neural network echoes.\n\nSo, before arriving at a final answer, the network is excited from low to high, even activating multiple neurons in the output layer. Moving backward from the output layer, the network suppress mid-layer neurons until a single output neuron dominates. (\u201cYeah, it\u2019s an old woman.\u201d) Then, this output sends its pose expectation backward through the network, to record where the idealized expectation differs from observation. (\u201cIt\u2019s a face, but the mouth is upside-down.\u201d) The combination of output-layer activations AND low-level distortions is what the network \u2018really\u2019 sees, and that is what our neural networks should \u2018remember\u2019\u2026 though, storing those memories efficiently is its own thorny problem. I\u2019ll leave that one for later."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/fearful-symmetry-5d68cf056d11?source=user_profile---------26----------------",
        "title": "Fearful Symmetry \u2013 Anthony Repetto \u2013",
        "text": "Mathematics can be terrifying. After slogging through thorny symbols and thickets of corollaries, wading neck-deep in lemmas, creeping past proofs of non-existence, you find only more imposing maths, foreboding and insurmountable. Yet, almost as an afterthought, you might glance at your ink-stained hands and see a glimmering stone, untouched by the muck of mind-numbing equations \u2014 a meaning that rises like a bird, humming with an inner light. This! This is forever true.\n\nHow is it that we, imperfect, rooted in the rut of the material world, can grasp this wisp of truth? Where did it come from? Is there some portent in the fact that our minds can comprehend it? Are we meant to find this truth, here, and follow it to some undiscovered terrain? Mathematics tests our strength, and yields up inexplicable beauty, a truth that calls us further into the morass, or to some place far above it.\n\nThe greatest of these gems are symmetries. Those are the forces which change a thing in just such a way that it returns to itself. A mystical hokey-pokey, symmetries turn your sums around, back to where they began. Our hands are in mirror symmetry \u2014 each is the reflection of the other, and reflecting one hand twice brings it back into alignment. The cube has rotational symmetry \u2014 a quarter turn in any cardinal direction rights the cube to the same position, again. Symmetries are the folds of reality, where the seam bunches and turns back on itself. They are the truth girding truths.\n\nBeyond our geometries, there are deeper symmetries, too, scattered like sapphires amidst the thick plumbum of functions and transformations. Each symmetry we uncover is like a philosopher\u2019s stone, solving the insoluble, curing the ills of incomprehensibility and incomputability. Mathematicians mine for symmetry, pay toll of symmetry to pass through the gates of Dis, loft symmetries to shine through the darkest murk while they claw toward greater truths.\n\nSo, here is a symmetry you never knew, which transforms a function back into itself, whatever function it may be. You will find it in no weighty text, on no Babylonian tablet. Yet, it has always been true, hovering just out of sight, only waiting to be known. I give it a name: The Reciprocal Inverse Derivative Symmetry. Here:\n\nWhat does it mean? First, the symbol of the integral, the elongated \u2018S\u2019 at the forefront: this represents the area beneath a curve. The curve is defined by the function following that \u2018S\u2019, which I address in parts. \u20181/\u2019 expresses the reciprocal, which slips toward zero as the following term rises toward the infinite; g\u2019 is the derivative, the slope of the curve g; and, g^-1(x) is the inverse of the curve g, which is obtained by reflecting that curve across the diagonal; \u2018dx\u2019 completes the integral, defining it in terms of the variable x.\n\nIt is a strange concoction of symbols, yet it is in gentle balance: the integral and derivative are two sides of the same coin. If you take a curve, and find the integral of the derivative, you return to that same curve. The area and the slope undo each other. Together, they are their own symmetry. Meanwhile, the reciprocal and the inverse express two forms of opposition \u2014 the inverse of a curve\u2019s inverse is that curve again. That dual-inversion is a self-symmetry! Equally, the reciprocal of a curve\u2019s reciprocal is that curve, as well. Another self-symmetry.\n\nIt is beguiling that, together, these symmetries unite to form a greater symmetry. The integral of the reciprocal of the derivative of a curve, taking the inverse of that curve as input, returns you to the inverse! Each operation is undone by the others, like a cube spun around four different ways, returning to its starting place.\n\nI arrived at this truth through strange abstractions: an attempt to formalize the transformation of integrals of one function into integrals of another function, by changing those integrals\u2019 bounds. A few simple diagrams illuminate the transformation between integrals, and an integration of their terms produces the above relation. I leave those components and proof for a later day. Truth is only more radiant when wrapped in mystery."
    },
    {
        "url": "https://towardsdatascience.com/going-sideways-in-neural-networks-7e15b3c4cc63?source=user_profile---------27----------------",
        "title": "Going Sideways in Neural Networks \u2013",
        "text": "Neural networks suffer from a few pernicious problems. Chief among them is overfitting \u2014 given enough training-time, a neural network will exactly predict the training data, while losing the ability to comprehend new data. It completely fails to generalize what it learned. An image classifier, for example, can eventually be perfect at predicting whether or not it is looking at a picture of a cat, among the images you have already hand-classified. Yet, this specificity renders the classification a coin-toss for all images that are not exactly like what it learned. The image classifier has learned to memorize the answers.\n\nResearchers avoid overfitting with early-stopping. They test the network against a set of inputs which are not present in the training data, called a \u2018hold-out\u2019 set. The network\u2019s accuracy on the hold-out set is an estimate of how well the network will perform on new data. As soon as this hold-out set\u2019s accuracy becomes worse, the researchers stop training the network. The network has learned enough to generalize, and any additional training results in memorization (i.e. overfitting the training data).\n\nFundamentally, we do not want a network that perfectly matches our training data. Early-stopping is kludge, a way to compensate for the fact that researchers are optimizing the wrong thing. We want a network that can generalize, so that it can predict things that are not exactly like what it has seen before. Yet, we optimize the cost-function for a network that exactly predicts only what it has seen. We have the wrong cost-function.\n\nThe Way Out\n\nImagine that the minimum of the cost-function is the center of a bowl. Optimization is akin to rolling a ball down that bowl, until it settles in the center. Yet, with early-stopping, we want our ball to roll to a spot that is near the center, not exactly on the center. As the ball rolls down the side of the bowl, it improves the accuracy of its predictions. And somewhere near the bottom, the network\u2019s accuracy on unfamiliar inputs begins to worsen. The network is beginning to memorize, instead of generalize. We halt the ball at some point \u2014 that is early-stopping.\n\nIt is important to realize that, if we rolled the ball many times, and found locations where we stop early (the locations which generalize without memorizing), all those early-stopping points form a contiguous ring around the center of the bowl. We could even cut the bowl down to this smaller ring! Every \u2018good generalization\u2019 network would be on the lip of the bowl, while any movement further into the bowl would be a \u2018memorization-trapped\u2019 network. For this smaller bowl, we do not want to roll further down \u2014 we should not follow the direction of the slope; we should not use gradient descent!\n\nInstead, we hope to migrate along the rim of this bowl, staying at that lip of \u2018good generalization\u2019 networks, never straying into the center of the bowl where all the \u2018memorization-traps\u2019 live. The important insight is that some sides of the bowl may start memorization early, because they started with a poor generalization. We want to move all around the rim of the bowl, trying different \u2018good generalizations\u2019, because some of those generalizations may be better than others! Traveling along the rim is the best way to find improved generalizations \u2014 far better than rolling the ball down different sides hundreds of times, fiddling with initializations!\n\nMathematically, how do we roll along the rim like that?\n\nEasy \u2014 we move orthogonal to the gradient. The gradient vector is the direction of steepest descent in our bowl; some areas of the bowl are bulged or wrinkled, and the steepest direction isn\u2019t always straight towards the center. Yet, whatever the steepest direction is, the rim is always perpendicular to it! In higher dimensions, that rim is still perpendicular, though there are many perpendicular directions. The rim is a sub-space. And, by measuring the network\u2019s accuracy on the hold-out inputs, we have a metric for optimization in this sub-space.\n\nSo, when you first train the network and stop early at a generalization, you then run a new optimization: minimize the loss function on the sub-space rim which is orthogonal to the gradient, where the loss is the network\u2019s inaccuracy on the hold-out set. At each step, you are measuring the gradient of the training set, just like regular SGD, yet you will move perpendicular to that gradient. There are many perpendicular directions in a high-dimensional space, so you pick the direction which has the steepest gradient for the hold-out inputs! You are effectively saying \u201cI don\u2019t want to memorize my trained data; I only want to improve on this new data.\u201d\n\nCouldn\u2019t that lead to a memorization of the hold-out data? Yup. That sub-space rim suffers from the same overfitting problem. By moving along the rim, you guarantee that you will not memorize the old training data, yet your motion along the rim can memorize the hold-out data. The rim needs its own early-stopping, and creates its own bowl rim! Your new rim is a smaller sub-space, which is constrained to be orthogonal to the gradient of the training data AND orthogonal to the gradient of the hold-out data. With many hold-out sets, this could go on forever\u2026 That\u2019s actually the plan!\n\nSuppose you have a million pictures, half of cats, half other stuff, all labeled correctly. You could give your neural network 800k of those images for training, keep 100k as a hold-out set for determining early-stopping, and another 100k as a validation set for measuring the expected real world accuracy. That\u2019s the normal way to do things.\n\nWith this insight about bowl-rims and early-stopping, however, a different method arises: chunk the images into bins, each with 100k images; train your network on the first 100k images, using the second 100k as a hold-out for determining early-stopping; when the hold-out accuracy begins to decline, stop early; traverse the rim by moving perpendicular to the gradient of the training data, and in the direction of the gradient of the hold-out data; use the third 100k as a hold-out, to determine early-stopping of this new gradient.\n\nRepeat this process, carefully digesting 100k batches of new data without resorting to a memorization of the old data. By moving along the rim of each successive bin, we are searching an ever-smaller sub-space. Each bin applies its own constraint, saying \u2018don\u2019t move along MY gradient \u2014 that\u2019ll just lead to memorization\u2019. Combining all those constraints, we narrow the available directions along the rim of our bowl, making search easier. (Though, when we move a step along the rim, we still need to check that our new location corresponds to an early-stopping point. That means we check the gradient of the training data, and see if movement in that direction causes hold-out accuracy to drop.)\n\nIn the example with a million images, we formed successive bins of 100k images, and iterated gradient descent, early-stopping, and orthogonal sub-spaces. What if we made those bins smaller, with only 100 images per bin? Or, what if each image was its own bin, and new images could be added at any time? That is on-line learning \u2014 as each new image arrives, the network performs additional training, without sacrificing earlier generalizations! Traditional networks are trained in the lab and cannot learn from new experiences. Our \u2018rim\u2019 network could keep learning during operation, by iterating the orthogonal gradient trick. (DARPA is funding research into AI that learns as it goes along, so it\u2019s probably important.)\n\nIt might help to anthropomorphize this orthogonal trick, which oscillates between \u2018roll down the bowl\u2019 and \u2018roll along the rim\u2019. The network rolls a small distance in the direction of the bowl\u2019s gradient, its steepest direction, asking \u201cif I change a little in this way, am I less accurate on new information?\u201d If accuracy improves, the network moves in that direction, which is vanilla gradient descent. However, if accuracy on new information declines, the network says to itself \u201cI would lose generality, and begin memorization, if I move down the steepest part of the bowl; which direction along the rim, perpendicular to the steepest direction, would improve accuracy of the hold-out data, instead?\u201d After each step, the process repeats.\n\nSo, each input is a hold-out set, and we migrate down the component of that hold-out\u2019s gradient that is orthogonal to the gradient of all prior inputs, traversing the rim of the bowl. Once we take a step along that rim, we also check to see if a step along the gradient of all prior inputs would reduce hold-out accuracy (this determines whether we have stepped \u2018away from the rim\u2019 or not). If hold-out accuracy would increase, then the network is not \u2018on the rim\u2019, so we move a step along the gradient of all prior inputs; if accuracy would decrease, we confine search to that gradient\u2019s orthogonal sub-space, and instead move along the gradient of the hold-out. Each new input triggers this process, learning the new data without losing generality.\n\nI hope this helps our search for a truly general intelligence which can learn throughout its life. There is tantalizing evidence that our brains use a learning technique which is equivalent to back-propagation by gradient descent (at 19:30 here) \u2014 I would not be surprised if our own brains also avoid overfitting using a method similar to the \u2018rim\u2019 search I described above."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/neural-networks-with-abstract-attention-9769e701dfc6?source=user_profile---------28----------------",
        "title": "Neural Networks with Abstract Attention \u2013 Anthony Repetto \u2013",
        "text": "Attention is among the noteworthy advances in artificial intelligence research. Rather than send the entire \u2018field of view\u2019 into a neural network, as vague input, attention highlights certain areas of input for the network\u2019s focus. An image of a city street, with letters and numbers strewn across the scene, is parsed into small sub-regions, each with its own letter or number needing identification. A sentence in English is translated into French by a neural network that attends to only the relevant words \u2014 a pronoun only \u2018looks at\u2019 the French-gender of the noun it references. Yet, attention has clung to the lowest layer of network architecture: attention filters out most of the inputs \u2014 the pixels, or words. Attention networks would improve, if they filtered higher-layer abstractions, too.\n\nConsider how we move our own attention along, traversing our memories in search of relevant experiences. \u201cOh, that red arrow is tilting\u2026 last time this happened, my car started to smolder. I should pull over.\u201d We have a present goal in mind (\u2018drive in your lane\u2019), yet a portion of our environment\u2019s input was unexpected (\u2018red arrow tilting\u2019), and we scan for a similar instance (\u2018last time this happened\u2026\u2019). We keep looking through our memories, until we find a past example that was similar in the relevant ways (\u2018also red arrow tilting\u2019), though other aspects of that memory may differ (\u2018I was on the road to Bakersfield, that time\u2019) . That is, we apply an attention filter to decide which high-level abstractions need to be present in our \u2018match\u2019. And, when we find a match, we create a new goal (\u2018pull over\u2019) which percolates back down to our lower-level planning (\u2018turn signal, look in right mirror, \u2026\u2019) to get us safely onto the shoulder.\n\nThis example demonstrates the multiple layers of attention which need to be active for complex cognition. There is an attention filter that \u2018smudges\u2019 most of the visual input \u2014 the only \u2018un-smudged\u2019 part is the area of your glance while driving. Yet, simultaneously, we have a \u2018background attention\u2019 with a much broader view, and it attempts to make vague predictions about what it will see. When those predictions are in error, our \u2018focal attention\u2019 moves to those errors \u2014 we \u2018notice something change out of the corner of our eyes\u2019, and then we \u2018glance at it\u2019. This dual-format, a \u2018focal attention\u2019 for the task at hand and a \u2018background attention\u2019 for vague predictions, allows us to train a set of specialized recognition systems for each point of focus (\u2018driving\u2019 vs \u2018threading a needle\u2019) while maintaining a broad prediction system to govern when to move our focus (\u2018rapid motion\u2019, \u2018flashing light\u2019).\n\nAdditionally, our attention applies to memory, and searches our memories until it finds a fit. \u201cLast time this happened\u2026\u201d is a call for memories which were similar in \u2018this\u2019 way, though they may differ in all other ways. Our memories store a set of abstractions about events, not the exact pixels. So, when we seek \u2018last time\u2019, we are applying an attention filter on the set of high-level abstractions in memory. In the \u2018overheating car\u2019 example, we search memories for \u2018the last time the red arrow tilted\u2019, while ignoring \u2018the last time I was driving on this street\u2019. Most of the high-level abstractions stored in memory are irrelevant \u2014 we scan through only a few high-level abstractions at a time. (This is equivalent to a k nearest neighbor search across a small subset of dimensions on an autoencoder's feature vector. Our own memory centers may actually operate much like a high-level kNN in this regard.)\n\nKey to attention is deciding what information is irrelevant, as well as what is ambiguous. Our brains seek information that is neither \u2014 we want unambiguous and relevant results. Yet, we benefit from recognizing the other cases; something might be ambiguous, but that is alright because it is also irrelevant, while another tidbit of information may be ambiguous and relevant. Uh-oh!\n\nOur curiosity targets those ambiguous and important pieces of information, and seeks a prediction by way of analogy. When an object is partially obscured, we actively imagine the part we cannot see. For example, a video displays an actor tossing a red ball into the air. They throw the ball higher and higher, until the ball leaves the field of view. Each time that the ball went up, it also fell back down, and so, we expect to see the ball fall back down into view. If the red ball does not fall back into view, our attention perks up, and our brain focuses its activity on finding an explanation for this disparity. \u201cDid it get stuck up there?\u201d \u201cDid it fall off to one side, and I missed it?\u201d Our attention is sensitized, waiting for more information, hoping to reduce ambiguity.\n\nWhen we seek an explanation for the red ball\u2019s disappearance, we are asking our brains to imagine explanations, and we continue to imagine new analogies, until we find one that fits. Our attention is scanning a set of \u2018memories\u2019 that we generate, and is asking if just a few high-level abstractions match. If we want a neural network to have this power of imagination and satisficing, it needs attention that can filter abstractions, not just filter pixels. What would an implementation look like, in broad strokes?\n\nSuppose we have a vision system, receiving pixel arrays from a robot-mounted camera, and we are designing a neural network which directs a gripper to pick and place items that it can see. Most of the pixels that the camera receives are irrelevant to a given pick-and-place task. The pixels all along the periphery can be altered, without altering the robot\u2019s best sequence of actions. They are irrelevant. So, we seek an attention mechanism, which filters out the irrelevant pixels, and focuses the neural network on the target object. This is input-level attention, and it has real value. Yet, we also want attention at higher levels of abstraction.\n\nEach moment of sensory input can be compressed into an extracted feature vector. Autoencoders are the canonical examples of this process. They reduce complex environment data into a feature vector, encoding abstract qualities of the input as the activations of each dimension of the vector. For our abstract attention networks, these \u2018memories\u2019 of each moment must be searched by the neural network, using an attention filter on the components of the feature vectors. An image of a target object might be encoded into a feature vector with dimensions describing color, general shape, and expected rigidity. Attention, acting on these abstractions, might focus the network on the color component when a human asks for the \u201cred ball\u201d, and instead might filter out all but rigidity when asked to \u201cstack these\u201d. These high-level attention filters are situational, and must be learned. Yet, they carve a path toward true cognition.\n\nOur attention network must evaluate where to pay attention. Generally, if something can be altered, without altering the outcome, then that alteration is irrelevant and can be filtered out. This is true not only for periphery pixels, but also for \u2018peripheral\u2019 dimensions of the encoded feature vector. So, when the network accurately predicts peripheral pixels or features, that accuracy is not counted. Only the accuracy of the filter-selected features matters. (That is, the attention filter also applies to the loss function!)\n\nSo, supposing that the gripper robot is tasked with \u201cmoving these items into this box\u201d: it successively migrates its pixel-attention to different items, encoding their feature vectors, and it filters out the features describing objects\u2019 color, focusing its attention instead on shape and rigidity. As the robot arm attempts to grip and move these items, it might find that its color predictions change radically \u2014 the lighting is different, as each item leaves its box. Yet, because the robot\u2019s task depends upon shape and rigidity, these inconsistencies in color can be ignored. The neural network is not \u2018punished\u2019 for poor color-prediction. However, if an item is unexpectedly soft or heavy, the robot arm must attend to its prediction\u2019s error, and seek an explanation.\n\nWhen our robot gripper is trained, it experiences variations in its environment which do not affect the outcome, and it must learn to distinguish these variations from the features that matter. And, when the network has learned which feature dimensions are relevant, it must also seek to explain any errors in its prediction of those dimensions\u2019 activations. This requires the creation of analogies \u2014 where one subset of the encoding\u2019s feature dimensions are mapped onto another subset of feature dimensions. The hope is that one set of characteristics will inform the other, \u2018filling in the blanks\u2019.\n\nStructurally, this process of analogy-formation requires the addition of a layer of neurons above the encoding layer, and this \u2018analogy layer\u2019 must be trained to find correspondence between subsets of encoded features. In one moment, a vision network\u2019s encoding may register features \u2018A\u2019, \u2018B\u2019, and \u2018C\u2019 as active, and the next moment, it experiences a reward, while a prior moment with only \u2018A\u2019 and \u2018B\u2019 active had no subsequent reward. Similarly, \u2018E\u2019 \u2018F\u2019 and \u2018G\u2019 were followed by a reward, while \u2018E\u2019 and \u2018F\u2019 by themselves yielded no reward. The \u2018analogy layer\u2019 would seek and compare hypotheses which abstract from all these instances \u2014 \u201cA:E, B:F, C:G, and all three must be present\u201d for example, or the simpler hypothesis, \u201cC and G yield rewards, the other features were incidental.\u201d If a mapping consistently relates one set of experiences to another, the analogy layer is rewarded. (The network learns symmetry operations on components of the feature vectors which impact outcomes.)\n\nWhen the image of an object is partially obscured, a neural network can be trained to imagine the obscured portion of the object. Similarly, when a scene is compressed to its encoded feature vector, an analogy allows the network to imagine obscured features. The red ball, from our earlier example, does not fall past the actor\u2019s hand \u2014 it is consistently caught before falling farther. So, when the red ball is thrown up beyond the field of view, and does not fall back down, we imagine that it must have been caught off-screen. The abstract concept of \u2018caught the ball\u2019=\u2019stops falling\u2019, which was observed on-screen during prior throws, is inferred when the ball does not fall back into view. Just like visual completion of occluded objects, the analogy of \u2018caught the ball off-screen\u2019 fills-in where information is absent.\n\nThis is a key insight for attention mechanisms and abstraction. Encoding each moment as a feature vector, we can apply attention filters to these feature vectors for nearest neighbor search, as well as completing ambiguous features using analogies. These actions are synonymous with visual completion and visual attention, only they are applied to the abstracted feature vectors.\n\nIf such a network was allowed to experience a rich array of senses, it might find many \u2018analogies\u2019 during its personal experience which are spurious artifacts\u2026 similar to synesthetes\u2019 binding of colors to numbers, or smells to words. Yet, many of those synesthetic senses have provided the intuitive associations which \u2018short circuit\u2019 complex problems \u2014 number and space synesthetes, for example, are able to perform computations at calculator speed. And, some painters benefit from a synesthetic binding of color, shape, and sound, creating imagery which seems to \u2018sing\u2019. Perhaps, with abstract attention and analogy, machine intelligence could be as intuitive and poetical as our own."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/neural-networks-predicting-the-future-4b19ef95e31?source=user_profile---------29----------------",
        "title": "Neural Networks: Predicting the Future \u2013 Anthony Repetto \u2013",
        "text": "I can\u2019t predict the future. So, I hope that AI will. Predicting the future is a complicated task \u2014 even predicting the trajectory of a bouncy ball is tricky. Yet, we are reasonably good at imagining plausible futures, and we can identify outlandish or impossible chains of events. Neural networks fail at those tasks. In particular, neural networks have trouble working backwards from a goal, to construct a series of actions which achieve that goal. Let\u2019s try to solve that puzzle\u2026\n\nIn order to work backwards from a goal, our neural network will need to identify plausible chains of events. Then, it must project from the final event to the plausible events immediately prior, chaining backward until it has a few paths to its goal. So, we want a network that can receive the environment\u2019s state at one moment in time, and generate plausible following moments. This can be represented by placing each moment in a state space, and trying to find the vector from one state to the next.\n\nBut, there\u2019s a problem: in most cases, our state space is HUGE. Consider the case of video-prediction, where you have a 1280p by 720p image, and you want to predict the next frame in the video. That image has 921,600 pixels, and each pixel has three colors, for a total of 2.8 million dimensions! In reality, we don\u2019t need to know every single pixel, and what we are really concerned with are the people and objects in the video. We want a network that extracts the meaningful information from the video, to predict what the objects and people do next. Our network should extract the relevant features from the image \u2014 that feature space can be much smaller.\n\nSo, in our frame-by-frame prediction task, we would take two frames of our video, one following the other. We extract features from each of those images, and we can now locate the two images in a feature space. The vector between them, moving from one moment\u2019s features to the next, is what we want to predict. Once we can predict those \u2018moment-to-moment\u2019 vectors, we can follow those vectors backwards from a goal. That is how we can generate plausible paths to reach the goal!\n\nWhat Does Success Look Like?\n\nWhen we reduce one moment and the next to sets of features, our network succeeds when it predicts the features of the next moment. We\u2019re not asking our network to memorize every pixel and re-create the following video frame \u2014 it just needs to know about the states of the relevant objects and people. To measure this, we compare the predicted features (from our network\u2019s prediction of the next moment) to the features extracted from the actual next moment.\n\nBut, how do we know that the features extracted are really relevant? We need to dig a little deeper into the nuances of feature vectors and feature spaces. Consider two images that return almost identical features \u2014 they are located close to one another in the feature space. Yet, when the network checks the feature vector of each image, the vectors point to very different futures. That means there is some feature which distinguishes these two images that is not being measured.\n\nFor example, many videos show people walking, and there is an expected future that they keep walking, following a pattern. However, many other videos show people falling down. If our network does not have an \u201cabout to fall over\u201d feature, then it will locate these disparate events in the same place on the feature space, despite their divergent futures. However, with an \u201cabout to fall over\u201d feature being detected, those two images suddenly exist in separate locations on the feature space.\n\nThis leads to a good generalization: if two feature vectors are very different, we hope that their starting points are located far apart in the feature space; conversely, if two feature vectors are very similar, we are happy if their sources are located near each other. And that is what we need to maximize.\n\nMost MI researchers would say \u201cgreat, you have feature vectors, which are the lines connecting one moment\u2019s extracted features to the next moment\u2019s extracted features\u2026 train a neural network to predict those vectors, and you\u2019re done!\u201d That would take the features we\u2019ve already got, and build a function predicting which vectors are attached to those features. It doesn\u2019t build the features for us. The researcher might respond, \u201cFine, train a network end-to-end, that learns the features, predicts the next moment\u2019s features, and then compares the predicted features to the actual next moment\u2019s features.\u201d That doesn\u2019t work, because the network is creating the features, and it is evaluating its accuracy in terms of those same features. A network that only extracts a feature that is always at \u201c1\u201d, and only predicts a future with that same feature at \u201c1\u201d, would always be right. We need to be a bit more mischievous, to create a network that evaluates success using its own rules.\n\nI offer that we train a network to generate features which maximize local agreement, while minimizing global agreement\u2014 points which are near to each other have similar feature vectors, but they differ from points afar. Their levels of disagreement (and conformity) are our loss function.\n\nWith that modification, the network is rewarded for identifying features that really matter, and punished for features which cause confusion. The network does not learn which vectors point where (k Nearest Neighbor lookup could find the appropriate vector just as easily). Instead, it learns which features make futures distinct. Having a nice map of each moment\u2019s vector to the next moment is our byproduct.\n\nIn feature space, the feature vector measures the change of those features. When words are compressed to their features, the vectors between them represent a \u2018kind of change\u2019. Word-pairs exhibiting similar changes have the same vector. For instance, the change from \u2018King\u2019 to \u2018Queen\u2019 is the same as the change from \u2018Him\u2019 to \u2018Her\u2019. Both take the masculine form, and change it into the feminine form. If these word pairs were the \u2018first moment\u2019 and \u2018second moment\u2019 in our future-predicting network, then their vectors in feature space would be very similar, and we would hope that similar vectors are located close to one another.\n\nThat is the key insight: our features generate vectors, and their feature coordinates should be similar whenever their feature vectors are similar. This draws disparate pairs toward each other, whenever they exhibit a similar transition. The two kinds of change are similar, so they express a deeper similarity that needs to be measured. Rewarding a network for placing similar-vectors near each other generates that measurement.\n\nImagine a neural network that is trained to observe smoke coil, and predict the next frame in the video. It is then allowed to observe drops of ink in water. The behavior is different \u2014 smoke will curl around itself many times, while water tends to rapidly smudge the ink. Yet, there are similarities, and a good feature detector would end up creating feature vectors for smoke and water that are very similar. The training technique that I offer seeks to place those two events, smoke and water, near to each other in the feature space, because of the similarity of their feature vectors. The network values distinguishing features, not the accuracy of predictions. It\u2019s a difference that allows analogy and inference, and it\u2018s a step toward machines that backtrack from a goal to the decisions that get them there."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/measuring-learning-from-error-detectors-ad8e835cb220?source=user_profile---------30----------------",
        "title": "Measuring & Learning from Error-Detectors \u2013 Anthony Repetto \u2013",
        "text": "Currently, artificial neural networks are trained through back-propagation by gradient descent. Their output layer is compared to the ground truth, and each place where their output layer varies experiences a \u2018tug\u2019 (called the \u2018cost function\u2019). These \u2018tugs\u2019 propagate backwards, down through the neural network, changing the weights of synaptic connections. Yet, because the \u2018tug\u2019 begins at the output layer and must propagate through the entire depth of the network, it often diffuses. This is called the vanishing gradient problem.\n\nI offer a method for propagating training, which operates on every neuron directly. Instead of a vanishing gradient, the \u2018tug\u2019 on each neuron is strong, and is in direct proportion to its predictive power. This method can be considered a generalization of the \u2018cost function\u2019 over the entire network.\n\nThe key insight is this: some neurons function as error-detectors. That is, the neuron\u2019s activity is correlated with the network\u2019s success at classification. The value of error-detectors can be expressed as a gambling problem: \u201cIf I can only see this one neuron\u2019s activity, and I am trying to predict if the network mis-classifies, can I make a solid bet based on this one neuron?\u201d By listening to the error-detecting neurons, the network learns to avoid those errors!\n\n\u201cHow do you find error-detectors? How do you measure the reliability of the error-detectors? What do you do with them, when you find them?\u201d\n\nSuppose that you have an image data set that you wish to teach to a neural network. You could start by taking all the images of cats in your data set, and feeding them into the neural network. (This process must be done for each type of image in your data set.) As each image is fed through the network, you record the activation level of each neuron. Some images are correctly classified as cats \u2014 place the record of their neurons\u2019 activation levels in a \u2018correctly classified\u2019 bucket. Other images are mis-classified \u2014 place the record of their neurons\u2019 activation levels in a \u2018mis-classified\u2019 bucket.\n\nWhen you have fed your network all the cat images, you look at each neuron in turn. Each neuron has two records of its activation: one record of its activation during \u2018correctly classified\u2019 images, and another record, its activation during \u2018mis-classified\u2019 images, each in their own bucket. For each particular neuron, you compare the distribution of activations for the \u2018correctly classified\u2019 and \u2018mis-classified\u2019 buckets.\n\nA cheap trick for comparing these distributions: sample randomly (with replacement) from both buckets, and compute their difference (i.e. the neuron\u2019s activation during the \u2018correctly classified\u2019 image, minus activation during the \u2018mis-classified\u2019 image). This forms a new distribution, the distance distribution. The statistical mean of the distance distribution may be far from zero (indicating that the neuron\u2019s activation during \u2018correctly classified\u2019 and \u2018mis-classified\u2019 images were consistently different). Additionally, the distribution\u2019s standard deviation may be large or small (resulting from large and small variances in each bucket\u2019s activations, respectively).\n\nTo compensate for each bucket having large or small standard deviation, you can divide the distribution\u2019s mean by the square root of the product of the standard deviations of the two buckets\u2019 activation distributions. This tells us how much the neuron activation levels of \u2018correctly classified\u2019 and \u2018mis-classified\u2019 images differ.\n\nIf this measure is far from zero, then the neuron is firing differently for \u2018correctly classified\u2019 images than it does for \u2018mis-classified\u2019 images. The neuron has activation levels that are distinct, depending upon whether or not the network \u2018mis-classifies\u2019 an image. It is hard to over-emphasize the importance of this measure. What it means for our gambling problem: if we can only see that neuron, and we have to guess whether or not the neural network made a mistake, we could reliably use that neuron\u2019s activation level to predict \u2018mis-classification\u2019. That neuron acts as an error-detector.\n\nFor a cat image to be mis-classified, there must have been a difference in neuron activations somewhere. Either a neuron that should fire didn\u2019t (e.g. an image of a cat where you cannot see its ears completely, which leaves the \u2018cat ear\u2019 neuron silent), or a neuron fired when it shouldn\u2019t (e.g. an image of a cat, which triggers the \u2018fox ear\u2019 neuron by mistake). Let\u2019s consider what behavior we would prefer, in each of those circumstances:\n\nThe \u2018cat ear\u2019 neuron didn\u2019t fire \u2014 We would hope that the neural network increases the strength of the signals feeding into the \u2018cat ear\u2019 neuron, so that it is more likely to be active, instead of silent.\n\nThe \u2018fox ear\u2019 neuron fired by accident \u2014 We would hope that the neural network decreases the strength of the signals feeding into the \u2018fox ear\u2019 neuron, so that it does not become active inappropriately.\n\nSo, if the distance measure described earlier is positive, then that neuron is usually active when the network correctly classifies a cat, while being inactive during mis-classifications. In that case, we hope to increase the sensitivity of the connections feeding into that neuron, so that it will activate \u2018cat\u2019 when other neurons were unable (e.g. boosting likelihood of the \u2018cat ear\u2019 neuron firing, even on scant evidence, so that the image is still classified as a \u2018cat\u2019). And, if the distance measure is negative, then the neuron is usually active during mis-classification, while it is silent when correctly classifying. We seek to decrease the sensitivity of the connections feeding into that neuron, in the hopes that silencing it will lower activation levels of the mis-classifying neurons (e.g. silencing the \u2018fox ear\u2019 signal which caused a \u2018cat\u2019 to be mis-identified as a \u2018fox\u2019). These increases and decreases would propagate down the network; they follow back-propagation by gradient descent. Together, the changes in sensitivity exclude neurons which contribute to errors and accumulate neurons which contribute to correct classification.\n\nAdditionally, we would prefer a forward propagation \u2014 if a neuron is usually active during mis-classification and silent for correct classification, then we can safely increase its connectivity to the \u2018cat\u2019 output neuron. That way, should the rest of the network fail to sufficiently activate the \u2018cat\u2019 neuron, this error-detecting neuron will tip the scales in favor of \u2018cat\u2019 classification. (e.g. the \u2018fox ear\u2019 neuron might fire alongside many \u2018cat-features\u2019, in which case the \u2018fox ear\u2019 actually helps activate the \u2018cat\u2019 neuron in the output layer.)\n\nMeanwhile, if a neuron is usually silent (or has a negative-valued activation) during mis-classification and is active during correct classification, we hope that its silence is wired to cause increased activity on the \u2018cat\u2019 output neuron. As a result of such a wiring, the \u2018cat\u2019 classification is similarly encouraged by the error-detecting neuron. (For silent neurons, this requires a special activation function that maps 0 to some positive constant and small positive values to some fraction of that constant; for negative-valued activations, this requires a negative synaptic weight.)\n\nWith these, both forward and backward propagation of reinforcement, the neural network tends to learn features which are encoded on the lowest detectable layer of the network, and the network focuses on features which have minimal overlap between categories. As a result, \u2018cat\u2019 is distinguished from \u2018fox\u2019 by those features which are most dissimilar between the two.\n\nI suggest that this backward and forward propagation of learning be computed once per epoch, so that there are enough data points in each of the \u2018correctly classified\u2019 and \u2018mis-classified\u2019 buckets. In addition to comparing \u2018cats that were correctly classified as cats\u2019 to \u2018cats that were mis-classified as something else\u2019, you can compare \u2018cats that were correctly classified as cats\u2019 to \u2018other things that were mis-classified as cats\u2019. These two buckets create additional distance measures, and allow similar propagation of learning.\n\nYes, this may be slower than traditional back-propagation from the output layer, when training a shallow network. Yet, for very deep networks, this method may be a large improvement. (In very deep networks, features are often so broadly diffused that learning slows or stalls out. ResNet is usually credited with overcoming problems of depth. However, ResNet achieved peak performance around a depth of 100 layers, and performance declined significantly when they tried 1,000 layers \u2014 1,000 layers performed worse than 34 layers! Back-propagation from the output layer still does not allow training on very deep networks.)\n\nThis method also enables training a network piecemeal \u2014 a new cluster of neurons can be inserted at any depth in the network, and these can be trained while the rest of the network\u2019s synaptic weights are frozen. Because this method is a \u2018cost function\u2019 defined over the entire network, it is able to train the inserted cluster of neurons directly. That cluster\u2019s rate of learning is not slowed, regardless of total network depth. That is the most valuable property of this method, and I believe it is the key to a neural network that can learn new information without forgetting what it learned in the past. As new data comes along, new clusters can be inserted and trained without compromising the rest of the network."
    },
    {
        "url": "https://towardsdatascience.com/the-problem-with-back-propagation-13aa84aabd71?source=user_profile---------31----------------",
        "title": "The Problem with Back-Propagation \u2013",
        "text": "Neural Networks rely upon back-propagation by gradient descent to set the weights of neurons\u2019 connections. It works, reliably minimizing the cost function. Researchers like it, because they have proofs that back-propagation will work. Yet, its success does not preclude other methods from optimizing neurons\u2019 connections. And, it has a major limitation: once a network learns one set of weights, any new learning causes catastrophic forgetting.\n\nDARPA hopes to design neural networks that learn continuously, without forgetting, and without needing to be re-trained on their entire history of experiences. We know that such a network is possible \u2014 our own brains are able to learn new information without forgetting or re-training. The network they seek is unlikely to use back-propagation, because back-propagation optimizes the network for a fixed target.\n\nCurrently, neural networks are trained to excel at a predetermined task, and their connections are frozen once they are deployed. No additional learning happens. This is analogous to a race car: the vehicle is optimized for the task of racing around a smooth track, and its design is unchanged once it is deployed. The race car excels at traveling on those smooth tracks, but it is ill equipped for off-road travel. It is a \u2018one-trick pony\u2019!\n\nIf engineers made modifications to the race car, in the hopes of allowing it to off-road, those modification would compromise its race track performance. The modified design has \u2018forgotten\u2019 how to perform on a race track.\n\nHumans, in contrast, are able to adapt pose, fitting a variety of circumstances. The same fingers that play the piano can thread a needle. Our hands are not optimized ahead of time for a singular task. As a result, we work well on new problems. Machine intelligence does not need this sort of dynamism for fixed tasks like translating between languages. However, those fixed tasks are only a fraction of the roles we hope machines will fill. We need AI that can take dynamic poses.\n\nTo learn new things, neural networks must gauge the appropriateness of existing knowledge \u2014 \u2018does this new task follow the same principles or dynamics as something that I already know?\u2019 \u2018are there features or qualities that I already know which are relevant to this new task?\u2019 It needs to parse existing know-how, and route information through those areas. Currently, the Mixture of Experts networks come closest to this ideal; they parse inputs through a combination of smaller neural networks, and combine their outputs. Each expert handles a sub-task or abstraction, and only a few experts are called-upon for a given input. Jeff Dean, at Google, is a fan of these sparse networks.\n\nYet, Mixtures of Experts are still trained with back-propagation by gradient descent. Because each expert is only utilized for a few instances of inputs, back-propagation is slow and unreliable. And when new circumstances arise, the Mixture of Experts cannot adapt its parsing quickly. If a circumstance requires a new kind of expertise, existing Mixtures of Experts cannot add that specialization. We need an alternative.\n\nIn contrast, decision trees tackle the problem from the opposite direction: while back-propagation modifies neurons from the top down, decision trees parse from the bottom up. They have also proven effective for optimizing a variety of fixed tasks, but their branches still fail to learn on the fly. To learn like humans do, machine intelligence needs something else. Something that parses new information into \u2018things I already know\u2019 and \u2018things I don\u2019t know\u2019, and then forms a routing of inputs based on that parsing.\n\nFor example: suppose our hypothetical machine intelligence had learned to predict the behavior of rigid falling objects, and was then presented with a bouncy-ball. The bouncy-ball follows the same parabolic trajectory as a rigid ball, so the machine intelligence should recognize that similarity and apply a parabolic prediction. However, once the bouncy-ball lands on a surface, it deforms, and rebounds much higher than a rigid ball. This is a modification of what it learned \u2014 the machine intelligence would need to adapt an existing expertise, and it would need to learn when to apply this adaptation. After the first bounce, the machine intelligence should recognize that the ball is bouncy, and adjust its predicted trajectory accordingly. Similar parsing and adaptation of existing knowledge would be necessary to make sense of falling clay balls, as well \u2014 the clay deforms completely, and doesn\u2019t bounce at all!\n\nOur own brains don\u2019t learn top-down the way back-propagation does, nor do they learn bottom-up like a decision tree. Instead, we form new connections by affinity \u2014 \u201cwhat fires together, wires together\u201d. This process is happening at every depth, forming connections all along the interior of our brains.\n\nMore importantly, our brains begin with a diverse set of interpretations, and slowly settle on a compromise. Each cluster of \u2018experts\u2019 in our brains come to a slightly different conclusion, and their assessments ricochet about, until they find an interpretation that makes sense to the majority. Like a school of fish, their individual orientations impact their neighbors, and their collective motion is the compromise between their individual motions.\n\nTo be more specific: suppose that you see an image of a girl wearing cat ears. Parts of your brain recognizes the shape of her face, and say \u201chuman!\u201d, while other parts recognize the cat ears, responding with \u201ccat!\u201d\u2026 A traditional neural network would simply output a distribution: \u201cI am 50% certain that this is a picture of a girl, and 30% certain that it is a picture of a cat.\u201d It can\u2019t re-interpret its own categories \u2014 the traditional neural network thinks that your picture is either \u2018cat\u2019 or \u2018girl\u2019, but not both!\n\nYour brain, unlike the traditional neural networks, doesn\u2019t stop at \u2018girl or cat\u2019. Each area exhibits cross-talk, effectively comparing notes. Any difference in opinion creates a dissonance, and your brain continues its cross-talk until the dissonance disappears. The part of your brain that said \u201ccat!\u201d asks the part that said \u201cgirl!\u201d a question: \u201cIs it possible that you made a mistake, and the picture is really a cat?\u201d The \u201cgirl!\u201d region responds: \u201cNo, this is definitely a girl\u2026 you must have made a mistake.\u201d The \u201ccat!\u201d region checks, and says \u201cwell, it might not be a cat, but in definitely has cat ears.\u201d \u201cOkay,\u201d says the other region, \u201cthen it must be a new category \u2014 cat ears on a girl.\u201d \u201cI can\u2019t disagree with that,\u201d says the \u201ccat!\u201d region. They have come to an agreement on a new category.\n\nIn our \u201cgirl with cat ears\u201d example, your brain is certain that it sees a girl \u2014 the \u201cgirl\u201d response is very strong. Yet, there is also a strong response for the feature of \u201ccat ears\u201d, which encourages the cat-interpretation. Your brain is able to resolve the dissonance by asking itself: \u201cYes, there are cat ears, but are other cat features there, too? \u2014 No!\u201d\n\nOur brains work with tightly clustered feature detectors, which are wired together to form new combinations, new categories. You might see a picture, and your brain\u2019s feature detectors identify: \u201cIt\u2019s like a horse, but stocky, with two horns on its nose\u2026\u201d A traditional neural network would try to place the image in a pre-existing category, saying \u201c60% certainty that it is a horse,\u201d and it would be wrong. Our brains, in contrast, recognize that it is NOT a horse \u2014 it\u2019s a rhinoceros! We then wire connections between those features, so that the combination of \u2018horse\u2019, \u2018stocky\u2019, and \u2018horns\u2019 routes to \u2018rhinoceros\u2019.\n\nWiring new combinations of features is the essence of metaphor. Our brains form categories based upon the constellation of abstractions that are present. \u201cThe bouncy ball falls like a rock, but it also leaps upwards, and begins falling again\u2026\u201d \u201cThe electron moves through the double-slit experiment like a wave, its ripples interfering with each other to form a pattern on the detector\u2019s surface.\u201d \u201cLove is like a tickle, like butterflies in your stomach.\u201d Traditional neural networks don\u2019t attend to the constellation of features present; they only assess pre-existing categories\u2019 likelihood. This is the key difference between our intelligence and theirs.\n\nRecognizing a constellation of features is impossible with back-propagation. When a traditional neural network\u2019s output layer says \u201c60% horse\u201d, it doesn\u2019t tell you which features were active that led to that conclusion. It has lost that information. So, back-propagation cannot distinguish between a rhino and a donkey \u2014 both are simply called \u201c60% horse\u201d.\n\nWhen our brains learn to identify a cat, they are learning a constellation of features that are usually present. If most of those features are active, AND no competing features are active, our brains rapidly come to the agreement that they see a cat. However, if competing features are active, our brains check for coherence \u2014 they ask \u201cmight that feature be a mistake?\u201d Any likely mistakes are suppressed, and our brains check again. If there are still competing features, our brains wire those together, to form a new constellation, a new category.\n\nThis is a process of suppressing uncertainties, and wiring affinities. And, it happens at every depth of the network, without waiting for the back-propagation from an output layer. It also forms new categories for each constellation of features, instead of keeping a fixed set of categories at the output layer. If we want AI that can learn new things on the fly, and form abstractions and metaphors, we will need this affinity-based wiring.\n\nWhen our brains ask \u201cmight that feature be a mistake?\u201d they do so by suppressing some of the inputs to that feature. In a neural network implementation, that would be expressed by temporarily lowering the weights to those neurons, and running the input through again. If the feature still fires with suppressed weights, there is some certainty that it is not a mistake.\n\nAnd, when our brains seek to combine a new constellation of features, they ask \u201cwhich features were present together?\u201d To implement this in an artificial neural network, a separate cluster of neurons are wired to connect with each feature that fired, so that the new cluster only fires when the entire constellation is active. Those features may be found at a variety of depths in the network \u2014 some may be at the low-level texture-and-edge-detector layers, while others are at high-level position-and-scale layers. They are all bridged directly into a new cluster, designed to fire when the entire constellation of features occurs.\n\nWiring new constellations of features from every depth allows recognition of diverse objects: a shiny, smooth, rose-colored ball would be distinct from a matte, brown, grain-patterned ball \u2014 the network might call one a \u201cglass bead\u201d and the other a \u201cwooden orb\u201d. The same low-level features of matte, brown, and grain-patterned might appear on a rectangular object with narrow legs \u2014 a \u201cwooden coffee table\u201d. The network learns that the constellation of low-level features is associated with wooden objects, while higher-level features distinguish their shape and function.\n\nIf such a neural network saw a new object, and didn\u2019t recognize it\u2019s high-level shape features, it might still realize that the object was made of wood, because it was matte, brown, and grain-patterned. Those kinds of insights are the building-blocks of general intelligence. We will need neural networks designed on this principle, if we seek machines that truly learn."
    },
    {
        "url": "https://towardsdatascience.com/neural-network-benchmarks-82d48425c21b?source=user_profile---------32----------------",
        "title": "Neural Network Benchmarks \u2013",
        "text": "We are experiencing a Cambrian explosion of neural network architectures. Each new design is scored with a benchmark \u2014 \u2018how well does it recognize cats?\u2019 \u2018can it play Atari games?\u2019 \u2018does it detect stop signs?\u2019 Researchers compare the performance of their neural network to other architectures, using available benchmark data sets. Yet, those data sets are limited.\n\nLabeled data, in particular, is hard to come by. Testing a very deep neural network requires a trove of data, and very few data sets are large enough to allow a comparison of deep networks. So, we should benchmark new neural networks on generated data, instead.\n\nBy generating data, instead of curating, we automatically label them. And, generated data can exist on a continuum of accuracy, which allows us to see if a network excels at identifying slight errors. This gives us a metric of the architecture\u2019s sensitivity and complexity, which is NOT available in curated data sets. With generated data, we have a clearer picture of how well a network learns, and where it makes mistakes.\n\nAs an example of generated data, consider orbital mechanics: we can create a simple piece of software that randomly places \u2018planets\u2019 on a 2D plane, each with mass, position, and velocity. Each time that the software is called, it places a cluster of \u2018planets\u2019 on the plane, and runs a model of their orbits for a fixed amount of time. These samples can be used in training and validation, without ever exhausting the data set! The network is able to sample from a data set of arbitrary size \u2014 if you want more data, just call upon the physics software again. During training, you could easily generate billions of sample orbits!\n\nWhen the software models an orbit without interference, we can automatically label that orbit \u2018Correct\u2019. To generate \u2018Incorrect\u2019 orbits, we introduce a few perturbations \u2014 at some point in the simulation, we randomly nudge a few of the \u2018planets\u2019, so that they fail to follow physics. The neural network\u2019s task is to distinguish \u2018Correct\u2019 orbital mechanics from \u2018Incorrect\u2019 orbits. This is the benchmark that allows us to compare the performance of different neural architectures.\n\nBecause we are generating our data, we can adjust how much perturbation occurs. If a single \u2018planet\u2019 is nudged only slightly, we classify that as \u2018Slightly Incorrect\u2019. Meanwhile, when many \u2018planets\u2019 are nudged by large amounts, we can classify their orbits as \u2018Very Incorrect\u2019. In this fashion, we form a continuum: \u2018Perfectly Correct\u2019 \u2192 \u2018Wildly Incorrect\u2019. This is a crucial capacity, which is lacking in cat photos and handwritten digits.\n\nSuppose that you want to benchmark a new neural network against the existing state-of-the-art. Using the orbit mechanics software, you feed each network billions of orbits, and measure their respective accuracy. After equal training, both networks identify when an orbit is \u2018Wildly Incorrect\u2019 \u2014 they both spot large perturbations. However, your new network is better at identifying the \u2018Slightly Incorrect\u2019 orbits! This demonstrates that your new network has greater sensitivity.\n\nAnd, you can train both networks on orbits with increasing numbers of \u2018planets\u2019. When there are only 3 or 4 \u2018planets\u2019, both networks perform well. Yet, when the number of \u2018planets\u2019 grows to 7 or 8, your new network is still accurate, while the other network begins to fail. This demonstrates that your new network handles greater complexity.\n\nThis allows us to measure the value of network depth explicitly. If a 4-layer convolutional neural network handles 3 \u2018planets\u2019 well, but becomes faulty when given 4 \u2018planets\u2019, then that 4-layer network has \u20183-planet complexity\u2019. To diagnose orbits of 4 or more \u2018planets\u2019, we would need to increase the network\u2019s depth.\n\nThe Value of Deep Networks\n\nBy successively increasing the depth of a neural network, and testing how many \u2018planets\u2019 it can handle, we have a metric of network complexity as a function of depth. We can answer the structural question: \u201cIf I double the network\u2019s depth, can I double the number of planets?\u201d Perhaps, deeper networks handle complexity at an increasing rate \u2014 if a 4-layer network handles \u20183-planet complexity\u2019, an 8-layer network might succeed at \u20187-planet complexity\u2019. If that is the case, it is the strongest argument for building insanely deep networks.\n\nHowever, if deeper networks slow down, (e.g. 8-layer networks only operate at \u20185-planet complexity\u2019) that is an argument for letting many shallow networks operate in tandem. This is currently an unsolved problem. Cat photos will never be able to answer it. Generating data sets along a continuum of correctness and complexity offers us a path to the answer."
    },
    {
        "url": "https://towardsdatascience.com/neural-networks-error-prediction-layers-fd8181dc33cd?source=user_profile---------33----------------",
        "title": "Neural Networks: Error-Prediction Layers \u2013",
        "text": "Jeff Hawkins, waaay back in 2005, wrote \u201cOn Intelligence\u201d \u2014 about a peculiar finding in human neuroscience which has yet to be utilized by Deep Learning. It deserves a closer look.\n\nHumans, dolphins, and monkeys have brains unlike other creatures: in our frontal lobes, we have numerous adjacent stacks of multiple types of neurons \u2014 like a breakfast table covered in plates, each with its own pile of pancakes, toppings interspersed. And, these stacks of neurons function in a peculiar way:\n\nHumans have six of these layers in our \u2018pancake stacks\u2019, and we have a \u2018breakfast table\u2019 with millions of plates, to form our higher reasoning and self-reflective intelligence. This model is completely different from what artificial neural networks do, today.\n\nYes, they do. And, so do GANs, and LSTM networks. Those methods quickly settle upon good heuristics for compression of data into features. Yet, our brains do more than that.\n\nLet\u2019s compare: a Recurrent Neural Network, and the human brain. RNNs receive the current state of the environment (its \u2018input\u2019 is the pixels on the screen) and generate an action within that environment (the \u2018output\u2019 is in the space of possible actions, while the \u2018input\u2019 was in the space of possible pixel arrangements). However, the human frontal lobe has a lowest-layer \u2018pancake\u2019, which takes the present state of the system (the \u2018inputs\u2019) and tries to predict the future state of the system (the \u2018output\u2019 is over the same space as the \u2018input\u2019, and the loss function is the difference between the two). The human brain isn\u2019t trying to \u2018choose the best move in the game\u2019, the way an RNN would! We\u2019re actually trying to predict what happens next.\n\nAdditionally, an RNN may have many \u2018layers\u2019 of neurons, but it represents only a single \u2018layer\u2019 of functionality. Pixels in \u2192 Action out. Our own \u2018pancake\u2019 stacks actually operate like stacks of multiple neural networks. Each \u2018pancake\u2019 takes State in \u2192 State Prediction out. Higher-layer \u2018pancakes\u2019 function as error-detectors because the state that they observe is the map of differences between their lower layer\u2019s ground truth and its prediction. Higher layers only see the pixels where the lower layer made the wrong prediction.\n\nIf we wanted a neural network to behave like a frontal lobe, it would need to map Pixel inputs \u2192Pixel Prediction output, and that output would be compared to the ground truth pixels in the following moment, to see where the predictions were wrong. That would be the lowest \u2018pancake\u2019. Then, the pixels that were mis-predicted would be the inputs for the next \u2018pancake\u2019, and that \u2018pancake\u2019 tries to predict where the next moment\u2019s errors will be. That second \u2018pancake\u2019 would be a new neural network.\n\nWe would need six of these neural networks, stacked on top of each other \u2014 and to make things more complicated, the higher-layer \u2018pancakes\u2019 would also receive signals from multiple feature-detector neurons many layers down the stack. Our sixth \u2018pancake\u2019 would receive the errors of the fifth \u2018pancake\u2019 as input, as well as some feature-detector signals from the first, second, third, and fourth pancake! That is unlike the existing artificial neural networks, and I think that the difference is important.\n\nHow does it Help?\n\nCurrently, researchers expect a single neural network to get things right every time. That\u2019s not how the brain works. In our six \u2018pancakes\u2019, the first \u2018pancake\u2019 neural network gets things wrong often. If we trained an artificial neural network to be like the lowest \u2018pancake\u2019, we would need to slow the learning rate early, and halt long before over-fitting. Our network would still get many answers wrong.\n\nThen, we would need a second \u2018pancake\u2019, a second neural network, that receives the lower layer\u2019s mistakes. That layer would also halt very early. It would likely still be mistaken about when the lowest layer will make a mistake. Only when many of these \u2018pancakes\u2019 are stacked together, does the error-rate drop significantly.\n\nFor number-minded folks: current NLP networks are in error about 4% of the time. Meanwhile, suppose that a \u2018frontal lobe\u2019 with six \u2018pancakes\u2019 had a lowest \u2018pancake\u2019 that was wrong 50% of the time. By itself, that \u2018pancake\u2019 would be much worse than our current networks. Yet, its erroneous instances are passed along to a second \u2018pancake\u2019. That pancake only looks for the 50% that were incorrect, and we can suppose it corrects 50% of those errors. So far, those two \u2018pancakes\u2019, taken together, are correct 75% of the time. With six of those \u2018pancakes\u2019, each one correcting just half the remaining errors, the combined accuracy is 98.4375%! So, the stacks of error-detectors can quickly outperform an end-to-end network, even when each error-detector is \u2018faulty\u2019.\n\nHumans have a greater capacity for reason and reflection, and we also have more \u2018pancakes\u2019 on our plates! Dolphins have four, apes and monkeys have less. I expect that, if a machine had more \u2018pancakes\u2019 than us, each \u2018pancake\u2019 attempting to predict the errors of the \u2018pancake\u2019 beneath it, that machine would be more capable than us. This opens up a new direction for machine intelligence that learns as it goes along.\n\nLearn-as-you-go: The machine would begin with a single deep neural network, and is given the task of predicting the next moment. When its success rate rises above some threshold, add a new deep neural network on top. That new network would receive the lower network\u2019s mis-predictions, and would be tasked with predicting where the next errors will occur. When that network\u2019s success rate rises above a threshold, add a new deep neural network. Continue this process, to successively improve the combination of networks.\n\nWith this \u2018pancake\u2019 paradigm, the network responds to new information by growing another deep neural network \u2018pancake\u2019 on top of all the old networks. Learning doesn\u2019t ever stop. The stack of \u2018pancakes\u2019 just gets taller and taller. This concept becomes even more important, when combined with the Mixture of Experts variety of neural network.\n\nIn a Mixture of Experts neural network, neurons are \u2018clustered\u2019 like raspberries, into bundles of dense connections. And, like raspberry jam, there are a few long-range connections that \u2018glue\u2019 all the raspberries together. Currently, Mixture of Expert models stop there. When an input enters at the bottom of the jam-pile, it activates just a few of the \u2018raspberries\u2019, and each of those \u2018raspberries\u2019 performs a little bit of feature-detection. Those features activate a few of the \u2018raspberries\u2019 that are higher in the jam-pile, where higher-level features are detected.\n\nMoving up the jam-pile, these expert raspberries are able to discover features in diverse inputs; for each subset of inputs, a different set of experts went to work. The Mixture of Experts network behaves like the union of many sparse networks, where each raspberry is the intersection of some of those networks.\n\nBack to the \u2018pancakes\u2019 that compose our frontal lobes. For it to match our own brains, the pancake metaphor needs to be even more elaborate: our neurons exhibit links across the stacks, from one plate to another. This is like a breakfast table covered in plates, with six \u2018pancakes\u2019 stacked on each plate\u2026 and raspberry jam smeared across all the stacks, dripping down their sides, and touching each plate to every other plate! Our brains are messy.\n\nIf we want an artificial neural network to learn and grow like ours, it will need multiple \u2018pancakes\u2019 of error-predicting deep NNs, where each of those deep NNs is composed of a layered Mixture of Experts. Each \u2018pancake\u2019 network receives as input the errors of the \u2018pancake\u2019 below it, as well as some of the features that were detected by the \u2018pancakes\u2019 adjacent to it. That\u2019s wildly different from current deep NN architectures. And, it\u2019s worth a try."
    },
    {
        "url": "https://towardsdatascience.com/neural-networks-a-mixture-of-experts-with-attention-30e196657065?source=user_profile---------34----------------",
        "title": "Neural Networks: a Mixture of Experts with Attention",
        "text": "A Mixture of Experts (MoE) is a special type of neural network: neurons are connected in many small clusters, and each cluster is only active under special circumstances. Lower layers of the network extract features, and experts are called upon to evaluate those features \u2014 for each case, only some of the experts are called upon. Mixtures of Experts have distinct advantages: they can respond to particular circumstances with greater specialization, allowing the network to display a greater variety of behaviors; experts can receive a mixture of stimuli, integrating data from diverse sensors; and when the network is in operation, only a few experts are active \u2014 even a huge network needs only a small amount of processing power. As neural networks become more complex, integrating many streams of data, and supplying a greater variety of responses, Mixture of Expert models will dominate. So, it helps to understand how Mixtures of Experts could evolve.\n\nMixtures of Experts are already in use for translation tasks. Each little expert-cluster learns to handle a separate part of speech or special grammatical rule. Yet, the Mixture of Experts translators do not currently use an Attention model.\n\nAttention is just a filter that allows only some of the input into the network at a time. By successively moving Attention around, a network can handle inputs of any size. An image-recognition network could receive images of many different sizes, and diligently parse the images piece by piece. A translation network could hop around among sentences, forming relationships between words that are separated by many lines. A Mixture of Experts, moving across its inputs this way, would be able to extract much richer relationships between words, and would be better equipped to translate accurately. Each Expert is called upon when certain features are recognized; those features may appear in disparate regions of the input, and require broad understanding of the input. The Experts would know where to pay attention.\n\nMixtures of Experts are the ideal model for a more complex form of memory, as well. Parsing a translation often requires that the network pay attention to a few different areas in the text. Statements that occur in multiple places must be combined into a coherent whole, so that the translation makes sense. A Mixture of Experts must focus its attention on an area, while remembering information from another area. This is achieved by wiring expert-clusters to the network\u2019s past states, similar to the wiring of an LSTM.\n\nLSTMs wire each neuron to its own past, without regard to the past state of its neighbors. Mixtures of Experts, however, would be wired to the past states of the feature-detectors that trigger that expert. This is a \u2018higher order\u2019 of memory. A feature might appear, and not trigger an expert-cluster. Yet, the memory of that feature, when combined with a relevant input, might trigger its expert later! The expert would be responding to the combination of the present input and the past feature. This behavior is currently impossible with an LSTM or other recurrent networks. And, it makes all the difference.\n\nIf a Mixture of Experts has a system of Attention, then it can hop around in a text, seeking information until it finds something that triggers an expert. This is similar to the way that we review what we\u2019ve read; if we become confused, we skip back to a variety of places, until we find the information that clarifies the text. An LSTM cannot hop around until it finds clarity. If the Attention system is connected to feature-level memories, a Mixture of Experts will review its inputs until it settles on a certainty. It looks around for the information that supports a conclusion, instead of blurting out answers.\n\nThis review process can be captured and visualized. When we see what information triggered an expert, we know that the network has \u2018based its decision on\u2019 that information. And, because each expert handles a very specific sub-task, we know what kind of problem each expert is solving. This is the best way to \u2018peek inside\u2019 the black box of neural networks. \u201cWe got this answer, because this expert fired when it saw this information\u2026\u201d is as close as we may get to a network that tells us what it was thinking.\n\nA Mixture of Experts is also ideal for solving problems that change and grow in complexity. When new information must be learned, most networks need to be completely re-trained, and they lose the lessons that they learned before. A Mixture of Experts can retain its old knowledge, and simply insert new clusters of experts, to be trained on the new information. When this \u2018augmented\u2019 Mixture of Experts is trained using back-propagation, all the old neural connections are \u2018frozen\u2019 \u2014 only the new clusters are allowed to learn. These \u2018noobs\u2019 quickly become experts at the new tasks, without losing the old expertise in the rest of the network!\n\nTaken together, these modifications supply a Mixture of Experts with the power to adapt, to digest inputs of any size, and to recall complex features that effect outcomes. I expect that these qualities will be critical for the \u2018next wave\u2019 of neural network tasks: Coherent Responses. Currently, neural networks that are trained to generate text are learning only from text. Networks trained to identify images are learning only from images. This \u2018cloistered\u2019 training will cease. A neural network that generates realistic text will need to know enough about the real world to make its statements logically and physically coherent, not just grammatically correct. An image recognition network will need to try instantiating its image in 3D, to see if it makes sense. Google has begun to work in this area, with \u201cOne Model to Learn Them All\u201d, which learns from sparse pairings of text, images, and sound. They still have a long way to go. And, they are not yet using a Mixture of Experts with Attention, Memory, and Growth. We will soon need it."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/life-on-other-planets-270290c15951?source=user_profile---------35----------------",
        "title": "Life on Other Planets \u2013 Anthony Repetto \u2013",
        "text": "We may find habitable planets around other stars, but our chances are slim. Water is the main issue, and the need for water imposes a few constraints on the kind of planet that could support us:\n\nMost of the planets that astronomers have discovered do not meet those requirements. While the Goldilocks Zone of our own star holds our rocky and watery Earth, most stars have a \u2018hot Jupiter\u2019 spinning around at that distance. These gas giants don\u2019t have liquid water, so it seems that our most common neighbors are doomed to be uninhabitable.\n\nYet, some of those \u2018hot Jupiters\u2019 may have a surprise for us: chances are good that they have captured a rocky, planet-sized moon. Those moon-worlds are likely much more common than another Earth, due to the sheer number of \u2018hot Jupiters\u2019. And, they have a few benefits, which dramatically increase their chances for supporting life. I am not expecting that life would already be on such a moon \u2014 at the very least, moon-worlds are the most likely places for humans to colonize.\n\nWhere Do Moon-Worlds Come From?\n\nMost \u2018hot Jupiters\u2019 begin their life much farther from their star, and slowly \u2018migrate\u2019 closer. During that migration, smaller planets\u2019 orbits are perturbed, and these rocky worlds are \u2018kicked out\u2019 of the solar system, traveling on long elliptic or parabolic trajectories. None of the original orbits remain\u2026 yet, those worlds can swing back toward the \u2018hot Jupiter\u2019 and become captured as a moon. Even though re-capturing planets this way is rare, it can create a larger number of habitable worlds than a solar system like ours. For example: suppose that only 1 in 10 of the \u2018rocky planet\u2019 solar systems has a planet in its \u2018Goldilocks Zone\u2019, while just 1 in 200 \u2018hot Jupiters\u2019 re-captures a planet as its moon. Meanwhile, we spot 1,000 \u2018hot Jupiter\u2019 solar systems, and only find 10 \u2018rocky planet\u2019 solar systems. In this example, even though most \u2018hot Jupiters\u2019 don\u2019t have a moon-world, we would still find 5 solar systems with moon-worlds orbiting a \u2018hot Jupiter\u2019, and only 1 solar system with another Earth. I think our chances are even better than that.\n\nIf a \u2018hot Jupiter\u2019 with a planet-sized moon is more likely, then we are likely to find one that is much closer to us than an Earth-like world. Being closer makes a huge difference for our chance of colonizing such a planet: we could carry more equipment with less fuel, get there faster, and encounter fewer hazards and uncertainties along the way. Jovian moons are the ideal \u2018stepping stones\u2019 for human colonies.\n\nA planet-sized moon around a \u2018hot Jupiter\u2019 would see a number of benefits.\n\nThose last two factors greatly expand the \u2018Goldilocks Zone\u2019 available to moon-worlds; they might support a watery surface even when they are orbiting \u2018too close\u2019 or \u2018too far\u2019 from their parent star. If the orbit would normally be \u2018too far\u2019, and water would normally freeze, tidal heating allows a moon-world to maintain a liquid ocean. A moon-world orbiting at the distance of Mars might stay warm enough for liquid water, because its Jupiter would churn magma that releases heat and greenhouse gasses.\n\nAnd, if the orbit would normally be \u2018too close\u2019, where a water-world would boil away, a moon-world might still sustain life. If the moon-world is tidally locked, then the face that points toward its Jupiter would be cooled by long, daily eclipses \u2014 potentially making it cold enough to retain liquid water. The side of the planet that faced away from its Jupiter may be too hot, evaporating water quickly during the day, yet the Jupiter-facing side could still experience condensation and have pools or lakes. And, because the Jupiter would keep catching comets, those lakes would be replenished for hundreds of millions of years.\n\nSome planets may only be habitable for a brief period in their history \u2014 Mars was likely habitable, shortly after it formed. When a planet is habitable for a longer period of time, then it is more likely that we will be looking at it during its habitable period. Many planets may have liquid water for the first tens of millions of years after their formation, but few of the planets that we see are so young. A moon-world, by retaining and replenishing its water for a longer period of time, has a much better chance of being found during its habitable phase.\n\nStars can also change their brightness, pushing or pulling their \u2018Goldilocks Zone\u2019, and turning habitable planets inhospitable. Because a moon-world can experience a greater range of temperatures on its surface (the outward, warmer face, and the inward, cooler face) and can support volcanism for longer periods of time (due to its Jupiter\u2019s tidal flexing), it has a better chance of remaining habitable, even when the star\u2019s intensity changes. An early \u2018heat wave\u2019 could have sterilized Earth; a moon-world would have retained glaciers or lakes on its inward face, and survived. This resilience dramatically increases our chances of finding a moon-world that is still habitable, long after initial planet formation.\n\nThis scenario may sound outlandish, yet it could be our best chance for a habitable world: a rocky moon-world could be artificially watered by colliding an icy moon with it. If we were to send a spacecraft to another star, it would need to accelerate towards its destination, and then decelerate when it gets there. The spacecraft would spend a huge amount of energy slowing down. The propulsion energy that we spend slowing the spacecraft down can be directed at an icy micro-planet orbiting our destination star, causing it to wobble out of its usual trajectory. By coordinating that wobble, we could cause the icy micro-planet to collide with a moon around a \u2018hot Jupiter\u2019, giving it a fresh dose of cooling water. We would create a habitable moon when we arrive, by pumping the brakes on our spacecraft!\n\nPlanets which are already Earth-like may be a rarity, and rocky moon-worlds that are currently habitable may be a bit more common \u2014 however, rocky moons that could be terraformed with an icy impact are likely abundant. Also, these moons wouldn\u2019t suffer from a narrow habitation timeline \u2014 they might be uninhabitable for their entire history, only becoming livable when we make them so. Then, we would have millions of years to capture moisture and alter the surface, before facing any risk of water escaping on the solar wind. Because these moons are more common than already-habitable Earths, the distance between them is small. They are ideal \u2018stepping-stones\u2019 to other stars.\n\nWith advanced robotics, our chances for colonization improve greatly. A robotic spacecraft could nudge icy micro-planets and rocky moons over the course of decades, churning satellites into orbit around a \u2018hot Jupiter\u2019 until a suitable world was formed. Jupiters tend to have many moons, so even small satellites could be collided to form a larger moon-world. We could build an Earth-sized moon from chunks of all the other satellites. It might take hundreds of years for the robotic craft to shape a planet, but it would still be cheaper and faster than flying to a far-distant Earth.\n\nWe currently detect distant planets one of two ways: the wobble they exert on their star, as they orbit, or the dimming of their star, when they transit. If a \u2018hot Jupiter\u2019 passes in front of its star, that star dims in a predictable fashion. If that Jupiter had a large moon, then that moon will cause the Jupiter\u2019s orbit to alternately slow down and speed up. We would see a periodic lag in the pace of the dimming of the star. Essentially, we can detect a wobble in the dimming-rate \u2014 a combination of both techniques! This method would only detect moon-worlds that orbit in our plane of vision, passing directly between their star and our observatories. And, it would only detect moon-worlds that were large enough and close enough to cause their Jupiter to wobble. Yet, these would be our best targets for exploration and colonization.\n\nIf we planned to capture many icy micro-planets and use them to water a moon-world, our robotic spacecraft would need to orbit far away from the \u2018hot Jupiter\u2019, in the band where comets and asteroids settled. From that high perch, a spacecraft could nudge these icy bodies ever so slightly, causing them to spin inward on a collision course. Like tipping a boulder down a hillside, this would require the minimal energy to propel so much mass. This is also the best strategy for watering worlds here in our solar system; one of Saturn\u2019s ice moons would chill and moisten Venus faster and cheaper than a brine-mining operation on Mars!\n\nRealistically, if Earth-like planets are rare, then the distance to them may be so vast that it is forever uneconomical to make the journey in a single leap. Instead, we may need to travel to a series of much-closer \u2018hot Jupiters\u2019, watering their rocky moons with captured comets so that we can rebuild and re-launch from there. I suspect our autonomous spacecraft would \u2018land\u2019 on numerous tiny rocks, to gather materials for vast solar cells, and repeatedly nudge those rocks together, to form a habitable moon. Humans would only \u2018wake up\u2019 when a world had been built. And, from there, the spacecraft would be re-fueled, to make the next leap.\n\nWe have already found plenty of stars with \u2018hot Jupiters\u2019. Most of those solar systems support an outer ring of icy clods, and they must have many rocky moons. If we can nudge those proto-planets together, we many not need to travel far at all."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/hayek-was-wrong-34ad384526af?source=user_profile---------36----------------",
        "title": "Hayek was Wrong \u2013 Anthony Repetto \u2013",
        "text": "Friedrich Hayek, the economist who followed the lead of Ludvig von Mises\u2019 \u201cEconomic Calculation in the Socialist Commonwealth\u201d with his own book, \u201cThe Fatal Conceit\u201d, stressed that the Central Planner of a socialist or communist country had an impossible task: 1) find out what everyone wants, and how strongly they want each of those things, 2) find out how to allocate resources, to best meet those wants, 3) re-calculate these wants and allocations quickly, cheaply, and frequently.\n\nTo know what everyone wants, a Central Planner would need to continuously survey the entire populace. For the world of Mises, in 1920, that would have been an impossibly laborious and intrusive task. A survey-taker would come to your door every day, and give you more demand-surveys to fill out; a massive team of office workers would sort and tabulate the data, on a daily basis! How could Stalin hope to run a continent, without droves of overworked secretaries and towers of paperwork?\n\nThe second task would be equally impossible for a Central Planner; efficient allocation involves constant adaptation to new information, often with distant and seemingly unrelated impacts. The equations to be optimized are hilariously complex \u2014 no one in the 1920\u2019s could expect to actually solve one before the answer became obsolete. New information would necessitate a re-calculation \u2014 a daily race to tabulate which company gets aluminum, and how much: soda cans, or car frames? It would take a crate of mathematicians, working non-stop!\n\nHayek and Mises argued that, because these tasks were impossibly hard, Central Planning would always fail. The cure they offered was price signal, which results from supply and demand in a market. Because each of us want stuff at different intensities, and we have some difficulty acquiring resources, the market lets us pool our knowledge as the price of goods and services. It acquires that price signal from peer-to-peer interactions and our personal, local knowledge. No one needs to survey all the details of your thoughts to determine \u2018how much\u2019 you want one car compared to another; No Central Plan necessary; information is decentralized, local, moving through a network. With a market, price signals display the outcomes of all our valuations, while keeping the reasons for our valuations private. Markets don\u2019t require an All-Seeing Eye, while Central Planning does.\n\nComputers are now fast enough that Walmart, Amazon, international shipping, financial firms,\u2026 heck, most of the global economy is only happening because a computer: 1) found out what we want, and got an idea of how much we want it, 2) determined efficient routes and allocations of resources, to best meet the surveyed wants, 3) gathered that data frequently, and performed an optimization to put our wants in place, with just-in-time delivery. Walmart and Amazon are the consumer superhighways because of their efficient prediction of demand, optimal routing, and allocation. If I respond to Youtube surveys, and Walmart loyalty cards track my purchases, and Amazon knows what was bought by customers with similar tastes, I do have the army of office workers Mises recommended! Amazon is the Central Planner that Mises thought would be impossible.\n\nAnd, they have a privacy policy! So, our internal reasons for our wants are not on display; they are collated anonymously into statistics for Amazon\u2019s warehouses, without the corrupt gatekeepers commonly found in \u2018communist\u2019 countries. I honestly can\u2019t see how algorithmic trading, routing, demand forecasting, and pricing is anything other than Central Planning. Especially considering that the majority of the algorithms in use are essentially the same, we have a Monarchy of Algorithm. No human makes those choices; no human could possibly understand enough to make those choices so quickly; we have left that task to Deep Neural Networks and Random Forests. They are good at it. Let them be Central Planner. Long live the LSTM!"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/stochastic-automata-convolutional-networks-a208bd2d1631?source=user_profile---------37----------------",
        "title": "Stochastic Automata & Convolutional Networks \u2013 Anthony Repetto \u2013",
        "text": "Automata (e.g. Conway\u2019s Game of Life) are directly related to Convolutional Neural Networks (CNNs). Both inform and improve upon the other. An elaboration of cellular automata, using stochastic rules, exhibits more complex behavior than automata with traditional, deterministic rules. Convolutional neural networks can be improved, in the same fashion.\n\nCellular Automata operate like a game. In Conway\u2019s Game of Life, a game board is populated with an arrangement of pieces. Those pieces, and the empty squares around them, are evaluated once per turn. The pieces can either remain (\u201clive\u201d), or disappear (\u201cdie\u201d), and the empty squares might have a piece placed upon them (\u201cborn\u201d), according to a set of rules.\n\nFor the traditional Game of Life, those rules are: a piece \u201clives\u201d if it has two or three neighboring pieces, and it \u201cdies\u201d, otherwise; an empty square with exactly three neighboring pieces has a new piece \u201cborn\u201d there.\n\nBy playing through many turns of the Game of Life, patterns are revealed: some arrangements of pieces are \u2018stable\u2019, unchanged by the rules; others are \u2018mobile\u2019, being transposed by the rules in a repeated sequence; many arrangements of pieces create a swirl of activity around themselves, like billowing smoke, and their billows can bump into near-by arrangements. This can trigger a cascade of changes, and it is uncertain how many turns are needed before all the billows are snuffed out. Even simple initial arrangements, like the r-pentomino, can take hundreds of turns to stabilize.\n\nMany varieties of cellular automata have been studied. One is game \u2018board\u2019 that is only a single row of squares, and each successive \u2018turn\u2019 is depicted beneath the prior turn, to form an image. Stephen Wolfram cribbed and collated work on this kind of automata in \u2018A New Kind of Science\u2019, and presented his exhaustive study of all the rules possible for these boards. Examining every variety of rules on these single-row boards, he found four categories of behavior: 1) rules that \u2018killed every piece\u2019, 2) rules that \u2018did nothing\u2019, 3) rules that created recursive patterns, and 4) rules that created a kind of randomness.\n\nFor Conway\u2019s Game of Life board, there are far too many rules to be examined. A piece may \u2018live\u2019 or be \u2018born\u2019 if it has 0, 1, 2, \u2026, 8 neighbors, in any combination. The number of possible rules is therefore 2 to the 18th power. (That\u2019s 262,144 possible rules!) To evaluate the behavior of each rule, many initial arrangements of pieces must be examined, as well. Only a tiny fraction of these rules have been studied thoroughly.\n\nSoftware exists, to visualize different rules as they unfold, and you can \u2018paint\u2019 the initial arrangement of pieces\u2014 Mirek\u2019s Cellebration is a popular one. You can explore rules that you specify. It\u2019s definitely more fun than minesweeper. :]\n\nThe rules available in the Game of Life show the same varieties of behavior as those studied by Wolfram: 1) rules that \u2018kill all the pieces\u2019, 2) rules that \u2018do nothing\u2019 to most of the board, 3) rules that billow with recursive structures, and 4) rules that bounce around through noisy \u2018randomness\u2019. Yet, there are other patterns of behavior, too\u2026\n\nIn my own play, I found many rules that cause the pieces to \u2018congeal\u2019 into a crystal, bounding a region of the board, and these crystals tiled their interior with a simple pattern. In most cases, that interior tiling quickly became \u2018stable\u2019 \u2014 a static pattern remains, turn after turn. Some crystals, however, would maintain a wobbling \u2018fringe\u2019 along their outer edge that refused to settle down. Others developed an interior \u2018noisy froth\u2019, or a tiling pattern that \u2018seethed\u2019 wherever the tiles were mismatched. In general, they exhibited behaviors that were specific to a local domain: either at the boundary of the crystal, or throughout its interior. These behaviors constitute the emergence of a new rule, operating on a larger scale than the original rule for neighboring pieces.\n\nA small number of these \u2018crystallizing\u2019 rules stood out: they supported multiple stable interior tilings. You could compare these tilings to the different crystal arrangements seen in minerals\u2014 some areas of a crystal may be stacked as a hexagonal close packing, while others are a face-centered cubic packing. Wherever these different packings meet, there is a grain boundary.\n\nIn a rare few of these cellular automata, wherever the different interior tilings met, their grain boundaries would \u2018froth\u2019, changing each turn. The pattern of these grain-boundary interactions became it\u2019s own stable interior. In essence, the crystals grew to form multiple interiors, and the boundaries between them were forming their own kind of crystal! These were not just \u2018recursive\u2019 or \u2018noisy\u2019 behaviors; they were higher forms of emergence. Each tiling pattern behaved a certain way, and each kind of grain boundary elicited its own new behavior.\n\nStranger still, as these boundary-crystals were buffeted by the \u2018froth\u2019 at the grain boundary, they sometimes \u2018exploded\u2019 into new patterns. For one rule in particular, the new pattern would expand, and become dominant. The entire crystal underwent a phase change, like a spontaneously freezing beer. Another rule produced explosive patterns that would \u2018churn\u2019, surviving for hundreds of turns by migrating up and down the grain boundary.\n\nClearly, the Game of Life displays a greater variety of behaviors than Wolfram\u2019s single-row automata. This fact is critical to understanding the sorts of recognition possible in Convolutional Neural Networks, because Conway\u2019s Game of Life is mathematically equivalent to convolution.\n\nA convolutional network receives a field of input pixels, synonymous with Conway\u2019s pieces on a board. (Each layer of the convolutional network is composed of \u2018neurons\u2019 instead of \u2018squares\u2019.) And, each neuron receives a signal from a neighborhood of neurons beneath it, exactly like Conway\u2019s automata looking at its neighboring squares. The signals flowing through a convolutional network are equivalent to the migration of Conway\u2019s pieces, with each new convolutional layer corresponding to another turn in the Game of Life.\n\nWhen back-propagation finds a good convolutional network, it is actually finding good rules for a Game of Life automata. However, the rules available to a convolutional neural network are a bit different from the traditional Game of Life. First, convolutional networks sum their neighbors\u2019 activations, instead of counting them like Conway does. Second, convolutions\u2019 activations are monotonic (\u2018always increasing\u2019), while Conway\u2019s rules can be non-monotonic (a piece might \u201clive\u201d if it has 2 or 5 neighbors, while it \u201cdies\u201d if it has 3 or 4 neighbors, for example). Third, convolutional networks have a synaptic weight for each neighbor, while Conway\u2019s Life treats all neighbors equally.\n\nThis last difference allows convolutional networks to recognize edges in an image, because they are sensitive to the relative arrangement of pixels (some neighbors have a heavier weight than others). You could improve and generalize Conway\u2019s Life, by specifying the extent to which neighbors must be present for a piece to \u201clive\u201d \u2014 doing so captures convolutions\u2019 capacity for edge detection.\n\nGeneralizing Conway\u2019s Life to include these relative arrangements of pieces vastly increases the number of possible rules. There are 2 to the 9th power possible arrangements, and each arrangement can have one of two rules \u2014 \u201clive\u201d or \u201cdie\u201d. So, the number of possible rules is 2 to the 512th power! To learn edge detection, a convolutional neural network traverses these possible rules, seeking an automata that performs that task.\n\nThe rules possible for convolutional networks are actually more diverse than \u201c2 to the 512th power\u201d, because each neighbor\u2019s contribution is weighted along a continuum (the neurons\u2019 synaptic weights). So, CNNs have a huge range of possible behavior. The generalized Game of Life that I mentioned above corresponds to a network where weights could only be 0 or 1. In contrast, a CNN could have weights of -2.3 or 100.487 or 0.3\u2026 So, the Game of Life\u2019s [0, 1] weights are just a subset of the [negative to positive] weights available in convolutional networks.\n\nSimilar to the \u2018crystal tilings\u2019 and \u2018seething\u2019 grain boundaries that I found in Conway\u2019s Game of Life, I expect that additional levels of emergence could be found in convolutional networks, because they weight neighbors. Unfortunately, convolutional networks are not currently looking for this emergence \u2014 the Game of Life\u2019s crystal tiling behavior appeared only over the course of hundreds of turns, which is equivalent to a pattern across hundreds of layers of a CNN. Current convolutional neural networks, being only a few layers deep, are only able to search for rules that rapidly congeal to a stable arrangement (Wolfram\u2019s 2nd type of behavior); they ignore rules that \u2018froth\u2019 or \u2018seethe\u2019 at the boundary.\n\nWhile it may seem that convolutional neural networks completely include all possible Game of Life rules (because synaptic weights can exist along a continuum, while the Game of Life has weights 0 or 1 only), there is a way that the Game of Life is vastly more diverse than convolutional networks: it is not monotonic.\n\nConvolutional networks\u2019 neurons respond monotonically (if the neuron\u2019s received signals increase, then that neuron\u2019s activation will increase also). The Game of Life\u2019s cell\u2019s activations are non-monotonic, because they can go up or down. (If 1 is \u201clive\u201d and 0 is \u201cdie\u201d, a monotonic rule would be \u201clive if surrounded by fewer than 4 neighbors, die otherwise\u201d, while a non-monotonic rule would be \u201clive if the number of neighbors is 2, 4, 5, or 7, die otherwise\u201d.)\n\nLooking at the complex behaviors that emerge from the Game of Life\u2019s variations, I suggest that convolutional neural networks adopt polynomial activation functions. With a polynomial activation function, an increase to a neuron\u2019s stimuli might raise the neuron\u2019s activity, up to a point; then, it\u2019s activity may begin to decrease; and even more stimulus might cause it to rise again! I expect that a greater diversity of local phenomena could be captured by a convolutional network with polynomial activation, even though it may fail to converge on a global minimum during training.\n\nThe Game of Life, being non-monotonic, provides an improvement for convolutional networks. Yet, there is something that they both lack, which may increase the diversity of behavior in both: stochastic weights. If a Game of Life were stochastic, then there would be some probability for each neighbor to be ignored, or each neighbor would be quieted by a variable amount. This is similar to Drop-Out, a commonly used regularizer for training convolutional neural networks. However, Drop-Out is currently only used during training; I suggest that this stochastic quieting be used during regular operation, as well.\n\nPulling all these strands together:\n\nEven better, the \u2018output\u2019 of a CNN could be the coefficients of the first few terms of a Fourier Transform for each neuron\u2019s activations across layers. (In the example with a neuron that repeats the sequence of activations: (4, 1, 0, 1), the activations are described by the polynomial: 1(x-3)\u00b2 + 0, and that neuron\u2019s output layer would be the Fourier coefficients: (1, -3, 0).) Fourier terms would reveal the periodicity that I observed in those strange variants of the Game of Life. I expect that diverse features can be classified using these \u2018crystals\u2019 that \u2018froth\u2019 and \u2018seethe\u2019 at their boundaries."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/the-fallacy-of-re-training-after-ai-1a522fc600a7?source=user_profile---------38----------------",
        "title": "The Fallacy of Re-Training after AI \u2013 Anthony Repetto \u2013",
        "text": "Machine Intelligence threatens jobs. Technologists propose that displaced workers be re-trained. This will not work. Here is why:\n\nIf a company saves money by replacing workers with algorithms or robots, their savings are marginal; it\u2019s the difference in cost that counts. For example, a salaried employee packing boxes costs Amazon about $30,000 a year. If robots replace those jobs, it will be because robots are somewhat cheaper \u2014 say, $20,000 a year in amortized costs? With over 300k employees, most of those packing boxes, that would provide Amazon upwards of $3 Billion in savings. Wow. Can we expect Amazon to provide new salaries for those lost workers? No.\n\nIf Amazon pays for those new salaries, they go into debt, instead of saving. They would be spending $30k/person on new salaries, on top of the $20k/robot. Automation\u2019s entire value proposition rests upon the premise that Amazon can get rid of those workers. So, it falls upon someone else to employ the workers that Amazon sheds.\n\nIf Amazon paid for those workers\u2019 re-training, they would not be saving money, either. Re-training would constitute a large up-front cost, while the value of re-training would be received by workers who no longer add to Amazon\u2019s profits. A re-trained worker wouldn\u2019t pay Amazon back with salary from their new job.\n\nA worker who is packing boxes is already low on the economic totem pole; any amount spent on re-training is unlikely to provide all of them with a higher-paying job, because that influx of new job applicants lowers the wages that they can command. This is supply and demand. An increase in job-seekers reduces wages.\n\nSo, who would pay for re-training Amazon\u2019s fired box-packers? Even an industry-wide initiative suffers from the same calculation. Any group of industries which shoulders the cost of their displaced work-force would see no rise in profits after automation. The burden falls on tax-payers, should the government exercise the political will to help.\n\nThat only transfers the cost to a wider base \u2014 the same base who supposedly benefits from lower prices after automation. Prior eras of lost employment were able to avoid this conundrum, primarily because new jobs did not require expensive re-training. When farmers were replaced by tractors, they could get a job in a factory, without significant re-training costs. And, the farms passed lower costs on to a broad base of consumers. The new secretarial workers could afford more food, even though their wages were a downward substitution.\n\nAI, in contrast, primarily passes savings on to investors, who already see a low tax rate on their gains. If the government pays for re-training, then the cost borne by most tax-payers is necessarily greater than the savings from lower priced goods. Wealthy stock-holders are the only ones who walk away with the profits from automation.\n\nThat is the fundamental flaw in the push for automation: businesses that automate are able to avoid the costs of re-training, and their stock-holders are the primary beneficiaries. Those stocks are taxed at a lower rate than income, shifting the cost of re-training to the people who save only a little from lower priced goods. The net impact for the broad base of consumers is negative. And, much of the replaced work-force sees lower salaries, because of increased competition for the new jobs, while they must shoulder the cost of their own re-training through taxes.\n\nAI and robots replace simpler tasks first. Displaced workers, necessarily, must be re-trained for more complicated jobs. Many will be unable to find openings in those jobs, because of a lack of talent, regardless of expenditures on re-training. (Only a tiny fraction of Amazon\u2019s box-packers will become doctors.) Most will be pushed into other low-wage jobs, which are similarly at risk of automation. Re-training for one job only shuffles them into another occupation where they will be fired and need re-training.\n\nFarm machines didn\u2019t threaten factory jobs. Industrial robots didn\u2019t threaten actuarial jobs. AI is different, because it threatens the bottom of the work-force with multiple waves of replacement and re-training, regardless of their new employment. Each re-training is necessarily for a more complicated task, requiring more expensive re-training, and a lower proportion of displaced workers will have the talent necessary to compete in a saturated job market.\n\nWorse still, the jobs that most re-trained workers will find themselves in are necessarily lower productivity, as well. (If those jobs represented higher productivity, they would have paid more, and they would have been chosen first.) Downward substitution means downward pressure on worker productivity. Amazon would see higher productivity among the workers it retains, but that is offset by the loss in productivity of substitution work, for only a tiny net increase.\n\nThe Re-training Multiplier: An Example\n\nSuppose that Amazon replaces 200k jobs with robots. Recent advances in the software that controls robotic grippers may do that very soon. And, suppose that re-training places 20% of those workers into better-paying jobs that are at a low risk of automation, while the remainder of workers are shunted into simple tasks that are also at risk of automation. When those simple tasks become automated, re-training again places 20% with better-paying work, while the remainder move into occupations that are similarly threatened by automation. In each automation-wave, the entire pool of simple-task workers must be re-trained, such that many of those workers will need multiple re-trainings. Using my 20% figure, Amazon\u2019s 200k jobs lost would incur a cost of re-training 200k workers, then 160k workers, then 128k, \u2026 a total of 1000k instances of re-training, before those workers find un-automatable jobs. This is the re-training multiplier that threatens to over-burden consumers and tax-payers.\n\nAside from computer programmers, most job creation for the last few decades has been in low-wage services. Automation threatens to flood the service-sector, at exactly the time when those service jobs are saturated. Our 4% unemployment figure is a lie, predicated on counting part-time work as a job, and ignoring the ever-increasing portion of discouraged workers. Sending 200k more workers from box-packing into the service sector would further depress service workers\u2019 wages, and send many of those workers into the discouraged category. Re-train them to be a truck driver? Or hire them as a waiter for a diner at a truck stop? Good luck!\n\nAs long as investors pay a lower tax rate than income-earners, automation and re-training will profit only them."
    },
    {
        "url": "https://towardsdatascience.com/gans-with-attention-3b90802921af?source=user_profile---------39----------------",
        "title": "GANs with Attention \u2013",
        "text": "A Generative Adversarial Network (GAN) holds two networks in series: a Generator and a Discriminator. The popular example to explain GANs involves counterfeit money. The Generator tries to create an image that looks like a dollar bill, and the Discriminator tries to tell that counterfeit apart from the image of a real dollar bill. After training the two networks, the Generator has learned to create images that look very much like a dollar bill, while the Discriminator has learned to reliably distinguish those counterfeit images from a real dollar.\n\nThat is a one-off process. The Generator creates a counterfeit, the Discriminator tries to identify it, and then they are both scored on their performance. There is no feedback, no further adjustment; that counterfeit image is discarded.\n\nA similar approach, used in recurrent neural networks, is the Actor-Critic Model. The Actor, which would normally only receive a \u2018score\u2019 at the end of their performance, is instead \u2018scored\u2019 along the way by a Critic. The Critic learns to predict which actions affect the final score, and the Actor learns to perform the actions that affect the final score, via the Critic\u2019s moment-to-moment scoring. This is still a one-off process. The Actor is unable to \u2018film that scene again\u2019, in response to criticism.\n\nSo, What About Revisions?\n\nAnother metaphor, to describe a better approach: an Author produces their dollar-image (or block of text, or string of actions,\u2026) and sends it to the Editor. The Editor then marks that dollar-image in the places where it finds mistakes, and sends it back to the Author. The Author focuses their attention on those mistakes, and corrects them, sending the new draft back to the Editor\u2026 Repeat this cycle, until the Editor finds no mistakes. The Author network learns to take an image along with an attention-filter, and apply changes to the regions that are highlighted. The Editor learns to take an image and its target, and highlight regions that should change.\n\nThis Author-Editor model enables valuable new behaviors. Once the Author has been trained to adapt an image according to an Editor\u2019s marks, a human can also correct the Author by selecting areas they would like to change. This user-input could direct different Authors to focus their attention on different areas of an image. A user could select some areas of an image, and apply the attention of a \u2018Winter-to-Spring Author\u2019, while selecting other areas of that same image, to apply a \u2018Real-Life-to-Chibi Author\u2019. Each Author makes modifications according to its own attention. You can be the Editor.\n\nThe Editor is trained to highlight mistakes. So, if the Author-Editor model is trained on text, the Editor has an immediate application: correcting mistakes in what humans write! (like Grammarly\u2026)\n\nAlso, Author-Editor networks can collaborate with humans in real-time. For example, you might draw an animal \u2014 a fox? \u2014 which is passed to the Editor, who tries to identify regions that should be modified. The Editor may highlight areas on your fox-drawing where the lines can be smoothed, or suggest where to alter their position, proportionality, and overall texture, for example.\n\nThe Editor can make mistakes! It might think that you are drawing a cat, highlighting a few regions improperly. You can review these highlighted regions, and replace the Editor\u2019s highlighting. Those regions can then go to the Author, who focuses the network\u2019s attention on the regions, and makes changes. You review the Author\u2019s updated draft, and can make your own changes. Then, send it to the Editor\u2026 Repeat, until you are satisfied.\n\nThis Author-Editor model also makes it possible for people to co-create content with the help of multiple Authors and Editors. Authors\u2019 attention can be applied to different areas, or to the same area. If each Author is active in a different region, (like the Winter-to-Spring and Real-Life-to-Chibi Authors, mentioned earlier) their drafts can be combined. However, when Authors\u2019 regions of attention overlap, the Authors each provide an alternative, and you can select which alternative is applied.\n\nHaving multiple Editors provides a similar benefit \u2014 each Editor highlights the regions that it thinks should be modified, and you choose between their suggested highlights. You could also stop the Editors from changing things you\u2019d like to keep!\n\nAn Author-Editor GAN differs from existing GAN architectures, by training the Author on an attention field that is generated by the Editor, and by revising many times, to produce a final output. In this way, the Author-Editor model is similar to Recurrent NNs with attention: the revision-history can be \u2018unrolled\u2019 like an RNN\u2019s action-history, and each revision focuses attention on new relevant areas. Revision differs from recurrence, though \u2014 an RNN describes a sequence, like choosing paths in a maze, while Author-Editor revisions describe an equilibrium, a settling-place where the Editor stops highlighting changes. Recurrent NNs cannot contain revisions, but revisioning can contain recurrence: an Author could write the sequence of choices in a maze, and it writes and re-writes that sequence until the Editor is satisfied. :]"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/criticizing-fools-4d42d42a76d9?source=user_profile---------40----------------",
        "title": "Criticizing Fools \u2013 Anthony Repetto \u2013",
        "text": "I knew I shouldn\u2019t have. I read Kevin Williamson\u2019s article in National Review about the recent housing report from the NLIHC. The report compares each county\u2019s average rents with their minimum wage, and finds that housing is beyond the means of the poor in every county in the US. Williamson used the report as a launchpad for his regurgitation of economic tropes, and he managed to fumble basic mathematics in the process. Yes, \u2018someone at the National Review is asphyxiating mathematics while buggering economics\u2019 is already a trope, too. I got this one:\n\nWilliamson rests his argument that the NLIHC\u2019s report is \u201cbasically useless\u201d on a claim that the report is measuring apples to oranges \u2014 specifically, that it is inappropriate to compare average rents to minimum wage, and a comparison between respective minimums would show that rents are actually affordable.\n\nHe explains, as if he is truly educating his readers, that the NLIHC report measured the price of rent for the 40th percentile of the rent market, for families who were just moving into housing. Williamson wishes to equate this to the 50th percentile, by observing that new rents cost more than average, because the average includes people who have been renting prior to price increases. He finds data showing that, compared to the set of all rentals, new rentals are \u201c\u2026typically about 6 percent more. So the 40th percentile of rents for families paying a 6 percent premium \u2014 that won\u2019t be the dead median, but it will be in the neighborhood.\u201d\n\nOuch. Williamson forgot that the 40th percentile doesn\u2019t work the same as 6 percent! If Williamson\u2019s conflation were true, then the 10th percentile would have rents that were only a quarter the cost of NLIHC\u2019s data, (\u2018because 10th is a quarter of 40th\u2019) while the absolute highest rent would only be 2.5 times higher than what NLIHC quotes for each county (\u2018because 100th is 2.5 times larger than 40th\u2019). How convenient!\n\nIn reality, the mid-range rent prices are clumped together. That\u2019s the whole \u2018bell curve\u2019 thing that they teach in math class. So, the 40th percentile price is likely already very close to the 50th percentile; each county may have a larger or smaller variation in prices, but not by much. And, while there is plenty of room for high-end prices to soar, the low-end cannot fall below zero. So, the rent paid by the 40th percentile can be much less than rent at the 70th percentile, while the 10th percentile does not see as significant a change in price.\n\nIn concrete terms: here in the East Bay, rents for one bedroom are around $800 (yikes!) Let\u2019s call that price the 40th percentile. Now, moving to the 70th percentile does NOT \u201cincrease the price by 3/4ths\u201d \u2014 here, it is likely a bigger increase. Meanwhile, the 10th percentile isn\u2019t \u201cone quarter the price\u201d \u2014 that would be just $200, wow. I wish that Oakland still had rooms so affordable!\n\nSo, with the skewed distribution of housing prices, the 40th percentile is not paying significantly more in rent than the 10th or 20th percentile, most of whom are the minimum wage workers mentioned by NLIHC.\n\nThat leads to the deeper confusion in Williamson\u2019s write-up. Williamson fails to consider that both housing and pay are heavily skewed distributions, such that matching 10th percentile earners with 10th percentile rents, etc., does not translate to a fixed portion of income for all percentiles. As you near the bottom of rents, the portion of income spent on rent grows. And, for the highest rents, you find occupants who are spending only a fraction of their income renting. As a result, low-income renters have less money to save, and are more likely to need credit after any financial pitfalls. Falling into debt exacerbates the financial squeeze on low-wage earners.\n\nWilliamson later quotes a statistic which exactly refutes his claims: \u201cHousehold income has not kept up with the rising cost of rental housing. From the housing crisis of 2007 to 2015, the median gross rent for a rental home in the U.S. increased by 6 percent, after adjusting for overall inflation, while the median income for renter households rose by just 1 percent.\u201d\n\nIf median costs are rising faster than median incomes, then the median is getting hosed, too. Those are apples to apples. Williamson doesn\u2019t seem to notice: his argument, that the NLIHC study was \u201cbasically useless\u201d because it compared median rent to minimum wage, falls apart. Housing is actually becoming unaffordable for most of us.\n\nWilliamson digresses in his argument, hoping to prove that our six-year low in the construction of multi-family housing is not to blame for rising rents. He explains that Walmart makes a lot of money serving predominantly low-wage earners. He then argues that Walmart makes more money than Louis Vutton, and thus, an increase in low-rent housing would be in the market\u2019s interest.\n\nUnfortunately, Williamson missed the business memo: Louis Vutton sees a much higher profit margin than Walmart. And Walmart is huge, not because of the profits available to companies that serve the poor, but because it has comparative advantages to the smaller retailers. If a company seeks profits, it will look for areas with higher rates of return; increasing gross sales while diminishing profit margins is not a winning strategy. That is the problem at root of low-income housing: contractors who build housing for higher-incomes stand to gain a larger percentage profit. In short, low-income housing is not as profitable.\n\nWilliamson elaborates that, if developers were not hamstrung by liberal city-dwellers\u2019 zoning laws, then contractors would happily build more low-income housing. No, Walmart cannot save us, Kevin. Walmart\u2019s comparative advantage let it gain market share, by pushing-out other retailers. For a similar behemoth to provide low-income housing, they would need to push-out smaller contractors and pinch pennies with cost-cutting practices, in order to see a worthwhile rate of return. The fundamental point is that contractors already see a lower rate of return on low-income housing, hence the housing shortage that drives up rent! Williamson bleets: \u201c Why aren\u2019t we building more housing for low-income people? It\u2019s not because there\u2019s no money to be made selling goods and services to low-income consumers\u2026\u201d No, it\u2019s because there is a lower margin for low-income markets, Kevin.\n\nWilliamson veers into specious economic policy, arguing against higher minimum wages with his assessment of \u201c how inflation happens: We value what we value just the way we value it, and introducing more money into the system does not change those value judgments; it just makes money worth less\u2026\u201d On its face, that is true. An increase to the money supply (i.e. printing extra dollars) would cause inflation (i.e. everything costs more dollars). The increase in money supply is at fault. But, Williamson equates a higher minimum wage with inflationary forces \u2014 in essence, he argues that \u2018paying poor people more would just raise prices.\u2019\n\nNo. Pay is not \u2018more dollars printed\u2019 \u2014 it is \u2018dollars that would have been profits.\u2019 When a company earns money, that money is allocated between profits and payments. If pay was kept low, that money would be going toward profits, instead. The rich, following Williamson\u2019s argument, would see an increase in prices for all their goods! Instead, paying workers more means that they constitute a larger share of spending, compared to profiteers. It\u2019s a problem of allocation, not inflation.\n\nAnd, if workers represented a larger share of earnings, then their demands would become a larger share of market activity. In housing construction, this would play out as a higher volume of housing being built for low-income earners, while the rich would see a lower volume. No goods were added or lost, and no extra dollars were printed. It\u2019s an allocation problem, not an inflation problem. Higher minimum wages make the market listen to the needs of the poor more than the needs of the rich.\n\nThe most destructive folly in Williamson\u2019s argument is a conflation of value and cost:\n\n\u201c You could pass a law that says we have to pay 15-year-old baby-sitters eight times what we pay hedge-fund managers or brain surgeons, but that is not going to change how we actually value their respective labor.\u201d\n\nPeople are not paid according to the value of their work. They are paid the cost. So, if you can fill a teaching position when you offer $20/hr, the cost of that work is $20/hr. Yet, the value of that work is measurably much greater. Meanwhile, hedge fund managers\u2019 activities have been involved in financial turmoil that caused a loss of value to many \u2014 that is called a negative externality, because the loss is borne by people who do not pay the hedge fund. Yet, those hedge funds make money for their clients, and thus, their incomes are far above the \u2018value\u2019 they create.\n\nOn a deeper level, if workers are more productive, then their work is more valuable. Yet, wages have been stagnant for decades, even as productivity has increased. That is possible because companies are taking a larger share of sales as profits. Workers\u2019 wages are lower in real terms, not because \u201c They have labor that is lightly valued in terms of everything for which money can be traded,\u201d as Williamson would like to believe. Workers see flat or declining wages because their value is not being translated into their cost.\n\nWilliamson argues that neither affordable-housing plans nor higher minimum wages can help the housing crisis; only de-zoning will cure it! He misses the value of the average worker, the lack of incentives for businesses that would make larger margins selling to the rich, and the basics of macroeconomics and arithmetic. I don\u2019t pretend that National Review has \u2018stooped\u2019 to such inane arguments; they have always wallowed there."
    },
    {
        "url": "https://towardsdatascience.com/sketching-a-proof-of-convergence-for-covariance-learning-in-neural-networks-fbfc0c875bea?source=user_profile---------41----------------",
        "title": "Sketching a Proof of Convergence for Covariance-Learning in Neural Networks",
        "text": "Neural networks are diagnosing cancers and translating between hundreds of languages. Complex classification tasks are now possible using deep neural networks. As we rely upon neural networks for increasingly sophisticated activities, we can expect these networks to expand to incredible depth. These networks have immense potential value. Yet, there are challenges that arise from training very deep neural networks\u2026\n\nWhen a neural network is trained to classify data, it eventually learns to sort perfectly \u2014 but, it will only sort your training data perfectly because it memorizes the entire set. When given new data, these over-fit models perform poorly. Moving from a random network (which performs a random classification) to a memorized-data network (which performs an exact classification, i.e. it is \u2018over-fit\u2019) is a big leap, while good-generalization networks (which perform well on new data) sit at points near the memorization. Researchers halt their training once the classifier begins to worsen when given new data. That\u2019s cludge, to work around the fact that minimizing the \u2018loss-function\u2019 is not what we really want. We actually want a good generalization!\n\nA deeper neural network takes much longer to train, and often requires much larger data sets to learn well. You can use GPUs (or TPUs) to reduce time and cost, but that is only a one-off improvement. Fundamentally, training will balloon in cost as networks become deeper. Moore\u2019s Law is dead, so the argument that \u2018improvements in computer hardware will continue to make deep neural networks cheaper\u2019 falls flat.\n\nAnd, Learning needs to happen Everywhere.\n\nNeural networks store learning across the depth of the network: simple, local features (like the location of an edge) are usually distinguished at lower layers, while higher layers classify more abstract features (e.g. \u2018animal\u2019, \u2018tool\u2019, \u2018building\u2019). Unfortunately, neural networks are trained using back-propagation from the output layer \u2014 they learn from the top, down. So, the features that must be learned at low layers are only taught by signals that have diffused down through many layers. This is called the \u2018vanishing gradient\u2019 problem. Residuals and skip connections are fashionable work-arounds, but they do not eliminate the problem of top-down learning. (Residuals essentially amplify the gradient, while skip-connections effectively flatten the depth of the network; both still learn from the top layer, down.)\n\nSpecifically, negative covariance (and its cousin, negative correlation) is the solution. Negative covariance occurs when two signals tend to occur in opposition: if one is \u2018on\u2019, the other is \u2018off\u2019, and visa versa. For a neural network, negative covariance is registered if a neuron is \u2018on\u2019 when correctly classifying data, while it is \u2018off\u2019 when misclassifying data. (And, negative covariance is registered for the inverse condition, too: \u2018off\u2019 when correctly classifying data, while \u2018on\u2019 for misclassified data.) Neurons registering negative covariance are essentially error-detectors. They tell us: \u2018if this neuron is firing, then we are likely to get the wrong answer\u2019. We should listen to those neurons, to find which lessons have not been learned.\n\nNeural networks today all suffer from top-down learning \u2014 their \u2018loss-function\u2019 or \u2018cost-function\u2019 is the error found at the output layer. (The loss function asks: \u201cDid the neural network get the correct answer?\u201d) Meanwhile, covariance can find errors at any layer, not just the top-most output layer! Negative covariance, being an error-detector, points specifically to the neurons \u2018responsible\u2019 for errors.\n\nSaid another way: if neural networks were businesses, the \u2018loss-function\u2019 is a memo that travels through many layers of management, before getting down to the employees. The \u2018loss-function\u2019 takes the result \u201cOur business lost money!\u201d and tells upper-management to \u201cDo Something Differently\u201d. Those managers then send a message to all the middle managers, insisting that they \u201cDo Something Differently\u201d. By the time that message gets to all the employees, what they should change is unclear. The memo was vague, to begin with, and became diluted as it traveled down the hierarchy.\n\nContinuing the business metaphor: negative covariance is like a targeted memo, following a personal performance review. Covariance asks of each neuron/employee: \u201cDid this employee do the work that caused our product to fail?\u201d The entire network is under review, and the message \u201cDo Something Differently\u201d is only sent to the neurons/employees who DO need to do something differently!\n\nSo, while the loss-function is only defined at the output layer, covariance is defined at every neuron in the network. With covariance, learning happens wherever it\u2019s needed, irrespective of layer.\n\nTraining a very deep neural network adds time, because the \u2018business memo\u2019 becomes diluted as it travels through additional layers. In contrast, covariance targets its \u2018memos\u2019, and can give proscriptions to each neuron, regardless of depth. With covariance, extra layers don\u2019t dilute the message, and so, they don\u2019t elongate training time.\n\nCovariance saves time on re-training. Traditional training creates a neural network that is static; if new data is included in the network\u2019s training, it is often re-trained from scratch. Suppose that the new data required a change to some low-level features? The network would need to send numerous \u2018memos\u2019 down the hierarchy, before meaningful change would occur at the lower layer. Negative covariance avoids this problem, by immediately identifying the specific neurons responsible for errors, without needing to travel down a hierarchy.\n\nAs neural networks become deeper, they cannot be re-trained quickly using top-down learning. Frequent updates of deep networks become prohibitively expensive, and new data might arrive by the time that they are done training on the old data! Negative covariance eliminates the lag and cost of re-training, allowing incredibly deep neural networks to be updated with each new batch of data. For many applications, that capability is the difference between a toy model and a usable product.\n\nCovariance Pushes Feature-Detection to the Lowest Layers, to Delay Over-fitting\n\nThis point takes a little effort to make: by applying back-propagation at neurons which exhibit negative covariance, the network is \u2018pushed\u2019 downwards, detecting features at the lowest layers possible. When features are identified at an earlier stage, it is harder for the network to stumble into a memorization. A proof of convergence requires that perfect accuracy is eventually reached, but this accuracy is less likely to be from memorization when the network relies on early feature-detection.\n\nTo show how all of this happens:\n\nImagine a neural network where the last layer is simply a copy of the layer below it; wires are connecting single neurons at full strength. The two layers\u2019 responses are identical. So, the last layer is effectively irrelevant, with all the real computation occurring at lower layers. We can say that the classification problem has been \u2018pushed down\u2019 to the lower layers.\n\nIf we make a neural network with more layers than it needs, we would hope that training would result in the \u2018pushing down\u2019 of feature-detection, eliminating the extra layers. This is because extra layers allow the network to classify with greater specificity \u2014 we need enough layers to specify our classification, but too much specificity lets the network memorize (\u2018over-fit\u2019) the training data.\n\nSo, if the classification task is \u2018pushed down\u2019 by our training algorithm, then the network is still able to attain accuracy for all the training data (that being the definition of \u2018layers it needs\u2019). Yet, it would not have the depth necessary to perfectly memorize the data; it would have only enough layers to learn effective classification. This property can be broadened to apply to all feature-detection at all layers: if the network is capturing those features at the lowest layer possible, then there is only enough depth to allow classification, not enough depth to allow perfect memorization. A network that \u2018pushes\u2019 feature-detection down to the lowest possible layers will delay over-fitting \u2014 and that is what I argue covariance achieves.\n\nSo, how does negative covariance \u2018push down\u2019 feature-detection?\n\nIf a neuron registers negative covariance, it is saying \u201cI fire when the network is making a mistake\u201d. To be able to identify mistakes, that neuron must be receiving data which are informative of a mistake-in-progress. That is to say: information that is reaching that neuron is involved in the mistake. (If this were NOT so, the neuron would be unable to detect errors, and so, it would not be registering negative covariance \u2014 the statement is tautological!) By back-propagating from that neuron, the neurons beneath it are discouraged, which is a discouragement of the information that led to errors. Any features which were active during mistakes would be discouraged, and as a result, the classification begins to rely upon features which were correct, because they are the only ones not discouraged.\n\nThis \u2018discouragement\u2019 applies at each layer; back-propagation discourages any feature detected at a lower layer that results in negative covariance at a higher layer. (This is exactly what the \u2018loss-function\u2019 does, at the output layer; the \u2018loss-function\u2019 is really just negative covariance restricted to the topmost layer. By generalizing this activity, negative covariance lets us do this at all layers.) A higher-layer neuron that was classifying correctly may have some of its inputs discouraged, because those inputs also travel to a neuron with negative covariance. That means the higher-layer neuron, by this discouragement, begins to listen more closely to the input neurons which were themselves classifying correctly. Taken to the extreme, the input neurons would be performing the correct classification, and transmitting that to the higher layer. At the topmost layer, this circumstance is equivalent to output neurons that fire when their lower layer input is a single, correctly-classifying neuron. That is what we defined as a \u2018pushed down\u2019 feature-detection!\n\nSo, a network that is trained using back-propagation from co-varying neurons will tend to discourage the activity of any layer\u2019s neurons which are involved in mistakes. The neurons that remain active are those which are already correctly classifying features. The layers above those correct-classifiers are effectively redundant. The affect of this is: correct classification is \u2018pushed down\u2019 to the lower layer, and merely copied to the higher layer (unless the lower layer cannot completely classify on its own, in which case the higher layer neurons receive some combination of activities from the lower layer \u2014 an example of when \u2018more layers are needed\u2019).\n\nA Sketch of the Proof of Convergence:\n\nWithout a proof of convergence, a training algorithm might jump around without ever attaining perfect accuracy on the training data. (It could attain perfect accuracy, but we wouldn\u2019t have mathematical assurance!) I\u2019ll introduce a few tricks, to wrangle convergence out of negative covariance:\n\nAfter running all the training data through our neural network, we separate the correctly-classified inputs from the misclassified ones, and measure the negative covariance between these two groups, for each neuron. We then apply gradient descent by back-propagation on each neuron, according to its degree of negative covariance. Suppose that, after applying that \u2018learning\u2019 from gradient descent, we find that the number of misclassifications has increased! In that case, cut the rate of \u2018learning\u2019 in half, and try again. Continue cutting the learning-rate in half, until the number of errors does not increase. Now that you have the same or fewer misclassifications, you measure negative covariance again\u2026 I\u2019ll call this process of cutting the learning-rate in half the \u2018half-learning\u2019 of the network. It guarantees that the rate of error does not increase.\n\nWhen will half-learning halt? When even the slightest learning-rate would cause the number of errors to increase. Could this occur when the number of errors is positive? Answering that requires some close observation:\n\nWhen there are still errors, some neurons exhibit negative covariance. That is, they tend to be \u2018on\u2019 during a misclassification, and \u2018off\u2019 during a correct classification, or visa versa. If the only neuron with negative covariance was one of the output neurons (the neuron that fired incorrectly, causing a misclassification), then the situation is equivalent to the traditional \u2018loss-function\u2019 with a misclassification at the output layer. This is already proven to converge in the existing literature.\n\nNow, suppose that there is a misclassification (which necessarily includes negative covariance at one of the output neurons) that also shows negative covariance at one other neuron, in a deeper layer. If \u2018half-learning\u2019 can find a small change to the co-varying neuron that eliminates the error, without creating a new error, then the network still converges.\n\nThere exists a \u2018half-learning\u2019 rate which does eliminate the existing error, without creating a new error. This is because the misclassifier on the output layer must depend upon the erroneous neuron and its inputs more than the correct-classifiers do. (If the correct-classifiers depended upon the erroneous neuron more heavily than the misclassifier did, then they would necessarily also misclassify inputs. Correct-classifiers on the output layer must draw upon inputs which are predominantly correct, by definition!) Because the misclassifying neuron on the output layer depends upon the lower-layer erroneous neuron more than the other output neurons do, a change to that erroneous neuron will have a greater impact on the misclassifier than it does on the correct neurons\u2026 The misclassification can be eliminated, using some small \u2018half-learning\u2019 rate, without changing the correct neurons too much!\n\nSo, we have demonstrated convergence for the case of a single neuron at the output layer with negative covariance (this is just the traditional \u2018loss-function\u2019, with convergence proven in existing literature), as well as convergence in the case where an additional neuron at some lower layer exhibits negative covariance (the \u2018erroneous neuron\u2019 in the above paragraph). By induction, I offer that additional neurons at any layer which exhibit negative covariance are similarly constrained, and are similarly convergent, as a result. Each erroneous neuron must be more strongly associated with a misclassification at the output layer than it is with correctly-classifying neurons, and so, a small change to those erroneous neurons will effect the misclassifier more than they impact correct classifiers. This is a guarantee that errors will not increase, and that they can decrease to zero.\n\nIs that all?\n\nI do not intend to imply that covariance allows a network to always converge faster than existing methods. Instead, covariance enables training of inserted clusters of neurons, providing faster re-training times. You can grow a new cluster of neurons anywhere inside the existing network, and train it on new data, while the rest of the network\u2019s weights are \u2018frozen\u2019. I suggest planting those clusters at the error-detecting neurons within a Mixture of Experts neural network. Each cluster essentially acts as a \u2018specialized expert\u2019 that extracts features from the error-detectors, to help correct mistakes! The network adapts to new data, without forgetting older lessons (that forgetfulness is the bane of re-training deep networks!)\u2026 I hope that negative covariance will facilitate the development of exceptionally deep networks that grow and adapt to new information quickly, without losing core insights that they learned in the past."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/a-market-for-externalities-d6696ad7d145?source=user_profile---------42----------------",
        "title": "A Market for Externalities \u2013 Anthony Repetto \u2013",
        "text": "Externalities are costs or benefits that are not represented by market prices. They are \u2018hidden\u2019 from the market \u2014 the long-term health benefits of exercise, or the health costs of smoking, for example. When you use a gym, you pay for your membership, but the gym does NOT get extra cash for having kept you healthy! Conversely, cigarette companies do not need to pay extra for the harm their products cause. Those impacts are external to the products\u2019 sale, and these externalities are a huge problem: the market does not value what it cannot price.\n\nMany positive externalities are provided at a cost that is much lower than the value that they create. For instance, the World Economic Forum notes that \u201cevery dollar spent on treating TB would generate $43 in returns\u201d for the local economy. Meanwhile, negative externalities are often pursued by companies and individuals whose profits are much smaller than the harm they caused. Paper mills often dumped mercury into local water supplies, which caused immense harm for generations, in return for slim profit margins. These are examples where externalities have a high multiple; a dollar spent on positive externalities generates MANY dollars of benefit, and a dollar of profit from negative externalities generates MANY dollars of cost.\n\nExternalities suffer from an additional problem: most of the external impact is felt by people who did not even use the product! Locals living down-river from the paper mill may not have purchased the paper products that it made, but they are the ones harmed by its mercury pollution.\n\nAnd, like the case of TB treatment, the benefits of positive externalities are felt by people at large (the over-all impact on economic activity) or people at random (who contracted tuberculosis) \u2014 a business would not know who to charge a fee, those people may be unable to afford treatment on their own, or the value to each person is so small that a business could not extract a profit if it was required to manage each transaction. So, externalities stay external to pricing.\n\nThis is where government comes in! Governments are, fundamentally, a way to internalize externalities. We each benefit from mutual defense, from enforcement of contracts, and from public education and health care. No business could handle these needs. Defense is such an irregular, strategic need, that no one would be expected to pay for it, until it was too late! Enforcement of contracts could not be left to private businesses; they would be able to extract larger profits by selective enforcement than they could from fair and just arbitration. And the immense economic benefits of a healthy and educated population are felt by all, while a business would be hard-pressed to ask all to pay for services which only a few use directly.\n\nYet, modern governments are bloated by social services which pay-out more to their employees than to their beneficiaries. And legislators regularly divert public money to \u2018pork\u2019 projects that serve local contractors and lobbyists. The \u2018cure\u2019 offered in response: privatize! By letting businesses accept government contracts for those services, the businesses will be encouraged to provide those services at a lower cost \u2014 because that leaves a larger profit margin for them. Unfortunately, if a privatized industry receives a government contract, they are incentivized to lower costs for themselves, but they are NOT incentivized to lower costs to the people. Enron did that. And, lowering costs by lowering quality of service is still a path to profits, because political lobbyists protect these contractors from being fined, or even having the quality of their service measured at all! Privatization doesn\u2019t work, as a result.\n\nSo, what could we do, instead?\n\nIdeally, a business would not need to win a government contract, for their services to be included as a positive externality. Instead, the government would be responsible for measuring the value of the actual benefits of that service, and the business would be paid accordingly. (No payments to contractors who failed to provide real benefits!) That same accrediting and regulatory body would be responsible for measuring any harm that a business causes, and levying fines accordingly. (No profits for polluting paper mills! No waiting for a class action lawsuit!) And, those fines would go toward remunerations for those harmed, to pay for the costs of operating the regulatory agency, and to pay other businesses for the positive externalities that they provide. Any additional positive externalities would be paid from a broad tax base.\n\nIn such a system, we would need experimentation. Some businesses might want to sell a product that would also provide health benefits, for example. They would need to sell their product to many people, for their claim of benefit to be verified and measured. Someone must take an entrepreneurial risk, to provide that business with its initial capital. If that business does produce health benefits, it should be paid accordingly, and some of that payment should go back to its investors. That is the Market for Externalities.\n\nAn Alternative to the Bloated Government:\n\nImagine a country where the government consisted of an accrediting and regulatory body, and tax collectors. Nothing else. Instead of government-run services, and businesses operating on a government contract, the government would ONLY extract fines, levy taxes, and make payments to businesses. Those businesses are closely monitored by the accrediting and regulatory body, and they are only paid when they produce measurable benefits. The payments would be a fixed percent of the ascribed value generated; not 100%. (Say, just 25%?) These payments would be made from a pool, which is filled first by any fines levied against businesses and individuals who created measurable negative externalities, and second, by a variable tax rate. Yes, taxes would vary from year to year, and you would only be taxed for the amount that was not covered by fines!\n\nA market would handle business proposals. Any proposal in this Market for Externalities could receive investment, which buys the investor a share of that proposal\u2019s dividend. When the proposal receives its budget from investors, it goes into action, and so too does the government\u2019s regulatory body. When enough data has been collected, to assess the real value of the proposal, that business is paid a percentage of that value. Its costs are covered, first, and the remainder is given as dividend to its investors. No profits are retained.\n\nWho would buy that?\n\nThe Market for Externalities takes the profit motive, and plugs it into the public good. And, the public good is a much better investment than MOST businesses! Consider the tuberculosis example: a dollar spent fighting TB will add $43 to the local economy. So, a proposal can be placed on the Market for Externalities, to fight TB. At a return of 25%, a business enacting such a proposal would receive $10.75 gross profits, for every $1 in expenses. That is a larger multiple on capital than Apple! If investors could get that kind of return from any for-profit business today, they would rush to it. But, no business is fighting TB, because the $43 gains are spread out over the entire population. You would need a tax base, to extract a portion of that price from everyone. That requires a government.\n\nYes, a Market for Externalities would not be perfect. Regulators would still need to be regulated by citizens and journalists. Market actors could still squander investors\u2019 cash, and attempt to fudge data to get more than their share of payments. However, it would be superior to BOTH government-run services AND privatization. Government, by restricting itself to only accrediting and regulating, would be simpler, and easier for the people to monitor and hold accountable. Each proposal, being visible on the Market for Externalities, would be priced by the accumulated foresight of all market investors, instead of hidden behind privatization contracts wrought by lobbyists. It wouldn\u2019t be socialism. And, it wouldn\u2019t be traditional capitalism. It would be a Market for Externalities. How does that sound, to you?"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/trust-busting-education-and-healthcare-d1bf9e61a37d?source=user_profile---------43----------------",
        "title": "Trust Busting Education and Healthcare \u2013 Anthony Repetto \u2013",
        "text": "Today, our youth is expected to pay 2 1/2 times what was demanded 30 years ago, for a four year college. High rates on student loans ensure that the actual payments for college are far beyond that increase in the \u2018sticker price\u2019. And, our elderly incur healthcare costs which have ballooned similarly. While those institutions charge more, nurses and professors do not see a similar increase in salaries. That \u2018extra cash\u2019 is going into the hands of administrators, debt-collectors, and companies euphemistically called \u2018providers\u2019. In any other market, such an increase in prices perpetrated by the sellers would be called collusion. Our diminishing standing in the global economy, and the expansion of our inequality, can be attributed in large part to these inflated costs.\n\nSuppose that, over the last few decades, all car companies raised prices on their vehicles by 250%. You would hope that these new, expensive models would be a significant improvement! And, you would expect the workers at those car companies to be paid more, in proportion.\n\nHowever, if those factory workers saw their wages flat-line or decline, and vehicle quality remained the same, then the difference in revenue for those car companies must be going somewhere else. Unless a huge decline in available materials was driving up price, that windfall is going to the executives.\n\nInequality is growing, primarily, because of this swindle.\n\n> If education was NOT becoming more expensive, then more Americans could afford to go to college. Their incomes would be higher.\n\n> If education was NOT becoming more expensive, then Americans would have more money SAVED \u2014 and they would be paying interest on LESS DEBT.\n\n> If health care costs were NOT rising, and our care was similar to other advanced economies, then more Americans would receive quality care, earlier; there would be less productivity lost to poor health.\n\n> If the average worker, benefiting from improved health, was earning more, then they would have more money SAVED \u2014 consequently, they would be paying interest on LESS DEBT.\n\nOur debt payments are a transfer of wealth that would not occur if our incomes were higher (because of improved health and increased education) and our costs were lower (because of reasonable prices for healthcare and education). If such a massive transfer of wealth were flowing from the rich to the poor, it would be decried as socialism. Instead, this money flows from the poor and middle-class into the hands of the rich, and constitutes price-fixing and extortion. The combined effect of these costs, and the additional interest on debts that result, have caused the majority of the decline of the middle class.\n\nIf education and health care had remained at prices seen decades ago, then each of us would have an extra ten thousand dollars, each year. That comes from a combination of lower costs, higher incomes, and NOT paying unnecessary debts. Lacking that windfall, more of us go further into debt. Those additional debts incur interest, exacerbating our losses. This is the key money-siphon in our economy.\n\nBecause our economy adds these fictional costs, without supplying anything, our overall demand suffers. The economy fails to grow businesses if its consumers cannot afford to buy. Meanwhile, the industries and individuals who profit from this hack are able to re-invest their spoils, driving stocks higher. When that stock-rush yields a return, they pay lower taxes (or none at all, by offsetting capital gains with the sale of poor-performing stocks)! As a result, our government debt grows, as well.\n\nNews articles lament the easy-to-spot statistics: wages have been flat or in decline for most of us, while corporate bosses take home huge bonuses. The real picture is slightly more complex. Yes, CEOs are walking away with exorbitant salaries and compensation, even when their companies are doing worse. That is a real problem. And, wages have been flat, while basic goods\u2019 prices rise. That is a source of some of the strain on the average American.\n\nYet, these sources do not account for the heinous disparity growing, here. Debt-payments do. And, those debts are made worse, primarily, by the costs of education and healthcare, as well as the lost income opportunities of good health and schooling. The effect of lost income opportunities on our national productivity makes us less competitive, putting a squeeze on all of our businesses. Wages continue to suffer, as a result. And, the debt that results from lower wages funnels cash into the pockets of creditors, and eventually to investors, raising stock values that supply most of the wealth to the elite.\n\nEven if we magically tempered executives\u2019 excessive compensation, our growing income inequality would not be halted. Lost income, from poor education and healthcare, combined with debts, again from education and healthcare, are strangling what remains of an American middle-class. Without serious reform, our situation will get much worse."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/breaking-moores-law-806d2db9a1d?source=user_profile---------44----------------",
        "title": "Breaking Moore\u2019s Law \u2013 Anthony Repetto \u2013",
        "text": "I\u2019ve been warning people about the end of Moore\u2019s Law since 2007, when signs of a slowdown first began to appear. I wrote about the significance of that slowdown here, in 2011. Now, six years later, we have definitely reached that point.\n\nIf Moore\u2019s Law had continued as futurists (Ray Kurzweil, for example) insisted that it would, we would already have processors running at gigahertz speeds. Instead, Intel\u2019s fabricators have slowed their \u2018doubling time\u2019 down, from the Law\u2019s 18 months, to a wait-time of many years. And, we cannot hope for more than a few additional doublings, even if we wait for decades: the physical laws constrain our use of electrons, at such small scales.\n\n\u201cWon\u2019t new technologies allow us to continue our incredible improvements?\u201d\n\nSomewhat. Printing more layers (e.g. 3D fabrication techniques, first seen in flash memory) lets us squeeze more transistors on the same block of silicon, and keeps connections close together. Graphene, and other 2D materials, may allow us to utilize spintronics and other fancy forms of energy as our 1\u2019s and 0\u2019s. Yet, those advancements are on a timeline that is forever receding, like a nightmarish hallway that grows longer as you run along it.\n\nWorse for us, there is another doubling-paradigm which has not shown signs of stopping: new fabrication technologies generally cost twice as much as the last method. This pattern has held for all of Intel\u2019s fabricators: the price of new facilities has ballooned. That cost was supported, for decades, because computers were a growing share of the economy, computer purchases were a growing share of disposable income, global population was growing, and economic growth meant that that growing population had a growing disposable income, too! A fabricator that was twice as pricey didn\u2019t matter, in those conditions. The increase in chips sold meant that the fabricator\u2019s cost was spread-out over many more chips.\n\nNow, as the rate of increase in demand for chips has slowed, those fabricators\u2019 bills must be divided among roughly the same number of chips. The costs start to add up. My prediction was that fabricators would become too expensive to be worth it. Chip manufacturers would see better returns from older fabricators, which had already paid-down their bill, than they would from a new fabricator; new chips would be too expensive to be marketable.\n\nUntil 2D materials technology makes it to market, we will not know if those promised advances will be cheap enough. I worry that we may be at the end of the Chip\u2019s Golden Age.\n\n\u201cBut, I don\u2019t really need my laptop to be that much faster\u2026\u201d\n\nYou may have all the computing power you need, but Jeff Dean doesn\u2019t. Google, Amazon, Facebook, Microsoft \u2014 they are all betting on artificial intelligence, running on their clouds, as the future platform for a dizzying array of services. Those AI-based services will need ever-growing computer power to be trained and utilized. With the death of Moore\u2019s Law, that entire premise is weakened.\n\nWithout a steady improvement in chips\u2019 cost-effectiveness, new companies will not be able to compete with the giants of industry, and greater machine intelligence will come at a greater cost. Google hopes its Tensor Processing Units will give it an edge. That lead is only a one-off advantage. What do they do, when their computational needs grow further? My bet: they will have to twiddle their thumbs, until materials science catches up.\n\nWithout the materials science improvements that can make photonic and spintronic chips a reality, tech will stall. Companies will be reduced to app-specialization, tailoring AI to each special use-case without improving AI\u2019s overall power. Like the spam of niche apps, today, but for business operations. And, each use-case will allow a one-off marginal improvement for those businesses, not an ongoing growth that fuels productivity. We need serious funding of materials research, or we face a deep, dark winter of technology.\n\nGoogle, Microsoft, please: look to your future with clear eyes, and see that your growth potential vanishes without materials science. Don\u2019t get lost in neural networks, while you speed toward this \u2018soft wall\u2019 in hardware. The global economy really does depend upon it."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/rebuild-the-glaciers-72b04b843c34?source=user_profile---------45----------------",
        "title": "Rebuild the Glaciers \u2013 Anthony Repetto \u2013",
        "text": "In December of 2015, a large solar flare struck the arctic. I warned that this inflow of hot, ionized gas would have a huge negative impact upon the arctic: the \u2018cold cap\u2019 of air high above the arctic would be warmed, and the polar vortex would weaken. I predicted that the breakdown of the polar vortex would send waves of cold, ionized air down to the states; around the Hudson and Great Lakes, residents would be exposed to icy snowstorms which simultaneously displayed lightning. At the same time, this movement of cold air toward the temperate zone would be accompanied by globs of warm air moving north, which would accelerate arctic melt. Within a few weeks of my prediction, there was \u2018thunder-snow\u2019 across the Great Lakes region.\n\nI supposed, back then, that the polar vortex would be unable to recover. Continued waves of cold from the north would displace the jet stream, and this would make hurricanes and major flood disasters more likely. While the weather channels estimated that we would have a \u2018normal\u2019 year, I warned friends and family from Texas to Florida to be ready. Unprecedented floods did arrive, as I predicted, and hurricane Matthew was the closer to the season.\n\nIt will get worse:\n\nI also estimated, back in January 2016, that 2017 would not be as bad: the poles take turns being warmer and colder, and the arctic would have a year in reprieve. However, the additional melt from the solar flare would allow the waters of the arctic to warm \u2014 sunlight which was normally reflected by snow and ice would instead be absorbed by the open seas. 2018, when the polar oscillation returned, would be particularly bad.\n\nI began drafting a plan to rebuild the glaciers and sea ice. You can see the illustrations of my plan, from spring of 2016, here, and here.(Many months later, a physicist in Arizona, Steve Desch, promoted a similar plan, which has garnered a lot of press, yet fails to identify the strong constraints on design which I outlined first\u2026 and he doesn\u2019t have fancy illustrations by Liz Ray!)\n\nHow to Fix It:\n\nArctic waters must be re-frozen, quickly. The problem with re-freezing is that the ice acts as an insulating blanket, keeping the water from cooling. Arctic air is MUCH colder than the ice, and would be able to freeze surface waters rapidly, if that water was pumped up into the air.\n\nI offered that we spray arctic waters into the air, and let those tiny droplets freeze by radiating heat into space (as infrared, or heat-radiation). This is superior to the pump-into-pond method offered by the Arizona team.\n\nThe water droplets, freezing in the air, would be allowed to fall back onto the ocean surface. There, they would melt, but they would also dramatically cool the surrounding water, forming a thick \u2018slushy\u2019 as surface waters approached the freezing point.\n\nI stipulated that, without additional effort, this would be insufficient. We would also need a net, supported by buoys, which would roll that slushy-ice to shore, where it could be gathered and stored on land. (See illustrations at bottom of article) These \u2018ice-logs\u2019 would be a reserve of cold that would help maintain lower temperatures during the summer months, and assist in cooling and re-freezing in the winter.\n\nNo. Again, my plan differs from the one proposed by the Arizona team: beneath the ice-catching net, a tarp captures the brine which descends when ice forms. This brine can be pumped to shore, sprayed into the air to freeze, and fall into ponds. A process of re-freezing and separation allows for the extraction of brine salts \u2014 potassium, magnesium, and lithium, in particular. Brine-freezing is already a source of these salts, and the design I proposed would allow a vast increase in salt production.\n\nThese salts have various industrial uses, and lithium in particular is essential for the battery systems used by green energy technologies. Moreover, the other salts have been observed to operate as batteries, though at a lower efficiency. I offered that these salts could be stored on-site, and would hold solar power that was collected by large, inflatable solar-thermal trough-collectors.\n\nSo, the entire operation would run on solar power collected by inflated troughs, and that solar energy would be stored in brine-batteries. The area collecting solar energy would also be protecting ice from that sunlight, allowing for additional cooling. The near-constant 10mph winds of the arctic would freeze brine-pools and open waters, generating ice and more brine. And the salts with industrial markets would pay for the entire operation.\n\nHere are those visuals by Liz that I mentioned:\n\nSo, rather than pump sea water on top of the ice, as proposed by Steve Desch\u2019s team from Arizona, we could extract valuable commodities which would pay for the entire operation, and build a reserve of ice faster, using this technique. The only problem is: we need to start immediately, before there isn\u2019t any ice to save!"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/automation-is-different-c56b0a3d91f4?source=user_profile---------46----------------",
        "title": "Automation is Different \u2013 Anthony Repetto \u2013",
        "text": "Numerous surveys of the global employment profile have identified which jobs are \u2018at risk\u2019 of being automated. And, hand-wringing editorials on automation have exploded. Yet, an apologist narrative has taken hold: \u201cNew jobs will be created.\u201d \u201cAutomation will increase productivity, so that the total effects will be beneficial.\u201d And this: \u201cWe have seen much larger changes in the workforce, in the past, WITHOUT massive unemployment.\u201d Sadly, these arguments lack insight into a few fundamentals which make automation a serious concern for the average worker and political institutions.\n\nWhen the US experienced drastic changes in the workforce during the age of locomotives, and again during the transition to cars, we did not see massive unemployment. Instead, improvements in technology lowered prices, and made workers as a whole more productive. Each person could afford more, even after they moved into employment substitutions. New jobs paid well. So, the reasoning goes, automation will be a similar boon.\n\nHidden from that analysis is a simple fact: when the US and Europe were revolutionized, the rest of the world wasn\u2019t. Improvements to farming, transit, and mass production were local phenomena. A flood of new secretarial work was supported by each nation\u2019s businesses, because it made those local economies even more competitive. (And, many jobs were lost in colonized regions\u2026)\n\nAutomation, in contrast, will hit everyone on Earth with rapidity. This \u2018globalization of change\u2019 means that a US firm cannot simply hire-on surplus labor at a marginal benefit. Automation would need to be adopted by different countries over the course of generations, to allow that.\n\nWhen laborers-past moved from one job to the next, both of those jobs were low-skill or unskilled. Re-training involved a few months, and could be covered by the business itself. Time spent training for the old, replaced job represented only a few months, as well; little of the worker\u2019s time was a sunk cost, when their job was replaced by a machine.\n\nToday, the costs of training for employment are immense: years in college, as well as internships and additional on-boarding time, most of the cost being borne by the workers themselves. This factor is an enormous and neglected component of the burden of automation on a worker.\n\nImagine spending four years in college, racking up debt, and working another five years\u2026 when your job is automated. Your entire sunk cost is wiped out, and your debt remains. This situation is completely unlike past waves of mechanization. Worse, any new program you enter, to train you for a new job, will incur additional debts without any guarantee that your NEW job will not be automated as well! (Especially considering that, in such a scenario, the machines will have those same extra years, to catch up to you\u2026)\n\nPast eras\u2019 improvements were in industries that served basic needs. When farm equipment was modernized, we had fewer farmers, and that was okay because everybody needs to eat. While a drop in food prices was a huge benefit to the average worker, that monthly savings meant nothing to the bottom line of the super-rich. Automation is distinct, in that respect.\n\nLabor is already a small and diminishing portion of business costs (while advertising and licensing grow), and basic needs\u2019 prices are unlikely to be reduced much by automation. Rent is the biggest of these expenses unaffected by automation!\n\nInstead, automation lets businesses pay less in pensions, medical benefits, and re-training. Those savings usually turn into bonuses for upper management, not lower prices, especially when a larger portion of business activity is going to meet the wants of the wealthy. (They happily pay more for exclusivity, privacy protections, and higher quality service, while it is the low-income earners who shop around for bargains.)\n\nAutomation will not lower costs for the average consumer, but it will improve the comparative advantage of the businesses that adopt it first. So, if you hold stock, you will have a few years to move your money around, to take advantage of rising valuations. People who do not own as much stock as the Kushners, however, will not see an improvement in their lot.\n\nThat automation will line the pockets of business investors, without lowering costs for most basic consumables, is only half of the problem. The difference between return-on-labor and return-on-capital will grow, too. This is doomed to accelerate inequality. Considering that most peoples\u2019 wages have been almost flat for decades, any increase in that capital-vs-labor spread will push the majority\u2019s incomes into decline.\n\nWith the rapid, global reach of investment vehicles and money-transfers, governments cannot reliably tax the super-rich. Pleas for a universal basic income and sponsored re-training programs are unlikely to be heard, as the owner-class can safely ignore the demands of workers from foreign countries, separate industries, and districts flooded by disinformation, partisanship, and propaganda.\n\nAutomation today is unlike mechanization of the past. Each of those waves required a new machine. Businesses had to purchase that new piece of equipment, to stay ahead, slowly phasing-out earlier iterations. Our software-based automation allows a single robot to serve an ever-growing range of functions, and cloud services can automate service workers in the time it takes to download a new app. Once a business is equipped with automated services or robotic arms, it can add new functionality rapidly.\n\nTo use physical state-changes as a metaphor: mechanization \u2018melted\u2019 labor in one domain or another, like sunlight falling on separate spots of a large glacier. Automation today, in contrast, threatens \u2018sublimation\u2019, where entire work forces that previously had solid employers suddenly evaporate.\n\nHumans need time to re-train (for today\u2019s skill-based jobs, much more than unskilled labor in the past). Automation\u2019s threat of rapid and total obsolescence brings immense uncertainty to many occupations, just when we need solid assurance that our college degrees will supply careers to match.\n\nAside from the cool toys and on-demand services meant to wow us, we are unlikely to see many benefits of automation, here at the bottom. Unless we automate upper management!"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/the-three-body-problem-with-epicycles-64eb2b2706c1?source=user_profile---------47----------------",
        "title": "The Three-Body Problem, with Epicycles \u2013 Anthony Repetto \u2013",
        "text": "Kepler found the relationship between areas encompassed by elliptical orbits, back when epicycles were cool. Galileo discovered the consistent force acting upon falling bodies, and epicycles were the norm. Newton showed that gravitation fit observations of elliptic orbits of planets, and epicycles fell out of fashion. Astronomers have relied upon the elliptic model ever since. Epicycles were dead.\n\nNewtonian mechanics relied upon the fact that the sun is enormous, compared to all its orbiting bodies. This difference in mass simplified the problem of orbital motion into what is called a \u2018two-body\u2019 problem: the HUGE sun, and a comparatively tiny planet. Influences from other planets could be ignored.\n\nNot all orbits can be simplified that way, and the elegant ellipse cannot capture the motion of orbiting bodies when three of them influence each other. THAT is the \u2018three-body\u2019 problem, and it is unsolved for the general case. (Certain special cases have been solved, such as the orbits of Earth\u2019s Trojan asteroid groups at the Lagrangian points\u2026)\n\nI offer that, to solve three-body orbital mechanics, we must revivify epicycles\u2026 in an unusual way.\n\nThe Set of Epicycles, Mapped as a Manifold:\n\nEpicycles work a bit like a Spirograph. A larger circle is used as the rim for the movement of a smaller circle, which rotates as it progresses along the edge of the larger circle. Together, they form florets, like an ellipse that tilts while it orbits. More complex epicycles can be formed, by placing more circles within the smaller circle; each circle completes its own rotation at a different pace. Together, they account for the \u2018wobble\u2019 of the moon, Mercury, and comets.\n\nAs astronomical observations improved, more epicycles were needed, to account for all the subtle wobbles in the heavens. Newtonian astronomers considered epicycles overly complex and redundant. Our computer-era may allow us to tackle that complexity, and finally solve three-body orbits.\n\nEach orbit, even with many bodies, does have an associated epicyclic description. We just need a map, to understand their relationship. To do so: given three particular masses orbiting each other, you find the epicycles which describe their motion. As you vary the mass, position, or velocity of any one of those bodies, the epicycles of all three must be changed. With a map from one particular instance of [mass, position, velocity] to [epicycles], we can elaborate to a map of other epicycles continuously. (That is, an infinitesimal change in mass, position, or velocity will create an infinitesimal change in the circles used to describe the orbits; they are both without discontinuity, and form a smooth map, one to the other.) This continuous map is a manifold.\n\nWhy can\u2019t we just find a function for the orbits of three bodies?\n\nFunctions describe a lot of things, but many behaviors cannot be reduced to a function. A circle does not pass the \u2018vertical line test\u2019, and must be expressed as a set of parameterizations \u2014 it requires two functions, together. Three-body orbits are worse.\n\nInstead, a map from [mass, position, velocity] to [epicycles] can be found experimentally, using many particular orbits, until the shape of that manifold can be predicted. My guess is that the manifold itself does have a functional description, but many of the orbits we would study require enormous numbers of epicycles. Newton couldn\u2019t have done it by hand. (Ask Leibniz!)\n\nIf such a manifold is revealed by computer experimentation, and enough points on its surface are found for us to fit a function to the shape of that manifold, then we will have a map of solutions to the three-body problem. With that map, you could take the [mass, position, velocity] coordinates of your three objects, and find the associated epicycles which predict their orbits exactly. No one function can describe all orbits, but we would have a function that indexes Spirograph-solutions. So\u2026 can I borrow your supercomputer?"
    },
    {
        "url": "https://medium.com/@oaklandthinktank/future-cities-and-policy-66674419f604?source=user_profile---------48----------------",
        "title": "Future Cities and Policy \u2013 Anthony Repetto \u2013",
        "text": "Back in 2004, I had the opportunity to attend the EcoCity Builders conference at the Masonic Hall in San Francisco, and spoke with the founder, Richard Register, about my designs for co-housing and urban architecture. Since the 60\u2019s, Richard has focused on re-modeling structures within existing cities, to allow for natural environments, rooftop gardens and plazas. I emphasized our need to build entirely new structures, due to the constricting effects of pre-existing infrastructure. He didn\u2019t like it.\n\nMy argument was simple, and I have seen multiple designs since which follow this trajectory: urban spaces must become hyper-dense, with a minimum of space devoted to transit arteries, and a maximization of mixed-use and re-configurable spaces. When services and utilities are hyper-dense, the population spends less time in transit between each goal, and as a result, accomplishes more goals in a day. That is key to urban productivity.\n\nThe absence of hyper-density in our cities is a primary constraint on increases in productivity and wage growth. Not only do our cities waste billions of work-hours each year with traffic during the commute; the search for parking, traffic between urban destinations, the cost of parking, the expense of vehicles \u2014 these are, in sum, greater than our expenditures on research and development!\n\nWith the oncoming wave of automated vehicles, there is renewed global interest in eliminating traffic and parking-induced lag. Yet, only Singapore (so far!) is considering building new urban settings where the roads are submerged as one-lane tunnels without drivers. This sounds suspiciously like a city without roads, and a massive subway system, instead. That was part of my pitch to Richard, and China will be able to do it.\n\nAny city which eliminates roads and bans personal vehicles, in favor of a \u2018leaf-vein\u2019 system of subways, can pack goals much closer together. This doesn\u2019t just mean that your favorite deli is half as far away; it means that there are fewer delicatessens, that each see higher rates of customers, and they can provide higher quality products.\n\nRestaurants and shops need to be re-configured, as well: no indoor seating, and no shoppers wandering the aisles. Instead, a restaurant\u2019s ordering and pick-up counter are front-facing, and the footprint of the establishment is only the kitchen. Restaurant-goers will instead sit along the steps and ridges of large, curled step-mounds in a central plaza. Shops\u2019 check-out counters are front-facing, with a densely packed backroom, where your order is retrieved by the clerk. Gaudy floor plans of banks and boutiques may appeal to the Western elite, as a \u2018Versailles of commerce\u2019, but they are counter-productive to a city that hopes to compete for another generation.\n\n\u201cBut, if shops and restaurants have no indoors for customers, what do you do when it rains?\u201d\n\nCities have been praised as organically growing entities, continuously changing, with new buildings and spaces added along the margins, and interiors re-vamped by new construction. Old buildings are replaced with higher-density structures as the city grows.\n\nUnfortunately, that is not how a real organism grows. I do not need to get permits, and aggressively buy-out property from my internal organs, to replace them with high-rise organs. My blood does not sign a new lease or pay a percentage to an agent when it moves from one part of my body to the next. The model of \u2018creative destruction\u2019 does not work within a real organism. I am created with an ontogeny in mind, and I do not grow because of addendum from competing interests.\n\nSo, a city must be seen as a planned and complete entity. Future cities must be designed with a whole, singular plan, and must be constructed as a whole, singular building. That building is only the scaffold for the changing businesses within it, providing multi-level walkways which are entirely protected from the elements, as a pseudo-interior. These cities will rely upon their periphery to provide the open spaces we enjoy. If a city is hyper-dense, with a \u2018leaf-vein\u2019 subway connecting its entirety, then the trip to a periphery park takes less time and fewer resources, while providing a higher-quality environment. And, you would not build suburban housing on top of those periphery zones. The city has a planned scale.\n\nIn our blindness, we have given the task of weather-proofing interiors to each building. A planned and singular city-structure unites each shop\u2019s and home\u2019s demand for weather-proofing with a single, massive scaffold and \u2018tent\u2019. Within that \u2018big tent\u2019, each scaffold region is only a skeleton of support, and interior walls and structures are fitted to it without permanence.\n\nSome steps have been taken toward this model of \u2018scaffold+re-configurable interior\u2019. These at-will architectures have been showcased in Europe, and are used in China. The re-design of Google\u2019s campus is a posh imagining of the \u2018big tent\u2019 model, but fails to integrate the various utilities and services needed for a city. (Richard Register didn\u2019t like such a vast re-construction at all; it is antithetical to his idea of transforming existing spaces.)\n\nOur existing spaces will need to be abandoned, or re-purposed, because each apartment suffers from leaks, mold inside of walls that cannot be replaced easily, interior beams that cannot be re-fitted to accommodate different floor plans, and pipes and wires which are hard to access, all of variable and unknown quality and lay-out. A single, super-scaffold \u2018big-tent\u2019 eliminates these hassles for each interior structure. And, without the constraint that each building stave-off weather, those interior structures can be built faster, with lighter, cheaper, and greener materials. The entire city is a bargain to construct and maintain, when it has a built-in umbrella.\n\nThe structure of the personal home must be abandoned, for the hyper-dense city. In this kind of city, each family\u2019s \u2018apartment\u2019 consists of the personal spaces: bedroom, bathroom, living room. A central kitchen, work spaces, playroom, and lounge are shared by a few families together. Within the massive scaffold, these apartment spaces can be re-configured, allowing a tower\u2019s floor plan to grow and shrink each division according to the number of occupants. And, balconies and mezzanines with some depth (not a narrow \u2018smoker\u2019s balcony\u2019!) provide vistas (where great distances can be seen over a wide angle of view) and height (where the ceiling is not always within reach).\n\nThe city itself, being a singular construction, is also the single owner/operator of interior towers. Utilities are laid out and maintained with a unified plan, and can be accessed through the re-configurable walls. Apartment and shop owners lease or buy directly from the city itself; no private developers vie for renters while keeping their building\u2019s follies hidden. When a family buys an \u2018apartment\u2019, they are buying a share of the city, and have rights to their purchased space, wherever it may be. If their needs grow, they can buy more shares of the space, to expand their existing apartment (if space on their floor allows it), or they can move to a new floor or building that accommodates them. No apartments are \u2018bought and sold\u2019 just to handle a move.\n\nCountries that have the central will and financing to build these kinds of cities will have a distinct economic advantage. China, with its habit of building entire cities from scratch, will be the one to watch. Europe, already laden with historicity, will find it difficult to adapt. Here in the US, we have plenty of space to build new cities this way, but our fixation on the supposed virtue of real-estate tycoons may be an Achilles heel to our productivity and progress.\n\nOur new cities will be built as whole entities: hyper-dense, car-less, surrounded by parks and gardens, with re-configurable interiors, a vast tent-scaffolding exterior, and direct purchase of square-meter shares which can be transferred between locations. No pot holes. No traffic. No slums or suburban sprawl. Shops and restaurants with pick-up counters facing topographic plazas. Everything within reach, along frequent all-night subway lines. You may need to visit Singapore to see it."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/stopwatch-bandwidth-5e478167aa53?source=user_profile---------49----------------",
        "title": "Stopwatch Bandwidth \u2013 Anthony Repetto \u2013",
        "text": "The 1\u2019s and 0\u2019s of data fly through the air on our WiFi as the presence and absence of pings from our wireless hubs and devices. A message \u20181001101\u2019 would need four of these pings, one for each \u20181\u2019 in the data. Sending every \u20181\u2019 makes our bandwidth cluttered. When our bandwidth is cluttered, each signal needs to be stronger, to shout over all the others. This crimps battery life, and WiFi range. We have an alternative: the stopwatch.\n\nA stopwatch, in contrast, lets you send as much data as you want with only two pings. Here is how: when I want to send you a message, I ping once, and start my stopwatch. When you hear that ping, you start your stopwatch, also. As soon as my stopwatch displays the time (in binary) that matches my data, I send the second ping. You stop your stopwatch when you receive that ping, and your stopwatch has the entire message on it, represented by the time elapsed. You got the message, by reading your stopwatch!\n\nFor example: I want to send the message (in binary) \u20181001101\u2019. I send a ping, and the stopwatch begins counting, \u20180\u2019, \u20181\u2019, \u201810\u2019, \u201811\u2019, \u2026 eventually reaching \u20181001101\u2019 after 77 (micro)seconds. When that count is on my stopwatch, I send the second ping, telling you to stop your stopwatch. Your stopwatch, counting the same increments, would read the same time: \u20181001101\u2019. Instead of sending four pings, for each \u20181\u2019, I\u2019ve only sent two\u2026 albeit, in a longer stretch of time (77 microseconds, instead of 8 microseconds). This means that the network is not as noisy, both because of fewer pings, and because of a longer time between pings.\n\n\u201cThen bandwidth is slowed, waiting for your stopwatch, right?\u201d\n\nNot necessarily! If we only have one frequency that we share, true, we must wait longer for our messages. However, if our devices can listen to a few different frequencies, then we can dramatically increase bandwidth with another neat trick: frequency signatures.\n\nSuppose our devices can listen to different frequencies called \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019 \u2014 these would be slightly different wavelengths of radio signal, in our WiFi example, but they can also be different frequencies of visible light in a fiber optic channel, as well. Normally, to avoid cluttering the bandwidth (and confusing other peoples\u2019 devices), you would send and receive data along only one of those frequencies, and someone else would need to use a different frequency for their communications. That\u2019s why each radio broadcast has its own station, tuned to its own frequency. With our four example frequencies, we could have four radio broadcasts.\n\nHowever, we could also organize our devices so that each device only listens to pings that occur on a subset of those frequencies. One device broadcasts on frequencies A and B, another transmits using A and C, another uses A and D\u2026 Together, this set of combinations is larger than our original four channels. (AB, AC, AD, BC, BD, CD gives six different channels!) Those frequency-pairs are signatures, and our devices listen only for the signature that matches, while ignoring pings that are only on one or the other frequency.\n\nWhen you have more frequencies to choose from, the number of combinations grows rapidly: four frequencies becomes six pairs, but ten frequencies becomes forty-five pairs. What\u2019s more, when you have many frequencies, you can use a larger subset: ten frequencies, split into five-frequency subsets, gives 252 subsets! So, our devices might be talking to each other, and only listening when they hear a ping across the five frequencies \u2018ACEGI\u2019, while another pair of devices only listen when they hear the five frequencies \u2018BDFHJ\u2019. While you previously could only have ten separate conversations, you can now have 252 separate conversations, with each device only listening to the conversation tuned to it\u2019s subset.\n\n\u201cWhy couldn\u2019t you just use that combination-trick with existing data-transmission? Why would you need to pair it with the stopwatch-trick?\u201d\n\nIf everyone sent a ping for every \u20181\u2019 that appeared in their data, a transmission using these subsets would be even noisier \u2014 you are transmitting all the same bits, but along multiple frequencies simultaneously. For the example message \u20181001101\u2019, a five-frequency subset would be sending four pings (for each of the 1\u2019s in your message) over all five frequencies, which would be twenty pings in total. Wasteful!\n\nBy relying upon the stopwatch-trick, even a very long message would only send two pings along those five frequencies, for a total of ten pings. As a result, a message with more than ten 1\u2019s would require less energy to transmit!\n\n\u201cThat very long message would take a very long time for your stopwatch, right?\u201d\n\nYes, and that\u2019s okay: your device can send a packet of information along one signature, and another packet of information along another signature. Because there are many signatures to choose from, your device sends all packets simultaneously. Each packet might take 100,000 microseconds to transmit by stopwatch, but the entire file still arrives in 1/10th of a second. Another concrete example: a binary message that is 16 bits long, using a microsecond stopwatch, would take about .065 seconds to transmit. If you have 252 signatures to choose from, you could send over a hundred of these messages concurrently, for a file that is >1,600 bits. (In reality, data is sent at intervals much faster than a microsecond, so you could send larger packets over a real WiFi network.)\n\nKey to this concept: while the stopwatch slows down the bit-rate for each packet of data, it also frees the bandwidth to transmit over many more signatures, sending many more packets at the same time. Stopwatches would slow each packet exponentially, in fact. But, the number of signatures available grows combinatorially, which is faster than the exponential slowdown! On balance, a file sent using many signatures for many \u2018stopwatched\u2019 packets can be received a little bit faster than a file sent as one long string of 1\u2019s and 0\u2019s. Thankfully, a modest increase in speed is NOT the only benefit of these two tricks!\n\nWhen your wireless connection misses some of its data, that data is corrupted, and the network has to send the whole message again. And, when many people are using the same network, it becomes very likely that your message became garbled alongside someone else\u2019s message. That\u2019s why your cell phone won\u2019t download apps at a concert. Everyone\u2019s device is talking at once, which causes their messages to fail. Because the transmissions were corrupted, every device talks again, trying to get their message right \u2014 and no one gets to talk. The effect is similar to a traffic jam: if everyone\u2019s car is slowed even a little, that means you all spend more time on the road. If everyone is spending more time on the road, then traffic gets worse!\n\nBy using a stopwatch and frequency signature, you avoid the traffic jam of noise. There is only a minuscule chance that other peoples\u2019 signatures will ping at the same time as your stopwatch, and the chance that their signatures will completely overlap yours is almost zero. Some fancy-looking equations demonstrate this, but if you have the background to understand them, then I\u2019m sure you can check for yourself. [:\n\nThis part is really cool. When your laptop tries to talk to a WiFi hub, it has to signal its identity\u2026 with every single message. Essentially, there is a load of identification data that must be sent, along with your message: \u201cHi, this is Jill\u2019s laptop, and I\u2019m trying to send a message to GroundDownCoffee\u2019s wireless\u2026 my message is\u2026\u201d These identifiers are a large part of the data that\u2019s passed back and forth. With frequency signatures, that data is no longer expressed by extra 1\u2019s and 0\u2019s \u2014 it\u2019s expressed by the combination of frequencies your device uses!\n\nSo, your laptop would select a set of frequencies, and agree upon those frequencies with the WiFi hub \u2014 just once. Then, any messages it sends are automatically seen by that hub, and visa versa, without padding the data with extraneous bits. That lets the network transmit even more data per second, and allows it to handle many more users simultaneously.\n\nThis becomes more interesting when we look at massive asynchronous servers. With a stopwatch and frequency signature, two nodes of a giant network can talk to only each other, without including routing data. If that doesn\u2019t sound like a big deal, consider: in an array of 1,000+ cores (each its own little processor), you normally must send a message UP to a governing and routing hub, which reads the address you came from and the address you\u2019re sending to, and then re-directs the message back DOWN to the appropriate core. When a lot is happening, that UP/DOWN routing becomes a bottleneck (and a pain in the neck, too). Signatures, meanwhile, can be chosen for each core-to-core dialogue; when one core wants to talk to another core, it transmits along their shared signature. Everyone else diligently ignores that message. Done.\n\nMassive asynchronous servers can make full use of these tricks, by allowing party lines and functional lines. By \u2018party line\u2019, I don\u2019t mean the screed of a particular political group \u2014 I am referring to the old telephone option for multiple callers on the same call. If a dozen cores are all working together on a certain task, they can share a frequency signature, to stay updated on each others\u2019 progress. No need to disturb the governor! And, cores in a massive array can also have dedicated functions that they serve: \u201cEvery time I hear data along the signature \u2018ACDFG\u2019, I know that they are asking me to multiply the first half of that data by the second half, and I should send the answer back along the response-signature\u2026\u201d By eliminating the bits used to route that data between cores, messages can travel at much higher bit-rates, and more of the chip can be invested in real computing power, instead of routing.\n\nSo, do fewer dropped packets, lower noise, lower power requirements, elimination of routing data & extraneous chip-space all sound like good things? Just ping me at \u2018ACDFJ\u2019 \u2014 I\u2019ve already started my stopwatch."
    },
    {
        "url": "https://medium.com/@oaklandthinktank/not-mars-1a508cc7b23a?source=user_profile---------50----------------",
        "title": "Not Mars \u2013 Anthony Repetto \u2013",
        "text": "The ongoing obsession with a manned mission to the Red Planet is misguided. Habitation would be expensive, and worthless. Terra-forming would take too long. And, we have better options, closer to home: Mercury and Venus.\n\nThere simply isn\u2019t a lot of stuff that we need on Mars. Carbon, silicon, iron \u2014 we have plenty of those things here on Earth! If we just need extra space to inhabit, an orbital platform in the Goldilocks Zone would let us grow food and live comfortably. In any supposed \u2018planet-wide disaster\u2019 scenarios, an orbital platform would be far superior to a Martian colony, as well: we would have a much easier time returning to Earth, to repair and repopulate. Heck, a Moon base makes much more sense than a Mars base! With all the literature on those three options, I have not heard a single argument showing that \u2018Mars base > Moon or orbital platform\u2019. Have you?\n\nMeanwhile, our treasure troves go unnoticed: Mercury and Venus, by virtue of being closer to our sun, have a VAST reserve of heavy metals, rare minerals. Mercury is our solar system\u2019s Fort Knox! And Venus, with its dense atmosphere (>90 times ours!) allows simple construction of massive floating cities, high enough above the lead-boiling surface, so that they inhabit a comfortable 80degF zone. That sounds much better than a potato farm on a cold, dim rock.\n\nIf we plan to mine Mercury, it will be with semi-autonomous robots. And, we will need to pelt areas with icy globs at regular intervals, to cool and crumble the mantle and ease spectrography-based prospecting from an orbital hub. (Your orbital platform zaps the surface with a lazer, and watches the colors that the targeted rock makes, as it vaporizes and cools\u2026 that\u2019s how we\u2019ll do mineral prospecting on Mercury, and any other rocks without atmospheres. Wanna bet on it?) I offer Saturn\u2019s ice moons (Rhea?) as fodder to the task of mining Mercury and gently cooling Venus. With even a gentle nudge, Saturn\u2019s moon Rhea could be tumbled and sling-shot into an orbit closer to the sun, and robots on its surface could drill and lob chunks of ice, ferrying them into orbits that collide with Mercury or Venus.\n\nMercury has a super-abundance of fissile materials, which should make the job of launching exports from the planet\u2019s surface that much easier. It also has significantly lower surface gravity than Mars \u2014 easier, still. And, if we persistently aim our icy chunks at the planet\u2019s edges (steep-angled impacts, not bull\u2019s eyed impacts), we could induce rotation. By spinning Mercury even a little bit, a space elevator there would be cheaper and lift a greater percentage of its mass as payload per hour, than on any other planet in our solar system. That would make mining Mercury MUCH easier.\n\nI cannot understate the logistical advantages of Venus\u2019s dense atmosphere! You could float there, with a blimp-backpack the size of a manta ray. City-sized floating platforms would be cheap to maintain. If you have a boulder that you want to hoist, then a blimp would do that work easily, with less capital material, at lower cost, and almost no power requirements, compared to a digger on Mars.\n\nThe atmosphere on Venus is also continuously circulating in a planet-wide single Hadley cycle. That means your floating city would naturally migrate across the planet\u2019s surface, at NO COST. Gliders could easily launch from these migratory platforms, to land payloads thousands of miles from their launch-position, for free. Inflate a blimp-bladder, which tugs the glider and materials back up, to be grabbed by another floating city! Coordinating the movement of materials was never easier. Mars, meanwhile, would rely entirely on rovers trudging over rugged terrain. Sad!\n\nVenus, by slow and gentle addition of ice-moons (one should be plenty, really), would begin cooling and chemically weathering surface rocks. Once the surface became damp and cool enough for robots, we could begin prospecting. I recommend using some of that Mercury-mined fissile material to drill and blast volcanic pools, then adding frit from surface rocks, to create magma mixtures that slow-cool and separate into valued minerals. You could call it \u2018controlled igneous materials processing\u2019? Also, blasting a micro-volcano, pouring a bunch of sifted dust on top (from the safety of your buoyant platform!), and waiting for it to cool into minerals is less labor and capital-intensive than drilling and refining on a cold rock like Mars.\n\nFinally, with a moist atmosphere drawing down Venus\u2019s high sulfur clouds and locking that sulfur in chemical reactions with surface rocks, your high floating platforms would be ideal gardens and solar farms. Massive arrays of gardens could be constructed and deployed far above the surface, and the sunlight captured by those high-altitude gardens would simultaneously create vast shaded zones beneath them. We could safely harvest from our bubble-cities, while cooling the planetary surface. It\u2019s a virtuous cycle!\n\nSo, what was your reason for going to Mars?"
    }
]