[
    {
        "url": "https://medium.com/gifs-ai/interactive-segmentation-with-convolutional-neural-networks-2e171a85df82?source=---------0",
        "title": "Interactive Segmentation with Convolutional Neural Networks",
        "text": "Recently animated stickers have increased in popularity due to their massive use in messaging applications or memes. Still, with existing tools, generating animated stickers is extremely challenging and time-consuming, making the task practically infeasible for non-experts. Removing the background of an arbitrary video (no green screen) is a menial task that involves manually segmenting the object in each frame of a video.\n\nAt gifs.com, we decided to tackle this problem and help people create animated stickers easily by using AI.\n\nAutomated animated sticker generation is a challenging problem to solve because of the complex nature of videos: they are subject to motion blur, bad composition, and occlusion. An object can be hard to segment due to its complex structure, small size (very little information) or large similarity between background and foreground. Also, a video clip can contain multiple objects, and we need to make sure the users extracts the object they are interested in.\n\nFirst, the user uses our interactive object segmentation tool to mark the object of interest in the first frame of the video. Then, the result will be propagated to the other frames and rendered as an animated sticker. For segmenting the object, i.e. instance segmentation, we use Computer Vision techniques that can infer the full segmentation from minimal user input.\n\nBoth segmentation steps (first frame and full video) rely on Convolutional Neural Networks, a type of a deep learning model. Deep learning is a good fit for our problem because of its recent improvements in Computer Vision. Convolutional Neural Networks have shown exceptional performance for image and video recognition. Those algorithms are capable of \u201cunderstanding\u201d the visual concept of an object (animal, car, \u2026) in an image.\n\nNext, we present the two steps of our method in more detail.\n\nA quick way to implement interactive segmentation is to use the GrabCut algorithm. It builds a model of the pixel distribution (colors) and performs well when the background and foreground are distinct, but outputs sub-optimal results when both are similar.\n\nTo get a beautiful sticker, we need a high-precision segmentation on the first frame. Because we were not satisfied by the GrabCut results, we decided to develop our method based on the latest research in deep learning. Inspired by recent work in interactive object segmentation with deep neural networks, we built a model that takes the image, the current segmentation result, and the user corrections as input and outputs a binary mask of the object.\n\nWe provide a brush tool to the user for correcting the first image of the video. Based on our production data, we have found that typical users tend to draw with a variety of patterns such as clicks, strokes or highlighting the whole object. Thus, we needed our algorithm to take into account a diversity of annotations and decided to include simulated strokes and clicks during the training phase to get the best results and give the user a great experience.\n\nAfter annotating one frame and successfully segmenting the object, we use a deep learning model based on the OSVOS paper to generate the segmentation in the other frames. OSVOS (One-Shot Video Object Segmentation) is a convolutional neural network (based on VGG) that uses generic semantic information to segment objects. For each sticker, the model is fine-tuned on frame/mask pairs. Then, we will infer the masks for all the frames in the video and combine the results to output an animated sticker with a transparent background.\n\nIf the object is fast moving or changes a lot towards the end of the video, we can get variable results. Thus, we allow the user to refine more frames in the video to improve the quality of the sticker."
    },
    {
        "url": "https://medium.com/gifs-ai/ridiculously-fast-shot-boundary-detection-with-fully-convolutional-neural-networks-da9d8c73e86c?source=---------1",
        "title": "Ridiculously Fast Shot Boundary Detection with Fully Convolutional Neural Networks",
        "text": "Shot boundary detection (SBD) is an important component of video analysis, as it is used in many video applications such as automatic highlight detection, action recognition and assisting in manual video editing. As such, its something our team at gifs.com cares deeply about.\n\nThe goal of SBD is to split an edited video into consecutive frames which show a continuous progression of video, as shown in this illustration:\n\nUnfortunately, however, all existing shot detection methods are have limited speed and accuracy. They often fail to detect slow or subtle transitions and are misled by strong visual changes, which are falsely labelled as shot changes. We show some typical failure cases below.\n\nAs shot detection is a core component of many of our products, we have have set out to improve SBD in accuracy as well as speed.\n\nToday, I am happy to present the results of our efforts: Our new method for ridiculously fast shot detection with fully convolutional neural network. It is significantly more accurate than previous methods, while running at 120x real-time speed. Thus, we can analyze a full-length movie in less than a minute. Pretty nice, isn\u2019t it?\n\nLet\u2019s see how it works.\n\nGiven that existing methods are not accurate enough, we turn to deep learning for salvation. Deep learning is a field of artificial intelligence that has shown strong performance in analyzing visual data, and therefore perfect for our problem of understanding videos. However, deep neural networks require large amounts of training data, in order to work well, and are computationally expensive when applied to video data.\n\nTwo core ideas allowed to make deep learning work for our task:\n\n1. All shots boundaries are created manually, by taking raw video, shortening it and combining video shots with some type of transition.\n\nThus, this can also be done automatically, by taking raw video, splitting it (randomly) into parts and recombining these parts with various transitions such as cuts, dissolves or fades. Therefore, it is cheap to generate a large-scale dataset.\n\n2. Understanding if a shot transition occurs at a particular frame requires looking at a context around it. In our work, we look at a context of 10 frames to decide if a shot change happens. To do this efficiently, we design a spatio-temporal deep network that is fully convolutional in time. This allows to take context into account without repeatedly analyzing the same frames. This idea of fully convolutional nets has previously been used for other applications such as Semantic Segmentation.\n\nWith these two ideas combined, we can train a deep neural network that can do fast and accurate shot detection. We tested our network on a standard shot detection dataset and obtained 88% accuracy, while the previously best method obtained 84%. That\u2019s a 25% error reduction. Also, our method runs at 120x real-time (on GPU), while the baseline method only achieves 7.7x.\n\nRemember the surfing video, where the dissolve is missed? With our new method it is now detected, thus allowing to correctly split the video into its parts.\n\nWhile we made significant improvements, our method is not perfect yet. E.g. when only a part of the scene changes, the method often fails, as in the example on the left. We are still improving our method to also handle cases like this.\n\nWe have presented an accurate and extremely fast method for shot boundary detection. The method works particularly well in detecting standard transitions such as cuts, dissolves, fades and wipes.\n\nIf you are interested in reading more technical details, please check our arXiv paper.\n\nOur new shot detection is already powering several products at gifs.com, such as our AI-based Gif creator, now giving you more relevant results that focus on what is most interesting. We are also providing a public API via Algorithmia, a machine-intelligence API marketplace. You can find it here."
    }
]