[
    {
        "url": "https://buzzrobot.com/gotta-autoencoder-em-all-60d5f8a19cf9?source=user_profile---------1----------------",
        "title": "Gotta Autoencode \u2019em All \u2013",
        "text": "Today, let\u2019s talk about Autoencoder. To do that, I will use an analogy with Pok\u00e9mon, OK? This will be fun. Imagine that you are Ash Ketchum (a.k.a Satoshi) and a big Snorlax appears in the middle of your path, preventing you from following your journey to be a Pok\u00e9mon master.\n\nIn this case, let\u2019s assume that you just need to throw the Pok\u00e9 Ball in the Snorlax to capture it and continue your journey. We can say that the Pok\u00e9 Ball is a Perfect Autoencoder.\n\nSnorlax is a big Pok\u00e9mon, and we need to put him inside a small Pok\u00e9 Ball. How we can do that? We have to \u201cencode\u201d him from his real form to a small representation that can be fit inside de Pok\u00e9 Ball. After that, if we want to let him out, we just need to \u201cdecode\u201d the small representation to his real form again.\n\nAutoencoder is just an artificial neural network that tries to reconstruct its input data. The idea of it is just that easy. The differential of the autoencoder is that it has a hidden layer that serves as a compressed representation of the input data. These kinds of models are used typically for the purpose of dimensionality reduction (compress data).\n\nPreviously, I said that Pok\u00e9 Ball is a Perfect Autoencoder and the reason for this is that the Pok\u00e9mon that enters in the Pok\u00e9 Ball can come back exactly as it was before. So, a Perfect Autoencoder can produce an output that is exactly the same as the input. Unfortunately, in reality, the Autoencoders can rarely achieve this perfect result.\n\nWell, now that you already knows what is an Autoencoder, let\u2019s see this in practice with Pok\u00e9mon."
    },
    {
        "url": "https://buzzrobot.com/the-lord-of-the-rnn-52b937e4af93?source=user_profile---------2----------------",
        "title": "The Lord of The RNN \u2013",
        "text": "So, what is the relation between the fiction of J.R.R. Tolkien and RNN in this article? Well, as I have listed above, we can train an RNN to Generating Text, right? So, how about we train an RNN with the \u201cThe Silmarillion\u201d, \u201cThe Hobbit\u201d and \u201cThe Lord of The Rings\u201d books? It would be awesome create some new lines of text that looks like the type of writing of Tolkien in these books. Imagine create a new story of F\u00ebanor, Fingolfin, Beren and Luthien, Smaug, Aragorn and Arwen, Frodo and Sam, Legolas and Gimli, Bilbo and Gandalf, Merry and Pippin, Faramir and Eowyn, Ents and Entwives, a new fellowship of the ring and so forth (this infinity of possibilities makes me super anxious to read all again).\n\nWho don\u2019t know about RNN and/or traditional neural network could be thinking right now why this kind of \u201cmemory\u201d is so important. What RNN can do with this \u201cmemory\u201d? Some common applications of an RNN nowadays are: Natural Language Processing, Speech Recognition, Machine Translation, Question Answering, Image Captioning, Generating Text and others things.\n\nNow, in the RNN, we can think that the network has a \u201cmemory\u201d that is able to store information about what has been passed in the network. In this case, inputs that have been passed to the network early can influencing the output of a new input. So, with RNN we can make use of sequential information of the data. For more details on how RNN works, I suggest reading the following tutorial: Recurrent Neural Networks Tutorial .\n\nIf you don\u2019t know RNN, the idea behind this is very simple. Let\u2019s think first about what a traditional neural network do. In a traditional neural network, one input generates an output, right? If you already have your network trained, every time you put some input in the network, the output will always be the same for that input. In this case, all the inputs and their respective outputs are independent of each other.\n\nI don\u2019t know you, but I love the world of \u201cThe Lord of The Rings\u201d. Not only the \u201cThe Lord of The Rings\u201d story, but also \u201cThe Hobbit\u201d, \u201cThe Silmarillion\u201d, \u201cUnfinished Tales\u201d and so on. I\u2019m fascinated with the fiction of J.R.R. Tolkien and the universe that he created. Beside that, I also love what Deep Learning can do. In this case, what a Recurrent Neural Network (RNN) can do.\n\nTo do that, I trained a special kind of RNN called Long Short Term Memory (LSTM). The LSTM network is an RNN that can learning long-term dependencies, which is very good in this case. Again, for more details on how LSTM works, I suggest reading the following tutorial: Understanding LSTM Networks.\n\nAfter training the LSTM network, I was able to generate new texts. I set, for each text, a total of 750 characters to be generated. The first text began with the \u201cF\u00ebanor and Fingolfin\u201d characters and the network output this:\n\nThe second text began with the \u201cBeren and L\u00fathien\u201d characters and the network output this:\n\nThe third text began with the \u201cAragorn and Arwen\u201d characters and the network output this:\n\nThe fourth text began with the \u201cSmaug, Bilbo and Gandalf\u201d characters and the network output this:\n\nThe fifth text began with the \u201cEnts and Entwives\u201d characters and the network output this:\n\nThe sixth and last text began with the \u201cA new fellowship of the ring\u201d characters and the network output this:\n\nOk ok, I know\u2026 this trained network can not replace Tolkien. Much of the text does not make much sense, but as we can see, the network learned how to form words and some sentences.\n\nThis is totally incredible!!! The power that the \u201cmemory\u201d give to the network is great. Perhaps, in the near future, someone will build a better network that is able to create texts similar to Tolkien\u2019s without anyone knowing that it was created by a computer (you can try this, would be awesome saw that).\n\nIf you want to check the code of this project, just go to my GitHub account."
    },
    {
        "url": "https://buzzrobot.com/whats-happening-inside-the-convolutional-neural-network-the-answer-is-convolution-2c22075dc68d?source=user_profile---------3----------------",
        "title": "What\u2019s happening inside the Convolutional Neural Network? The answer is Convolution.",
        "text": "I started working with Machine Learning in the college, but I never went Deeper (get it?) until learned about Convolutional Neural Network (CNN). At the beginning, I was a little confused about how CNN really works, how equal CNN was compared to a common Neural Network.\n\nIn order to understand CNN in an easy way you have to know about Neural Network and how they work, because the idea of learning is the same in both cases. In a second moment, knowing what a Convolution is and what it can do is the key to understand what\u2019s happening inside the CNN.\n\nI suppose that by being here reading this, you already know about a common Neural Network and how it works. In this way, let\u2019s talk about Convolution.\n\nConvolution is a mathematical operation that basically takes two functions (f and g) to produce a third one (f*g). The math behind this is pretty easy, in the way that if you know a little bit about integral you are ready to go (I will not enter into this math). There\u2019s no physical meaning in the Convolution operation, but I will show what it can do.\n\nIn the context of CNN, a Convolution is the treatment of a matrix by another called kernel. This has been applied in image processing before the creation of CNN. I believe that the gif below is well able to exemplify how this process occurs.\n\nIn words, what happens is this: the kernel is moving in the input, from left to right and from top to bottom, and each one of the values on the kernel is multiplied by the value on the input on the same position. The results obtained by the multiplication are then summed and the local output is generated. Let\u2019s look the first position of the gif: We multiply and sum the values (0*7) + (-1*7) +(0*6) +(-1*7) +(5*7) +(-1*6) +(0*6) +(-1*6) +(0*4) and get the value 9. We move the kernel and repeat the process again to create all the output.\n\nThis can be used for blurring, embossing, edge detection and much more. Let\u2019s take one picture and apply some kernels to obtain something easy to understand. I took a picture of my little friend Batman and applied 3 kernels, as we can see:\n\nThis is very nice, right? With one picture and one kernel, we can create something that can be useful for us. But there are some questions that we have to ask: What will happen if we don\u2019t know what kind of kernel that can lead us to our goal? How many kernels are necessary to extract information that we need to our goal?\n\nCNN works exactly on top of that. If you have a classification problem, the CNN will learn a group of kernels that can give you the information that you need to get in your goal. This way, the learning process learns kernels that are useful to the problem of classification. In this learning process, we can discover kernels that are good in extracting relevant information to the problem.\n\nFinally, we can then say that CNN is not a 7-headed monster or a magic trick, it is just math. I hope this text can help someone who is lost as I was."
    }
]