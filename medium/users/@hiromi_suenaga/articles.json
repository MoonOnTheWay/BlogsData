[
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-12-215dfbf04a94?source=user_profile---------1----------------",
        "title": "Deep Learning 2: Part 2 Lesson 12 \u2013 Hiromi Suenaga \u2013",
        "text": "My personal notes from fast.ai course. These notes will continue to be updated and improved as I continue to review the course to \u201creally\u201d understand it. Much appreciation to Jeremy and Rachel who gave me this opportunity to learn. Very hot technology but definitely deserving to be in the cutting edge deep learning part of the course because they are not quite proven to be necessarily useful for anything but they are nearly there and will definitely get there. We are going to focus on the things where they are definitely going to be useful in practice and there is a number of areas where they may turn out to be useful but we don\u2019t know yet. So I think the area that they are definitely going to be useful in practice is the kind of thing you see on the left of the slide \u2014 which is for example turning drawing into rendered pictures. This comes from a paper that just came out 2 days ago, so there\u2019s a very active research going on right now. From the last lecture [1:04]: One of our diversity fellows Christine Payne has a master\u2019s in medicine from Stanford and so she had an interest in thinking what it would look like if we built a language model of medicine. One of the things we briefly touched on back in lesson 4 but didn\u2019t really talk much about last time is this idea that you can actually seed a generative language model which means you\u2019ve trained a language model on some corpus and then you are going to generate some text from that language model. You can start off by feeding it a few words to say \u201chere is the first few words to create the hidden state in the language model and generate from there please. Christine did something clever which was to seed it with a question and repeat the question three times and let it generate from there. She fed a language model lots of different medical texts and fed in questions as you see below: What Jeremy found interesting about this is it\u2019s pretty close to being a believable answer to the question for people without master\u2019s in medicine. But it has no bearing on reality whatsoever. He thinks it is an interesting kind of ethical and user experience quandary. Jeremy is involved in a company called doc.ai that\u2019s trying to doing a number of things but in the end provide an app for doctors and patients which can help create a conversational user interface around helping them with their medical issues. He\u2019s been continually saying to the software engineers on that team please don\u2019t try to create a generative model using LSTM or something because they are going to be really good at creating bad advice that sounds impressive \u2014 kind of like political pundits or tenured professor who can say bullcrap with great authority. So he thought it was really interesting experiment. If you\u2019ve done some interesting experiments, share them in the forum, blog, Twitter. Let people know about it and get noticed by awesome people. Let\u2019s talk about CIFAR10 and the reason is that we are going to be looking at some more bare-bones PyTorch stuff today to build these generative adversarial models. There is no fastai support to speak up at all for GANs at the moment \u2014 there will be soon enough but currently there isn\u2019t so we are going to be building a lot of models from scratch. It\u2019s been a while since we\u2019ve done much serious model building. We looked at CIFAR10 in the part 1 of the course and we built something which was getting about 85% accuracy and took a couple hours to train. Interestingly, there is a competition going on now to see who can actually train CIFAR10 the fastest (DAWN), and the goal is to get it to train to 94% accuracy. It would be interesting to see if we can build an architecture that can get to 94% accuracy because that is a lot better than our previous attempt. Hopefully in doing so we will learn something about creating good architectures, that will be then useful for looking at GANs today. Also it is useful because Jeremy has been looking much more deeply into the last few years\u2019 papers about different kinds of CNN architectures and realizes that a lot of the insights in those papers are not being widely leveraged and clearly not widely understood. So he wants to show you what happens if we can leverage so me of that understanding. The notebook is called darknet because the particular architecture we are going to look at is very close to the darknet architecture. But you will see in the process that the darknet architecture as in not the whole YOLO v3 end-to-end thing but just the part of it that they pre-trained on ImageNet to do classification. It\u2019s almost like the most generic simple architecture you could come up with, so it\u2019s a really great starting point for experiments. So we will call it \u201cdarknet\u201d but it\u2019s not quite that and you can fiddle around with it to create things that definitely aren\u2019t darknet. It\u2019s really just the basis of nearly any modern ResNet based architecture. CIFAR10 is a fairly small dataset [8:06]. The images are only 32 by 32 in size, and it\u2019s a great dataset to work with because: You can train it relatively quickly unlike ImageNet Actually quite hard to recognize the images because 32 by 32 is too small to easily see what\u2019s going on. It is an under-appreciated dataset because it\u2019s old. Who wants to work with small old dataset when they could use their entire server room to process something much bigger. But it\u2019s is a really great dataset to focus on. Go ahead and import our usual stuff and we are going to try and build a network from scratch to train this with[8:58]. A really good exercise for anybody who is not 100% confident with their broadcasting and PyTorch basic skill is figure out how Jeremy came up with these numbers. These numbers are the averages and standard deviations for each channel in CIFAR10. Try and make sure you can recreate those numbers and see if you can do it with no more than a couple of lines of code (no loops!). Because these are fairly small, we can use a larger batch size than usual and the size of these images is 32 [9:46]. Transformations [9:57], normally we have this standard set of side_on transformations we use for photos of normal objects. We are not going to use that here because these images are so small that trying to rotate a 32 by 32 image a bit is going to introduce a lot of blocky distortions. So the standard transformations that people tend to use is a random horizontal flip and then we add 4 pixels (size divided by 8) of padding on each side. One thing which works really well is by default fastai does not add black padding which many other libraries do. Fastai takes the last 4 pixels of the existing photo and flip it and reflect it, and we find that we get much better results by using reflection padding by default. Now that we have 40 by 40 image, this set of transforms in training will randomly pick a 32 by 32 crops, so we get a little bit of variation but not heaps. Wo we can use the normal to grab our data. Now we need an architecture and we are going to create one which fits in one screen [11:07]. This is from scratch. We are using predefined , , modules but we are not using any blocks or anything. The entire thing is in one screen so if you are ever wondering can I understand a modern good quality architecture, absolutely! Let\u2019s study this one. class Darknet(nn.Module):\n\n def make_group_layer(self, ch_in, num_blocks, stride=1):\n\n return [conv_layer(ch_in, ch_in*2,stride=stride)\n\n ] + [(ResLayer(ch_in*2)) for i in range(num_blocks)]\n\n\n\n def __init__(self, num_blocks, num_classes, nf=32):\n\n super().__init__()\n\n layers = [conv_layer(3, nf, ks=3, stride=1)]\n\n for i,nb in enumerate(num_blocks):\n\n layers += self.make_group_layer(nf, nb, stride=2-(i==1))\n\n nf *= 2\n\n layers += [nn.AdaptiveAvgPool2d(1), Flatten(), \n\n nn.Linear(nf, num_classes)]\n\n self.layers = nn.Sequential(*layers)\n\n \n\n def forward(self, x): return self.layers(x) The basic starting point with an architecture is to say it\u2019s a stacked bunch of layers and generally speaking there is going to be some kind of hierarchy of layers [11:51]. At the very bottom level, there is things like a convolutional layer and a batch norm layer, but any time you have a convolution, you are probably going to have some standard sequence. Normally it\u2019s going to be: We will start by determining what our basic unit is going to be and define it in a function ( ) so we don\u2019t have to worry about trying to keep everything consistent and it will make everything a lot simpler.\n\nThe gradient of Leaky ReLU (where x < 0) varies but something about 0.1 or 0.01 is common. The idea behind it is that when you are in the negative zone, you don\u2019t end up with a zero gradient which makes it very hard to update it. In practice, people have found Leaky ReLU more useful on smaller datasets and less useful in big datasets. But it is interesting that for the YOLO v3 paper, they used Leaky ReLU and got great performance from it. It rarely makes things worse and it often makes things better. So it\u2019s probably not bad if you need to create your own architecture to make that your default go-to is to use Leaky ReLU. You\u2019ll notice that we don\u2019t define PyTorch module in , we just do [14:07]. This is something if you read other people\u2019s PyTorch code, it\u2019s really underutilized. People tend to write everything as a PyTorch module with and , but if the thing you want is just a sequence of things one after the other, it\u2019s much more concise and easy to understand to make it a . Residual block [14:40]: As mentioned before that there is generally a number of hierarchies of units in most modern networks, and we know now that the next level in this unit hierarchy for ResNet is the ResBlock or residual block (see ). Back when we last did CIFAR10, we oversimplified this (cheated a little bit). We had coming in and we put that through a , then we added it back up to to go out. In the real ResBlock, there are two of them. When we say \u201cconv\u201d we are using it as a shortcut for our (conv, batch norm, ReLU). One interesting insight here is the number of channels in these convolutions [16:47]. We have some coming in (some number of input channels/filters). The way the darknet folks set things up is they make every one of these Res layers spit out the same number of channels that came in, and Jeremy liked that and that\u2019s why he used it in because it makes life simpler. The first conv halves the number of channels, and then second conv doubles it again. So you have this funneling effect where 64 channels coming in, squished down with a first conv down to 32 channels, and then taken back up again to 64 channels coming out. Question: Why is in the [17:54]? Thanks for asking! A lot of people forget this or don\u2019t know about it, but this is a really important memory technique. If you think about it, this , it\u2019s the lowest level thing, so pretty much everything in our ResNet once it\u2019s all put together is going to be many \u2019s. If you do not have , it\u2019s going to create a whole separate piece of memory for the output of the ReLU so it\u2019s going to allocate a whole bunch of memory that is totally unnecessary. Another example is that the original in looked like: Hopefully some of you might remember that in PyTorch pretty much every every function has an underscore suffix version which tells it to do it in-place. is equivalent to and in-place version of is so this will reduce memory usage: These are really handy little tricks. Jeremy forgot the at first but he was having to decrease the batch size to much lower amounts and it was driving him crazy \u2014 then he realized that that was missing. You can also do that with dropout if you have dropout. Here are what to look out for: Question: In ResNet, why is bias usually set to False in conv_layer [19:53]? Immediately after the , there is a . Remember, has 2 learnable parameters for each activation \u2014 the thing you multiply by and the thing you add. If we had bias in and then add another thing in , we would be adding two things which is totally pointless \u2014 that\u2019s two weights where one would do. So if you have a BatchNorm after a , you can either tell not to include the add bit or easier is to tell not to include the bias. There is no particular harm, but again, it\u2019s going to take more memory because that is more gradients that it has to keep track of, so best to avoid. Also another little trick is, most people\u2019s \u2019s have padding as a parameter [21:11]. But generally speaking, you should be able to calculate the padding easily enough. If you have a kernel size of 3, then obviously that is going to overlap by one unit on each side, so we want padding of 1. Or else, if it\u2019s kernel size of 1, then we don\u2019t need any padding. So in general, padding of kernel size \u201cinteger divided\u201d by 2 is what you need. There\u2019re some tweaks sometimes but in this case, this works perfectly well. Again, trying to simplify my code by having the computer calculate stuff for me rather than me having to do it myself. Another thing with the two \u2019s [22:14]: We had this idea of bottleneck (reducing the channels and then increase them again), there is also what kernel size to use. The first one has 1 by 1 . What actually happen in 1 by 1 conv? If we have 4 by 4 grid with 32 filters/channels and we will do 1 by 1 conv, the kernel for the conv looks like the one in the middle. When we talk about the kernel size, we never mention the last piece \u2014 but let\u2019s say it\u2019s 1 by 1 by 32 because that\u2019s the part of the filters in and filters out. The kernel gets placed on the first cell in yellow and we get a dot product these 32 deep bits which gives us our first output. We then move it to the second cell and get the second output. So there will be bunch of dot products for each point in the grid. It is allowing us to change the dimensionality in whatever way we want in the channel dimension. We are creating filters and we will have dot products which are basically different weighted averages of the input channels. With very little computation, it lets us add this additional step of calculations and nonlinearities. It is a cool trick to take advantage of these 1 by 1 convs, creating this bottleneck, and then pulling it out again with 3 by 3 convs \u2014 which will take advantage of the 2D nature of the input properly. Or else, 1 by 1 conv doesn\u2019t take advantage of that at all. These two lines of code, there is not much in it, but it\u2019s a really great test of your understanding and intuition about what is going on [25:17] \u2014 why does it work? why do the tensor ranks line up? why do the dimensions all line up nicely? why is it a good idea? what is it really doing? It\u2019s a really good thing to fiddle around with. Maybe create some small ones in Jupyter Notebook, run them yourself, see what inputs and outputs come in and out. Really get a feel for that. Once you\u2019ve done so, you can then play around with different things. One of the really unappreciated papers is this one [26:09] \u2014 Wide Residual Networks. It\u2019s really quite simple paper but what they do is they fiddle around with these two lines of code: What is we did instead of ? What if we added ? They come up with this kind of simple notation for defining what the two lines of code can look like and they show lots of experiments. What they show is that this approach of a bottlenecking of decreasing the number of channels which is almost universal in ResNet is probably not a good idea. In fact, from the experiments, definitely not a good idea. Because what happens is it lets you create really deep networks. The guys who created ResNet got particularly famous for creating 1001 layer network. But the thing about 1001 layers is you can\u2019t calculate layer 2 until you are finished layer 1. You can\u2019t calculate layer 3 until you finish calculating layer 2. So it\u2019s sequential. GPUs don\u2019t like sequential. So what they showed is that if you have less layers but with more calculations per layer \u2014 so one easy way to do that would be to remove , no other changes: Try this at home. Try running CIFAR and see what happens. Even multiply by 2 or fiddle around. That lets your GPU do more work and it\u2019s very interesting because the vast majority of papers that talk about performance of different architectures never actually time how long it takes to run a batch through it. They say \u201cthis one takes X number of floating-point operations per batch\u201d but they never actually bother to run it like a proper experimentalists and find out whether it\u2019s faster or slower. A lot of the architectures that are really famous now turn out to be slow as molasses and take crap loads of memory and just totally useless because the researchers never actually bothered to see whether they are fast and to actually see whether they fit in RAM with normal batch sizes. So Wide ResNet paper is unusual in that it actually times how long it takes as does the YOLO v3 paper which made the same insight. They might have missed the Wide ResNet paper because the YOLO v3 paper came to a lot of the same conclusions but Jeremy is not sure they sited the Wide ResNet paper so they might not be aware that all that work has been done. It\u2019s great to see people are actually timing things and noticing what actually makes sense. Question: What is your opinion on SELU (scaled exponential linear units)? [29:44] SELU is largely for fully connected layers which allows you to get rid of batch norm and the basic idea is that if you use this different activation function, it\u2019s self normalizing. Self normalizing means it will always remain at a unit standard deviation and zero mean and therefore you don\u2019t need batch norm. It hasn\u2019t really gone anywhere and the reason is because it\u2019s incredibly finicky \u2014 you have to use a very specific initialization otherwise it doesn\u2019t start with exactly the right standard deviation and mean. Very hard to use it with things like embeddings, if you do then you have to use a particular kind of embedding initialization which doesn\u2019t make sense for embeddings. And you do all this work, very hard to get it right, and if you do finally get it right, what\u2019s the point? Well, you\u2019ve managed to get rid of some batch norm layers which weren\u2019t really hurting you anyway. It\u2019s interesting because the SELU paper \u2014 the main reason people noticed it was because it was created by the inventor of LSTM and also it had a huge mathematical appendix. So people thought \u201clots of maths from a famous guy \u2014 it must be great!\u201d but in practice, Jeremy doesn\u2019t see anybody using it to get any state-of-the-art results or win any competitions. contains a bunch of [31:28]. is going to have some number of channels/filters coming in. We will double the number of channels coming in by just using the standard . Optionally, we will halve the grid size by using a stride of 2. Then we are going to do a whole bunch of ResLayers \u2014 we can pick how many (2, 3, 8 etc) because remember ResLayers do not change the grid size and they don\u2019t change the number of channels, so you can add as many as you like without causing any problems. This is going to use more computation and more RAM but there is no reason other than that you can\u2019t add as many as you like. , therefore, is going to end up doubling the number of channels because the initial convolution doubles the number of channels and depending on what we pass in a , it may also halve the grid size if we put . And then we can do a whole bunch of Res block computations as many as we like. To define our , we are going to pass in something that looks like this [33:13]: What this says is create five group layers: the first one will contain 1 extra ResLayer, the second will contain 2, then 4, 6, 3 and we want to start with 32 filters. The first one of ResLayers will contain 32 filters, and there\u2019ll just be one extra ResLayer. The second one, it\u2019s going to double the number of filters because that\u2019s what we do each time we have a new group layer. So the second one will have 64, and then 128, 256, 512 and that\u2019ll be it. Nearly all of the network is going to be those bunches of layers and remember, every one of those group layers also has one convolution at the start. So then all we have is before that all happens, we are going to have one convolutional layer at the very start, and at the very end we are going to do our standard adaptive average pooling, flatten, and a linear layer to create the number of classes out at the end. To summarize [34:44], one convolution at one end, adaptive pooling and one linear layer at the other end, and in the middle, these group layers each one consisting of a convolutional layer followed by number of ResLayers. Adaptive average pooling [35:02]: Jeremy\u2019s mentioned this a few times, but he\u2019s yet to see any code out there, any example, anything anywhere, that uses adaptive average pooling. Every one he\u2019s seen writes it like where is a particular number \u2014 this means that it\u2019s now tied to a particular image size which definitely isn\u2019t what you want. So most people are still under the impression that a specific architecture is tied to a specific size. That\u2019s a huge problem when people think that because it really limits their ability to use smaller sizes to kick-start their modeling or to use smaller size for doing experiments. Sequential [35:53]: A nice way to create architectures is to start out by creating a list, in this case this is this is a list with just one in, and returns another list. Then we can append that list to the previous list with and do the same for another list containing . Finally we will call of all those layers. Now the is just . This is a nice picture of how to make your architectures as simple as possible. There are a lot you can fiddle around with. You can parameterize the divider of to make it a number that you pass in to pass in different numbers- maybe do times 2 instead. You can also pass in things that change the kernel size, or change the number of convolutional layers. Jeremy has a version of this which he is going to run for you which implements all of the different parameters that were in the Wide ResNet paper, so he could fiddle around to see what worked well. Once we\u2019ve got that, we can use to take our PyTorch module and a model data object, and turn them into a learner [37:08]. Give it a criterion, add a metrics if we like, and then we can fit and away we go. Question: Could you please explain adaptive average pooling? How does setting to 1 work [37:25]? Sure. Normally when we are doing average pooling, let\u2019s say we have 4x4 and we did [40:35]. That creates 2x2 area (blue in the below) and takes the average of those four. If we pass in , the next one is 2x2 shown in green and take the average. So this is what a normal 2x2 average pooling would be. If we didn\u2019t have any padding, that would spit out 3x3. If we wanted 4x4, we can add padding. What if we wanted 1x1? Then we could say that would do 4x4 in yellow and average the whole lot which results in 1x1. But that\u2019s just one way to do it. Rather than saying the size of the pooling filter, why don\u2019t we instead say \u201cI don\u2019t care what the size of the input grid is. I always want one by one\u201d. That\u2019s where you say . In this case, you don\u2019t say what\u2019s the size of the pooling filter, you instead say what the size of the output we want. We want something that\u2019s one by one. If you put a single integer , it assumes you mean by . In this case, adaptive average pooling 1 with a 4x4 grid coming in is the same as average pooling (4, 4). If it was 7x7 grid coming in, it would be the same as average pooling (7, 7). It is the same operation, it\u2019s just expressing it in a way that regardless of the input, we want something of that sized output. DAWNBench [37:43]: Let\u2019s see how we go with our simple network against these state-of-the-art results. Jeremy has the command ready to go. We\u2019ve taken all that stuff and put it into a simple Python script, and he modified some of the parameters he mentioned to create something he called network which doesn\u2019t officially exist but it has a bunch of changes to the parameters we talked about based on Jeremy\u2019s experiments. It has bunch of cool stuff like: This is going to run on AWS p3 which has 8 GPUs and Volta architecture GPUs which have special support for half-precision floating-point. Fastai is the first library to actually integrate the Volta optimized half-precision floating-point into the library, so you can just do and get that support automatically. And it\u2019s also the first to integrate one cycle. What this actually does is it\u2019s using PyTorch\u2019s multi-GPU support [39:35]. Since there are eight GPUs, it is actually going to fire off eight separate Python processors and each one is going to train on a little bit and then at the end it\u2019s going to pass the gradient updates back to the master process that is going to integrate them all together. So you will see lots of progress bars pop up together. You can see it\u2019s training three or four seconds when you do it this way. Where else, when Jeremy was training earlier, he was getting 30 seconds per epoch. So doing it this way, we can train things ~10 times faster which is pretty cool. It\u2019s done! We got to 94% and it took 3 minutes and 11 seconds. Previous state-of-the-art was 1 hour 7 minutes. Was it worth fiddling around with those parameters and learning a little bit about how these architectures actually work and not just using what came out of the box? Well, holy crap. We just used a publicly available instance (we used a spot instance so it costs us $8 per hour \u2014 for 3 minutes 40 cents) to train this from scratch 20 times faster than anybody has ever done it before. So that is one of the craziest state-of-the-art result. We\u2019ve seen many but this one just blew it out of the water. This is partly thanks to fiddling around with those parameters of the architecture, mainly frankly about using Leslie Smith\u2019s one cycle. Reminder of what it is doing [44:35], for learning rate, it creates upward path that is equally long as the downward path so it\u2019s true triangular cyclical learning rate (CLR). As per usual, you can pick the ratio of x and y (i.e. starting LR / peak LR). In In this case, we picked 50 for the ratio. So we started out with much smaller learning rate. Then it has this cool idea where you get to say what percentage of your epochs is spent going from the bottom of the triangle all the way down pretty much to zero \u2014 that is the second number. So 15% of the batches are spent going from the bottom of our triangle even further.\n\nThat is not the only thing one cycle does, we also have momentum. Momentum goes from .95 to .85. In other words, when learning rate is really low, we use a lot of momentum and when the learning rate is really high, we use very little momentum which makes a lot of sense but until Leslie Smith showed this in the paper, Jeremy has never seen anybody do it before. It\u2019s a really cool trick. You can now use that by using parameter in fastai (forum post by Sylvain) and you should be able to replicate the state-of-the-art result. You can use it on your own computer or your paper space, the only thing you won\u2019t get is the multi-GPU piece, but that makes it a bit easier to train anyway. Question: contains stride equals 2, so this means stride is one for layer one and two for everything else. What is the logic behind it? Usually the strides I have seen are odd [46:52]. Strides are either one or two. I think you are thinking of kernel sizes. So stride=2 means that I jump two across which means that you halve your grid size. So I think you might have got confused between stride and kernel size there. If you have a stride of one, the grid size does not change. If you have a stride of two, then it does. In this case, because this is CIFAR10, 32 by 32 is small and we don\u2019t get to halve the grid size very often because pretty quickly we are going to run out of cells. So that is why the first layer has a stride of one so we don\u2019t decrease the grid size straight away. It is kind of a nice way of doing it because that\u2019s why we have a low number at first . We can start out with not too much computation on the big grid, and then we can gradually doing more and more computation as the grids get smaller and smaller because the smaller grid the computation will take less time We are going to talk about generative adversarial networks also known as GANs and specifically we are going to focus on Wasserstein GAN paper which included Soumith Chintala who went on to create PyTorch. Wasserstein GAN (WGAN) was heavily influenced by the deep convolutional generative adversarial network paper which also Soumith was involved with. It is a really interesting paper to read. A lot of it looks like this: The good news is you can skip those bits because there is also a bit that looks like this: A lot of papers have a theoretical section which seems to be there entirely to get past the reviewer\u2019s need for theory. That\u2019s not true with WGAN paper. The theory bit is actually interesting \u2014 you don\u2019t need to know it to use it, but if you want to learn about some cool ideas and see the thinking behind why this particular algorithm, it\u2019s absolutely fascinating. Before this paper came out, Jeremy knew nobody who studied the math it\u2019s based on, so everybody had to learn the math. The paper does a pretty good job of laying out all the pieces (you have to do a bunch of reading yourself). So if you are interested in digging into the deeper math behind some paper to see what it\u2019s like to study it, I would pick this one because at the end of that theory section, you\u2019ll come away saying \u201cI can see now why they made this algorithm the way it is.\u201d The basic idea of GAN is it\u2019s a generative model[51:23]. It is something that is going to create sentences, create images, or generate something. It is going to try and create thing which is very hard to tell the difference between generated stuff and real stuff. So generative model could be used to face-swap a video \u2014 a very controversial thing of deep fakes and fake pornography happening at the moment. It could be used to fake somebody\u2019s voice. It could be used to fake the answer to a medical question \u2014 but in that case, it\u2019s not really a fake, it could be a generative answer to a medical question that is actually a good answer so you are generating language. You could generate a caption to an image, for example. So generative models have lots of interesting applications. But generally speaking, they need to be good enough that for example if you are using it to automatically create a new scene for Carrie Fisher in the next Star Wars movie and she is not around to play that part anymore, you want to try and generate an image of her that looks the same then it has to fool the Star Wars audience into thinking \u201cokay, that doesn\u2019t look like some weird Carrie Fisher \u2014 that looks like the real Carrie Fisher. Or if you are trying to generate an answer to a medical question, you want to generate English that reads nicely and clearly, and sounds authoritative and meaningful. The idea of generative adversarial network is we are going to create not just a generative model to create the generated image, but a second model that\u2019s going to try to pick which ones are real and which ones are generated (we will call them \u201cfake\u201d). So we have a generator that is going to create our fake content and a discriminator that\u2019s going to try to get good at recognizing which ones are real and which ones are fake. So there are going to be two models and they are going to be adversarial meaning the generator is going to try to keep getting better at fooling the discriminator into thinking that fake is real, and the discriminator is going to try to keep getting better at discriminating between the real and the fake. So they are going to go head to head. It is basically as easy as Jeremy just described [54:14]: We are going to build two models in PyTorch We are going to create a training loop that first of all says the loss function for the discriminator is \u201ccan you tell the difference between real and fake, then update the weights of that. We are going to create a loss function for the generator which is \u201ccan you generate something which fools the discriminator and update the weights from that loss. And we are going to loop through that a few times and see what happens. Looking at the code [54:52] There is a lot of different things you can do with GANS. We are going to do something that is kind of boring but easy to understand and it\u2019s kind of cool that it\u2019s even possible which is we are going to generate some pictures from nothing. We are just going to get it to draw some pictures. Specifically, we are going to get it to draw pictures of bedrooms. Hopefully you get a chance to play around with this during the week with your own datasets. If you pick a dataset that\u2019s very varied like ImageNet and then get a GAN to try and create ImageNet pictures, it tends not to do so well because it\u2019s not clear enough what you want a picture of. So it\u2019s better to give it, for example, there is a dataset called CelebA which is pictures of celebrities\u2019 faces that works great with GANs. You create really clear celebrity faces that don\u2019t actually exist. The bedroom dataset is also a good one \u2014 pictures of the same kind of thing. There is something called LSUN scene classification dataset [55:55]. Download the LSUN scene classification dataset bedroom category, unzip it, and convert it to jpg files (the scripts folder is here in the folder): This isn't tested on Windows - if it doesn't work, you could use a Linux box to convert the files, then copy them over. Alternatively, you can download this 20% sample from Kaggle datasets. In this case, it is much easier to go the CSV route when it comes to handling our data. So we generate a CSV with the list of files that we want, and a fake label \u201c0\u201d because we don\u2019t really have labels for these at all. One CSV file contains everything in that bedroom dataset, and another one contains random 10%. It is nice to do that because then we can most of the time use the sample when we are experimenting because there is well over a million files even just reading in the list takes a while. files = PATH.glob('bedroom/**/*.jpg')\n\n\n\nwith CSV_PATH.open('w') as fo:\n\n for f in files: fo.write(f'{f.relative_to(IMG_PATH)},0\n\n') files = PATH.glob('bedroom/**/*.jpg')\n\n\n\nwith CSV_PATH.open('w') as fo:\n\n for f in files:\n\n if random.random()<0.1: \n\n fo.write(f'{f.relative_to(IMG_PATH)},0\n\n') This will look pretty familiar [57:10]. This is before Jeremy realized that sequential models are much better. So if you compare this to the previous conv block with a sequential model, there is a lot more lines of code here \u2014 but it does the same thing of conv, ReLU, batch norm. class ConvBlock(nn.Module):\n\n def __init__(self, ni, no, ks, stride, bn=True, pad=None):\n\n super().__init__()\n\n if pad is None: pad = ks//2//stride\n\n self.conv = nn.Conv2d(ni, no, ks, stride, padding=pad, \n\n bias=False)\n\n self.bn = nn.BatchNorm2d(no) if bn else None\n\n self.relu = nn.LeakyReLU(0.2, inplace=True)\n\n \n\n def forward(self, x):\n\n x = self.relu(self.conv(x))\n\n return self.bn(x) if self.bn else x The first thing we are going to do is to build a discriminator [57:47]. A discriminator is going to receive an image as an input, and it\u2019s going to spit out a number. The number is meant to be lower if it thinks this image is real. Of course \u201cwhat does it do for a lower number\u201d thing does not appear in the architecture, that will be in the loss function. So all we have to do is to create something that takes an image and spits out a number. A lot of this code is borrowed from the original authors of this paper, so some of the naming scheme is different to what we are used to. But it looks similar to what we had before. We start out with a convolution (conv, ReLU, batch norm). Then we have a bunch of extra conv layers \u2014 this is not going to use a residual so it looks very similar to before a bunch of extra layers but these are going to be conv layers rather than res layers. At the end, we need to append enough stride 2 conv layers that we decrease the grid size down to no bigger than 4x4. So it\u2019s going to keep using stride 2, divide the size by 2, and repeat till our grid size is no bigger than 4. This is quite a nice way of creating as many layers as you need in a network to handle arbitrary sized images and turn them into a fixed known grid size. Question: Does GAN need a lot more data than say dogs vs. cats or NLP? Or is it comparable [59:48]? Honestly, I am kind of embarrassed to say I am not an expert practitioner in GANs. The stuff I teach in part one is things I am happy to say I know the best way to do these things and so I can show you state-of-the-art results like we just did with CIFAR10 with the help of some of the students. I am not there at all with GANs so I am not quite sure how much you need. In general, it seems it needs quite a lot but remember the only reason we didn\u2019t need too much in dogs and cats is because we had a pre-trained model and could we leverage pre-trained GAN models and fine tune them, probably. I don\u2019t think anybody has done it as far as I know. That could be really interesting thing for people to think about and experiment with. Maybe people have done it and there is some literature there we haven\u2019t come across. I\u2019m somewhat familiar with the main pieces of literature in GANs but I don\u2019t know all of it, so maybe I\u2019ve missed something about transfer learning in GANs. But that would be the trick to not needing too much data. Question: So the huge speed-up a combination of one cycle learning rate and momentum annealing plus the eight GPU parallel training in the half precision? Is that only possible to do the half precision calculation with consumer GPU? Another question, why is the calculation 8 times faster from single to half precision, while from double the single is only 2 times faster [1:01:09]? Okay, so the CIFAR10 result, it\u2019s not 8 times faster from single to half. It\u2019s about 2 or 3 times as fast from single to half. NVIDIA claims about the flops performance of the tensor cores, academically correct, but in practice meaningless because it really depends on what calls you need for what piece \u2014 so about 2 or 3x improvement for half. So the half precision helps a bit, the extra GPUs helps a bit, the one cycle helps an enormous amount, then another key piece was the playing around with the parameters that I told you about. So reading the wide ResNet paper carefully, identifying the kinds of things that they found there, and then writing a version of the architecture you just saw that made it really easy for us to fiddle around with parameters, staying up all night trying every possible combination of different kernel sizes, numbers of kernels, number of layer groups, size of layer groups. And remember, we did a bottleneck but actually we tended to focus instead on widening so we increase the size and then decrease it because it takes better advantage of the GPU. So all those things combined together, I\u2019d say the one cycle was perhaps the most critical but every one of those resulted in a big speed-up. That\u2019s why we were able to get this 30x improvement over the state-of-the-art CIFAR10. We have some ideas for other things \u2014 after this DAWN bench finishes, maybe we\u2019ll try and go even further to see if we can beat one minute one day. That\u2019ll be fun. class DCGAN_D(nn.Module):\n\n def __init__(self, isize, nc, ndf, n_extra_layers=0):\n\n super().__init__()\n\n assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n\n\n\n self.initial = ConvBlock(nc, ndf, 4, 2, bn=False)\n\n csize,cndf = isize/2,ndf\n\n self.extra = nn.Sequential(*[ConvBlock(cndf, cndf, 3, 1)\n\n for t in range(n_extra_layers)])\n\n\n\n pyr_layers = []\n\n while csize > 4:\n\n pyr_layers.append(ConvBlock(cndf, cndf*2, 4, 2))\n\n cndf *= 2; csize /= 2\n\n self.pyramid = nn.Sequential(*pyr_layers)\n\n \n\n self.final = nn.Conv2d(cndf, 1, 4, padding=0, bias=False)\n\n\n\n def forward(self, input):\n\n x = self.initial(input)\n\n x = self.extra(x)\n\n x = self.pyramid(x)\n\n return self.final(x).mean(0).view(1) So here is our discriminator [1:03:37].The important thing to remember about an architecture is it doesn\u2019t do anything rather than have some input tensor size and rank, and some output tensor size and rank. As you see the last conv has one channel. This is different from what we are used to because normally our last thing is a linear block. But our last layer here is a conv block. It only has one channel but it has a grid size of something around 4x4 (no more than 4x4). So we are going to spit out (let\u2019s say it\u2019s 4x4), 4 by 4 by 1 tensor. What we then do is we then take the mean of that. So it goes from 4x4x1 to a scalar. This is kind of like the ultimate adaptive average pooling because we have something with just one channel and we take the mean. So this is a bit different \u2014 normally we first do average pooling and then we put it through a fully connected layer to get our one thing out. But this is getting one channel out and then taking the mean of that. Jeremy suspects that it would work better if we did the normal way, but he hasn\u2019t tried it yet and he doesn\u2019t really have a good enough intuition to know whether he is missing something \u2014 but it will be an interesting experiment to try if somebody wants to stick an adaptive average pooling layer and a fully connected layer afterwards with a single output. So that\u2019s a discriminator. Let\u2019s assume we already have a generator \u2014 somebody says \u201cokay, here is a generator which generates bedrooms. I want you to build a model that can figure out which ones are real and which ones aren\u2019t\u201d. We are going to take the dataset and label bunch of images which are fake bedrooms from the generator, and a bunch of images of real bedrooms from LSUN dataset to stick a 1 or a 0 on each one. Then we\u2019ll try to get the discriminator to tell the difference. So that is going to be simple enough. But we haven\u2019t been given a generator. We need to build one. We haven\u2019t talked about the loss function yet \u2014 we are going to assume that there\u2019s some loss function that does this thing. A generator is also an architecture which doesn\u2019t do anything by itself until we have a loss function and data. But what are the ranks and sizes of the tensors? The input to the generator is going to be a vector of random numbers. In the paper, they call that the \u201cprior.\u201d How big? We don\u2019t know. The idea is that a different bunch of random numbers will generate a different bedroom. So our generator has to take as input a vector, stick it through sequential models, and turn it into a rank 4 tensor (rank 3 without the batch dimension) \u2014 height by width by 3. So in the final step, (number of channel) is going to have to end up being 3 because it\u2019s going to create a 3 channel image of some size. class DeconvBlock(nn.Module):\n\n def __init__(self, ni, no, ks, stride, pad, bn=True):\n\n super().__init__()\n\n self.conv = nn.ConvTranspose2d(ni, no, ks, stride, \n\n padding=pad, bias=False)\n\n self.bn = nn.BatchNorm2d(no)\n\n self.relu = nn.ReLU(inplace=True)\n\n \n\n def forward(self, x):\n\n x = self.relu(self.conv(x))\n\n return self.bn(x) if self.bn else x class DCGAN_G(nn.Module):\n\n def __init__(self, isize, nz, nc, ngf, n_extra_layers=0):\n\n super().__init__()\n\n assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n\n\n\n cngf, tisize = ngf//2, 4\n\n while tisize!=isize: cngf*=2; tisize*=2\n\n layers = [DeconvBlock(nz, cngf, 4, 1, 0)]\n\n\n\n csize, cndf = 4, cngf\n\n while csize < isize//2:\n\n layers.append(DeconvBlock(cngf, cngf//2, 4, 2, 1))\n\n cngf //= 2; csize *= 2\n\n\n\n layers += [DeconvBlock(cngf, cngf, 3, 1, 1) \n\n for t in range(n_extra_layers)]\n\n layers.append(nn.ConvTranspose2d(cngf, nc, 4, 2, 1,\n\n bias=False))\n\n self.features = nn.Sequential(*layers)\n\n\n\n def forward(self, input): return F.tanh(self.features(input)) Question: In ConvBlock, is there a reason why batch norm comes after ReLU (i.e. ) [1:07:50]? I would normally expect to go ReLU then batch norm [1:08:23] that this is actually the order that makes sense to Jeremy. The order we had in the darknet was what they used in the darknet paper, so everybody seems to have a different order of these things. In fact, most people for CIFAR10 have a different order again which is batch norm \u2192 ReLU \u2192 conv which is a quirky way of thinking about it, but it turns out that often for residual blocks that works better. That is called a \u201cpre-activation ResNet.\u201d There is a few blog posts out there where people have experimented with different order of those things and it seems to depend a lot on what specific dataset it is and what you are doing with \u2014 although the difference in performance is small enough that you won\u2019t care unless it\u2019s for a competition. So the generator needs to start with a vector and end up with a rank 3 tensor. We don\u2019t really know how to do that yet. We need to use something called a \u201cdeconvolution\u201d and PyTorch calls it transposed convolution \u2014 same thing, different name. Deconvolution is something which rather than decreasing the grid size, it increases the grid size. Sl as with all things, it\u2019s easiest to see in an Excel spreadsheet. Here is a convolution. We start, let\u2019s say, with a 4 by 4 grid cell with a single channel. Let\u2019s put it through a 3 by 3 kernel with a single output filter. So we have a single channel in, a single filter kernel, so if we don\u2019t add any padding, we are going to end up with 2 by 2. Remember, the convolution is just the sum of the product of the kernel and the appropriate grid cell [1:11:09]. So there is our standard 3 by 3 conv one channel one filter. So the idea now is we want to go the opposite direction [1:11:25]. We want to start with our 2 by 2 and we want to create a 4 by 4. Specifically we want to create the same 4 by 4 that we started with. And we want to do that by using a convolution. How would we do that? If we have a 3 by 3 convolution, then if we want to create a 4 by 4 output, we are going to need to create this much padding: Because with this much padding, we are going to end up with 4 by 4. So let\u2019s say our convolutional filter was just a bunch of zeros then we can calculate our error for each cell just by taking this subtraction: Then we can get the sum of absolute values (L1 loss) by summing up the absolute values of those errors: So now we could use optimization, in Excel it\u2019s called \u201csolver\u201d to do a gradient descent. So we will set the Total cell equal to minimum and we\u2019ll try and reduce our loss by changing our filter. You can see it\u2019s come up with a filter such that Result is almost like Data. It\u2019s not perfect, and in general, you can\u2019t assume that a deconvolution can exactly create the same exact thing you want because there is just not enough. Because there is 9 things in the filter and 16 things in the result. But it\u2019s made a pretty good attempt. So this is what a deconvolution looks like \u2014 a stride 1, 3x3 deconvolution on a 2x2 grid cell input. Question: How difficult is it to create a discriminator to identify fake news vs. real news [1:13:43]? You don\u2019t need anything special \u2014 that\u2019s just a classifier. So you would just use the NLP classifier from previous class and lesson 4. In that case, there is no generative piece, so you just need a dataset that says these are the things that we believe are fake news and these are the things we consider to be real news and it should actually work very well. To the best of our knowledge, if you try it you should get as good a result as anybody else has got \u2014 whether it\u2019s good enough to be useful in practice, Jeremy doesn\u2019t know. The best thing you could do at this stage would be to generate a kind of a triage that says these things look pretty sketchy based on how they are written and then some human could go in and fact check them. NLP classifier and RNN can\u2019t fact-check things but it could recognize that these are written in that kind of highly popularized style which often fake news is written in so maybe these ones are worth paying attention to. That would probably be the best you could hope for without drawing on some kind of external data sources. But it\u2019s important to remember the discriminator is basically just a classifier and you don\u2019t need any special techniques beyond what we\u2019ve already learned to do NLP classification. To do deconvolution in PyTorch, just say: The reason it\u2019s called a ConvTranspose is because it turns out that this is the same as the calculation of the gradient of convolution. That\u2019s why they call it that. One on the left is what we just saw of doing a 2x2 deconvolution. If there is a stride 2, then you don\u2019t just have padding around the outside, but you actually have to put padding in the middle as well. They are not actually quite implemented this way because this is slow to do. In practice, you\u2019ll implement them in a different way but it all happens behind the scene, so you don\u2019t have to worry about it. We\u2019ve talked about this convolution arithmetic tutorial before and if you are still not comfortable with convolutions and in order to get comfortable with deconvolutions, this is a great site to go to. If you want to see the paper, it is A guide to convolution arithmetic for deep learning. looks identical to a except it has the word [1:17:49]. We just go conv \u2192 relu \u2192 batch norm as before, and it has input filters and output filters. The only difference is taht stride 2 means that the grid size will double rather than half. Question: Both and seem to do the same thing, i.e. expand grid-size (height and width) from previous layer. Can we say is always better than , since is merely resize and fill unknowns by zero\u2019s or interpolation [1:18:10]? No, you can\u2019t. There is a fantastic interactive paper on distill.pub called Deconvolution and Checkerboard Artifacts which points out that what we are doing right now is extremely suboptimal but the good news is everybody else does it. Have a look here, could you see these checkerboard artifacts? These are all from actual papers and basically they noticed every one of these papers with generative models have these checkerboard artifacts and what they realized is it\u2019s because when you have a stride 2 convolution of size three kernel, they overlap. So some grid cells gets twice as much activation, So even if you start with random weights, you end up with a checkerboard artifacts. So deeper you get, the worse it gets. Their advice is less direct than it ought to be, Jeremy found that for most generative models, upsampling is better. If you , it\u2019s basically doing the opposite of pooling \u2014 it says let\u2019s replace this one grid cell with four (2x2). There is a number of ways to upsample \u2014 one is just to copy it all across to those four, and other is to use bilinear or bicubic interpolation. There are various techniques to try and create a smooth upsampled version and you can choose any of them in PyTorch. If you do a 2 x 2 upsample and then regular stride one 3 x 3 convolution, that is another way of doing the same kind of thing as a ConvTranspose \u2014 it\u2019s doubling the grid size and doing some convolutional arithmetic on it. For generative models, it pretty much always works better. In that distil.pub publication, they indicate that maybe that\u2019s a good approach but they don\u2019t just come out and say just do this whereas Jeremy would just say just do this. Having said that, for GANS, he hasn\u2019t had that much success with it yet and he thinks it probably requires some tweaking to get it to work, The issue is that in the early stages, it doesn\u2019t create enough noise. He had a version where he tried to do it with an upsample and you could kind of see that the noise didn\u2019t look very noisy. Next week when we look at style transfer and super-resolution, you will see really comes into its own. The generator, we can now start with the vector [1:22:04]. We can decide and say okay let\u2019s not think of it as a vector but actually it\u2019s 1x1 grid cell, and then we can turn it into a 4x4 then 8x8 and so forth. That is why we have to make sure it\u2019s a suitable multiple so that we can create something of the right size. As you can see, it\u2019s doing the exact opposite as before. It\u2019s making the cell size bigger and bigger by 2 at a time as long as it can until it gets to half the size that we want, and then finally we add more on at the end with stride 1. Then we add one more ConvTranspose to finally get to the size that we wanted and we are done. Finally we put that through a and that will force us to be in the zero to one range because of course we don\u2019t want to spit out arbitrary size pixel values. So we have a generator architecture which spits out an image of some given size with the correct number of channels with values between zero and one. At this point, we can now create our model data object [1:23:38]. These things take a while to train, so we made it 128 by 128 (just a convenient way to make it a little bit faster). So that is going to be the size of the input, but then we are going to use transformation to turn it into 64 by 64. There\u2019s been more recent advances which have attempted to really increase this up to high resolution sizes but they still tend to require either a batch size of 1 or lots and lots of GPUs [1:24:05]. So we are trying to do things that we can do with a single consumer GPU. Here is an example of one of the 64 by 64 bedrooms. Putting them all together [1:24:30] We are going to do pretty much everything manually so let\u2019s go ahead and create our two models \u2014 our generator and discriminator and as you can see they are DCGAN, so in other words, they are the same modules that appeared in this paper. It is well worth going back and looking at the DCGAN paper to see what these architectures are because it\u2019s assumed that when you read the Wasserstein GAN paper that you already know that. Question: Shouldn\u2019t we use a sigmoid if we want values between 0 and 1 [1:25:06]? As usual, our images have been normalized to have a range from -1 to 1, so their pixel values don\u2019t go between 0 and 1 anymore. This is why we want values going from -1 to 1 otherwise we wouldn\u2019t give a correct input for the discriminator (via this post). So we have a generator and a discriminator, and we need a function that returns a \u201cprior\u201d vector (i.e. a bunch of noise)[1:25:49]. We do that by creating a bunch of zeros. is the size of \u2014 very often in our code, if you see a mysterious letter, it\u2019s because that\u2019s the letter they used in the paper. Here, is the size of our noise vector. We then use normal distribution to generate random numbers between 0 and 1. And that needs to be a variable because it\u2019s going to be participating in the gradient updates. So here is an example of creating some noise and resulting four different pieces of noise. We need an optimizer in order to update our gradients [1:26:41]. In the Wasserstein GAN paper, they told us to use RMSProp: We can easily do that in PyTorch: In the paper, they suggested a learning rate of 0.00005 ( ), we found seem to work, so we made it a little bit bigger. Now we need a training loop [1:27:14]: A training loop will go through some number of epochs that we get to pick (so that\u2019s going to be a parameter). Remember, when you do everything manually, you\u2019ve got to remember all the manual steps to do: You have to set your modules into training mode when you are training them and into evaluation mode when you are evaluating because in training mode batch norm updates happen and dropout happens, in evaluation mode, those two things gets turned off. We are going to grab an iterator from our training data loader We are going to see how many steps we have to go through and then we will use to give us a progress bar, and we are going to go through that many steps. The first step of the algorithm in the paper is to update the discriminator (in the paper, they call discriminator a \u201ccritic\u201d and is the weights of the critic). So the first step is to train our critic a little bit, and then we are going to train our generator a little bit, and we will go back to the top of the loop. The inner loop in the paper correspond to the second loop in our code. What we are going to do now is we have a generator that is random at the moment [1:29:06]. So our generator will generate something that looks like the noise. First of all, we need to teach our discriminator to tell the difference between the noise and a bedroom \u2014 which shouldn\u2019t be too hard you would hope. So we just do it in the usual way but there is a few little tweaks: We are going to grab a mini batch of real bedroom photos so we can just grab the next batch from our iterator, turn it into a variable. Then we are going to calculate the loss for that \u2014 so this is going to be how much the discriminator thinks this looks fake (\u201cdoes the real one look fake?\u201d). Then we are going to create some fake images and to do that we will create some random noise, and we will stick it through our generator which at this stage is just a bunch of random weights. That will create a mini batch of fake images. Then we will put that through the same discriminator module as before to get the loss for that (\u201chow fake does the fake one look?\u201d). Remember, when you do everything manually, you have to zero the gradients ( ) in your loop. If you have forgotten about that, go back to the part 1 lesson where we do everything from scratch. Finally, the total discriminator loss is equal to the real loss minus the fake loss. So you can see that here [1:30:58]: They don\u2019t talk about the loss, they actually just talk about one of the gradient updates. In PyTorch, we don\u2019t have to worry about getting the gradients, we can just specify the loss and call then discriminator\u2019s [1:34:27]. There is one key step which is that we have to keep all of our weights which are the parameters in PyTorch module in the small range of -0.01 and 0.01. Why? Because the mathematical assumptions that make this algorithm work only apply in a small ball. It is interesting to understand the math of why that is the case, but it\u2019s very specific to this one paper and understanding it won\u2019t help you understand any other paper, so only study it if you are interested. It is nicely explained and Jeremy thinks it\u2019s fun but it won\u2019t be information that you will reuse elsewhere unless you get super into GANs. He also mentioned that after the came out and improved Wasserstein GAN came out that said there are better ways to ensure that your weight space is in this tight ball which was to penalize gradients that are too high, so nowadays there are slightly different ways to do this. But this line of code is the key contribution and it is what makes it Wasserstein GAN: At the end of this, we have a discriminator that can recognize real bedrooms and our totally random crappy generated images [1:36:20]. Let\u2019s now try and create some better images. So now set trainable discriminator to false, set trainable generator to true, zero out the gradients of the generator. Our loss again is (discriminator) of the generator applied to some more random noise. So it\u2019s exactly the same as before where we did generator on the noise and then pass that to a discriminator, but this time, the thing that\u2019s trainable is the generator, not the discriminator. In other words, in the pseudo code, the thing they update is \u019f which is the generator\u2019s parameters. So it takes noise, generate some images, try and figure out if they are fake or real, and use that to get gradients with respect to the generator, as opposed to earlier we got them with respect to the discriminator, and use that to update our weights with RMSProp with an alpha learning rate [1:38:21]. def train(niter, first=True):\n\n gen_iterations = 0\n\n for epoch in trange(niter):\n\n netD.train(); netG.train()\n\n data_iter = iter(md.trn_dl)\n\n i,n = 0,len(md.trn_dl)\n\n with tqdm(total=n) as pbar:\n\n while i < n:\n\n set_trainable(netD, True)\n\n set_trainable(netG, False)\n\n d_iters = 100 if (first and (gen_iterations < 25) \n\n or (gen_iterations % 500 == 0)) else 5\n\n j = 0\n\n while (j < d_iters) and (i < n):\n\n j += 1; i += 1\n\n for p in netD.parameters(): \n\n p.data.clamp_(-0.01, 0.01)\n\n real = V(next(data_iter)[0])\n\n real_loss = netD(real)\n\n fake = netG(create_noise(real.size(0)))\n\n fake_loss = netD(V(fake.data))\n\n netD.zero_grad()\n\n lossD = real_loss-fake_loss\n\n lossD.backward()\n\n optimizerD.step()\n\n pbar.update()\n\n\n\n set_trainable(netD, False)\n\n set_trainable(netG, True)\n\n netG.zero_grad()\n\n lossG = netD(netG(create_noise(bs))).mean(0).view(1)\n\n lossG.backward()\n\n optimizerG.step()\n\n gen_iterations += 1\n\n \n\n print(f'Loss_D {to_np(lossD)}; Loss_G {to_np(lossG)}; '\n\n f'D_real {to_np(real_loss)}; Loss_D_fake\n\n {to_np(fake_loss)}') You\u2019ll see that it\u2019s unfair that the discriminator is getting trained ncritic times ( in above code) which they set to 5 for every time we train the generator once. And the paper talks a bit about this but the basic idea is there is no point making the generator better if the discriminator doesn\u2019t know how to discriminate yet. So that\u2019s why we have the second while loop. And here is that 5: d_iters = 100 if (first and (gen_iterations < 25) \n\n or (gen_iterations % 500 == 0)) else 5 Actually something which was added in the later paper or maybe supplementary material is the idea that from time to time and a bunch of times at the start, you should do more steps at the discriminator to make sure that the discriminator is capable. Let\u2019s train that for one epoch: Then let\u2019s create some noise so we can generate some examples. But before that, reduce the learning rate by 10 and do one more pass: Then let\u2019s use the noise to pass it to our generator, then put it through our denormalization to turn it back into something we can see, and then plot it: And we have some bedrooms. These are not real bedrooms, and some of them don\u2019t look particularly like bedrooms, but some of them look a lot like bedrooms, so that\u2019s the idea. That\u2019s GAN. The best way to think about GAN is it is like an underlying technology that you will probably never use like this, but you will use in lots of interesting ways. For example, we are going to use it to create a cycle GAN. Question: Is there any reason for using RMSProp specifically as the optimizer as opposed to Adam etc. [1:41:38]? I don\u2019t remember it being explicitly discussed in the paper. I don\u2019t know if it\u2019s just experimental or the theoretical reason. Have a look in the paper and see what it says. From experimenting I figured that Adam and WGANs not just work worse \u2014 it causes to completely fail to train meaningful generator. Finally, as a negative result, we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam [8] (with \u03b21>0) on the critic, or when one uses high learning rates. Since the loss for the critic is nonstationary, momentum based methods seemed to perform worse. We identified momentum as a potential cause because, as the loss blew up and samples got worse, the cosine between the Adam step and the gradient usually turned negative. The only places where this cosine was negative was in these situations of instability. We therefore switched to RMSProp [21] which is known to perform well even on very nonstationary problems Question: Which could be a reasonable way of detecting overfitting while training? Or of evaluating the performance of one of these GAN models once we are done training? In other words, how does the notion of train/val/test sets translate to GANs [1:41:57]? That is an awesome question, and there\u2019s a lot of people who make jokes about how GANs is the one field where you don\u2019t need a test set and people take advantage of that by making stuff up and saying it looks great. There are some famous problems with GANs, one of them is called Mode Collapse. Mode collapse happens where you look at your bedrooms and it turns out that there\u2019s only three kinds of bedrooms that every possible noise vector maps to. You look at your gallery and it turns out they are all just the same thing or just three different things. Mode collapse is easy to see if you collapse down to a small number of modes, like 3 or 4. But what if you have a mode collapse down to 10,000 modes? So there are only 10,000 possible bedrooms that all of your noise vectors collapse to. You wouldn\u2019t be able to see in the gallery view we just saw because it\u2019s unlikely you would have two identical bedrooms out of 10,000. Or what if every one of these bedrooms is basically a direct copy of one of the input \u2014 it basically memorized some input. Could that be happening? And the truth is, most papers don\u2019t do a good job or sometimes any job of checking those things. So the question of how do we evaluate GANS and even the point of maybe we should actually evaluate GANs properly is something that is not widely enough understood even now. Some people are trying to really push. Ian Goodfellow was the first author on the most famous deep learning book and is the inventor of GANs and he\u2019s been sending continuous stream of tweets reminding people about the importance of testing GANs properly. If you see a paper that claims exceptional GAN results, then this is definitely something to look at. Have they talked about mode collapse? Have they talked about memorization? And so forth. Question: Can GANs be used for data augmentation [1:45:33]? Yeah, absolutely you can use GAN for data augmentation. Should you? I don\u2019t know. There are some papers that try to do semi-supervised learning with GANs. I haven\u2019t found any that are particularly compelling showing state-of-the-art results on really interesting datasets that have been widely studied. I\u2019m a little skeptical and the reason I\u2019m a little skeptical is because in my experience, if you train a model with synthetic data, the neural net will become fantastically good at recognizing the specific problems of your synthetic data and that\u2019ll end up what it\u2019s learning from. There are lots of other ways of doing semi-supervised models which do work well. There are some places that can work. For example, you might remember Otavio Good created that fantastic visualization in part 1 of the zooming conv net where it showed letter going through MNIST, he, at least at that time, was the number one in autonomous remote control car competitions, and he trained his model using synthetically augmented data where he basically took real videos of a car driving around the circuit and added fake people and fake other cars. I think that worked well because A. he is kind of a genius and B. because I think he had a well defined little subset that he had to work in. But in general, it\u2019s really really hard to use synthetic data. I\u2019ve tried using synthetic data and models for decades now (obviously not GANs because they\u2019re pretty new) but in general it\u2019s very hard to do. Very interesting research question. We are going to use cycle GAN to turn horses into zebras. You can also use it to turn Monet prints into photos or to turn photos of Yosemite in summer into winter. This is going to be really straight forward because it\u2019s just a neural net [1:44:46]. All we are going to do is we are going to create an input containing lots of zebra photos and with each one we\u2019ll pair it with an equivalent horse photo and we\u2019ll just train a neural net that goes from one to the other. Or you could do the same thing for every Monet painting \u2014 create a dataset containing the photo of the place \u2026oh wait, that\u2019s not possible because the places that Monet painted aren\u2019t there anymore and there aren\u2019t exact zebra versions of horses \u2026how the heck is this going to work? This seems to break everything we know about what neural nets can do and how they do them. So somehow these folks at Berkeley cerated a model that can turn a horse into a zebra despite not having any photos. Unless they went out there and painted horses and took before-and-after shots but I believe they didn\u2019t [1:47:51]. So how the heck did they do this? It\u2019s kind of genius. The person I know who is doing the most interesting practice of cycle GAN right now is one of our students Helena Sarin @glagolista. She is the only artist I know of who is a cycle GAN artist.\n\nHere are some more of her amazing works and I think it\u2019s really interesting. I mentioned at the start of this class that GANs are in the category of stuff that is not there yet, but it\u2019s nearly there. And in this case, there is at least one person in the world who is creating beautiful and extraordinary artworks using GANs (specifically cycle GANs). At least a dozen people I know of who are just doing interesting creative work with neural nets more generally. And the field of creative AI is going to expand dramatically. Here is the basic trick [1:50:11]. This is from the cycle GAN paper. We are going to have two images (assuming we are doing this with images). The key thing is they are not paired images, so we don\u2019t have a dataset of horses and the equivalent zebras. We have bunch of horses, and bunch of zebras. Grab one horse X, grab one zebra Y. We are going to train a generator (what they call here a \u201cmapping function\u201d) that turns horse into zebra. We\u2019ll call that mapping function G and we\u2019ll create one mapping function (a.k.a. generator) that turns a zebra into a horse and we will call that F. We will create a discriminator just like we did before which is going to get as good as possible at recognizing real from fake horses so that will be Dx. Another discriminator which is going to be as good as possible at recognizing real from fake zebras, we will call that Dy. That is our starting point. The key thing to making this work [1:51:27]\u2014 so we are generating a loss function here (Dx and Dy). We are going to create something called cycle-consistency loss which says after you turn your horse into a zebra with your generator, and check whether or not I can recognize that it\u2019s a real. We turn our horse into a zebra and then going to try and turn that zebra back into the same horse that we started with. Then we are going to have another function that is going to check whether this horse which are generated knowing nothing about x \u2014 generated entirely from this zebra Y is similar to the original horse or not. So the idea would be if your generated zebra doesn\u2019t look anything like your original horse, you\u2019ve got no chance of turning it back into the original horse. So a loss which compares x-hat to x is going to be really bad unless you can go into Y and back out again and you\u2019re probably going to be able to do that if you\u2019re able to create a zebra that looks like the original horse so that you know what the original horse looked like. And vice versa \u2014 take your zebra, turn it into a fake horse, and check that you can recognize that and then try and turn it back into the original zebra and check that it looks like the original. So notice F (zebra to horse) and G (horse to zebra) are doing two things [1:53:09]. They are both turning the original horse into the zebra, and then turning the zebra back into the original horse. So there are only two generators. There isn\u2019t a separate generator for the reverse mapping. You have to use the same generator that was used for the original mapping. So this is the cycle-consistency loss. I think this is genius. The idea that this is a thing that could even be possible. Honestly when this came out, it just never occurred to me as a thing that I could even try and solve. It seems so obviously impossible and then the idea that you can solve it like this \u2014 I just think it\u2019s so darn smart. It\u2019s good to look at the equations in this paper because they are good examples \u2014 they are written pretty simply and it\u2019s not like some of the Wasserstein GAN paper which is lots of theoretical proofs and whatever else [1:54:05]. In this case, they are just equations that lay out what\u2019s going on. You really want to get to a point where you can read them and understand them. So we\u2019ve got a horse X and a zebra Y[1:54:34]. For some mapping function G which is our horse to zebra mapping function then there is a GAN loss which is a bit we are already familiar with it says we have a horse, a zebra, a fake zebra recognizer, and a horse-zebra generator. The loss is what we saw before \u2014 it\u2019s our ability to draw one zebra out of our zebras and recognize whether it is real or fake. Then take a horse and turn it into a zebra and recognize whether that\u2019s real or fake. You then do one minus the other (in this case, they have a log in there but the log is not terribly important). So this is the thing we just saw. That is why we did Wasserstein GAN first. This is just a standard GAN loss in math form. Question: All of this sounds awfully like translating in one language to another then back to the original. Have GANs or any equivalent been tried in translation [1:55:54]? Paper from the forum. Back up to what I do know \u2014 normally with translation you require this kind of paired input (i.e. parallel text \u2014 \u201cthis is the French translation of this English sentence\u201d). There has been a couple of recent papers that show the ability to create good quality translation models without paired data. I haven\u2019t implemented them and I don\u2019t understand anything I haven\u2019t implemented, but they may well be doing the same basic idea. We\u2019ll look at it during the week and get back to you. Cycle-consistency loss [1:57:14]: So we\u2019ve got a GAN loss and the next piece is the cycle-consistency loss. So the basic idea here is that we start with our horse, use our zebra generator on that to create a zebra, use our horse generator on that to create a horse and compare that to the original horse. This double lines with the 1 is the L1 loss \u2014 sum of the absolute value of differences [1:57:35]. Where else if this was 2, it would be the L2 loss so the 2-norm which would be the sum of the squared differences. We now know this squiggle idea which is from our horses grab a horse. This is what we mean by sample from a distribution. There\u2019s all kinds of distributions but most commonly in these papers we\u2019re using an empirical distribution, in other words we\u2019ve got some rows of data, grab a row. So here, it is saying grab something from the data and we are going to call that thing x. To recapture: Compare it to the original and sum of the absolute values Do it for zebra to horse as well And add the two together That is our cycle-consistency loss. Now we get our loss function and the whole loss function depends on: the cycle-consistency loss for our two generators We have a lambda here which hopefully we are kind of used to this idea now that is when you have two different kinds of loss, you chuck in a parameter there you can multiply them by so they are about the same scale [1:59:23]. We did a similar thing with our bounding box loss compared to our classifier loss when we did the localization. Then for this loss function, we are going to try to maximize the capability of the discriminators to discriminate, whilst minimizing that for the generators. So the generators and the discriminators are going to be facing off against each other. When you see this min max thing in papers, it basically means this idea that in your training loop, one thing is trying to make something better, the other is trying to make something worse, and there\u2019re lots of ways to do it but most commonly, you\u2019ll alternate between the two. You will often see this just referred to in math papers as min-max. So when you see min-max, you should immediately think adversarial training. Let\u2019s look at the code. We are going to do something almost unheard of which is I started looking at somebody else\u2019s code and I was not so disgusted that I threw the whole thing away and did it myself. I actually said I quite like this, I like it enough I\u2019m going to show it to my students. This is where the code came from, and this is one of the people that created the original code for cycle GANs and they created a PyTorch version. I had to clean it up a little bit but it\u2019s actually pretty darn good. The cool thing about this is that you are now going to get to see almost all the bits of fast.ai or all the relevant bits of fast.ai written in a different way by somebody else. So you\u2019re going to get to see how they do datasets, data loaders, models, training loops, and so forth. You\u2019ll find there is a directory [2:02:12] which is basically nearly the original with some cleanups which I hope to submit as a PR sometime . It was written in a way that unfortunately made it a bit over connected to how they were using it as a script, so I cleaned it up a little bit so I could use it as a module. But other than that, it\u2019s pretty similar. So is their code copied from their github repo with some minor changes. The way mini library has been set up is that the configuration options, they are assuming, are being passed into like a script. So they have method and I\u2019m basically passing in an array of script options (where\u2019s my data, how many threads, do I want to dropout, how many iterations, what am I going to call this model, which GPU do I want run it on). That gives us an object which you can see what it contains. You\u2019ll see that it contains some things we didn\u2019t mention that is because it has defaults for everything else that we didn\u2019t mention. So rather than using fast.ai stuff, we are going to largely use cgan stuff. The first thing we are going to need is a data loader. So this is also a great opportunity for you again to practice your ability to navigate through code with your editor or IDE of choice. We are going to start with . You should be able to go find symbol or in vim tag to jump straight to and we can see that\u2019s creating a . Then we can see is a . We can see that it\u2019s going to use a standard PyTorch DataLoader, so that\u2019s good. We know if you are going to use a standard PyTorch DataLoader, you have pass it a dataset, and we know that a dataset is something that contains a length and an indexer so presumably when we look at it\u2019s going to do that. Here is and this library does more than just cycle GAN \u2014 it handles both aligned and unaligned image pairs [2:04:46]. We know that our image pairs are unaligned so we are going to . As expected, it has and . For length, A and B are our horses and zebras, we got two sets, so whichever one is longer is the length of the . is going to: Randomly grab something from each of our two horses and zebras Open them up with pillow (PIL) Run them through some transformations Then we could either be turning horses into zebras or zebras into horses, so there\u2019s some direction Return our horse, zebra, a path to the horse, and a path of zebra Hopefully you can kind of see that this is looking pretty similar to the kind of things fast.ai does. Fast.ai obviously does quite a lot more when it comes to transforms and performance, but remember, this is research code for this one thing and it\u2019s pretty cool that they did all this work. We\u2019ve got a data loader so we can go and load our data into it [2:06:17]. That will tell us how many mini-batches are in it (that\u2019s the length of the data loader in PyTorch). Next step is to create a model. Same idea, we\u2019ve got different kind of models and we\u2019re going to be doing a cycle GAN. Here is our . There is quite a lot of stuff in , so let\u2019s go through and find out what\u2019s going to be used. At this stage, we\u2019ve just called initializer so when we initialize it, it\u2019s going to go through and define two generators which is not surprising a generator for our horses and a generator for zebras. There is some way for it to generate a pool of fake data and then we\u2019re going to grab our GAN loss, and as we talked about our cycle-consistency loss is an L1 loss. They are going to use Adam, so obviously for cycle GANS they found Adam works pretty well. Then we are going to have an optimizer for our horse discriminator, an optimizer for our zebra discriminator, and an optimizer for our generator. The optimizer for the generator is going to contain the parameters both for the horse generator and the zebra generator all in one place. So the initializer is going to set up all of the different networks and loss functions we need and they are going to be stored inside this [2:08:14]. It then prints out and shows us exactly the PyTorch model we have. It\u2019s interesting to see that they are using ResNets and so you can see the ResNets look pretty familiar, so we have conv, batch norm, Relu. is just the same as batch norm basically but it applies to one image at a time and the difference isn\u2019t particularly important. And you can see they are doing reflection padding just like we are. You can kind of see when you try to build everything from scratch like this, it is a lot of work and you can forget the nice little things that fast.ai does automatically for you. You have to do all of them by hand and only you end up with a subset of them. So over time, hopefully soon, we\u2019ll get all of this GAN stuff into fast.ai and it\u2019ll be nice and easy. We\u2019ve got our model and remember the model contains the loss functions, generators, discriminators, all in one convenient place [2:09:32]. I\u2019ve gone ahead and copied and pasted and slightly refactored the training loop from their code so that we can run it inside the notebook. So this one should look a lot familiar. A loop to go through each epoch and a loop to go through the data. Before we did this, we set up . This is actually not a PyTorch dataset, I think this is what they used slightly confusingly to talk about their combined what we would call a model data object \u2014 all the data that they need. Loop through that with to get a progress bar, and so now we can go through and see what happens in the model. total_steps = 0\n\n\n\nfor epoch in range(opt.epoch_count, opt.niter + opt.niter_decay+1):\n\n epoch_start_time = time.time()\n\n iter_data_time = time.time()\n\n epoch_iter = 0\n\n\n\n for i, data in tqdm(enumerate(dataset)):\n\n iter_start_time = time.time()\n\n if total_steps % opt.print_freq == 0: \n\n t_data = iter_start_time - iter_data_time\n\n total_steps += opt.batchSize\n\n epoch_iter += opt.batchSize\n\n model.set_input(data)\n\n model.optimize_parameters()\n\n\n\n if total_steps % opt.display_freq == 0:\n\n save_result = total_steps % opt.update_html_freq == 0\n\n\n\n if total_steps % opt.print_freq == 0:\n\n errors = model.get_current_errors()\n\n t = (time.time() - iter_start_time) / opt.batchSize\n\n\n\n if total_steps % opt.save_latest_freq == 0:\n\n print('saving the latest model(epoch %d,total_steps %d)'\n\n % (epoch, total_steps))\n\n model.save('latest')\n\n\n\n iter_data_time = time.time()\n\n if epoch % opt.save_epoch_freq == 0:\n\n print('saving the model at the end of epoch %d, iters %d' \n\n % (epoch, total_steps))\n\n model.save('latest')\n\n model.save(epoch)\n\n\n\n print('End of epoch %d / %d \\t Time Taken: %d sec' %\n\n (epoch, opt.niter + opt.niter_decay, time.time() \n\n - epoch_start_time))\n\n model.update_learning_rate() [2:10:32]: It\u2019s a different approach to what we do in fast.ai. This is kind of neat, it\u2019s quite specific to cycle GANs but basically internally inside this model is this idea that we are going to go into our data and grab the appropriate one. We are either going horse to zebra or zebra to horse, depending on which way we go, is either horse or zebra, and vice versa. If necessary put it on the appropriate GPU, then grab the appropriate paths. So the model now has a mini-batch of horses and a mini-batch of zebras. Now we optimize the parameters [2:11:19]. It\u2019s kind of nice to see it like this. You can see each step. First of all, try to optimize the generators, then try to optimize the horse discriminators, then try to optimize the zebra discriminator. is a part of PyTorch, as well as . So the interesting bit is the actual thing that does the back propagation on the generator. Here it is [2:12:04]. Let\u2019s jump to the key pieces. There\u2019s all the formula that we just saw in the paper. Let\u2019s take a horse and generate a zebra. Let\u2019s now use the discriminator to see if we can tell whether it\u2019s fake or not ( ). Then let\u2019s pop that into our loss function which we set up earlier to get a GAN loss based on that prediction. Let\u2019s do the same thing going the opposite direction using the opposite discriminator then put that through the loss function again. Then let\u2019s do the cycle consistency loss. Again, we take our fake which we created and try and turn it back again into the original. Let\u2019s use the cycle consistency loss function we created earlier to compare it to the real original. And here is that lambda \u2014 so there\u2019s some weight that we used and that would set up actually we just use the default that they suggested in their options. Then do the same for the opposite direction and then add them all together. We then do the backward step. That\u2019s it. So we can do the same thing for the first discriminator [2:13:50]. Since basically all the work has been done now, there\u2019s much less to do here. There that is. We won\u2019t step all through it but it\u2019s basically the same basic stuff that we\u2019ve already seen. So is calculating the losses and doing the optimizer step. From time to time, save and print out some results. Then from time to time, update the learning rate so they\u2019ve got some learning rate annealing built in here as well. Kind of like fast.ai, they\u2019ve got this idea of schedulers which you can then use to update your learning rates. For those of you are interested in better understanding deep learning APIs, contributing more to fast.ai, or creating your own version of some of this stuff in some different back-end, it\u2019s cool to look at a second API that covers some subset of some of the similar things to get a sense for how they are solving some of these problems and what the similarities/differences are. We train that for a little while and then we can just grab a few examples and here we have them [2:15:29]. Here are horses, zebras, and back again as horses. It took me like 24 hours to train it even that far so it\u2019s kind of slow [2:16:39]. I know Helena is constantly complaining on Twitter about how long these things take. I don\u2019t know how she\u2019s so productive with them. I will mention one more thing that just came out yesterday [2:16:54]: There is now a multi-modal image to image translation of unpaired. So you can basically now create different cats for instance from this dog. This is basically not just creating one example of the output that you want, but creating multiple ones. This came out yesterday or the day before. I think it\u2019s pretty amazing. So you can kind of see how this technology is developing and I think there\u2019s so many opportunities to maybe do this with music, speech, writing, or to create kind of tools for artists."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34?source=user_profile---------2----------------",
        "title": "Deep Learning 2: Part 2 Lesson 11 \u2013 Hiromi Suenaga \u2013",
        "text": "Let\u2019s build a sequence-to-sequence model! We are going to be working on machine translation. Machine translation is something that\u2019s been around for a long time, but we are going to look at an approach called neural translation which uses neural networks for translation. Neural machine translation appeared a couple years ago and it was not as good as the statistical machine translation approaches that use classic feature engineering and standard NLP approaches like stemming, fiddling around with word frequencies, n-grams, etc. By a year later, it was better than everything else. It is based on a metric called BLEU \u2014 we are not going to discuss the metric because it is not a very good metric and it is not interesting, but everybody uses it.\n\nWe are seeing machine translation starting down the path that we saw starting computer vision object classification in 2012 which just surpassed the state-of-the-art and now zipping past it at great rate. It is unlikely that anybody watching this is actually going to build a machine translation model because https://translate.google.com/ works quite well. So why are we learning about machine translation? The reason we are learning about machine translation is that the general idea of taking some kind of input like a sentence in French and transforming it into some other kind of output with arbitrary length such as a sentence in English is a really useful thing to do. For example, as we just saw, Hamel took GitHub issues and turn them into summaries. Another example is taking videos and turning them into descriptions, or basically anything where you are spitting out an arbitrary sized output which is very often a sentence. Maybe taking a CT scan and spitting out a radiology report \u2014 this is where you can use sequence to sequence learning.\n\nWe are going to use bi-directional GRU (basically the same as LSTM) with attention \u2014 these general ideas can also be used for lots of other things as you see above.\n\nWe are going to try to translate French into English by following the standard neural network approach:\n\nAs usual, we need pair. In this case, x: French sentence, y: English sentence which you will compare your prediction against. We need lots of these tuples of French sentences with their equivalent English sentence \u2014 that is called \u201cparallel corpus\u201d and harder to find than a corpus for a language model. For a language model, we just need text in some language. For any living language, there will be a few gigabytes at least of text floating around the internet for you to grab. For translation, there are some pretty good parallel corpus available for European languages. The European Parliament has every sentence in every European language. Anything that goes to the UN is translated to lots of languages. For French to English, we have particularly nice thing which is pretty much any semi official Canadian website will have a French version and an English version[12:13].\n\nFrench/English parallel texts from http://www.statmt.org/wmt15/translation-task.html . It was created by Chris Callison-Burch, who crawled millions of web pages and then used a set of simple heuristics to transform French URLs onto English URLs (i.e. replacing \u201cfr\u201d with \u201cen\u201d and about 40 other hand-written rules), and assume that these documents are translations of each other.\n\nFor bounding boxes, all of the interesting stuff was in the loss function, but for neural translation, all of the interesting stuff is going to be int he architecture [13:01]. Let\u2019s zip through this pretty quickly and one of the things Jeremy wants you to think about particularly is what are the relationships or the similarities in terms of the tasks we are doing and how we do it between language modeling vs. neural translation.\n\nThe first step is to do the exact same thing we do in a language model which is to take a sentence and chuck it through an RNN[13:35].\n\nNow with the classification model, we had a decoder which took the RNN output and grabbed three things: and over all of the time steps, and the value of the RNN at the last time step, stack all those together and put it through a linear layer [14:24]. Most people do not do that and just use the last time step, so all the things we will be talking about today uses the last time step.\n\nWe start out by chucking the input sentence through an RNN and out of it comes some \u201chidden state\u201d (i.e. some vector that represents the output of an RNN that has encoded the sentence).\n\nStephen used the word \u201cencoder\u201d, but we tend to use the word \u201cbackbone\u201d. Like when we talked about adding a custom head to an existing model, the existing pre-trained ImageNet model, for example, we say that is our backbone and then we stick on top of it some head that does the task we want. In sequence to sequence learning, they use the word encoder, but it basically is the same thing \u2014 it is some piece of a neural network architecture that takes the input and turns it into some representation which we can then stick a few more layers on top to grab something out of it such as we did for the classifier where we stack a linear layer on top of it to turn int into a sentiment. This time though, we have something that\u2019s a little bit harder than just creating sentiment [16:12]. Instead of turning the hidden state into a positive or negative sentiment, we want to turn it into a sequence of tokens where that sequence of token is the German sentence in Stephen\u2019s example.\n\nThis is sounding more like the language model than the classifier because the language had multiple tokens (for every input word, there was an output word). But the language model was also much easier because the number of tokens in the language model output was the same length as the number of tokens in the language model input. Not only they were the same length, but they exactly matched up (e.g. after word one comes word two, after word two comes word three, and so forth). For translating language, you don\u2019t necessarily know that the word \u201che\u201d will be translated as the first word in the output (unfortunately, it is in this particular case). Very often, the subject object order will be different or there will be some extra words inserted, or some pronouns we will need to add some gendered article, etc. This is the key issue we are going to have to deal with is the fact that we have an arbitrary length output where the tokens in the output do not correspond to the same order or the specific tokens in the input [17:31]. But the general idea is the same. Here is an RNN to encode the input, turns it into some hidden state, then this is the new thing we are going to learn is generating a sequence output.\n\nWe already know:\n\nBut we do not know yet how to do a general purpose sequence to sequence, so that\u2019s the new thing today. Very little of this will make sense unless you really understand lesson 6 how an RNN works.\n\nWe learnt that an RNN at its heart is a standard fully connected network. Below is one with 4 layers \u2014 takes an input and puts it through four layers, but at the second layer, it concatenates in the second input, third layer concatenated in the third input, but we actually wrote this in Python as just a four layer neural network. There was nothing else we used other than linear layers and ReLUs. We used the same weight matrix every time when an input came in, we used the same matrix every time when we went from one of the hidden states to the next \u2014 that is why these arrows are the same color.\n\nWe can redraw the above diagram like the below [19:29].\n\nNot only did we redraw it but we took the four lines of linear linear linear linear code in PyTorch and we replaced it with a for loop. Remember, we had something that did exactly the same thing as below, but it just had four lines of code saying and we replaced it with a for loop because that\u2019s nice to refactor. The refactoring which does not change any of the math, any of the ideas, or any of the outputs is an RNN. It\u2019s turning a bunch of separate lines in the code into a Python for loop.\n\nWe could take the output so that it is not outside the loop and put it inside the loop [20:25]. If we do that, we are now going to generate a separate output for every input. The code above, the hidden state gets replaced each time and we end up just spitting out the final hidden state. But if instead, we had something that said and returned at the end, that would be the picture below.\n\nThe main thing to remember is when we say hidden state, we are referring to a vector \u2014 technically a vector for each thing in the mini-batch so it\u2019s a matrix, but generally when Jeremy speaks about these things, he ignores the mini-batch piece and treat it for just a single item.\n\nWe also learned that you can stack these layers on top of each other [21:41]. So rather than the left RNN (in the diagram above) spitting out output, they could just spit out inputs into a second RNN. If you are thinking at this point \u201cI think I understand this but I am not quite sure\u201d that means you don\u2019t understand this. The only way you know that you actually understand it is to go and write this from scratch in PyTorch or Numpy. If you can\u2019t do that, then you know you don\u2019t understand it and you can go back and re-watch lesson 6 and check out the notebook and copy some of the ideas until you can. It is really important that you can write that from scratch \u2014 it\u2019s less than a screen of code. So you want to make sure you can create a 2 layer RNN. Below is what it looks like if you unroll it.\n\nTo get to a point that we have (x, y) pairs of sentences, we will start by downloading the dataset [22:39]. Training a translation model takes a long time. Google\u2019s translation model has eight layers of RNN stacked on top of each other. There is no conceptual difference between eight layers and two layers. If you are Google and you have more GPUs or TPUs than you know what to do with, then you are fine doing that. Where else, in our case, it\u2019s pretty likely that the kind of sequence to sequence models we are building are not going to require that level of computation. So to keep things simple [23:22], let\u2019s do a cut-down thing where rather than learning how to translate French into English for any sentence, let\u2019s learn to translate French questions into English questions \u2014 specifically questions that start with what/where/which/when. So here is a regex which looks for things that start with \u201cwh\u201d and end with a question mark.\n\nWe go through the corpus [23:43], open up each of the two files, each line is one parallel text, zip them together, grab the English question and the French question, and check whether they match the regular expressions.\n\nDump that out as a pickle so we don\u2019t have to do it again and so now we have 52,000 sentence pairs and here are some examples:\n\nOne nice thing about this is that what/who/where type questions tend to be fairly short [24:08]. But the idea that we could learn from scratch with no previous understanding of the idea of language let alone of English or French that we could create something that can translate one to the other for any arbitrary question with only 50k sentences sounds like a ludicrously difficult thing to ask this to do. So it would be impressive if we can make any progress what so ever. This is very little data to do a very complex exercise.\n\ncontains the tuples of French and English [24:48]. You can use this handy idiom to split them apart into a list of English questions and a list of French questions.\n\nThen we tokenize the English questions and tokenize the French questions. So remember that just means splitting them up into separate words or word-like things. By default [25:11], the tokenizer that we have here (remember this is a wrapper around the spaCy tokenizer which is a fantastic tokenizer) assumes English. So to ask for French, you just add an extra parameter . The first time you do this, you will get an error saying you don\u2019t have the spaCy French model installed so you can run to grab the French model.\n\nIt is unlikely that any of you are going to have RAM problems here because this is not particularly big corpus but some of the students were trying to train a new language models during the week and were having RAM problems. If you do, it\u2019s worth knowing what these functions ( ) are actually doing. is processing every sentence across multiple processes [25:59]:\n\nThe function above finds out how many CPUs you have, divide it by two (because normally with hyper-threading they don\u2019t actually all work in parallel), then in parallel run this function. So that is going to spit out a whole separate Python processes for every CPU you have. If you have a lot of cores, that is a lot of Python processes \u2014 everyone is going to load all this data in and that can potentially use up all your RAM. So you could replace that with just rather than to use less RAM. Or you could just use less cores. At the moment, we are calling which calls on a list and asks to split it into a number of equal length things according to how many CPUs you have. So you could replace that to split into a smaller list and run it on less things.\n\nHaving tokenized the English and French, you can see how it gets split up [28:04]:\n\nYou can see the tokenization for French is quite different looking because French loves their apostrophes and their hyphens. So if you try to use an English tokenizer for a French sentence, you\u2019re going to get a pretty crappy outcome. You don\u2019t need to know heaps of NLP ideas to use deep learning for NLP, but just some basic stuff like use the right tokenizer for your language is important [28:23]. Some of the students this week in our study group have been trying to build language models for Chinese instance which of course doesn\u2019t really have the concept of a tokenizer in the same way, so we\u2019ve been starting to look at sentence piece which splits things into arbitrary sub-word units and so when Jeremy says tokenize, if you are using a language that doesn\u2019t have spaces in, you should probably be checking out sentence piece or some other similar sub-word unit thing instead. Hopefully in the next week or two, we will be able to report back with some early results of these experiments with Chinese.\n\nSo having tokenized it [29:25], we will save that to disk. Then remember, the next step after we create tokens is to turn them into numbers . To do that, we have two steps \u2014 the first is to get a list of all of the words that appear and then we turn every word into the index. If there are more than 40,000 words that appear, then let\u2019s cut it off there so it doesn\u2019t go too crazy. We insert a few extra tokens for beginning of stream ( ), padding ( ), end of stream ( ), and unknown ( ). So if we try to look up something that wasn\u2019t in the 40,000 most common, then we use a to return 3 which is unknown.\n\nNow we can go ahead and turn every token into an ID by putting it through the string to integer dictionary ( ) we just created and then at the end of that let\u2019s add the number 2 which is the end of stream. The code you see here is the code Jeremy writes when he is iterating and experimenting [30:25]. Because 99% of the code he writes while iterating and experimenting turns out to be totally wrong or stupid or embarrassing and you don\u2019t get to see it. But there is not point refactoring that and making it beautiful when he\u2019s writing it so he wanted you to see all the little shortcuts he has. Rather than having some constant for marker and using that, when he is prototyping he just does the easy stuff. Not so much that he ends up with broken code but he tries to find some middle ground between beautiful code and code that works.\n\nQuestion: Just heard him mention that we divide the number of CPUs by 2 because with hyper-threading, we don\u2019t get a speed-up using all the hyper threaded cores. Is this based on practical experience or is there some underlying reason why we wouldn\u2019t get additional speedup [31:18]? Yes, it\u2019s just practical experience and it\u2019s not all things seemed like this, but I definitely noticed with tokenization \u2014 hyper-threading seemed to slow things down a little bit. Also if I use all the cores, often I want to do something else at the same time (like running some interactive notebook) and I don\u2019t have any spare room to do that.\n\nNow for our English and French, we can grab a list of IDs [32:01]. When we do that, of course, we need to make sure that we also store the vocabulary. There is no point having IDs if we don\u2019t know what a number 5 represents, there is no point having a number 5. So that\u2019s our vocabulary and reverse mapping that we can use to convert more corpuses in the future.\n\nJust to confirm it\u2019s working, we can go through each ID, convert the int to a string, and spit that out \u2014 there we have our sentence back now with an end of stream marker at the end. Our English vocab is 17,000 and our French vocab is 25,000, so that\u2019s not too big and not too complex vocab that we are dealing with.\n\nWe spent a lot of time on the forum during the week discussing how pointless word vectors are and how you should stop getting so excited about them \u2014 and now we are going to use them. Why? All the stuff we\u2019ve been learning about using language models and pre-trained proper models rather than pre-trained linear single layers which is what word vectors are, applies equally well to sequence to sequence. But Jeremy and Sebastian are starting to look at that. There is a whole thing for anybody interested in creating some genuinely new highly publishable results, the entire area of sequence to sequence with pre-trained language models has not been touched yet. Jeremy believes it is going to be just as good as classifications. If you work on this and you get to the point where you have something that is looking exciting and you want help publishing it, Jeremy is very happy to help co-author papers. So feel free to reach out when you have some interesting results.\n\nAt this stage, we do not have any of that, so we are going to use very little fastai [34:14]. All we have is word vectors \u2014 so let\u2019s at least use decent word vectors. Word2vec is very old word word vectors. There are better word vectors now and fast.text is a pretty good source of word vectors. There is hundreds of languages available for them, and your language is likely to be represented.\n\nfasttext Python library is not available in PyPI but here is a handy trick [35:03]. If there is a GitHub repo that has a setup.py and reqirements.txt in it, you can just chuck at the start then stick that in your and it works. Hardly anybody seems to know this and if you go to the fasttext repo, they won\u2019t tell you this \u2014 they\u2019ll say you have to download it and into it and blah but you don\u2019t. You can just run this:\n\nTo use the fastText library, you\u2019ll need to download fasttext word vectors for your language (download the \u2018bin plus text\u2019 ones).\n\nAbove are our English and French models. There are a text version and a binary version. The binary version is faster, so we will use that. The text version is also a bit buggy. We are going to convert it into a standard Python dictionary to make it a bit easier to work with [35:55]. This is just going through each word with a dictionary comprehension and save it as a pickle dictionary:\n\nNow we have our pickle dictionary, we can go ahead and look up a word, for example, a comma [36:07]. That will return a vector. The length of the vector is the dimensionality of this set of word vectors. In this case, we have 300 dimensional English and French word vectors.\n\nFor reasons you will see in a moment, we also want to find out what the mean and standard deviation of our vectors are. So the mean is about zero and standard deviation is about 0.3.\n\nOften corpuses have a pretty long tailed distribution of sequence length and it\u2019s the longest sequences that tend to overwhelm how long things take, how much memory is used, etc. So in this case, we are going to grab 99th to 97th percentile of the English and French and truncate them to that amount. Originally Jeremy was using 90 percentiles (hence the variable name):\n\nWe are nearly there [37:24]. We\u2019ve got our tokenized, numerixalized English and French dataset. We\u2019ve got some word vectors. So now we need to get it ready for PyTorch. PyTorch expects a object and hopefully by now you can say that a Dataset object requires two things \u2014 a length ( )and an indexer ( ). Jeremy started out writing which turned out to be just a generic [37:52].\n\nNow we need to grab our English and French IDs and get a training set and a validation set. One of the things which is pretty disappointing about a lot of code out there on the internet is that they don\u2019t follow some simple best practices. For example, if you go to PyTorch website, they have an example section for sequence to sequence translation. Their example does not have a separate validation set. Jeremy tried training according to their settings and tested it with a validation set and it turned out that it overfit massively. So this is not just a theoretical problem \u2014 the actual PyTorch repo has the actual official sequence to sequence translation example which does not check for overfitting and overfits horribly [39:41]. Also it fails to use mini-batches so it actually fails to utilize any of the efficiency of PyTorch whatsoever. Even if you find code in the official PyTorch repo, don\u2019t assume it\u2019s any good at all. The other thing you\u2019ll notice is that pretty much every other sequence to sequence model Jeremy found in PyTorch anywhere on the internet has clearly copied from that crappy PyTorch repo because all has the same variable names, it has the same problems, it has the same mistakes.\n\nAnother example is that nearly every PyTorch convolutional neural network Jeremy found does not use an adaptive pooling layer [40:27]. So in other words, the final layer is always average pool (7,7). They assume that the previous layer is 7 by 7 and if you use any other size input, you get an exception, and therefore nearly everybody Jeremy has spoken who uses PyTorch thinks that there is a fundamental limitation of CNNs that they are tied to the input size and that has not been true since VGG. So every time Jeremy grabs a new model and stick it in the fastai repo, he has to go and search for \u201cpool\u201d and add \u201cadaptive\u201d to the start and replace the 7 with a 1and now it works on any sized object. So just be careful. It\u2019s still early days and believe it or not, even though most of you have only started in the last year your deep learning journey, you know quite a lot more about a lot of the more important practical aspects than the vast majority of people that have publishing and writing stuff in official repos. So you need to have a little more self-confidence than you might expect when it comes to reading other people\u2019s code. If you find yourself thinking \u201cthat looks odd\u201d, it\u2019s not necessarily you.\n\nIf the repo you are looking at doesn\u2019t have a section on it saying here is the test we did where we got the same results as the paper that\u2019s supposed to be implementing, that almost certainly means they haven\u2019t got the same results of the paper they\u2019re implementing, but probably haven\u2019t even checked [42:13]. If you run it, definitely won\u2019t get those results because it\u2019s hard to get things right the first time \u2014 it takes Jeremy 12 goes. If they haven\u2019t tested it once, it\u2019s almost certainly won\u2019t work.\n\nHere is an easy way to get training and validation sets [42:45]. Grab a bunch of random numbers \u2014 one for each row of your data, and see if they are bigger than 0.1 or not. That gets you a list of booleans. Index into your array with that list of booleans to grab a training set, index into that array with the opposite of that list of booleans to get your validation set.\n\nNow we can create our dataset with our X\u2019s and Y\u2019s (i.e. French and English)[43:12]. If you want to translate instead English to French, switch these two around and you\u2019re done.\n\nNow we need to create DataLoaders [43:22]. We can just grab our data loader and pass in our dataset and batch size. We actually have to transpose the arrays \u2014 we won\u2019t go into the details about why, but we can talk about it during the week if you\u2019re interested but have a think about why we might need to transpose their orientation. Since we\u2019ve already done all the pre-processing, there is no point spawning off multiple workers to do augmentation, etc because there is no work to do. So will save you some time. We have to tell it what our padding index is \u2014 that is pretty important because what\u2019s going to happen is that we\u2019ve got different length sentences and fastai will automatically stick them together and pad the shorter ones so that they are all equal length. Remember a tensor has to be rectangular.\n\nIn the decoder in particular, we want our padding to be at the end, not at the start [44:29]:\n\nSampler [44:54] Finally, since we\u2019ve got sentences of different lengths coming in and they all have to be put together in a mini-batch to be the same size by padding, we would much prefer that the sentences in a mini-batch are of similar sizes already. Otherwise it is going to be as long as the longest sentence and that is going to end up wasting time and memory. Therefore, we are going to use the sampler tricks that we learnt last time which is the validation set, we are going to ask it to sort everything by length first. Then for the training set, we are going to randomize the order of things but to roughly make it so that things of similar length are about in the same spot.\n\nModel Data [45:40] At this point, we can create a model data object \u2014 remember a model data object really does one thing which is it says \u201cI have a training set and a validation set, and an optional test set\u201d and sticks them into a single object. We also has a path so that it has somewhere to store temporary files, models, stuff like that.\n\nWe are not using fastai for very much at all in this example. We used PyTorch compatible Dataset and and DataLoader \u2014 behind the scene it is actually using the fastai version because we need it to do the automatic padding for convenience, so there is a few tweaks in fastai version that are a bit faster and a bit more convenient. We are also using fastai\u2019s Samplers, but there is not too much going on here.\n\nNone of this is going to be new [47:41]. That is all going to be using very direct simple techniques that we\u2019ve already learned.\n\nLet\u2019s start with the encoder [48:15]. In terms of the variable naming here, there is identical attributes for encoder and decoder. The encoder version has the decoder version has .\n\nWe need to create an embedding layer because remember \u2014 what we are being passed is the index of the words into a vocabulary. And we want to grab their fast.text embedding. Then over time, we might want to also fine tune to train that embedding end-to-end.\n\n[49:37]: It is important that you know now how to set the rows and columns for your embedding so the number of rows has to be equal to your vocabulary size \u2014 so each vocabulary has a word vector. The size of the embedding is determined by fast.text and fast.text embeddings are size 300. So we have to use size 300 as well otherwise we can\u2019t start out by using their embeddings.\n\nwill initially going to give us a random set of embeddings [50:12]. So we will go through each one of these and if we find it in fast.text, we will replace it with the fast.text embedding. Again, something you should already know is that ( ):\n\nNow that we\u2019ve got our weight tensor, we can just go through our vocabulary and we can look up the word in our pre-trained vectors and if we find it, we will replace the random weights with that pre-trained vector [52:35]. The random weights have a standard deviation of 1. Our pre-trained vectors has a standard deviation of about 0.3. So again, this is the kind of hacky thing Jeremy does when he is prototyping stuff, he just multiplied it by 3. By the time you see the video of this, we may able to put all this sequence to sequence stuff into the fastai library, you won\u2019t find horrible hacks like that in there (sure hope). But hack away when you are prototyping. Some things won\u2019t be in fast.text in which case, we\u2019ll just keep track of it [53:22]. The print statement is there so that we can see what\u2019s going on (i.e. why are we missing stuff?). Remember we had about 30,000 so we are not missing too many.\n\nJeremy has started doing some stuff around incorporating large vocabulary handling into fastai \u2014 it\u2019s not finished yet but hopefully by the time we get here, this kind of stuff will be possible [56:50].\n\nThe key thing to know is that encoder takes our inputs and spits out a hidden vector that hopefully will learn to contain all of the information about what that sentence says and how it sets it [58:49]. If it can\u2019t do that, we can\u2019t feed it into a decoder and hope it to spit our our sentence in a different language. So that\u2019s what we want it to learn to do. We are not going to do anything special to make it learn to do that \u2014 we are just going to do the three things (data, architecture, loss function) and cross our fingers.\n\nDecoder [59:58]: How do we now do the new bit? The basic idea of the new bit is the same. We are going to do exactly the same thing, but we are going to write our own for loop. The for loop is going to do exactly what the for loop inside PyTorch does for encoder, but we are going to do it manually. How big is the for loop? It\u2019s an output sequence length ( ) which was something passed to the constructor which is equal to the length of the largest English sentence. Since we are translating into English, so it can\u2019t possibly be longer than that at least in this corpus. If we then used it on some different corpus that was longer, this is going to fail \u2014 you could always pass in a different parameter, of course. So the basic idea is the same [1:01:06].\n\nNormally, a recurrent neural network works on a whole sequence at a time, but we have a for loop to go through each part of the sequence separately [1:01:37]. Wo we have to add a leading unit axis to the start ( ) to basicaly say this is a sequence of length one. We are not really taking advantage of the recurrent net much at all \u2014 we could easily re-write this with a linear layer.\n\nOne thing to be aware of is [1:02:34]: What is the input to the embedding? The answer is it is the previous word that we translated. The basic idea is if you are trying to translate the 4th word of the new sentence but you don\u2019t know what the third word you just said was, that is going to be really hard. So we are going to feed that in at each time step. What was the previous word at the start? There was none. Specifically, we are going to start out with a beginning of stream token ( ) which is zero.\n\n[1:05:24]: it is a tensor whose length is equal to the number of words in our English vocabulary and it contains the probability for every one of those words that it is that word.\n\n: It looks in its tensor to find out which word has the highest probability. in PyTorch returns two things: the first thing is what is that max probability and the second is what is the index into the array of that max probability. So we want that second item which is the word index with the largest thing.\n\n: It contains the word index into the vocabulary of the word. If it\u2019s one (i.e. padding), that means we are done \u2014 we reached the end because we finished with a bunch of padding. If it\u2019s not one, let\u2019s go back and continue.\n\nEach time, we appended our outputs (not the word but the probabilities) to the list [1:06:48] which we stack up into a tensor and we can now go ahead and feed that to a loss function.\n\nThe loss function is categorical cross entropy loss. We have a list of probabilities for each of our classes where the classes are all the words in our English vocab and we have a target which is the correct class (i.e. which is the correct word at this location). There are two tweaks which is why we need to write our own loss function but you can see basically it is going to be cross entropy loss.\n\n2. expects a rank 2 tensor, but we have sequence length by batch size, so let\u2019s just flatten out. That is what does.\n\nThe difference between and : will not put to in the GPU if you do not have one. You can also set to to force it to not use GPU that can be handy for debugging.\n\nWe then need something that tells it how to handle learning rate groups so there is a thing called that you can pass it to which treats the whole thing as a single learning rate group [1:09:40]. So this is the easiest way to turn a PyTorch module into a fastai model.\n\nWe could just call Learner to turn that into a learner, but if we call RNN_Learner, it does add in and that can be handy sometimes. In this case, we really could have said but also works.\n\nRemember the model attribute of a learner is a standard PyTorch model so we can pass some which we can grab out of our validation set or you could or whatever you like to get some predictions. Then we convert those predictions into words by going to grab the index of the highest probability words to get some predictions. Then we can go through a few examples and print out the French, the correct English, and the predicted English for things that are not padding.\n\nAmazingly enough, this kind of simplest possible written largely from scratch PyTorch module on only fifty thousand sentences is sometimes capable, on validation set, of giving you exactly the right answer. Sometimes the right answer is in slightly different wording, and sometimes sentences that really aren\u2019t grammatically sensible or even have too many question marks. So we are well on the right track. We think you would agree even the simplest possible seq-to-seq trained for a very small number of epochs without any pre-training other than the use of word embeddings is surprisingly good. We are going to improve this later but the message here is even sequence to sequence models you think is simpler than they could possibly work even with less data than you think you could learn from can be surprisingly effective and in certain situations this may be enough for your needs.\n\nQuestion: Would it help to normalize punctuation (e.g. vs. )? [1:13:10] The answer to this particular case is probably yes \u2014 the difference between curly quotes and straight quotes is really semantic. You do have to be very careful though because it may turn out that people using beautiful curly quotes like using more formal language and they are writing in a different way. So if you are going to do some kind of pre-processing like punctuation normalization, you should definitely check your results with and without because nearly always that kind of pre-processing make things worse even when you\u2019re sure it won\u2019t.\n\nQuestion: What might be some ways of regularizing these seq2seq models besides dropout and weight decay? [1:14:17] Let me think about that during the week. AWD-LSTM which we have been relying a lot has dropouts of many different kinds and there is also a kind of a regularization based on activations and on changes. Jeremy has not seen anybody put anything like that amount of work into regularizing sequence to sequence model and there is a huge opportunity for somebody to do like the AWD-LSTM of seq-to-seq which might be as simple as stealing all the ideas from AWD-LSTM and using them directly in seq-to-seq that would be pretty easy to try. There\u2019s been an interesting paper that Stephen Merity added in the last couple weeks where he used an idea which take all of these different AWD-LSTM hyper parameters and train a bunch of different models and then use a random forest to find out the feature importance \u2014 which ones actually matter the most and then figure out how to set them. You could totally use this approach to figure out for sequence to sequence regularization approaches which one is the best and optimize them and that would be amazing. But at the moment, we don\u2019t know if there are additional ideas to sequence to sequence regularization beyond what is in that paper for regular language model.\n\nFor classification, the approach to bi-directional Jeremy suggested to use is take all of your token sequences, spin them around, train a new language model, and train a new classifier. He also mentioned that wikitext pre-trained model if you replace with in the name, you will get the pre-trained backward model he created for you. Get a set of predictions and then average the predictions just like a normal ensemble. That is how we do bi-dir for that kind of classification. There may be ways to do it end-to-end, but Jeremy hasn\u2019t quite figured them out yet and they are not in fastai yet. So if you figure it out, that\u2019s an interesting line of research. But because we are not doing massive documents where we have to chunk it into separate bits and then pool over them, we can do bi-dir very easily in this case. It is literally as simple as adding to our encoder. People tend not to do bi-directional for the decoder partly because it is kind of considered cheating but maybe it can work in some situations although it might need to be more of an ensemble approach in the decoder because it\u2019s a bit less obvious. But encoder it\u2019s very simple \u2014 and we now have a second RNN that is going the opposite direction. The second RNN is visiting each token in the opposing order so when we get to the final hidden state, it is the first (i.e. left most) token . But the hidden state is the same size, so the final result is that we end up with a tensor with an extra axis of length 2. Depending on what library you use, often that will be then combined with the number of layers, so if you have 2 layers and bi-directional \u2014 that tensor dimension is now length 4. With PyTorch it depends which bit of the process you are looking at as to whether you get a separate result for each layer and/or for each bidirectional bit. You have to look up the documentation and it will tell you input\u2019s output\u2019s tensor sizes appropriate for the number of layers and whether you have .\n\nIn this particular case, you will see all the changes that had to be made [1:19:38]. For example ,when we added , the layer now needs number of hidden times 2 (i.e. ) to reflect the fact that we have that second direction in our hidden state. Also in it\u2019s now .\n\nQuestion: Why is making the decoder bi-directional considered cheating? [1:20:13] It\u2019s not just cheating but we have this loop going on so it is not as simple as having two tensors. Then how do you turn those two separate loops into a final result? After talking about it during the break, Jeremy has gone from \u201ceverybody knows it doesn\u2019t work\u201d to \u201cmaybe it could work\u201d, but it requires more thought. It is quite possible during the week, he\u2019ll realize it\u2019s a dumb idea, but we\u2019ll think about it.\n\nQuestion: Why do you need to set a range to the loop? [1:20:58] Because when we start training, everything is random so will probably never be true. Later on, it will pretty much always break out eventually but basically we are going to go forever. It\u2019s really important to remember when you are designing an architecture that when you start, the model knows nothing about anything. So you want to make sure if it\u2019s going to do something at least it\u2019s vaguely sensible.\n\nWe got 3.58 cross entropy loss with single direction [1:21:46]. With bi-direction, we got down to 3.51, so that improved a little. It shouldn\u2019t really slow things down too much. Bi-directional does mean there is a little bit more sequential processing have to happen, but it is generally a good win. In the Google translation model, of the 8 layers, only the first layer is bi-directional because it allows it to do more in parallel so if you create really deep models you may need to think about which ones are bi-directional otherwise we have performance issues.\n\nNow let\u2019s talk about teacher forcing. When a model starts learning, it knows nothing about nothing. So when the model starts learning, it is not going to spit out \u201cEr\u201d at the first step, it is going to spit out some random meaningless word because it doesn\u2019t know anything about German or about English or about the idea of language. And it is going to feed it to the next process as an input and be totally unhelpful. That means, early learning is going to be very difficult because it is feeding in an input that is stupid into a model that knows nothing and somehow it\u2019s going to get better. So it is not asking too much eventually it gets there, but it\u2019s definitely not as helpful as we can be. So what if instead of feeing in the thing I predicted just now, what if we instead we feed in the actual correct word was meant to be. We can\u2019t do that at inference time because by definition we don\u2019t know the correct word - it has to translate it. We can\u2019t require the correct translation in order to do translation.\n\nSo the way it\u2019s set up is we have this thing called which is probability of forcing [1:24:01]. If some random number is less than that probability then we are going to replace our decoder input with the actual correct thing. If we have already gone too far and if it is already longer than the target sequence, we are just going to stop because obviously we can\u2019t give it the correct thing. So you can see how beautiful PyTorch is for this. The key reasons that we switched to PyTorch at this exact point in last year\u2019s class was because Jeremy tried to implement teacher forcing in Keras and TensorFlow and went even more insane than he started. It was weeks of getting nowhere then he saw on Twitter Andrej Karpathy said something about this thing called PyTorch that just came out and it\u2019s really cool. He tried it that day, by the next day, he had teacher forcing. All this stuff of trying to debug things was suddenly so much easier and and this kind of dynamic thing is so much easier. So this is a great example of \u201chey, I get to use random numbers and if statements\u201d.\n\nHere is the basic idea [1:25:29]. At the start of training, let\u2019s set really high so that nearly always it gets the actual correct previous word and so it has a useful input. Then as we trained a bit more, let\u2019s decrease so that by the end is zero and it has to learn properly which is fine because it is now actually feeding in sensible inputs most of the time anyway.\n\n: \u201cprobability of forcing\u201d. High in the beginning zero by the end.\n\nLet\u2019s now write something such that in the training loop, it gradually decreases [1:26:01] How do we do that? One approach would be to write our own training loop but let\u2019s not do that because we already have a training loop that has progress bars, uses exponential weighted averages to smooth out the losses, keeps track of metrics, and does bunch of things. They also keep track of calling the reset for RNN at the start of the epoch to make sure the hidden state is set to zeros. What we\u2019ve tended to find is that as we start to write some new thing and we need to replace some part of the code, we then add some little hook so that we can all use that hook to make things easier. In this particular case, there is a hook that Jeremy has ended up using all the time which is the hook called the stepper. If you look at the source code, model.py is where our fit function lives which is the lowest level thing that does not require learner or anything much at all \u2014 just requires a standard PyTorch model and a model data object. You just need to know how many epochs, a standard PyTorch optimizer, and a standard PyTorch loss function. We hardly ever used in the class, we normally call , but calls this.\n\nWe have looked at the source code sometime [1:27:49]. We\u2019ve seen how it loos through each epoch and that loops through each thing in our batch and calls . is the thing that is responsible for:\n\nSo by default, uses a particular class called which basically calls the model, zeros the gradient, calls the loss function, calls , does gradient clipping if necessary, then calls the optimizer. They are basic steps that back when we looked at \u201cPyTorch from scratch\u201d we had to do. The nice thing is, we can replace that with something else rather than replacing the training loop. If you inherit from , then write your own version of , you can just copy and paste the contents of step and add whatever you like. Or if it\u2019s something that you\u2019re going to do before or afterwards, you could even call . In this case, Jeremy rather suspects he has been unnecessarily complicated [1:29:12] \u2014 he probably could have done something like:\n\nBut as he said, when he is prototyping, he doesn\u2019t think carefully about how to minimize his code \u2014 he copied and pasted the contents of the and he added a single line to the top which was to replace in the module with something that gradually decreased linearly for the first 10 epochs, and after 10 epochs, it is zero. So total hack but good enough to try it out. The nice thing is that everything else is the same except for the addition of these three lines:\n\nAnd the only thing we need to do differently is when we call , we pass in our customized stepper class.\n\nAnd now our loss is down to 3.49. We needed to make sure at least do 10 epochs because before that, it was cheating by using the teacher forcing.\n\nThis next trick is a bigger and pretty cool trick. It\u2019s called \u201cattention.\u201d The basic idea of attention is this \u2014 expecting the entirety of the sentence to be summarized into this single hidden vector is asking a lot. It has to know what was said, how it was said, and everything necessary to create the sentence in German. The idea of attention is basically maybe we are asking too much. Particularly because we could use this form of model (below) where we output every step of the loop to not just have a hidden state at the end but to have a hidden state after every single word. Why not try and use that information? It\u2019s already there but so far we\u2019ve just been throwing it away. Not only that but bi-directional, we got two vectors of state every step that we can use. How can we do this?\n\nLet\u2019s say we are translating a word \u201cliebte\u201d right now [1:32:34]. Which of previous 5 pieces of state do we want? We clearly want \u201clove\u201d because it is the word. How about \u201czu\u201d? We probably need \u201ceat\u201d and \u201cto\u201d and loved\u201d to make sure we have gotten the tense right and know that I actually need this part of the verb and so forth. So depending on which bit we are translating, we would need one or more bits of these various hidden states. In fact, we probably want some weighting of them. In other words, for these five pieces of hidden state, we want a weighted average [1:33:47]. We want it weighted by something that can figure out which bits of the sentence is the most important right now. How do we figure out something like which bits of the sentence are important right now? We create a neural net and we train the neural net to figure it out. When do we train that neural net? End to end. So let\u2019s now train two neural nets [1:34:18]. Well, we\u2019ve already got a bunch \u2014 RNN encoder, RNN decoder, a couple of linear layers, what the heck, let\u2019s add another neural net into the mix. This neural net is going to spit out a weight for every one of these states and we will take the weighted average at every step, and it\u2019s just another set of parameters that we learn all at the same time. So that is called \u201cattention\u201d.\n\nThe idea is that once that attention has been learned, each word is going to take a weighted average as you can see in this terrific demo from Chris Olah and Shan Carter [1:34:50]. Check out this distill.pub article \u2014 these things are interactive diagrams that shows you how the attention works and what the actual attention looks like in a trained translation model.\n\nWith attention, most of the code is identical. The one major difference is this line: . We are going to take a weighted average and the way we are going to do the weighted average is we create a little neural net which we are going to see here:\n\nWe use softmax because the nice thing about softmax is that we want to ensure all of the weights that we are using add up to 1 and we also expect that one of those weights should probably be higher than the other ones [1:36:38]. Softmax gives us the guarantee that they add up to 1 and because it has in it, it tends to encourage one of the weights to be higher than the other ones.\n\nLet\u2019s see how this works [1:37:09]. We are going to take the last layer\u2019s hidden state and we are going to stick it into a linear layer. Then we are going to stick it into a nonlinear activation, then we are going to do a matrix multiply. So if you think about it \u2014 a linear layer, nonlinear activation, matrix multiple \u2014 it\u2019s a neural net. It is a neural net with one hidden layer. Stick it into a softmax and then we can use that to weight our encoder outputs. Now rather than just taking the last encoder output, we have the whole tensor of all of the encoder outputs which we just weight by this neural net we created.\n\nIn Python, is the matrix product, the element-wise product\n\nQuestion: Could you please explain attention again? [1:39:46] Sure! Let\u2019s go back and look at our original encoder.\n\nThe RNN spits out two things: it spits out a list of the state after every time step ( ), and it also tells you the state at the last time step ( )and we used the state at the last time step to create the input state for our decoder which is one vector below:\n\nBut we know that it\u2019s creating a vector at every time steps (orange arrows), so wouldn\u2019t it be nice to use them all? But wouldn\u2019t it be nice to use the one or ones that\u2019s most relevant to translating the word we are translating now? So wouldn\u2019t it be nice to be able to take a weighted average of the hidden state at each time step weighted by whatever is the appropriate weight right now. For example, \u201cliebte\u201d would definitely be time step #2 is what it\u2019s all about because that is the word I\u2019m translating. So how do we get a list of weights that is suitable fore the word we are training right now? The answer is by training a neural net to figure out the list of weights. So anytime we want to figure out how to train a little neural net that does any task, the easiest way, normally always to do that is to include it in your module and train it in line with everything else. The minimal possible neural net is something that contains two layers and one nonlinear activation function, so is one linear layer.\n\nIn fact, instead of a linear layer, we can even just grab a random matrix if we do not care about bias [1:42:18]. is a random tensor wrapped up in a .\n\n: Remember, a is identical to PyTorch but it just tells PyTorch \u201cI want you to learn the weights for this please.\u201d [1:42:35]\n\nSo when we start out our decoder, let\u2019s take the current hidden state of the decoder, put that into a linear layer ( ) because what is the information we use to decide what words we should focus on next \u2014 the only information we have to go on is what the decoder\u2019s hidden state is now. So let\u2019s grab that:\n\nThat\u2019s it \u2014 a little neural net. It doesn\u2019t do anything. It\u2019s just a neural net and no neural nets do anything they are just linear layers with nonlinear activations with random weights. But it starts to do something if we give it a job to do. In this case, the job we give it to do is to say don\u2019t just take the final state but now let\u2019s use all of the encoder states and let\u2019s take all of them and multiply them by the output of that little neural net. So given that the things in this little neural net are learnable weights, hopefully it\u2019s going to learn to weight those encoder hidden states by something useful. That is all neural net ever does is we give it some random weights to start with and a job to do, and hope that it learns to do the job. It turns out, it does.\n\nEverything else in here is identical to what it was before. We have teacher forcing, it\u2019s not bi-directional, so we can see how this goes.\n\nTeacher forcing had 3.49 and now with nearly exactly the same thing but we\u2019ve got this little minimal neural net figuring out what weightings to give our inputs and we are down to 3.37. Remember, these loss are logs, so is quite a significant change.\n\nNot bad. It\u2019s still not perfect but quite a few of them are correct and again considering that we are asking it to learn about the very idea of language for two different languages and how to translate them between the two, and grammar, and vocabulary, and we only have 50,000 sentences and a lot of the words only appear once, I would say this is actually pretty amazing.\n\nQuestion: Why do we use tanh instead of ReLU for the attention mini net? [1:46:23] I don\u2019t quite remember \u2014 it\u2019s been a while since I looked at it. You should totally try using value and see how it goes. Obviously tanh the key difference is that it can go in each direction and it\u2019s limited both at the top and the bottom. I know very often for the gates inside RNNs, LSTMs, and GRUs, tanh often works out better but it\u2019s been about a year since I actually looked at that specific question so I\u2019ll look at it during the week. The short answer is you should try a different activation function and see if you can get a better result.\n\nWhat we can do also is we can grab the attentions out of the model by adding return attention parameter to function. You can put anything you\u2019d like in function argument. So we added a return attention parameter, false by default because obviously the training loop it doesn\u2019t know anything about it but then we just had something here says if return attention, then stick the attentions on as well ( ). The attentions is simply the value just chuck it on a list ( ). We can now call the model with return attention equals true and get back theprobabilities and the attentions [1:47:53]:\n\nWe can now draw pictures, at each time step, of the attention.\n\nWhen you are Chris Olah and Shan Carter, you make things that looks like \u261fwhen you are Jeremy Howard, the exact same information looks like \u261d\ufe0e[1:48:24]. You can see at each different time step, we have a different attention.\n\nIt\u2019s very important when you try to build something like this, you don\u2019t really know if it\u2019s not working right because if it\u2019s not working (as per usual Jeremy\u2019s first 12 attempts of this were broken) and they were broken in a sense that it wasn\u2019t really learning anything useful. Therefore, it was giving equal attention to everything and it wasn\u2019t worse \u2014 it just wasn\u2019t much better. Until you actually find ways to visualize the thing in a way that you know what it ought to look like ahead of time, you don\u2019t really know if it\u2019s working [1:49:16]. So it\u2019s really important that you try to find ways to check your intermediate steps in your outputs.\n\nQuestion: What is the loss function of the attentional neural network? [1:49:31] No, there is no loss function for the attentional neural network. It is trained end-to-end. It is just sitting inside our decoder loop. The loss function for the decoder loop is the same loss function because the result contains exactly same thing as before \u2014 the probabilities of the words. How come the mini neural net learning something? Because in order to make the outputs better and better, it would be great if it made the weights of weighted-average better and better. So part of creating our output is to please do a good job of finding a good set of weights and if it doesn\u2019t do a good job of finding good set of weights, then the loss function won\u2019t improve from that bit. So end-to-end learning means you throw in everything you can into one loss function and the gradients of all the different parameters point in a direction that says \u201chey, you know if you had put more weight over there, it would have been better.\u201d And thanks to the magic of the chain rule, it knows to put more weight over there, change the parameter in the matrix multiply a little, etc. That is the magic of end-to-end learning. It is a very understandable question but you have to realize there is nothing particular about this code that says this particular bits are separate mini neural network anymore than the GRU is a separate little neural network, or a linear layer is a separate little function. It\u2019s all ends up pushed into one output which is a bunch of probabilities which ends up in one loss function that returns a single number that says this either was or wasn\u2019t a good translation. So thanks to the magic of the chain rule, we then back propagate little updates to all the parameters to make them a little bit better. This is a big, weird, counterintuitive idea and it\u2019s totally okay if it\u2019s a bit mind-bending. It is the bit where even back to lesson 1 \u201chow did we make it find dogs vs. cats?\u201d \u2014 we didn\u2019t. All we did was we said \u201cthis is our data, this is our architecture, this is our loss function. Please back propagate into the weights to make them better and after you\u2019ve made them better a while, it will start finding cats from dogs.\u201d In this case (i.e. translation), we haven\u2019t used somebody else\u2019s convolutional network architecture. We said \u201chere is a custom architecture which we hope is going to be particularly good at this problem.\u201d Even without this custom architecture, it was still okay. But we made it in a way that made more sense or we think it ought to do worked even better. But at no point, did we do anything different other than say \u201chere is a data, here is an architecture, here is a loss function \u2014 go and find the parameters please\u201d And it did it because that\u2019s what neural nets do.\n\nSo that is sequence-to-sequence learning [1:53:19].\n\nWe are going to do something bringing together for the first time our two little worlds we focused on \u2014 text and images [1:55:49]. This idea came up in a paper by an extraordinary deep learning practitioner and researcher named Andrea Frome. Andrea was at Google at the time and her crazy idea was words can have a distributed representation, a space, which particularly at that time was just word vectors. And images can be represented in a space. In the end, if we have a fully connected layer, they ended up as a vector representation. Could we merge the two? Could we somehow encourage the vector space that the images end up with be the same vector space that the words are in? And if we could do that, what would that mean? What could we do with that? So what could we do with that covers things like well, what if I\u2019m wrong what if I\u2019m predicting that this image is a beagle and I predict jumbo jet and Yannet\u2019s model predicts corgi. The normal loss function says that Yannet\u2019s and Jeremy\u2019s models are equally good (i.e. they are both wrong). But what if we could somehow say though you know what corgi is closer to beagle than it is to jumbo jets. So Yannet\u2019s model is better than Jeremy\u2019s. We should be able to do that because in word vector space, beagle and corgi are pretty close together but jumbo jet not so much. So it would give us a nice situation where hopefully our inferences would be wrong in saner ways if they are wrong. It would also allow us to search for things that are not in ImageNet Synset ID (i.e. a category in ImageNet). Why did we have to train a whole new model to find dog vs. cats when we already have something that found corgis and tabbies. Why can\u2019t we just say find me dogs? If we had trained it in word vector space, we totally could because they are word vector, we can find things with the right image vector and so forth. We will look at some cool things we can do with it in a moment but first of all let\u2019s train a model where this model is not learning a category (one hot encoded ID) where every category is equally far from every other category, let\u2019s instead train a model where we\u2019re finding a dependent variable which is a word vector. so What word vector? Obviously the word vector for the word you want. So if it\u2019s corgi, let\u2019s train it to create a word vector that\u2019s the corgi word vector, and if it\u2019s a jumbo jet, let\u2019s train it with a dependent variable that says this is the word vector for a jumbo jet.\n\nIt is shockingly easy [1:59:17]. Let\u2019s grab the fast text word vectors again, load them in (we only need English this time).\n\nSo for example, \u201cjeremy\u201d and \u201cJeremy\u201d have a correlation of .6.\n\nJeremy doesn\u2019t like bananas at all, and \u201cbanana\u201d and \u201cJeremy\u201d .14. So words that you would expect to be correlated are correlated and words that should be as far away from each other as possible, unfortunately, they are still slightly correlated but not so much [1:59:41].\n\nLet\u2019s now grab all of the ImageNet classes because we actually want to know which one is corgi and which one is jumbo jet.\n\nWe have a list of all of those up on files.fast.ai that we can grab them.\n\nLet\u2019s also grab a list of all of the nouns in English which Jeremy made available here:\n\nSo we have the names of each of the thousand ImageNet classes and all of the nouns in English according to WordNet which is a popular thing for representing what words are and are not. We can now load that list of ImageNet classes, turn that into a dictionary, so contains the class IDs for the 1000 images that are in the competition dataset.\n\nHere is an example. A \u201ctench\u201d apparently is a kind of fish.\n\nLet\u2019s do the same thing for all those WordNet nouns [2:01:11]. It turns out that ImageNet is using WordNet class names so that makes it nice and easy to map between the two.\n\nSo these are our two worlds \u2014 we have the ImageNet thousand and we have the 82,000 which are in WordNet.\n\nSo we want to map the two together which is as simple as creating a couple of dictionaries to map them based on the Synset ID or the WordNet ID.\n\nWhat we need to do now is grab the 82,000 nouns in WordNet and try and look them up in fast text. We\u2019ve managed to look up 49,469 of them in fast text. We now have a dictionary that goes from synset ID which is what WordNet calls them to word vectors. We also have the same thing specifically for the 1k ImageNet classes.\n\nNow we grab all of the ImageNet which you can download from Kaggle now [2:02:54]. If you look at the Kaggle ImageNet localization competition, that contains the entirety of the ImageNet classifications as well.\n\nIt has a validation set of 28,650 items in it. For every image in ImageNet, we can grab its fast text word vector using the synset to word vector ( ) and we can stick that into the image vectors array ( ), stack that all up into a single matrix and save that away.\n\nNow what we have is something for every ImageNet image, we also have the fast text word vector that it is associated with [2:03:43] by looking up the synset ID \u2192 WordNet \u2192 Fast text \u2192 word vector.\n\nHere is a cool trick [2:04:06]. We can now create a model data object which specifically is an image classifier data object and we have this thing called I\u2019m not sure if we\u2019ve used it before but we can pass it a list of file names (all of the file names in ImageNet) and an array of our dependent variables (all of the fast text word vectors). We can then pass in the validation indexes which in this case is just all of the last IDs \u2014 we need to make sure that they are the same as ImageNet uses otherwise we will be cheating. Then we pass in which means this puts a lie again to this image classifier data is now an image regressive data so continuous equals True means don\u2019t one hot encode my outputs but treat them just as continuous values. So now we have a model data object that contains all of our file names and for every file name a continuous array representing the word vector for that. So we have data, now we need an architecture and the loss function.\n\nLet\u2019s create an architecture [2:05:26]. We\u2019ll revise this next week, but we can use the tricks we\u2019ve learnt so far and it\u2019s actually incredibly simple. Fastai has a which is what gets called when you say and you specify:\n\nSo this is now a convolutional neural network that does not have any softmax or anything like that because it\u2019s regression it\u2019s just a linear layer at the end and that\u2019s our model [2:06:55]. We can create a ConvLearner from that model and give it an optimization function. So now all we need is a loss function.\n\nLoss Function [2:07:38]: Default loss function for regression is L1 loss (the absolute differences) \u2014 that is not bad. But unfortunately in really high dimensional spaces (anybody who has studied a bit of machine learning probably knows this) everything is on the outside (in this case, it\u2019s 300 dimensional). When everything is on the outside, distance is not meaningless but a little bit awkward. Things tend to be close together or far away, it doesn\u2019t really mean much in these really high dimensional spaces where everything is on the edge. What does mean something, though, is that if one thing is on the edge over here, and one thing is on the edge over there, we can form an angle between those vectors and the angle is meaningful. That is why we use cosine similarity when we are looking for how close or far apart things are in high dimensional spaces. If you haven\u2019t seen cosine similarity before, it is basically the same as Euclidean distance but it\u2019s normalized to be a unit norm (i.e. divided by the length). So we don\u2019t care about the length of the vector, we only care about its angle. There is a bunch of stuff that you could easily learn in a couple of hours but if you haven\u2019t seen it before, it\u2019s a bit mysterious. For now, just know that loss functions and high dimensional spaces where you are trying to find similarity, you care about angle and you don\u2019t care about distance [2:09:13]. If you didn\u2019t use the following custom loss function, it would still work but it\u2019s a little bit less good. Now we have data, architecture, and loss function, therefore, we are done. We can go ahead and fit.\n\nWe are training on all of ImageNet that is going to take a long time. So is your friend. Remember ? That is the thing we\u2019ve learnt ages ago that caches the output of the final convolutional layer and just trains the fully connected bit. Even with , it takes about 3 minutes to train an epoch on all of ImageNet. So this is about an hour worth of training, but it\u2019s pretty cool that with fastai, we can train a new custom head on all of ImageNet for 40 epochs in an hour or so.\n\nAt the end of all that, we can now say let\u2019s grab the 1000 ImageNet classes, let\u2019s predict on our whole validation set, and take a look at a few pictures [2:10:26].\n\nBecause validation set is ordered, tall the stuff of the same type are in the same place.\n\nNearest neighbor search [2:10:56]: What we can now do is we can now use nearest neighbors search. So nearest neighbors search means here is one300 dimensional vector and here is a whole a lot of other 300 dimensional vectors, which things is it closest to? Normally that takes a very long time because you have to look through every 300 dimensional vector, calculate its distance, and find out how far away it is. But there is an amazing almost unknown library called NMSLib that does that incredibly fast. Some of you may have tried other nearest neighbor\u2019s libraries, I guarantee this is faster than what you are using \u2014 I can tell you that because it\u2019s been bench marked by people who do this stuff for a living. This is by far the fastest on every possible dimension. We want to create an index on angular distance, and we need to do it on all of our ImageNet word vectors. Adding a whole batch, create the index, and now we can query a bunch of vectors all at once, get the 10 nearest neighbors. The library uses multi-threading and is absolutely fantastic. You can install from pip ( ) and it just works.\n\nIt tells you how far away they are and their indexes [2:12:13].\n\nSo now we can go through and print out the top 3 so it turns out that bird actually is a limpkin. Interestingly the fourth one does not say it\u2019s a limpkin and Jeremy looked it up. He doesn\u2019t know much about birds but everything else is brown with white spots, but the 4th one isn\u2019t. So we don\u2019t know if that is actually a limpkin or if it is mislabeled but sure as heck it doesn\u2019t look like the other birds.\n\nThis is not a particularly hard thing to do because there is only a thousand ImageNet classes and it is not doing anything new. But what if we now bring in the entirety of WordNet and we now say which of those 45 thousand things is it closest to?\n\nExactly the same result. It is now searching all of the WordNet.\n\nNow let\u2019s do something a bit different \u2014 which is to take all of our predictions ( ) so basically take our whole validation set of images and create a KNN index of the image representations because remember, it is predicting things that are meant to be word vectors. Now let\u2019s grab the fast text vector for \u201cboat\u201d and boat is not an ImageNet concept \u2014 yet we can now find all of the images in our predicted word vectors (i.e. our validation set) that are closest to the word boat and it works even though it is not something that was ever trained on.\n\nWhat if we now take engine\u2019s vector and boat\u2019s vector and take their average and what if we now look in our nearest neighbors for that [2:14:04]?\n\nThese are boats with engines. I mean, yes, the middle one is actually a boat with an engine \u2014 it just happens to have wings on as well. By the way, sail is not an ImageNet thing , neither is boat. Here is the average of two things that are not ImageNet things and yet with one exception, it\u2019s found us two sailboats.\n\nOkay, let\u2019s do something else crazy. Let\u2019s open up an image in the validation set. Let\u2019s call on that image to get its word vector like thing, and let\u2019s do a nearest neighbor search on all the other images.\n\nAnd here are all the other images of whatever that is. So you can see, this is crazy \u2014 we\u2019ve trained a thing on all of ImageNet in an hour, using a custom head that required basically like two lines fo code, and these things run in 300 milliseconds to do these searches.\n\nJeremy taught this basic idea last year as well, but it was in Keras, and it was pages and pages of code, and everything took a long time and complicated. And back then, Jeremy said he can\u2019t begin to think all of the stuff you could do with this. He doesn\u2019t think anybody has really thought deeply about this yet, but he thinks it\u2019s fascinating. So go back and read the DeVICE paper because Andrea had a whole bunch of other thoughts and now that it is so easy to do, hopefully people will dig into this now. Jeremy thinks it\u2019s crazy and amazing.\n\nAlright, see you next week!"
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b?source=user_profile---------3----------------",
        "title": "Deep Learning 2: Part 2 Lesson 9 \u2013 Hiromi Suenaga \u2013",
        "text": "How to jump around fastai source How to view model inputs from a DataLoader Awkward rough edges of fastai:\n\nA classifier is anything with dependent variable is categorical or binomial. As opposed to regression which is anything with dependent variable is continuous. Naming is a little confusing but will be sorted out in future. Here, is because our dependent variable is the coordinates of bounding box \u2014 hence this is actually a regressor data. Normally, we use these shortcuts Jeremy created for us, but they are simply lists of random augmentations. But you can easily create your own (most if not all of them start with \u201cRandom\u201d). As you can see, the image gets rotated and lighting varies, but bounding box is not moving and is in a wrong spot [6:17]. This is the problem with data augmentations when your dependent variable is pixel values or in some way connected to the independent variable \u2014 they need to be augmented together. As you can see in the bounding box coordinates , our image is 224 by 224 \u2014 so it is neither scaled nor cropped. The dependent variable needs to go through all the geometric transformation as the independent variables. To do this [7:10], every transformation has an optional parameter: indicates that the y value represents coordinate. This needs to be added to all the augmentations as well as which is responsible for cropping, zooming, resizing, padding, etc. Now, the bounding box moves with the image and is in the right spot. You may notice that sometimes it looks odd like the middle on in the bottom row. This is the constraint of the information we have. If the object occupied the corners of the original bounding box, your new bounding box needs to be bigger after the image rotates. So you must be careful of not doing too higher rotations with bounding boxes because there is not enough information for them to stay accurate. If we were doing polygons or segmentations, we would not have this problem. This why the box gets bigger So here, we do maximum of 3 degree rotation to avoid this problem [9:14]. It also only rotates half of the time ( ). will run a small batch of data through a model and prints out the size of tensors at every layer. As you can see, right before the layer, the tensor has the shape of 512 by 7 by 7. So if it were a rank 1 tensor (i.e. a single vector) its length will be 25088 (512 * 7 * 7)and that is why our custom header\u2019s input size is 25088. Output size is 4 since it is the bounding box coordinates. Let\u2019s combine the two to create something that can classify and localize the largest object in each image. There are 3 things that we need to do to train a neural network: We need a object whose independent variable is the images, and dependent variable is a tuple of bounding box coordinates and class label. There are several ways to do this, but here is a particularly lazy and convinient way Jeremy came up with is to create two objects representing the two different dependent variables we want (one with bounding boxes coordinates, one with classes). A dataset can be anything with and . Here's a dataset that adds a 2nd label to an existing dataset: : contains both independent and dependent variables : returns an independent variable and the combination of two dependent variables. We\u2019ll use it to add the classes to the bounding boxes labels. Here is an example dependent variable: We can replace the dataloaders\u2019 datasets with these new ones. We have to alize the images from the dataloader before they can be plotted. The architecture will be the same as the one we used for the classifier and bounding box regression, but we will just combine them. In other words, if we have classes, then the number of activations we need in the final layer is 4 plus . 4 for bounding box coordinates and probabilities (one per class). We\u2019ll use an extra linear layer this time, plus some dropout, to help us train a more flexible model. In general, we want our custom head to be capable of solving the problem on its own if the pre-trained backbone it is connected to is appropriate. So in this case, we are trying to do quite a bit \u2014 classifier and bounding box regression, so just the single linear layer does not seem enough. If you were wondering why there is no after the first , ResNet backbone already has as its final layer. The loss function needs to look at these activations and decide if they are good \u2014 whether these numbers accurately reflect the position and class of the largest object in the image. We know how to do this. For the first 4 activations, we will use L1Loss just like we did before (L1Loss is like a Mean Squared Error \u2014 instead of sum of squared errors, it uses sum of absolute values). For rest of the activations, we can use cross entropy loss. def detn_loss(input, target):\n\n bb_t,c_t = target\n\n bb_i,c_i = input[:, :4], input[:, 4:]\n\n bb_i = F.sigmoid(bb_i)*224\n\n # I looked at these quantities separately first then picked a \n\n # multiplier to make them approximately equal\n\n return F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20 : Our custom dataset returns a tuple containing bounding box coordinates and classes. This assignment will destructure them. : the first is for the batch dimension. : we know our image is 224 by 224. will force it to be between 0 and 1, and multiply it by 224 to help our neural net to be in the range of what it has to be. Question: As a general rule, is it better to put BatchNorm before or after ReLU [18:02]? Jeremy would suggest to put it after a ReLU because BathNorm is meant to move towards zero-mean one-standard deviation. So if you put ReLU right after it, you are truncating it at zero so there is no way to create negative numbers. But if you put ReLU then BatchNorm, it does have that ability and gives slightly better results. Having said that, it is not too big of a deal either way. You see during this part of the course, most of the time, Jeremy does ReLU then BatchNorm but sometimes does the opposite when he wants to be consistent with the paper. Question: What is the intuition behind using dropout after a BatchNorm? Doesn\u2019t BatchNorm already do a good job of regularizing [19:12]? BatchNorm does an okay job of regularizing but if you think back to part 1 when we discussed a list of things we do to avoid overfitting and adding BatchNorm is one of them as is data augmentation. But it\u2019s perfectly possible that you\u2019ll still be overfitting. One nice thing about dropout is that is it has a parameter to say how much to drop out. Parameters are great specifically parameters that decide how much to regularize because it lets you build a nice big over parameterized model and then decide on how much to regularize it. Jeremy tends to always put in a drop out starting with and then as he adds regularization, he can just change the dropout parameter without worrying about if he saved a model he want to be able to load it back, but if he had dropout layers in one but no in another, it will not load anymore. So this way, it stays consistent. Now we have out inputs and targets, we can calculate the L1 loss and add the cross entropy [20:39]: This is our loss function. Cross entropy and L1 loss may be of wildly different scales \u2014 in which case in the loss function, the larger one is going to dominate. In this case, Jeremy printed out the values and found out that if we multiply cross entropy by 20 that makes them about the same scale. It is nice to print out information as you train, so we grabbed L1 loss and added it as metrics. A detection accuracy is in the low 80\u2019s which is the same as what it was before. This is not surprising because ResNet was designed to do classification so we wouldn\u2019t expect to be able to improve things in such a simple way. It certainly wasn\u2019t designed to do bounding box regression. It was explicitly actually designed in such a way to not care about geometry \u2014 it takes the last 7 by 7 grid of activations and averages them all together throwing away all the information about where everything came from. Interestingly, when we do accuracy (classification) and bounding box at the same time, the L1 seems a little bit better than when we just do bounding box regression [22:46]. If that is counterintuitive to you, then this would be one of the main things to think about after this lesson since it is a really important idea. The idea is this \u2014 figuring out what the main object in an image is, is kind of the hard part. Then figuring out exactly where the bounding box is and what class it is is the easy part in a way. So when you have a single network that\u2019s both saying what is the object and where is the object, it\u2019s going to share all the computation about finding the object. And all that shared computation is very efficient. When we back propagate the errors in the class and in the place, that\u2019s all the information that is going to help the computation around finding the biggest object. So anytime you have multiple tasks which share some concept of what those tasks would need to do to complete their work, it is very likely they should share at least some layers of the network together. Later today, we will look at a model where most of the layers are shared except for the last one. Here are the result [24:34]. As before, it does a good job when there is single major object in the image. We want to keep building models that are slightly more complex than the last model so that if something stops working, we know exactly where it broke. Here are functions from the previous notebook: PATH = Path('data/pascal')\n\ntrn_j = json.load((PATH / 'pascal_train2007.json').open())\n\nIMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', \n\n 'categories']\n\nFILE_NAME,ID,IMG_ID,CAT_ID,BBOX = 'file_name','id','image_id', \n\n 'category_id','bbox'\n\n\n\ncats = dict((o[ID], o['name']) for o in trn_j[CATEGORIES])\n\ntrn_fns = dict((o[ID], o[FILE_NAME]) for o in trn_j[IMAGES])\n\ntrn_ids = [o[ID] for o in trn_j[IMAGES]]\n\n\n\nJPEGS = 'VOCdevkit/VOC2007/JPEGImages'\n\nIMG_PATH = PATH/JPEGS def get_trn_anno():\n\n trn_anno = collections.defaultdict(lambda:[])\n\n for o in trn_j[ANNOTATIONS]:\n\n if not o['ignore']:\n\n bb = o[BBOX]\n\n bb = np.array([bb[1], bb[0], bb[3]+bb[1]-1, \n\n bb[2]+bb[0]-1])\n\n trn_anno[o[IMG_ID]].append((bb,o[CAT_ID]))\n\n return trn_anno\n\n\n\ntrn_anno = get_trn_anno() mc = [set([cats[p[1]] for p in trn_anno[o]]) for o in trn_ids]\n\nmcs = [' '.join(str(p) for p in o) for o in mc] One of the students pointed out that by using Pandas, we can do things much simpler than using and shared this gist. The more you get to know Pandas, the more often you realize it is a good way to solve lots of different problems. Question: When you are incrementally building on top of smaller models, do you reuse them as pre-trained weights? or do you toss it away then retrain from scratch [27:11]? When Jeremy is figuring stuff out as he goes like this, he would generally lean towards tossing away because reusing pre-trained weights introduces unnecessary complexities. However, if he is trying to get to a point where he can train on really big images, he will generally start on much smaller and often re-use these weights. fig, axes = plt.subplots(3, 4, figsize=(12, 8))\n\nfor i,ax in enumerate(axes.flat):\n\n ima=md.val_ds.denorm(x)[i]\n\n ya = np.nonzero(y[i]>0.4)[0]\n\n b = '\n\n'.join(md.classes[o] for o in ya)\n\n ax = show_img(ima, ax=ax)\n\n draw_text(ax, (0,0), b)\n\nplt.tight_layout() Multi-class classification is pretty straight forward [28:28]. One minor tweak is the use of in this line so that each object type appear once.: mc = [set([cats[p[1]] for p in trn_anno[o]]) for o in trn_ids] We have an input image that goes through a conv net which outputs a vector of size where . This gives us an object detector for a single largest object. Let\u2019s now create one that finds 16 objects. The obvious way to do this would be to take the last linear layer and rather than having outputs, we could have outputs. This gives us 16 sets of class probabilities and 16 sets of bounding box coordinates. Then we would just need a loss function that will check whether those 16 sets of bounding boxes correctly represented the up to 16 objects in the image (we will go into the loss function later). The second way to do this is rather than using , what if instead, we took from our ResNet convolutional backbone and added an with stride 2 [31:32]? This will give us a tensor \u2014 here let\u2019s make it so that we get a tensor where the number of elements is exactly equal to the number of elements we wanted. Now if we created a loss function that took a tensor and and mapped it to 16 objects in the image and checked whether each one was correctly represented by these activations, this would work as well. It turns out, both of these approaches are actually used [33:48]. The approach where the output is one big long vector from a fully connected linear layer is used by a class of models known as YOLO (You Only Look Once), where else, the approach of the convolutional activations is used by models which started with something called SSD (Single Shot Detector). Since these things came out very similar times in late 2015, things are very much moved towards SSD. So the point where this morning, YOLO version 3 came out and is now doing SSD, so that\u2019s what we are going to do. We will also learn about why this makes more sense as well. Let\u2019s imagine that we had another then we would have tensor. Basically, it is creating a grid that looks something like this: This is how the geometry of the activations of the second extra convolutional stride 2 layer are. Remember, stride 2 convolution does the same thing to the geometry of the activations as a stride 1 convolution followed by maxpooling assuming the padding is ok. Let\u2019s talk about what we might do here [36:09]. We want each of these grid cell to be responsible for finding the largest object in that part of the image. Why do we care about the idea that we would like each convolutional grid cell to be responsible for finding things that are in the corresponding part of the image? The reason is because of something called the receptive field of that convolutional grid cell. The basic idea is that throughout your convolutional layers, every piece of those tensors has a receptive field which means which part of the input image was responsible for calculating that cell. Like all things in life, the easiest way to see this is with Excel [38:01].\n\nTake a single activation (in this case in the maxpool layer)and let\u2019s see where it came from [38:45]. In excel you can do Formulas \u2192 Trace Precedents. Tracing all the way back to the input layer, you can see that it came from this 6 x 6 portion of the image (as well as filters). What is more, the middle portion has lots of weights coming out of where else cells in the outside only have one weight coming out. So we call this 6 x 6 cells the receptive field of the one activation we picked. 3x3 convolution with opacity 15% \u2014 clearly the center of the box has more dependencies Note that the receptive field is not just saying it\u2019s this box but also that the center of the box has more dependencies [40:27] This is a critically important concept when it comes to understanding architectures and understanding why conv nets work the way they do. The architecture is, we will have a ResNet backbone followed by one or more 2D convolutions (one for now) which is going to give us a grid. We start with ReLU and dropout Then stride 1 convolution. The reason we start with a stride 1 convolution is because that does not change the geometry at all \u2014 it just lets us add an extra layer of calculation. It lets us create not just a linear layer but now we have a little mini neural network in our custom head. is defined above \u2014 it does convolution, ReLU, BatchNorm, and dropout. Most research code you see won\u2019t define a class like this, instead they write the entire thing again and again. Don\u2019t be like that. Duplicate code leads to errors and poor understanding. At the end, the output of step 3 is which gets passed to . has two separate convolutional layers each of which is stride 1 so it is not changing the geometry of the input. One of them is of length of the number of classes (ignore for now and is for \u201cbackground\u201d \u2014 i.e. no object was detected), the other\u2019s length is 4. Rather than having a single conv layer that outputs , let\u2019s have two conv layers and return their outputs in a list. This allows these layers to specialize just a little bit. We talked about this idea that when you have multiple tasks, they can share layers, but they do not have to share all the layers. In this case, our two tasks of creating a classifier and creating and creating bounding box regression share every single layers except the very last one. At the end, we flatten out the convolution because Jeremy wrote the loss function to expect flattened out tensor, but we could totally rewrite it to not do that. The first draft was released this week. It is very heavily orient towards the idea of expository programming which is the idea that programming code should be something that you can use to explain an idea, ideally as readily as mathematical notation, to somebody that understands your coding method. The idea goes back a very long way, but it was best described in the Turing Award lecture of 1979 by probably Jeremy\u2019s greatest computer science hero Ken Iverson. He had been working on it since well before 1964 but 1964 was the first example of this approach of programming he released which is called APL and 25 years later, he won the Turing Award. He then passed on the baton to his son Eric Iverson. Fastai style guide is an attempt at taking some of these ideas. The loss function needs to look at each of these 16 sets of activations, each of which has four bounding box coordinates and class probabilities and decide if those activations are close or far away from the object which is the closest to this grid cell in the image. If nothing is there, then whether it is predicting background correctly. That turns out to be very hard to do. The loss function needs to take each of the objects in the image and match them to one of these convolutional grid cells to say \u201cthis grid cell is responsible for this particular object\u201d so then it can go ahead and say \u201cokay, how close are the 4 coordinates and how close are the class probabilities. Here is our goal [49:56]: Our dependent variable looks like the one on the left, and our final convolutional layer is going to be in this case . We then flatten that out into a vector. Our goal is to come up with a function which takes in a dependent variable and also some particular set of activations that ended up coming out of the model and returns a higher number if these activations are not a good reflection of the ground truth bounding boxes; or a lower number if it is a good reflection. Make sure these shapes make sense. Let\u2019s now look at the ground truth [53:24]: Note that bounding box coordinates have been scaled to between 0 and 1 \u2014 basically we are treating the image as being 1x1, so they are relative to the size of the image. We already have function. This (gt: ground truth) function simply converts tensors into numpy array. def torch_gt(ax, ima, bbox, clas, prs=None, thresh=0.4):\n\n return show_ground_truth(ax, ima, to_np((bbox*224).long()),\n\n to_np(clas), \n\n to_np(prs) if prs is not None else None, thresh) The above is a ground truth. Here is our grid cells from our final convolutional layer [54:44]: Each of these square boxes, different papers call them different things. The three terms you\u2019ll hear are: anchor boxes, prior boxes, or default boxes. We will stick with the term anchor boxes. What we are going to do for this loss function is we are going to go through a matching problem where we are going to take every one of these 16 boxes and see which one of these three ground truth objects has the highest amount of overlap with a given square [55:21]. To do this, we have to have some way of measuring amount of overlap and a standard function for this is called Jaccard index (IoU). We are going to go through and find the Jaccard overlap for each one of the three objects versus each of the 16 anchor boxes [57:11]. That is going to give us a matrix. Here are the coordinates of all of our anchor boxes (centers, height, width): Here are the amount of overlap between 3 ground truth objects and 16 anchor boxes: What we could do now is we could take the max of dimension 1 (row-wise) which will tell us for each ground truth object, what the maximum amount that overlaps with some grid cell as well as the index: We will also going to look at max over a dimension 0 (column-wise) which will tell us what is the maximum amount of overlap for each grid cell across all of the ground truth objects [59:08]: What is particularly interesting here is that it tells us for every grid cell what is the index of the ground truth object which overlaps with it the most. Zero is a bit overloaded here \u2014 zero could either mean the amount of overlap was zero or its largest overlap is with object index zero. It is going to turn out not to matter but just FYI. There is a function called which we will not worry about for now [59:57]. It is super simple code but it is slightly awkward to think about. Basically what it does is it combines these two sets of overlaps in a way described in the SSD paper to assign every anchor box to a ground truth object. The way it assign that is each of the three (row-wise max) gets assigned as is. For the rest of the anchor boxes, they get assigned to anything which they have an overlap of at least 0.5 with (column-wise). If neither applies, it is considered to be a cell which contains background. Now you can see a list of all the assignments [1:01:05]. Anywhere that has gets assigned background. The three row-wise max anchor box has high number to force the assignments. Now we can combine these values to classes: Then add a threshold and finally comes up with the three classes that are being predicted: And here are what each of these anchor boxes is meant to be predicting: gt_clas[1-pos] = len(id2cat)\n\n[id2cat[o] if o<len(id2cat) else 'bg' for o in gt_clas.data] So that was the matching stage [1:02:29]. For L1 loss, we can: take the activations which matched ( ) subtract from those the ground truth bounding boxes take the absolute value of the difference take the mean of that. For classifications, we can just do a cross entropy We will end up with 16 predicted bounding boxes, most of them will be background. If you are wondering what it predicts in terms of bounding box of background, the answer is it totally ignores it. Tweak 1. How do we interpret the activations [1:04:16]? The way we interpret the activation is defined here: We grab the activations, we stick them through (remember is the same shape as sigmoid except it is scaled to be between -1 and 1) which forces it to be within that range. We then grab the actual position of the anchor boxes, and we will move them around according to the value of the activations divided by two ( ). In other words, each predicted bounding box can be moved by up to 50% of a grid size from where its default position is. Ditto for its height and width \u2014 it can be up to twice as big or half as big as its default size. Tweak 2. We actually use binary cross entropy loss instead of cross entropy [1:05:36] Binary cross entropy is what we normally use for multi-label classification. Like in the planet satellite competition, each satellite image could have multiple things. If it has multiple things in it, you cannot use softmax because softmax really encourages just one thing to have the high number. In our case, each anchor box can only have one object associated with it, so it is not for that reason that we are avoiding softmax. It is something else \u2014 which is it is possible for an anchor box to have nothing associated with it. There are two ways to handle this idea of \u201cbackground\u201d; one would be to say background is just a class, so let\u2019s use softmax and just treat background as one of the classes that the softmax could predict. A lot of people have done it this way. But that is a really hard thing to ask neural network to do [1:06:52] \u2014 it is basically asking whether this grid cell does not have any of the 20 objects that I am interested with Jaccard overlap of more than 0.5. It is a really hard to thing to put into a single computation. On the other hand, what if we just asked for each class; \u201cis it a motorbike?\u201d \u201cis it a bus?\u201d, \u201c is it a person?\u201d etc and if all the answer is no, consider that background. That is the way we do it here. It is not that we can have multiple true labels, but we can have zero. First we take the one hot embedding of the target (at this stage, we do have the idea of background) Then we remove the background column (the last one) which results in a vector either of all zeros or one one. This is a minor tweak, but it is the kind of minor tweak that Jeremy wants you to think about and understand because it makes a really big difference to your training and when there is some increment over a previous paper, it would be something like this [1:08:25]. It is important to understand what this is doing and more importantly why. So now we have [1:09:39]: A way to convert activations to bounding box A way to map anchor boxes to ground truth Now all it\u2019s left is SSD loss function. def ssd_1_loss(b_c,b_bb,bbox,clas,print_it=False):\n\n bbox,clas = get_y(bbox,clas)\n\n a_ic = actn_to_bb(b_bb, anchors)\n\n overlaps = jaccard(bbox.data, anchor_cnr.data)\n\n gt_overlap,gt_idx = map_to_ground_truth(overlaps,print_it)\n\n gt_clas = clas[gt_idx]\n\n pos = gt_overlap > 0.4\n\n pos_idx = torch.nonzero(pos)[:,0]\n\n gt_clas[1-pos] = len(id2cat)\n\n gt_bbox = bbox[gt_idx]\n\n loc_loss = ((a_ic[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n\n clas_loss = loss_f(b_c, gt_clas)\n\n return loc_loss, clas_loss\n\n\n\ndef ssd_loss(pred,targ,print_it=False):\n\n lcs,lls = 0.,0.\n\n for b_c,b_bb,bbox,clas in zip(*pred,*targ):\n\n loc_loss,clas_loss = ssd_1_loss(b_c,b_bb,bbox,clas,print_it)\n\n lls += loc_loss\n\n lcs += clas_loss\n\n if print_it: print(f'loc: {lls.data[0]}, clas: {lcs.data[0]}')\n\n return lls+lcs The function which is what we set as the criteria, it loops through each image in the mini-batch and call function (i.e. SSD loss for one image). is where it is all happening. It begins by de-structuring and . Let\u2019s take a closer look at [1:10:38]: A lot of code you find on the internet does not work with mini-batches. It only does one thing at a time which we don\u2019t want. In this case, all these functions ( , , ) is working on, not exactly a mini-batch at a time, but a whole bunch of ground truth objects at a time. The data loader is being fed a mini-batch at a time to do the convolutional layers. Because we can have different numbers of ground truth objects in each image but a tensor has to be the strict rectangular shape, fastai automatically pads it with zeros (any target values that are shorter) [1:11:08]. This was something that was added recently and super handy, but that does mean that you then have to make sure that you get rid of those zeros. So gets rid of any of the bounding boxes that are just padding. Get rid of the padding Check that there is an overlap greater than something around 0.4~0.5 (different papers use different values for this) Find the indices of things that matched Assign background class for the ones that did not match Then finally get L1 loss for the localization, binary cross entropy loss for the classification, and return them which gets added in In practice, we want to remove the background and also add some threshold for probabilities, but it is on the right track. The potted plant image, the result is not surprising as all of our anchor boxes were small (4x4 grid). To go from here to something that is going to be more accurate, all we are going to do is to create way more anchor boxes. Question: For the multi-label classification, why aren\u2019t we multiplying the categorical loss by a constant like we did before [1:15:20]? Great question. It is because later on it will turn out we do not need to. There are 3 ways to do this:\n\n3. Use more convolutional layers as sources of anchor boxes (the boxes are randomly jittered so that we can see ones that are overlapping [1:16:28]): Combining these approaches, you can create lots of anchor boxes (Jeremy said he wouldn\u2019t print it, but here it is): anc_grids = [4, 2, 1]\n\nanc_zooms = [0.75, 1., 1.3]\n\nanc_ratios = [(1., 1.), (1., 0.5), (0.5, 1.)]\n\n\n\nanchor_scales = [(anz*i,anz*j) for anz in anc_zooms \n\n for (i,j) in anc_ratios]\n\nk = len(anchor_scales)\n\nanc_offsets = [1/(o*2) for o in anc_grids] anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n\n for ao,ag in zip(anc_offsets,anc_grids)])\n\nanc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n\n for ao,ag in zip(anc_offsets,anc_grids)])\n\nanc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0) anc_sizes = np.concatenate([np.array([[o/ag,p/ag] \n\n for i in range(ag*ag) for o,p in anchor_scales])\n\n for ag in anc_grids])\n\ngrid_sizes = V(np.concatenate([np.array([ 1/ag \n\n for i in range(ag*ag) for o,p in anchor_scales])\n\n for ag in anc_grids]), \n\n requires_grad=False).unsqueeze(1)\n\nanchors = V(np.concatenate([anc_ctrs, anc_sizes], axis=1), \n\n requires_grad=False).float()\n\nanchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:]) We have a vector of ground truth (sets of 4 bounding box coordinates and a class) We have a neural net that takes some input and spits out some output activations Compare the activations and the ground truth, calculate a loss, find the derivative of that, and adjust weights according to the derivative times a learning rate. We need a loss function that can take ground truth and activation and spit out a number that says how good these activations are. To do this, we need to take each one of ground truth objects and decide which set of activations is responsible for that object [1:21:58] \u2014 which one we should be comparing to decide whether the class is correct and bounding box is close or not (matching problem). Since we are using SSD approach, so it is not arbitrary which ones we match up [1:23:18]. We want to match up the set of activations whose receptive field has the maximum density from where the real object is. The loss function needs to be some consistent task. If in the first image, the top left object corresponds with the first 4+c activations, and in the second image, we threw things around and suddenly it\u2019s now going with the last 4+c activations, the neural net doesn\u2019t know what to learn. Once matching problem is resolved, the rest is just the same as the single object detection. YOLO \u2014 the last layer is fully connected (no concept of geometry) SSD \u2014 the last layer is convolutional For every grid cell which can be different sizes, we can have different orientations and zooms representing different anchor boxes which are just like conceptual ideas that every one of anchor boxes is associated with one set of activations in our model. So however many anchor boxes we have, we need to have that times activations. That does not mean that each convolutional layer needs that many activations. Because 4x4 convolutional layer already has 16 sets of activations, the 2x2 layer has 4 sets of activations, and finally 1x1 has one set. So we basically get 1 + 4 + 16 for free. So we only needs to know where is the number of zooms by the number of aspect ratios. Where else, the grids, we will get for free through our architecture. The model is nearly identical to what we had before. But we have a number of stride 2 convolutions which is going to take us through to 4x4, 2x2, and 1x1 (each stride 2 convolution halves our grid size in both directions). After we do our first convolution to get to 4x4, we will grab a set of outputs from that because we want to save away the 4x4 anchors. Once we get to 2x2, we grab another set of now 2x2 anchors Then finally we get to 1x1 We then concatenate them all together, which gives us the correct number of activations (one activation for every anchor box). Here, we printed out those detections with at least probability of . Some of them look pretty hopeful but others not so much.\n\nWhen people refer to the multi-box method, they are talking about this paper. This was the paper that came up with the idea that we can have a loss function that has this matching process and then you can use that to do object detection. So everything since that time has been trying to figure out how to make this better. In parallel, Ross Girshick was going down a totally different direction. He had these two-stage process where the first stage used the classical computer vision approaches to find edges and changes of gradients to guess which parts of the image may represent distinct objects. Then fit each of those into a convolutional neural network which was basically designed to figure out if that is the kind of object we are interested in. R-CNN and Fast R-CNN are hybrid of traditional computer vision and deep learning. What Ross and his team then did was they took the multibox idea and replaced the traditional non-deep learning computer vision part of their two stage process with the conv net. So now they have two conv nets: one for region proposals (all of the things that might be objects) and the second part was the same as his earlier work. You Only Look Once: Unified, Real-Time Object Detection At similar time these paper came out. Both of these did something pretty cool which is they achieved similar performance as the Faster R-CNN but with 1 stage. They took the multibox idea and they tried to figure out how to deal with messy outputs. The basic ideas were to use, for example, hard negative mining where they would go through and find all of the matches that did not look that good and throw them away, use very tricky and complex data augmentation methods, and all kind of hackery. But they got them to work pretty well. Then something really cool happened late last year which is this thing called focal loss. They actually realized why this messy thing wasn\u2019t working. When we look at an image, there are 3 different granularities of convolutional grid (4x4, 2x2, 1x1) [1:37:28]. The 1x1 is quite likely to have a reasonable overlap with some object because most photos have some kind of main subject. On the other hand, in the 4x4 grid cells, the most of 16 anchor boxes are not going to have a much of an overlap with anything. So if somebody was to say to you \u201c$20 bet, what do you reckon this little clip is?\u201d and you are not sure, you will say \u201cbackground\u201d because most of the time, it is the background. Question: I understand why we have a 4x4 grid of receptive fields with 1 anchor box each to coarsely localize objects in the image. But what I think I\u2019m missing is why we need multiple receptive fields at different sizes. The first version already included 16 receptive fields, each with a single anchor box associated. With the additions, there are now many more anchor boxes to consider. Is this because you constrained how much a receptive field could move or scale from its original size? Or is there another reason? [1:38:47] It is kind of backwards. The reason Jeremy did the constraining was because he knew he was going to be adding more boxes later. But really, the reason is that the Jaccard overlap between one of those 4x4 grid cells and a picture where a single object that takes up most of the image is never going to be 0.5. The intersection is much smaller than the union because the object is too big. So for this general idea to work where we are saying you are responsible for something that you have better than 50% overlap with, we need anchor boxes which will on a regular basis have a 50% or higher overlap which means we need to have a variety of sizes, shapes, and scales. This all happens in the loss function. The vast majority of the interesting stuff in all of the object detection is the loss function. The key thing is this very first picture. The blue line is the binary cross entropy loss. If the answer is not a motorbike [1:41:46], and I said \u201cI think it\u2019s not a motorbike and I am 60% sure\u201d with the blue line, the loss is still about 0.5 which is pretty bad. So if we want to get our loss down, then for all these things which are actually back ground, we have to be saying \u201cI am sure that is background\u201d, \u201cI am sure it\u2019s not a motorbike, or a bus, or a person\u201d \u2014 because if I don\u2019t say we are sure it is not any of these things, then we still get loss. That is why the motorbike example did not work [1:42:39]. Because even when it gets to lower right corner and it wants to say \u201cI think it\u2019s a motorbike\u201d, there is no payoff for it to say so. If it is wrong, it gets killed. And the vast majority of the time, it is background. Even if it is not background, it is not enough just to say \u201cit\u2019s not background\u201d \u2014 you have to say which of the 20 things it is. So the trick is to trying to find a different loss function [1:44:00] that looks more like the purple line. Focal loss is literally just a scaled cross entropy loss. Now if we say \u201cI\u2019m .6 sure it\u2019s not a motorbike\u201d then the loss function will say \u201cgood for you! no worries\u201d [1:44:42]. The actual contribution of this paper is to add to the start of the equation [1:45:06] which sounds like nothing but actually people have been trying to figure out this problem for years. When you come across a paper like this which is game-changing, you shouldn\u2019t assume you are going to have to write thousands of lines of code. Very often it is one line of code, or the change of a single constant, or adding log to a single place. A couple of terrific things about this paper [1:46:08]: Remember, -log(pt) is the cross entropy loss and focal loss is just a scaled version. When we defined the binomial cross entropy loss, you may have noticed that there was a weight which by default was none: When you call , you can pass in the weight. Since we just wanted to multiply a cross entropy by something, we can just define . Here is the entirety of focal loss [1:50:23]: If you were wondering why alpha and gamma are 0.25 and 2, here is another excellent thing about this paper, because they tried lots of different values and found that these work well: This time things are looking quite a bit better. So our last step, for now, is to basically figure out how to pull out just the interesting ones. All we are going to do is we are going to go through every pair of these bounding boxes and if they overlap by more than some amount, say 0.5, using Jaccard and they are both predicting the same class, we are going to assume they are the same thing and we are going to pick the one with higher value. It is really boring code, Jeremy didn\u2019t write it himself and copied somebody else\u2019s. No reason particularly to go through it. def nms(boxes, scores, overlap=0.5, top_k=100):\n\n keep = scores.new(scores.size(0)).zero_().long()\n\n if boxes.numel() == 0: return keep\n\n x1 = boxes[:, 0]\n\n y1 = boxes[:, 1]\n\n x2 = boxes[:, 2]\n\n y2 = boxes[:, 3]\n\n area = torch.mul(x2 - x1, y2 - y1)\n\n v, idx = scores.sort(0) # sort in ascending order\n\n idx = idx[-top_k:] # indices of the top-k largest vals\n\n xx1 = boxes.new()\n\n yy1 = boxes.new()\n\n xx2 = boxes.new()\n\n yy2 = boxes.new()\n\n w = boxes.new()\n\n h = boxes.new()\n\n\n\n count = 0\n\n while idx.numel() > 0:\n\n i = idx[-1] # index of current largest val\n\n keep[count] = i\n\n count += 1\n\n if idx.size(0) == 1: break\n\n idx = idx[:-1] # remove kept element from view\n\n # load bboxes of next highest vals\n\n torch.index_select(x1, 0, idx, out=xx1)\n\n torch.index_select(y1, 0, idx, out=yy1)\n\n torch.index_select(x2, 0, idx, out=xx2)\n\n torch.index_select(y2, 0, idx, out=yy2)\n\n # store element-wise max with next highest score\n\n xx1 = torch.clamp(xx1, min=x1[i])\n\n yy1 = torch.clamp(yy1, min=y1[i])\n\n xx2 = torch.clamp(xx2, max=x2[i])\n\n yy2 = torch.clamp(yy2, max=y2[i])\n\n w.resize_as_(xx2)\n\n h.resize_as_(yy2)\n\n w = xx2 - xx1\n\n h = yy2 - yy1\n\n # check sizes of xx1 and xx2.. after each iteration\n\n w = torch.clamp(w, min=0.0)\n\n h = torch.clamp(h, min=0.0)\n\n inter = w*h\n\n # IoU = i / (area(a) + area(b) - i)\n\n rem_areas = torch.index_select(area, 0, idx) \n\n # load remaining areas)\n\n union = (rem_areas - inter) + area[i]\n\n IoU = inter/union # store result in iou\n\n # keep only elements with an IoU <= overlap\n\n idx = idx[IoU.le(overlap)]\n\n return keep, count def show_nmf(idx):\n\n ima=md.val_ds.ds.denorm(x)[idx]\n\n bbox,clas = get_y(y[0][idx], y[1][idx])\n\n a_ic = actn_to_bb(b_bb[idx], anchors)\n\n clas_pr, clas_ids = b_clas[idx].max(1)\n\n clas_pr = clas_pr.sigmoid()\n\n\n\n conf_scores = b_clas[idx].sigmoid().t().data\n\n\n\n out1,out2,cc = [],[],[]\n\n for cl in range(0, len(conf_scores)-1):\n\n c_mask = conf_scores[cl] > 0.25\n\n if c_mask.sum() == 0: continue\n\n scores = conf_scores[cl][c_mask]\n\n l_mask = c_mask.unsqueeze(1).expand_as(a_ic)\n\n boxes = a_ic[l_mask].view(-1, 4)\n\n ids, count = nms(boxes.data, scores, 0.4, 50)\n\n ids = ids[:count]\n\n out1.append(scores[ids])\n\n out2.append(boxes.data[ids])\n\n cc.append([cl]*count)\n\n cc = T(np.concatenate(cc))\n\n out1 = torch.cat(out1)\n\n out2 = torch.cat(out2)\n\n\n\n fig, ax = plt.subplots(figsize=(8,8))\n\n torch_gt(ax, ima, out2, cc, out1, 0.1)\n\nThere are some things still to fix here [1:53:43]. The trick will be to use something called feature pyramid. That is what we are going to do in lesson 14. Talking a little more about SSD paper [1:54:03] When this paper came out, Jeremy was excited because this and YOLO were the first kind of single-pass good quality object detection method that come along. There has been this continuous repetition of history in the deep learning world which is things that involve multiple passes of multiple different pieces, over time, particularly where they involve some non-deep learning pieces (like R-CNN did), over time, they always get turned into a single end-to-end deep learning model. So I tend to ignore them until that happens because that\u2019s the point where people have figured out how to show this as a deep learning model, as soon as they do that they generally end up something much faster and much more accurate. So SSD and YOLO were really important. The model is 4 paragraphs. Papers are really concise which means you need to read them pretty carefully. Partly, though, you need to know which bits to read carefully. The bits where they say \u201chere we are going to prove the error bounds on this model,\u201d you could ignore that because you don\u2019t care about proving error bounds. But the bit which says here is what the model is, you need to read real carefully. If you jump straight in and read a paper like this, these 4 paragraphs would probably make no sense. But now that we\u2019ve gone through it, you read those and hopefully thinking \u201coh that\u2019s just what Jeremy said, only they sad it better than Jeremy and less words [2:00:37]. If you start to read a paper and go \u201cwhat the heck\u201d, the trick is to then start reading back over the citations. Double bars and two 2\u2019s like this means Mean Squared Error log(c) and log(1-c), and x and (1-x) they are all the pieces for binary cross entropy: This week, go through the code and go through the paper and see what is going on. Remember what Jeremy did to make it easier for you was he took that loss function, he copied it into a cell and split it up so that each bit was in a separate cell. Then after every sell, he printed or plotted that value. Hopefully this is a good starting point."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-8-5ae195c49493?source=user_profile---------4----------------",
        "title": "Deep Learning 2: Part 2 Lesson 8 \u2013 Hiromi Suenaga \u2013",
        "text": "Two main differences from what we are used to:\n\n1.We have multiple things that we are classifying.\n\nThis if not unheard of \u2014 we did that in the planet satellite data in part 1.\n\n2. Bounding boxes around what we are classifying.\n\nA bounding box has a very specific definition which is it\u2019s a rectangle and the rectangle has the object entirely fitting within it but it is no bigger than it has to be.\n\nOur job will be to take data that has been labeled in this way and on data that is unlabeled to generate classes of the objects and each one of those their bounding boxes. One thing to note is that labeling this kind of data is generally more expensive [37:09]. For object detection datasets, annotators are given a list of object classes and asked to find every single one of them of any type in a picture along with where they are. In this case why isn\u2019t there a tree or jump labeled? That is because for this particular dataset, they were not one of the classes that annotators were asked to find and therefore not part of this particular problem.\n\nYou may find a line left behind which will give you an error if you only have one GPU. This is how you select a GPU when you have multiple, so just set it to zero or take out the line entirely.\n\nThere is a number of standard object detection datasets just like ImageNet being a standard object classification dataset [41:12]. The classic ImageNet equivalent is Pascal VOC.\n\nWe will be looking at the Pascal VOC dataset. It\u2019s quite slow, so you may prefer to download from this mirror. There are two different competition/research datasets, from 2007 and 2012. We\u2019ll be using the 2007 version. You can use the larger 2012 for better results, or even combine them [42:25](but be careful to avoid data leakage between the validation sets if you do this).\n\nUnlike previous lessons, we are using the python 3 standard library for our paths and file access. Note that it returns an OS-specific class (on Linux, ) so your output may look a little different [44:50]. Most libraries that take paths as input can take a pathlib object - although some (like ) can't, in which case you can use to convert it to a string.\n\nGenerator is something in Python 3 which you can iterate over.\n\nThe reason that things generally return generators is that if the directory had 10 million items in, you don\u2019t necessarily want 10 million long list. Generator lets you do things \u201clazily\u201d.\n\nAs well as the images, there are also annotations \u2014 bounding boxes showing where each object is. These were hand labeled. The original version were in XML [47:59], which is a little hard to work with nowadays, so we uses the more recent JSON version which you can download from this link.\n\nYou can see here how includes the ability to open files (amongst many other capabilities).\n\nHere is not divided by but it is path slash [45:55]. gets you children in that path. returns a object which has an method. This JSON file contains not the images but the bounding boxes and the classes of the objects.\n\nIt\u2019s helpful to use constants instead of strings, since we get tab-completion and don\u2019t mistype.\n\nSide Note: What people most comment on when they see Jeremy working in real time having seen his classes [51:21]:\n\n\u201cWow, you actually don\u2019t know what you are doing, do you\u201d. 99% of the things he does don\u2019t work and small percentage of things that do work end up here. He mentioned this because machine learning, particularly deep learning is incredibly frustrating [51:45]. In theory, you just define the correct loss function and the flexible enough architecture, and you press train and you are done. But if that was actually all that took, then nothing would take any time. The problem is that all the steps along the way until it works, it doesn\u2019t work. Like it goes straight to infinity, crashes with an incorrect tensor size, etc. He will endeavor to show you some kind of debugging techniques as we go, but it is one of the hardest things to teach. The main thing it requires is tenacity. The difference between the people who are super effective and the ones who do not seem to go very far has never been about intellect. It\u2019s always been about sticking with it \u2014 basically never giving up. It\u2019s particularly important with this kind of deep learning because you don\u2019t get that continuous reward cycle [53:04]. It\u2019s a constant stream of doesn\u2019t work, doesn\u2019t work, doesn\u2019t work, until eventually it does so it\u2019s kind of annoying.\n\nA is useful any time you want to have a default dictionary entry for new keys [55:05]. If you try and access a key that doesn\u2019t exist, it magically makes itself exist and it sets itself equal to the return value of the function you specify (in this case ).\n\nHere we create a dict from image IDs to a list of annotations (tuple of bounding box and class id).\n\nWe convert VOC\u2019s height/width into top-left/bottom-right, and switch x/y coords to be consistent with numpy. If given datasets are in crappy formats, take a couple of moments to make things consistent and make them the way you want them to be [1:01:24]\n\nSome libs take VOC format bounding boxes, so this let\u2019s us convert back when required [1:02:23]:\n\nWe will use fast.ai\u2019s in order to display it:\n\nYou can use Visual Studio Code (vscode \u2014 open source editor that comes with recent versions of Anaconda, or can be installed separately), or most editors and IDEs, to find out all about the function. vscode things to know:\n\nIf you are using PyCharm Professional Edition on Mac like I am:\n\nFastai uses OpenCV. TorchVision uses PyTorch tensors for data augmentations etc. A lot of people use Pillow . Jeremy did a lot of testing of all of these and he found OpenCV was about 5 to 10 times faster than TorchVision. For the planet satellite image competition [1:11:55], TorchVision was so slow that they could only get 25% GPU utilization because they were doing a lot of data augmentation. Profiler showed that it was all in TorchVision.\n\nPillow is quite a bit faster but it is not as fast as OpenCV and also is not nearly as thread-safe [1:12:19]. Python has this thing called the global interpreter lock (GIL) which means that two thread can\u2019t do pythonic things at the same time \u2014 which makes Python a crappy language for modern programming but we are stuck with it. OpenCV releases the GIL. One of the reasons fast.ai library is so fast is because it does not use multiple processors like every other library does for data augmentations \u2014 it actually does multiple threads. The reason it could do multiple thread is because it uses OpenCV. Unfortunately OpenCV has an inscrutable API and documentations are somewhat obtuse. That is why Jeremy tried to make it so that no one using fast.ai needs to know that it\u2019s using OpenCV. You don\u2019t need to know what flags to pass to open an image. You don\u2019t need to know that if the reading fails, it doesn\u2019t show an exception \u2014 it silently returns .\n\nDon\u2019t start using PyTorch for your data augmentation or start bringing in Pillow \u2014 you will find suddenly things slow down horribly or the multi-threading won\u2019t work anymore. You should stick to using OpenCV for your processing [1:14:10]\n\nMatplotlib is so named because it was originally a clone of Matlab\u2019s plotting library. Unfortunately, Matlab\u2019s plotting library is not great, but at that time, it was what everybody knew. At some point, the matplotlib folks realized that and added a second API which was an object-oriented API. Unfortunately, because nobody who originally learnt matplotlib learnt the OO API, they then taught the next generation of people the old Matlab style API. Now there are not many examples or tutorials that use the much better, easier to understand, and simpler OO API. Because plotting is so important in deep learning, one of the things we are going to learn in this class is how to use this API.\n\nMatplotlib\u2019s is a really useful wrapper for creating plots, regardless of whether you have more than one subplot. Note that Matplotlib has an optional object-oriented API which I think is much easier to understand and use (although few examples online use it!)\n\nIt returns two things \u2014 you probably won\u2019t care about the first one (Figure object), the second one is Axes object (or an array of them). Basically anywhere you used to say something, you now say something, and it will now do the plotting to that particular subplot. This is helpful when you want to plot multiple plots so you can compare next to each other.\n\nA simple but rarely used trick to making text visible regardless of background is to use white text with black outline, or visa versa. Here\u2019s how to do it in matplotlib.\n\nNote that in argument lists is the splat operator. In this case it's a little shortcut compared to writing out .\n\nPackaging it all up [1:21:20]\n\nWhen you are working with a new dataset, getting to the point that you can rapidly explore it pays off.\n\nRather than trying to solve everything at once, let\u2019s make continual progress. We know how to find the biggest object in each image and classify it, so let\u2019s start from there. Jeremy\u2019s approach to Kaggle competition is half an hour every day [1:24:00]. At the end of that half hour, submit something and try to make it a little bit better than yesterday\u2019s.\n\nThe first thing we need to do is to go through each of the bounding boxes in an image and get the largest one. A lambda function is simply a way to define an anonymous function inline. Here we use it to describe how to sort the annotation for each image \u2014 by bounding box size (descending).\n\nWe subtract the upper left from the bottom right and multiply ( ) the values to get an area .\n\nNow we have a dictionary from image id to a single bounding box \u2014 the largest for that image.\n\nYou need to look at every stage when you have any kind of processing pipeline [1:28:01]. Assume that everything you do will be wrong the first time you do it.\n\nOften it\u2019s easiest to simply create a CSV of the data you want to model, rather than trying to create a custom dataset [1:29:06]. Here we use Pandas to help us create a CSV of the image filename and class. is there because dictionary does not have an order and the order of columns matters.\n\nFrom here it\u2019s just like Dogs vs Cats!\n\nOne thing that is different is . The default strategy for creating 224 by 224 image in fast.ai is to first resize it so that the smallest side is 224. Then to take a random squared crop during the training. During validation, we take the center crop unless we use data augmentation.\n\nFor bounding boxes, we do not want to do that because unlike an image net where the thing we care about is pretty much in the middle and pretty big, a lot of the things in object detection is quite small and close to the edge. By setting to , it will not crop and therefore, to make it square, it squishes it [1:32:09]. Generally speaking, a lot of computer vision models work a little bit better if you crop rather than squish, but they still work pretty well if you squish. In this case, we definitely do not want to crop, so this is perfectly fine.\n\nYou already know that inside of a model data object, we have bunch of things which include training data loader and training data set. The main thing to know about data loader is that it is an iterator that each time you grab the next iteration of stuff from it, you get a mini batch. The mini batch you get is of whatever size you asked for and by default the batch size is 64. In Python, the way you grab the next thing from an iterator is with next but you can\u2019t just do that. The reason you can\u2019t say that is because you need to say \u201cstart a new epoch now\u201d. In general, not just in PyTorch but for any Python iterator, you need to say \u201cstart at the beginning of the sequence please\u201d. The say you do that is to use which will grab an iterator out of \u2014 specifically as we will learn later, it means that this class has to have defined an method which returns some different object which then has an method.\n\nIf you want to grab just a single batch, this is how you do it ( : independent variable, : dependent variable):\n\nWe cannot send this straight to [1:35:30]. For example, is not a numpy array, not on CPU, and the shape is all wrong ( ). Further more, they are not numbers between 0 and 1 because all of the standard ImageNet pre-trained models expect our data to have been normalized to have a zero mean and 1 standard deviation.\n\nAs you see, there is a whole bunch of things that has been done to the input to get it ready to be passed to a pre-trained model. So we have a function called for denormalize and also fixes up dimension order etc. Since denormalization depends on the transform [1:37:52], and dataset knows what transform was used to create it, so that is why you have to do and pass the mini-batch after turning it into numpy array:\n\nWe intentionally remove the first few points and the last few points [1:38:54], because often the last few points shoots so high up towards infinity that you can\u2019t see anything so it is generally a good idea. But when you have very few mini-batches, it is not a good idea. When your LR finder graph looks like above, you can ask for more points on each end (you can also make your batch size really small):\n\nAccuracy isn\u2019t improving much \u2014 since many images have multiple different objects, it\u2019s going to be impossible to be that accurate.\n\nHow to understand the unfamiliar code:\n\nMethod 1 [1:42:28]: You can take the contents of the loop, copy it, create a cell above it, paste it, un-indent it, set and put them all in separate cells.\n\nYou can use the python debugger to step through code.\n\nCommands you need to know:\n\nComment [1:49:10]: (on the right below) makes it all pretty:"
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-3-fa4065d8cb1e?source=user_profile---------5----------------",
        "title": "Machine Learning 1: Lesson 3 \u2013 Hiromi Suenaga \u2013",
        "text": "Understanding the data better by using machine learning\n\nHow to look at larger datasets\n\nQuestion: When to use random forests [2:41]? Cannot think of anything offhand that it is definitely not going to be at least somewhat useful. So it is always worth trying. The real question might be in what situation should we try other things as well, and the short answer to that is for unstructured data (image, sound, etc), you almost certainly want to try deep learning. For collaborative filtering model (groceries competition is of that kind), neither random forest nor deep learning approach is exactly what you want and you need to do some tweaks.\n\nReading CSV took a minute or two, and we saved it to a feather format file. Feather format is almost the same format that it lives in RAM, so it is ridiculously fast to read and write. The first thing we do is in the lesson 2 notebook is to read in the feather format file.\n\nAn interesting little issue that was brought up during the week is in function. function does the following:\n\nProblem #1: Your test set may have missing values in some columns that were not in your training set or vice versa. If that happens, you are going to get an error when you try to do the random forest since the \u201cmissing\u201d boolean column appeared in your training set but not in the test set.\n\nProblem #2: Median of the numeric value in the test set may be different from the training set. So it may process it into something which has different semantics.\n\nSolution: There is now an additional return variable from which is a dictionary whose keys are the names of the columns that had missing values, and the values of the dictionary are the medians. Optionally, you can pass to as an argument to make sure that it adds those specific columns and uses those specific medians:\n\nLet\u2019s walk through the same process when you are working with a really large dataset. It is almost the same but there are a few cases where we cannot use the defaults because defaults run a little bit too slowly.\n\nIt is important to be able to explain the problem you are working on. The key things to understand in a machine learning problem are:\n\nThis is what we call a relational dataset. Relational dataset is one where we have a number of different pieces of information that we can join together. Specifically this kind of relational dataset is what we refer to as \u201cstar schema\u201d where there is some central transactions table. In this competition, the central transactions table is which contains the number units that were sold by , , and . From this, we can join various bits of metadata (hence the name \u201cstar\u201d schema \u2014 there is also one called \u201csnowflake\u201d schema).\n\nQuestion: Are there any performance consideration to specifying vs. [18:33]? The key performance here was to use the smallest number of bits that I could to fully represent the column. If we used for , the maximum is bigger than 255 and it will not fit. On the other hand, if we used for the , it is using more bits than necessary. Given that the whole purpose here was to avoid running out of RAM, we do not want to use up 8 times more memory than necessary. When you are working with large datasets, very often you will find that the slow piece is reading and writing to RAM, not the CPU operations. Also as a rule of thumb, smaller data types often will run faster particularly if you can use Single Instruction Multiple Data (SIMD) vectorized code, it can pack more numbers into a single vector to run at once.\n\nQuestion: Do we not have to shuffle the data anymore [20:11]? Although here I have read in the whole thing, when I start I never start by reading in the whole thing. By using a UNIX command , you can get a random sample of data at the command prompt and then you can just read that. This is a good way, for example, to find out what data types to use \u2014 read in a random sample and let Pandas figure it out for you. In general, I do as much work as possible on a sample until I feel confident that I understand the sample before I move on.\n\nTo pick a random line from a file using use the option. This limits the output to the number specified. You can also specify the output file:\n\n[21:28]\u2014 is a general purpose Python datatype which is slow and memory heavy. The reason for this is it is a boolean which also has missing values, so we need to deal with this before we can turn it into a boolean as you see below:\n\nPandas is generally fast, so you can summarize every column of all 125 million records in 20 seconds:\n\nQuestion: Wouldn\u2019t four years ago around the same time frame be important (e.g. around Christmas time)[25:06]? Exactly. It is not that there is no useful information from four years ago so we do not want to entirely throw it away. But as a first step, if you were to submit the mean, you would not submit the mean of 2012 sales, but probably want to submit the mean of last month\u2019s sale.And later on, we might want to weight more recent dates more highly since they are probably more relevant. But we should do bunch of exploratory data analysis to check that.\n\nHere is what the bottom of the data looks like [26:00].\n\nWe can add date part as usual. It takes a couple of minutes, so we should run through all this on sample first to make sure it works. Once you know everything is reasonable, then go back and run on a whole set.\n\nThese lines of code are identical to what we saw for bulldozers competition. We do not need to run or since all of the data types are already numeric (remember applies the same categorical codes to validation set as the training set) [27:59].\n\nCall to check the missing values and so forth.\n\nThese lines of code again are identical. Then there are two changes [28:48]:\n\nWe have learned about last week. We probably do not want to create a tree from 125 million records (not sure how long that will take). You can start with 10k or 100k and figure out how much you can run. There is no relationship between the size of the dataset and how long it takes to build the random forests \u2014 the relationship is between the number of estimators multiplied by the sample size.\n\nQuestion: What is ? In the past, it has always been [29:42]. The number of jobs is the number of cores to use. I was running this on a computer that has about 60 cores and if you try to use all of them, it spent so much time spinning out jobs and it was slower. If you have lots of cores on your computer, sometimes you want less ( means use every single core).\n\nAnother change was . This converts data frame into an array of floats and we fit it on that. Inside the random forest code, they do this anyway. Given that we want to run a few different random forests with a few different hyper parameters, doing this once myself saves 1 min 37 sec.\n\nIf you run a line of code that takes quite a long time, you can put in front.\n\nSo this got us 0.76 validation root mean squared log error.\n\nThis gets us down to 0.71 even though it took a little longer.\n\nThis brought this error down to 0.70. did not really help. So we have a \u201creasonable\u201d random forest here. But this does not give a good result on the leader board [33:42]. Why? Let\u2019s go back and see the data:\n\nThese are the columns we had to predict with (plus what were added by ). Most of the insight around how much of something you expect to sell tomorrow is likely to be wrapped up in the details about where the store is, what kind of things they tend to sell at the store, for a given item, what category of item it is. Random forest has no ability to do anything other than create binary splits on things like day of week, store number, item number. It does not know type of items or location of stores. Since its ability to understand what is going on is limited, we probably need to use the entire 4 years of data to even get some useful insights. But as soon as we start using the whole 4 years of data, a lot of the data we are using is really old. There is a Kaggle kernel that points out that what you could do is [35:54]:\n\nWe will talk about this in the next class, but if you can figure out how you start with that model and make it a little bit better, you will be above 30th place.\n\nQuestion: Could you try to capture seasonality and trend effects by creating new columns like average sales in the month of August [38:10]? It is a great idea. The thing to figure out is how to do it because there are details to get right and are difficult- not intellectually difficult but they are difficult in a way that makes you headbutt your desk at 2am [38:41].\n\nCoding you do for machine learning is incredibly frustrating and incredibly difficult. If you get a detail wrong, much of the time it is not going to give you an exception it will just silently be slightly less good than it otherwise would have been. If you are on Kaggle, you will know that you are not doing as well as other people. But otherwise you have nothing to compare against. You will not know if your company\u2019s model is half as good as it could be because you made a little mistake. This is why practicing on Kaggle now is great.\n\nEven for Jeremy, there is an extraordinary array of them. As you get to get to know what they are, you will start to know how to check for them as you go. You should assume every button you press, you are going to press the wrong button. That is fine as long as you have a way to find out.\n\nUnfortunately there is not a set of specific things you should always do, you just have to think what you know about the results of this thing I am about to do. Here is a really simple example. If you created that basic entry where you take the mean by date, by store number, by on promotion, you submitted it, and got a reasonable score. Then you think you have something that is a little bit better and you do predictions for that. How about you create a scatter plot showing the prediction of your average model on one axis versus the predictions of your new model on the other axis. You should see that they just about form a line. If they do not, then that is a very strong suggestion that you screwed something up.\n\nQuestion: How often do you pull in data from other sources to supplement dataset you have [41:15]? Very often. The whole point of star schema is that you have a centric table, and you have other tables coming off it that provide metadata about it. On Kaggle, most competitions have the rule that you can use external data as long as post on the forum and is publicly available (double check the rule!). Outside of the Kaggle, you should always be looking for what external data you could possibly leverage.\n\nQuestion: How about adding Ecuador\u2019s holidays to supplement the data? [42:52] That information is actually provided. In general, one way of tackling this kind of problem is to create lots of new columns containing things like average number of sales on holidays, average percent change in sale between January and February, etc. There has been a pervious competition for a grocery chain in Germany that was almost identical. The person who won was a domain expert and specialist in doing logistics predictions. He created lots of columns based on his experience of what kinds of things tend to be useful for making predictions. So that is an approach that can work. The third place winner did almost no feature engineering, however, and they also had one big oversight which may have cost them the first place win. We will be learning a lot more about how to win this competition and ones like it as we go.\n\nIf you do not have a good validation set, it is hard, if not possible, to create a good model. If you are trying to predict next month\u2019s sales and you build models. If you have no way of knowing whether the models you have built are good at predicting sales a month ahead of time, then you have no way of knowing whether it is actually going to be any good when you put your model in production. You need a validation set that you know is reliable at telling you whether or not your model is likely to work well when you put it in production or use it on the test set.\n\nNormally you should not use your test set for anything other than using it right at the end of the competition or right at the end of the project to find out how you did. But there is one thing you can use the test set for in addition \u2014 that is to calibrate your validation set [46:02].\n\nWhat Terrance did here was that he built four different models and submitted each of the four models to Kaggle to find out its score. X-axis is the score Kaggle told us on the leaderboard, and y-axis he plotted the score on a particular validation set he was trying out to see whether the validation set was going to be any good. If your validation set is good, then the relationship between the leaderboards score (i.e. the test set score) should lie in a straight line. Ideally, it will lie on the line, but honestly that does not matter too much as long as relatively speaking it tells you which models are better than which other models, then you know which model is the best. In this case, Terrance has managed to come up with a validation set which looks like it is going to predict the score well. That is really cool because he can go away and try a hundred different types of models, feature engineering, weighting, tweaks, hyper parameters, whatever else, see how they go on the validation set, and not have to submit to Kaggle. So you will get a lot more iterations, a lot more feedback. This is not just true for Kaggle but every machine learning project you do. In general, if your validation set is not showing nice fit line, you need think carefully [48:02]. How is the test set constructed? How is my validation set different? You will have to draw lots of charts and so forth to find out.\n\nQuestion: How do you construct a validation set as close to the test set [48:23]? Here are a few tips from Terrance:\n\nWe start by reading in our feather files for Blue Books for Bulldozers competition. Reminder: we have already read in the CSV, processed it into categories, and save it in feather format. The next thing we do is call to turn categories into integers, deal with missing values, and pull out the dependent variable. Then create a validation set just like last week:\n\nLast week, there was a bug in that was shuffling the dataframe when gets passed in hence causing the validation set to be not the latest 12000 records. This issue was fixed.\n\nQuestion: Why is both input and out put of this function [53:03]? returns a dictionary telling you which columns were missing and for each of those columns what the median was.\n\nOnce we have done , this is what it looks like. is the log of the sale price."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-2-d9aebd7dd0b0?source=user_profile---------6----------------",
        "title": "Machine Learning 1: Lesson 2 \u2013 Hiromi Suenaga \u2013",
        "text": "This small deterministic tree has R\u00b2 of 0.4028 after fitting so this is not a good model but better than the mean model since it is greater than 1 and we can actually draw [ 33:00 ]:\n\nWe are going to build a forest made of trees. Let\u2019s start by looking at trees. In scikit-learn, they do not call them trees but estimators .\n\nAs you see above, when calling split_vals , we do not put the result to a validation set. _ indicates that we are throwing away the return value. We want to keep validation set the same all the time.\n\nOne way to speed things up is to pass in the subset parameter to proc_df which will randomly sample the data:\n\nIf you put %time , it will tell you how long things took. The rule of thumb is that if something takes more than 10 seconds to run, it is too long to do interactive analysis with it. So what we do is we try to make sure that things can run in a reasonable time. And then when we are finished at the end of the day, we can say ok, this feature engineering, these hyper parameters, etc are all working well, and we will now re-run it the big slow precise way.\n\nThis is one of these examples where the code does not follow PEP8. Being able to look at something in one go with your eyes and over time learn to immediately see what is going on has a lot of values. Also consistently use particular letters or abbreviation to mean particular things works well in data science. But if you are doing take-home interview test, follow PEP8 standard.\n\nQuestion : Wouldn\u2019t it eventually overfit to the validation set? [ 25:30 ] Yes, actually that is the issue. That would eventually have the possibility of overfitting on the validation set and when you try it on the test set or submit it to Kaggle, it turns out not to be very good. This happens in Kaggle competitions all the time and they actually have a fourth dataset which is called the private leader board set. Every time you submit to Kaggle, you actually only get feedback on how well it does on the public leader board set and you do not know which rows they are. At the end of the competition, you get judged on a different dataset entirely called the private leader board set. The only way to avoid this is to actually be a good machine learning practitioner and know how to set these parameters as effectively as possible which we are going to be doing partly today and over the next few weeks.\n\nQuestion : Why not choose random set of rows as a validation set[ 24:19 ]? Because if we did that, we would not be replicating the test set. If you actually look at the dates in the test set, they are a set of dates that are more recent than any date in the training set. So if we used a validation set that was a random sample, that is much easier because we are predicting the value of a piece of industrial equipment on this day when we already have some observations from that day. In general, anytime you are building a model that has a time element, you want your test set to be a separate time period and therefore you really need your validation set to be of separate time period as well.\n\nAs you see, R\u00b2 is .982 on the training set, and only .887 on the validation set which makes us think that we are overfitting quite badly. But not too badly as RMSE of 0.25 would have put us in the top 25% of the competition.\n\nQuestion : We have converted categorical variable into numbers but other models convert it to different columns using one hot encoding \u2014 which approach should we use [ 22:55 ]? We are going to tackle that today.\n\nYou must actually remove the second holdout set (test set) from the data, give it to somebody else, and tell them not let you look at it until you promise you are finished. Otherwise it is so hard not to look at it. In the world of psychology and sociology, it is known as replication crisis or P-hacking. That is why we want to have a test set.\n\nQuestion : Could you explain the difference between a validation set and a test set [ 20:58 ]? One of the things we are going to learn today is how to set hyper parameters. Hyper parameters are tuning parameters that are going to change how your model behaves. If you just have one holdout set (i.e. one set of data that you are not using to train with) and we use that to decide which set of hyper parameter to use. If we try a thousand different sets of hyper parameters, we may end up overfitting to that holdout set. So what we want to do is to have a second holdout set (the test set) where we can say I have done the best I can and now just once right at the end, I am going to see whether it works.\n\nWe now have something which hopefully looks like Kaggle test set \u2014 close enough that using this would give us reasonably accurate scores. The reason we want this is because on Kaggle, you can only submit so many times and if you submit too often, you will end up fitting to the leaderboard anyway. In real life, we want to build a model that is going to work well in the production.\n\nIf your dataset has a time piece in it (as is in Blue Book competition), you would likely want to predict future prices/values/etc. What Kaggle did was to give us data representing a particular date range in the training set, and then the test set presented a future set of dates that wasn\u2019t represented in the training set. So we need to create a validation set that has the same properties:\n\nCreating a validation set is the most important thing you need to do when you are doing a machine learning project. What you need to do is to come up with a dataset where the score of your model on that dataset is going to be representative of how well your model is going to do in the real world.\n\nIt is good at running through points we gave it, but it is not going to be very good at running through points we didn\u2019t give it. That is why we always want to have a validation set.\n\nIn our case, R\u00b2= 0.98 is a very good model. However, it might be the case that it looks like the one on the right:\n\nR\u00b2 is the ratio between how good your model is (RMSE)vs. how good is the na\u00efve mean model (RMSE).\n\nR\u00b2 is not necessarily what you are actually trying to optimize, but it is a number you can use for every model and you can start to get a feel of what .8 looks like or what .9 looks like. Something you may find interesting is to create synthetic 2D datasets with different amounts of random noise, and see what they look like on a scatterplot and their R\u00b2 to get a feel of how close they are to the actual value.\n\nSo when your R\u00b2 is negative, it means your model is worse than predicting the mean.\n\nThen we made everything in the dataset to numbers by doing the following:\n\nFastai library is a collections of best techniques to achieve state-of-the-art result. For structured data analysis, scikit-learn has a lot of great code. So what fastai does is to help us get things into scikit-learn and then interpret things out from scikit-learn.\n\nWe want to start building a random forest from scratch [36:28]. The first step is to create a tree. The first step to create a tree is to create the first binary decision. How are you going to do it?\n\nWe now have a single number that represents how good a split is which is the weighted average of the mean squared errors of the two groups that creates [42:13]. We also have a way to find the best split which is to try try every variable and to try every possible value of that variable and see which variable and which value gives us a split with the best score.\n\nQuestion: Are there circumstances when it is better to split into 3 groups [45:12]? It is never necessary to do more than one split at a level because you can just split them again.\n\nThis is the entirety of creating a decision tree. Stopping condition:\n\nRight now, our decision tree has R\u00b2 of 0.4. Let\u2019s make it better by removing . By doing so, the training R\u00b2 becomes 1 (as expected since each leaf node contains exactly one element) and validation R\u00b2 is 0.73 \u2014 which is better than the shallow tree but not as good as we would like.\n\nTo make these trees better, we will create a forest. To create a forest, we will use a statistical technique called bagging.\n\nMichael Jordan developed a technique called the Bag of Little Bootstraps in which he shows how to use bagging for absolutely any kind of model to make it more robust and also to give you confidence intervals.\n\nSo what is bagging? Bagging is an interesting idea which is what if we created five different models each of which was only somewhat predictive but the models gave predictions that were not correlated with each other. That would mean that the five models would have profound different insights into the relationships in the data. If you took the average of those five models, you are effectively bringing in the insights from each of them. So this idea of averaging models is a technique for Ensembling.\n\nWhat if we created a whole a lot of trees \u2014 big, deep, massively overfit trees but each one, let\u2019s say, we only pick a random 1/10 of the data. Let\u2019s say we do that a hundred times (different random sample every time). They are overfitting terribly but since they are all using different random samples, they all overfit in different ways on different things. In other words, they all have errors but the errors are random. The average of a bunch of random errors is zero. If we take the average of these trees each of which have been trained on a different random subset, the error will average out to zero and what is left is the true relationship \u2014 and that\u2019s the random forest.\n\nby default is 10 (remember, estimators are trees).\n\nQuestion: Are you saying we average 10 crappy models and we get a good model? [51:25] Exactly. Because the crappy models are based on different random subsets and their errors are not correlated with each other. If the errors were correlated, this will not work.\n\nThe number of trees to use is the first of our hyper parameters we are going to tune to achieve higher metric.\n\nQuestion: The subset you are selecting, are they exclusive? Can there be overlaps? [52:27] We talked about picking 1/10 at random, but what scikit-learn does by default is for n rows, it picks out n rows with replacement \u2014 which is called bootstrapping. If memory serves correctly, on average, 63.2% of the rows will be represented and many of them will be appear multiple times.\n\nThe entire purpose of modeling in machine learning is to find a model which tells you which variables are important and how do they interact together to drive your dependent variable. In practice, the difference between using the random forest space to find your nearest neighbors vs. Euclidean space is the difference between a model that makes good predictions and the model that makes meaningless predictions.\n\nThe effective machine learning model is accurate at finding the relationships in the training data and generalizes well to new data [55:53]. In bagging, that means that each of your individual estimators, you want them to be as predictive as possible but for the predictions of your individual trees to be as uncorrelated as possible. The research community found that the more important thing seems to be creating uncorrelated trees rather than more accurate trees. In scikit-learn, there is another class called which is an extremely randomized tree model. Rather than trying every split of every variable, it randomly tries a few splits of a few variables which makes training much faster and it can build more trees \u2014 better generalization. If you have crappy individual models, you just need more trees to get a good end model.\n\nEach tree is stored in an attribute called . For each tree, we will call with our validation set. concatenates them together on a new axis, so the resulting has the shape of (10 trees, 12000 validation set). The mean of 10 predictions for the first data is 9.07, and the actual value is 9.10. As you can see, none of the individual prediction is close to 9.10, but the mean ends up pretty good.\n\nHere is a plot of R\u00b2 values given first i trees. As we add more trees, R\u00b2 improves. But it seems as though it has flattened out.\n\nAs you see, adding more trees do not help much. It will not get worse but it will stop improving things much. This is the first hyper parameter to learn to set \u2014 a number of estimators. A method of setting is, as many as you have time to fir and that actually seems to be helping.\n\nAdding more trees slows it down, but with less trees you can still get the same insights. So when Jeremy builds most of his models, he starts with 20 or 30 trees and at the end of the project or at the end of the day\u2019s work, he will use 1000 trees and run it over night.\n\nSometimes your dataset will be small and you will not want to pull out a validation set because doing so means you now do not have enough data to build a good model. However, random forests have a very clever trick called out-of-bag (OOB) error which can handle this (and more!)\n\nWhat we could do is to recognize that in our first tree, some of the rows did not get used for training. What we could do is to pass those unused rows through the first tree and treat it as a validation set. For the second tree, we could pass through the rows that were not used for the second tree, and so on. Effectively, we would have a different validation set for each tree. To calculate our prediction, we would average all the trees where that row is not used for training. If you have hundreds of trees, it is very likely that all of the rows are going to appear many times in these out-of-bag samples. You can then calculate RMSE, R\u00b2, etc on these out-of-bag predictions.\n\nSetting to true will do exactly this and create an attribute called to the model and as you see in the print_score function, if it has this attributes, it will print it out at the end.\n\nQuestion: Wouldn\u2019t always lower than the one for the entire forest [1:12:51]? The accuracy tends to be lower because each row appears in less trees in the OOB samples than it does in the full set of trees. So OOB R\u00b2 will slightly underestimate how generalizable the model is, but the more trees you add, the less serious that underestimation is.\n\nOOB score will come in handy when setting hyper parameters [1:13:47]. There will be quite a few hyper parameters that we are going to set and we would like to find some automated say to set them. One way to do that is to do grid search. Scikit-learn has a function called grid search and you pass in a list of all the hyper parameters you want to tune and all of the values of these hyper parameters you want to try. It will run your model on every possible combination of all these hyper parameters and tell you which one is the best. OOB score is a great choice for getting it to tell you which one is the best.\n\nEarlier, we took 30,000 rows and created all the models which used a different subset of that 30,000 rows. Why not take a totally different subset of 30,000 each time? In other words, let\u2019s leave the entire 389,125 records as is, and if we want to make things faster, pick a different subset of 30,000 each time. So rather than bootstrapping the entire set of rows, just randomly sample a subset of the data.\n\n: Just as before, we use 20,000 of them in our training set (before it was out of 30,000, this time it is out of 389,125).\n\nThis will take the same amount of time to run as before, but every tree has an access to the entire dataset. After using 40 estimators, we get the R\u00b2 score of 0.876.\n\nQuestion: What samples is this OOB score calculated on [1:18:26]? Scikit-learn does not support this out of box, so is a custom function. So OOB score needs to be turned off when using as they are not compatible. will turn it back to the way it was.\n\nThe biggest tip [1:20:30]: Most people run all of their models on all of the data all of the time using their best possible parameters which is just pointless. If you are trying to find out which feature is important and how they are related to each other, having that 4th decimal place of accuracy is not going to change any of your insights at all. Do most of your models on a large enough sample size that your accuracy is reasonable (within a reasonable distance of the best accuracy you can get) and taking a small number of seconds to train so that you can interactively do your analysis.\n\nLet\u2019s get a baseline for this full set to compare to:\n\nHere OOB is higher than validation set. This is because our validation set is a different time period whereas OOB samples are random. It is much harder to predict a different time period.\n\nThe RMSLE of 0.2286 would get us to the top 20 of this competition \u2014 with brainless random forest with some brainless minor hyper parameter tuning. This is why Random Forest is such an important not just first step but often only step of machine learning. It is hard to screw up.\n\nLet\u2019s take a look at one of the split point in the small single tree.\n\nWhy does this even work? Imagine the only thing that mattered was and nothing else mattered. It can pick out a single element by first splitting then . With just two splits, we can pull out a single category. Tree is infinitely flexible even with a categorical variable. If there is a particular category which has different level of price, it can gradually zoom in on those group by using multiple splits. Random forest is very easy to use and very resilient.\n\nNext lesson, we will learn about how to analyze the model to learn more about the data to make it even better."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-1-84a1dc2b5236?source=user_profile---------7----------------",
        "title": "Machine Learning 1: Lesson 1 \u2013 Hiromi Suenaga \u2013",
        "text": "Depending on time and class interests, we\u2019ll cover something like (not necessarily in this order):\n\nWhat is ML? Why do we use it?\n\nData science \u2260Software engineering [08:43]. You will see code that does not follow PEP 8 and things like , but go along with it for a while. What we are doing right now is prototyping models, and prototyping models has a very different set of best practices that are taught nowhere. The key is to be able to do things very interactively and iteratively. Jupyter notebook makes this easy. If you ever wondered what is, you can do one of the three things:\n\nEntering to Kaggle competition will let you know whether you are competent at this kind of data in this kind of model. Is the accuracy bad because the the data is so noisy that you cannot do better? Or is it actually an easy dataset and you have made a mistake? When you are working on your own project with your own dataset, you will not get this kind of feedback \u2014 we just have to know that we have good effective techniques to reliably building baseline model.\n\nMachine learning should help us understand a dataset, not just make predictions about it [15:36]. So by picking an area which we are not familiar with, it is a good test of whether we can build an understanding. Otherwise what can happen is that your intuition about the data can make it very difficult for you to be open-minded enough to see what the data really says.\n\nThere are a few options to download the data:\n\nJupyter trick [21:39] \u2014 you can open web-based terminal like so:\n\nThe goal of this competition is to use the training set which contains data through the end of 2011 to predict the sale price of bulldozers.\n\nLet\u2019s look at the data[25:25]:\n\nStructured data: Columns representing a wide range of different types of things such as identifier, currency, date, size.\n\nis the most important library when you are working with structured data which is usually imported as .\n\nIn Jupyter Notebook, if you type a variable name and press whether that being Dataframe, video, HTML, etc \u2014 it will generally figure out a way of displaying it for you [32:13].\n\nThe variable we want to predict is called Dependent Variable in this case our dependent variable is\n\nQuestion: Should you never look at the data because of the risk of overfit? [33:08] We want to find out at least enough to know that we have managed to imported okay, but tend not to really study it at all at this point, because we do not want to make too many assumptions about it. Many books say to do a lot of exploratory data analysis (EDA) first. We will learn machine learning driven EDA today.\n\nRoot mean squared log error. The reason we use log is because generally, you care not so much about missing by $10 but missing by 10%. So if it was $1000,000 item and you are $100,000 off or if it was a $10,000 item and you are $1,000 off \u2014 we would consider those equivalent scale issues.\n\nQuestion: What about a curse of dimensionality? [38:16] There are two concepts you often hear \u2014 curse of dimensionality and no free lunch theorem. They are both largely meaningless and basically stupid and yet many people in the field not only know that but think the opposite so it is well worth explaining. The curse of dimensionality is this idea that the more columns you have, it creates a space that is more and more empty. There is this fascinating mathematical idea that the more dimensions you have, the more all of the points sit on the edge of that space. If you just have a single dimension where things are random, then they are spread out all over. Where else, if it is a square then the probability that they are in the middle means that they cannot be on the edge of either dimension so it is a little less likely that they are not on the edge. Each dimension you add, it becomes multiplicatively less likely that the point is not on the edge of at least one dimension, so in high dimensions, everything sits on the edge. What that means in theory is that the distance between points is much less meaningful. So if we assume it matters, then it would suggest that when you have lots of columns and you just use them without being careful to remove the ones you do not care about that things will not work. This turns out not to be the case for number of reasons\n\nMost popular and important package for machine learning in Python. It is not the best at everything (e.g. XGBoost is better than Gradient Boosting Tree), but pretty good at nearly everything.\n\nEverything in scikit-learn has the same form.\n\nThe above code will result in an error. There was a value inside the dataset \u201cConventional\u201d, and it did not know how to create a model using that String. We have to pass numbers to most machine learning models and certainly to random forests. So step 1 is to convert everything into numbers.\n\nThis dataset contains a mix of continuous and categorical variables.\n\nHere are some of the information we can extract from date \u2014 year, month, quarter, day of month, day of week, week of year, is it a holiday? weekend? was it raining? was there a sport event that day? It really depends on what you are doing. If you are predicting soda sales in SoMa, you would probably want to know if there was a San Francisco Giants ball game that day. What is in a date is one of the most important piece of feature engineering you can do and no machine learning algorithm can tell you whether the Giants were playing that day and that it was important. So this is where you need to do feature engineering.\n\nThe method extracts particular date fields from a complete datetime for the purpose of constructing categoricals. You should always consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can\u2019t capture any trend/cyclical behavior as a function of time at any of these granularities.\n\nQuestion: [55:40] What is the difference between and ? It is safer to use square brackets especially when assigning values and there is a possibility that the column does not already exist.\n\nAfter running , it added many numerical columns and removed column. This is not quite enough to get passed the error we saw earlier as we still have other columns that contain string values. Pandas has a concept of a category data type, but by default it would not turn anything into a category for you. Fast.ai provides a function called which creates categorical variables for everything that is a String. Behind the scenes, it creates a column that is an integer and it is going to store a mapping from the integers to the strings. is called \u201ctrain\u201d because it is training data specific. It is important that validation and test sets will use the same category mappings (in other words, if you used 1 for \u201chigh\u201d for a training dataset, then 1 should also be for \u201chigh\u201d in validation and test datasets). For validation and test dataset, use instead.\n\nThe order does not matter too much, but since we are going to be creating a decision tree that split things at a single point (i.e. vs. and , and vs. ) which is a little bit weird. To order them in a sensible manner, you can do the following:\n\nThere is a kind of categorical variable called \u201cordinal\u201d. An ordinal categorical variable has some kind of order (e.g. \u201cLow\u201d < \u201cMedium\u201d < \u201cHigh\u201d). Random forests are not terribly sensitive for that fact, but it is worth noting.\n\nThe above will add a number of empty values for each series, we sort them by the index ( ), and divide by a number of dataset.\n\nReading CSV took about 10 seconds, and processing took another 10 seconds, so if we do not want to wait again, it is a good idea to save them. Here we will save it in a feather format. What this is going to do is to save it to disk in exactly the same basic format that it is in RAM. This is by far the fastest way to save something, and also to read it back. Feather format is becoming standard in not only Pandas but in Java, Apache Spark, etc.\n\nWe can read it back as so:\n\nWe will replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-4-2048a26d58aa?source=user_profile---------8----------------",
        "title": "Deep Learning 2: Part 1 Lesson 4 \u2013 Hiromi Suenaga \u2013",
        "text": "For minor numerical precision reasons, it turns out to be better to tahe the log of the softmax than softmax directly [ 15:03 ]. That is why when we get predictions out of our models, we have to do np.exp(log_preds) .\n\n(7): Softmax \u2014 The activation function that returns numbers that adds up to 1 and each of them is between 0 and 1:\n\n(6): Linear \u2014 the second linear layer that takes those 512 activations from the previous linear layer and put them through a new matrix multiply 512 by 120 and outputs 120 activations\n\n(2): Linear layer simply means a matrix multiply. This is a matrix which has 1024 rows and 512 columns, so it will take in 1024 activations and spit out 512 activations.\n\nlearn \u2014 This will display the layers we added at the end. These are the layers we train when precompute=True\n\nQuestion : Do we need to adjust the learning rate after adding dropouts?[ 24:33 ] It does not seem to impact the learning rate enough to notice. In theory, it might but not enough to affect us.\n\nQuestion : Why monitor loss and not accuracy? [ 23:53 ] Loss is the only thing that we can see for both the validation set and the training set. As we learn later, the loss is the thing that we are actually optimizing so it is easier to monitor and understand what that means.\n\nQuestion : Can we have different level of dropout by layer? [ 22:41 ] Yes, that is why it is called ps :\n\nQuestion : Why does average activation matter? [ 21:15 ] If we just deleted a half of activations, the next activation who takes them as input will also get halved, and everything after that. For example, fluffy ears are fluffy if this is greater than 0.6, and now it is only fluffy if it is greater than 0.3 \u2014 which is changing the meaning. The goal here is delete activations without changing the meanings.\n\nQuestion : Is there a particular way in which you can determine if it is overfitted? [ 19:53 ]. Yes, you can see the training loss is much lower than the validation loss. You cannot tell if it is too overfitted. Zero overfitting is not generally optimal. The only thing you are trying to do is to get the validation loss low, so you need to play around with a few things and see what makes the validation loss low. You will get a feel for it overtime for your particular problem what too much overfitting looks like.\n\nYou may have noticed, it has been adding two Linear layers [ 16:19 ]. We do not have to do that. There is xtra_fc parameter you can set. Note: you do need at least one which takes the output of the convolutional layer (4096 in this example) and turns it into the number of classes (120 dog breeds):\n\nIn Fast.ai, you can pass in ps which is the p value for all of the added layers. It will not change the dropout in the pre-trained network since it should have been already trained with some appropriate level of dropout:\n\nQuestion : Do you have to do anything to accommodate for the fact that you are throwing away activations? [ 13:26 ] We do not, but PyTorch does two things when you say p=0.5 . It throws away half of the activations, and it doubles all the activations that are already there so that average activation does not change.\n\nHave you wondered why the validation losses better than the training losses particularly early in the training? [ 12:32 ] This is because we turn off dropout when we run inference (i.e. making prediction) on the validation set. We want to be using the best model we can.\n\nThis has been absolutely critical in making modern deep learning work and just about solve the problem of generalization. Geoffrey Hinton and his colleagues came up with this idea loosely inspired by the way the brain works.\n\nRandomly throwing away half of the activations in a layer has an interesting effect. An important thing to note is for each mini-batch, we throw away a different random half of activations in that layer. It forces it to not overfit. In other words, when a particular activation that learned just that exact dog or exact cat gets dropped out, the model has to try and find a representation that continues to work even as random half of the activations get thrown away every time.\n\nIf we applied dropout with p=0.5 to Conv2 layer, it would look like the above. We go through, pick an activation, and delete it with 50% chance. So p=0.5 is the probability of deleting that cell. Output does not actually change by very much, just a little bit.\n\nQuestion : If you are using year as a category, what happens when a model encounters a year it has never seen before? [ 31:47 ] We will get there, but the short answer is that it will be treated as an unknown category. Pandas has a special category called unknown and if it sees a category it has not seen before, it gets treated as unknown.\n\nQuestion : Do you ever bin continuous variables?[ 31:02 ] Jeremy does not bin variables but one thing we could do with, say max temperature, is to group into 0\u201310, 10\u201320, 20\u201330, and call that categorical. Interestingly, a paper just came out last week in which a group of researchers found that sometimes binning can be helpful.\n\nHere is what our data looks like. Even though we set some of the columns as \u201ccategory\u201d (e.g. \u2018StoreType\u2019, \u2018Year\u2019), Pandas still display as string in the notebook.\n\nAfter processing, year 2014 for example becomes 2 since categorical variables have been replaced with contiguous integers starting at zero. The reason for that is, we are going to put them into a matrix later, and we would not want the matrix to be 2014 rows long when it could just be two rows.\n\nNow we have a data frame which does not contain the dependent variable and where everything is a number. That is where we need to get to to do deep learning. Check out Machine Learning course on further details. Another thing that is covered in Machine Learning course is validation sets. In this case, we need to predict the next two weeks of sales therefore we should create a validation set which is the last two weeks of our training set:\n\nFor any Kaggle competitions, it is important that you have a strong understanding of your metric \u2014 how you are going to be judged. In this competition, we are going to be judged on Root Mean Square Percentage Error (RMSPE).\n\nNow we have a standard model data object which we are familiar with and contains , , , , etc.\n\nRemember, you never want to put ReLU in the last layer because softmax needs negatives to create low probabilities.\n\nFor regression problems (not classification), you can even skip the softmax layer.\n\nWe create a new matrix of 7 rows and as many columns as we choose (4, for example) and fill it with floating numbers. To add \u201cSunday\u201d to our rank 1 tensor with continuous variables, we do a look up to this matrix, which will return 4 floating numbers, and we use them as \u201cSunday\u201d.\n\nInitially, these numbers are random. But we can put them through a neural net and update them in a way that reduces the loss. In other words, this matrix is just another bunch of weights in our neural net. And matrices of this type are called \u201cembedding matrices\u201d. An embedding matrix is something where we start out with an integer between zero and maximum number of levels of that category. We index into the matrix to find a particular row, and we append it to all of our continuous variables, and everything after that is just the same as before (linear \u2192 ReLU \u2192 etc).\n\nQuestion: What do those 4 numbers represent?[55:12] We will learn more about that when we look at collaborative filtering, but for now, they are just parameters that we are learning that happen to end up giving us a good loss. We will discover later that these particular parameters often are human interpretable and quite interesting but that a side effect.\n\nQuestion: Do you have good heuristics for the dimensionality of the embedding matrix? [55:57] I sure do! Let\u2019s take a look.\n\nThen pass the embedding size to the learner:\n\nQuestion: Is there a way to initialize embedding matrices besides random? [58:14] We will probably talk about pre-trained more later in the course, but the basic idea is if somebody else at Rossmann had already trained a neural network to predict cheese sales, you may as well start with their embedding matrix of stores to predict liquor sales. This is what happens, for example, at Pinterest and Instacart. Instacart uses this technique for routing their shoppers, and Pinterest uses it for deciding what to display on a webpage. They have embedding matrices of products/stores that get shared in the organization so people do not have to train new ones.\n\nQuestion: What is the advantage of using embedding matrices over one-hot-encoding? [59:23] For the day of week example above, instead of the 4 numbers, we could have easily passed 7 numbers (e.g. [0, 1, 0, 0, 0, 0, 0] for Sunday). That also is a list of floats and that would totally work \u2014 and that is how, generally speaking, categorical variables have been used in statistics for many years (called \u201cdummy variables\u201d). The problem is, the concept of Sunday could only ever be associated with a single floating-point number. So it gets this kind of linear behavior \u2014 it says Sunday is more or less of a single thing. With embeddings, Sunday is a concept in four dimensional space. What we tend to find happen is that these embedding vectors tend to get these rich semantic concepts. For example, if it turns out that weekends have a different behavior, you tend to see that Saturday and Sunday will have some particular number higher.\n\nThe idea of an embedding is what is called a \u201cdistributed representation\u201d \u2014 the most fundamental concept of neural networks. This is the idea that a concept in neural network has a high dimensional representation which can be hard to interpret. These numbers in this vector does not even have to have just one meaning. It could mean one thing if this is low and that one is high, and something else if that one is high and that one is low because it is going through this rich nonlinear function. It is this rich representation that allows it to learn such interesting relationships.\n\nQuestion: Are embeddings suitable for certain types of variables? [01:02:45] Embedding is suitable for any categorical variables. The only thing it cannot work well for would be something with too high cardinality. If you had 600,000 rows and a variable had 600,000 levels, that is just not a useful categorical variable. But in general, the third winner in this competition really decided that everything that was not too high cardinality, they put them all as categorical. The good rule of thumb is if you can make a categorical variable, you may as well because that way it can learn this rich distributed representation; where else if you leave it as continuous, the most it can do is to try and find a single functional form that fits it well.\n\nLooking up an embedding with an index is identical to doing a matrix product between a one-hot encoded vector and the embedding matrix. But doing so is terribly inefficient, so modern libraries implement this as taking an integer and doing a look up into an array.\n\nQuestion: Could you touch on using dates and times as categorical and how that affects seasonality? [01:06:59] There is a Fast.ai function called which takes a data frame and a column name. It optionally removes the column from the data frame and replaces it with lots of column representing all of the useful information about that date such as day of week, day of month, month of year, etc (basically everything Pandas gives us).\n\nSo for example, day of week now becomes eight rows by four columns embedding matrix. Conceptually this allows our model to create some interesting time series models. If there is something that has a seven day period cycle that goes up on Mondays and down on Wednesdays but only for daily and only in Berlin, it can totally do that \u2014 it has all the information it needs. This is a fantastic way to deal with time series. You just need to make sure that the cycle indicator in your time series exists as a column. If you did not have a column called day of week, it would be very difficult for the neural network to learn to do mod seven and look up in an embedding matrix. It is not impossible but really hard. If you are predicting sales of beverages in San Francisco, you probably want a list of when the ball game is on at AT&T park because that is going to to impact how many people are drinking beer in SoMa. So you need to make sure that the basic indicators or periodicity is in your data, and as long as they are there, neural net is going to learn to use them.\n\nBy using all of the training data, we achieved a RMSPE around 0.09711. There is a big difference between public leader board and private leader board, but we are certainly in the top end of this competition.\n\nSo this is a technique for dealing with time series and structured data. Interestingly, compared to the group that used this technique (Entity Embeddings of Categorical Variables), the second place winner did way more feature engineering. The winners of this competition were actually subject matter experts in logistics sales forecasting so they had their own code to create lots and lots of features. Folks at Pinterest who build a very similar model for recommendations also said that when they switched from gradient boosting machines to deep learning, they did way less feature engineering and it was much simpler model which requires less maintenance. So this is one of the big benefits of using this approach to deep learning \u2014 you can get state of the art results but with a lot less work.\n\nQuestion: Are we using any time series in any of these? [01:15:01] Indirectly, yes. As we just saw, we have a day of week, month of year, etc in our columns and most of them are being treated as categories, so we are building a distributed representation of January, Sunday, and so on. We are not using any classic time series techniques, all we are doing is true fully connected layers in a neural net. The embedding matrix is able to deal with things like day of week periodicity in a much richer way than than any standard time series techniques.\n\nQuestion regarding the difference between image models and this model [01:15:59]: There is a difference in a way we are calling . In imaging we just did and pass the data:\n\nFor these kinds of models, in fact for a lot of the models, the model we build depends on the data. In this case, we need to know what embedding matrices we have. So in this case, the data objects creates the learner (upside down to what we have seen before):\n\nSummary of steps (if you want to use this for your own dataset) [01:17:56]:\n\nStep 1. List categorical variable names, and list continuous variable names, and put them in a Pandas data frame\n\nStep 2. Create a list of which row indexes you want in your validation set\n\nStep 4. Create a list of how big you want each embedding matrix to be\n\nStep 5. Call \u2014 you can use these exact parameters to start with:\n\nQuestion: How to use data augmentation for this type of data, and how does dropout work? [01:18:59] No idea. Jeremy thinks it has to be domain-specific, but he has never seen any paper or anybody in industry doing data augmentation with structured data and deep learning. He thinks it can be done but has not seen it done. What dropout is doing is exactly the same as before.\n\nQuestion: What is the downside? Almost no one is using this. Why not? [01:20:41] Basically the answer is as we discussed before, no one in academia almost is working on this because it is not something that people publish on. As a result, there have not been really great examples people could look at and say \u201coh here is a technique that works well so let\u2019s have our company implement it\u201d. But perhaps equally importantly, until now with this Fast.ai library, there has not been any way to do it conveniently. If you wanted to implement one of these models, you had to write all the custom code yourself. There are a lot of big commercial and scientific opportunity to use this and solve problems that previously haven\u2019t been solved very well.\n\nThe most up-and-coming area of deep learning and it is two or three years behind computer vision. The state of software and some of the concepts is much less mature than it is for computer vision. One of the things you find in NLP is there are particular problems you can solve and they have particular names. There is a particular kind of problem in NLP called \u201clanguage modeling\u201d and it has a very specific definition \u2014 it means build a model where given a few words of a sentence, can you predict what the next word is going to be.\n\nHere we have 18 months worth of papers from arXiv (arXiv.org) and this is an example:\n\nHere are what the output of a trained language model looks like. We did simple little tests in which you pass some priming text and see what the model thinks should come next:\n\nIt learned by reading arXiv papers that somebody who is writing about computer networking would talk like this. Remember, it started out not knowing English at all. It started out with an embedding matrix for every word in English that was random. By reading lots of arXiv papers, it learned what kind of words followed others.\n\nHere we tried specifying a category to be computer vision:\n\nIt not only learned how to write English pretty well, but also after you say something like \u201cconvolutional neural network\u201d you should then use parenthesis to specify an acronym \u201c(CNN)\u201d.\n\nA language model can be incredibly deep and subtle, so we are going to try and build that \u2014 not because we care about this at all, but because we are trying to create a pre-trained model which is used to do some other tasks. For example, given an IMDB movie review, we will figure out whether they are positive or negative. It is a lot like cats vs. dogs \u2014 a classification problem. So we would really like to use a pre-trained network which at least knows how to read English. So we will train a model that predicts a next word of a sentence (i.e. language model), and just like in computer vision, stick some new layers on the end and ask it to predict whether something is positive or negative.\n\nWhat we are going to do is to train a language model, making that the pre-trained model for a classification model. In other words, we are trying to leverage exactly what we learned in our computer vision which is how to do fine-tuning to create powerful classification models.\n\nQuestion: why would doing directly what you want to do not work? [01:31:34] It just turns out it doesn\u2019t empirically. There are several reasons. First of all, we know fine-tuning a pre-trained network is really powerful. So if we can get it to learn some related tasks first, then we can use all that information to try and help it on the second task. The other is IMDB movie reviews are up to a thousands words long. So after reading a thousands words knowing nothing about how English is structured or concept of a word or punctuation, all you get is a 1 or a 0 (positive or negative). Trying to learn the entire structure of English and then how it expresses positive and negative sentiments from a single number is just too much to expect.\n\nQuestion: Is this similar to Char-RNN by Karpathy? [01:33:09] This is somewhat similar to Char-RNN which predicts the next letter given a number of previous letters. Language model generally work at a word level (but they do not have to), and we will focus on word level modeling in this course.\n\nQuestion: To what extent are these generated words/sentences actual copies of what it found in the training set? [01:33:44] Words are definitely words it has seen before because it is not a character level so it can only give us the word it has seen before. Sentences, there are rigorous ways of doing it but the easiest would be by looking at examples like above, you get a sense of it. Most importantly, when we train the language model, we will have a validation set so that we are trying to predict the next word of something that has never seen before. There are tricks to using language models to generate text like beam search.\n\nWe do not have separate test and validation in this case. Just like in vision, the training directory has bunch of files in it:\n\nNow we will check how many words are in the dataset:\n\nBefore we can do anything with text, we have to turn it into a list of tokens. Token is basically like a word. Eventually we will turn them into a list of numbers, but the first step is to turn it into a list of words \u2014 this is called \u201ctokenization\u201d in NLP. A good tokenizer will do a good job of recognizing pieces in your sentence. Each separated piece of punctuation will be separated, and each part of multi-part word will be separated as appropriate. Spacy does a lot of NLP stuff, and it has the best tokenizer Jeremy knows. So Fast.ai library is designed to work well with the Spacey tokenizer as with torchtext.\n\nA field is a definition of how to pre-process some text.\n\nNow we create the usual Fast.ai model data object:\n\nAfter building our object, it automatically fills the object with a very important attribute: . This is a vocabulary, which stores which unique words (or tokens) have been seen in the text, and how each word will be mapped to a unique integer id.\n\nis sorted by frequency except for the first two special ones. Using , torchtext will turn words into integer IDs for us :\n\nQuestion: Is it common to do any stemming or lemma-tizing? [01:45:47] Not really, no. Generally tokenization is what we want. To keep it as general as possible, we want to know what is coming next so whether it is future tense or past tense or plural or singular, we don\u2019t really know which things are going to be interesting and which are not, so it seems that it is generally best to leave it alone as much as possible.\n\nQuestion: When dealing with natural language, isn\u2019t context important? Why are we tokenizing and looking at individual word? [01:46:38] No, we are not looking at individual word \u2014 they are still in order. Just because we replaced I with a number 12, they are still in that order. There is a different way of dealing with natural language called \u201cbag of words\u201d and they do throw away the order and context. In the Machine Learning course, we will be learning about working with bag of words representations but my belief is that they are no longer useful or in the verge of becoming no longer useful. We are starting to learn how to use deep learning to use context properly.\n\nWhat happens in a language model is even though we have lots of movie reviews, they all get concatenated together into one big block of text. So we predict the next word in this huge long thing which is all of the IMDB movie reviews concatenated together.\n\nQuestion: Why not split by a sentence? [01:53:40] Not really. Remember, we are using columns. So each of our column is of length about 1 million, so although it is true that those columns are not always exactly finishing on a full stop, they are so darn long we do not care. Each column contains multiple sentences.\n\nPertaining to this question, Jeremy found what is in this language model matrix a little mind-bending for quite a while, so do not worry if it takes a while and you have to ask a thousands questions.\n\nNow that we have a model data object that can fee d us batches, we can create a model. First, we are going to create an embedding matrix.\n\nHere are the: # batches; # unique tokens in the vocab; length of the dataset; # of words\n\nThis is our embedding matrix looks like:\n\nResearchers have found that large amounts of momentum (which we\u2019ll learn about later) don\u2019t work well with these kinds of RNN models, so we create a version of the Adam optimizer with less momentum than its default of . Any time you are doing NLP, you should probably include this line:\n\nFast.ai uses a variant of the state of the art AWD LSTM Language Model developed by Stephen Merity. A key feature of this model is that it provides excellent regularization through Dropout. There is no simple way known (yet!) to find the best values of the dropout parameters below \u2014 you just have to experiment\u2026\n\nHowever, the other parameters ( , , and ) shouldn't generally need tuning.\n\nQuestion: There are word embedding out there such as Word2vec or GloVe. How are they different from this? And why not initialize the weights with those initially? [02:02:29] People have pre-trained these embedding matrices before to do various other tasks. They are not called pre-trained models; they are just a pre-trained embedding matrix and you can download them. There is no reason we could not download them. I found that building a whole pre-trained model in this way did not seem to benefit much if at all from using pre-trained word vectors; where else using a whole pre-trained language model made a much bigger difference. Maybe we can combine both to make them a little better still.\n\nQuestion: What is the architecture of the model? [02:03:55] We will be learning about the model architecture in the last lesson but for now, it is a recurrent neural network using something called LSTM (Long Short Term Memory).\n\nIn the sentiment analysis section, we'll just need half of the language model - the encoder, so we save that part.\n\nLanguage modeling accuracy is generally measured using the metric perplexity, which is simply of the loss function we used.\n\nWe can play around with our language model a bit to check it seems to be working OK. First, let\u2019s create a short bit of text to \u2018prime\u2019 a set of predictions. We\u2019ll use our torchtext field to numericalize it so we can feed it to our language model.\n\nWe haven\u2019t yet added methods to make it easy to test a language model, so we\u2019ll need to manually go through the steps.\n\nLet\u2019s see what the top 10 predictions were for the next word after our short text:\n\n\u2026and let\u2019s see if our model can generate a bit more text all by itself!\n\nSo we had pre-trained a language model and now we want to fine-tune it to do sentiment classification.\n\nTo use a pre-trained model, we will need to the saved vocab from the language model, since we need to ensure the same words map to the same IDs.\n\ntells torchtext that a text field should be tokenized (in this case, we just want to store the 'positive' or 'negative' single label).\n\nThis time, we need to not treat the whole thing as one big piece of text but every review is separate because each one has a different sentiment attached to it.\n\nis a torchtext method that creates train, test, and validation sets. The IMDB dataset is built into torchtext, so we can take advantage of that. Take a look at to see how to define your own fastai/torchtext datasets.\n\nNow you can go ahead and call that gets us our learner. Then we can load into it the pre-trained language model ( ).\n\nBecause we\u2019re fine-tuning a pretrained model, we\u2019ll use differential learning rates, and also increase the max gradient for clipping, to allow the SGDR to work better.\n\nWe make sure all except the last layer is frozen. Then we train a bit, unfreeze it, train it a bit. The nice thing is once you have got a pre-trained language model, it actually trains really fast.\n\nA recent paper from Bradbury et al, Learned in translation: contextualized word vectors, has a handy summary of the latest academic research in solving this IMDB sentiment analysis problem. Many of the latest algorithms shown are tuned for this specific problem.\n\nAs you see, we just got a new state of the art result in sentiment analysis, decreasing the error from 5.9% to 5.5%! You should be able to get similarly world-class results on other NLP classification problems using the same basic steps.\n\nThere are many opportunities to further improve this, although we won\u2019t be able to get to them until part 2 of this course.\n\nThere is a really fantastic researcher called Sebastian Ruder who is the only NLP researcher who has been really writing a lot about pre-training, fine-tuning, and transfer learning in NLP. Jeremy was asking him why this is not happening more, and his view was it is because there is not a software to make it easy. Hopefully Fast.ai will change that.\n\nThe dataset looks like this:\n\nIt contains ratings by users. Our goal will be for some user-movie combination we have not seen before, we have to predict a rating.\n\nTo make it more interesting, we will also actually download a list of movies so that we can interpret what is actually in these embedding matrices.\n\nThis is what we are creating \u2014 this kind of cross tab of users by movies.\n\nFeel free to look ahead and you will find that most of the steps are familiar to you already."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-3-74b0ef79e56?source=user_profile---------9----------------",
        "title": "Deep Learning 2: Part 1 Lesson 3 \u2013 Hiromi Suenaga \u2013",
        "text": "Often the notebook assumes that your data is in data folder. But maybe you want to put them somewhere else. In that case, you can use symbolic link (symlink for short):\n\nMake sure you had clicked on the Download button from your computer once and accepted the rules:\n\nReplace <username> , <password> with your credential and <competition> is what follows /c/ in the URL. For example, if you are trying to download dog breed data from https://www.kaggle.com /c/ dog-breed-identification the command would look like:\n\nKaggle CLI is a good tool to use when you are downloading from Kaggle. Because it is downloading data from Kaggle website (through screen scraping), it breaks when the website changes. When that happens, run pip install kaggle-cli --upgrade .\n\nMy personal notes from fast.ai course . These notes will continue to be updated and improved as I continue to review the course to \u201creally\u201d understand it. Much appreciation to Jeremy and Rachel who gave me this opportunity to learn.\n\nHere is an end to end process to get a state of the art result for dogs vs. cats:\n\nTensorflow \u2014 If you want to convert things you learned in this class, do more work with Keras, but it would take a bit more work and is hard to get the same level fo results. Maybe there will be TensorFlow compatible version of Fast.ai in future. We will see.\n\nIt is important to understand how to use libraries other than Fast.ai. Keras is a good example to look at because just like Fast.ai sits on top of PyTorch, it sits on top of varieties of libraries such as TensorFlow, MXNet, CNTK, etc.\n\nQuestion: What happens if the input had 3 channels? [1:05:30] It will look something similar to the Conv1 layer which has 2 channels \u2014 therefore, filters have 2 channels per filter. Pre-trained ImageNet models use 3 channels. Some of the techniques you can use when you do when you do have less than 3 channel is to either duplicate one of the channels to make it 3, or if you have 2, then get an average and consider that as the third channel. If you have 4 channels, you could add extra level to the convolutional kernel with all zeros.\n\nWe have gotten as far as fully connected layer (it does classic matrix product). In the excel sheet, there is one activation. If we want to look at which one of ten digit the input is, we actually want to calculate 10 numbers.\n\nLet\u2019s look at an example where we are trying to predict whether a picture is a cat, a dog, or a plane, or fish, or a building. Our goal is:\n\nTo do this, we need a different kind of activation function (a function applied to an activation).\n\nWhy do we need non-lineality? If you stack multiple linear layers, it is still just a linear layer. By adding non-linear layers, we can fit arbitrarily complex shapes. The non-linear activation function we used was ReLU.\n\nSoftmax only ever occurs in the final layer. It outputs numbers between 0 and 1, and they add up to 1. In theory, this is not strictly necessary \u2014 we could ask out neural net to learn a set of kernels which give probabilities that line up as closely as possible with what we want. In general with deep learning, if you can construct your architecture so that the desired characteristics are as easy to express as possible, you will end up with better models (learn more quickly and with less parameters).\n\nAll the math that you need to be familiar with to do deep learning:\n\n2. We then add up the column (182.75), and divide the by the sum. The result will always be positive since we divided positive by positive. Each number will be between 0 and 1, and the total will be 1.\n\nQuestion: What kind of activation function do we use if we want to classify the picture as cat and dog? [1:20:27] It so happens that we are going to do that right now. One reason we might want to do that is to do multi-label classification.\n\nSoftmax does not like to predicting multiple things. It wants to pick one thing.\n\nFast.ai library will automatically switch into multi-label mode if there is more than one label. So you do not have to do anything. But here is what happens behind the scene:\n\nIf you are never sure what arguments a function takes, hit .\n\nBehind the scenes, PyTorch and fast.ai are turning our labels into one-hot-encoded labels. If the actual label is dog, it will look like:\n\nWe take the difference between and , add them up to say how much error there is (i.e. loss function) [1:31:02].\n\nOne-hot-encoding is terribly inefficient for storing, so we will store an index value (single integer) rather than 0\u2019s and 1\u2019s for the target value ( ) [1:31:21]. If you look at the values for the dog breeds competition, you won\u2019t actually see a big lists of 1\u2019s and 0's, but you will wee a single integer. And internally, PyTorch is converting the index to one-hot-encoded vector (even though you will literally never see it). PyTorch has different loss functions for ones that are one hot encoded and others that are not \u2014 but these details are hidden by the fast.ai library so you do not have to worry about it. But the cool thing to realize is that we are doing exactly the same thing for both single label classification and multi label classification.\n\nQuestion: Does it make sense to change the base of log for softmax?[01:32:55] No, changing the base is just a linear scaling which neural net can learn easily:\n\nA couple of questions people have asked what this does [01:38:46]:\n\nWhen we specify what transforms to apply, we send a size:\n\nOne of the things the data loader does is to resize the images on-demand. This has nothing to do with . If the initial image is 1000 by 1000, reading that JPEG and resizing it to 64 by 64 take more time than training the convolutional net. tells it that we will not use images bigger than so go through once and create new JPEGs of this size. Since images are rectangular, so new JPEGs whose smallest edge is (center-cropped). It will save you a lot of time.\n\nInstead of , we used F-beta for this notebook \u2014 it is a way of weighing false negatives and false positives. The reason we are using it is because this particular Kaggle competition wants to use it. Take a look at planet.py to see how you can create your own metrics function. This is what gets printed out at the end\n\nQuestion: Why don\u2019t we start training with differential learning rate rather than training the last layers alone? [01:50:30]\n\nYou can skip training just the last layer and go straight to differential learning rates, but you probably do not want to. Convolutional layers all contain pre-trained weights, so they are not random \u2014 for things that are close to ImageNet, they are really good; for things that are not close to ImageNet, they are better than nothing. All of our fully connected layers, however, are totally random. Therefore, you would always want to make the fully connected weights better than random by training them a bit first. Otherwise if you go straight to unfreeze, then you are actually going to be fiddling around with those early layer weights when the later ones are still random \u2014 which is probably not what you want.\n\nQuestion: When you use the differential learning rates, do those three learning rates spread evenly across the layers? [01:55:35] We will talk more about this later in the course but the fast.ai library, there is a concept of \u201clayer groups\u201d. In something like ResNet50, there are hundreds of layers and you probably do not want to write hundreds of learning rates, so the library decided for you how to split them and the last one always refers to just the fully connected layers that we have randomly initialized and added.\n\nQuestion: Learning rate finder for a very small dataset returned strange number and the plot was empty [01:58:57] \u2014 The learning rate finder will go through a mini-batch at a time. If you have a tiny dataset, there is just not enough mini-batches. So the trick is to make your batch size very small like 4 or 8.\n\nThere are two types of dataset we use in machine learning:\n\nStructured data is often ignored in academics because it is pretty hard to get published in fancy conference proceedings if you have a better logistics model. But it is the thing that makes the world goes round, makes everybody money and efficiency. We will not ignore it because we are doing practical deep learning, and Kaggle does not either because people put prize money up on Kaggle to solve real-world problems:\n\nThere are a lot of data pre-processing This notebook contains the entire pipeline from the third place winner (Entity Embeddings of Categorical Variables). Data processing is not covered in this course, but is covered in machine learning course in some detail because feature engineering is very important."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-2-eeae2edd2be4?source=user_profile---------10----------------",
        "title": "Deep Learning 2: Part 1 Lesson 2 \u2013 Hiromi Suenaga \u2013",
        "text": "So far, we have not retrained any of pre-trained features \u2014 specifically, any of those weights in the convolutional kernels. All we have done is we added some new layers on top and learned how to mix and match pre-trained features. Images like satellite images, CT scans, etc have totally different kinds of features all together (compare to ImageNet images), so you want to re-train many layers. For dogs and cats, images are similar to what the model was pre-trained with, but we still may find it is helpful to slightly tune some of the later layers. Here is how you tell the learner that we want to start actually changing the convolutional filters themselves: \u201cfrozen\u201d layer is a layer which is not being trained/updated. unfreezes all the layers. Earlier layers like the first layer (which detects diagonal edges or gradient) or the second layer (which recognizes corners or curves) probably do not need to change by much, if at all. Later layers are much more likely to need more learning. So we create an array of learning rates (differential learning rate): : for the first few layers (basic geometric features) : for layers we added on top Why 3? Actually they are 3 ResNet blocks but for now, think of it as a group of layers. Question: What if I have a bigger images than the model is trained with? [50:30] The short answer is, with this library and modern architectures we are using, we can use any size we like. Question: Can we unfreeze just specific layers? [51:03] We are not doing it yet, but if you wanted, you can do (which will unfreeze layers from layer onwards). Jeremy almost never finds it helpful and he thinks it is because we are using differential learning rates, and the optimizer can learn just as much as it needs to. The one place he found it helpful is if he is using a really big memory intensive model and he is running out of GPU, the less layers you unfreeze, the less memory and time it takes. Using differential learning rate, we are up to 99.5%! [52:28] Earlier we said is the number of epochs, but it is actually cycles. So if , it will do 3 cycles where each cycle is 2 epochs (i.e. 6 epochs). Then why did it 7? It is because of : this multiplies the length of the cycle after each cycle (1 epoch + 2 epochs + 4 epochs = 7 epochs). Intuitively speaking [53:57], if the cycle length is too short, it starts going down to find a good spot, then pops out, and goes down trying to find a good spot and pops out, and never actually get to find a good spot. Earlier on, you want it to do that because it is trying to find a spot that is smoother, but later on, you want it to do more exploring. That is why seems to be a good approach. We are introducing more and more hyper parameters having told you that there are not many. You can get away with just choosing a good learning rate, but then adding these extra tweaks helps get that extra level-up without any effort. In general, good starting points are: Question: why do smoother surfaces correlate to more generalized networks? [55:28] Say you have something spiky (blue line). X-axis is showing how good this is at recognizing dogs vs. cats as you change this particular parameter. Something to be generalizable means that we want it to work when we give it a slightly different dataset. Slightly different dataset may have a slightly different relationship between this parameter and how cat-like vs. dog-like it is. It may, instead look like the red line. In other words, if we end up at the blue pointy part, then it will not going to do a good job on this slightly different dataset. Or else, if we end up on the wider blue part, it will still do a good job on the red dataset. Here is some interesting discussion about spiky minima. Our model has achieved 99.5%. But can we make it better still? Let\u2019s take a look at pictures we predicted incorrectly:\n\nHere, Jeremy printed out the whole of these pictures. When we do the validation set, all of our inputs to our model must be square. The reason is kind of a minor technical detail, but GPU does not go very quickly if you have different dimensions for different images. It needs to be consistent so that every part of the GPU can do the same thing. This may probably be fixable but for now that is the state of the technology we have. To make it square, we just pick out the square in the middle \u2014 as you can see below, it is understandable why this picture was classified incorrectly:\n\nWe are going to do what is called \u201cTest Time Augmentation\u201d. What this means is that we are going to take 4 data augmentations at random as well as the un-augmented original (center-cropped). We will then calculate predictions for all these images, take the average, and make that our final prediction. Note that this is only for validation set and/or test set. To do this, all you have to do is \u2014 which brings up the accuracy to 99.65%! Questions on augmentation approach[01:01:36]: Why not border or padding to make it square? Typically Jeremy does not do much padding, but instead he does a little bit of zooming. There is a thing called reflection padding that works well with satellite imagery. Generally speaking, using TTA plus data augmentation, the best thing to do is try to use as large image as possible. Also, having fixed crop locations plus random contrast, brightness, rotation changes might be better for TTA. Question: Data augmentation for non-image dataset? [01:03:35] No one seems to know. It seems like it would be helpful, but there are very few number of examples. In natural language processing, people tried replacing synonyms for instance, but on the whole the area is under researched and under developed. Question: Is fast.ai library open source?[01:05:34] Yes. He then covered the reason why Fast.ai switched from Keras + TensorFlow to PyTorch Random note: PyTorch is much more than just a deep learning library. It actually lets us write arbitrary GPU accelerated algorithms from scratch \u2014 Pyro is a great example of what people are now doing with PyTorch outside of deep learning. The simple way to look at the result of a classification is called confusion matrix \u2014 which is used not only for deep learning but in any kind of machine learning classifier. It is helpful particularly if there are four or five classes you are trying to predict to see which group you are having the most trouble with. Let\u2019s look at the pictures again [01:13:00] Most incorrect cats (only the left two were incorrect \u2014 it displays 4 by default):\n\nUse to find highest learning rate where loss is still clearly improving Train last layer from precomputed activations for 1\u20132 epochs Train last layer with data augmentation (i.e. ) for 2\u20133 epochs with Set earlier layers to 3x-10x lower learning rate than next higher layer. Rule of thumb: 10x for ImageNet like images, 3x for satellite or medical imaging Use again (Note: if you call having set differential learning rates, what it prints out is the learning rate of the last layers.) Let\u2019s do it again: Dog Breed Challenge [01:16:37] You can use Kaggle CLI to download data for Kaggle competitions Notebook is not made public since it is an active competition This is a little bit different to our previous dataset. Instead of folder which has a separate folder for each breed of dog, it has a CSV file with the correct labels. We will read CSV file with Pandas. Pandas is what we use in Python to do structured data analysis like CSV and usually imported as : How many dog images per breed \u2014 we will zoom in up to 1.1 times \u2014 last time, we used but since the labels are in CSV file, we will call instead. \u2014 we need to specify where the test set is if you want to submit to Kaggle competitions \u2014 there is no folder but we still want to track how good our performance is locally. So above you will see: : Open CSV file, create a list of rows, then take the length. because the first row is a header. Hence is the number of images we have. : \u201cget cross validation indexes\u201d \u2014 this will return, by default, random 20% of the rows (indexes to be precise) to use as a validation set. You can also send to get different amount. \u2014 File names has at the end, but CSV file does not. So we will set so it knows the full file names. You can access to training dataset by saying and contains a lot of things including file names ( ) Now we check image size. If they are huge, then you have to think really carefully about how to deal with them. If they are tiny, it is also challenging. Most of ImageNet models are trained on either 224 by 224 or 299 by 299 images Dictionary comprehension \u2014 key: name of the file , value: size of the file will unpack a list. will pair up elements of tuples to create a list of tuples. Matplotlib is something you want to be very familiar with if you do any kind of data science or machine learning in Python. Matplotlib is always referred to as . Question: How many images should we use as a validation set? [01:26:28] Using 20% is fine unless the dataset is small \u2014 then 20% is not enough. If you train the same model multiple times and you are getting very different validation set results, then your validation set is too small. If the validation set is smaller than a thousand, it is hard to interpret how well you are doing. If you care about the third decimal place of accuracy and you only have a thousand things in your validation set, a single image changes the accuracy. If you care about the difference between 0.01 and 0.02, you want that to represent 10 or 20 rows. Normally 20% seems to work fine. Here is the regular two lines of code. When we start working with new dataset, we want everything to go super fast. So we made it possible to specify the size and start with something like 64 which will run fast. Later, we will use bigger images and bigger architectures at which point, you may run out of GPU memory. If you see CUDA out of memory error, the first thing you need to do is to restart kernel (you cannot recover from it), then make the batch size smaller. Reminder: a is one pass through the data, a is how many epochs you said is in a cycle If you trained a model on smaller size images, you can then call and pass in a larger size dataset. That is going to take your model, however it has been trained so far, and it is going to let you continue to train on larger images. Starting training on small images for a few epochs, then switching to bigger images, and continuing training is an amazingly effective way to avoid overfitting. As you see, validation set loss (0.2274) is much lower than training set loss (0.28341) \u2014 which means it is under fitting. When you are under fitting, it means is too short (learning rate is getting reset before it had the chance to zoom in properly). So we will add (i.e. 1st cycle is 1 epoch, 2nd cycle is 2 epochs, and 3rd cycle is 4 epochs) Now the validation loss and training loss are about the same \u2014 this is about the right track. Then we try : Try running one more cycle of 2 epochs Unfreezing (in this case, training convolutional layers did not help in the slightest since the images actually came from ImageNet) Remove validation set and just re-run the same steps, and submit that \u2014 which lets us use 100% of the data. Question: How do we deal with unbalanced dataset? [01:38:46] This dataset is not totally balanced (between 60 and 100) but it is not unbalanced enough that Jeremy would give it a second thought. A recent paper says the best way to deal with very unbalanced dataset is to make copies of the rare cases. We added a couple of layers on the end of it which start out random. With everything frozen and , all we are learning is the layers we have added. With , data augmentation does not do anything because we are showing exactly the same activations each time. We then set which means we are still only training the layers we added because it is frozen but data augmentation is now working because it is actually going through and recalculating all of the activations from scratch. Then finally, we unfreeze which is saying \u201cokay, now you can go ahead and change all of these earlier convolutional filters\u201d. Question: Why not just set from the beginning? The only reason to have is it is much faster (10 or more times). If you are working with quite a large dataset, it can save quite a bit of time. There is no accuracy reason ever to use . Use to find highest learning rate where loss is still clearly improving Train last layer with data augmentation (i.e. ) for 2\u20133 epochs with Set earlier layers to 3x-10x lower learning rate than next higher layer Question: Does reducing the batch size only affect the speed of training? [1:43:34] Yes, pretty much. If you are showing it less images each time, then it is calculating the gradient with less images \u2014 hence less accurate. In other words, knowing which direction to go and how far to go in that direction is less accurate. So as you make the batch size smaller, you are making it more volatile. It impacts the optimal learning rate that you would need to use, but in practice, dividing the batch size by 2 vs. 4 does not seem to change things very much. If you change the batch size by much, you can re-run learning rate finder to check. Question: What are the grey images vs. the ones on the right? Layer 1, they are exactly what the filters look like. It is easy to visualize because input to it are pixels. Later on, it gets harder because inputs are themselves activations which is a combination of activations. Zeiler and Fergus came up with a clever technique to show what the filters tend to look like on average \u2014 called deconvolution (we will learn in Part 2). Ones on the right are the examples of patches of image which activated that filter highly. Question: What would you have done if the dog was off to the corner or tiny (re: dog breed identification)? [01:47:16] We will learn about it in Part 2, but there is a technique that allows you to figure out roughly which parts of an image most likely have the interesting things in them. Then you can crop out that area. Two things we can do immediately to make it better: Assuming the size of images you were using is smaller than the average size of images you have been given, you can increase the size. As we have seen before, you can increase it during training. Use better architecture. There are different ways of putting together what size convolutional filters and how they are connected to each other, and different architectures have different number of layers, size of kernels, filters, etc. We have been using ResNet34 \u2014 a great starting point and often a good finishing point because it does not have too many parameters and works well with small dataset. There is another architecture called ResNext which was the second-place winner in last year\u2019s ImageNet competition.ResNext50 takes twice as long and 2\u20134 times more memory than ResNet34. Here is the notebook which is almost identical to the original dogs. vs. cats. which uses ResNext50 which achieved 99.75% accuracy."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-1-602f73869197?source=user_profile---------11----------------",
        "title": "Deep Learning 2: Part 1 Lesson 1 \u2013 Hiromi Suenaga \u2013",
        "text": "In order to train a neural network, you will most certainly need Graphics Processing Unit (GPU) \u2014 specifically NVIDIA GPU because it is the only one that supports CUDA (the language and framework that nearly all deep learning libraries and practitioners use). There are several ways to rent GPU: Crestle [04:06], Paperspace [06:10] Introduction to Jupyter Notebook and Dogs vs. Cats [12:39] You can run a cell by selecting it and hitting (you can hold down and hit multiple times to keep going down the cells), or you can click on Run button at the top. A cell can contain code, text, picture, video, etc. # This file contains all the main external libs we'll use\n\nfrom fastai.imports import * First look at pictures [15:39] tells to use bash (shell) instead of python If you are not familiar with training set and validation set, check out Practical Machine Learning class (or read Rachel\u2019s blog) This folder structure is the most common approach for how image classification dataset is shared and provided. Each folder tells you the label (e.g. or ). \u2014 This is a Python 3.6. format string which is a convenient to format a string. The three items (e.g. ) represents Red Green Blue pixel values between 0 and 255 The idea is to take these numbers and use them to predict whether those numbers represent a cat or a dog based on looking at lots of pictures of cats and dogs. This dataset comes from Kaggle competition, and when it was released (back in 2013) the state-of-the-art was 80% accurate. Here are the three lines of code necessary to train a model: This will do 3 epochs which means it is going to look at the entire set of images three times. The last of three numbers in the output is the accuracy on the validation set. The first two are the value of loss function (in this case the cross-entropy loss) for the training set and the validation set. The start (e.g. , ) is the epoch number. We achieved ~99% (which would have won the Kaggle competition back in 2013) in 17 seconds with 3 lines of code! [21:49] A lot of people assume that deep learning takes a huge amount of time, lots of resources, and lots of data \u2014 that, in general, is not true! The library takes all of the best practices and approaches they can find \u2014 each time a paper comes out that looks interesting, they test it out and if it works well for a variety of datasets and they can figure out how to tune it, it gets implement it in the library. Fast.ai curates all these best practices and packages up for you, and most of the time, figures out the best way to handle things automatically. Fast.ai sits on top of a library called PyTorch which is a really flexible deep learning, machine learning, and GPU computation library written by Facebook. Most people are more familiar with TensorFlow than PyTorch, but most of the top researchers Jeremy knows nowadays have switched across to PyTorch. Fast.ai is flexible that you can use all these curated best practices as much or as little as you want. It is easy to hook in at any point and write your own data augmentation, loss function, network architecture, etc, and we will learn all that in this course. This is what the validation dataset label (think of it as the correct answers) looks like: What do these 0\u2019s and 1\u2019s represents? contains the validation and training data Let\u2019s make predictions for the validation set (predictions are in log scale): The output represents a prediction for cats, and prediction for dogs preds = np.argmax(log_preds, axis=1) # from log probabilities to 0 or 1\n\nprobs = np.exp(log_preds[:,1]) # pr(dog) In PyTorch and Fast.ai, most models return the log of the predictions rather than the probabilities themselves (we will learn why later in the course). For now, just know that to get probabilities, you have to do Make sure you familiarize yourself with numpy ( ) The number above the image is the probability of being a dog\n\nMore interestingly, here are what the model thought it was definitely a dog but turns out to be a cat, or vice versa:\n\nWhy is it important to look at these images? The first thing Jeremy does after he builds a model is to find a way to visualize what it has built. Because if he wants to make the model better, then he needs to take advantage of the things that is doing well and fix the things that is doing badly. In this case, we have learned something about the dataset itself which is that there are some images that are in here that probably should not be. But it is also clear that this model has room to improve (e.g. data augmentation \u2014 which we will learn later). Now you are ready to build your own image classifier (for regular photos \u2014 maybe not CT scan)! For example, here is what one of the students did. Check out this forum post for different way of visualizing the results (e.g. when there are more than 2 categories, etc) Bottom-up: learn each building block you need, and eventually put them together Hard to know the \u201cbig picture\u201d Hard to know which pieces you\u2019ll actually need fast.ai: Get students using a neural net right away, getting results ASAP Gradually peel back the layers, modify, look under the hood Image classifier with deep learning (with fewest lines of code) Multi-label classification and different kinds of images (e.g. satellite images) Structured data (e.g. sales forecasting) \u2014 structured data is what comes from database or spreadsheet Generative language model: How to write your own Nietzsche philosophy from scratch character by character Back to computer vision \u2014 not just recognize a cat photo, but find where the cat is in the photo (heat map) and also learn how to write our own architecture from scratch (ResNet) Image classification algorithm is useful for lots and lots of things. For example, AlphaGo [42:20] looked at thousands and thousands of go boards and each one had a label saying whether the go board ended up being the winning or the losing player\u2019s. So it learnt an image classification that was able to look at a go board and figure out whether it was a good or bad \u2014 which is the most important step in playing go well: to know which move is better. Another example is an earlier student created an image classifier of mouse movement images and detected fraudulent transactions. Machine learning was invented by Arthur Samuel. In the late 50s, he got an IBM mainframe to play checkers better than he could by inventing machine learning. He made the mainframe to play against itself lots of times and figure out which kind of things led to victories, and used that to, in a way, write its own program. In 1962, Arthur Samuel said one day, the vast majority of computer software would be written using this machine learning approach rather than written by hand. C-Path (Computational Pathologist)[45:42] is an example of traditional machine learning approach. He took pathology slides of breast cancer biopsies, consulted many pathologists on ideas about what kinds of patterns or features might be associated with long-term survival. Then they wrote specialist algorithms to calculate these features, run through logistic regression, and predicted the survival rate. It outperformed pathologists, but it took domain experts and computer experts many years of work to build. A class of algorithm that have these three properties is Deep Learning. Underlying function that deep learning uses is called the neural network: All you need to know for now is that it consists of a number of simple linear layers interspersed with a number of simple non-linear layers. When you intersperse these layers, you get something called the universal approximation theorem. What universal approximation theorem says is that this kind of function can solve any given problem to arbitrarily close accuracy as long as you add enough parameters. The neural network example shown above has one hidden layer. Something what we learned in the past few years is that these kind of neural network was not fast or scalable unless we added multiple hidden layers \u2014 hence called \u201cDeep\u201d learning. Here are some fo the examples: Neural networks and deep learning\n\nIn this chapter I give a simple and mostly visual explanation of the universality theorem. We'll go step by step\u2026neuralnetworksanddeeplearning.com\n\nA combination of linear layer followed by an element-wise nonlinear function allows us to create arbitrarily complex shapes \u2014 this is the essence of the universal approximation theorem. How to set these parameters to solve problems [01:04:25] Stochastic Gradient Descent \u2014 we take small steps down the hill. The step size is called learning rate If learning rate is too large, it will diverge instead of converge If learning rate is too small, it will take forever We started with something incredibly simple but if we use it as a big enough scale, thanks to the universal approximation theorem and the use of multiple hidden layers in deep learning, we actually get the very very rich capabilities. This is actually what we used when we used when we trained our dog vs cat recognizer. The first number is the learning rate. The learning rate determines how quickly or how slowly you want to update the weights (or parameters). Learning rate is one of the most difficult parameters to set, because it significantly affect model performance. The method helps you find an optimal learning rate. It uses the technique developed in the 2015 paper Cyclical Learning Rates for Training Neural Networks, where we simply keep increasing the learning rate from a very small value, until the loss starts decreasing. We can plot the learning rate across batches to see what this looks like. Our object contains an attribute that contains our learning rate scheduler, and has some convenient plotting functionality including this one: Jeremy is currently experimenting with increasing the learning rate exponentially vs. linearly. We can see the plot of loss versus learning rate to see where our loss stops decreasing: We then pick the learning rate where the loss is still clearly improving \u2014 in this case (0.01) As many as you would like, but accuracy might start getting worse if you run it for too long. It is something called \u201coverfitting\u201d and we will learn more about it later. Another consideration is the time available to you. 1. \u2014 it will do an auto complete when you cannot remember the function name 2. \u2014 it will show you the arguments of a function 3. \u2014 it will bring up a documentation (i.e. docstring) 4. \u2014 it will open a separate window with the same information. Typing followed by a function name in a cell and running it will do the same as 5. Typing two question mark will display the source code 6. Typing in Jupyter Notebook will open up a window with keyboard shortcuts. Try learning 4 or 5 shortcuts a day 8. Please remember about the forum and http://course.fast.ai/ (for each lesson) for up-to-date information."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-7-1b9503aff0c?source=user_profile---------12----------------",
        "title": "Deep Learning 2: Part 1 Lesson 7 \u2013 Hiromi Suenaga \u2013",
        "text": "The theme of Part 1 is:\n\nPart 2 of the course:\n\nReminder: RNN is not in any way different or unusual or magical \u2014 just a standard fully connected network.\n\nThere is no known good way. Somebody recently won a Kaggle competition by doing data augmentation which randomly inserted parts of different rows \u2014 something like that may be useful here. But there has not been any recent state-of-the-art NLP papers that are doing this kind of data augmentation.\n\nThere are a couple things to think about:\n\nWhen using an existing API which expects data to be certain format, you can either change your data to fit that format or you can write your own dataset sub-class to handle the format that your data is already in. Either is fine, but in this case, we will put our data in the format TorchText already support. Fast.ai wrapper around TorchText already has something where you can have a training path and validation path, and one or more text files in each path containing bunch of text that are concatenated together for your language model.\n\nWe remove the use of and replace it with . PyTorch source code looks like the following. You should be able to read and understand (Note: they do not concatenate the input and the hidden state, but they sum them together \u2014 which was our first approach):\n\nQuestion about [44:06]: As we have seen last week, is forcing the value to be between -1 and 1. Since we are multiplying by this weight matrix again and again, we would worry that (since it is unbounded) might have more gradient explosion problem. Having said that, you can specify to use different whose default is and ask it to use if you wanted to.\n\nIn practice, nobody really uses since even with , gradient explosions are still a problem and we need use low learning rate and small to get them to train. So what we do is to replace with something like .\n\nAbove is what code looks like, and our new model that utilize this is below:\n\nAs a result, we can lower the loss down to 1.36 ( one was 1.54). In practice, GRU and LSTM are what people uses.\n\nLSTM has one more piece of state in it called \u201ccell state\u201d (not just hidden state), so if you do use a LSTM, you have to return a tuple of matrices in (exactly the same size as hidden state):\n\nThe code is identical to GRU one. The one thing that was added was which does dropout after each time step and doubled the hidden layer \u2014 in a hope that it will be able to learn more and be resilient as it does so.\n\nCIFAR 10 is an old and well known dataset in academia \u2014 well before ImageNet, there was CIFAR 10. It is small both in terms of number of images and size of images which makes it interesting and challenging. You will likely be working with thousands of images rather than one and a half million images. Also a lot of the things we are looking at like in medical imaging, we are looking at a specific area where there is a lung nodule, you are probably looking at 32 by 32 pixels at most.\n\nIt also runs quickly, so it is much better to test our your algorithms. As Ali Rahini mentioned in NIPS 2017, Jeremy has the concern that many people are not doing carefully tuned and throught-about experiments in deep learning, but instead, they throw lots of GPUs and TPUs or lots of data and consider that a day. It is important to test many versions of your algorithm on dataset like CIFAR 10 rather than ImageNet that takes weeks. MNIST is also good for studies and experiments even though people tend to complain about it.\n\nCIFAR 10 data in image format is available here\n\nFrom this notebook by our student Kerem Turgutlu:\n\nWith a simple one hidden layer model with 122,880 parameters, we achieved 46.9% accuracy. Let\u2019s improve this and gradually build up to a basic ResNet architecture.\n\nSimplify function by creating (our first custom layer!). In PyTorch, layer definition and neural network definitions are identical. Anytime you have a layer, you can use it as a neural net, when you have a neural net, you can use it as a layer.\n\nIt is something where you try turning on and off different pieces of your model to see which bits make which impacts, and one of the things that wasn\u2019t done in the original batch norm paper was any kind of effective ablation. And one of the things therefore that was missing was this question which was just asked \u2014 where to put the batch norm. That oversight caused a lot of problems because it turned out the original paper did not actually put it in the best spot. Other people since then have now figured that out and when Jeremy show people code where it is actually in the spot that is better, people say his batch norm is in the wrong spot.\n\nLet\u2019s increase the depth of the model. We cannot just add more of stride 2 layers since it halves the size of the image each time. Instead, after each stride 2 layer, we insert a stride 1 layer.\n\nThe accuracy remained the same as before. This is now 12 layers deep, and it is too deep even for batch norm to handle. It is possible to train 12 layer deep conv net but it starts to get difficult. And it does not seem to be helping much if at all.\n\nWhere x is prediction from the previous layer, y is prediction from the current layer.Shuffle around the formula and we get:formula shuffle\n\nThe difference y \u2212 x is residual. The residual is the error in terms of what we have calculated so far. What this is saying is that try to find a set of convolutional weights that attempts to fill in the amount we were off by. So in other words, we have an input, and we have a function which tries to predict the error (i.e. how much we are off by). Then we add a prediction of how much we were wrong by to the input, then add another prediction of how much we were wrong by that time, and repeat that layer after layer \u2014 zooming into the correct answer. This is based on a theory called boosting.\n\nHere, we increased the size of features and added dropout.\n\n85% was a state-of-the-art back in 2012 or 2013 for CIFAR 10. Nowadays, it is up to 97% so there is a room for improvement but all based on these tecniques:\n\nQuestion [02:01:07]: Can we apply \u201ctraining on the residual\u201d approach for non-image problem? Yes! But it has been ignored everywhere else. In NLP, \u201ctransformer architecture\u201d recently appeared and was shown to be the state of the art for translation, and it has a simple ResNet structure in it. This general approach is called \u201cskip connection\u201d (i.e. the idea of skipping over a layer) and appears a lot in computer vision, but nobody else much seems to be using it even through there is nothing computer vision specific about it. Good opportunity!\n\nGoing back dogs and cats. We will create resnet34 (if you are interested in what the trailing number means, see here \u2014 just different parameters).\n\nOur ResNet model had Relu \u2192 BatchNorm. TorchVision does BatchNorm \u2192Relu. There are three different versions of ResNet floating around, and the best one is PreAct (https://arxiv.org/pdf/1603.05027.pdf).\n\nWe pick a specific image, and use a technique called CAM where we take a model and we ask it which parts of the image turned out to be important."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-6-de70d626976c?source=user_profile---------13----------------",
        "title": "Deep Learning 2: Part 1 Lesson 6 \u2013 Hiromi Suenaga \u2013",
        "text": "Optimization for Deep Learning Highlights in 2017\n\nTable of contents: Deep Learning ultimately is about finding a minimum that generalizes well -- with bonus points for\u2026ruder.io We took a deep dive to collaborative filtering last week, and we ended up re-creating class ( ) in fast.ai library. Let\u2019s visualize what the embeddings look like [notebook]. Inside of a learner , you can get a PyTorch model itself by calling . looks like a regular function, but requires no parenthesis when you call it. is an instance of which is a thin wrapper of PyTorch model that allows us to use \u201clayer groups\u201d which is not a concept available in PyTorch and fast.ai uses it to apply different learning rates to different sets of layers (layer group). PyTorch model prints out the layers nicely including layer name which is what we called them in the code. refers to an embedding layer for an item bias \u2014 movie bias, in our case. What is nice about PyTorch models and layers is that we can call them as if they are functions. So if you want to get a prediction, you call and pass in variables. Layers require variables not tensors because it needs to keep track of the derivatives \u2014 that is the reason for to convert tensor to variable. PyTorch 0.4 will get rid of variables and we will be able to use tensors directly. The function will take a variable or a tensor (regardless of being on the CPU or GPU) and returns a numpy array. Jeremy\u2019s approach [12:03] is to use numpy for everything except when he explicitly needs something to run on the GPU or he needs its derivatives \u2014 in which case he uses PyTorch. Numpy has been around longer than PyTorch and works well with other libraries such as OpenCV, Pandas, etc. A question regarding CPU vs. GPU in production. The suggested approach is to do inference on CPU as it is more scalable and you do not need to put things in batches. You can move a model onto the CPU by typing , similarly a variable by typing (from CPU to GPU would be ).If your server does not have GPU, it will run inference on CPU automatically. For loading a saved model that was trained on GPU, take a look at this line of code in : Now that we have movie bias for top 3000 movies, and let\u2019s take a look at ratings: will allow you to iterate through multiple lists at the same time. About sorting key \u2014 Python has function but plain is just one more character. [(1.3070084, 'Shawshank Redemption, The (1994)'),\n\n (1.1196285, 'Godfather, The (1972)'),\n\n (1.0844109, 'Usual Suspects, The (1995)'),\n\n (0.96578616, \"Schindler's List (1993)\"),\n\n ...] Each movie has 50 embeddings and it is hard to visualize 50 dimensional space, so we will turn it into a three dimensional space. We can compress dimensions using several techniques: Principal Component Analysis (PCA) (Rachel\u2019s Computational Linear Algebra class covers this in detail \u2014 which is almost identical to Singular Value Decomposition (SVD)) We will take a look at the first dimension \u201ceasy watching vs. serious\u201d (we do not know what it represents but can certainly speculate by looking at them): The second dimension \u201cdialog driven vs. CGI\u201d [(0.058975246, 'Bonfire of the Vanities (1990)'),\n\n (0.055992026, '2001: A Space Odyssey (1968)'),\n\n (0.054682467, 'Tank Girl (1995)'),\n\n (0.054429606, 'Purple Rose of Cairo, The (1985)'),\n\n ...] [(-0.1064609, 'Lord of the Rings: The Return of the King, The (2003)'),\n\n (-0.090635143, 'Aladdin (1992)'),\n\n (-0.089208141, 'Star Wars: Episode V - The Empire Strikes Back (1980)'),\n\n (-0.088854566, 'Star Wars: Episode IV - A New Hope (1977)'),\n\n ...]\n\nWhat actually happens when you say ? The second paper to talk about categorical embeddings. FIG. 1. caption should sound familiar as they talk about how entity embedding layers are equivalent to one-hot encoding followed by a matrix multiplication. The interesting thing they did was, they took the entity embeddings trained by a neural network, replaced each categorical variable with the learned entity embeddings, then fed that into Gradient Boosting Machine (GBM), Random Forest (RF), and KNN \u2014 which reduced the error to something almost as good as neural network (NN). This is a great way to give the power of neural net within your organization without forcing others to learn deep learning because they can continue to use what they currently use and use the embeddings as input. GBM and RF train much faster than NN. They also plotted the embeddings of states in Germany which interestingly (\u201cwhackingly enough\u201d as Jeremy would call it) resembled an actual map. They also plotted the distances of stores in physical space and embedding space \u2014 which showed a beautiful and clear correlation. There also seems to be correlation between days of the week, or months of the year. Visualizing embeddings can be interesting as it shows you what you expected see or what you didn\u2019t. Skip-Gram is specific to NLP. A good way to turn an unlabeled problem into a labeled problem is to \u201cinvent\u201d labels. Word2Vec\u2019s approach was to take a sentence of 11 words, delete the middle word, and replace it with a random word. Then they gave a label 1 to the original sentence; 0 to the fake one, and built a machine learning model to find the fake sentences. As a result, they now have embeddings they can use for other purposes. If you do this as a single matrix multiplier (shallow model) rather than deep neural net, you can train this very quickly \u2014 the disadvantage is that it is a less predictive model, but the advantages are that you can train on a very large dataset and more importantly, the resulting embeddings have linear characteristics which allow us to add, subtract, or draw nicely. In NLP, we should move past Word2Vec and Glove (i.e. linear based methods) because these embeddings are less predictive. The state of the art language model uses deep RNN. To learn any kind of feature space, you either need labeled data or you need to invent a fake task [35:45] Is one fake task better than another? Not well studied yet. Intuitively, we want a task which helps a machine to learn the kinds of relationships that you care about. In computer vision, a type of fake task people use is to apply unreal and unreasonable data augmentations. If you can\u2019t come up with great fake tasks, just use crappy one \u2014 it is often surprising how little you need. Autoencoder [38:10] \u2014 it recently won an insurance claim competition. Take a single policy, run it through neural net, and have it reconstruct itself (make sure that intermediate layers have less activations than the input variable). Basically, it is a task whose input = output which works surprisingly well as a fake task. In computer vision, you can train on cats and dogs and use it for CT scans. Maybe it might work for language/NLP! (future research) A way to use test set properly was added to the notebook. For more detailed explanations, see Machine Learning course. is used to make sure that the test set and the training set have the same categorical codes. Keep track of which contains the mean and standard deviation of each continuous column, and apply the same to the test set. Do not rely on Kaggle public board \u2014 rely on your own thoughtfully created validation set. Going over a good Kernel for Rossmann There is a jump on sales before and after the store closing. 3rd place winner deleted closed store rows before they started any analysis. Don\u2019t touch your data unless you, first of all, analyze to see what you are doing is okay \u2014 no assumptions. will take you to the class definition will take you to a definition of what\u2019s under the cursor to find the usage of what\u2019s under the cursor You can switch between tabs with and , With you can add a new tab; and with a regular or you close a tab. If you map and to your F7/F8 keys you can easily switch between files. Slowly but surely, what used to be just \u201cmagic\u201d start to look familiar. As you can see, returns which is fast.ai concept that wraps data and PyTorch model: Inside of you see how it is creating which we now know more about. is used to register a list of layers. We will talk about next week, but rest, we have seen before. Similarly, we now understand what\u2019s going on in the function. call embedding layer with ith categorical variable and concatenate them all together go through each one of our linear layers, call it, apply relu and dropout then final linear layer has a size of 1 if is passed in, apply sigmoid and fit the output within a range (which we learned last week) To make sure we are totally comfortable with SGD, we will use it to learn . If we can solve something with 2 parameters, we can use the same technique to solve 100 million parameters. # Here we generate some fake data\n\ndef lin(a,b,x): return a*x+b\n\n\n\ndef gen_fake_data(n, a, b):\n\n x = s = np.random.uniform(0,1,n) \n\n y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n\n return x, y\n\n\n\nx, y = gen_fake_data(50, 3., 8.)\n\n\n\nplt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\"); To get started, we need a loss function. This is a regression problem since the output is continuous output, and the most common loss function is the mean squared error (MSE). Regression \u2014 the target output is a real number or a whole vector of real numbers We will make 10,000 more fake data and turn them into PyTorch variables because Jeremy doesn\u2019t like taking derivatives and PyTorch can do that for him: Then create random weight for and , they are the variables we want to learn, so set . Then set the learning rate and do 10,000 epoch of full gradient descent (not SGD as each epoch will look at all of the data): learning_rate = 1e-3\n\nfor t in range(10000):\n\n # Forward pass: compute predicted y using operations on Variables\n\n loss = mse_loss(a,b,x,y)\n\n if t % 1000 == 0: print(loss.data[0])\n\n \n\n # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n\n # After this call a.grad and b.grad will be Variables holding the gradient\n\n # of the loss with respect to a and b respectively\n\n loss.backward()\n\n \n\n # Update a and b using gradient descent; a.data and b.data are Tensors,\n\n # a.grad and b.grad are Variables and a.grad.data and b.grad.data are Tensors\n\n a.data -= learning_rate * a.grad.data\n\n b.data -= learning_rate * b.grad.data\n\n \n\n # Zero the gradients\n\n a.grad.data.zero_()\n\n b.grad.data.zero_()\n\ncalculate the loss (remember, and are set to random initially) from time to time (every 1000 epochs), print out the loss will calculate gradients for all variables with and fill in property update to whatever it was minus LR * ( accesses a tensor inside of a variable) when there are multiple loss functions or many output layers contributing to the gradient, PyTorch will add them together. So you need to tell when to set gradients back to zero ( in the means that the variable is changed in-place). The last 4 lines of code is what is wrapped in function Let\u2019s do this just Numpy (without PyTorch) [1:07:01] We actually have to do calculus, but everything else should look similar: Just for fun, you can use to animate: Tip: Fast.ai AMI did not come with . So if you see Run and print out a list of available MovieWriters If is among it. Otherwise install it. Let\u2019s learn how to write philosophy like Nietzsche. This is similar to a language model we learned in lesson 4, but this time, we will do it one character at a time. RNN is no different from what we have already learned. All shapes are activations (an activation is a number that has been calculated by a relu, matrix product, etc.). An arrow is a layer operation (possibly more than one). Check out Machine Learning course lesson 9\u201311 for creating this from scratch. We will cover how to flatten a layer next week more, but the main method is called \u201cadaptive max pooling\u201d \u2014 where we average across the height and the width and turn it into a vector. dimension and activation function (e.g. relu, softmax) are not shown here We are going to implement this one for NLP. Input can be one-hot-encoded character (length of vector = # of unique characters) or a single integer and pretend it is one-hot-encoded by using an embedding layer. The difference from the CNN one is that then char 2 inputs gets added. Let\u2019s implement this without torchtext or fast.ai library so we can see. 'PREFACE\n\n\n\n\n\nSUPPOSING that Truth is a woman--what then? Is there not ground\n\nfor suspecting that all philosophers, in so far as they have been\n\ndogmatists, have failed to understand women--that the terrible\n\nseriousness and clumsy importunity with which they have usually paid\n\ntheir addresses to Truth, have been unskilled and unseemly methods for\n\nwinning a woman? Certainly she has never allowed herself ' Always good to put a null or an empty character for padding. Mapping of every character to a unique ID, and a unique ID to a character char_indices = dict((c, i) for i, c in enumerate(chars))\n\nindices_char = dict((i, c) for i, c in enumerate(chars)) Now we can represent the text with its ID\u2019s: idx = [char_indices[c] for c in text]\n\nidx[:10] Generally, you want to combine character level model and word level model (e.g. for translation). Character level model is useful when a vocabulary contains unusual words \u2014 which word level model will just treat as \u201cunknown\u201d. When you see a word you have not seen before, you can use a character level model. There is also something in between that is called Byte Pair Encoding (BPE) which looks at n-gram of characters. cs = 3 \n\nc1_dat = [idx[i] for i in range(0, len(idx)-cs, cs)]\n\nc2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\n\nc3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\n\nc4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)] Note that since we are skipping by 3 (the third argument of ) \u2019s are our inputs, is our target value. \u2014 the size of the embedding matrix. Here is the updated version of the previous diagram. Notice that now arrows are colored. All the arrows with the same color will use the same weight matrix. The idea here is that a character would not have different meaning (semantically or conceptually) depending on whether it is the first, the second, or the third item in a sequence, so treat them the same.\n\n[1:29:58] It is important that this uses a square weight matrix whose size matches the output of . Then and will be the same shape allowing us to sum them together as you see in is only there to make the three lines identical to make it easier to put in a for loop later. We will reuse [1:32:20]. If we stack , , and , we will get , , in the method. will come in handy when you want to train a model in raw-er approach, what you put in , you will get back in Because it is a standard PyTorch model, don\u2019t forget \u201cVariabize\u201d the tensor, and put it through the model \u2014 which will give us 512x85 tensor containing prediction (batch size * unique character) Create a standard PyTorch optimizer \u2014 for which you need to pass in a list of things to optimize, which is returned by We do not find a learning rate finder and SGDR because we are not using , so we would need to manually do learning rate annealing (set LR a little bit lower) def get_next(inp):\n\n idxs = T(np.array([char_indices[c] for c in inp]))\n\n p = m(*VV(idxs))\n\n i = np.argmax(to_np(p))\n\n return chars[i] This function takes three characters and return what the model predict as the fourth. Note: returns index of the maximum values. We can simplify the previous diagram as below: Let\u2019s implement this. This time, we will use the first 8 characters to predict the 9th. Here is how we create inputs and output just like the last time: c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)] Notice that they are overlaps (i.e. 0\u20137 to predict 8, 1\u20138 to predict 9). class CharLoopModel(nn.Module):\n\n # This is an RNN!\n\n def __init__(self, vocab_size, n_fac):\n\n super().__init__()\n\n self.e = nn.Embedding(vocab_size, n_fac)\n\n self.l_in = nn.Linear(n_fac, n_hidden)\n\n self.l_hidden = nn.Linear(n_hidden, n_hidden)\n\n self.l_out = nn.Linear(n_hidden, vocab_size)\n\n \n\n def forward(self, *cs):\n\n bs = cs[0].size(0)\n\n h = V(torch.zeros(bs, n_hidden).cuda())\n\n for c in cs:\n\n inp = F.relu(self.l_in(self.e(c)))\n\n h = F.tanh(self.l_hidden(h+inp))\n\n \n\n return F.log_softmax(self.l_out(h), dim=-1) Most of the code is the same as before. You will notice that there is one loop in function. It is a sigmoid that is offset. It is common to use hyperbolic tanh in the hidden state to hidden state transition because it stops it from flying off too high or too low. For other purposes, relu is more common. This now is a quite deep network as it uses 8 characters instead of 2. And as networks get deeper, they become harder to train. We now will try something else for [1:46:04]. The reason is that the input state and the hidden state are qualitatively different. Input is the encoding of a character, and h is an encoding of series of characters. So adding them together, we might lose information. Let\u2019s concatenate them instead. Don\u2019t forget to change the input to match the shape ( instead of ). class CharLoopConcatModel(nn.Module):\n\n def __init__(self, vocab_size, n_fac):\n\n super().__init__()\n\n self.e = nn.Embedding(vocab_size, n_fac)\n\n self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n\n self.l_hidden = nn.Linear(n_hidden, n_hidden)\n\n self.l_out = nn.Linear(n_hidden, vocab_size)\n\n \n\n def forward(self, *cs):\n\n bs = cs[0].size(0)\n\n h = V(torch.zeros(bs, n_hidden).cuda())\n\n for c in cs:\n\n inp = torch.cat((h, self.e(c)), 1)\n\n inp = F.relu(self.l_in(inp))\n\n h = F.tanh(self.l_hidden(inp))\n\n \n\n return F.log_softmax(self.l_out(h), dim=-1) This gives some improvement. PyTorch will write the loop automatically for us and also the linear input layer. For reasons that will become apparent later on, will return not only the output but also the hidden state. The minor difference in PyTorch is that will append a new hidden state to a tensor instead of replacing (in other words, it will give back all ellipses in the diagram) . We only want the final one so we do In PyTorch version, a hidden state is rank 3 tensor (in our version, it was rank 2 tensor) [1:51:58]. We will learn more about this later, but it turns out you can have a second RNN that goes backwards. The idea is that it is going to be better at finding relationships that go backwards \u2014 it is called \u201cbi-directional RNN\u201d. Also you can have an RNN feeds to an RNN which is called \u201cmulti layer RNN\u201d. For these RNN\u2019s, you will need the additional axis in the tensor to keep track of additional layers of hidden state. For now, we will just have 1 there, and get back 1. def get_next(inp):\n\n idxs = T(np.array([char_indices[c] for c in inp]))\n\n p = m(*VV(idxs))\n\n i = np.argmax(to_np(p))\n\n return chars[i] def get_next_n(inp, n):\n\n res = inp\n\n for i in range(n):\n\n c = get_next(inp)\n\n res += c\n\n inp = inp[1:]+c\n\n return res get_next_n('for thos', 40)\n\n'for those the same the same the same the same th' This time, we loop times calling each time, and each time we will replace our input by removing the first character and adding the character we just predicted. For an interesting homework, try writing your own \u201c \u201d without looking at PyTorch source code. From the last diagram, we can simplify even further by treating char 1 the same as char 2 to n-1. You notice the triangle (the output) also moved inside of the loop, in other words, we create a prediction after each character. Predicting chars 2 to n using chars 1 to n-1 One of the reasons we may want to do this is the redundancies we had seen before: We can make it more efficient by taking non-overlapping sets of character this time. Because we are doing multi-output, for an input char 0 to 7, the output would be the predictions for char 1 to 8. This will not make our model any more accurate, but we can train it more efficiently. Notice that we are no longer doing since we want to keep all of them. But everything else is identical. One complexity[2:00:37] is that we want to use the negative log-likelihood loss function as before, but it expects two rank 2 tensors (two mini-batches of vectors). But here, we have rank 3 tensor: Transpose the first two axes because PyTorch expects 1. sequence length (how many time steps), 2. batch size, 3. hidden state itself. is 512 by 8, whereas is 8 by 512. PyTorch does not generally actually shuffle the memory order when you do things like \u2018transpose\u2019, but instead it keeps some internal metadata to treat it as if it is transposed. When you transpose a matrix, PyTorch just updates the metadata . If you ever see an error that says \u201cthis tensor is not continuous\u201d , add after it and error goes away. is same as . indicates as long as it needs to be. Remember that is the lowest level fast.ai abstraction that implements the training loop. So all the arguments are standard PyTorch things except for which is our model data object which wraps up the test set, the training set, and the validation set. Question [2:06:04]: Now that we put a triangle inside of the loop, do we need a bigger sequence size? If we have a short sequence like 8, the first character has nothing to go on. It starts with an empty hidden state of zeros. We will learn how to avoid that problem next week. The basic idea is \u201cwhy should we reset the hidden state to zeros every time?\u201d (see code below). If we can line up these mini-batches somehow so that the next mini-batch joins up correctly representingthe next letter in Nietsche\u2019s works, then we can move to the constructor. is a loop applying the same matrix multiply again and again. If that matrix multiply tends to increase the activations each time, we are effectively doing that to the power of 8 \u2014 we call this a gradient explosion. We want to make sure the initial will not cause our activations on average to increase or decrease. A nice matrix that does exactly that is called identity matrix: We can overwrite the randomly initialized hidden-hidden weight with an identity matrix: This was introduced by Geoffrey Hinton et. al. in 2015 (A Simple Way to Initialize Recurrent Networks of Rectified Linear Units) \u2014 after RNN has been around for decades. It works very well, and you can use higher learning rate since it is well behaved."
    },
    {
        "url": "https://medium.com/@hiromi_suenaga/deep-learning-2-part-1-lesson-5-dd904506bee8?source=user_profile---------14----------------",
        "title": "Deep Learning 2: Part 1 Lesson 5 \u2013 Hiromi Suenaga \u2013",
        "text": "Each prediction is a dot product of movie embedding vector and user embedding vector. In linear algebra term, it is equivalent of matrix product as one is a row and one is a column. If there is no actual rating, we set the prediction to zero (think of this as test data \u2014 not training data). We then use Gradient Descent to minimize our loss. Microsoft excel has a \u201csolver\u201d in the add-ins that would minimize a variable by changing selected cells ( is the method you want to use). This can be called \u201cshallow learning\u201d (as opposed to deep learning) as there is no nonlinear layer or a second linear layer. So what did we just do intuitively? The five numbers for each movie is called \u201cembeddings\u201d (latent factors) \u2014 the first number might represent how much it is sci-fi and fantasy, the second might be how much special effect is used for a movie, the third might be how dialog driven it is, etc. Similarly, each user also has 5 numbers representing, for example, how much does the user like sci-fi fantasy, special effects, and dialog-driven in movies. Our prediction is a cross product of these vectors. Since we do not have every movie review for every user, we are trying to figure out which movies are similar this movie and how other users who rated other movies similarly to this user rate this movie (hence the name \u201ccollaborative\u201d). What do we do with a new user or a new movie \u2014 do we have to retrain a model? We do not have a time to cover this now, but basically you need to have a new user model or a new movie model that you would use initially and over time you will need to re-train the model. This should look familiar by now. We create a validation set by picking random set of ID\u2019s. is a weight decay for L2 regularization, and is how big an embedding matrix we want. We then get a learner that is suitable for the model data, and fit the model: Since the output is Mean Squared Error, you can take RMSE by: The output is about 0.88 which outperforms the bench mark of 0.91. You can get a prediction in a usual way: And you can also plot using seaborn (built on top of matplotlib):\n\nWhen we have a mathematical operator between tensors in numpy or PyTorch, it will do element-wise assuming that they both have the same dimensionality. The below is how you would calculate the dot product of two vectors (e.g. (1, 2)\u22c5(2, 2) = 6 \u2014 the first rows of matrix a and b): We do this by creating a Python class that extends and override function. Now we can call it and get the expected result (notice that we do not need to say to call the function \u2014 it is a PyTorch magic.): This implementation has two additions to the class: Look up our users and movies in above embedding matrices It is quite possible that user ID\u2019s are not contiguous which makes it hard to use as an index of embedding matrix. So we will start by creating indexes that starts from zero and contiguous and replace column with the index by using Panda\u2019s function with an anonymous function and do the same for . Tip: is a handy line of code to keep in your tool belt! Note that is a constructor which is now needed because our class needs to keep track of \u201cstates\u201d (how many movies, mow many users, how many factors, etc). We initialized the weights to random numbers between 0 and 0.05 and you can find more information about a standard algorithm for weight initialization, \u201cKaiming Initialization\u201d here (PyTorch has He initialization utility function but we are trying to do things from scratch here) [46:58]. is not a tensor but a variable. A variable does the exact same operations as a tensor but it also does automatic differentiation. To pull a tensor out of a variable, call attribute. All the tensor functions have a variation with trailing underscore (e.g. ) will do things in-place. We are reusing (from fast.ai library) from Rossmann notebook, and that is the reason behind why there are both categorical and continuous variables in function in class [50:20]. Since we do not have continuous variable in this case, we will ignore and use the first and second columns of as and . Note that they are mini-batches of users and movies. It is important not to manually loop through mini-batches because you will not get GPU acceleration, instead, process a whole mini-batch at a time as you see in line 3 and 4 of function above [51:00\u201352:05]. is what gives us the optimizers in PyTorch. is one of the function inherited from that gives us all the weight to be updated/learned. This function is from fast.ai library [54:40] and is closer to regular PyTorch approach compared to we have been using. It will not give you features like \u201cstochastic gradient descent with restarts\u201d or \u201cdifferential learning rate\u201d out of box. Bias \u2014 to adjust to generally popular movies or generally enthusiastic users. is PyTorch version of broadcasting [1:04:11] for more information, see Machine Learning class or numpy documentation. Can we squish the ratings so that it is between 1 and 5? Yes! By putting the prediction through sigmoid function will result in number between 1 and 0. So in our case, we can multiply that by 4 and add 1 \u2014 which will result in number between 1 and 5. is a PyTorch functional ( ) that contains all functions for tensors, and is imported as in most cases. Let\u2019s take a look at fast.ai code [1:13:44] we used in our Simple Python version. In file, calls function that creates class that is identical to what we created. We go back to excel sheet to understand the intuition. Notice that we create user_idx to look up Embeddings just like we did in the python code earlier. If we were to one-hot-encode the user_idx and multiply it by user embeddings, we will get the applicable row for the user. If it is just matrix multiplication, why do we need Embeddings? It is for computational performance optimization purposes.\n\nRather than calculating the dot product of user embedding vector and movie embedding vector to get a prediction, we will concatenate the two and feed it through neural net. Notice that we no longer has bias terms since layer in PyTorch already has a build in bias. is a number of activations a linear layer creates (Jeremy calls it \u201cnum hidden\u201d). It only has one hidden layer, so maybe not \u201cdeep\u201d, but this is definitely a neural network. Notice that the loss functions are also in (here, it s mean squared loss). Now that we have neural net, there are many things we can try: Use different embedding sizes for user embedding and movie embedding Not only user and movie embeddings, but append movie genre embedding and/or timestamp from the original data. What is happening in the training loop? [1:33:21] Currently, we are passing off the updating of weights to PyTorch\u2019s optimizer. What does an optimizer do? and what is a ? We are going to implement gradient descent in an excel sheet (graddesc.xlsm) \u2014 see worksheets right to left. First we create a random x\u2019s, and y\u2019s that are linearly correlated with the x\u2019s (e.g. y= a*x + b). By using sets of x\u2019s and y\u2019s, we will try to learn a and b. To calculate the error, we first need a prediction, and square the difference:\n\nTo reduce the error, we increase/decrease a and b a little bit and figure out what would make the error decrease. This is called finding the derivative through finite differencing. Finite differencing gets complicated in high dimensional spaces [1:41:46], and it becomes very memory intensive and takes a long time. So we want to find some way to do this more quickly. It is worthwhile to look up things like Jacobian and Hessian (Deep Learning book: section 4.3.1 page 84). The faster approach is to do this analytically [1:45:27]. For this, we need a chain rule: Here is a great article by Chris Olah on Backpropagation as a chain rule. Now we replace the finite-difference with an actual derivative WolframAlpha gave us (notice that finite-difference output is fairly close to the actual derivative and good way to do quick sanity check if you need to calculate your own derivative): And this is how you do SGD with excel sheet. If you were to change the prediction value with the output from CNN spreadsheet, we can train CNN with SGD. Come on, take a hint \u2014 that\u2019s a good direction. Please keep doing that but more. With this approach, we will use a linear interpolation between the current mini-batch\u2019s derivative and the step (and direction) we took after the last mini-batch (cell K9): Compared to de/db whose sign (+/-) is random, the one with momentum will keep going the same direction a little bit faster up till certain point. This will reduce a number of epochs required for training. Adam is much faster but the issue has been that final predictions are not as good as as they are with SGD with momentum. It seems as though that it was due to the combined usage of Adam and weight decay. The new version that fixes this issue is called AdamW.\n\n: a linear interpolation of derivative and previous direction (identical to what we had in momentum) : a linear interpolation of derivative squared + derivative squared from last step ( ) The idea is called \u201cexponentially weighted moving average\u201d (in another words, average with previous values multiplicatively decreased) Learning rate is much higher than before because we are dividing it by square root of . If you take a look at fast.ai library (model.py), you will notice that in function, it does not just calculate average loss, but it is calculating the exponentially weighted moving average of loss. Another helpful concept is whenever you see `\u03b1(\u2026) + (1-\u03b1)(\u2026)`, immediately think linear interpolation. We calculated exponentially weighted moving average of gradient squared, take a square root of that, and divided the learning rate by it. When there is high variance in gradients, gradient squared will be large. When the gradients are constant, gradient squared will be small. If gradients are changing a lot, we want to be careful and divide the learning rate by a big number (slow down) If gradients are not changing much, we will take a bigger step by dividing the learning rate with a small number Adaptive learning rate \u2014 keep track of the average of the squares of the gradients and use that to adjust the learning rate. So there is just one learning rage, but effectively every parameter at every epoch is getting a bigger jump if the gradient is constant; smaller jump otherwise. There are two momentums \u2014 one for gradient, and the other for gradient squared (in PyTorch, it is called a beta which is a tuple of two numbers) When there are much more parameters than data points, regularizations become important. We had seen dropout previously, and weight decay is another type of regularization. Weight decay (L2 regularization) penalizes large weights by adding squared weights (times weight decay multiplier) to the loss. Now the loss function wants to keep the weights small because increasing the weights will increase the loss; hence only doing so when the loss improves by more than the penalty. The problem is that since we added the squared weights to the loss function, this affects the moving average of gradients and the moving average of the squared gradients for Adam. This result in decreasing the amount of weight decay when there is high variance in gradients, and increasing the amount of weight decay when there is little variation. In other words, \u201cpenalize large weights unless gradients varies a lot\u201d which is not what we intended. AdamW removed the weight decay out of the loss function, and added it directly when updating the weights."
    }
]