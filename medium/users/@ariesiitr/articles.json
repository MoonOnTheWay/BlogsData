[
    {
        "url": "https://medium.com/@ariesiitr/image-processing-with-opencv-45c3a5cefd10?source=user_profile---------1----------------",
        "title": "Image Processing with OpenCV \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Visual information is the most important type of information perceived, processed and interpreted by the human brain. Image processing is a method to perform some operations on an image, in order to extract some useful information from it. An image is nothing more than a two dimensional matrix (3-D in case of coloured images) which is defined by the mathematical function f(x,y) where x and y are the two co-ordinates horizontally and vertically. The value of f(x,y) at any point is gives the pixel value at that point of an image, the pixel value describes how bright that pixel is, and/or what color it should be.\n\nFor grayscale images the pixel value is a single number that represents the brightness of that pixel, the most common pixel format is the byte image, which is stored as an 8-bit integer giving a range of possible values from 0 to 255. As a convention is taken to be black, and 255 is taken to be white the values in between make up the different shades of gray.\n\nTo represent color images, separate red, green and blue components must be specified for each pixel (assuming a RGB color model), and so the pixel `value\u2019 becomes a vector of three numbers. Often the three different components are stored as three separate `grayscale\u2019 images known as color planes (one for each of red, green and blue), which have to be recombined when displaying or processing.\n\nNow allow me to introduce the color models formally as follows, (citing wiki) a color model is an abstract mathematical model describing the way colors can be represented as tuples of numbers, typically as three or four values or color components. When this model is associated with a precise description of how the components are to be interpreted (viewing conditions, etc.), the resulting set of colors is called color space.\n\nOnce known how the images could be represented, let\u2019s focus on the image processing side and specifically with OpenCV and python.\n\nSub-tasks in image processing could be categorized as follows :\n\nOpenCV gives the flexibility to capture image directly from a pre-recorded video stream, camera input feed, or a directory path.\n\nDepending on the use case there are various methods which could be applied, some very common ones are as follows :\n\nRepresentation of intensity distribution vs no. of pixels of an image is termed as the histogram.\n\nEqualization stretches out the intensity range in order to suit contrast levels appropriately. More resources here and here.\n\nErosion and Dilation belong to the group of morphological transformations and widely used together for the treatment of noise or detection of intensity bumps.\n\nHere\u2019s an extensive resource on the same from the documentation.\n\nNoise has a very peculiar property that its mean is zero, and this is what helps in its removal by averaging it out.\n\nOpenCV provides four variations of this technique.\n\nExplicit definitions of each could be found here.\n\nOnce the pre-processed image is ready, information could be extracted from it. This is the part from where the code base begins to get implementation specific and purely inclined towards the end goals.\n\nThis part could not be generalized as the prior ones but rather has to be discovered along the way building up the module.\n\nFinally closing in with this articles, I would like to encourage you to read the below mentioned follow up links :"
    },
    {
        "url": "https://medium.com/@ariesiitr/electronic-prototyping-boards-32d35da9d75a?source=user_profile---------2----------------",
        "title": "Electronic Prototyping Boards \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Moreover, the Arduino is not a single board, it is a family of boards , sensors, and shields with different functionalities from where you can choose the best suiting your application.\n\nArduino is an open source and one of the most widely used electronic prototyping boards. Its heart is AVR ATmega328 microcontroller which has its own microprocessor, RAM and EEPROM. It can be used to control various devices, from a small LED or a motor to anything which you can think of. Different types of hardware and sensors can be interfaced with Arduino using relays, power outlets and shields. It comes with an easy to use software platform to program its IC using basic C and C++. So, you can tell your board what to do by sending a set of instructions to the microcontroller on the board. It works with digital input-output and analog input signals. It also facilitates serial communication using various protocols like I2C , SPI and UART. You can learn more about these protocols to decide which to use.\n\nWhenever we think of any system or project, the first basic thing that comes to mind is that it is going to take some inputs, process them and produce some results. I am going to introduce you to two such development boards where you can implement almost anything which can be categorized as an electronic system. These platforms can help you take your design from paper to actual working systems or we can say from imagination to reality. So, let\u2019s get familiar with Arduino and Raspberry Pi.\n\nIt is the best device to start with and get basic hands on experience for electronic enthusiasts on prototyping. Thanks to its large and diverse community of users, troubleshooting and experimenting with Arduino has become so easy that even children can use it.\n\nThese are some of the best Arduino tutorials for beginners.\n\nLearn more about Arduino and its applications refer to these blogs:\n\nLearn and explore the possibilities and know what people have done with this amazing board.\n\nRaspberry Pi has nothing to do either with Raspberry or with Pi. You won\u2019t believe this, but it is a complete computer in itself. It is a cheap and small computer originally developed for educational and programming purposes but soon it gained huge popularity among electronic enthusiasts. It became popular especially for applications where people wanted to use something more than just a microcontroller but cannot use a full computer.\n\nIt is slower than a computer but has all the features similar to a Linux based computer. It has USB Ports, Ethernet Port, GPIO headers, HDMI Port, MicroSD card slot, Audio Output pin and Power Supply pin. It has its own CPU, GPU, 1GB Memory and built in Bluetooth and Wifi Modules in the latest version. Ever imagined if you can hold your computer in your palm!\n\nAs a result of all these extraordinary features, it is used in various important and complex applications in various fields. It has a well documented and updated resources which make it easy to learn and use.\n\nEverything related to Raspberry Pi, from tutorials to project ideas can be found on its website. You can find some good tutorials here also.\n\nBoth, Pi and Arduino have their own special features and limitations. Now you have to decide what are you going to use for your project. Keep thinking and come up with some great ideas and see them working.\n\nGet ready to make exciting projects with them!"
    },
    {
        "url": "https://medium.com/@ariesiitr/nlp-3ee666add774?source=user_profile---------3----------------",
        "title": "Natural Language Processing \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "NLP is a branch that consists of analyzing, understanding, and deriving information from the text data in a smart and efficient manner.\n\nNLP is used to analyze text, allowing machines to understand how human\u2019s speak. This human-computer interaction enables real-world applications like automatic text summarization, sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction, stemming, and more. NLP is commonly used for text mining, machine translation, and automated question answering.\n\nSome basic tasks of NLP are:\n\nBy utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as \u2014 automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n\nHow to represent the sentence for processing?\n\nThere are multiple ways to build a sentence vector representation:\n\n1. Bag-of-words: Where we have each word as a dimension (and hence sentence is a vector with dimension |V|, where V is the vocabulary). Each word dimension is given a value equal to number of word occurrences in the sentence.\n\n2. TFIDF based: Where in bag of words , tf\u2013idf is used instead of number of word occurrence in sentence.\n\n3. Word embedding: For representing words, a sentence vector is made by neural networks (recursively combining word embeddings with a generative model like recursive/recurrent Neural Net or using some other non-neural network algorithm like doc2vec for this purpose). Here the sentence vector generally has a similar shape when compared to word embeddings.\n\nThe following models a text document using bag-of-words.\n\nHere are two simple text documents:\n\nBased on these two text documents, a list is constructed as follows:\n\nIn practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \u201cbag of words\u201d, we can calculate various measures to characterize the text. The most common type of characteristics, or features calculated from the Bag-of-words model is term frequency, namely, the number of times a term appears in the text. For the example above, we can construct the following two lists to record the term frequencies of all the distinct words:\n\nConceptually, we can view bag-of-word model as a special case of the n-gram model, with n=1.\n\nConnect with ArIES to know more.\n\nSee the following links for implementation of various NLP tasks:"
    },
    {
        "url": "https://medium.com/@ariesiitr/https-medium-com-ariesiitr-aries-recruitment-2017-c98c927828d6?source=user_profile---------4----------------",
        "title": "ArIES Recruitment 2017 \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Artificial Intelligence and Electronics Society (ArIES) is an open campus group of IITR, with a mission to solve impactful problems with Artificial Intelligence and Electronics. We always welcome suggestions, ideas and exciting problems to work on.\n\nWe also represent our institute in Inter IIT Technical Meet where project members of some of our selected projects compete at a national level. We also showcase our work in Srishti. We take up projects of Institute level importance such as Licence Plate Recognition System to be deployed at various gates of the Institute.\n\nWe openly welcome talented minds from IITR. To properly align the interests of the members with the project requirements, we have come up with this selection process. The steps in the selection process for project allotment are as follows:\n\nAugmented Reality Models: You will be learning to make 3D Models of real life devices for Virtual Reality Headsets.\n\nAutomated Soldering Station: You will be making a setup that can automatically solder on a perf board according to the given input.\n\nBiometric Lock: A biometric system to control the lock on doors. This will involve the use of fingerprint sensor, Raspberry Pi and locking mechanisms.\n\nCustomized Hand Writing Bot: Make a bot that can write and further enhance it to mimic any person\u2019s handwriting.\n\nData Acquisition System using ATmega32: Work directly with the microcontroller that makes up the brain behind Arduino. You will be using a GPRS/GSM Module to receive, process and transmit data.\n\nEEG Controlled Bot: Use your brain signals to control real world objects, starting with a small bot.\n\nHaunting Eye: Eyes that stare right at you and follow your movement. You will use image processing to detect a person and use robotic eyes to follow them.\n\nLED Cube (8 x 8 x 8): You will be devising a setup that can show a complete 3D image by exploiting persistence of vision to rapidly combine smaller images.\n\nMicrophone slot for acoustic guitar: A system to control the sound effects of your acoustic guitar just like an electric guitar. You will learn to design an analog amplifier with active filters to attenuate noise and will apply signal processing techniques on Raspberry Pi to give the sound effects of an Electric Guitar.\n\nPropeller Clock: You will be devising a setup that can show a complete image by exploiting persistence of vision to rapidly combine smaller images.\n\nQuadcopter: Make your own flying machines and make them do cool stuff. Let your imaginations fly high.\n\nRecommendation System: Just like IMDb, we propose a recommendation system for our academic course work, suggesting electives according to your interests and user ratings.\n\nSelf-balancing bot using Reinforcement Learning: The project aims at balancing an unstable robot using PID controllers initially and then using reinforcement learning.\n\nTempescope: A live weather box to simulate real time weather of any location of your choice. It shall be able to simulate various weather conditions like thunderstorm, rainfall, fog, etc.\n\nText to Narrate Film: You will use Machine Learning to make E-books more interactive by showing relevant images alongside."
    },
    {
        "url": "https://medium.com/@ariesiitr/introduction-to-deep-learning-for-computer-vision-c11a544eb2b3?source=user_profile---------5----------------",
        "title": "Introduction to Deep Learning For Computer Vision \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "I will advise the reader to first go through this blog on Artificial Neural Networks.\n\nFrom the biological science point of view, computer vision aims to come up with computational models of the human visual system. From the engineering point of view, computer vision aims to build autonomous systems which could perform some of the tasks which the human visual system can perform (and even surpass it in many cases). The two goals are of course intimately related. Deep learning is growing rapidly and is surpassing traditional approaches for machine learning since 2012 by a factor of approximately 10%\u201320% in accuracy. This blog gives an introduction to Deep Learning and its application in Computer Vision.\n\nFor computer vision tasks, special architecture of Deep Learning is used and that is called a Convolutional Neural Network. Firstly we look into the basic components of a Conv net:\n\nA Conv layer consists of spatial filters that are convolved along the spatial dimensions and summed up along the depth dimension of the input volume. Due to weight sharing, they are much more efficient than fully-connected layers. A Conv layer has w\u22c5h\u22c5d\u22c5n number of parameters without bias (w is width of the filter, h is height of the filter, d is depth of the filter, n is number of filters) that need to be learned during training. In general one starts with a large filter size (e.g. 11x11) and a low depth (e.g. 32) and reduces the spatial filter dimensions (e.g. to 3x3) while increasing the depth (e.g. to 256) throughout the network.\n\nConv layers are often followed by a Pool layer in order to reduce the spatial dimension of the volume for the next filter \u2014 this is the equivalent of a subsampling operation. The pooling operation itself has no learnable parameters. There are two types of pooling, Max-pooling consists of splitting the input in patches and outputting the maximum value in that patch, whereas in Average pooling we output the average value in that patch.\n\nMost of the time, max-pooling layers are used in Deep Learning models due to the easier gradient computation. During Backpropagation, the gradient only flows in the direction of the single max activation which can be computed very efficiently. A few other architectures use average pooling, mostly at the end of a network or before the fully connected layers and without a noticeable increase in performance.\n\nTo understand all above operations mathematically one can refer to this.\n\nThe Fully Connected(FC) layer connects every output from the previous layer with each neuron. Usually, the FC layer is used at the end to combine all spatially distributed activations of the previous Conv layers. The FC layers have the highest number of parameters (n_input*n_neurons, where n_input is the number of outputs of the previous layer and n_neurons is the number of neurons) in the model most computing time(almost 90%) is spent in the early Conv layers.\n\nThe final output layer of a Deep Neural Network plays a crucial role for the task of the whole network. Common choices are:\n\nAfter defining a final output layer, one need to define as well a loss function for the given task. Picking the right loss is crucial for training a Deep Neural Net, common choices are:\n\nThis section describes how one stacks the components described in the previous section-\n\nThese models contain convolutional layers(with a non-linear activation), pooling layers(non parametric) and Fully connected layers at the end.\n\nA convolution layer extracts image features by convolving the input with multiple filters. It contains a set of 2-dimensional filters that are stacked into a 3-dimensional volume where each filter is applied to a volume constructed from all filter responses of the previous layer. If one considers the RGB channels of a 256x256 sized input image as a 256x256x3 volume, a 5x5x3 filter would be applied along a 5x5 2-dimensional region of the image and summed up across all 3 color channels. If the first layer after the RGB volume consist of 48 filters, it is represented as a volume of 5x5x3x48 weight parameters and 48 bias parameters. Using a convolution operation on the input volume and the filter volumes, the filter response (so called activation) results in an output volume with the dimensions 251x251x48 (using stride 1 and no padding). By padding the input layer with zeroes, one can force to keep the spatial dimensions of the activations constant throughout the layers. Each convolutional layer is followed by a non-linear activation function(preferably ReLU) which results in an activation with the same dimensions as the output volume of the previous convolutional layer. A pooling layer subsamples the previous layer and outputs a volume of same depth but reduced spatial dimensions. Using a max-pooling with 2*2 filter with stride 2(filter shifted for 2 pixels on every iteration), one ends up with a 128*128*48 volume after pooling.\n\nMany layers of convolutions(with activation) and pooling are stacked and output of the last conv-pool layer is fed to a fully connected layer. After fully connected layer the output of this layer is fed to the Final Output Layer which produces the probability for various classes as the output.\n\nFor reference here is the Alexnet architecture:\n\nFor further studying conv nets here are a few useful links:"
    },
    {
        "url": "https://medium.com/@ariesiitr/actuators-and-sensors-246eef0badd9?source=user_profile---------6----------------",
        "title": "Actuators and Sensors \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Actuators are something that control or move things around in a system, for e.g. by actuating a valve we mean to open and close it as per our need. An actuator is requires an energy source and a control signal which can be in the form of an electric or even mechanical signal.\n\nActuators range from hydraulic, pneumatic to even thermal/ magnetic actuators but in this blog we\u2019ll only be talking about electrical actuators i.e. motors.\n\nElectric motors operate with the interaction of winding currents with magnetic fields to generate a rotating torque (or motion) thus converting electrical energy into mechanical energy. There are several types of motors some of which are listed below.\n\nLet\u2019s take an example of an RC toy car, we use DC motors to actuate the motion of the wheels of the car, increasing the angular velocity of the left wheel to turn right and vice versa. All you need to do is apply power, and weeeee it spins, to rotate it in reverse direction just rotate the polarity of the input, while to vary it\u2019s speed simply change the input voltage levels.\n\nBrush-Less DC motors are more power efficient, with significantly lower noise than DC motors. They are often used in pumps, fans and electric vehicles. Moreover, owing to their lightweight high RPM values, most of the drones these days are predominantly based on BLDC motors.\n\nServo motors are not used for continuous rotation (like wheels) instead, they are used for position control systems like robotic arms. There main purpose is to rotate/ move something by a fixed angle/ distance. Unlike DC motors they have a feedback control signal in addition to the power supply.\n\nTheir working principal is similar to DC motors except for the fact that it has multiple coils. In order to generate motor rotation, these coils need to activated in a particular pattern. They can be used for precise step movements such as in the case of a clock.\n\nSensors are the ones that that detect the changes in a system and send the information to a computer processor (usually a microcontroller). Sensors are used in everyday life like touch sensors in your phone to smoke detectors in our homes, we are completely surrounded by sensors. Let\u2019s take a look at some of the sensors that we might be using in our projects.\n\nWhenever you use a compass application on your phone, your phone somehow happens to know the direction your phone is pointing. Also, for a VR Headset, you move your head and you happen to be moving in the application too, ever wondered how is your phone able to get this kind of information. Accelerometers are the key to getting this information.\n\nYou all have an idea about the working of a SONAR, an ultrasonic sensor works on a similar principle. On receiving a signal from a microcontroller, your sensor emits an ultrasonic wave (f>20KHz) and detects an echo after some time. Based on this time interval, it sends a voltage signal to the microcontroller encoding the information of the distance of the object from the sensor."
    },
    {
        "url": "https://medium.com/@ariesiitr/haunting-eye-e39490feb778?source=user_profile---------7----------------",
        "title": "Haunting Eye \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Many of you might have seen the movie Lord of the Rings, and have been fascinated with the all seeing eye of Sauron. Following the fascination with a technological viewpoint, we plan on realizing the same using basic electronic components.\n\nFollowing a person wherever they go, a haunting eye is to be installed in ArIES so as to make the place more interactive and a fun place. Using a camera to detect the faces of the people in a camera frame, and then using a microcontroller for actuating the motors in order to move around the camera (or pseudo eyes) rendering a feeling of being watched all the time :P\n\nMachine learning can be used to detect the basic features of a face and predict its presence in a picture. The following picture gives you a crude idea wherein the model is first trying to identify the individual features of a face like two eyes, a nose, lips etc and then summing up the information received to say this image contains a face in this region. This can also be done using pretrained models available in OpenCV.\n\nRefer to the following link to get basic hands on experience in using OpenCV.\n\nARDUINO\n\nArduino is an open-source platform used for building electronic projects. It consists of both, a physically programmable circuit (or microcontroller) and an application that runs on your PCs (IDE) which is used to upload a piece of code onto the microcontroller. All one needs to do is simply write up a piece of code, connect the microcontroller with your PC using a USB cable and you are good to go.\n\nRaspberry Pi\n\nRaspberry Pi is a small single board computer originally designed to help in prototyping of various projects and educational purposes. This credit card sized computer has been adopted by electronic enthusiasts who require much more processing than a basic microcontroller (like Arduino) along with its functionality of being used as an embedded system.\n\nThe following is an outstanding blog to get started with Arduino and Raspberry Pi.\n\nDC Motors\n\nA DC motor is provided with 2 inputs, a power and a ground pin, which rotates continuously on supplying power, until the power is detached. The speed of rotation of a DC motor can be controlled by PWM(pulse width modulation) thus enabling them to be used in tools, toys, appliances etc. examples are: fans being used in computers for cooling or car wheels controlled by a radio.\n\nServo Motor\n\nA servo motor is a rotary/ linear actuator which is similar to a DC motor except for the fact that you can precisely control the amount of rotation of the motor along with its speed of rotation. It basically consists of 3 pins, a power, a ground and a control pin. The motor is continuously supplied with power with the servo motor control circuit changing the draw to drive the servo motor. They are used for applications like moving a robotic arm or controlling a rudder on a boat.\n\nFor more information regarding motors, you can visit this website."
    },
    {
        "url": "https://medium.com/@ariesiitr/shivanshu-iitr-62df5b9f3535?source=user_profile---------8----------------",
        "title": "Led Grid Display \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Umm let me start this in this way\u2026\n\nWe all picked engineering for a reason and among most of us the main driving factor to become an engineer is that we love to innovate and we love to tinker with technology. I bet you that you\u2019ve tinkered with almost each and every electronic toy you had in your childhood. And to be honest, Tony Stark\ud83d\udc7b plays a major role in the creation of this innovative mindset for the current generation. This innovation and recreation instinct is what makes us an engineer and I\u2019m sure so many of you are very enthusiastic to know more about Electronics culture in the campus.\n\nSo, if you\u2019re Looking for a quick and easy weekend craft project, I\u2019ve got you covered! Today I\u2019ll show you how to make a Display! Calm down I\u2019m not giving you an alternate display to watch movies when you\u2019re free from your \u201cgripping lectures\u201d(IYKWIM \ud83d\ude1c ). I\u2019m just taking you back to the good old days when solid state devices came into existence and replaced old high power consumption devices like vacuum tube diode, which were used for display purposes before the semiconductor devices came into existence. So, if you want to know more about the history of evolution of Display, I would love to tell you about it in brief and if you\u2019re not in mood to go through the history, you can skip the history part and start learning about the project.\n\nWith all of the advancements being made in display technology, it\u2019s hard to believe that the knowledge used to create and develop this technology is over a hundred years old. In fact, the first baby steps into the field of display technology began as far back as 1897 when Karl Ferdinand Braun, a physicist and inventor, built the first Cathode-Ray Tube. This small tube would enable the very first televisions to be built and create an industry that has advanced in leaps and bounds from its humble beginnings.\n\nThe second major break in display technology came ten years after Karl Ferdinand Braun in 1907 with the discovery of Electroluminescence. This naturally occurring phenomenon would provide the first breakthrough for LED technology. 1952 saw the development of the very first curved screen display, and was installed in only a handful of movie theaters across the country. That technology would not be available to consumers for over fifty years.\n\nThe next big step forward in display history was the invention of the first LED bulb in 1961. Robert Biard and Gary Pittman patented the first infrared LED light for Texas Instruments. The very next year Nick Holonyack produced the first visible light LED. Two years later, in 1964, display technology took another leap with the invention of LCD and Plasma screens by American inventor James Fergason.\n\nEven though touchscreen smartphone displays are relatively new technology, the first of these displays was invented as early as 1965 and was first used for air traffic controllers. Similarly HDTV really gets its start in Japan during the 1960s and 1970s, even though HDTVs don\u2019t reach the U.S. until 1998. By the time you get to displays in the 90s, OLEDs have been invented by Kodak and we have the first full color plasma displays.\n\nIncredibly, the display industry has rapidly expanded and will continue to do so. Displays of various sizes, forms and technology will be developed for diverse applications.\n\nWith it the necessity for accurate display testing systems will grow considerably as well. Konica Minolta has a number of display measurement systems available to meet the ever-changing needs in the display industry.\n\nEr\u2026 the best example of the project which you\u2019re about to build is Railway Station display board, Airport Displays, Sensex display board etc. and it is widely used as a cheap Display board around the world.\n\n So, here we go\u2026.\n\nWhat we are trying to build is a matrix made up of LEDs in which each Led will act as a working pixel(a minute area of illumination on a display screen, one of many from which an image is composed). By the help of this matrix we can display some basic characters. As the pixel spacing is not dense in our project, we can display only some basic characters.\n\nThe trick behind the display is multiplexing(I\u2019m not explaining the details right here in the blog, you can contact me if you want to know more). It is basically a way to split information into little pieces and send it one by one. In this way you can save a lot of pins on the Arduino and keep your program quite simple. In a dot matrix display, multiple LEDs are wired together in rows and columns. This is done to minimize the number of pins required to drive them. We have 3 shift registers which multiply the number of outputs and save lots of Arduino pins.\n\nEach shift register has 8 outputs and you only need 3 Arduino pins to control these shift registers.\n\nI also used the 4017 decade counter to scan the rows, and you can scan up to 10 rows with it because you have only 10 outputs but to control it you need only 2 Arduino pins.\n\n1) LEDs- we are using through hole leds as they are easy to handle, preferably use clear lens led, although my led grid was made up of diffused lens Leds.\n\n2) Resistors-To limit voltage and current through Led, rating of led used is Forward voltage 2V, forward current= 20mA.\n\n3) Decade counter - It is a serial digital counter that counts ten digits. The 4017 is a very useful chip and it\u2019s a good idea to know how to work with it.\n\n4) Arduino- It is an open-source platform used for building electronics projects. If you\u2019re not familiar with Arduino I would suggest to follow the blog\n\n5) Shift Registers- This is a memory storage device which is used in various way to reduce complexation of the hardware but increases complexation in coding. We are using it to reduce number of pins of microcontroller used.\n\nSoldering 144 LEDs in a matrix formation can be a little tricky if you don\u2019t have a general idea how. You need to bend the positive lead of the LED down towards the other ones and make a column, and snip off the leads you didn\u2019t use and try to make the connections as low as you can get, and you do this to all of the positive leads.\n\nNegative leads are connected in a column and that\u2019s make soldering tricky because the positive rows are in the way, so you will need to make a 90 degrees bend with the negative lead and make a bridge over the positive row to the next negative lead, and so on to the next LEDs.\n\nFollow this schematic to make the control board.\n\nFinally we\u2019re about to use our display but before doing that we have to define characters to be displayed in a matrix and have to do a little bit of complex programming in Arduino . You can try it on your own and if you need any kind of help check out my Github account."
    },
    {
        "url": "https://medium.com/@ariesiitr/machine-learning-a7437c4f0dd9?source=user_profile---------9----------------",
        "title": "Machine Learning \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Starting with what is machine learning\u2026.\n\nMachine Learning is the science of getting computers to act without being absolutely programmed. People generally got confused about the differences in AI and ML, so let\u2019s clear that first. ML is a technique of Artificial Intelligence(AI) which consists of various algorithms that make computers, or computer-controlled robots think intelligently, just like an intellectual human being thinks. Various applications of machine learning include self-driving cars, practical speech recognition, effective web search, and image recognition and classification. Machine learning is so prevalent today that you probably use it dozens of times a day without knowing it.\n\nGetting into the technicalities, Machine Learning can be broadly classified into two categories :- Supervised Learning and Unsupervised Learning. A brief explanation of some of the machine learning algorithms follows:\n\nRegression is a mathematical procedure used to find a curve that closely fits a given series of data. The goal of the analysis is to minimize the difference between the data point and the value predicted by the function. There are various different techniques, the most common being by the method of least squares.\n\nFor instance, say you want to find the price of a house given its size. You already have some data points i.e. the price of a house for various sizes. Then the price can be predicted by finding a curve which fits the available data and plotting the required data point on that curve.\n\nLogistic regression is a regression model where the dependent variable is categorical. A binary dependent variable can take only two values, \u201c0\u201d and \u201c1\u201d, which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick whereas multinomial logistic regression can also be used to classify objects according to more than two categories.\n\nSuppose, you want to find out whether an e-mail is spam or not then you can use the concept of binary dependent variable. And if you want to classify given flowers into say, three categories then multinomial regression will be used.\n\nIt is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. Right hyperplane is chosen by using the large-margin classifier.\n\nIt is a beautiful biologically-inspired programming criterion which enables a computer to learn from observational data. Neural networks are a subset of algorithms built around a model of artificial neurons spread across three or more layers. Nowadays, problems like image recognition, speech recognition, and natural language processing are solved using neural networks and deep learning.\n\nClustering algorithms are used in search engines that try to group similar objects in one cluster and the dissimilar objects far from each other. The goal is to cluster unlabeled data(unsupervised learning) into number of clusters and then providing the result for the searched data according to the nearest similar object which are grouped around the data to be searched. The most commonly used type of clustering algorithm is K-means.\n\nAs an illustration, take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.\n\nGoogle uses clustering algorithms to classify different contents as News by parsing though the matter and examining the keywords.\n\nThe algorithm represents each customer as a vector of all items on sale. Each entry in the vector is positive if the customer bought or rated the item, negative if the customer disliked the item, or empty if the customer has not made his or her opinion known. Most of the entries are empty for most of the customers. The algorithm then creates its recommendations by calculating a similarity value between the current customer and everyone else. Amazon/Flipkart/Netflix use collaborative filtering for recommendation.\n\nThe best known use of Na\u00efve Bayesian classification is spam filtering. It is a probabilistic classifier based on Bayes\u2019 theorem.\n\nFor example, Emails use Bayes\u2019 formula for calculating the probability of an email to be classified as a spam, given already existing spams. This can be done by calculating probabilities associated with each word of the text to be classified as a spam.\n\nA brillliant application of Machine Learning algorithms by Microsoft is in the following video:-\n\nFor more details of the topic, you can refer to these lectures."
    },
    {
        "url": "https://medium.com/@ariesiitr/tempescope-edebd695be53?source=user_profile---------10----------------",
        "title": "Tempescope \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Finding the weather usually involves searching the web, news, or using an app. But think about how great would it be if it was as easy as looking outside the window?\n\nThe idea behind a tempescope is to create a device that can provide the physical visualization and adds to the aesthetic value of the room.\n\nThe initial idea is to have the tempescope replicate 4 weather conditions namely\n\nUsing the data available from the internet through the smartphone, tempescope can visualize local weather conditions or the weather of a certain place that you want to have in your room.\n\nBuilding a tempescope requires you to have knowledge of creating an android app(just a basic app) for sharing weather data and handling an arduino microcontroller to act as a control center for the whole device.\n\nThe device uses a mist diffuser and a fan to create the cloudy conditions and uses a water pump to create rainy conditions. LEDs are used to create the appropriate lighting conditions in the device.\n\nTempescope is a three section device. The bottom section houses the arduino, water pump, mist diffuser and the fan. The middle section is the stage and the top section houses the LEDs.\n\nSome tutorials to help you get started on this project:-\n\nAlso for more information about Tempescope do read:-\n\n\u201cThe main requirement for making any good project is patience and dedication.\u201d"
    },
    {
        "url": "https://medium.com/@ariesiitr/drones-quadcopters-9113e78be45d?source=user_profile---------11----------------",
        "title": "Drones : Quadcopters \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Like this? Want to make your own quadcopter and do awesome stuff with it like that or just curious? Nevertheless we are here to help.\n\nThis blog aims to introduce you to the fantastic world of drones and more importantly help you make one. Essentially it is a collection of helpful resources from all over the internet to help you get started. Go through them properly, they will also help you in the selection process for the Quadcopter team of ArIES. So without any further ado, let\u2019s get started.\n\nWhat is the first thing that comes to your mind when you hear Drones? You might know them as the weapons that were used to attack in Pakistan. But there exist a more friendly version of them too. You must have seen them in your family marriages capturing those beautiful moments, or being used by the police to regulate traffic and discipline crowd, and during rescue operations. Well, here at ArIES, we prefer to work with the smaller, the nicer version.\n\nBefore getting deeper into the drone technology, let us first get some insights about the drone market and their applications.\n\nThese drones find applications in a variety of fields.\n\nJack Brown in this blog, presents a nice overview of the drone world. Let us now dive a little deeper into the basic technology and the physics that make these drones as awesome as they are.\n\nThe type of drone that we work with is called a quadcopter, meaning, it has four motors enabling its movement. This article perfectly describes the basic physics that make a quadcopter fly. You must have observed that it is quite similar to a helicopter. If not, try to find some similarity between the working of a quadcopter and a helicopter.\n\nAlong with the knowledge of quadcopter physics, you must also understand what a quadcopter is made of. Here you will be able to get quite a handful of information in this regards. What\u2019s missing here is how the quadcopter knows its orientation. The flight controller takes help from an accelerometer and a gyroscope housed within itself. They provide different types of information which when clubbed together provides us the orientation of the quadcopter.\n\nAnyway, now you know that making a drone isn\u2019t rocket science. You now understand how a quadcopter works and what are the components that make it work. But this is just the tip of the iceberg. You saw the above video, you know what more it can do other than just cinematography or surveying. Presently at ArIES, we are working on quite a few number of projects. Besides making them for the fun and the thrill we are working on making -"
    },
    {
        "url": "https://medium.com/@ariesiitr/recommendation-system-64ceed75d670?source=user_profile---------12----------------",
        "title": "Recommendation System \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Ever wondered, \u201cwhat algorithm google uses to maximize its target ads revenue?\u201d. What about the e-commerce websites which advocates you through options such as \u2018people who bought this also bought this\u2019. Or \u201cHow does Facebook automatically suggest us to tag friends in pictures\u201d?\n\nCompanies nowadays are building smart and intelligent recommendation engines by studying the past behavior of their users. Hence providing them recommendations and choices of their interest in terms of \u201cRelevant Job postings\u201d, \u201cMovies of Interest\u201d, \u201cSuggested Videos\u201d, \u201cFacebook friends that you may know\u201d and \u201cPeople who bought this also bought this\u201d etc.\n\nOften termed as Recommender Systems, they are simple algorithms which aim to provide the most relevant and accurate items to the user by filtering useful stuff from of a huge pool of information base. Recommendation engines discovers data patterns in the data set by learning consumers choices and produces the outcomes that co-relates to their needs and interests.\n\nIn this article, we will explain two types of recommendation algorithms that are also used by most of the tech giants like Google and Facebook in their advanced recommender system modules.\n\nConsider a scenario of an e-commerce website which sells thousands of smartphones. With growing number of customers every day, the task in hand is to showcase the best choices of smartphones to the users according to their tastes and preferences.\n\nTo understand how recommendation engine works, let\u2019s slice the data into a sample set of five smartphones with two major features \u201cBattery and Display\u201d. The five smartphones have following properties:\n\nUsing these characteristics, we can create an Item \u2014 Feature Matrix. Value in the cell represents the rating of the smartphone feature out of 1.\n\nOur sample set also consist of four active users with their preferences.\n\nUsing their interests, we can create a User \u2014 Feature Matrix as follows:\n\nWe have two matrices: Item \u2014 Feature and User \u2014 Feature. We can create the recommendation of smartphones for our users using following algorithm:\n\nContent based systems, recommends item based on a similarity comparison between the content of the items and a user\u2019s profile. The feature of items are mapped with feature of users in order to obtain user \u2014 item similarity. The top matched pairs are given as recommendations, as demonstrated below. Feature vector is a simple array of features of that item.\n\nAlso, every item representation as a feature vector:\n\nContent Based Item \u2014 User Mapping Recommendations are given by the equation:\n\nSmartphones S2, S3 and S1 has the highest recommendation scores, Hence S2, S3 and S1 are recommended to Aman.\n\nOne more technique could be used for recommender systems. This technique is known as Collaborative Filtering. Try reading about it on your own and understand the basic principle underlying.\n\nThere also exists more advanced techniques like ALS : Alternating Least Square Recommendations and Hybrid Recommendation Engines. The Recommendation Engines have become an important need with the growing information space.\n\nI think this was pretty intuitive and clear and gives you a feel about what the algorithm is actually doing. I also feel that till now you have started developing interest hopefully. If not, just start thinking about the applications of recommendation systems in real life and I am sure that you will certainly gain interest in it. ;)"
    },
    {
        "url": "https://medium.com/@ariesiitr/signal-processing-46ecfdbbf1ff?source=user_profile---------13----------------",
        "title": "SIGNAL PROCESSING \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "Signal processing is a broad field which takes into account the analysis, synthesis and modification of signals. Signals are functions that provide information of a particular phenomenon, such as sound, images, and biological measurements. For example, signal processing techniques are used to improve signal transmission, storage efficiency, and signal quality, and to emphasise or detect components of interest in a measured signal.\n\nEEG signals are electrical signal generated by in the brain. These signals are generated to perform different activities.\n\nIn Brain-Computer Interface design, EEG signal processing aims at translating raw EEG signals into the class of these signals, i.e., into the estimated mental state of the user. This translation is usually achieved using a pattern recognition approach, whose two main steps are the following:\n\nAs an example, let us consider a Motor Imagery (MI)-based BCI, i.e., a BCI that can recognized imagined movements such left hand or right hand imagined movements (see Figure). In this case, the two mental states to identify are imagined left hand movement on one side and imagined right hand movement on the other side. To identify them from EEG signals, typical features are band power features, i.e., the power of the EEG signal in a specific frequency band. For MI, band power features are usually extracted in the \u00b5 (about 8\u221212 Hz) and \u03b2 (about 16\u221224 Hz) frequency bands, for electrode localized over the motor cortex areas of the brain (around locations C3 and C4 for right and left hand movements respectively). Such features are then typically classified using a Linear Discriminant Analysis (LDA) classifier.\n\nIt should be mentioned that EEG signal processing is often built using machine learning. This means the classifier and/or the features are automatically tuned, generally for each user, according to examples of EEG signals from this user. These examples of EEG signals are called a training set, and are labeled with their class of belonging (i.e., the corresponding mental state). Based on these training examples, the classifier will be tuned in order to recognize as appropriately as possible the class of the training EEG signals. Features can also be tuned in such a way, e.g., by automatically selecting the most relevant channels or frequency bands to recognize the different mental states. Designing BCI based on machine learning (most current BCI are based on machine learning) therefore consists of 2 phases:\n\n\u2022 Calibration (a.k.a., training) phase: This consists in 1) Acquiring training EEG signals (i.e., training examples) and 2) Optimizing the EEG signal processing pipeline by tuning the feature parameters and/or training the classifier.\n\n\u2022 Use (a.k.a., test) phase: This consists in using the model (features and classifier) obtained during the calibration phase in order to recognize the mental state of the user from previously unseen EEG signals, in order to operate the BCI."
    },
    {
        "url": "https://medium.com/@ariesiitr/augmented-and-virtual-reality-97767c3abb20?source=user_profile---------14----------------",
        "title": "Augmented and Virtual Reality \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "With just one Google or YouTube search, you will know the craze of augmented and virtual reality over the years. In the summer after my sophomore year, I got the opportunity to work as an intern in this field. This blog will not only help you understand AR/VR but also in the selection procedure of ArIES (ArIES is Aritificial Intelligence and Electronics Society of IIT Roorkee).\n\nAccording to Wikipedia, augmented reality is a technology that superimposes a computer-generated image on a user\u2019s view of the real world, thus providing a composite view. Basically, manipulating the real world in the camera view with the use of augmented 3D objects is augmented reality.\n\nAccording to Dictionary, Virtual Reality is the computer-generated simulation of a three-dimensional image or environment that can be interacted with in a seemingly real or physical way by a person using special electronic equipment, such as a helmet with a screen inside or gloves fitted with sensors.\n\nFor augmented reality, no hardware is required, just your cell phone and the images printed on paper or on any other object.\n\nFor virtual reality, a VR Headset is required and of course your mobile.\n\nNow, there are good number of platforms for making applications related to AR/VR but for beginning, most famous is the Unity 3D with Vuforia extension.\n\nThere are also separate plugins for Google VR Headset and some others but those are specific and need not require much attention.\n\nObviously, you need to have good programming skills and object-oriented programming skills. Programming language used in Unity is C# or JavaScript. I prefer C# because it is more related to C and hence easily understandable.\n\nThe main technology behind Augmented Reality is the Image Processing (Hey, you got a topic to read. Careful, I won\u2019t be telling you further).\n\nFirst of all, you need to install Unity 3D which is a known platform for Game Development. Then you need to install Vuforia plugin in unity itself from official site of Vuforia.\n\nWhen you will open the unity editor, you need to drag and drop the Image-Target from prefab folder to the scene and the 3D object to the scene. You are done to create a fundamental AR app. The best thing about unity is that it is multi-platform i.e. you can build your application for android, iOS, windows, etc. from the same code.\n\nFor learning in detail, refer to the links provided at the end.\n\nNow, what Vuforia does that it detects the pre-defined image in the camera view using different image detection algorithms and give the image region to you in the scene to manipulate. At this stage, you need not worry to go deep inside image processing but at least have a good idea about it.\n\nThe most commonly technology used for image processing is Computer Vision but again Computer Vision itself is a vast topic and you need to learn OpenCV for it.\n\nFor Virtual Reality, we use a pre-build virtual environment in which camera of the scene is usually on the head of the person. So that you can see the virtual surroundings from your perspective.\n\nNow again you can manipulate the objects using your mathematics and programming skills and let the user interact with the virtual reality.\n\nNow, we combine the augmented reality with VR headset and this becomes really amazing !! We split the mobile screen into two parts and show the two parts to your eyes respectively. This gives the perception of depth of the view. The virtual environment is not purely virtual but the mixture of real environment and the virtual objects and you have created what everybody loves.\n\nThe important thing about AR/VR is that there is always enough to explore and always there is a room for new idea to implement.\n\nSee these videos to get the feel of this tech:\n\nThis last video is to be seen by VR headset.\n\nStudy Material to help you get started:-\n\nWith these links, you can get started to make your own AR/VR applications. You need to have some basic knowledge of programming. If you already have studies any programming language, you can skip first link. If you have basic idea of Object Oriented Programming, you can skip the second link too. The third and fourth links are very important (you can\u2019t skip those).\n\nYou need to do at least one of the tutorial from the official site of Unity 3D and then from the last link you will get going for making your application.\n\nGetting Started with Vuforia for Unity Development\n\nFor getting any other information regarding the blog, content, selection process or any other thing, feel free to contact me."
    },
    {
        "url": "https://medium.com/@ariesiitr/an-artificial-neural-network-ann-is-a-computational-model-that-is-inspired-by-the-way-biological-c17b07166d4c?source=user_profile---------15----------------",
        "title": "Artificial Neural Network \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "An Artificial Neural Network (ANN) is a computational model that is inspired by the way biological neural networks in the human brain process information. Artificial Neural Networks have generated a lot of excitement in Machine Learning research and industry, thanks to many breakthrough results in speech recognition, computer vision and text processing. In this blog post we will try to develop an understanding of a particular type of Artificial Neural Network called the Multi Layer Perceptron.\n\nThe basic unit of computation in a neural network is the neuron, often called a node or unit. It receives input from some other nodes, or from an external source and computes an output. Each input has an associated weight (w), which is assigned on the basis of its relative importance to other inputs. The node applies a function f (defined below) to the weighted sum of its inputs as shown in Figure 1 below:\n\nThe above network takes numerical inputs X1 and X2 and has weights w1 and w2 associated with those inputs. Additionally, there is another input 1 with weight b (called the Bias) associated with it. We will learn more details about role of the bias later.\n\nThe output Y from the neuron is computed as shown in the Figure 1. The function f is non-linear and is called the Activation Function. The purpose of the activation function is to introduce non-linearity into the output of a neuron. This is important because most real world data is non linear and we want neurons to learn these non linear representations.\n\nEvery activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:\n\nThe below figures show each of the above activation functions.\n\nImportance of Bias: The main function of Bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node receives). See this link to learn more about the role of bias in a neuron.\n\nThe feedforward neural network was the first and simplest type of artificial neural network devised. It contains multiple neurons (nodes) arranged in layers. Nodes from adjacent layers have connections or edges between them. All these connections have weights associated with them.\n\nAn example of a feedforward neural network is shown in Figure 3.\n\nFigure 3: an example of feedforward neural network\n\nA feedforward neural network can consist of three types of nodes:\n\nIn a feedforward network, the information moves in only one direction \u2014 forward \u2014 from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network (this property of feed forward networks is different from Recurrent Neural Networks in which the connections between the nodes form a cycle).\n\nTwo examples of feedforward networks are given below:\n\nA Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non \u2014 linear functions.\n\nFigure 4 shows a multi layer perceptron with a single hidden layer. Note that all connections have weights associated with them, but only three weights (w0, w1, w2) are shown in the figure.\n\nInput Layer: The Input layer has three nodes. The Bias node has a value of 1. The other two nodes take X1 and X2 as external inputs (which are numerical values depending upon the input dataset). As discussed above, no computation is performed in the Input layer, so the outputs from nodes in the Input layer are 1, X1 and X2 respectively, which are fed into the Hidden Layer.\n\nHidden Layer: The Hidden layer also has three nodes with the Bias node having an output of 1. The output of the other two nodes in the Hidden layer depends on the outputs from the Input layer (1, X1, X2) as well as the weights associated with the connections (edges). Figure 4 shows the output calculation for one of the hidden nodes (highlighted). Similarly, the output from other hidden node can be calculated. Remember that f refers to the activation function. These outputs are then fed to the nodes in the Output layer.\n\nOutput Layer: The Output layer has two nodes which take inputs from the Hidden layer and perform similar computations as shown for the highlighted hidden node. The values calculated (Y1 and Y2) as a result of these computations act as outputs of the Multi Layer Perceptron.\n\nGiven a set of features X = (x1, x2, \u2026) and a target y, a Multi Layer Perceptron can learn the relationship between the features and the target, for either classification or regression.\n\nLets take an example to understand Multi Layer Perceptrons better. Suppose we have the following student-marks dataset:\n\nThe two input columns show the number of hours the student has studied and the mid term marks obtained by the student. The Final Result column can have two values 1 or 0 indicating whether the student passed in the final term. For example, we can see that if the student studied 35 hours and had obtained 67 marks in the mid term, he / she ended up passing the final term.\n\nNow, suppose, we want to predict whether a student studying 25 hours and having 70 marks in the mid term will pass the final term.\n\nThis is a binary classification problem where a multi layer perceptron can learn from the given examples (training data) and make an informed prediction given a new data point. We will see below how a multi layer perceptron learns such relationships.\n\nThe process by which a Multi Layer Perceptron learns is called the Backpropagation algorithm. I would recommend reading this Quora answer by Hemanth Kumar (quoted below) which explains Backpropagation clearly.\n\nNow that we have an idea of how Backpropagation works, lets come back to our student-marks dataset shown above.\n\nThe Multi Layer Perceptron shown in Figure 5 (adapted from Sebastian Raschka\u2019s excellent visual explanation of the backpropagation algorithm) has two nodes in the input layer (apart from the Bias node) which take the inputs \u2018Hours Studied\u2019 and \u2018Mid Term Marks\u2019. It also has a hidden layer with two nodes (apart from the Bias node). The output layer has two nodes as well \u2014 the upper node outputs the probability of \u2018Pass\u2019 while the lower node outputs the probability of \u2018Fail\u2019.\n\nIn classification tasks, we generally use a Softmax function as the Activation Function in the Output layer of the Multi Layer Perceptron to ensure that the outputs are probabilities and they add up to 1. The Softmax function takes a vector of arbitrary real-valued scores and squashes it to a vector of values between zero and one that sum to one. So, in this case,\n\nAll weights in the network are randomly assigned. Lets consider the hidden layer node marked V in Figure 5 below. Assume the weights of the connections from the inputs to that node are w1, w2 and w3 (as shown).\n\nThe network then takes the first training example as input (we know that for inputs 35 and 67, the probability of Pass is 1).\n\nThen output V from the node in consideration can be calculated as below (f is an activation function such as sigmoid):\n\nSimilarly, outputs from the other node in the hidden layer is also calculated. The outputs of the two nodes in the hidden layer act as inputs to the two nodes in the output layer. This enables us to calculate output probabilities from the two nodes in output layer.\n\nSuppose the output probabilities from the two nodes in the output layer are 0.4 and 0.6 respectively (since the weights are randomly assigned, outputs will also be random). We can see that the calculated probabilities (0.4 and 0.6) are very far from the desired probabilities (1 and 0 respectively), hence the network in Figure 5 is said to have an \u2018Incorrect Output\u2019.\n\nWe calculate the total error at the output nodes and propagate these errors back through the network using Backpropagation to calculate the gradients. Then we use an optimization method such as Gradient Descent to \u2018adjust\u2019 all weights in the network with an aim of reducing the error at the output layer. This is shown in the Figure 6 below (ignore the mathematical equations in the figure for now).\n\nSuppose that the new weights associated with the node in consideration are w4, w5 and w6 (after Backpropagation and adjusting weights).\n\nIf we now input the same example to the network again, the network should perform better than before since the weights have now been adjusted to minimize the error in prediction. As shown in Figure 7, the errors at the output nodes now reduce to [0.2, -0.2] as compared to [0.6, -0.4] earlier. This means that our network has learnt to correctly classify our first training example.\n\nFigure 7: the MLP network now performs better on the same input\n\nWe repeat this process with all other training examples in our dataset. Then, our network is said to have learnt those examples.\n\nIf we now want to predict whether a student studying 25 hours and having 70 marks in the mid term will pass the final term, we go through the forward propagation step and find the output probabilities for Pass and Fail.\n\nI have avoided mathematical equations and explanation of concepts such as \u2018Gradient Descent\u2019 here and have rather tried to develop an intuition for the algorithm. For a more mathematically involved discussion of the Backpropagation algorithm, refer to this link.\n\nAdam Harley has created a 3d visualization of a Multi Layer Perceptron which has already been trained (using Backpropagation) on the MNIST Database of handwritten digits.\n\nThe network takes 784 numeric pixel values as inputs from a 28 x 28 image of a handwritten digit (it has 784 nodes in the Input Layer corresponding to pixels). The network has 300 nodes in the first hidden layer, 100 nodes in the second hidden layer, and 10 nodes in the output layer (corresponding to the 10 digits).\n\nAlthough the network described here is much larger (uses more hidden layers and nodes) compared to the one we discussed in the previous section, all computations in the forward propagation step and backpropagation step are done in the same way (at each node) as discussed before.\n\nFigure 8 shows the network when the input is the digit \u20185\u2019.\n\nFigure 8: visualizing the network for an input of \u20185\u2019\n\nA node which has a higher output value than others is represented by a brighter color. In the Input layer, the bright nodes are those which receive higher numerical pixel values as input. Notice how in the output layer, the only bright node corresponds to the digit 5 (it has an output probability of 1, which is higher than the other nine nodes which have an output probability of 0). This indicates that the MLP has correctly classified the input digit. I highly recommend playing around with this visualization and observing connections between nodes of different layers.\n\nI have skipped important details of some of the concepts discussed in this post to facilitate understanding. I would recommend going through Part1, Part2, Part3 and Case Study from Stanford\u2019s Neural Network tutorial for a thorough understanding of Multi Layer Perceptrons."
    },
    {
        "url": "https://medium.com/@ariesiitr/home-automation-and-pir-passive-infrared-sensor-5f9592863b20?source=user_profile---------16----------------",
        "title": "Home Automation and PIR(Passive Infrared Sensor) \u2013 Artificial Intelligence And Electronics Society \u2013",
        "text": "HALLOWEEN IS COMING up faster than you think: time to dust off the gravestones and the zombie lawn decorations! This year you may want to add a little something extra to give the trick-or-treaters a bit of a thrill.\n\n\u201cHome automation\u201d is a category that can span really simple to extremely complex tasks depending on who you ask. It\u2019s easy to get overwhelmed by jargon and devices and standards. \n\nBefore the invention of mobile phones nobody has thought we could contact anybody anywhere anytime but the technology made communication easier and now mobile phones are everyones\u2019 heartbeat. The point of consideration is; any new technology or invention is just a dream until it reaches masses. In just a few years, the Internet has started to add connectivity to everything from home audio to even garage doors. Known as the \u2018Internet of Things\u2019 this growth in technology is now leading to affordable and easy-to-use systems, apps, and hardware for automating your entire home.\n\nHome automation involves automatic controlling of all electrical or electronic devices in homes or even remotely through wireless communication. The sensors detects light, motion, temperature and other sensing elements, and then send that data to the main controlling devices.\n\nJust imagine how carefree our life would be if everything could be controlled by a single switch or app in our smart phones. You put your feet into your room and there is already someone or something giving you a motherly feel, a sensor which senses your arrival and switches on lights, fan and music system for you. If you are still wondering what it is then remember the switching on of lights in washrooms of shopping malls.Occupancy sensor is the cool tool.\n\nSensors based on different technologies like infrared, microwave and ultrasonic are being included in the wide world of IoT and home automation. Occupancy sensors are typically used to save energy, provide automatic control, and comply with building codes.\n\nVarious applications of occupancy sensor found out till now include :\n\n\u2022Turning on street lights on less dense roads when any vehicle passes \n\n\u2022 Acting as virtual security guard in lawns of your house\n\n\u2022 Turning on home appliances when the owner foots into house\n\n\u2022 Wide and wise use in robotics\n\nThese are very few but most popular areas, more such innovative applications are in their dormant stages.\n\nPIR sensors work on the principle of sensing Passive Infrared Radiation emitted by objects or individuals using pyroelectric sensor. When a warm body like a human or animal passes by, it first intercepts one half of the PIR sensor, which causes a positive differential change between the two halves. When the warm body leaves the sensing area, the reverse happens, whereby the sensor generates a negative differential change. These change pulses are what is detected. Area of detection of these sensors varies from 5m to as high as 60m. PIR sensors are compatible with all interfaces like Arduino, Raspberry pi, etc.\n\nLet\u2019s dig deeper into the working and configuration of PIR sensor;\n\nThe lens protecting the sensor converges all the infrared rays falling on it to the pyroelectric sensor window. The optics behind the structure of Fresnel lens can be exploited to increase the range of detection.\n\nThe converged rays fall onto a RE002B sensor which has two windows which sense any input which is no less than 10 microwatts and detects any spatial temperature variations. Output of these windows serve as input to JFET whose output voltage is sent to an amplifier for further processing of the signal. The signal processing in the sensor circuit include passing the analog signal to a comparator to determine when to trigger it, then to multistage amplifier to get the final digital output.\n\nWe can even set the trigerring pulse width by changing values of resistor and capacitor connected to 3\u20135 pins of BISS0001 chip. Sensitivity of the sensor can also be increased. \n\nFuther, your code determines how you can meet your requirements in the best possible ways.\n\nSome more captivating and enthralling IoT projects are:\n\n\u2022 Home automation using Bluetooth module\n\n\u2022 Home automation using Zigbee, WiFi, GSM module"
    }
]