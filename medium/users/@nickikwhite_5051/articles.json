[
    {
        "url": "https://medium.com/@nickikwhite_5051/token-sale-models-part-iv-token-lock-up-e33c4fe1de30?source=user_profile---------1----------------",
        "title": "Token Sale Models Part IV, token lock-up \u2013 Nick White \u2013",
        "text": "A tricky problem around ICOs that I have mentioned but not discussed solutions for is the problem of people dumping your token the minute it goes live on an exchange. Part of the problem is that you don\u2019t want the price of your token to plummet off the bat as your early investors seek to take their profits. This may undermine the legitimacy of your platform and tarnish your reputation going forward. But more importantly, if you want your platform to succeed, you don\u2019t want your investors to be in it for short term gains. You want investors that actually believe in your product and will become users, miners, and advocates of your token.\n\nOne way to avoid this situation of speculators dominating your token sale is using a lock-up mechanism in your token distribution. What this means is that a person may invest in your token sale in agreement to receive a certain amount of your tokens, but that they will not have access to these tokens until a predetermined date. Until that time, the tokens will be \u201clocked-up,\u201d meaning that they exist and will be allocated to the appropriate account but cannot yet be used on the platform or exchanged. Now, one of the primary advantages of participating in an ICO from an investor\u2019s stand point is liquidity and locking tokens up stands diametrically opposed to this. This is why lock-up coins are often offered at a discount to encourage investment from those that believe in the longterm viability and vision of the platform over those that just want to make a quick buck.\n\nToken lock-up was used in practice by Protocol Labs in their Filecoin ICO earlier this year. Investors in the Filecoin ICO were presented with several options of discounts and lock-up times. They could choose a 6 month lock-up for 0% discount, 1 year lock-up for 7.5% discount, 2 year lock-up for 15% discount and a 3 year lock-up for 20% discount. Although I\u2019ve been using the term \u201clock-up,\u201d in the case of Filecoin\u2019s ICO, the correct nomenclature would be \u201cvesting.\u201d In order to avoid a flood of new tokens in the market that would arrive all at once when the lock-up period ends, Filecoin chose to vest its tokens to investors at a constant rate over time, issuing a fraction of them every time a new block was added to the blockchain. This ensures a slow trickle of tokens coming online which should help keep the token price and the overall network stable.\n\nAnother use of token lock-up is to demonstrate goodwill and establish trust for the founding organization of any platform. In a standard ICO, a sizable fraction of tokens are reserved for the developers of the platform themselves. This purse for the founders serves as a source of leverage over the platform as well as a source of future funding when the money raised through the ICO runs out. However, technically nothing is stopping the development team from doing the same thing as speculators and dumping their tokens to make more money and run. Clearly this would not be in the interest of any honest dev team, but with all the scams out there in the space, it is certainly being done.\n\nAny honest team should be committed to growing the value of their platform and should believe that their token is a good investment in the long term. Therefore, to prove that their motivations are in the right place, some developer teams are choosing to voluntarily lock-up their own token allocations for a given amount of time. This is sort of like when a company like Apple buys back their own stock from the public market, its a signal that the company believes in its own future. This builds investor confidence, something that is in shorter and shorter supply in the cooling ICO market."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/token-sale-models-part-iii-proportional-refund-1243cbd43286?source=user_profile---------2----------------",
        "title": "Token Sale Models Part III, proportional refund \u2013 Nick White \u2013",
        "text": "Aside from the goal of raising money, there are many other goals you may have for the outcome of your ICO. As discussed in previous posts, you may want to ensure an equal opportunity for investors to participate, you may wish to ensure that you token is not pumped and dumped the day it goes up on an exchange, you may want to provide the transparency of information needed for investors to make an informed decision, or you may simply want to maximize the total money you raise. Along with running a capped or uncapped sale and using price schedules throughout the course of your sale, there are a few other techniques that have been used to achieve the desired outcome for an ICO.\n\nOne problem with running a capped sale is the gold-rush mentality to buy up all the tokens before they run out. Often this leads to whales paying high transaction fees to place their large allocations before any of the smaller players have a chance to participate. A solution to this is to run an uncapped sale so that everyone has a chance to participate, but then you run into the problem of having an unclear valuation of your network which makes it difficult for investors to know the implied value of your token. With a capped sale, at least there is information transparency. To solve these problems, a hybrid approach has been proposed called a proportional refund.\n\nIn a proportional refund, the total number of tokens being sold is given up front as well as the total amount of money being raised. This sets a \u201ccap\u201d in a sense, but the difference is that the total amount of money that can be pledged to the ICO is allowed to exceed this cap. The token sale goes on taking in as much money as investors are willing to commit, very much like an uncapped sale, until the sale ends. At the end of the sale, the sold tokens are distributed according to the proportion of money invested by each participant and all excess contributions are refunded. So as an example, if 2 investors placed 300 ETH and 100 ETH in my token sale that was capped at 100 ETH, 75% of the tokens for sale would go to the first investor and 25% would go to the second. Then, 225 ETH would be refunded to the first investor and 75 ETH would be refunded to the second investor.\n\nThe nice part of this design is that you get the benefits of both a capped and uncapped sale. Like in an uncapped sale, anyone can contribute to the sale and get a piece of the pie. And like in a capped sale, investors know the price at which they are buying the tokens, even if they may not know how many tokens they will end up getting. This sounds good on the surface, but it still doesn\u2019t resolve the issue of being excluded from the sale by big players. Certainly you will always be able to contribute to the sale and get at least a small fraction of the tokens sold, but if a few whales put up 95% of the total ETH in the sale, then only 5% will go to the smaller players. And there\u2019s nothing stopping a whale from majorly over allocating to the sale just to push other players out.\n\nOne modification I would make to this model is to set a cap for the total fraction of tokens that can go to a single investor. The issue with this is that it is unenforceable since someone can make multiple addresses to contribute to the sale from. So rather than having 50% of the sale going to one address, you have 5% of the tokens going to 10 different addresses that are each owned by the same individual for a total of 50%. It may appear more equally distributed, but the real results have not changed.\n\nProportional refund is no silver bullet, but it seems to be a good alternative to the other models we\u2019ve covered so far. If you happen to be looking for some smart contract source code to launch a proportional allocation token sale, here\u2019s a github repo published by Ramesh Nair that should give you a good start."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/token-sale-models-part-ii-price-schedules-b36716a1a9de?source=user_profile---------3----------------",
        "title": "Token Sale Models Part II, Price Schedules \u2013 Nick White \u2013",
        "text": "Initial Coin Offerings have ushered in a new era of fundraising, but as with any new technology, there are a lot of issues with this method that have yet to be ironed out. The blockchain community has been experimenting with different models for their token sales in order to mitigate these problems. There will never be one single token sale model that is the best for everyone, but several techniques have emerged as popular and seem to be effective. In part I, we covered the three most basic token sale models. First, no ICO just pure mining, second, a capped sale at a fixed price, and third, an uncapped sale at a fixed price. Now we\u2019ll dive into some of the more sophisticated approaches.\n\nOne of the most powerful ways to shape the incentives of your token sale model is to make a price schedule. This is a predetermined way in which the price of your token will change over the duration of your token sale. Most prices schedules either begin high and end low or begin low and end high. Starting the price high and ending it low makes the most sense if you are releasing a capped coin sale as it mitigates the gold rush mentality to buy up all the tokens right at the beginning of the sale. In this scheme, investors are incentivized to buy later on in the sale as the price of the token falls so that they can get a better deal. Ideally, this means that investors can enter into the sale at a point in which they think the valuation is fair. If the tokens sell out before an investor participates, then theoretically that investor wouldn\u2019t have wanted to buy in anyway since he thought the price was still too high.\n\nThis decreasing price schedule is called a \u201cDutch auction\u201d after a type of auction in which the auctioneer progressively lowers the price until a bidder agrees to take it. The Gnosis team chose to use this Dutch auction model in their ICO but they chose to cap the total amount of money raised rather than the total number of coins sold which had some interesting results. The graph below illustrates the price dynamic of this token sale model and how it should in theory disincentivize investors from jumping in at the very beginning of the sale. In fact, this is not what happened during the GNO sale and it sold out within a few hours of opening. That\u2019s a bubble for you.\n\nOn the other hand, setting a price schedule that begins low and ends high is best suited for an uncapped sale. If you remember from the last post, the issue with an uncapped sale is that investors don\u2019t know how many tokens will be minted and therefore they don\u2019t know what the implied valuation of the project is until the end of the sale when all the orders have come in. This means that the best strategy in an uncapped sale is to wait until the very end of the sale before buying any tokens. That way you can observe how many tokens are sold and have the best idea of what the implied network valuation is and how fair the price is before you decide to participate. However, if everyone waits until the end of the sale to participate, then no one knows what the implied valuation will be since no tokens have been sold.\n\nThis is where the increasing price schedule comes to the rescue. By starting the price of your token low and having it slowly increase over the course of the sale, you balance the incentive to wait and see how much total money the sale will raise with the incentive to get in early at a lower price. Investors that want a discount will buy into the sale early which will give investors who want more certainty about the valuation the information they need to feel comfortable participating, even if they have to participate at a higher price. This trade off between better information about the implied valuation of the project versus buying the token at a discount allows each investor to make their own decision about which is most important to them and everyone wins.\n\nAn example of the increasing price schedule in action is the Ethereum ICO. The price began at .0005 BTC per ETH and then rose (the price in the chart looks like it is decreasing because the units are reversed) to .00075 BTC per ETH by the end of the 42-day sale. The green line at the bottom represents the amount of money raised in dollars per day. As you can see, there was a lot of interest in the token right at the beginning, then the next wave came as a large portion of investors entered right before the price was set to start increasing. Investment continued to trickle in for the rest of the sale, with a couple of small bumps right at the end.\n\nIn this case it\u2019s not clear that Ethereum\u2019s price schedule was very effective in mitigating the problem of investors flooding into an uncapped token sale at the very end of the investing window. In a sense it did achieve that goal in that the flood didn\u2019t occur at the end, but in another sense it didn\u2019t solve the problem so much as shift the flood\u2019s timing to the end of the discount period. That said, this ICO took place in 2014 with few previous token sale models to learn from, so it was quite innovative for its time.\n\nThe price schedule is an important design parameter for any token sale model but there are a few other parameters to consider which I\u2019ll cover in the next post. The images and many of the ideas in this post came from Vitalik Buterin\u2019s blog. Thanks Vitalik."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/token-sale-models-part-1-649f8eec29fc?source=user_profile---------4----------------",
        "title": "Token Sale Models Part I, Capped & Uncapped \u2013 Nick White \u2013",
        "text": "So you\u2019ve got your whitepaper, you\u2019ve got your impressive list of advisors, you\u2019ve got press and visibility in the cryptosphere, heck, maybe you even got Paris Hilton to promote your project. Now its time for you to launch your token sale! But wait, there\u2019s one last piece you\u2019ve forgot to figure out and that is your token sale model. You should have included it in your whitepaper, but oh well, better late than never.\n\nOn a more serious note, the way in which you structure token sale has a big influence on the outcome of your ICO. There has been a lot of debate about the optimal method for a token sale but the truth is that there is no one size fits all. The optimal method depends on your goals for your ICO. Do you want to maximize the amount of money you raise? Do you want to encourage a fair distribution of the coins that you are selling among the investors? Do you want to avoid a pump and dump of your coin once it goes live on exchanges? Do you want to minimize any negative downstream effects of your token sale on the rest of the token ecosystem? As you can see, there are a lot of things that you have got to consider in the design of your token sale model.\n\nLet\u2019s examine some common token sale models to illustrate how these various trade-offs affect the outcome of your ICO and the future of your project. First and perhaps least obviously, you could choose not to do a token sale at all. Bitcoin never had a token sale. Instead, Bitcoin mints its coins as a reward to the miner of each new block in the blockchain. If your goal is to raise money to support the development of your project, then this is probably not the approach you would take. What this does illustrate is that the ICO is not a required step to launching your blockchain. However, even if raising money is not a priority, I do think you\u2019re more likely to attract miners and a community to support your blockchain if you do an ICO.\n\nThe next token sale model is the \u201ccapped sale\u201d model. In this case, you sell a fixed number of coins at a predetermined fixed price. The advantage of this is that it gives a fixed valuation for your network which makes the process very transparent for investors. If investors believe your network is worth more than the valuation implied by the token price, they can feel confident in purchasing your coins. The downside to this model is that if you are launching a particularly hot product, often it becomes a race to buy up as many tokens as possible. For example, the capped BAT token sale sold out $35 Million of tokens in 30 seconds. This is sort of like how tickets to Coachella Music Festival or Burning Man sell out within minutes of going on sale. People are poised on the webpage to send in their bid request the instant your token sale goes live so that they don\u2019t miss out on the party.\n\nThe result is bigger investors paying higher transaction fees to get in first and buying up a majority of tokens before the average investors have a chance to get in. Then when your token goes live, these bigger investors can dump your coins at an inflated price onto all the people who were excluded because they know the others are desperate to get in. This is just like the scalpers that resell festival tickets higher than face value. This is probably not the kind of result you would intend for your token sale, but at the same time this is kind of a good problem to have as it means your project is popular.\n\nAn easy alternative to the capped model is to do an \u201cuncapped\u201d token sale. This means what it sounds like, which is that you sell your coins at a predetermined price but you do not cap the total number of coins you will give out. So the more people that invest, the more total tokens you will mint. The nice part about this is that everyone gets to participate. No one needs to rush to buy your coins before they sell out and no one gets pushed out of the sale by the bigger investors. Or do they?\n\nThe major downside to this model is that as an investor although you know the price of the tokens you are buying, you don\u2019t know the implied valuation of the project because you don\u2019t know how many tokens there will be by the end of the sale. So even though this may allow you to include smaller investors, these smaller players may get a raw deal if too many coins are sold. However, the uncapped model has a major advantage for the team launching the ICO in that there is no cap to the amount of money you could raise. Rather than set your own valuation of your project, you let the market do it. Just imagine how much money the BAT project could have raised if they had chosen to take an uncapped approach.\n\nI\u2019m going to end here for now and continue discussion of other token sale models in Part II. For now, I hope you have begun to see how important the token sale model is in how it affects your goals as the team launching the sale as well as how it affects investors and your community once the project goes live. We\u2019ll cover reverse Dutch auctions, proportional refunds, and other interesting token sale models in the next post."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/ethereums-erc20-token-ecosystem-c2320ccf3567?source=user_profile---------5----------------",
        "title": "Ethereum\u2019s ERC20 Token Ecosystem \u2013 Nick White \u2013",
        "text": "Most people know of Ethereum as the second generation blockchain platform and the closest competitor to Bitcoin. Lots of buzzwords are thrown around Ethereum such as the fact that it is the first \u201cTuring-Complete\u201d blockchain and that it enables this new technology called a \u201csmart contract.\u201d Ethereum proposes nothing less than to become the \u201cworld computer\u201d and power an ecosystem of decentralized apps or \u201cdapps\u201d that may one day replace many of the centralized software applications we use today like Facebook. Along with all the promise of Ethereum\u2019s platform, it has enjoyed a meteoric rise to prominence from a price of roughly $1 USD less than a year ago to over $300 USD today.\n\nWhat I did not know about Ethereum though, is that it has become the backbone of the growing alt-coin market. According to Etherscan, there are 12,783 tokens built on the Ethereum platform as of this writing. Among these tokens include some of the most successful blockchain projects to date such as OmiseGo, Augur, EOS, and TenX. In fact, the 12,783 tokens I referred to are not the extent of Ethereum\u2019s ecosystem, those are just are the tokens that have complied with the ERC20 standard. If you\u2019re like me you might be asking, what does it even mean to \u201claunch a token on top of Ethereum\u201d and what is this opaque term \u201cERC20 standard\u201d?\n\nTo understand what it means to build a token on top of the Ethereum ecosystem, let\u2019s think about what it would take to launch a token from scratch. You could fork a preexisting blockchain protocol which would be relatively simple, but then you\u2019d have to convince a community of people to support your new chain by running nodes on your network, also known as \u201cmining\u201d. Needless to say, this takes a lot of credibility and marketing, and its a feat that very few projects have successfully achieved. Now instead imagine that you could copy and paste some code for a smart contract, make a few edits, and voila, mint your own token that is compatible with the wider cryptotoken market to boot. This is exactly what Ethereum\u2019s platform and the ERC20 token standard enable you to do.\n\nOn the Ethereum, you can create a smart contract that acts as a ledger for your tokens. After minting your tokens and distributing them according to the proportions determined by your presale, users can then interact with the smart contract in order to send tokens back and forth between one another. ERC20, which stands for Ethereum Request for Comments 20, came from a forum discussion about establishing a common functionality between token smart contracts to ensure compatibility and ease of use across the ecosystem. The result of this discussion was the ERC20 standard which defines a set of functions that your smart contract should implement such as checking what the total token supply is, how many tokens belong to a certain address, and transferring tokens between addresses. By complying with ERC20, you ensure that you token will be readily accessed by the wider ecosystem and increase the likelihood of being listed on an exchange since the exchange would not have to implement new functionality to include your token.\n\nIt\u2019s no wonder that many projects have decided to launch their tokens on top of Ethereum\u2019s platform. The process of writing a smart contract to manage your tokens and the simple integration into a pre-existing token ecosystem via the ERC20 standard make it significantly easier to launch your token sale than it would be attempting to bootstrap your own blockchain from scratch. Some have argued that this functionality is in fact Ethereum\u2019s killer app \u2014 not the decentralized apps that are promised to come in the future. Only time will tell where Ethereum\u2019s true potential lies, but there is no doubt that Ethereum has significantly lowered the barrier to entry in the cryptotoken market and sired a massive proliferation in altcoins. Maybe its time to mint your own!"
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/token-regulation-f3b9ffa8bd9e?source=user_profile---------6----------------",
        "title": "Token Regulation \u2013 Nick White \u2013",
        "text": "ICOs have had quite a heyday in the past year, but it seems clear that this freewheeling era is coming to an end. Regulatory bodies have issued statements addressing the legality of ICOs and trading of cryptoassets in key markets around the world. Many of these regulators have not passed a final judgement on how they will address this new investment vehicle but they have indicated that stricter policies will be implemented in due time. The uncertainty of what these new regulations will be has caused some doubts in the crypto space, but ultimately it\u2019s important to realize that regulators are not trying to curtail this emerging space \u2014 they are aiming to protect retail investors.\n\nRegulation is important and is the result of years of trying to curb bad actors that have attempted to exploit more traditional markets. Here are some of the risks that regulators are considering trying to manage:\n\nThese are all essential things to regulate in order to keep the market stable and fair for all participants. Regulators do not want to block the innovation happening in the blockchain community. They just want to stop bad actors from taking advantage of honest participants. That being said, even if you consider yourself an honest participant, it is worth treading carefully in this space until regulations settle out.\n\nHere are a few of the key points I\u2019ve learned about keeping ICOs safe from any legal backlash:\n\nFirst, establish your token offering entity in a legal and economic safe haven such as the Cayman Islands, the British Virgin Islands, the Isle of Man, Gibraltar, Switzerland, Singapore or Hong Kong. By issuing your token from these one of these places, you can minimize the risk that your entity will be sued in violation of some kind of tax law or security regulation.\n\nSecond, make sure the investors you allow to participate in your token sale are in appropriate jurisdictions. As it turns out, even if you are issuing your token from a different country, the jurisdiction of the person buying the token on the other end could expose you to some ugly regulations. That is why in more recent ICOs, only US citizens who are accredited investors have been permitted to participate. It is simply not worth the risk to allow retail investors in the US to participate as that could lead to nasty lawsuits with the SEC when regulations come into play in the future.\n\nThird, this applies to both security tokens as well as utility tokens. Even if your token does not represent a share in a company, a debenture, or participation in an underlying fund, it still may fall under the category of a security in current regulations. Since the law has not yet been established, there is not yet a differentiation between \u201cutility tokens\u201d and \u201csecurity tokens\u201d so it\u2019s best to play it safe until this distinction has been clarified.\n\nDespite all these warnings, the truth is that any market that chooses to regulate ICOs too heavily risks losing the growth of this budding ecosystem to countries with more favorable regulations. The desire to capture this growth and benefit from this new economic paradigm will keep the regulatory climate competitive. Teams will certainly have to put more effort into complying with ICO regulations in the future, but this is all part of the market maturing. I believe this will lead to sustainable innovation and investment going forward."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/accelerators-in-the-age-of-icos-a693fece7dab?source=user_profile---------7----------------",
        "title": "Accelerators in the Age of ICOs \u2013 Nick White \u2013",
        "text": "For those of us dialed into the blockchain space, it seems like everyone is launching an Initial Coin Offering, or ICO. Teams from around the world are putting together whitepapers, advisory boards, and marketing campaigns to promote a token sale for their company or product. And they\u2019re raising a LOT of money this way. Over $2 Billion USD was raised in this year alone. The ICO market seems to be cooling off recently, and this is probably a good thing. I\u2019ve heard of some very questionable products and teams that have raised significant capital, and even some of the projects that seemed destined for success, such as Tezos, have not measured up to the hype.\n\nHowever, regardless of the short term outcomes of this recent ICO bubble, to those of us in venture capital, it\u2019s clear that token sales will change the landscape of venture investment for good. VC firms may not be relevant 10 years from now. By that time startups may be able to raise all the capital they need through a token sale. Perhaps VC\u2019s can reposition themselves to participate in pre-token sale but it seems inevitable that they will lose their exclusive access and leverage in the startup investment market. Who knows how this will play out, but it seems evident to me that if there is one type of legacy VC institution that will remain relevant and perhaps even gain in importance in this new ICO landscape, it will be accelerator programs.\n\nIf you think about it, an accelerator already positions a team quite well for an ICO. During a typical accelerator program, a team pushes to develop a basic version of their product and to get performance metrics that demonstrate the effectiveness of what they have built. An accelerator also connects a startup to a network of advisors who help with development and strategy as well as lend the company legitimacy. Finally, an accelerator program gives a company visibility and media exposure as well as the accelerator\u2019s brand approval. And honestly, with just a proof of concept or whitepaper, an impressive board of advisors, and some good branding and media exposure, teams have been raising millions of dollars through their token sales.\n\nI think an accelerator could quite easily adapt its business model towards preparing their companies not for seed funding and follow on rounds, but instead prepare their teams for an ICO. And in this case everyone wins. The team raises on better terms and hopefully gives up less equity. The accelerator gains much desired liquidity through the tokenization of the companies in its portfolio. Token sale investors gain access to investing in promising startups at a much earlier stage with potentially much higher returns.\n\nBefore we get ahead of ourselves, it\u2019s worth mentioning that there are still a lot of unknowns on the side of regulation. How will major governments like the US and China decide to react to this emerging trend? Will this door close before it even fully opens? I hope not. Whatever happens, these are exciting times to be part of an accelerator program. This is an incredible opportunity to push the boundaries of traditional venture capital into a new paradigm and increase our effectiveness at growing the next generation of world-changing startups."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/hong-kong-blockchain-conference-b53dec869e44?source=user_profile---------8----------------",
        "title": "Hong Kong Blockchain Conference \u2013 Nick White \u2013",
        "text": "I attended the 2nd Annual Blockchain Conference Hong Kong this morning. There were a few things that struck me as interesting during the conference which I will share here.\n\nWallace Lynch, Founder and CEO of Alpha Token, spoke about how blockchains are an entirely new economic model in that they can create a business in which there are no external parties that control and extract value from the system. Joshua Lavin from Block.one also touched on this concept during his presentation about the EOS platform. A blockchain powered application transforms users and suppliers into shareholders in the platform. Rather than extracting value from the ecosystem by passing profits along to shareholders as is done in a traditional economic system, these profits are returned to the ecosystem and everyone benefits. Furthermore, this incentivizes early adopters of the platform to spread the word since as more people join, the tokens that a user holds appreciate in value. This will mean more shared wealth creation as well as increased virality of Blockchain platforms and applications.\n\nSomething else that struck me is that users and providers aren\u2019t the only players in the blockchain ecosystem, developers play a critical and ongoing role in the development and maintenance of the platform\u2019s code. Therefore it is not just essential to think about aligning incentives between users and providers in a blockchain ecosystem, but to align incentives for developers as well. One idea I spoke to Joshua Lavin about was having a Kaggle type of competition platform in which bounties are offered to developers for building the needed code. The code quality could be measured by some objective metrics and after the competition window closes, the best code would be selected and the developers rewarded for their effort.\n\nAlthough the era of tokens is enticing, many speakers throughout the conference stressed that not every business is tokenizeable. Frankly, many businesses cannot be run in a decentralized fashion. Ridesharing is a great example of a business that would be perfect to tokenize since it is simply a software application that basically connects and facilitates payments between drivers and riders peer-to-peer. On the other hand, a more traditional business such as a restaurant franchise would not make sense to tokenize. That\u2019s a stupid example, but in the current ICO hype we are seeing lots of businesses coming out with their own token when really it makes no sense for their business to operate on a blockchain. That said, another interesting subject that was mentioned is the tokenization of non-traditional assets. Tokenization will allow more streamlined trade of assets that previously were difficult to exchange.\n\nI think Blockchain as a technology is very powerful but we are just at the beginning. The technical side of things is maturing quickly, and in my opinion, the hardest problems in Blockchain will not be the code or the cryptography but rather the engineering of the soft aspects of a system. Such as the governance structure, the incentive system, and the crypto-economics. These aspects will be trickier to iron out, but the first team to do so will create something truly extraordinary."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/the-problem-of-vanishing-gradients-330fc874d3e3?source=user_profile---------9----------------",
        "title": "The Problem of Vanishing Gradients \u2013 Nick White \u2013",
        "text": "Recurrent Neural Networks are a powerful tool in the AI Engineer\u2019s toolkit. However, were it not for a creative architecture called a Long Short-Term Memory network or LSTM, RNNs would be very difficult to train and therefore not nearly as useful as they are today. The issue in training an RNN is the so called \u201cVanishing Gradient\u201d problem.\n\nFor those of you familiar with Back Propagation, the most common method of training feed-forward neural networks, you will know that the error signal at the output of the network must be passed back layer by layer to determine how to adjust each synaptic weight of the network all the way back to the beginning. Essentially, Back-Propagation is a way of finding out how much each weight contributed to the error at the output no matter how far back along in the network that weight may be located. The amount that you should change each synaptic weight is called the \u201cgradient.\u201d\n\nBack-Prop has worked wonders for the field of AI and is truly the workhorse of all AI training. But as it turns out, simple Back-Propagation does not work in training a recurrent neural network. The feedback loops in an RNN make it quite tricky. The first problem is that as you Back-Propagate the error through a loop, you cannot rely on the activations of the current state of the network to tell you how much a certain weight affected the error. In essence, if you want to trace back the error through network, you must not just back propagate through the various layers, but you must also back-propagate through the time-state of the network.\n\nThe solution to this problem is an algorithm called \u201cBack Propagation Through Time\u201d or BPTT. The way it works it that you \u201cunroll\u201d the RNN and keep a running memory of the previous activations of the neurons in the network for a given number of time steps backward. So if you want to train your network to be able to have a memory 100 time steps backward, then you will unroll it into 100 copies of itself such that it looks like a normal feed-forward network. Then you simply apply normal back propagation to this \u201cunrolled\u201d network with a slight catch. As you back propagate, you are actually finding the error signal and gradient for the weights of your RNN 100 times, one for each copy of the network. So, to find the true gradient of your network, you take the average of these 100 gradient signals.\n\nIt may sound complicated at first, but Back Propagation Through Time is really quite a simple generalization of normal Back-Prop. BPTT is a nice solution to this first problem of training RNNs but as it turns out there is a much trickier problem still lurking. And that is the \u201cVanishing Gradient\u201d problem. You see, in order to pass back the error signal through time, you have to apply a correction term that enables you to pass the error through the activation function at the output of each neuron. As you slap on this correction term with each layer you pass through, the correction terms begin to add up. Although in fact, they don\u2019t just add up, they multiply up and this causes big problems since the correction terms tend to be either greater than 1 or less than 1.\n\nIf the correction terms are greater than 1, the overall correction begins to blow up, causing an infinite error signal. On the other hand, if the correction terms are less than 1, the overall correction decays to 0 and \u201cvanishes\u201d, hence the name \u201cVanishing Gradient\u201d problem. Therefore, no matter how far back in time you would like to train your RNN, you can only go so far back before your error signal is meaningless. And sadly, without an error signal or gradient, you cannot train your network.\n\nAs it turns out, this problem does not just apply to RNNs but to ultra-deep neural networks as well. Once you hit a critical threshold of depth, Back-Propagation reaches its limit. Luckily, a brilliant researcher names Sepp Hochreiter came up with a creative solution to the Vanishing Gradient problem in 1997 by proposing a new RNN architecture he called a Long Short-Term Memory network.\n\nI\u2019ll cover this architecture and how it solves the vanishing gradient problem in an upcoming post. I hope you enjoyed reading!"
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/a-first-foray-into-recurrent-neural-networks-6051122acee3?source=user_profile---------10----------------",
        "title": "A First Foray Into Recurrent Neural Networks \u2013 Nick White \u2013",
        "text": "If you\u2019re new to Recurrent Neural Networks, commonly referred to as RNNs, that\u2019s okay. Me too! This post is a summary of my first learnings about RNNs.\n\nRecurrent Neural Networks differ from most other neural network architectures due to the fact that RNNs have feedback loops between neurons. Typical neural network architectures are strictly \u201cfeed-forward\u201d meaning that the information passes in a directed fashion from layer to layer all the way to the output. This feedforward structure makes the network simpler to train, but it also limits the network\u2019s capabilities. While feed-forward network takes in a fixed size input and produces a fixed size output, a recurrent network can take in an arbitrary length sequential input and produce an arbitrary length sequential output. This means that recurrent networks can be much more general than their feed-forward relatives.\n\nAs Andrej Karpathy explains in an excellent blog post,\n\nSound pretty cool? Well it is. RNNs can do some pretty incredible things. They are used for translation, speech recognition, text generation, and much more. One rule of thumb is that if you are trying to perform a learning task in which there is some form of sequential structure or time structure to the problem, an RNN will likely be best suited to handle it. Essentially, an RNN is able to preserve a \u201cmemory\u201d of the previous inputs as its internal state of activations. Therefore the output of the RNN does not just depend on the current input, but all the previous inputs it has seen as well. This concept of memory is why one of the most popular forms of RNN is called a \u201cLong Short-Term Memory\u201d network or LSTM for short.\n\nIf you think about this from a biological perspective, RNNs make a lot of sense. Its clear that the brain is not strictly feed-forward. Rather, the brain is a complex web of neurons with countless feedback loops. The stimuli that we receive don\u2019t just flash for a brief instant in our mind \u2014 they echo around in our brain for a length of time before they are forgotten.\n\nI\u2019ve got a lot more to learn about RNN architectures and training methods, but its clear that they are an essential part of any AI Engineer\u2019s toolkit. I look forward to sharing my further learnings in a future post!"
    },
    {
        "url": "https://codeburst.io/what-is-intelligence-6a5404754077?source=user_profile---------11----------------",
        "title": "What Is Intelligence? \u2013",
        "text": "In the field of Artificial Intelligence, we tend to move full speed ahead in building and training models without considering the philosophical aspects of what we\u2019re doing. It\u2019s great to focus on development, but we also have a responsibility to consider the bigger picture from time to time. One of the things that drew me to Artificial Intelligence in the first place is that there is an equal amount of philosophical and technical depth to the discipline. Furthermore, I have found that the philosophy and the theory enrich one another.\n\nOne question AI Engineers don\u2019t often stop to think about is, ironically, the concept of Intelligence itself. What is Intelligence? A good starting point is this quote from Kevin Kelly.\n\nMy interpretation of this is that there is no one form of intelligence. There is instead a multiplicity of types of intelligence, each one distinct from the others. For this reason, it is not possible to create a single absolute measure of intelligence or to compare intelligences across their different forms. As an example, one person may be very gifted at mathematics but awkward in social situations while another struggles with math but is a natural when it comes to meeting people and forming relationships. You might say one has a high IQ and the other has a high EQ. Who is more intelligent? Neither. Their intelligences are just different.\n\nThe same goes for different neural network architectures. Which neural network is the most intelligent? This question is meaningless unless you specify the task which you want to neural network to solve. What kind of intelligence does your task require? A convolutional neural network is not inherently better than a recurrent neural network; they each excel in different situations and fail miserably in others. Try teaching a convolutional neural network to do translation or a recurrent neural network to do object detection. You won\u2019t get very far.\n\nHowever, you may have a task in which you need both types of intelligence supplied by the two architectures such as action classification in a video stream. You will need the convolutional neural network to extract features from the images and the recurrent neural network to analyze the features of the images over time to detect an action like drinking from a cup. To me, the job of an AI Engineer is to know what kinds of intelligences are needed for an application, then to choose the right architectures to train the appropriate intelligences and finally to combine those intelligences together in a way in which they can cooperate and become greater than the sum of their parts.\n\nSo the question \u201cwhat is intelligence\u201d is actually central to the practice of Artificial Intelligence. Furthermore, grappling with this seemingly esoteric question can help you frame your practice of AI and become a more effective engineer."
    },
    {
        "url": "https://towardsdatascience.com/review-of-andrew-ngs-new-deeplearning-ai-course-742e5324fbf6?source=user_profile---------12----------------",
        "title": "Review of Andrew Ng\u2019s New Deeplearning.ai Course \u2013",
        "text": "A brain-dump of my learnings, observations and opinions\n\nAndrew Ng recently announced a new series of courses on Deep Learning that he would release on Coursera. So far, 3 of 5 courses are up and I finished the 3rd course last night. While I wait for the last two courses to be released, I figured I would offer my take on the courses thus far.\n\nThis course is meant to take you from zero knowledge of Deep Learning all the way to being able to train your own deep net including CNN\u2019s and RNN\u2019s. He covers the very basics all the way up to the higher level strategic aspects of doing deep learning. It is comprehensive and packed with information. He takes care to present everything sequentially so that each new piece builds upon the last and you don\u2019t get lost.\n\nYou need to have working knowledge of linear algebra, calculus and statistics. If you don\u2019t, it\u2019s not the end of the world. He doesn\u2019t force you to do anything difficult, and he\u2019s good at giving intuition even if you don\u2019t understand the reason the mathematics work out that way. At the end of the day, a lot of Deep Learning relies on developing intuitions because frankly, you can\u2019t know what\u2019s happening inside of a neural network analytically. It is a black box. However, that being said, knowing the fundamental math and analysis for how back-prop works and why certain techniques like normalization work will help you be better at your craft. But the bottom line is that with the frameworks out there these days, you don\u2019t need to know much to get something working.\n\nAt the end of the day, this course will teach you a lot, but it won\u2019t make you a good programmer. Most of the programming assignments are spoon-fed. You don\u2019t need to write any of the functions themselves, they are all laid out for you and you just need to implement a few lines and get it to run. And often they give you the exact line of code you need to write. It can become a simple copy and paste exercise in which you don\u2019t really learn anything. That\u2019s a shame. However, the advantage of this is that you get to do a bunch of different cool programming assignments with rather impressive results in a much shorter amount of time. This means that you get the satisfaction of doing something great, and it gets you hooked.\n\nThis course will get you excited about deep learning. I can\u2019t say it will make you an epic programmer because it\u2019s not that challenging, but Andrew will give you a tremendous amount of knowledge on the subject. I also think Andrew\u2019s goal was to make deep learning as simple as possible in this course. This is your gateway drug. He wanted to make it accessible. And the reality is, as Andrew himself says, that Deep Learning is remarkably simple, so why does everyone have the perception that it\u2019s hard? I think it\u2019s because the coding and concepts are communicated in the wrong way that is overly complex. Andrew\u2019s genius in this course is to present the conceptual and coding parts of Deep Learning in a simple way that almost anyone can grasp. Bravo!\n\nOne of the coolest parts of the Deep Learning course is that Andrew has added in a \u201cHeroes of Deep Learning\u201d interview at the end of each week in which he chats with the biggest names in the field of deep learning, like Geof Hinton, Yoshua Bengio, Andej Karpathy, etc. These interviews are fantastic. They let you feel like part of the conversation with all the heavy-weights. You get a window into their worlds, how they got to where they are, and where they think the field is headed. These talks are so valuable. They are loaded with insights and inspiration. Although these interviews are \u201coptional\u201d you should absolutely take advantage of them. A big thank you to Andrew and all the \u201cheroes\u201d for making these interviews and sharing them with the world.\n\nCourse 1 \u2014 Learn the fundamentals of how neural networks work. Program linear regression for a single neuron, then back-prop shallow nets and finally deep nets.\n\nCourse 2 \u2014 Learn about methods to accelerate training for your neural networks. This includes hyper-parameter tuning, normalization, regularization, momentum, RMS prop, and Adam. This is all about how to optimize your models. Deep Learning is so computation heavy, that a lot of these tricks are what make it possible in the world today. Cutting training time by 50% is huge! Not just in time, but in costs. That\u2019s what these techniques can help you acheive.\n\nCourse 3 \u2014 Learn how to approach your machine learning project. How to ask the right questions and then how to iterate to get the best results. This will teach you reliable methods to achieve good performance. This includes transfer learning, multi-task learning, how to structure your training, dev and test sets, how to set meaningful benchmarks. How to combine deep learning with hand-designed tasks.\n\nI encourage you to jump into the course. Even if you feel like you you\u2019re knowledgeable about the subject, you will learn something you didn\u2019t. Plus, how can you resist Andrew\u2019s charm? He\u2019s like a big Deep Learning panda. I just want to give him a squeeze. In all seriousness, Andrew has clearly put a lot of thought and effort into this course, and let me tell you, it has paid off. This course is destined to be a classic much like his original \u201cMachine Learning\u201d class that he posted on Coursera back in 2012. He is a great instructor, and Deep Learning is one of the technologies that will transform our world in the next few decades.\n\nAI is the new electricity and this course will put the power in your hands."
    },
    {
        "url": "https://medium.com/all-technology-feeds/the-case-for-ethics-in-ai-c3be6c5cdc61?source=user_profile---------13----------------",
        "title": "The Case for Ethics in AI \u2013 All Technology Feeds(Powered by Algoworks) \u2013",
        "text": "The Case for Ethics in AI Balancing the risks and benefits of the ultimate technology\n\nThe development of Artificial Intelligence promises to give humankind new power, but like many technologies before it, AI is a double-edged sword. On one hand, AI could add economic value of $50 trillion by 2025. On the other hand, AI may severely disrupt our social, economic, and political systems or perhaps even cause the extinction of the human race. Indeed, the dangers of AI may outweigh the benefits.\n\nIn order to chart a course that will enable us to reap the benefits of AI while mitigating the risks as much as possible, there are crucial safety measures we must take, ethical questions we must answer, and economic planning we must take into account. This spans the subjects of law, politics, economics, physics, mathematics, computer science and more. In fact, developing these guidelines may be more difficult than developing AI itself and unlike in engineering problems, there is no clear right answer.\n\nOne thing is indisputable. Research into the economics, ethics and safety of AI must catch up to and keep pace with the research into the technology. Thankfully this is starting to happen. Deepmind recently published a paper in collaboration with the Future of Humanity Institute titled \u201cSafely Interruptible Agents,\u201d which outlines methods of keeping a safe off-switch for any reinforcement learning AI agent that gets out of control. Yet as new algorithmic methods are invented, we\u2019ll need new techniques to keep them safe.\n\nIn a similar vein, as AI gains new capabilities and further transforms the economy, we\u2019ll need new policies to keep the world from coming unglued. People have proposed rapid re-education for new jobs as a way to get workers displaced by AI back into the workforce. But what happens when we have Strong-AI that is better than a human at any cognitive task? There may not be any new jobs to re-educate for at all. Such a situation is long in the future, but the beginning of the economic impacts of AI are not far away. We need to start planning now.\n\nAs we hand over more and more control to autonomous AI agents, it is inevitable that these agents will be faced with difficult ethical decisions. In the famous \u201cTrolley Problem,\u201d one must decide whether or not to steer a trolley to kill one person in order to save the lives of five others. If an autonomous car encounters the same situation, how should it decide who to kill? Furthermore, if the autonomous car kills someone, who is legally responsible?\n\nThe most fearsome and least-well understood danger occurs when AI becomes able to increase its own intelligence, causing an exponential explosion in its intelligence. At this moment, the AI becomes so capable that it cannot be stopped and can act so quickly that we may not have time to hit the off-switch. Such an AI may have an innocent objective like Nick Bostrom\u2019s hypothetical paperclip-making machine, but if its objective is not aligned with our own, it will think nothing of killing everything in order to achieve its goal.\n\nIf you think the idea of an AI apocalypse is absurd, you\u2019re not alone. Many authorities in the AI community doubt that an \u201cintelligence explosion\u201d would be dangerous or could happen at all. However, a recent survey of AI experts demonstrates there is cause for concern. According to the study, 40% believe that the \u201cintelligence explosion\u201d argument is valid and 70% believe that superintelligent AI could be a threat. In addition, 36% think research in AI safety is as important or more important than research into general AI.\n\nTo avoid the AI Armageddon, we must make sure that any AI\u2019s objectives are aligned with our own. This is a difficult problem to solve but with research into game theory and decision theory we could potentially develop mathematical methods of proving that an AI algorithm is safe. To test our theories we could deploy these agents in isolated simulations so that they are confined to a world from which they cannot escape and in which they can do no harm.\n\nBut that still leaves us with the hardest questions of all. In order to know if an AI\u2019s objective are aligned with our own, we must first know what our objectives are. So, what are our objectives? As individuals, as a species? Furthermore, when strong-AI is able to do all our work for us, what will be our purpose? Is this a future that we want, or should we stop AI development before that happens?\n\nElon Musk famously said, \u201cwith artificial intelligence we are summoning the demon.\u201d It sounds dramatic, but it could be true. Others like Ray Kurzweil predict that AI will bring about a transcendent singularity. Whatever you believe, superintelligent AI will be the last technology we ever create. So we\u2019d better get it right the first time."
    },
    {
        "url": "https://medium.com/@nickikwhite_5051/ai-is-the-new-electricity-637a074f1047?source=user_profile---------14----------------",
        "title": "Why AI Is The New Electricity \u2013 Nick White \u2013",
        "text": "Andrew Ng recently quipped, \u201cAI is the new electricity.\u201d To some this statement may seem like an exaggeration, but I think Andrew is on to something.\n\nIn the early 19th century, electricity was a fringe area of research. Then, over the course of several decades, the fundamental theories of electricity were worked out by pioneering physicists like Micheal Faraday, Georg Ohm and James Maxwell. This theoretical groundwork enabled men like Alexander Graham Bell, Nikola Tesla and Thomas Edison to bring the brilliance of electricity to the masses.\n\nUsing this novel force called electricity, these first electrical engineers invented the telephone, the induction motor and the lightbulb. These innovations had such a dramatic impact that by the end of the 19th century, \u201celectricity turned from a scientific curiosity into an essential tool for modern life, becoming a driving force of the Second Industrial Revolution.\u201d\n\nFastforward to now, the early 21st century. AI is no longer a fringe research topic \u2014 it is one of the hottest technologies in the world. Thanks to ongoing research in AI and machine learning beginning in the 1950\u2019s and continuing through the \u201cAI winter\u201d until AI\u2019s present golden age, a large foundation of theoretical groundwork for AI has been laid down.\n\nWe are currently living through the moment in time for the modern day Thomas Edisons of AI to use these theories to invent revolutionary new products, the likes of which we have only, or have never, dreamed of. I believe that in the coming decades AI will complete its transition from scientific curiosity to an essential tool for modern life and become a driving force for the Second Machine Age.\n\nPeople often remark how amazing it is that the Apollo 11 mission to the moon was successful given that their onboard computer had less computing power than a cell-phone circa 2009. We\u2019ve become so dependent on computers that we\u2019re awed that we were ever able to accomplish things without them.\n\nIf we wind back the clock even further to a world before electricity, the contrast is even starker. This technology is now so important and widespread that it\u2019s hard for us to imagine life without electricity.\n\nIn the future, when AI is as ubiquitous as electricity is today, we will be awed by all the things we achieved before we had computers that could think. We will say to ourselves, \u201cHow did we ever live without it?\u201d"
    }
]