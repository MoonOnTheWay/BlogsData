[
    {
        "url": "https://towardsdatascience.com/deep-surveillance-6b389abeaf95?source=user_profile---------1----------------",
        "title": "Deep Surveillance \u2013",
        "text": "Kalliatakis et al. (2017)[4] have compiled the Human Rights Understanding (HRUN) data set. This collection of 400 manually labeled images files includes photos of child soldiers and violent interactions between police officers and civilians.\n\nA data set of this small size can only be effectively processed with the help of transfer learning.\n\nTo this end, the authors compare the performance of 10 established convolutional neural networks. The models are either pre-trained on the ImageNet data set or \u2014 in the case of the 8-layer Places architecture \u2014 optimized for a collection of 10 million scene photographs[5]. To use the models as feature extractors, the layers generating the predictions are removed.\n\nThe second component of the model is a linear SVM classifier that is trained on the HRUN data sets and accepts the extracted features as input.\n\nUsing a 50/50 split for training and test images, the authors report excellent results. The transfer learning approach reached an average precision of 90% for the child soldiers category and close to 96% for violent interactions between the police and civilians. Interestingly, the best results were achieved with the Places architecture.\n\nVideos, of course, are sequences of images. While most state-of-the-art image classification systems use convolutional layers in one form or another, sequential data is frequently processed by Long Short-Term Memory (LSTM) Networks. Consequently, a combination of these two building blocks is expected to perform well on a video classification task.\n\nOne such combination has the self-descriptive name of ConvLSTM[6]. Standard LSTM uses simple matrix multiplication to weigh the input and previous state inside the different gates. In ConvLSTM, these operations are replaced by convolutions.\n\nA paper by Sudhakaran and Lanz (2017) tests how well this approach works for the detection of violence in video content[7].\n\nTo force the network to model the changes over time, the authors use the difference of two adjacent frames as the input at each step. The AlexNet architecture is then used to generate a vector representation that is sent to the ConvLSTM instance. The final hidden state, after all frames have been processed, is forwarded to a sequence of fully connected layers that computes the classification.\n\nThe model is evaluated on the small data sets. The Hockey Fight Dataset consists of 500 videos of ice hockey matches, showing either fights or other content. The Movies Dataset contains 100 fight scenes and 100 scenes without violence. The Violent-Flows Crowd Violence Dataset is a collection of 246 videos depicting violent and non-violent crowd behavior at sports events. To augment the data, the authors perform random cropping and horizontal flipping.\n\nThe paper reports a second place on the Violent-Flows data set, state-of-the-art results for violence detection in ice hockey videos and a perfect result on the Movies Dataset .\n\nFor the Hockey Dataset, using the difference of two adjacent frames as input and a pre-trained AlexNet for feature extraction increases the accuracy from 94% to 97% compared to a randomly initialized network with individual frames as input.\n\nThese results are remarkable considering that violent and non-violent scenes can exhibit a high degree of feature overlap. A closer look at some of the lower-level details is required, for example, to distinguish a fight from a hug in an ice hockey match.\n\nIn a civilized society, peaceful co-existence is the norm and violence is the exception. This fortunate fact allows Sultani et al. (2018)[8] to treat intelligent surveillance as an anomaly detection problem. In addition to interpersonal violence, the 13 anomalies they consider include other arson, theft and accidents.\n\nUsing the search functionality on YouTube and LiveLeak, the researchers compiled a set of videos showing real-world anomalies. Only unedited recordings by surveillance cameras made it into the final collection of 1,900 videos. The data set is equally balanced between anomalies (labeled as positive) and normal events (labeled as negative).\n\nEach video is represented as a bag of m temporal segments. In the positive case, at least one of the m segment is assumed to contain an anomaly. In the negative case, none of segments contain an anomaly.\n\nTo collect examples for a larger number of videos, annotators provided labels on the level of bags, and not on the level of individual segments. In other words, the data set tells you whether a given video show any anomaly at all. It does not tell you when the anomaly occurs.\n\nThe following notation refers to the i-th segment in a bag B representing a video V. The letters a and n are used to denote anomalous and normal events, respectively:\n\nThe function f assigns an anomaly score between 0 and 1 to each segment.\n\nA key idea is to push the highest-scoring positive segments as far away from the highest-scoring negative segments as possible. This essential objective is expressed using the following hinge loss function:\n\nIn the best possible case, the highest segment score is 1 for the anomalous video and 0 for the normal video. This results in a loss of 0:\n\nIn the worst case, the scores are reversed and the loss is 2:\n\nThe scoring function f uses the representation that is extracted from the pre-trained convolutional 3D (C3D) network, an architecture that was specifically designed with transfer learning in mind.\n\nImages are two-dimensional. Video analysis is spatio-temporal: it adds time as the third dimension. In the C3D network described in Tran et al. (2015)[9], videos are resized to 128x171 (a 4:3 aspect ratio) and split into clips of 16 frames each. Using three color channels, the input has a size of 3x16x128x171. Convolutional filters in this network have have a d x k x k format, where d refers to temporal dimension and k x k refers to the spatial dimensions. Empirical results suggest that a 3x3x3 configuration is an appropriate choice.\n\nThe first five blocks in the network consists of one or two convolutionals layer followed by a pooling operation.To generate predictions, the computation is continued by a sequence of two fully-connected layers (identified as fc6 and fc7) and finally completed by a softmax layer. The authors of the C3D network trained the model on the Sports-1M data set, a collection of more than one million videos from 487 sports categories.\n\nThe representational power of the trained model can be reused for other tasks. A video from a different data set is first split into the required format of 16 frame long clips. The fc6 clip activations of the individual clips are then averaged to form an L2-normalized feature vector with 4096 entries.\n\nGoing back to the anomaly detector, this feature vector is used as the input to a 3-layer fully-connected neural network with Dropout. The last layer in this architecture has just one unit and computes the anomaly score through the application of the sigmoid activation function to the weighted input."
    },
    {
        "url": "https://towardsdatascience.com/gpu-accelerated-neural-networks-in-javascript-195d6f8e69ef?source=user_profile---------2----------------",
        "title": "GPU-accelerated Neural Networks in JavaScript \u2013",
        "text": "I intend to maintain this article and expand it into a Github repository. Please let me know when you come across relevant news.\n\nWhile its feature set is geared towards neural networks, deeplearn.js can be described as a general-purpose machine learning framework. Propel is a library for scientific computing that offers automatic differentiation. Gpu.js provides a convenient way to run JavaScript functions on the GPU. Brain.js is a continuation of an older neural network library and uses gpu.js for hardware acceleration.\n\nIn the end, four projects have made it onto the list.\n\nLibraries that are designed to run existing models (especially those trained with Python frameworks) are not included in this overview.\n\nThey all implement GPU acceleration in the browser through WebGL and fall back to the CPU if a suitable graphics card is not present.\n\nAll projects listed below are actively maintained, have thousands of stars on Github and are distributed through NPM or CDNs.\n\nThis article looks at the ongoing convergence of these trends and provides an overview of the projects that are bringing GPU-accelerated neural networks to the JavaScript world.\n\nTraining neural networks with deep architectures is a computationally intensive process that has led to state-of-the-art results across many important domains of machine intelligence.\n\nMeanwhile, the use of GPU acceleration has expanded well beyond computer graphics and is now an integral part of machine learning.\n\nJavaScript has conquered the Web and made inroads on the server, mobile phones, the desktop and other platforms.\n\nAccording to the Octoverse 2017 report, JavaScript is the most popular language on Github. Measured by the number of pull requests, the level of JavaScript activity is comparable to that of Python, Java and Go combined.\n\nDeeplearn.js is the most popular project among the four and described as a \u201chardware-accelerated JavaScript library for machine intelligence\u201d. It is supported by the Google Brain team and a community of more than 50 contributors. The two main authors are Daniel Smilkov and Nikhil Thorat.\n\nWritten in TypeScript and modeled after Tensorflow, deeplearn.js supports a growing subset of the features provided in Google Brain\u2019s flagship open-source project. The API essentially has three parts.\n\nThe first part covers functions used to create, initialize and transform tensors, the array-like structures that hold the data.\n\nThe next part of the API provides the operations that are performed on tensors. This includes basic mathematical operations, reduction, normalization and convolution. Support for recurrent neural networks is rudimentary at this point, but does include stacks of Long Short Term Memory Network cells.\n\nThe third part revolves around model training. All of the popular optimizers, from stochastic gradient descent to Adam, are included. Cross entropy loss, on the other hand, is the only loss function that is mentioned in the reference.\n\nThe remainder of the API is used to set up the environment and manage resources.\n\nExperimental GPU acceleration in node.js can be achieved through headless-gl (see issue #49).\n\nThe project website has a number of memorable demos. These include piano performances by a recurrent neural network, a visual interface to build models and a webcam application based on a SqueezeNet (an image classifier with a relatively small number of parameters).\n\nPropel is described as \u201cdifferentiable programming for JavaScript\u201d. The work of the two main authors, Ryan Dahl and Bert Belder, is complemented by eleven contributors.\n\nAutomatic differentiation (AD) is at the core of this project and frees us from the need to manually specify derivatives. For a given function f(x) defined with the supported tensor operations, the gradient function can be obtained using grad. The multi-variable case is covered by multigrad.\n\nBeyond AD, it does not seem entirely clear where the project is heading. While a \u201cnumpy-like infrastructure\u201d is mentioned as a goal on the website, the API is under \u201cheavy development\u201d and includes functionality associated with neural networks and computer vision. Using the load function, the content of npy files can be parsed and used as tensors.\n\nIn a browser environment, Propel makes use of the WebGL capabilities in deeplearn.js. For GPU acceleration in Node, the project uses TensorFlow\u2019s C API.\n\nWhile most of my experience is with CUDA rather than WebGL, I can attest to the time-consuming nature of GPU programming. I was therefore pleasantly surprised when I came across gpu.js. With around 5,700 stars on Github, the project is comparable to deeplearn.js in terms of its popularity and has 18 contributors. Several individuals have made substantial contributions over time. Robert Plummer is the main author.\n\nA kernel, in the current context, is a function that is executed on the GPU rather than the CPU. With gpu.js, kernels can be written in a subset of JavaScript. The code is then compiled and run on the GPU. Node.JS support through OpenCL has been added a few weeks ago.\n\nNumbers and arrays of numbers with up to three dimensions are used as input and output. In addition to basic mathematical operations, gpu.js supports local variables, loops and if/else statements.\n\nTo enable code reuse and allow for a more modular design, custom functions can be registered and then used from within the kernel code.\n\nWithin the JavaScript definition of a kernel, the this object provides the thread identifiers and holds values that are constant inside the actual kernel but dynamic on the outside.\n\nThe project specializes in accelerated JavaScript functions and does not attempt to provide a neural network framework. For that, we can turn to a library that depends on gpu.js.\n\nBrain.js is the successor to harthur/brain, a repository with a history dating back to the ancient times of 2010.\n\nIn total, close to 30 individuals have contributed to these two repositories.\n\nSupport for GPU-accelerated neural nets is based on gpu.js and has, arguably, been the most important development in the project\u2019s recent history.\n\nIn addition to feed-forward networks, Brain.js includes implementations of three important types of recurrent neural networks: classic Elman networks, Long-short Term Memory Networks and the more recent networks with Gated Recurrent Units.\n\nThe demos included in the repository are at an early stage. A neural network learning color contrast preferences is shown on the homepage. Two other demos, one involving the detection of characters drawn with ASCII symbols, can be found in the source code."
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-from-scratch-part-4-10117c005a28?source=user_profile---------3----------------",
        "title": "Machine Learning From Scratch: Part 4 \u2013",
        "text": "Functions are at the core of machine learning.\n\nAt the beginning of a project, the business objectives are specified in terms of functions.\n\nDuring the project, functions are used at just about every step to process the data, to discover patterns and to evaluate the performance of the system.\n\nAt the end of the project, what is ultimately delivered to the client, is another function. The deliverable may be coded in a programming language or take the form of a composition of several lower-level functions, but it is a function nonetheless.\n\nMany readers will be familiar with functions from mathematics and/or programming.\n\nMathematically, a function specifies how elements in one set are related to elements of another set.\n\nProgrammatically, a function processes an input to generate an output.\n\nThe following graphic combines aspects from both definitions:\n\nThese two definitions are consistent with each other.[1] The acceptable input to a function belongs to one set and the output that the function generates belongs to another another set. Functions define how we get from one to the other.\n\nAnd that\u2019s all there is to know about the essence of functions.\n\nAs a preview of what is to come, I will mention that the input to the functions at the center of machine learning are the representations of objects that were discussed last time (e.g., pixel intensities and the presence or absence of certain words). The output of functions are the predicted values for the targets that we are interested in.\n\nTo define a function mathematically, we need to specify two things:\n\nThe fact that a function f maps elements from set A to elements in set B is written as f: A \u2192 B.\n\nThe two sets, A and B, are called domain and co-domain, respectively. They can and, in many cases, do refer to the same set.\n\nThe letters used in this notation (f, A and B) can be understood as placeholders. We will often use different names.\n\nThe input to a function f is usually denoted with the letter x. In the context of machine learning, x usually refers to the array that represents an object.\n\nThe letter y designates the output of a function. We will use the letter y to denote predictions.\n\nCombining these building blocks, the following equation simply states that y is the output of the application of function f to the input x: f(x) = y.\n\nIn many cases, we will need to refer to more than one function. To keep them apart, one of the following naming strategies can be employed:\n\nConsider the self-descriptive example of a function named fahrenheit_to_celsius.\n\nGiven that temperatures of trillions of degrees of Fahrenheit have been achieved in the laboratory[2], I think it\u2019s fair to use the set of real numbers for this function. At the level of sets, we can write:\n\nAt the level of individual elements, we have:\n\nUsing a style that emphasizes the equivalence between mathematics and programming, this function corresponds to the following Python code[3]:"
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-from-scratch-part-3-ed572330367d?source=user_profile---------4----------------",
        "title": "Machine Learning From Scratch: Part 3 \u2013",
        "text": "In many NLP projects, the initial vocabulary is passed through a pipeline: a series of operations in which the output of one step is the input to the next step.\n\nThe particular sequence of operations described in this section simplifies the data and eliminates irrelevant words.\n\nWarning! Proceed with caution! While many of the steps mentioned below work quite reliably across different tasks and domains, they can filter out relevant words when applied too aggressively.\n\nFunction words, such as pronouns, are a case in point. These are words that mainly contribute to the syntax of a sentence. In most applications, we can remove function words without causing a significant loss of information. For some psycholinguistic applications, however, these words can provide useful features.[5] For example, one study found that similarity in the use of function words predicts romantic interest and relationship stability.[6]\n\nWith the obligatory warning out of the way, here is an overview of the entire process:\n\nThe first step we apply can be called normalization. Contracted forms (such as I\u2019ve) are expanded (I have), while elongated words (such as looove) are reduced to their conventional form (love). Personally, I have nothing anything against looove. It\u2019s just that patterns easier to recognize when all expressions of love have the same number of o\u2019s. The normalized vocabulary looks like this:\n\n{ a, Best, disappointing, ever, hate, have, I, love, movie, seen, this, What, . }\n\nThe next step is conversion to lowercase. Does it make a difference for sentiment analysis whether the first letter in the words best and what is uppercase? \u2014 No, not really:\n\n{ a, best, disappointing, ever, hate, have, i, love, movie, see, this, what, . }\n\nAnother common step is to remove non-alphabetic words. The full stop, for example, carries little relevant information. Let\u2019s get rid of it:\n\n{ a, best, disappointing, ever, hate, have, i, love, movie, see, this, what }\n\nPart 2 mentioned that some words, including articles, determiners, prepositions and basic verbs occur in just about every text: in positive reviews and in negative reviews, in movie reviews and in non-movie reviews, in the previous sentence, in this sentence, in the next sentence \u2026 you get the idea. These words are called stop words. Let\u2019s delete them:\n\n{ best, disappointing, ever, hate, love, movie, see }\n\nAnd there are some words that behave like stop words within a particular domain, even though they are not considered to be stop words in general. For example, the words movie and see occur in a large fraction of movie reviews, without providing clues about the polarity of an opinion. I will refer to these words as domain-specific stop words. Deleting these words is the final step in the pipeline and yields the following result:\n\nOverall, we have eliminated 8 out of 13 members of the vocabulary and have arrived at what I think is an intuitively plausible set of words that are relevant for sentiment analysis.\n\nUsing the five remaining words (best, disappointing, ever, hate, love) as features, we can now represent each of the documents as a vector with five entries. For every feature, there is a corresponding vector entry.\n\nBinary vectors are an appropriate choice for short documents. These are vectors whose entries are all 0 or 1.\n\nWe assign a feature value of 1 if the feature (word) is present in the document and a value of 0 if the feature is absent.\n\nConsider the third document as an example (Best movie I\u2019ve ever seen.). Two of the five features are present in this document: the first feature (best) and the third one (ever). Consequently, we set the first and the third entry to 1 and the other entries to 0. This gives us the vector [ 1 0 1 0 0 ].\n\nApplying the same procedure to the every document in the corpus, we obtain the following vectorial representations:"
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-from-scratch-part-2-99ce4c78a3cc?source=user_profile---------5----------------",
        "title": "Machine Learning From Scratch: Part 2 \u2013",
        "text": "Some of the first associations that should come to mind when you hear the term \u201cdata\u201d are collecting, counting, evaluating, logging, measuring, quantifying, rating, surveying, tracking and weighing. These are the activities that provide the precious raw material for machine learning.\n\nData is any collection of measured attribute values. Biomarkers, financial numbers, sales figures, network connections, survey responses, user activity, video recordings, word counts and the number of available flavors in your favorite ice cream parlor all fall under the rubric of data.\n\nA datum is a single measurement of an attribute value. One datum is the number of words in this article (2,268).\n\nSuppose we are interested in a particular set of attributes and measure their values on several occasions \u2014 perhaps at different times, at different locations or for different objects. A collection of such measurements taken on any one of these occasions is a data point.\n\nFitness trackers, for example, take measurements of cardiovascular performance, activity levels and sleep quality. The data collected for a particular person at a particular time constitutes one data point.\n\nAn example is a data point that has been collected in an effort to solve a machine learning problem.\n\nIf we wanted to predict the performance of online advertisements, each example would characterize a particular display of an advertisement that occurred in the past. This includes features with regard to the position, format and design of the advertisement as well as the presence or absence of certain words and demographic data about the user it was presented to.\n\nAn example is labeled when it includes a target value and is unlabeled when the target value is absent.\n\nIn the advertising example, an example is labeled when it includes a performance measure, such as a yes/no value that indicates whether the ad was clicked on.\n\nSupervised learning uses data sets with labeled examples. Unsupervised learning, a type of learning that will be covered in later chapters, is an attempt to recognize patterns in unlabeled examples.\n\nDuring development, data sets are used as sources for pattern recognition and to evaluate the performance of a machine learning system.\n\nIn production, the system is shown new data points. These data points are often similar but rarely identical to the examples that were available during development.\n\nThe following tree shows the types of collections that will be used throughout this series:\n\nThe two basic collection types are sets and lists.\n\nA set is a collection of distinct objects. In other words, no object can occur more than once. The objects that belong to a set are called members or elements. The number of elements in a set is known as its cardinality or the size of the set.\n\nSpecific sets are denoted with capital letters. Members of sets are written in lowercase letters.\n\nCurly brackets indicate a collection of a objects form a set. A set of basic ice cream flavors could look like this: S = { vanilla, chocolate, strawberry }.\n\nThe fact that an element x belongs to a set S is written x \u2208 S. The size of a set S is denoted with vertical bars: |S|.\n\nIn the previous example, we have vanilla \u2208 S and |S|=3.\n\nA list, by contrast, is a collection in which objects can occur more than once. The objects that belong to a list are referred to as items or elements. The number of items that a list contains is known as the size or the length of the list.\n\nI will use square brackets to denote lists. A customer order of two scoops of strawberry ice cream and one scoop of vanilla at our basic ice cream parlor can be represented through the following list: [ strawberry, strawberry, vanilla ].\n\nNote that a list is the right choice in this case. A business that used sets to represent orders would be unable to serve the correct number of requested scoops of ice cream.\n\nLists can be further divided into two sub-types:\n\nA tuple is a list of a fixed length. In other words, you can neither add additional items nor remove any of the existing items.\n\nTuples of length 2 and 3 are called pairs and triples, respectively. Tuples of length 1 are unlikely to be used anytime soon in this series. (For the sake of completeness, I will note that they are referred to as singletons.) Two different naming schemes are used for tuples with a length of 4 or greater. Some people prefer Latin prefixes and refer to these lists as quadruples, quintuples, sextuples, etc. Others prefer to call them 4-tuples, 5-tuples, 6-tuples, and so forth.\n\nA tuple is denoted by parentheses and can be used to represent a datum.\n\nSuppose you are running an e-commerce site that offers a single product and allows customers to post a rating. In this case, a basic representation of a rating takes the form of a pair: (customer id, rating). As soon as you add a second product to the site, the format needs to extended from a pair to a triple: (customer id, product id, rating). A fourth item becomes necessary when you offer multiple products and allow customers to change their ratings over time: (customer id, product id, date and time, rating).\n\nIn other words, the length of a tuple often depends on how much contextual information you would like to encode.\n\nA variable-length list is used when the number of items changes or cannot be predicted in advance. New items can be added and existing items can removed. Unless stated otherwise, the term list will always refer to lists of a variable length.\n\nTo conclude our discussion of basic collections, I\u2019m providing an overview of the three types that we have covered:\n\nBefore I forget, one last thing on this topic: collections can be organized in collections.\n\nIn other words, we can have sets of sets, sets of lists, sets of tuples, lists of sets, list of lists, list of tuples, tuples of sets, tuples of lists and tuples of tuples. I will refer to these objects as higher-order collections.\n\nAn image, for example, can be represented through the red/green/blue pixel intensities. For each of these three color channels, we have one list of intensities. The image as a whole can be regarded as a list of three lists. Several images, in turn, form a list of lists of three lists.\n\nIt was mentioned that a datum can be thought of as a tuple and that a data point is a collection of attribute values (measured for a particular purpose and on a particular occasion). Equipped with the concept of higher-order collections, we can now understand a data point as a set of tuples. Finally, a data set in supervised learning can be described as a list of labeled examples, which, in turn, are pairs, consisting of a set of tuples (the data point) and a target value.\n\nAs you can see, we can easily build increasingly complex higher-order collections from basic collections."
    },
    {
        "url": "https://towardsdatascience.com/machine-learning-from-scratch-part-1-76603dececa6?source=user_profile---------6----------------",
        "title": "Machine Learning From Scratch: Part 1 \u2013",
        "text": "Since machine learning deals with unknown object attributes, a natural starting point is to talk about objects.\n\nFor purposes of this tutorial, an object is anything that has attributes.\n\nI\u2019ve promised that I will make abstract notions concrete and use visuals where it seems appropriate. Here is a photo that shows three exemplary objects (of desire):\n\nAmong other attributes, each of these scoops of ice cream has a characteristic taste, a range of colors it is presented in, a certain level of popularity, a price it sells for, etc.\n\nAn attribute is something that characterizes an object.\n\nLet me warn you ahead of time. I love concept trees. Consequently, you will see many of them over the course of this series. Here is the first one:\n\nAt a high level, we can distinguish between measured attributes and unmeasured attributes.\n\nMany attributes are measurable. Simple examples include the weight of a scoop of ice cream and the price at which a unit is sold at a particular place and time. Out of the attributes that are measurable, only a fraction will actually be measured as part of the data collection activities during a project.\n\nOther attributes will not be measured because they are either irrelevant, beyond our current understanding or outside of our budget. The brain, presumably, has attributes that we could measure to create the perfect new ice cream flavor. In practice, however, such an effort is constrained by our imperfect understanding of how the brain generates the subjective experience of tasting delicious ice cream and the price tag put on the required scanning equipment.\n\nLet\u2019s further break down measured attributes into two sub-types:\n\nThe target is the attribute that we care about and that we would like to predict.\n\nSome of terms that are largely synonymous are dependent variable, unobserved variable, explained variable, output variable and outcome measurement.\n\n[I should note that some of the concepts used in machine learning are known under different names in other communities. In many ways, machine learning is a descendant of statistics and there is a strong overlap between the two fields. Consequently, you may encounter variants that are popular with statisticians as you dive deeper into the literature.\n\nI will mention some of the frequently used synonyms when a new term is introduced and then consistently use the variant that is most popular within the machine learning community and fits in best with the overall picture.]\n\nOften times, the target is known for some objects and unknown for others. For example, we might want to predict the unknown future sales figures for a product based, in part, on known historic sales figures.\n\nFeatures are attributes that are (potentially) related to the target.\n\nThe price of a product, for example, is related to the demand for the product. This relationship may be strong or weak depending on the circumstances[3], but price is certainly one of the first features that should come to mind.\n\nIn other communities, features are often referred to as independent variables, observed variables, explanatory variables and input variables.\n\nAn attribute value is the value of an attribute with regard to a particular object.\n\nIf target are the sales figures for the next quarter, an example of a target value could be 8,000 units.\n\nAnalogous to targets, we can use the term feature value to refer to the value of a specific feature with regard to a particular object. A value of the price feature in US dollars might be 99.99.\n\nMachine learning is based on the premise that there are relationships between features and targets that repeat in a predictable manner. Let\u2019s refer to these relationships as patterns.\n\nIf we were living in a world without patterns, there would be no use for machine learning and this tutorial would neither have been written nor read. Luckily, the universe that we find ourselves in is highly structured. The success of empirical science[4] is a testament to the idea that patterns exist and that they can be discovered.\n\nIn the not-so-distant past, Herculean efforts were made to engineer features for specific tasks. For example, there is a large literature on features that were developed for certain computer vision problems, such as the ability of robots to recognize objects in a room.[5]\n\nOver the last decade, progress in machine learning has made it possible to automate the search for features to some extent.[6]\n\nTo wrap up this article, let\u2019s use a concrete example to gain more insight into the relationship between features and targets.\n\nSuppose you\u2019ve just started to work on a system with the objective to automatically recognize animals in photos.\n\nThe photo above has the attribute of showing an orangutan mother and her infant, a fact that is obvious to us.\n\nTo the computer vision system, however, this is far from obvious. At the beginning of the development, it does not have any knowledge about orangutans, mothers or infants.\n\nBefore I describe what we can do to change this, I would like to ask you a question.\n\nHow do you solve this task? That is, how do you understand that the photo above shows two orangutans?\n\nHuman vision (and primate vision in general) is so efficient and occurs with so little conscious effort that it can take a while to come up with a precise and compelling answer.\n\nI would suggest you pause and think about it for a moment. In doing so, you might anticipate some of ideas described below and eventually realize that at least some forms of machine learning are more intuitive than they may seem."
    },
    {
        "url": "https://towardsdatascience.com/smells-like-machine-learning-progress-611a2851acec?source=user_profile---------7----------------",
        "title": "Smells Like Machine Learning Progress \u2013",
        "text": "Alongside the ratings, the data set provides 4,884 chemical features. Let\u2019s discuss one subset of features, the Moreau-Broto autocorrelations, using the example of methyl butyrate.\n\nThis particular molecule, which exhibits a pleasant fruity smell[5], consists of five carbon atoms, ten hydrogen atoms and two oxygen atoms. Leaving out hydrogen, the atoms are arranged in the following way:\n\nWe can think of this arrangement in terms of a molecular graph, with nodes representing atoms and edges corresponding to bonds between atoms. The number of edges/bonds between two atoms is referred to as the distance.\n\nEvery atom in our example has at least one neighbor of distance 1. For some pairs, such as the pair of carbon atom atom 2 and carbon atom 4, there is a path of distance 2. The maximum distance in the methyl butyrate molecule is 6.\n\nTo compute Moreau-Broto autocorrelations[6], we need two loops. Let i denote the index of the outer loop and let j be the index for the inner loop. The outer loop iterates from 1 to the index of the last atom minus d. For a given distance of d, the inner loop iterates from i+d to the last index. The one other ingredient we have to decide on is a property that we access for each atom. A common choice, and the one used in the example below, is atomic mass.\n\nFor each pair of atoms (i, j) connected by a path of distance d, we compute the mass of atom i times the mass of atom j. The Moreau-Broto autocorrelation for the atomic mass and a distance of d is the sum of these products.\n\nSuppose we set the distance d to 2. To obtain the feature value, we have to consider the following pairs: (1, 3), (2, 4), (3, 5) and (3, 6), (4, 7). Let w_k denote the mass of atom k. Then, the value of the feature is the result of the following computation:"
    },
    {
        "url": "https://medium.com/@aisummary/so-what-do-i-need-to-know-to-understand-this-bdafed13bcf4?source=user_profile---------8----------------",
        "title": "So, what do I need to know to understand this? \u2014 How machine learning can enhance human learning",
        "text": "According to one report, 9,400 MOOCs have been announced in 2016, up from 6,850 last year.\n\nSelecting a subset of courses is not enough. To make more out of the available opportunities, one has to decide on the right sequence of courses. And why stop at the level of courses? Sometimes, the best way to understand a concept mentioned in one course is to work through the material on a prerequisite other concept explained in a different course.\n\nIn an optimal sequence of educational material, later units build upon knowledge acquired in earlier units in a way that takes into account existing knowledge and maximizes the return on the time invested in learning.\n\nWhen course material requires conceptual knowledge that a student does not have, the experience will be frustrating. Material that barely expands upon existing knowledge, on the other hand, can be a waste of time.\n\nFiguring the right sequence can be overwhelming task and, in a way, requires the very knowledge that one is seeking to acquire. A knowledgeable guide may not always be available to tailor a curriculum to the interests and needs of an individual.\n\nThis is where machine learning comes in."
    },
    {
        "url": "https://towardsdatascience.com/the-fruits-of-deep-learning-how-convolutional-neural-networks-support-robotic-harvesting-and-yield-35e2d383e2d8?source=user_profile---------9----------------",
        "title": "The Fruits of Deep Learning: How Convolutional Neural Networks Support Robotic Harvesting and Yield\u2026",
        "text": "Since its publication, a number of new papers have built upon DeepFruits to solve similar and more challenging problems.\n\nOne notable example is a paper published by Bargoti & Underwood on fruit detection in orchards[1]. The researchers used a robotic vehicle during daylight hours to capture high-resolution whole tree images for three fruit types: apples, almonds and mangoes. In total, the data set contains more than 2,000 training images and almost 500 test images.\n\nThe authors emphasize the increased difficulty due to the high pixel count per fruit and the low fruit-count per image. Almond trees, for example, can host 1,000\u201310,000 almonds and are of a smaller size than the other fruit varieties.\n\nBoth DeepFruits and the system described in the Bargoti & Underwood paper use Faster R-CNN and treat fruit detection as a set of binary problems: one detector is trained for each fruit type. Another commonality is the application of non-maximum suppression (NMS), a procedure designed to handle overlapping regions. NMS eliminates regions associated with lower confidence that are strongly overlapping with high-confidence candidates, as measured by the Intersection over Union score.\n\nBargoti & Underwood experimented with different data augmentation techniques and found that the largest boost in performance is achieved by flipping and scaling the available images.\n\nVGG-16 requires 2.5 GB of GPU memory for 0.25 megapixels image. To overcome this bottleneck, the authors employ an approach that they refer to as \u201cTiled Faster R-CNN\u201d: detections are performed using a window of appropriate size that is sliding over the image. To ensure that fruits are not split across tiles, the overlap between two tiles is greater than the maximum fruit size. NMS is then applied over the combined output over all tiles.\n\nUsing this setup, the VGG variant of Faster R-CNN outperformed the shallower ZF variant and achieved F1 scores above 0.9 for apples and mangoes. For the smaller sized and frequently occurring almonds, the fruit detection result is reported to be close to 0.78. Interestingly, detection performance reaches 0.6 for apples with just five images and increases only by 0.01 for the last doubling of training images.\n\nFinally, the authors point out that the right choice of the Faster R-CNN depends on the task at hand. Computational efficiency, for example, is more important for robotic harvesting than it is for yield mapping which can be performed offline."
    },
    {
        "url": "https://medium.com/@aisummary/introducing-awesome-capsule-networks-897f1b81d1e3?source=user_profile---------10----------------",
        "title": "Introducing awesome-capsule-networks \u2013 Sebastian Kwiatkowski \u2013",
        "text": "The number of awesome resources related to capsule networks is growing steadily. I\u2019ve launched a curated list to compile the best ones.\n\nIf you are aware of additional resources, please pull a request or contact me. Your feedback and contributions are always welcome!"
    }
]