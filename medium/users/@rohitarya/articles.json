[
    {
        "url": "https://towardsdatascience.com/convolutional-neural-network-to-steer-a-vehicle-inside-a-game-2aab41a5ef60?source=user_profile---------1----------------",
        "title": "Convolutional Neural Network to steer a vehicle inside a game",
        "text": "Now coming to the labels, here is a histogram of steering wheel axis. The positive corresponds to the amount of steering wheel rotation for a right turn and vice-versa.\n\nThe whole data was shuffled and then split into the train (87%) and the validation set (13%). Model training with hyper-parameters tuning was done on the training set while validation set was used for checking the final accuracy of the model. Shuffling made sure that both train and validation set has data from all different driving conditions like daylight, night, raining, etc.\n\nThis is an important part of data preparation because cropping insures the learning algorithm only focuses on ROI, not the whole image and scaling down ensures the there are not too many input parameters to the network.\n\nMost parts of the images were not very useful and hence region of interest(ROI) was cropped and scaled down to achieve images of size 200 x 66 x 3.\n\nI got dataset for approx 160k images from here . The dataset was generated by taking the screenshot of the game window along with steering wheel axis captured by a joystick at 10fps by driving for ~ 5 hours. The steering wheel axis is not in degrees but in a different scale of the joystick which ranges from -10k (left turn) to 10k (right turn). The original image size is 1024 X 768 X 3 pixels. Here are some example images \u2014\n\nAny supervised machine learning problem works on labeled data (X, Y). Where X is the input and Y being the output. The learning algorithm learns the mapping function Y = f(X). In this problem, input X is the image of the road at any given point and output Y is the steering wheel axis.\n\nAlmost two years back, Nvidia published PilotNet paper demonstrating a convolutional neural network (CNN) for self-driving cars in an end-to-end approach. In this post, I want to present how I used that architecture to predict steering wheel axis for a vehicle inside a game. I have formulated steering angle prediction as a regression problem.\n\nThere are very few examples of sharp turns and most the data was between -2500 to 2500. Also as this distribution is normal, mean squared error (MSE) loss (which has been used for regression) should work well.\n\nAdditionally, data was augmented by flipping the images horizontally and hence flipping the steering axis as well.\n\nThis almost doubled the training data.\n\nI picked the famous PilotNet architecture by Nvidia with some modifications. Here is the original architecture\n\nI additionally added Batch Normalization(BN) (on the channel axis for convolutional layers) after each layer for a faster training. I also tried using Dropout for regularisation but that didn\u2019t seem to affect the accuracy hence removed it. There has been a lot of debate on when to apply BN which is either before the non-linearity or after the non-linearity. Since I used Relu activation at each layer, applying BN after the Relu activation was increasing the mean and reducing the variance of the hidden layer activation because BN was not considering the negative activations. Hence, I applied BN after the non-linearity, which worked out fine for me.\n\nI scaled down the input image to 200 x 66 (same as PilotNet) to keep parameters of the fully connected layers low (parameters of the convolutional layers are not affected). This was important to avoid overfitting. Models with very high params have a high entropy and they tend to overfit (i.e. memorize the training set). With low entropy, gradient descent algorithm forces the model to learn important patterns in the data instead of memorizing the training set. While having very low params is also bad as the model may not learn anything.\n\nThe increase in parameters could have been avoided by using max-pooling but it is generally used for spatial invariance which is not desired in this case.\n\nThe input to the model was normalized by dividing the input pixels by 255. There are better ways to normalize input images using the mean and variance of the whole training set but this also works fine. I used mean squared error loss without any regularization. I came up with all these design choices after testing these parameters on the validation set. The rest of the network parameters remain unchanged.\n\nThis the final architecture which I used with .\n\nI used Keras with Tensorflow backend for all the experiment as well as final training.\n\nThe dataset had a very high correlation between adjacent images and hence it was important to shuffle for training. Along with that, after every epoch, the data set was re-shuffled so that every batch is unique across multiple epochs.\n\nSince the keras doesn\u2019t have for regression task, I had to write my own data_generator with data augmentation.\n\nDeciding what minibatch size to use was also tricky. If we use a very small batch size, the computed gradients might be very inaccurate hence the training will be noisy. If you pick a very large batch size, it may not fit in memory. I chose to use 128 as minibatch size.\n\nI used stochastic gradient descent optimizer with momentum and learning rate decay to train the network.\n\nThe model was trained for 41 epochs on CPU for ~ 30 hours to achieve validation mean squared error of 0.1166 and validation mean absolute error of 0.2429 (at 36th epoch) which corresponds to a mean error of 160 (=0.2429 x OUTPUT_NORMALIZATION) on the scale of 20k."
    },
    {
        "url": "https://medium.freecodecamp.org/how-we-reduced-memory-footprint-by-50-in-our-android-app-49efa5c93ad8?source=user_profile---------2----------------",
        "title": "How we reduced our Android app\u2019s memory footprint by 50%",
        "text": "How we reduced our Android app\u2019s memory footprint by 50%\n\nLike any other startup momentum-obsessed startup, we didn\u2019t spend a lot of time to building an efficient product on the first go. We shipped our Android app, and it was working \u201cjust fine.\u201d\n\nAs we started scaling up in terms of our offering to the customers, our app became bulky \u2014 with tons of images \u2014 and we started to see performance issues. Our app became slow and froze on low-end devices. Battery consumption also increased.\n\nTo debug this issue, we used a memory monitoring tool provided by Android Studio. As we scrolled through a very long list of product images, this is what we observed:\n\nTo explain these graphs a little better:\n\nJust by opening the product list page, the app consumed 15 megabytes of memory. If we scrolled all the way to the bottom of the product list page, the app consumed 50 megabytes of memory, with a lot of GC events.\n\nWhen scrolling through other product lists we observed similar patterns. Here are the graphs:\n\nAgain we observed this pattern. By this time Android had allocated maximum memory (for bitmaps) \u2014 which it could have allocated to our app by killing other apps\u2019 processes in the background \u2014 and the net memory allocation has hit 57 megabytes along with several GC events.\n\nThese graphs are from the Android runtime. Dalvik behaves even worse in terms of memory management and Garbage Collection.\n\nIn Android, bitmaps represent the largest contiguous blocks of memory. They occupy heaps, which results in lots of contention to find free space to allocate new bitmaps as we scroll. This then results in more GC events so it can free up memory to provide the necessary space. Since there were so many images getting loaded in the list, these GC events were degrading the performance of our app.\n\nGC events causes the application to freeze until it finishes. A few of these won\u2019t matter, but too many of these events will result in a lower frame rate. Running a Garbage Collector also causes more CPU, which consumes battery usage.\n\nAlso, the higher the memory usage of an app, the more likely the system will decide to kill it when it\u2019s running in the background.\n\nWe had to solve this problem before we could move any further with product development. And for that, we moved to an object pool concept for bitmaps, as advised by Colt in this video:\n\nSo the idea is instead of creating a brand new bitmap, you use an existing piece of memory to load the bitmap into:\n\nThen when you scroll through a long list of images, there\u2019s no need to load all the images into separate memory allocations. You can just allocate a maximum number of bitmaps that are going to be visible, then reuse their memory \u2014 thereby avoiding those horrific GC events.\n\nHere are the results showing the improvements:\n\nWe scrolled through the same product list again after doing the changes. We observed that with an initial 15 megabytes of memory already allocated, there was only 27 megabytes of total memory allocation done by the time we\u2019d scrolled to the bottom of the page (and with a very few GC events).\n\nWe scrolled a few more product lists and we observed no extra memory allocations (Bitmap Pool magic) and therefore no major GC events.\n\nIn the end, we had successfully reduced bitmap memory footprint by almost 50%.\n\nWe need to be mindful of the fact that Android has some constraint for reusing bitmaps, with respect to the physical size of the existing bitmaps:\n\nWe also try to use same pixel format for reusing bitmaps. So to load an image as an bitmap, we use the bitmap allocation.\n\nThe good news is you don\u2019t have to do all this on your own. There are already some amazing libraries like Glide and Fresco which have built-in capabilities for reusing bitmap memory. All you have to do is make sure your bitmaps can be reused. (Remember that there are constraints with respect to bitmap dimension and .) If you do not want to use these, you can just plug a Bitmap Pool into your existing image loader. Using these libraries will also help you save precious memory by pre-scaling bitmaps, and many more things.\n\nAlong with this, we also started using the format, which takes only 16 bits per pixel \u2014 as compared to which takes 32bits per pixel \u2014 on low memory devices. This further reduces our memory footprint.\n\nThere are many awesome things that you can do to improve your app\u2019s performance. Will keep posting them. Let\u2019s build better apps."
    },
    {
        "url": "https://medium.freecodecamp.org/face-centering-android-library-build-on-top-of-google-vision-api-f88661b97959?source=user_profile---------3----------------",
        "title": "Face centering Android library build on top of Google Vision API",
        "text": "In our Android apps, when we crop photos to display them, we often encounter the problem of positioning faces properly.\n\nThis inspired me to create a tool that will locate faces and in an image (if there are any) and center the cropped image around them.\n\nHere\u2019s how I did it.\n\nI started with Face Detection API of Google\u2019s mobile vision. This API provides:\n\nSince I just wanted the position of the face, I only used the face detection component. To start with that, I created the face detector:\n\nNow given a bitmap, I created a frame instance from the bitmap to supply to the detector:\n\nNow, I tried to detect faces synchronously in the frame:\n\nOnce I got faces, I chose one face (for now) to crop the image around, keeping that face in the center.\n\nNow to begin cropping the image:\n\nYou can find the full code in my GitHub repositories below.\n\nHere are some results of it:\n\nI finally exported this as a library, which you can find below.\n\nI am planning to release it for Fresco too.\n\nFeel free to make use of this tool, and help me improve it over on GitHub."
    },
    {
        "url": "https://medium.com/jumbotail-technology/faster-and-smaller-serialized-data-in-android-7c68d32e15fc?source=user_profile---------4----------------",
        "title": "Faster and smaller serialized data in Android \u2013 Jumbotail Technology \u2013",
        "text": "Serialization is a process of converting an in-memory object into a formatted chunk of data which can be converted back into the in-memory object.\n\nEasiest solution is to implement Serializable interface and it pretty much solves our problem.\n\nBut this solution is not memory efficient (as it creates transient memory allocations) and is slow for Android. We can use Gson library to make this memory efficient and faster.\n\nBut since Gson uses JSON format, it produces bloated file. Formats like JSON and XML tend to decode slower and produce verbose result. Of-course these formats are good as they are human readable and changes can be easily made into them, but they cost extra data to be sent to the user. Good news is that, we can use binary serializers like FlatBuffers to get rid of these problems.\n\nFlatBuffers (relatively similar to Protocol Buffers) is an efficient cross platform serialization library for C++, C#, C, Go, Java, JavaScript, PHP, and Python. It was originally created at Google for game development and other performance-critical applications.\n\nWe can go beyond this to make serialized data even smaller.\n\nWe store data as Objects and thus serializing an array of those objects preserves their structures in the data stream. This is called Array-Of-Structures.\n\nConsidering above Person class, this is how an array of Person\u2019s object will look like in serialized stream:\n\nYou can notice here that for every object, value is against its full property name. Thus creating bloated stream. Now you may say that, we can apply GZIP compression on top of that to make a smaller stream. But GZIP compresses file by finding duplicate data as long as they are in a window of 32k characters from each other. Thus larger serialized classes result in large distance between similar data types, which will result in less duplication in the 32k characters window.\n\nTo curb this problem, what we can do is to use Structures-of-Array form. Which means, given an array of objects, take each property of the class and make an array of the property values. Above persons\u2019 array would look like this:\n\nThis is not object oriented, but we have effectively removed bloating from the serialized stream. Along with that, GZIP compression algorithm has a better chance of finding duplicates in 32k characters window as similar data types are close.\n\nThis also reduces raw object count (and thus memory overhead) thereby increasing data locality and makes better use of precious memory bandwidth and CPU cache space. (I will discuss memory performance in Android in some other blog.)\n\nThese are few techniques for making serialized stream very small. Once you have transposed your data in Struct-of-Array form, you can achieve better compression and faster serialization by adopting FlatBuffers. Thus applying different layers of awesomeness on top of each other can make the smoothest app possible."
    },
    {
        "url": "https://medium.com/@rohitarya/changing-the-networking-layer-of-roposo-android-app-5ab7e0da3121?source=user_profile---------5----------------",
        "title": "Changing the networking layer of Roposo Android app",
        "text": "First thing after joining Roposo I did was talking about app performance. We already had about 2 million users and lots of features were being released every month. And we were not putting any eye on improving speed, reducing the memory footprint, and optimizing battery usage. I started by rewriting the networking layer of the app.\n\nWhat was being done in the app for networking till now:\n\nWell, it sounds like what anybody would start with.\n\nNow as Apache HTTP Client deprecated in Android 6.0, we are sort of forced stop using that. And I chose OkHttp as the HTTP Client. We had also implemented GZIP compression for all the HTTP requests and responses on our own but since OkHttp handles that on its own, we got rid of that part. So the second task was replacing AsyncTask because they are not very good with memory footprints and if they are not handled carefully, cause memory leaks. I wanted to use Retrofit, an async client for Android, but since Async task was being used everywhere with Apache JSON (as Retrofit requires Json convertor and Apache JSON was not supported). I stuck to use OkHttp\u2019s synchronous calls with AsyncTask for few weeks which I later on replaced with OkHttp\u2019s asynchronous calls.\n\nOkHttp also provides support for HTTP/2.0 which enables socket sharing for all connections to the same host and also midway request cancelling to save bandwidth and battery. We haven\u2019t yet enabled HTTP/2.0 on our servers but are moving towards it.\n\nThat\u2019s all I have done till now. Will update the blog as we proceed further. What I have planned to do is:"
    }
]