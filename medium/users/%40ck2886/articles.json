[
    {
        "url": "https://medium.com/@ck2886/word2vec-96ecf6335192?source=user_profile---------1----------------",
        "title": "Word2Vec \u2013 Chaitanya Kulkarni \u2013",
        "text": "Imagine we have 50,000 words and we are feeding them one at a time. The traditional technique would be to one hot encode every word and feed it into your algorithm. But this would mean that 49,999 elements in the vector are just zero, this is very inefficient from a computational point of view. Therefore we need embeddings.\n\nOne more thing when we are working with words is that we want our algorithm to understand the relationship between words, i.e. we want the algorithm to understand that red, blue and green are colors or at least understand that they mean something very close or they have some sort of a relationship with each other.\n\nThe solution to both of the above problems is something called as word 2 vec embedding. The concept is based on the fact that similar words appear in similar context. For example\n\nIn both the above sentence red and blue are describing the color of the box(is the context).\n\nNext Question would be how would you implement this idea. There are 2 methods:\n\nAs shown in figure, in cbow we take the context as input and try to predict the desired word as the output.\n\nIn skip-gram we take the word as an input but we try to predict the context as output.\n\nIn this above cases we are interested in what values does each word get. These values are obtained from the projection layer(as shown in the diagram above). Below diagram shows the output of a skip-gram I trained. In this we can observe that words with similar meaning appear close to each other(like man and woman, left and right).\n\nIt is observed that cbow trains faster while skip gram is better at more accurately creating the vector space.\n\nYou can find my implementation of Word2Vec(skip-gram) here: https://github.com/ck2886/Word2Vec_SkipGram/blob/master/Skip-Gram_word2vec.ipynb"
    },
    {
        "url": "https://medium.com/@ck2886/learning-rate-tuning-and-optimizing-d03e042d0500?source=user_profile---------2----------------",
        "title": "Learning Rate Tuning and Optimizing \u2013 Chaitanya Kulkarni \u2013",
        "text": "Learning rate is one of the most important hyper parameters to select while you are trying to optimize your Neural Network.\n\nOnce we initialize our neural network and calculate the error, next we calculate partial derivative of the error with respect to input(we use the chain rule to calculate it so we can update our weights at every layer). This tells us in which direction are we suppose to update our weights in order to reduce the error for the network. The quantity by which we need to update our weights is decided by a hyper parameter we set known as Learning Rate.\n\nThe quantity by which we are suppose to move is decided by multiplying the the value we obtained by the partial derivative by the learning rate.\n\nConsider the diagram below, out goal is to reach the lowest point of the curve. If we set a very high value for the learning rate we overshoot otherwise if the learning rate is too slow we end up taking an extremely long time to reach the minima. By keeping the learning rate very small we have another issue of the the value getting stuck in local minima as shown the figure below.\n\nTherefore if we choose the wrong learning rate we might never reach the global minima which leads to bad performance of the network.\n\nThe advantages of having the a high learning rate is that we reach close to the global minima very soon, and the advantage of having a small learning rate is we can reach the most optimal solution for the given dataset.\n\nOne efficient solution to solve this sort of problem is by using something known as learning rate decay. In this process what we do is we have a high learning rate at the beginning of our training so that we can get very close to the global minima very soon. And as the training time progresses we continuously decrease the learning rate after a certain number of epochs.\n\nBy doing this we are able to have the advantages of both high and low learning rate i.e. faster and accurate convergence on a given dataset."
    },
    {
        "url": "https://medium.com/@ck2886/lstm-explained-633d89384004?source=user_profile---------3----------------",
        "title": "LSTM Explained \u2013 Chaitanya Kulkarni \u2013",
        "text": "LSTMs are improvements over the traditional RNNs. These have the capability to remember important events from the past. One major issue we were facing in RNNs is that we were not able to remember long history this is because we were trying to remember everything from the past by brute force(that is we were trying to back propagate through time and trying to update the weights. which lead to the gradient value becoming very small after BPTT for a few cells. This is called the Vanishing Gradient Problem).\n\nLSTMs were the solutions to this problem.\n\nLSTMs solve this by remembering 2 memories. One is the short term memory and the second is the long term memory.\n\nTherefore an LSTM has 3 inputs and 3 outputs as shown in the diagram.\n\nThe LSTM Cell has mainly 4 gates.\n\nThe Learn Gate: This takes the short term memory and Input to the cell as inputs and learns new information and updates its weights\n\nThe Forget Gate: This takes the long term memory as an input and multiplies it by a forget factor and outputs the new updated Long term memory.\n\nThe Remember Gate: This gate tries to understand what to remember after the new event. This is done by adding the outputs of Learn Gate and Forget Gate. The output of the remember gate is the new Long term Memory\n\nThe Use Gate: This gate tries to learn the new short term memory by combining the long term memory information and the short term memory information to give the new short term memory.\n\nThis is basically the high level idea of an LSTM."
    },
    {
        "url": "https://medium.com/@ck2886/transfer-learning-for-tensorflow-ef06afd70384?source=user_profile---------4----------------",
        "title": "Transfer Learning for Tensorflow \u2013 Chaitanya Kulkarni \u2013",
        "text": "Transfer learning or inductive transfer is a research problem in machine Learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.\n\nIt is especially useful when we have a small dataset. In this article we will discuss on how to retrain only the last layers of the network using tensorflow.\n\nTransfer Learning is extensively useful while dealing with Convolutional Neural Networks where once completely trained Convolutional layers become very good at identifying patterns. Thus by retraining the classifiers i.e. only the final layers, we will be able to save a lot of time when training.\n\nNext Question to answer is from which networks do I need to retrain the models from. The networks trained for imagenet competition would be a good start. You can find more information of Imagenet Networks in this following amazing link:https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\n\nOnce you decided on what network you want to use for transfer learning then the question arises is to how to do it in Tensorflow.\n\nThe method I found to be useful is by downloading the weights and the network from github repositories and passing your current images / data through the pre-trained networks and saving the outputs from the desired layers. Then we need to build a new layer for classification and pass the output saved from the pre-trained model and retrain only the final nano layers as shown in the diagram using a suitable optimizer.\n\nYou can find link to one of the mini projects I did for my Udacity Nanodegree. here:https://github.com/ck2886/TransferLearning_Tensoflow/blob/master/Transfer_Learning.ipynb"
    }
]