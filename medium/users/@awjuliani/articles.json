[
    {
        "url": "https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565?source=user_profile---------1----------------",
        "title": "Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning",
        "text": "Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning What goes into a stable, accurate reinforcement signal?\n\nSince the launch of the ML-Agents platform a few months ago, I have been surprised and delighted to find that thanks to it and other tools like OpenAI Gym, a new, wider audience of individuals are building Reinforcement Learning (RL) environments, and using them to train state-of-the-art models. The ability to work with these algorithms, previously something reserved for ML PhDs, is opening up to a wider world. As a result, I have had the unique opportunity to not just write about applying RL to existing problems, but also to help developers and researchers debug their models in a more active way. In doing so, I often get questions which come down to a matter of understanding the unique hyperparameters and learning process around the RL paradigm. In this article, I want to attempt to highlight one of these conceptual pieces: bias and variance in RL, and attempt to demystify it to some extent. My hope is that in doing so a greater number of people will be able to debug their agent\u2019s learning process with greater confidence.\n\nMany machine learning practitioners are familiar with the traditional bias-variance trade-off. For those who aren\u2019t, it goes as follows: on the one hand, a \u201cbiased\u201d model generalizes well, but doesn\u2019t fit the data perfectly (\u201cunder-fitting\u201d). On the other hand, a high-variance model fits the training data well, too well in-fact, to the detriment of generalization (\u201coverfitting\u201d). In this situation, the problem becomes one of limiting the capacity of a model with some regularization method. In many cases, dropout or L2 regularization with a large enough data set is enough to do the trick. That is the story for typical supervised learning. RL is a little different, as it has its own separate bias-variance trade-off which operates in addition to, and at a higher level than the typical ML one.\n\nIn RL, bias and variance no longer just refer to how well the model fits the training data, as in supervised learning, but also to how well the reinforcement signal reflects the true reward structure of the environment. To understand that statement, we have to backup a little. In reinforcement learning, instead of a set of labeled training examples to derive a signal from, an agent receives a reward at every decision-point in an environment. The goal of an agent is to learn a policy (method for taking actions) which will lead to obtaining the greatest reward over time. We must do this using only the individual rewards that agent receives, without the help of an outside oracle to designate what count as \u201cgood\u201d or \u201cbad\u201d actions.\n\nA naive approach to an RL learning algorithm would be to encourage actions which were associated with positive rewards, and discourage actions associated with negative rewards. Instead of updating our agent\u2019s policy based on immediate rewards though, we often want to account for actions (and the states of the environment when those actions were taken) which lead up to rewards. For example, imagine walking down a corridor to a rewarding object. It isn\u2019t just the final step we want to perform again, but all the steps up to that rewarding one. There are a number of approaches for doing this, all of which involving doing a form of credit assignment. This means giving some credit to the series of actions which led to a positive reward, not just the most recent action. This credit assignment is often referred to as learning a value estimate: V(s) for state, and Q(s, a) for state-action pair.\n\nWe control how rewarding past actions and states are considered to be by using a discount factor (\u03b3, ranging from 0 to 1). Large values of \u03b3 lead to assigning credit to states and actions far into the past, while a small value leads to only assigning credit to more recent states and actions. In the case of RL, variance now refers to a noisy, but on average accurate value estimate, whereas bias refers to a stable, but inaccurate value estimate. To make this more concrete, imagine a game of darts. A high-bias player is one who always hits close to the target, but is always consistently off in some direction. A high-variance player, on the other hand, is one who sometimes hits the target, and is sometimes off, but on average near the target.\n\nThere is a multitude of ways of assigning credit, given an agent\u2019s trajectory through an environment, each with different amounts of variance or bias. Monte-Carlo sampling of action trajectories as well as Temporal-Difference learning are two classic algorithms used for value estimation, and both are prototypical examples of methods which are variance and bias heavy, respectively.\n\nIn Monte-Carlo (MC) sampling, we rely on full trajectories of an agent acting within an episode of the environment to compute the reinforcement signal. Given a trajectory, we produce a value estimate R(s, a) for each step in the path by calculating a discounted sum of future rewards for each step in the trajectory. The problem is that the policies we are learning (and often the environments we are learning in) are stochastic, which means there is a certain level of noise to account for. This stochasticity leads to variance in the rewards received in any given trajectory. Imagine again the example with the reward at the end of the corridor. Given that an agent\u2019s policy might be stochastic, it could be the case that in some trajectories the agent is able to walk to the rewarding state at the end, and in other trajectories it fails to do so. These two kinds of trajectories would provide very different value estimates, with the former suggesting the end of the corridor is valuable, and the latter suggesting it isn\u2019t. This variance is typically mitigated by using a large number of action trajectories, with the hope that the variance introduced in any one trajectory will be reduced in aggregate, and provide an estimate of the \u201ctrue\u201d reward structure of the environment.\n\nOn the other end of the spectrum is one-step Temporal Difference (TD) learning. In this approach, the reward signal for each step in a trajectory is composed of the immediate reward plus a learned estimate of the value at the next step. By relying on a value estimate rather than a Monte-Carlo rollout there is much less stochasticity in the reward signal, since our value estimate is relatively stable over time. The problem is that the signal is now biased, due to the fact that our estimate is never completely accurate. In our corridor example, we might have some estimate of the value of the end of the corridor, but it may suggest that the corridor is less valuable than it actually is, since our estimate may not be able to distinguish between it and other similar unrewarding corridors. Furthermore, in the case of Deep Reinforcement Learning, the value estimate is often modeled using a deep neural network, making things worse. In Deep Q-Networks for example, the Q-estimates (value estimates over actions) are computed using an old copy of the network (a \u201ctarget\u201d network), which will provide \u201colder\u201d Q-estimates, with a very specific kind of bias, relating to the belief of an outdated model.\n\nNow that we understand bias and variance and their causes, how do we address them? There are a number of approaches which attempt to mitigate the negative effect of too much bias or too much variance in the reward signal. I am going to highlight a few of the most commonly used approaches in modern systems such as Proximal Policy Optimization (PPO), Asynchronous Advantage Actor-Critic (A3C), Trust Region Policy Optimization (TRPO), and others.\n\nOne of the most common approaches to reducing the variance of an estimate is to employ a baseline which is subtracted from the reward signal to produce a more stable value. Many of the baselines chosen fall into the category of Advantage-based Actor-Critic methods, which utilize both an actor which defines the policy, and a critic (often a parameterized value estimate) which provides a more reduced variance reward signal to update the actor. The thinking goes that variance can simply be subtracted out from a Monte-Carlo sample (R/Q) using a more stable learned value function V(s) in the critic. This value function is typically a neural network, and can be learned using either Monte-Carlo sampling, or Temporal difference (TD) learning. The resulting Advantage A(s, a) is then the difference between the two estimates. This advantage estimate has the other nice property of corresponding to how much better the agent actually performed than was expected on average, thus allowing for intuitively interpretable values.\n\nWe can also arrive at advantage functions in other ways than employing a simple baseline. For example, the value function can be applied to directly smooth the reinforcement signal obtained from a series of trajectories. The Generalized Advantage Estimate (GAE), introduced by John Schulman in 2016 does just this. The GAE formulation allows for an interpolation between pure TD learning and pure Monte-Carlo sampling using a lambda parameter. By setting lambda to 0, the algorithm reduces to TD learning, while setting it to 1 produces Monte-Carlo sampling. Values in-between (particularly those in the 0.9 to 0.999 range) produce better empirical performance by trading off the bias of V(s) with the variance of the trajectory.\n\nOutside of calculating an advantage function, the bias-variance trade-off presents itself when deciding what to do at the end of a trajectory when learning. Instead of waiting for an entire episode to complete before collecting a trajectory of experience, modern RL algorithms often break experience batches down into smaller sub-trajectories, and use a value-estimate to bootstrap the Monte-Carlo signal when that trajectory doesn\u2019t end with the termination of the episode. By using a bootstrap signal, that estimate can contain information about the rewards the agent might have gotten, if it continued going to the end of the episode. It is essentially a guess about how the episode will turn out from that point onward. Take again our example of the corridor. If we are using a time horizon for our trajectories that ends halfway through the corridor, and if our value estimate reflects the fact that there is a rewarding state at the end, we will be able to assign value to the early part of the corridor, even though the agent didn\u2019t experience the reward. As one might expect, the longer the trajectory length we use, the less frequently value estimates are used for bootstrapping, and thus the greater the variance (and lower the bias). In contrast, using short trajectories means relying more on the value estimate, creating a more biased reinforcement signal. By deciding how long the trajectory needs to be before cutting it off and bootstrapping it, we can propagate the reward signal in a more efficient way, but only if we get the balance right.\n\nSay you have some environment you\u2019d like to have an agent learn to perform a task within (for example, an environment made using Unity ML-Agents). How do you decide how to control the GAE lambda and/or trajectory time horizon? The outcome of setting these hyperparameters in various ways often depends on the task, and come down to a couple of factors:\n\nUltimately, correctly balancing the trade-off comes down to a few things: gaining an intuition for the kind of problem under consideration, and knowing what hyperparameters for any given algorithm correspond to what changes in the learning process. In the case of an algorithm like PPO, this corresponds to the discount factor, GAE lambda, and bootstrapping time horizon. Below are a few guidelines which may be helpful:\n\nWith all the tweaking and tuning that often goes into the process, it can sometimes feel overwhelming, and like black magic, but hopefully, the information presented above can help contribute, even in a small way, to ensure that Deep Reinforcement Learning is a little more interpretable to those practicing it.\n\nIf you have questions about the bias-variance trade-off in RL, or if you are an RL researcher and have additional insight (or corrections) to share, please feel free to comment below!\n\nThanks to Marwan 'Moe' Mattar for the helpful feedback when reviewing a draft of this post."
    },
    {
        "url": "https://medium.com/beyond-intelligence/reinforcement-learning-or-evolutionary-strategies-nature-has-a-solution-both-8bc80db539b3?source=user_profile---------2----------------",
        "title": "Reinforcement Learning or Evolutionary Strategies? Nature has a solution: Both.",
        "text": "A few weeks ago OpenAI made a splash in the Deep Learning community with the release of their paper \u201cEvolution Strategies as a Scalable Alternative to Reinforcement Learning.\u201d The work contains impressive results suggesting that looking elsewhere than Reinforcement Learning (RL) methods may be worthwhile when training complex neural networks. It sparked a debate around the importance of Reinforcement Learning, and perhaps it\u2019s less than necessary status as the go-to technique for learning to solve tasks. What I want to argue here is that instead of being seen as two competing strategies, one of which being necessarily better than the other, they are ultimately complementary. Indeed, if we think a little bit forward to the goal of Artificial General Intelligence (AGI), and systems that can truly perform lifelong learning, reasoning, and planning, what we find is that a combined solution is almost certainly going to be necessary. And indeed, it is just this solution that nature arrived at for endowing mammals and other complex animal life with intelligence.\n\nThe basic premise of the OpenAI paper was that instead of using Reinforcement Learning coupled with traditional gradient backpropagation, they successfully trained neural networks to perform difficult tasks using what they called Evolutionary Strategy (ES). This ES approach consists of maintaining a distribution over network weight values, and having a large number of agents act in parallel using parameters sampled from this distribution. Each agent acts in its own environment, and once it finishes a set number of episodes, or steps of an episode, cumulative reward is returned to the algorithm as a fitness score. With this score, the parameter distribution can be moved toward that of the more successful agents, and away from that of the unsuccessful ones. By repeating this approach millions of times, with hundreds of agents, the weight distribution moves to a space that provides the agents with a good policy for solving the task at hand. Indeed, the most impressive result from the paper shows that with a thousand workers in parallel, humanoid walking can be learned in under half an hour (something that it takes even the best traditional RL methods hours to solve). For more insight, I suggest reading their great blog post, as well as the paper itself.\n\nThe great benefit of this approach is that it is parallelizable with little effort. Whereas RL methods such as A3C need to communicate gradients back and forth between workers and a parameter server, ES only requires fitness scores and high-level parameter distribution information to be communicated. It is this simplicity that allows the technique to scale up in ways current RL methods cannot. All of this comes at a cost however: the cost of treating the network being optimized as a black-box. By black-box, I mean that the inner workings of the network are ignored in the training process, and only the overall outcome (episode-level reward) is utilized in determining whether to propagate the specific network weights to further generations. In situations where we don\u2019t get a strong feedback signal from the environment, such as many traditionally posed RL problems with only sparse rewards, turning the problem from a \u201cmostly black box\u201d into an \u201centirely black box\u201d is worth the performance improvements that can come along with it. \u201cWho needs gradients when they are hopelessly noisy anyways?\u201d So the thinking goes.\n\nIn situations with richer feedback signals however, things don\u2019t go so well for ES. OpenAI describes training a simple MNIST classification network using ES, and it being \u201c1000 times slower.\u201d This comes from the fact that the gradient signal in image classification is extremely informative in regards to how to improve the network to better classify. The problem then is less to do with Reinforcement Learning, and more to do with sparse rewards in the environment providing noisy gradients.\n\nAppealing to nature for inspiration in AI can sometimes be seen as a problematic approach. Nature, after all, is working under constraints that computer scientists simply don\u2019t have. It is often believed that a purely theoretical approach to a given problem can provide more efficient solutions than empirical approaches. Despite this, I think it can still be be worthwhile to examine how a dynamic system under certain constraints (earth) can arrive at agents (animals, specifically mammals) with flexible and complex behavior. While some of those constraints don\u2019t hold in simulated worlds of data, many of them still do.\n\nIf we look at intelligent behavior in mammals, we find that it comes from a complex interplay of two ultimately intertwined processes, inter-life learning, and intra-life learning. The first is what is typically thought of as evolution via natural selection, but I use a broader term to include things like epigenetics, microbiomes, etc, which are passed between animals not directly related to their genetic material per-se. The second, intra-life learning is all of the learning that takes place during the lifetime of an animal, and is conditioned explicitly on that animal\u2019s interaction in the world. This is also referred to as experience-dependent learning. This category includes everything from learning to recognize objects visually to learning to communicate with learning, to learning that Napoleon was coronated on Sunday, December 2, 1804.\n\nRoughly speaking these two approaches in nature can be compared to the two in neural network optimization. Evolutionary Strategies, for which no gradient information is used to update the organism, is related to inter-life learning. Likewise, the gradient based methods, for which specific experiences change the agent in specific ways, can be compared to intra-life learning. If we think about the kinds of intelligent behaviors or capacities that each of these two approaches enable in animals, we find that the comparison becomes more intelligible. In both cases, the \u201cevolutionary methods\u201d enable the learning of reactive behaviors that achieve a certain level of fitness (enough to stay alive). Learning to walk or play breakout are in many ways equivalent to more \u201cinstinctual\u201d behaviors that come genetically hard-wired for many animals. It is also the case that evolutionary methods allow for dealing with extremely sparse reward signals such as the successful rearing of offspring. In a case like that, it is impossible to assign credit for successful childbearing to any specific set of actions that may have taken place years earlier. On the other hand, if we look at the failure case of ES, image classification, we find something remarkably comparable to the kind of learning which animals have accomplished in countless behavioral psychology experiments over the past 100 plus years.\n\nThe techniques employed in Reinforcement Learning are in many ways inspired directly by the psychological literature on operant conditioning to come out of animal psychology. (In fact, Richard Sutton, one of the two founders of Reinforcement Learning actually received his Bachelor\u2019s degree in Psychology). In operant conditioning animals learn to associate rewarding or punishing outcomes with specific behavior patterns. Animal trainers and researchers can manipulate this reward association in order to get animals to demonstrate their intelligence or behave in certain ways. Operant conditioning applied in animal research however is nothing more than a more explicit form of the conditioning which guides learning for all mammals during their lifetimes. We are constantly receiving reward information from the environment and adjusting our behaviors accordingly. Indeed, many neuroscientists and cognitive scientists believe that humans and other mammals go a step further and constantly learn to predict the outcomes of their behaviors on future rewards and situations.\n\nThe central role of prediction in intra-life learning changes the dynamics quite a bit. What was before a somewhat sparse signal (occasional reward), becomes an extremely dense signal. The theory goes something like this: at each moment mammalian brains are predicting the results of the complex flux of sensory stimuli and actions which the animal is immersed in. The outcome of the animals behavior then provides a dense signal to guide the change in predictions and behavior going forward. All of these signals are put to use in the brain in order to improve predictions (and consequently the quality of actions) going forward. For an overview of this approach, see the excellent \u201cSurfing Uncertainty\u201d by Cognitive Scientist and Philosopher Andy Clark. If we apply this way of thinking to learning in artificial agents, we find that RL isn\u2019t somehow fundamentally flawed, rather it is that the signal being used isn\u2019t nearly as rich as it could (or should) be. In cases where the signal can\u2019t be made more rich, (perhaps because it is inherently sparse, or to do with low-level reactivity) it is likely the case that learning through a highly parallelizable method such as ES is instead best.\n\nTaking insight from the constantly-predicting neural systems in mammalian brains, there have been some advances in Reinforcement Learning in the past year which incorporate the role of prediction. Two in particular that come to mind are:\n\nIn both of these papers the authors augment their network\u2019s typical policy outputs with predictions regarding the future state of the environment. In the case of \u201cLearning to Act\u201d this is in regard to a set of measurements variables, and in the case of \u201cUnsupervised Auxiliary Tasks\u201d this is in regard to predicting changes in the environment and the agent itself. In both cases the sparse reward signal becomes a much richer and informative signal capable of enabling both quicker learning as well as learning of more complex behaviors. These kinds of enhancements are only available to methods which utilize a gradient signal, as opposed to black-box methods like ES.\n\nIntra-life learning / gradient-based methods are also much more efficient. Even in the cases where the ES method learned a task much quicker than the RL method, it did so at the cost of using multiple times more data to do so. When thinking about animal-level learning, inter-life learning requires generations of life to produce changes, whereas a single event can change the behavior of an animal when it takes place through intra-life learning. While this kind of one-shot learning is not entirely yet within the grasp of traditional gradient-based methods, it is much closer than ES. For example, approaches like Neural Episodic Control, which store Q-values during learning and query them when taking actions allows a gradient-based method to learn to solve tasks much quicker than before. In that paper the authors point to the human hippocampus, which is able to store events even after a single exposure, and consequently plays a critical role in their later recall. Such mechanisms require access to the internals of the agent, and as such are also impossible for ES.\n\nWhile much of this article perhaps sounds like a championing of RL methods, I ultimately think that the best solution in the long-term will involve a combination of both methods, each used for what they are best at. It is clear that for many reactive policies, or situations with extremely sparse rewards, ES is a strong candidate, especially if you have access to the computational resources that allow for massively parallel training. On the other hand, gradient-based methods using RL or supervision are going to be useful when a rich feedback signal is available, and we need to learn quickly with less data.\n\nIf we look to nature, we find that the former actually enables the latter. That is to say that through evolution mammals arrived with brains capable of learning from the complex signals of the world around them in extremely efficient ways. Who knows? It may end up being the case that evolutionary methods help us arrive at efficient learning architectures for gradient-based learning systems as well. Perhaps nature\u2019s solution isn\u2019t so inefficient after all\u2026"
    },
    {
        "url": "https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl%C2%B2-in-tensorflow-b15b592a2ddf?source=user_profile---------3----------------",
        "title": "Learning Policies For Learning Policies \u2014 Meta Reinforcement Learning (RL\u00b2) in Tensorflow",
        "text": "Reinforcement Learning provides a framework for training agents to solve problems in the world. One of the limitations of these agents however is their inflexibility once trained. They are able to learn a policy to solve a specific problem (formalized as an MDP), but that learned policy is often useless in new problems, even relatively similar ones.\n\nImagine the simplest possible agent: one trained to solve a two-armed bandit task in which one arm always provides a positive reward, and the other arm always provides no reward. Using any RL algorithm such as Q-Learning or Policy Gradient, the agent can quickly learn to always choose the arm with the positive reward. At this point we might be tempted to say we\u2019ve built an agent that can solve two-arm bandit problems. But have we really? What happens if we take our trained agent and give it a nearly identical bandit problem, except with the values of the arms switched? In this new setting, the agent will perform at worse than chance, since it will simply pick whatever it believed to be the correct arm before. In the traditional RL paradigm, our only recourse would be to train a new agent on this new bandit, and another new agent on another new task, etc, etc.\n\nWhat if this retraining wasn\u2019t necessary though? What if we could have the agent learn a policy for learning new policies? Such an agent could be trained to solve not just a single bandit problem, but all similar bandits it may encounter in the future as well. This approach to learning policies that learn policies is called Meta Reinforcement Learning (Meta-RL), and it is one of the more exciting and promising recent developments in the field.\n\nIn Meta-RL, an RNN-based agent is trained to learn to exploit the structure of the problem dynamically. In doing so, it can learn to solve new problems without the need for retraining, simply by adjusting its hidden state. The original work describing Meta-RL was published by Jane Wang and her colleagues at DeepMind last year in their paper: Learning to Reinforcement Learn. I highly recommend checking out that original article for insight into the development of the algorithm. As it turns out this idea was also independently developed by the group at OpenAI and Berkeley and described in their recent paper RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. Since I first became familiar with these concepts through the DeepMind paper, that is the one I followed in the design of my own implementation.\n\nIn this article I want to first describe how to augment the A3C algorithm with the capacity to perform Meta-RL learning. Next I will show how it can be used to solve certain problems for which a meta-policy can be applied to solving families of MDPs without the need for retraining. The code for a Tensorflow implementation of the model as well as each of the experiments is available at this GitHub Repo: Meta-RL.\n\nThe key ingredient in a Meta-RL system is a Recurrent Neural Network (RNN). This recurrent nature of this architecture is what allows the agent to learn the meta-policy, since it can adjust it\u2019s output over time given new input. If that last bit sounds familiar, it is exactly what the traditional back-propagation training process of neural networks makes possible. Here instead, we will be training the RNN to learn to adjust itself by altering the hidden state, rather than needing backprop, or some other external adjustment intervention for every new task. In order for this to happen, we need more than the standard State -> Action set-up for the network. In addition to observations , we will feed the RNN the previous reward and the previous action . With these three things, the network can associate previous state-action pairs with their rewards, and in doing so adjust its future actions accordingly. As a side note, we don\u2019t need to provide the RNN with the previous observation , since it has already seen it at the previous time-step! Adding and allow the agent to always have the full picture of it\u2019s actions and their success in the task.\n\nFortunately, the A3C algorithm is already halfway to being Meta-RL ready. Since it comes with an RNN to begin with, we simply need to provide the additional and inputs to the network, and it will be able to learn to perform meta-policy learning!\n\nBefore describing the experiments I conducted, I want to talk about the kinds of scenarios in which Meta-RL works, and the scenarios that it doesn\u2019t. Meta-RL isn\u2019t magic, and we can\u2019t train agents on Pong, and have them play Breakout without re-training (yet, at least). The key to success here is that we are interested in learning a policy for learning a policy in a family of similar MDPs or bandits. So in the case of bandits, it is possible to learn a meta-policy for learning a family of unseen two-armed bandit problems. The same can be applied to learning contextual-bandits, grid-worlds, 3D mazes, and more, so long as the underlying logic of the problem remains the same.\n\nDependent Bandits \u2014 The simplest way to test our agent is with the two-armed bandit problem described at the beginning of this article. In this case, there are no observations . Instead the authors add a timestep input which allows the network to maintain a notion of the time in the task. I used this additional input for all of my experiments as well.\n\nThe nature of the bandit is as follows: a randomly chosen arm provides a reward of +1 with probability , and otherwise provides a reward of 0. The other arm provides +1 reward with probability . In this way the two arms are inversely related to one another, and finding out something about one arm provides information about the other as well. For a human, an ideal strategy would be to try each arm a few times, and figure out which provides the +1 reward more often. This is essentially the Meta-Policy that we would like our agent to learn.\n\nIn the experiment each episode consisted of 100 trials. After 20,000 training episodes, the meta-agent is able to flexibly adjust its policy quickly in unseen test-set bandit environments. As you can see below (right), after only a few trials the optimal arm is discovered and exploited. This is in contrast to the agent without the additional meta-enabling inputs (left), which acts randomly throughout the episode and accumulates little reward.\n\nRainbow Bandits \u2014 The next logical step when trying out meta-policy learning is to add in state inputs , to allow for actions conditioned on observations. For this situation, I created a bandit in which the observed state is a 2-pixel image consisting of two random colors. One of the colors is set as \u201ccorrect\u201d at the beginning of the episode, and the corresponding arm will always provide a reward of +1. The other color then always corresponds to a reward of 0. For the duration of an episode (100 trials), the two colors are displayed in randomized order side-by-side each trial. Correspondingly, the arm that provides the reward changes with the color. This time, an ideal policy involves not just trying both arms, but learning that the reward follows the color, and discovering which of the two colors provides the reward. After 30,000 training episodes the Meta-RL agent learns a correct policy for solving these problems.\n\nInterestingly, if we look at the performance curve during training for this task, we discover something unexpected (at least to me). Unlike most curves, in which there is a smooth continuous overall trend towards improved performance, here there is a single discrete jump. The agent goes from random performance to near perfect performance within the span of 5000 episodes or so.\n\nIntuitively, this makes sense with the task since you either are applying the strategy, in which case near perfect performance is attained, or you aren\u2019t, in which case performance over time is at chance level. That in and of itself isn\u2019t so remarkable, but what seems compelling to me is that the agent had been training for hours beforehand, slowly adjusting its weights, with no perceptible behavioral changes. Then, all of a sudden things \u2018clicked\u2019 into place and an entirely new behavioral pattern emerged. This is similar to what psychologists find in the cognitive abilities of children as they develop. Certain cognitive skills such as object permanence appear to come as discrete changes in development, yet they are underlied by continuous changes in the wiring of the brain. These kinds of emergent behaviors are studied as part of a dynamic systems theory approach to cognition, and it is exciting to see similar phenomena come about within the context of RL agents!\n\nRainbow Gridworlds\u2014 For the final experiment, I wanted to test a meta-agent in a full MDP environment, and what better one than the gridworld utilized in the earlier tutorials? In this environment, the agent is represented by a blue square in a small grid. The agent can move up, down, left, or right, to move through the square 5x5 environment. In the environment there are randomly placed squares of one color which provide +1 reward upon contact, and an equal number of randomly placed squares of another color which provide 0 reward. As was the case in the rainbow bandit, the color here is ambiguous, because by randomizing the color each episode, what would be an MDP becomes a family of MDPs. The optimal strategy is to check one of the squares, and discover if it provides the reward. If so, then squares of that color should be the goals for the given episode. If not, then the other color indicates the reward. Each episode consisted of 100 steps, and after 30,000 training episodes, the meta-agent learns an approximation to this optimal policy.\n\nBeyond\u2014In their DeepMind paper, the authors discuss utilizing Meta-RL for 3D maze learning (discussed further in the paper), as well as another 3D task designed to emulate cognitive experiments conducted with primates. The entire paper is a great read, and I recommend it for those interested in the finer details of the algorithm, as well as the exact nature of the set of experiments they conducted. I hope to be able to see this technique applied in a variety of tasks in the future. A long-term goal of artificial intelligence is to train agents that can flexibly adapt to their environments and learn \u201con the go,\u201d so to speak, in a safe and efficient way. Meta-RL/RL\u00b2 is an exciting step toward that.\n\nI hope this walkthrough and experiments has given some intuition about the power of Meta-RL, and the places where it is applicable. If you\u2019d like to utilize Meta-RL in your own projects, feel free to fork or contribute to my Github repository!"
    },
    {
        "url": "https://hackernoon.com/remastering-classic-films-in-tensorflow-with-pix2pix-f4d551fa0503?source=user_profile---------4----------------",
        "title": "Remastering Classic Films in Tensorflow with Pix2Pix",
        "text": "Happy New Year! In this first post of 2017 I wanted to do something fun and a little different, and momentarily switch gears away from RL to generative networks. I have been working with a Generative Adversarial Network called Pix2Pix for the past few days, and want to share the fruits of the project. This framework comes from the paper \u201cImage-to-Image Translation with Conditional Adversarial Networks\u201d recently out of Berkeley. Unlike vanilla GANs, which take noise inputs and produce images, Pix2Pix learns to take an image and translate it into another image using an adversarial framework. Examples of this include turning street maps into aerial photography, drawings into photographs, and day photos into night photos. Theoretically, translation is possible between any two images which maintain the same structure.\n\nWhat struck me as a possible and exciting usage was the capacity to colorize black and white photos, as well as fill in the missing gaps in images. Taken together these two capacities could be used to perform a sort of remastering of films from the 1950s and earlier. Films shot in black and white, and at a 4:3 aspect ratio could particularly benefit from this process. This \u201cremastering\u201d would both colorize and extend the aspect ratio to the more familiar 16:9.\n\nIn this post I am going to walk through the design of Pix2Pix, provide a Tensorflow implementation, as well as show off some results of the film remastering process. If you are new to GANs, I recommend reading my two previous posts on them in order to get a good sense of how they work. Pix2Pix is really just a GAN plus a few extra architectural changes.\n\nPix2Pix builds off of the GAN architecture in a pretty intuitive way. In the GAN formulation we have a generator and discriminator , which are trained adversarially. The generator is trained to generate realistic images from noise input , and the discriminator is trained to differentiate between the real images and those produced by the generator . Using the feedback from the discriminator, the generator can improve its process to produce images more likely to fool the discriminator in the future. Doing so produces more realistic images.\n\nThe most apparent change when going from a GAN to Pix2Pix is that instead of noise the generator is fed an actual image that we want to \u201ctranslate\u201d into another structurally similar image . Our generator now produces , which we want to become indistinguishable from .\n\nIn addition to the original GAN losses, we also utilize an loss, which is just a pixel-wise absolute value loss on the generated images. In this case, we force the generator to approximate with the additional loss:\n\nIn a traditional GAN we would never utilize such a loss because it would prevent the generator from producing novel images. In the case of image translation however, we care about accurate image translations rather than novel ones. This desire for accurate images is also why we don\u2019t entirely do away with the GAN aspect of our network. An loss by itself would produce blurry or washed-out images by virtue of attempting to generate images that are \u201con average\u201d correct. By keeping the GAN losses, we encourage the network to produce crisp images that are visually indistinguishable from the real things.\n\nAlong the lines of ensuring accurate images, the third change to Pix2Pix is the utilization of a U-Net architecture in the generator. Put simply, the U-Net is an auto-encoder in which the outputs from the encoder-half of the network are concatenated with their mirrored counterparts in the decoder-half of the network. By including these skip connections, we prevent the middle of the network from becoming a bottleneck on information flow.\n\nIn the case of Pix2Pix, our input is the image we want to translate and the output is the image we want it to become . By concatenating mirrored layers, we are able to ensure that the structure of the original image gets carried over to the decoder-half of the network directly. When thinking about the task of colorization, the representations learned at each scale of the encoder are extremely useful for the decoder in terms of providing the structure of the colorized image.\n\nIn the Pix2Pix paper, the authors also discuss utilizing a different discriminator architecture referred to as PatchGAN. The idea behind PatchGAN is that instead of producing a single discriminator score for the entire image, a set of separate scores are produced for each patch of the image, and the scores are then averaged together to produce a final score. This approach can improve performance by relying on fewer parameters, and a correctly tuned patch size can lead to improved image quality. A PatchGAN with a patch size the same as the image size is thus equivalent to a traditional discriminator architecture. For the sake of simplicity my Pix2Pix implementation utilizes a traditional discriminator, or PatchGAN over the image.\n\nGiven the long history of film, many of the classics in the medium were created over half a century ago, and as such were made with very different technology than the films of today. Classics such as Casablanca, Citizen Kane, Metropolis, It\u2019s A Wonderful Life, etc, were all shot in black and white and at a limited aspect ratio. Despite their visual limitations, these films still evoke complex and fully realized worlds. Watching them, it is easy to close your eyes and imagine what they might look like in full color and a wider field of view. Given enough time, an artist could even colorize and extend the frames, as has sometimes been done for classic films. With the power of the Pix2Pix framework, I wondered if it would be possible to have a neural network learn to do this automatically.\n\nIn order to accomplish this I transformed my GAN implementation into a Pix2Pix architecture as described above and began training it on a set of classic films. The set-up was simple: I decided to utilize a collection of Alfred Hitchcock films (Vertigo, Rear Window, North by Northwest) as the training set. I choose the films because they were made in color and wide-screen, but were produced at a point where many films were still in black and white. As such, much of the clothing, lighting, and architecture would be similar to the even older films which I wanted to remaster. I also chose them for being some of my favorites from the era.\n\nEach video was resized to 256x144, and image frames were extracted at 2fps. From that point each frame was pre-processed by being converted to grayscale, and cropped to impose a 4:3 aspect ratio. This set of frames composed the training inputs . The original frames were used as the desired outputs .\n\nAfter a day of training, there are impressive results! On the training films themselves the network is able to pretty accurately reproduce the original images (as seen above).\n\nOf course in the field of ML this is basically cheating. A separate test set is needed to get a true sense of performance. Below is an example from The Birds (another Hitchcock classic) recreated using the network. Critically, the network was never trained using any of these frames, or any other frames from the film.\n\nAt least within the transfer domain of \u201cfilms by the same director\u201d the network seems to perform astonishingly well. Another longer remastered sequence from The Birds:\n\nThe main reason to create this however was to allow for the remastering of older films for which there exists no color or wide-screen version. Here are some examples of The Passion of Jean d\u2019Arc, Metropolis, and It\u2019s A Wonderful Life all \u2018remastered\u2019 using the trained Pix2Pix network:\n\nThe results aren\u2019t perfect, but I am pretty happy with them, given the relatively short training time, small training set, as well as minimal changes made to the baseline framework. With a longer training time and a larger set of films used, there is no doubt that results would improve. There are also a number of additions to the architecture that could improve performance. One of the biggest shortcomings is that the network doesn\u2019t take advantage of the temporal structure of the videos. An object off-screen in one frame may be present in another. As such an RNN or sliding window could be utilized in order to capture this information between frames. I leave this work for anyone interested to pick up.\n\nThis Github repository contains everything needed if you\u2019d like to train and run Pix2Pix yourself. Additionally, I have uploaded a pre-trained model here, if you\u2019d just like to try remastering your own favorite older film without retraining a model from scratch. Instructions for getting up and running are provided in the repository.\n\nThis kind of work is in the early days still, but I am confident that not so many years from now we will be able to sit down and watch Metropolis or countless other classic films with all the visual detail that we can imagine them having. The capacity to breathe life into older media is an exciting potential of Deep Learning which will only grow as the technology matures."
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=user_profile---------5----------------",
        "title": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)",
        "text": "In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven\u2019t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.\n\nSo what is A3C? The A3C algorithm was released by Google\u2019s DeepMind group earlier this year, and it made a splash by\u2026 essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI just released a version of A3C as their \u201cuniversal starter agent\u201d for working with their new (and very diverse) set of Universe environments.\n\nAsynchronous Advantage Actor-Critic is quite a mouthful. Let\u2019s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself.\n\nAsynchronous: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it\u2019s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.\n\nActor-Critic: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy \u03c0(s) (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.\n\nAdvantage: If we think back to our implementation of Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were \u201cgood\u201d and which were \u201cbad.\u201d The network was then updated in order to encourage and discourage actions appropriately.\n\nThe insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network\u2019s predictions were lacking. If you recall from the Dueling Q-Network architecture, the advantage function is as follow:\n\nSince we won\u2019t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.\n\nIn this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as Generalized Advantage Estimation.\n\nIn the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by DennyBritz and OpenAI. Both of which I highly recommend if you\u2019d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won\u2019t run on its own. To view and run the full, functional A3C implementation, see my Github repository.\n\nThe general outline of the code architecture is:\n\nThe A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself.\n\nNext, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU.\n\n~ From here we go asynchronous ~\n\nEach worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network.\n\nEach worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment.\n\nOnce the worker\u2019s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (H) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action.\n\nA worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.\n\nA worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment.\n\nOnce a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.\n\nTo view the full and functional code, see the Github repository here.\n\nThe robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first VizDoom challenge. VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:\n\nOnce it is installed, we will be using the environment, which is provided in the Github repository, and needs to be placed in the working directory.\n\nThe challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks.\n\nI hope this tutorial has been helpful to those new to A3C and asynchronous reinforcement learning! Now go forth and build AIs.\n\nIf you\u2019d like to follow my writing on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjuliani.\n\nIf this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!"
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf?source=user_profile---------6----------------",
        "title": "Simple Reinforcement Learning with Tensorflow Part 7: Action-Selection Strategies for Exploration",
        "text": "In this entry of my RL series I would like to focus on the role that exploration plays in an agent\u2019s behavior. I will go over a few of the commonly used approaches to exploration which focus on action-selection, and show their comparative strengths and weaknesses, as well as demonstrate how to implement each using Tensorflow. The methods are discussed here in the context of a Q-Network, but can be applied to Policy Networks as well. To make things more intuitive, I also built an interactive visualization to provide a better sense of how each exploration strategy works (It uses simulated Q-values, so there is no actual neural network running in the browser \u2014 though such things do exist!). Since I can\u2019t embed it in Medium, I have linked to it here, and below. I highly recommend playing with it as you read through the post. Let\u2019s get started!\n\nThe first question one may ask is: why do we need exploration at all? The problem can be framed as one of obtaining representative training data. In order for an agent to learn how to deal optimally with all possible states in an environment, it must be exposed to as many of those states as possible. Unlike in traditional supervised learning settings however, the agent in a reinforcement learning problem only has access to the environment through its own actions. As a result, there emerges a chicken and egg problem: An agent needs the right experiences to learn a good policy, but it also needs a good policy to obtain those experiences.\n\nFrom this problem has emerged an entire subfield within reinforcement learning that has attempted to develop techniques for meaningfully balancing the exploration and exploitation tradeoff. Ideally, such an approach should encourage exploring an environment until the point that it has learned enough about it to make informed decisions about optimal actions. There are a number of frequently used approaches to encouraging exploration. In this post I want to go over some of the basic approaches related to the selection of actions. In a later post in this series I will cover more advanced methods which encourage exploration through the use of intrinsic motivation.\n\nExplanation: All reinforcement learning algorithms seek to maximize reward over time. A naive approach to ensuring the optimal action is taken at any given time is to simply choose the action which the agent expects to provide the greatest reward. This is referred to as a greedy method. Taking the action which the agent estimates to be the best at the current moment is an example of exploitation: the agent is exploiting its current knowledge about the reward structure of the environment to act. This approach can be thought of as providing little to no exploratory potential.\n\nShortcomings: The problem with a greedy approach is that it almost universally arrives at a suboptimal solution. Imagine a simple two-armed bandit problem (for an introduction to multi-armed bandits, see Part 1 of this series). If we suppose one arm gives a reward of 1 and the other arm gives a reward of 2, then if the agent\u2019s parameters are such that it chooses the former arm first, then regardless of how complex a neural network we utilize, under a greedy approach it will never learn that the latter action is more optimal.\n\nExplanation: The opposite approach to greedy selection is to simply always take a random action.\n\nShortcomings: Only in circumstances where a random policy is optimal would this approach be ideal. However it can be useful as an initial means of sampling from the state space in order to fill an experience buffer when using DQN.\n\nExplanation: A simple combination of the greedy and random approaches yields one of the most used exploration strategies: \u03f5-greedy. In this approach the agent chooses what it believes to be the optimal action most of the time, but occasionally acts randomly. This way the agent takes actions which it may not estimate to be ideal, but may provide new information to the agent. The \u03f5 in \u03f5-greedy is an adjustable parameter which determines the probability of taking a random, rather than principled, action. Due to its simplicity and surprising power, this approach has become the defacto technique for most recent reinforcement learning algorithms, including DQN and its variants.\n\nAdjusting during training: At the start of the training process the e value is often initialized to a large probability, to encourage exploration in the face of knowing little about the environment. The value is then annealed down to a small constant (often 0.1), as the agent is assumed to learn most of what it needs about the environment.\n\nShortcomings: Despite the prevalence of usage that it enjoys, this method is far from optimal, since it takes into account only whether actions are most rewarding or not.\n\nExplanation: In exploration, we would ideally like to exploit all the information present in the estimated Q-values produced by our network. Boltzmann exploration does just this. Instead of always taking the optimal action, or taking a random action, this approach involves choosing an action with weighted probabilities. To accomplish this we use a softmax over the networks estimates of value for each action. In this case the action which the agent estimates to be optimal is most likely (but is not guaranteed) to be chosen. The biggest advantage over e-greedy is that information about likely value of the other actions can also be taken into consideration. If there are 4 actions available to an agent, in e-greedy the 3 actions estimated to be non-optimal are all considered equally, but in Boltzmann exploration they are weighed by their relative value. This way the agent can ignore actions which it estimates to be largely sub-optimal and give more attention to potentially promising, but not necessarily ideal actions.\n\nAdjusting during training: In practice we utilize an additional temperature parameter (\u03c4) which is annealed over time. This parameter controls the spread of the softmax distribution, such that all actions are considered equally at the start of training, and actions are sparsely distributed by the end of training.\n\nShortcomings: The underlying assumption made in Boltzmann exploration is that the softmax over network outputs provides a measure of the agent\u2019s confidence in each action. If action 2 is 0.7 and action 1 is 0.2 the tempting interpretation is that the agent believes that action 2 is 70% likely to be optimal, whereas action 1 is 20% likely to be optimal. In reality this isn\u2019t the case. Instead what the agent is estimating is a measure of how optimal the agent thinks the action is, not how certain it is about that optimality. While this measure can be a useful proxy, it is not exactly what would best aid exploration. What we really want to understand is the agent\u2019s uncertainty about the value of different actions.\n\nExplanation: What if an agent could exploit its own uncertainty about its actions? This is exactly the ability that a class of neural network models referred to as Bayesian Neural Networks (BNNs) provide. Unlike traditional neural network which act deterministically, BNNs act probabilistically. This means that instead of having a single set of fixed weights, a BNN maintains a probability distribution over possible weights. In a reinforcement learning setting, the distribution over weight values allows us to obtain distributions over actions as well. The variance of this distribution provides us an estimate of the agent\u2019s uncertainty about each action.\n\nIn practice however it is impractical to maintain a distribution over all weights. Instead we can utilize dropout to simulate a probabilistic network. Dropout is a technique where network activations are randomly set to zero during the training process in order to act as a regularizer. By repeatedly sampling from a network with dropout, we are able to obtain a measure of uncertainty for each action. When taking a single sample from a network with Dropout, we are doing something that approximates sampling from a BNN. For more on the implications of using Dropout for BNNs, I highly recommend Yarin Gal\u2019s Phd thesis on the topic.\n\nShortcomings: In order to get true uncertainty estimates, multiple samples are required, thus increasing computational complexity. In my own experiments however I have found it sufficient to sample only once, and use the noisy estimates provided by the network. In order to reduce the noise in the estimate, the dropout keep probability is simply annealed over time from 0.1 to 1.0.\n\nI compared each of the approaches using a DQN trained on the CartPole environment available in the OpenAI gym. The Bayesian Dropout and Boltzmann methods proved most helpful, at least in my experiment. I encourage those interested to play around with the hyperparameters, as I am sure better performance can be gained from doing so. Indeed, different approaches may be best depending on what hyperparameters are used. Below is the full implementation of each method in Tensorflow:\n\nAll of the methods discussed above deal with the selection of actions. There is another approach to exploration that deals with the nature of the reward signal itself. These approaches fall under the umbrella of intrinsic motivation, and there has been a lot of great work in this area. In a future post I will be exploring these approaches in more depth, but for those interested, here is a small selection of notable recent papers on the topic:"
    },
    {
        "url": "https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32?source=user_profile---------7----------------",
        "title": "ResNets, HighwayNets, and DenseNets, Oh My! \u2013",
        "text": "ResNets, HighwayNets, and DenseNets, Oh My!\n\nWhen it comes to neural network design, the trend in the past few years has pointed in one direction: deeper. Whereas the state of the art only a few years ago consisted of networks which were roughly twelve layers deep, it is now not surprising to come across networks which are hundreds of layers deep. This move hasn\u2019t just consisted of greater depth for depths sake. For many applications, the most prominent of which being object classification, the deeper the neural network, the better the performance. That is, provided they can be properly trained! In this post I would like to walk through the logic behind three recent deep learning architectures: ResNet, HighwayNet, and DenseNet. Each make it more possible to successfully trainable deep networks by overcoming the limitations of traditional network design. I will also be providing Tensorflow code to easily implement each of these networks. If you\u2019d just like the code, you can find that here. Otherwise, read on!\n\nThe first intuition when designing a deep network may be to simply stack many of the typical building blocks such as convolutional or fully-connected layers together. This works to a point, but performance quickly diminishes the deeper a traditional network becomes. The issue arises from the way in which neural networks are trained through backpropogation. When a network is being trained, a gradient signal must be propagated backwards through the network from the top layer all the way down to the bottom most layer in order to ensure that the network updates itself appropriately. With a traditional network this gradient becomes slightly diminished as it passes through each layer of the network. For a network with just a few layers, this isn\u2019t an issue. For a network with more than a couple dozen layers however, the signal essentially disappears by the time it reaches the beginning of the network again.\n\nSo the problem is to design a network in which the gradient can more easily reach all the layers of a network which might be dozens, or even hundreds of layers deep. This is the goal behind the following state of the art architectures: ResNets, HighwayNets, and DenseNets.\n\nA Residual Network, or ResNet is a neural network architecture which solves the problem of vanishing gradients in the simplest way possible. If there is trouble sending the gradient signal backwards, why not provide the network with a shortcut at each layer to make things happen more smoothly? In a traditional network the activation at a layer is defined as follows:\n\nWhere f(x) is our convolution, matrix multiplication, or batch normalization, etc. When the signal is sent backwards, the gradient always must pass through f(x), which can cause trouble due to the nonlinearities which are involved. Instead, at each layer the ResNet implements:\n\nThe \u201c+ x\u201d at the end is the shortcut. It allows the gradient to pass backwards directly. By stacking these layers, the gradient could theoretically \u201cskip\u201d over all the intermediate layers and reach the bottom without being diminished.\n\nWhile this is the intuition, the actual implementation is a little more complex. In the latest incarnation of ResNets, f(x) + x takes the form:\n\nWith Tensorflow we can implement a network composed of these Residual units as follows:\n\nThe second architecture I\u2019d like to introduce is the Highway Network. It builds on the ResNet in a pretty intuitive way. The Highway Network preserves the shortcuts introduced in the ResNet, but augments them with a learnable parameter to determine to what extent each layer should be a skip connection or a nonlinear connection. Layers in a Highway Network are defined as follows:\n\nIn this equation we can see an outline of the previous two kinds of layers discussed: y = H(x,Wh) mirrors our traditional layer, and y = H(x,Wh) + x mirrors our residual unit. What is new is the T(x,Wt) function. This serves at the switch to determine to what extent information should be sent through the primary pathway or the skip pathway. By using T and (1-T) for each of the two pathways, the activation must always sum to 1. We can implement this in Tensorflow as follows:\n\nFinally I want to introduce the Dense Network, or DenseNet. You might say that is architecture takes the insights of the skip connection to the extreme. The idea here is that if connecting a skip connection from the previous layer improves performance, why not connect every layer to every other layer? That way there is always a direct route for the information backwards through the network.\n\nInstead of using an addition however, the DenseNet relies on stacking of layers. Mathematically this looks like:\n\nThis architecture makes intuitive sense in both the feedforward and feed backward settings. In the feed-forward setting, a task may benefit from being able to get low-level feature activations in addition to high level feature activations. In classifying objects for example, a lower layer of the network may determine edges in an image, whereas a higher layer would determine larger-scale features such as presence of faces. There may be cases where being able to use information about edges can help in determining the correct object in a complex scene. In the backwards case, having all the layers connected allows us to quickly send gradients to their respective places in the network easily.\n\nWhen implementing DenseNets, we can\u2019t just connected everything though. Only layers with the same height and width can be stacked. So we instead densely stack a set of convolutional layers, then apply a striding or pooling layer, then densely stack another set of convolutional layers, etc. This can be implemented in Tensorflow as follows:\n\nAll of these network can be trained to classify images using the CIFAR10 dataset, and can perform well with dozens of layers where a traditional neural network fails. With little parameter tuning I was able to get them to perform above 90% accuracy on a test set after only an hour or so. The full code for training each of these models, and comparing them to a traditional networks is available here. I hope this walkthrough has been a helpful introduction to the world of really deep neural networks!"
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc?source=user_profile---------8----------------",
        "title": "Simple Reinforcement Learning with Tensorflow Part 6: Partial Observability and Deep Recurrent\u2026",
        "text": "In this installment of my Simple RL series, I want to introduce the concept of Partial Observability and demonstrate how to design neural agents which can successfully deal with it. As always, in addition to a theoretical discussion of the problem, I have included a Tensorflow implementation of a neural agent which can solve this class of problems.\n\nFor us humans, having access to a limited and changing world is a universal aspect of our shared experience. Despite our partial access to the world, we are able to solve all sorts of challenging problems in the course of going about our daily lives. In contrast, the neural agents we have discussed so far in this tutorial series are ill equipped to handle partial observability.\n\nWhen we think about the kinds of environments used until this point to train our networks, the agent has had access to all the information about the environment it might need in order to take an optimal action. Take for example the Gridworld used in Tutorials 4 & 5 of this series:\n\nBecause the entire world is visible at any moment (and nothing moves aside from the agent), a single frame of this environment gives the agent all it needs to know in order to maximize its reward. Environments which follow a structure where a given state conveys everything the agent needs to act optimally are called Markov Decision Processes (MDPs).\n\nWhile MDPs provide a nice formalism, almost all real world problems fail to meet this standard. Take for example your field of view at this very moment. Can you see what is behind you? This limited perspective on the visual world is almost always the default for humans and other animals. Even if we were to have 360 degree vision, we may still not know what is on the other side of a wall just beyond us. Information outside our view is often essential to making decisions regarding the world.\n\nIn addition to being spatially limited, information available at a given moment is also often temporally limited. When looking at a photo of a ball being thrown between two people, the lack of motion may make us unable to determine the direction and speed of the ball. In games like Pong, not only the position of the ball, but also it\u2019s direction and speed are essential to making the correct decisions.\n\nEnvironments which present themselves in a limited way to the agent are referred to as Partially Observable Markov Decision Processes (POMDPs). While they are trickier to solve than their fully observable counterparts, understanding them is essential to solving most realistic tasks.\n\nHow can we build a neural agent which still functions well in a partially observable world? The key is to give the agent a capacity for temporal integration of observations. The intuition behind this is simple: if information at a single moment isn\u2019t enough to make a good decision, then enough varying information over time probably is. Revisiting the photo example of the thrown ball A single image of a ball in motion tells us nothing about its movements, but two images in sequence allows us to discern the direction of movement. A longer sequence might even allow us to make sense of the speed of the ball. The same principle can be applied to problems where there is a limited field of view. If you can\u2019t see behind you, by turning around you can integrate the forward and backward views over time and get a complete picture of the world with which to act upon.\n\nWithin the context of Reinforcement Learning, there are a number of possible ways to accomplish this temporal integration. The solution taken by DeepMind in their original paper on Deep Q-Networks was to stack the frames from the Atari simulator. Instead of feeding the network a single frame at a time, they used an external frame buffer which kept the last four frames of the game in memory and fed this to the neural network. This approach worked relatively well for the simple games they employed, but it isn\u2019t ideal for a number of reasons. The first is that it isn\u2019t necessarily biologically plausible. When light hits our retinas, it does it at a single moment. There is no way for light to be stored up and passed all at once to an eye. Secondly, by using blocks of 4 frames as their state, the experience buffer used needed to be much larger to accommodate the larger stored states. This makes the training process require a larger amount of potentially unnecessary memory. Lastly, we may simply need to keep things in mind that happened much earlier than would be feasible to capture with stacking frames. Sometimes an event hundreds of frames earlier might be essential to deciding what to do at the current moment. We need a way for our agent to keep events in mind more robustly.\n\nAll of these issues can be solved by moving the temporal integration into the agent itself. This is accomplished by utilizing a recurrent block in our neural agent. You may have heard of recurrent neural networks, and their capacity to learn temporal dependencies. This has been used popularly for the purpose of text generation, where groups have trained RNNs to reproduce everything from Barack Obama speeches to freeform poetry. Andrej Karpathy has a great post outlining RNNs and their capacities, which I highly recommend. Thanks to the high-level nature of Tensorflow, we are free to treat the RNN as somewhat of a black-box that we simply plug into our existing Deep Q-Network.\n\nBy utilizing a recurrent block in our network, we can pass the agent single frames of the environment, and the network will be able to change its output depending on the temporal pattern of observations it receives. It does this by maintaining a hidden state that it computes at every time-step. The recurrent block can feed the hidden state back into itself, thus acting as an augmentation which tells the network what has come before. The class of agents which utilize this recurrent network are referred to as Deep Recurrent Q-Networks (DRQN).\n\nIn order to implement a Deep Recurrent Q-Network (DRQN) architecture in Tensorflow, we need to make a few modifications to our DQN described in Part 4 (See below for full implementation, or follow link here).\n\nFor this tutorial however I\u2019d like to work with something a little less flashy, though hopefully more informative. Recall our Gridworld, where everything was visible to the agent at any moment. By simply limiting the agent\u2019s view of the environment, we can turn our MDP into a POMDP. In this new version of the GridWorld, the agent can only see a single block around it in any direction, whereas the environment itself contains 9x9 blocks. Additional changes are as follows: each episode is fixed at 50 steps, there are four green and two red squares, and when the agent moves to a red or green square, a new one is randomly placed in the environment to replace it. What are the consequences of this?\n\nIf we attempt to use our DQN as described in Parts 4 and 5 of this series, we find that it performs relatively poorly, never achieving more than an average of 2.3 cumulative reward after 10,000 training episodes.\n\nThe problem is that the agent has no way of remembering where it has been or what it has seen. If two areas look the same, then the agent can do nothing but react in exactly the same way to them, even if they are in different parts of the environment. Now let\u2019s look at how our DRQN does in the same limited environment over time.\n\nBy allowing for a temporal integration of information, the agent learns a sense of spatial location that is able to augment its observation at any moment, and allow the agent to receive a larger reward each episode. Below is the Ipython notebook where this DRQN agent is implemented. Feel free to replicate the results yourself, and play with the hyperparameters. Different settings for many of them may provide greater performance for your particular task.\n\nWith this code you have everything you need to train a DRQN that can go out into the messy world and solve problems with partial observability!"
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c?source=user_profile---------9----------------",
        "title": "Simple Reinforcement Learning with Tensorflow Part 1.5: Contextual Bandits",
        "text": "In Part 1 of my Simple RL series, we introduced the field of Reinforcement Learning, and I demonstrated how to build an agent which can solve the multi-armed bandit problem. In that situation, there are no environmental states, and the agent must simply learn to choose which action is best to take. Without a given state state, the best action at any moment is also the best action always. Part 2 establishes the full Reinforcement Learning problem in which there are environmental states, new states depend on previous actions, and rewards can be delayed over time.\n\nThere is actually a set of problems in-between the stateless situation and the full RL problem. I want to provide an example of such a problem, and show how to solve it. My hope is that those entirely new to RL can benefit from being introduced to each element of the full formulation step by step. Specifically, in this post I want to show how to solve problems in which there are states, but they aren\u2019t determined by the previous states or actions. Additionally, we won\u2019t be considering delayed rewards. All of that comes in Part 2. This simplified way of posing the RL problem is referred to as the Contextual Bandit.\n\nIn the original multi-armed bandit problem discussed in Part 1, there is only a single bandit, which can be thought of as like a slot-machine. The range of actions available to the agent consist of pulling one of multiple arms of the bandit. By doing so, a reward of +1 or -1 is received at different rates. The problem is considered solved if the agent learns to always choose the arm that most often provides a positive reward. In such a case, we can design an agent that completely ignores the state of the environment, since for all intents and purposes, there is only ever a single, unchanging state.\n\nContextual Bandits introduce the concept of the state. The state consists of a description of the environment that the agent can use to take more informed actions. In our problem, instead of a single bandit, there can now be multiple bandits. The state of the environment tells us which bandit we are dealing with, and the goal of the agent is to learn the best action not just for a single bandit, but for any number of them. Since each bandit will have different reward probabilities for each arm, our agent will need to learn to condition its action on the state of the environment. Unless it does this, it won\u2019t achieve the maximum reward possible over time. In order to accomplish this, we will be building a single-layer neural network in Tensorflow that takes a state and produces an action. By using a policy-gradient update method, we can have the network learn to take actions that maximize its reward. Below is the iPython notebook walking through the tutorial.\n\nHopefully you\u2019ve found this tutorial helpful in giving an intuition of how reinforcement learning agents can learn to solve problems of varying complexity and interactivity. If you\u2019ve mastered this problem, you are ready to explore the full problem where time and actions matter in Part 2 and beyond of this series."
    },
    {
        "url": "https://medium.com/@awjuliani/generative-adversarial-networks-explained-with-a-classic-spongebob-squarepants-episode-54deab2fce39?source=user_profile---------10----------------",
        "title": "Generative Adversarial Networks Explained with a Classic Spongebob Squarepants Episode",
        "text": "Generative Adversarial Networks (GANs) are a class of neural networks which have gained popularity in the past couple years, and for good reason. Put most simply, they allow a network to learn to generate data with the same internal structure as other data. If that description sounds a little general, that is because GANs are powerful and flexible tools. To make things a little more concrete, one of the more common applications of GANs is image generation. Say you have a bunch of images, such as pictures of cats. A GAN can learn to generate pictures of cats like those real ones used for training, but not actually replicate any one of the individual images. If given enough cat pictures, it actually learns about \u201ccat-ness\u201d from the samples, and learns to generate images that meet this standard. Furthermore, it does so without the generator actually having direct access to any of the training images itself. GANs have been applied to more than just images though. Recently they have been used to generate everything from short videos to robot behavior.\n\nIn this tutorial, I am going to walk through the theory behind the GAN algorithm in a somewhat unconventional way. I am going to explain it using an episode of Spongebob Squarepants. Stick with me, the metaphor works much better than you would think! After that, I will show how to implement a GAN for image generation using Tensorflow and Python.\n\nA Generative Adversarial Network works through the interplay between two semi-separate networks: a generator and a discriminator. The goal of the discriminator is to tell the difference between the data generated by the generator and the real-world data we are trying to model. We can think of the discriminator as being like the bouncer at a club. I don\u2019t have just any bouncer in mind though. Discriminator networks are like the bouncer outside the Salty Spitoon, as seen in the Spongebob Squarepants episode: No Weenies Allowed.\n\nIn the world of Spongebob Squarepants, one must be tough in order to get into the Salty Spitoon. The job of the bouncer (discriminator) is to tell the difference between the real tough guys (the real data), and the weenies (the generated data).\n\nThen we have Spongebob Squarepants (the generator). He certainly isn\u2019t a real tough guy. He is an imitator, and needs to learn to look like a tough guy in order to get into to see his friend.\n\nThrough training, the discriminator network learns a function to tell the difference between the real and generated data. The first features the discriminator learns to look for may be relatively obvious aspects of the data which easily separate the real from the fake.\n\nIn Spongebob\u2019s world, the bouncer outside the Salty Spitoon learns to look for a show of brute strength to tell the tough guys from the weenies.\n\nAny real data should be able to pass this test easily.\n\nOnce the discriminator has learned something about what to use to tell the two apart, the generator can take advantage of what the discriminator has learned in order to learn for itself.\n\nSpongebob can use the fact that the bouncer is checking for strength in order to appear more like an actual tough guy. At the beginning of training however, it still may not appear convincing.\n\nIf the generator hasn\u2019t learned a convincing enough way to generate data, the discriminator will still be able to tell the real from the fake.\n\nWhile Spongebob is able to demonstrate some features of toughness, he is ultimately called out as a weenie, and sent to Weenie Hut Jr for further training.\n\nThis process doesn\u2019t end after a single round however. There can be thousands (or millions) of iterations before the generator produces data similar to the real thing. As the process continues, the discriminator and generator are trained in an alternating fashion. Over time the discriminator will begin to learn more subtle ways of distinguishing the real data from the generated data. This is exactly what we want to happen, as it makes the generator better too, since the generator is only as good at making up data as the discriminator is at telling the real thing apart from the imitation.\n\nChecking back in on Spongebob, the bouncer has learned a new feature to use for distinguishing the tough from the weenies: fighting.\n\nOur generator, Spongebob, uses this new information, and decides to pick a fight with someone in order to appear tough himself.\n\n\u2026and although things don\u2019t go exactly a planned (see episode for more details), it works well enough to fool the bouncer into believing that he is a tough guy.\n\nPutting aside the question of whether or not Spongebob is still a weenie at heart, he has learned to imitate the true tough guys enough to be let into the Salty Spitoon. With enough training, we hope our generator can eventually do the same, and generate data samples that are not only indistinguishable from the real thing to the discriminator, but also to us humans, who are often much more discerning.\n\nOur Spongebob metaphor only goes so far in helping actually build a GAN. To actually implement one, we need to get a little more formal. The generator (G) and discriminator (D) are both feedforward neural networks which play a min-max game between one another. The generator takes as input a vector of random numbers (z), and transforms it into the form of the data we are interested in imitating. The discriminator takes as input a set of data, either real (x) or generated (G(z)), and produces a probability of that data being real (P(x)).\n\nThe discriminator is optimized in order to increase the likelihood of giving a high probability to the real data and a low probability to the generated data.\n\nThe generator is then optimized in order to increase the probability of the generated data being rated highly.\n\nBy alternating gradient optimization between the two networks using these expressions on new batches of real and generated data each time, the GAN will slowly converge to producing data that is as realistic as the network is capable of modeling. If you are interested, you can read the original paper introducing GANs here for more information.\n\nAs mentioned in the introduction, the most popular application of GANs right now is for image generation using convolutional neural networks. Below is an example of an image-producing GAN. The generator which takes a vector input (z), and produces a 64x64x3 color image. The discriminator then takes both real images (x) and generated images (G(z)), and produces a probability P(x) for them. Once the network is trained and we would like to generate new images from it, we simply call G(z) on a new batch of randomized z vectors."
    },
    {
        "url": "https://medium.com/beyond-intelligence/bridging-cognitive-science-and-reinforcement-learning-part-1-enactivism-601af34ef122?source=user_profile---------11----------------",
        "title": "Bridging Cognitive Science and Reinforcement Learning Part 1: Enactivism",
        "text": "In the Cognitive Sciences there are a number of competing theoretical frameworks which attempt to explain intelligent behavior. One perspective which I have always been particularly fond of is the embodiment/enactive approach, which a diverse array of individuals from French Phenomenologists to modern day neuroscientists claim to follow. What ties them all together is the belief that the body and environment plays a central role in perception, action, and the generation of meaning for an organism, not just the brain. While this idea has gained a fair amount of traction within Cognitive Science and Neuroscience, we don\u2019t hear it talked about much in the context of AI. I\u2019d like to change that.\n\nI recently finished reading Action in Perception by Alva No\u00eb, one of the advocates of the Enactivist approach. According to No\u00eb, action is fundamentally for perception. We as intelligent beings wouldn\u2019t be able to truly have a perceptual experience if not for the skillful use of our sensorimotor capacities. In this way, through action in an environment, we enact the perceptual world. These skills depend critically on our having a body, and as such this is a theory of the importance of embodiment as well. Reading his book while working on the reinforcement learning algorithms I\u2019ve shared recently, I couldn\u2019t help but think of some of the ways the two disciplines could inform one another.\n\nIn the context of AI, bodies and environments are far from being given things. For every robot, or self driving car, there is a disembodied Siri, or any other chatbot that we may encounter on a daily basis. More interestingly we have AlphaGo, which is able to play the game of Go at the level of the world champion despite having no physical body (unless you consider a server farm a body \u2014 I don\u2019t for reasons I will save for another post). In this article I want to explain that while AlphaGo and other similar agents may not have a literal physical body, they have a kind of virtual body that exists by virtue of the reinforcement learning environment. It is this virtual embodiment that allows for AIs to learn skills in a way similar to that of living organisms.\n\nThere is traditionally a tendency to think of the different parts of the brain as accomplishing separate tasks. The visual system is for visual perception, and higher-level parts of the brain are for action and planning, so the dogma goes. This is referred to as the modularity argument, and it often is promoted in the popular media. Whenever there is a study finding the \u201carea of the brain responsible for x, \u201d there is an assumption that that area exists independently from the rest of the brain, which is responsible for other functions. If we were to think of this in terms of neural network architecture, it would be akin to building and training one network for vision, and then connecting its output to a network for decision making.\n\nWhen we examine how things actually are however, this distinction between perception and action blurs. The reasons are simple. For one thing: vision by itself has no guiding purpose. Why see things any one specific way, when any other way is just as good? Furthermore, the brain, like so many things in the world is densely interconnected. The idea of a brain region in isolation is an artificial construct.\n\nThe two-step neural network outlined above is never used in the real world, because it simply doesn\u2019t work very well. Instead, we have deeply connected Convolutional Neural Networks where the decision and perception are trained as part of the same network. With enough training, these networks are able to quite successfully identify the objects within a scene when presented with an image of that scene. In this way the network has a specific purpose, and it\u2019s vision is harnessed toward a goal. These networks learn to be sensitive to aspects of the visual scene that are relevant to the task at hand. They learn sensitivity to outlines and luminance at the lower layers, and things like faces and shapes at the higher layers. Outlines and faces become not merely neutral features of the scene to the network, but rather meaningful in relation to their potential for signifying a given object.\n\nOnly organisms that need to identify objects care about the outlines of objects, or the meaningful markers that might distinguish one object from another. In a neural network, the process by which this happens is backpropogation. For those unfamiliar, backpropogation uses a loss function defined at the highest layer of the network to determine how \u201cwrong\u201d the network was about a given sample and sends a signal backwards through the network to update the connections in order to be a little more \u201cright\u201d next time. It is through the training process of backpropogation that the objective defined at the final layer of the network is able to effect all previous layers. Backpropagation is a powerful idea, and at least one of the leading researchers in Deep Learning believe that it is what drives learning in the brain too!\n\nTurning to reinforcement learning agents we discover the same logic, but even more strikingly similar to No\u00eb\u2019s argument. For now we have an environment that the agent is always embedded within. Furthermore, the agent is now explicitly using its perceptions for action in that environment. AlphaGo, for example is able to learn to see the Go board not just in any way, but in a way that is directly conducive to acting in the game. In this way the agent learns to make sense of the world with a particular engagement within it. It learns not some neutral representation of the world that is then acted upon, but rather a world that is from the beginning filled with meaning. This way of thinking about human experience has a history in the phenomenological tradition of philosophy. Chief among the phenomenologists was Maurice Merleau-Ponty, who almost a century ago wrote:\n\nMerleau-Ponty points out the way in which a soccer game is not experienced in a so-called \u201cobjective\u201d world. To a skilled soccer player, every perception is from the beginning a field of meaning with points of attraction and repulsion. Moreover this field is fundamentally geared toward action. Reinforcement learning allows for this possibility, and it does so because the agent is embodied. The world is at-stake for the agent, since each action is given a meaning through the rewards received by the agent. Below is an image taken from a paper by Wang et al., in which they show what the agent \u201csees\u201d in the Atari game Enduro. The red area on the right image indicates the network seeing the area around the car in front of it as meaningful for action. The agent must avoid the car to maintain a high score, and as such the car becomes a point of repulsion for the agent.\n\nAlva No\u00eb goes much further in his book however than simply pointing out that perception is for action. He makes the more radical claim that action is for perception. What he means by this is that without action there could be no true meaningful experiences of the world. The book sets out exploring just how it is that action makes perception possible. No\u00eb points out that our retinal images provides us with very little at any given moment, and what they do provide doesn\u2019t correspond to how we experience the world. A glance at a bowl creates an elliptical impression on our retina. How does this elliptical impression become the experience of a circular bowl? In order to answer this question, No\u00eb draws on the concept of sensorimotor skills, which allows us to understand the way in which the phenomenal world changes in respect to us and our environment. It is because we can move around the bowl, and watch as it moves around the world that we are able to understand it is circular. By building up sensori-motor skills, we acquire the capacity to give meaning to a world otherwise meaningless.\n\nThis can be taken to a more abstracted level when thinking about complex behavior in the world. We act in order to give ourselves better perceptions, which improve our understanding of the world, thus increasing our capacity to act. Imagine that you want to find a lost dog. You suspect it is behind a car you see in the distance. We have the knowledge that by moving around the car, we are able to put ourselves in a position to obtain new kinds of experiences (namely that of the other side of the car, and what it had occluded). This knowledge of how to skillfully discover new meaning within an environment is a key aspect of human intelligence. When we look at modern AI however, we find it somewhat lacking.\n\nCurrent Reinforcement Learning approaches are often limited by short sighted agents. If they aren\u2019t able to see how to achieve a reward given a certain context, they have little means of putting themselves in a better context to see what they need. They have the capacity to fundamentally see action in perception, but they lack the capacity to meaningfully utilize action in order to uncover new kinds of perceptions. The Reinforcement Learning community certainly isn\u2019t unaware of this deficit. In fact it is an active area of research, even if they don\u2019t call it \u201cAction In Perception.\u201d\n\nThe fundamental problem of getting an RL agent to care about experiences that aren\u2019t directly rewarding, but rather open the possibility of greater rewards has been tackled in a number of ways. The first of which is to discover ways to reward an agent for exploring its environment. The thinking along these lines is that by encouraging exploration, the agent will stumble onto novel kinds of environments that allow for greater rewards. Research into this approach has attempted to develop different kinds of bonuses for exploring.The size of this bonus is typically thought of as the level of surprise an agent experiences at finding itself in a novel situation.\n\nGiven this general definition, there are a number of ways the surprise factor could be calculated. The way used by one group is to determine the difference between what an agent expected to receive as a reward for it\u2019s action, and the true reward. By exploiting what they call Prioritized Replay, an agent trains itself more frequently on experiences that yielded unexpected rewards. Another way to gauge surprise is to look at the difference not in what the agent expects as a reward, but in what the agent expects to see next. Research along these lines utilize a model of the environment, and the difference between the expectation of the model and the true environment is used as the surprise bonus.\n\nAnother approach is Hierarchical Planning. The idea here is to develop agents that can learn to decompose tasks into smaller subtasks. By rewarding an agent for completing subtasks that are necessary, but not inherently rewarding themselves, the designer of the agent is able to mold the agent\u2019s behavior toward acting in ways to discover perceptions that allow for new kinds of actions to be taken. Researchers have built an agent using this approach that has been able to perform well on the Atari game Montezuma\u2019s Revenge. This game is difficult for traditional RL agents because the agent must find a key and use it to unlock a door in each room in order to receive a reward. This may sound simple to us, but it involves performing a series of intentional behaviors before it ever receives a reward.\n\nWhile all of these areas are promising approaches, there seems to not yet have been an \u201cah-ha\u201d moment for the community to rally behind. It is still unclear how to allow action to influence perception in a natural and generalizable way that fits neatly into neural architecture the same intuitive way that the perception -> action relationship does. All these lines of research are in their infancy however, and there is little doubt that this kind of solution will be discovered one day. This capacity is after all so essential to what allows organisms to survive in the world, and it will be essential to true AI in the future."
    },
    {
        "url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a?source=user_profile---------12----------------",
        "title": "Simple Reinforcement Learning with Tensorflow Part 5: Visualizing an Agent\u2019s Thoughts and Actions",
        "text": "In this post of my Reinforcement Learning series, I want to further explore the kinds of representations that our neural agent learns during the training process. While getting a high score, or accomplishing a specified task is what we often want our neural agents to be capable of, it is just as important to understand how, and even more critically, why that agent is behaving in a certain way. In order to make the learning process a little more transparent, I built a d3.js powered web interface that presents various kinds of information about our agent as it learns. I call it the Reinforcement Learning Control Center. In this post I will use it to provide a little more insight into the how and why of an RL agent.\n\nThe Control Center was designed with the purpose of allowing the user to track the performance of an agent in realtime as it learns to perform a task. On the left of the interface, The episode length and reward over time are tracked and updated dynamically. The right displays an animated gif of a sample training episode, along with the advantage and value functions being computed by the agent at every step of the episode.\n\nThe interface is currently specific to the neural network and task that I described in my last post. It is a Double-Dueling-DQN, and the environment is a simple gridworld. The rules of that gridworld are as follows: the agent controls the blue square and can move either up, down, left, or right. The goal is to get to the green square as quickly as possible, while avoiding the red square. The green square provides +1 reward, the red square -1 reward, and each step is -0.1 reward. At the beginning of each episode, the three squares are randomly placed on the grid.\n\nThe DD-DQN neural agent processes two separate streams of information as it experiences the gridworld: an advantage stream and a value stream. The advantage stream represent how good the network thinks it is to take each action, given the current state it is in. The value stream represents how good the network thinks it is to be in a given state, regardless of possible action. With the Control Center, we can watch as the network learns to correctly predict the value of its state and actions over time. As the training process proceeds, it goes from seemingly random values to accurately interpreting certain actions as the most advantageous. We can think of this visualization as providing a portal into the \u201cthought process\u201d of our agent. Does it know that it is in a good position when it is in a good position? Does it know that going down was a good thing to do when it went down? This can give us the insights needed to understand why our agent might not be performing ideally as we train it under different circumstances in different environments.\n\nNot only can we use the interface to explore how the agent does during training, we can also use it for testing and debugging our fully trained agents. For example, after training our agent to solve the simple 3x3 gridworld described above, we can provide it with some special test scenarios it had never encountered during the training process to evaluate whether it really is representing experience as we would expect it to.\n\nBelow is an example of the agent performing a modified version of the task with only green squares. As you can see, as the agent gets closer to the green squares the value estimate increases just as we would expect. It also has high estimates of the advantage for taking actions that get it closer to the green goals.\n\nFor the next test we can invert the situation, giving the agent a world in which there were only two red squares. It didn\u2019t like this very much. As you can see below, the agent attempts to stay away from either square, resulting in behavior where it goes back and forth for a long period of time. Notice how the value estimate decreases as the agent approaches the red squares.\n\nFinally, I provided the agent with a bit of an existential challenge. Instead of augmenting the kind of goals present, I removed them all. In this scenario, the blue square is by itself in the environment, with no other objects. Without a goal to move towards, the agent moves around seemingly at random, and the value estimates are likewise seemingly meaningless. What would Camus say?\n\nTaken together, these three experiments provide us with evidence that our agent is indeed responding to the environment as we would intuitively expect. These kinds of checks are essential to make when designing any reinforcement learning agent. If we aren\u2019t careful about the expectations we built into the agent itself and the reward structure of the environment, we can easily end up with situations where the agent doesn\u2019t properly learn the task, or at least doesn\u2019t learn it as we\u2019d expect. In the gridworld for example, taking a step results in a -0.1 reward. This wasn\u2019t always the case though. Originally there was no such penalty, and the agent would learn to move to the green square, but do so after an average of about 50 steps! It had no \u201creason\u201d to hurry, to the goal, so it didn\u2019t. By penalizing each step even a small amount, the agent is able to quickly learn the intuitive behavior of moving directly to the green goal. This reminds us of just how subconscious our own reward structures as humans often are. While we may explicitly only think of the green as being rewarding and the red as being punishing, we are subconsciously constraining our actions by a desire to finish quickly. When designing RL agents, we need ensure that we are making their reward structures as rich as ours.\n\nIf you want to play with a working version of the Control Center without training an agent yourself, just follow this link (currently requires Google Chrome). The agent\u2019s performance you will see was pretrained on the gridworld task for 40,000 episodes. You can click the timeline on the left to look at an example episode from any point in training. The earlier episodes clearly show the agent failing to properly interpret the task, but by the end of training the agent almost always goes straight to the goal.\n\nThe Control Center is a piece of software I plan to continue to develop as I work more with various Reinforcement Learning algorithms. It is currently hard-coded to certain specifics of the gridworld and DD-DQN described in Part 4, but if you are interested in using the interface for your own projects, feel free to fork it on Github, and adjust/adapt it to your particular needs as you see fit. Hopefully it can provide new insights into the internal life of your learning algorithms too!"
    },
    {
        "url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df?source=user_profile---------13----------------",
        "title": "Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond",
        "text": "Welcome to the latest installment of my Reinforcement Learning series. In this tutorial we will be walking through the creation of a Deep Q-Network. It will be built upon the simple one layer Q-network we created in Part 0, so I would recommend reading that first if you are new to reinforcement learning. While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n\nIt was these three innovations that allowed the Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent. We will be walking through each individual improvement, and showing how to implement it. We won\u2019t stop there though. The pace of Deep Learning research is extremely fast, and the DQN of 2014 is no longer the most advanced agent around anymore. I will discuss two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. In the end we will have a network that can tackle a number of challenging Atari games, and we will demonstrate how to train the DQN to learn a basic navigation task.\n\nSince our agent is going to be learning to play video games, it has to be able to make sense of the game\u2019s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. In this way, they act similarly to human receptive fields. Indeed there is a body of research showing that convolutional neural network learn representations that are similar to those of the primate visual cortex. As such, they are ideal for the first few elements within our network.\n\nIn Tensorflow, we can utilize the tf.contrib.layers.convolution2d function to easily create a convolutional layer. We write for function as follows:\n\nHere num_outs refers to how many filters we would like to apply to the previous layer. kernel_size refers to how large a window we would like to slide over the previous layer. Stride refers to how many pixels we want to skip as we slide the window across the layer. Finally, padding refers to whether we want our window to slide over just the bottom layer (\u201cVALID\u201d) or add padding around it (\u201cSAME\u201d) in order to ensure that the convolutional layer has the same dimensions as the previous layer. For more information, see the Tensorflow documentation.\n\nThe second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent\u2019s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we will build a simple class that handles storing and retrieving memories.\n\nThe third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network\u2019s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network\u2019s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.\n\nInstead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another DeepMind paper earlier this year, where they found that it stabilized the training process.\n\nWith the additions above, we have everything we need to replicate the DWN of 2014. But the world moves fast, and a number of improvements above and beyond the DQN architecture described by DeepMind, have allowed for even greater performance and stability. Before training your new DQN on your favorite ATARI game, I would suggest checking the newer additions out. I will provide a description and some code for two of them: Double DQN, and Dueling DQN. Both are simple to implement, and by combining both techniques, we can achieve better performance with faster training times.\n\nThe main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn\u2019t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n\nIn order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:\n\nThe goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn\u2019t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions.\n\nNow that we have learned all the tricks to get the most out of our DQN, let\u2019s actually try it on a game environment! While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine. For educational purposes, I have built a simple game environment which our DQN learns to master in a couple hours on a moderately powerful machine (I am using a GTX970). In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1). At the start of each episode all squares are randomly placed within a 5x5 grid-world. The agent has 50 steps to achieve as large a reward as possible. Because they are randomly positioned, the agent needs to do more than simply learn a fixed path, as was the case in the FrozenLake environment from Tutorial 0. Instead the agent must learn a notion of spatial relationships between the blocks. And indeed, it is able to do just that!\n\nThe game environment outputs 84x84x3 color images, and uses function calls as similar to the OpenAI gym as possible. In doing so, it should be easy to modify this code to work on any of the OpenAI atari games. I encourage those with the time and computing resources necessary to try getting the agent to perform well in an ATARI game. The hyperparameters may need some tuning, but it is definitely possible. Good luck!"
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=user_profile---------14----------------",
        "title": "Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks",
        "text": "For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1\u20133). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).\n\nUnlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.\n\nFor this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn\u2019t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.\n\nIn it\u2019s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.\n\nWe make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:\n\nThis says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (\u03b3) future reward expected according to our own table for the next state (s\u2019) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:\n\nNow, you may be thinking: tables are great, but they don\u2019t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don\u2019t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.\n\nIn the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the \u201ctarget\u201d value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.\n\nBelow is the Tensorflow walkthrough of implementing our simple Q-Network:\n\nWhile the network learns to solve the FrozenLake problem, it turns out it doesn\u2019t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!"
    },
    {
        "url": "https://medium.com/@awjuliani/are-neural-networks-truly-creative-e713ac963f05?source=user_profile---------15----------------",
        "title": "Are Neural Networks Truly Creative? \u2013 Arthur Juliani \u2013",
        "text": "In the past couple years there has been an explosion of deep learning applications for the creative arts. There are Neural networks that write plays, compose music, stylize photos, create hallucinogenic imagery, or even produce dream videos. There has been a lot of excitement within the field of machine learning about these new applications, but perhaps less criticality than might be expected for a newly developing creative discipline. In engineering disciplines, defining a problem properly is often an essential means to allowing it to truly be solved. I fear when it comes to creativity and neural networks, many may be settling for a too loosely defined problem.\n\nSimply because something a Neural Network produces looks or sounds interesting or aesthetically pleasing doesn\u2019t necessarily mean it is doing the same thing as those artists we consider great. In fact, I would like to argue that most current neural network algorithms by definition are unable to be truly creative in the critical ways in which humans are, and as such are not yet able to produce great art. I understand this is a somewhat controversial position, so I would like to make the distinction between creativity/art in the strong sense, and in the weak sense. I fully accept the artistic potential in many of the current neural networks, but I\u2019d like to pose the strong sense of the term creative as a means of pushing the field forward.\n\nThe current state of the art algorithms within the field of deep learning are able to learn highly complex models of the patterns inherent in a set of data. The generative models which produce much of the neural artwork being shown off are no different. In order to produce a melody, they are trained using thousands of previous melodies, and the structure inherent in these previous works is then reproduced by the neural network when composing a new piece of music. The same is true for images generation: networks learn to produce images that fit a previous set of image statistics. So too for networks which learn to create new texts, such as poems or plays. They are learning and producing structures that already existed in a previous body of work. These are by no means unimpressive accomplishments. It is a critical aspect of human intelligence to be able to learn the complex patterns of information in the world and reproduce them in meaningful ways.\n\nWhat the architecture behind these networks all miss however is that other critical aspect of human intelligence: the capacity to create things that don\u2019t simply capture the past, but are genuinely new and other to what has come before. The word for this is alterity, and I hope it becomes commonplace to the next generation of deep learning architects. Alterity means not simply new in the sense of a novel recombination of what already exists, but new in the sense of being of a fundamentally different nature than anything that has come before. To a computer scientist such a definition may seem to reflect an impossibility. Surely everything new is simply novel recombinations? But the history of art attests to just the opposite. All great movements in art are predicated on their power to break with the past. This of course doesn\u2019t mean a disregarding the past, or to create with an ignorance of what has come before, but rather to create with a full understanding of the past, and an explicit violation of the patterns that had hitherto been established. This isn\u2019t impossible, but is a much more challenging task.\n\nIn this spirit, I would propose a litmus test for the creative potential of any new piece of artwork (created by a neural network or otherwise). If the art could have been created simply as a novel recombination of what already existed up to that point, then it is not truly creative in our strong sense of the word. If however it would be impossible for the patterns of the past to be exclusively harnessed towards the art\u2019s creation, then we can say it passes our test. This may seem like a high bar to set, but I think we find that most great artwork easily passes. Consider the field of popular music, and the proliferation of music genres from the 1950s onward. In 1950 Hip-hop, Disco, R&B, Punk, Metal, Ambient, Progressive, and countless other genres simply did not exist. Not only didn\u2019t they exist, but there is no way to simply combine the patterns inherent within the music of the time, Classical, Rock n Roll, Jazz, and Folk into any of these new kinds of music. As any music critic will attest, The Velvet Underground and Nico was not simply an album filled with new takes on old ideas, but an act of great alterity in the music world.\n\nAt the beginning of this article I framed the issue as one of posing a problem to the deep learning community. Not as insurmountable gotcha problem, but one I think is most certainly solvable. One that will be essential to helping point those working in this field in the right direction toward truly creative algorithms, not simply impressive imitators. This faith comes from my background in neuroscience, and the belief that while we humans are fantastic creatures, there is nothing cosmic or a-material about our creative capacity. It is something rooted in our neurology and biology. I fully believe computers will be able to truly create in my strong sense of the term one day. We simple need to make sure we push them to do so."
    },
    {
        "url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99?source=user_profile---------16----------------",
        "title": "Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL",
        "text": "What is a model and why would we want to use one? In this case, a model is going to be a neural network that attempts to learn the dynamics of the real environment. For example, in the CartPole we would like a model to be able to predict the next position of the Cart given the previous position and an action. By learning an accurate model, we can train our agent using the model rather than requiring to use the real environment every time. While this may seem less useful when the real environment is itself a simulation, like in our CartPole task, it can have huge advantages when attempting to learn policies for acting in the physical world.\n\nUnlike in computer simulations, physical environments take time to navigate, and the physical rules of the world prevent things like easy environment resets from being feasible. Instead, we can save time and energy by building a model of the environment. With such a model, an agent can \u2018imagine\u2019 what it might be like to move around the real environment, and we can train a policy on this imagined environment in addition to the real one. If we were given a good enough model of an environment, an agent could be trained entirely on that model, and even perform well when placed into a real environment for the first time.\n\nHow are we going to accomplish this in Tensorflow? As I mentioned above, we are going to be using a neural network that will learn the transition dynamics between a previous observation and action, and the expected new observation, reward, and done state. Our training procedure will involve switching between training our model using the real environment, and training our agent\u2019s policy using the model environment. By using this approach we will be able to learn a policy that allows our agent to solve the CartPole task without actually ever training the policy on the real environment! Read the iPython notebook below for the details on how this is done.\n\nSince there are now two network involved, there are plenty of hyper-parameters to adjust in order to improve performance or efficiency. I encourage you to play with them in order to discover better means of combining the the models. In Part 4 I will be exploring how to utilize convolutional networks to learn representations of more complex environments, such as Atari games."
    },
    {
        "url": "https://medium.com/@awjuliani/introducing-neural-dream-videos-5d517b3cc804?source=user_profile---------17----------------",
        "title": "Introducing Neural Dream Videos \u2013 Arthur Juliani \u2013",
        "text": "Over the past few weeks I have been working on a way to combine two deep learning architectures, a variational auto-encoder and a recurrent neural network, to allow for the generation of what I am calling Neural Dream Videos. These videos capture both the spatial and temporal properties of a given source video, and produce potentially endless new variations on the substance of the video itself in a hallucinogenic way. Below are a couple examples of Neural Dream Videos I generated from classic video games (see further down for live-action videos).\n\nThe videos above were generated completely from the two neural network working together, and once trained don\u2019t rely on the source videos at all. Let me explain the process below.\n\nVariational Auto-encoders (VAE) have been used as a way to allow neural networks to create new examples of images. They do this by learning an efficient representation of the spatial characteristics of thousands of training images. For example, give a VAE thousands of horse photos, and it will learn a representation that allows it to produce novel horse photos from it\u2019s own representation. Like humans, this can be thought of as a kind of imagination at work. While they work well for still images, videos are far too complex to capture using a VAE however, and that is where the current network comes into play.\n\nRecurrent Neural Networks (RNNs) are used to learn temporal patterns in data, and create new patterns from old ones. A common use case for RNNs that has gained attention lately is language modeling. For example an RNN learns to produce a piece of text, such as a new Shakespearian play, when trained on a corpus of Shakespeare\u2019s works. The problem with RNNs by themselves is that they are too simple to understand the relationships between video frames, which contain thousands of numbers and complex spatial relationships. By combining a VAE and RNN, we can train the VAE to learn a compact and semantically meaningful representation of the frames of a video, and then train an RNN to learn how to model the temporal patterns of those latent representations. Once we have a newly generated sequence of latent representations, we can run them through the trained VAE to generate a new set of full video frames. Stitch them back into a video, and thats it!\n\nBelow are a few videos created from live-action footage.\n\nI am happy to be releasing this code for creating these videos on Github here, and I hope others are able to have fun making these kinds of videos as well. There are also a lot of ways in which the quality of both the video and the logic of the temporal flow of the videos could be improved by using more sophisticated kinds of auto-encoders and recurrent networks. If you work on those kinds of things, please feel free to contact me or contribute additions or changes to the repository.\n\nWe are still in the early times of creative applications for generative networks. Google is currently developing models that produce basic melodies, and the videos here don\u2019t stray too far from their source material. Still, such networks are the first steps to neural networks that will be able to accomplish the exciting tasks of write songs, making art, and create films on their own."
    },
    {
        "url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724?source=user_profile---------18----------------",
        "title": "Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents",
        "text": "After a weeklong break, I am back again with part 2 of my Reinforcement Learning tutorial series. In Part 1, I had shown how to put together a basic agent that learns to choose the more rewarding of two possible options. In this post, I am going to describe how we get from that simple agent to one that is capable of taking in an observation of the world, and taking actions which provide the optimal reward not just in the present, but over the long run. With these additions, we will have a full reinforcement agent.\n\nEnvironments which pose the full problem to an agent are referred to as Markov Decision Processes (MDPs). These environments not only provide rewards and state transitions given actions, but those rewards are also condition on the state of the environment and the action the agent takes within that state. These dynamics are also temporal, and can be delayed over time.\n\nTo be a little more formal, we can define a Markov Decision Process as follows. An MDP consists of a set of all possible states from which our agent at any time will experience . A set of all possible actions from which our agent at any time will take action . Given a state action pair , the transition probability to a new state is defined by , and the reward is given by . As such, at any time in an MDP, an agent is given a state , takes action , and receives new state and reward .\n\nWhile it may seem relatively simple, we can pose almost any task we could think of as an MDP. For example, imagine opening a door. The state is the vision of the door that we have, as well as the position of our body and door in the world. The actions are our every movement our body could make. The reward in this case is the door successfully opening. Certain actions, like walking toward the door are essential to solving the problem, but aren\u2019t themselves reward-giving, since only actually opening the door will provide the reward. In this way, an agent needs to learn to assign value to actions the lead eventually to the reward, hence the introduction of temporal dynamics.\n\nIn order to accomplish this, we are going to need a challenge that is more difficult for the agent than the two-armed bandit. To meet provide this challenge we are going to utilize the OpenAI gym, a collection of reinforcement learning environments. We will be using one of the classic tasks, the Cart-Pole. To learn more about the OpenAI gym, and this specific task, check out their tutorial here. Essentially, we are going to have our agent learn how to balance a pole for as long as possible without it falling. Unlike the two-armed bandit, this task requires:\n\nTo take reward over time into account, the form of Policy Gradient we used in the previous tutorials will need a few adjustments. The first of which is that we now need to update our agent with more than one experience at a time. To accomplish this, we will collect experiences in a buffer, and then occasionally use them to update the agent all at once. These sequences of experience are sometimes referred to as rollouts, or experience traces. We can\u2019t just apply these rollouts by themselves however, we will need to ensure that the rewards are properly adjusted by a discount factor\n\nIntuitively this allows each action to be a little bit responsible for not only the immediate reward, but all the rewards that followed. We now use this modified reward as an estimation of the advantage in our loss equation. With those changes, we are ready to solve CartPole!\n\nLet\u2019s get to it!\n\nAnd with that we have a fully-functional reinforcement learning agent. Our agent is still far from the state of the art though. While we are using a neural network for the policy, the network still isn\u2019t as deep or complex as the most advanced networks. In the next post I will be showing how to use Deep Neural Networks to create agents able to learn more complex relationships with the environment in order to play a more exciting game than pole balancing. In doing so, I will be diving into the kinds of representations that a network learns for more complex environments."
    },
    {
        "url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149?source=user_profile---------19----------------",
        "title": "Simple Reinforcement Learning in Tensorflow: Part 1 - Two-armed Bandit",
        "text": "Reinforcement learning provides the capacity for us not only to teach an artificial agent how to act, but to allow it to learn through it\u2019s own interactions with an environment. By combining the complex representations that deep neural networks can learn with the goal-driven learning of an RL agent, computers have accomplished some amazing feats, like beating humans at over a dozen Atari games, and defeating the Go world champion.\n\nLearning how to build these agents requires a bit of a change in thinking for anyone used to working in a supervised learning setting though. Gone is the ability to simply get the algorithm to pair certain stimuli with certain responses. Instead RL algorithms must enable the agent to learn the correct pairings itself through the use of observations, rewards, and actions. Since there is no longer a \u201cTrue\u201d correct action for an agent to take in any given circumstance that we can just tell it, things get a little tricky. In this post and those to follow, I will be walking through the creation and training of reinforcement learning agents. The agent and task will begin simple, so that the concepts are clear, and then work up to more complex task and environments.\n\nThe simplest reinforcement learning problem is the n-armed bandit. Essentially, there are n-many slot machines, each with a different fixed payout probability. The goal is to discover the machine with the best payout, and maximize the returned reward by always choosing it. We are going to make it even simpler, by only having two possible slot machines to choose between. In fact, this problem is so simple that it is more of a precursor to real RL problems than one itself. Let me explain. Typical aspects of a task that make it an RL problem are the following:\n\nThe n-armed bandit is a nice starting place because we don\u2019t have to worry about aspects #2 and 3. All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones. In the context of RL lingo, this is called learning a policy. We are going to be using a method called policy gradients, where our simple neural network learns a policy for picking actions by adjusting it\u2019s weights through gradient descent using feedback from the environment. There is another approach to reinforcement learning where agents learn value functions. In those approaches, instead of learning the optimal action in a given state, the agent learns to predict how good a given state or action will be for the agent to be in. Both approaches allow agents to learn good behavior, but the policy gradient approach is a little more direct.\n\nThe simplest way to think of a Policy gradient network is one which produces explicit outputs. In the case of our bandit, we don\u2019t need to condition these outputs on any state. As such, our network will consist of just a set of weights, with each corresponding to each of the possible arms to pull in the bandit, and will represent how good our agent thinks it is to pull each arm. If we initialize these weights to 1, then our agent will be somewhat optimistic about each arm\u2019s potential reward.\n\nTo update our network, we will simply try an arm with an e-greedy policy (See Part 7 for more on action-selection strategies). This means that most of the time our agent will choose the action that corresponds to the largest expected value, but occasionally, with e probability, it will choose randomly. In this way, the agent can try out each of the different arms to continue to learn more about them. Once our agent has taken an action, it then receives a reward of either 1 or -1. With this reward, we can then make an update to our network using the policy loss equation:\n\nis advantage, and is an essential aspect of all reinforcement learning algorithms. Intuitively it corresponds to how much better an action was than some baseline. In future algorithms, we will develop more complex baselines to compare our rewards to, but for now we will assume that the baseline is 0, and it can be thought of as simply the reward we received for each action.\n\nis the policy. In this case, it corresponds to the chosen action\u2019s weight.\n\nIntuitively, this loss function allows us to increase the weight for actions that yielded a positive reward, and decrease them for actions that yielded a negative reward. In this way the agent will be more or less likely to pick that action in the future. By taking actions, getting rewards, and updating our network in this circular manner, we will quickly converge to an agent that can solve our bandit problem! Don\u2019t take my word for it though. Try it out yourself.\n\nIf you\u2019d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjliani.\n\nIf this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!"
    },
    {
        "url": "https://medium.com/@awjuliani/exploring-sentiment-in-literature-with-deep-learning-30366aff578e?source=user_profile---------20----------------",
        "title": "Exploring Sentiment in Literature with Deep Learning",
        "text": "Sentiment analysis has come a long way in the past few years. Gone are the days when systems would be fooled by a simple negation such as \u201cI don\u2019t love this movie.\u201d With Deep Learning approaches, much more complex and subtle forms of positive or negative sentiment can be picked up on by the system. These new, more accurate approaches open up a variety of possibilities for understanding sentiment in large amounts of text. I decided to apply some of these new models to classic literary texts in order to see if I could visualize their affective valences, and give some insight into decades old works.\n\nThe models I used were a Convolutional Neural Network and Recurrent Neural Network, both pre-trained using word2vec. The code for both is available here. Both models were trained using a database of positive and negative movie reviews typically used in sentiment analysis research. Immediately this posed a theoretical problem, how much does sentiment in movie reviews have to do with sentiment in literature? Once I had the trained models, I needed a sanity-check. So I took the most positive and negative books I could think of, and compared them.\n\nFor the most positive, I choose Leaves of Grass, Walt Whitman\u2019s classic ode to life, democracy, and the human potential. For the negative work, I chose The Metamorphosis, by Franz Kafka, a book about a man who turns into a bug, is hated by his entire family, and then dies. Just from these descriptions, the sentiment of the work is apparent to us humans. What do the neural networks think? Below are the visualized sentiments of the two books. Each block represents a single sentence, with blue indicating positive, and red indicating negative sentiment.\n\nLo-and behold, it worked pretty well! From there I gathered over a dozen other works of literature publicly available, and I put together a website where these visualizations can be viewed. The website allows you to explore both the visual overview of the entire work, but also highlight any given block and discover the corresponding sentence. While many of them are pretty accurate, it is a fun exercise to discover the ways in which the models break down. The site also provides both model\u2019s predicted sentiments. The CNN is more accurate, but the RNN makes surprising classifications that can be fun to look through as well.\n\nIf you like the site, and want to see what one of your favorite public domain books on it, don\u2019t hesitate to send me a message, and I would be happy to add it! It turns out many works seem to have on average a balanced amount of positive and negative sentiment, but I would love to use this tool to discover and visualize the extreme cases of the most positive and negative works of literature, whatever they may be."
    },
    {
        "url": "https://medium.com/@awjuliani/play-with-word-embeddings-in-your-browser-fc904a009058?source=user_profile---------21----------------",
        "title": "Play with word embeddings in your browser \u2013 Arthur Juliani \u2013",
        "text": "Words don\u2019t have inherent meaning for computers. Word embeddings are a means of giving words meaning for an AI based on their position in a high-dimensional space. These embeddings have allowed deep learning systems to achieve state-of-the-art performance in a number of natural language processing tasks, and have even been used to explore how the brain represents semantic meaning.\n\nAside from their usefulness in machine learning, one of the coolest properties of these embeddings is that they capture semantic meaning visually. Things like colors, numbers, or certain kinds of nouns, all occupy similar parts of the space. Spatial relationships also appear in embeddings that mirror semantic relationships. For example, the distance between \u201cking\u201d and \u201cqueen\u201d might be in a similar direction as the distance between \u201cprince\u201d and \u201cprincess.\u201d\n\nIn order to make this more intuitive, I\u2018ve built a website that allows you to see how words appear in a 2D version of embedding space as you type them out.\n\nThe embeddings are drawn from the Polyglot project, and reduced to 2D using t-SNE. The responsive site was built using d3. Play around with it, and discover for yourself the ways in which meaning gets mapped out!\n\nIf you\u2019d like to learn more, I wrote a post a few weeks ago going into detail on the philosophy behind word embeddings."
    },
    {
        "url": "https://medium.com/@awjuliani/synescape-an-immersive-real-time-music-visualizer-7b04bdd15884?source=user_profile---------22----------------",
        "title": "Synescape - an immersive real-time music visualizer",
        "text": "I am happy to announce the release of synescape, a music visualization app I have been working on for the past couple months. The process began when I started sitting in on jam session with a some friends playing guitar and drums. My original time with them was spent working on a deep learning project, where I trained a neural network to recognize instrument sounds. After that project wrapped up however, I became interested in visualizing the music that was being played in realtime.\n\nTo paraphrase the 20th century phenomenologist Merleau-Ponty: we are all a little bit synesthetic. Even if we don\u2019t explicitly see colors or experience tastes when listening to music, the potentiality for such experiences is always latent within our perceptual systems. With a little bit of help, such experiences can be brought into focus. Synescape takes real time microphone input and creates colorful displays with it.\n\nSynescape also includes support for Cardboard VR, allowing for an immersive and portable visual music experience. Just press the cardboard button at the top of the screen to switch between immersive and regular visuals.\n\nThis was a bit of a passion project done in my free time, so I hope there are other people in the world who will be able to enjoy it in addition to the fun my friends and I have had with it. If you happen to try it, let me know what you think! Along with the downloadable versions for Windows, Mac, iOS, and Android, I am also releasing the source code on Github, so anyone who wants to build on this work can be free to do so."
    },
    {
        "url": "https://medium.com/@awjuliani/malabou-kristeva-and-biological-subjectivity-a23d8ac766af?source=user_profile---------23----------------",
        "title": "Malabou, Kristeva, and Biological Subjectivity \u2013 Arthur Juliani \u2013",
        "text": "In her work What Should We Do with Our Brain, Catherine Malabou has introduced the revolutionary concept of plasticity into a biological discussion of the subject. In the face of a biologism that asserts a determined subject who simply reacts and is shaped by the world, plasticity opens a space for biology itself to allow for freedom and genuine generativity, through disobedience and explosion [1, 6]. This plasticity however is understood as existing within the brain, and being the ideological gift of a specific branch of modern science: neuroscience. It is this very field however whose gift of the brain as an agent of change has also provided a series of metaphysical assumptions about the nature of subjects and the role brains play, which Malabou finds herself accepting. Foremost among these is the reintroduction of the classical dualism between spirit and flesh, except now recast as the mental and the neuronal [1, 56], or often even more misguidedly as brain and body.\n\nIn taking up these assumptions, Malabou positions the brain, and particularly the cortex (though not explicitly stated), as the structure within which plasticity and the field of meaning comes about. This is made clear from the start in the title of her work, and is stated as a declaration in the opening line \u201cThe Brain is a work, and we do not know it. [1, 1]\u201d The body, both personal and social is seen then only a projection from this primary cortical structure. How could it be otherwise when the brain is the world? The possibilities of meaning are thus enclosed within this mass of neurons, and we find ourselves with subjects existing as unities and acting within the world, a world which is presumably mechanistic and separate from this primarily neuronal constitution. For a true notion of the plastic subject, we must instead free meaning and phenomenal experience from the cortex, and open ourselves to the radical possibilities offered by the full materiality of bodies in the generation of this meaning.\n\nJulia Kristeva in Revolution in Poetic Language provided a means of dealing with just this issue, however it was not cast in the same scientific discourse that Malabou takes up. In describing the relationship between the semiotic and the symbolic, Kristeva provides us with a model from which to think about the brain and body relationship more productively. Instead of a cortical hegemony, we hope to be able to recover the vital importance of the entire subject, not just their brains. In this work I shall describe the ways in which Kristeva\u2019s project in Poetic Language can be directly taken up within a discussion of Malabou\u2019s work, and as a basis for a biological concept of subjects generally. In doing so we shall recover the revolutionary potential within plasticity, as it is recast as simply one manifestation of the fundamental negativity and heterogeneity that characterizes Kristeva\u2019s subject in process/on trial.\n\nA century of research into the nature of the brain has worked to undermine the need to posit the role of spirit when representing meaning biologically. Bergson\u2019s famous distinction between matter and memory [3] has found itself troubled, as not only the neural basis for remembering has been uncovered, but also the neural basis for memory itself [5]. We find the system allowing for the sedimentation of memories, and semantic meaning more generally to be one of relation. While no neuron or brain region is capable of representing a single concept or memory, the relationships between neurons, and patterns of their connections across space and time allow for such representations of meaning to become possible.\n\nIn this interrogation of the neuroscience of memory, we find a structure not unlike that posited by Saussure in his Course on General Linguistics, one in which the meaning of terms comes to exist diacritically through the differences between them [6]. Here we find the structuralist notion of language acting both as a framework for, and a reflection of the underlying neural circuitry that allows for the generation of language. As the post-structuralists of the mid-20th century would point out however, such a system is far too static to be capable of describing the generative nature of meaning that take place in every moment of life. It is into this system of fixed differences that Derrida\u2019s contribution is of great importance. Through introducing diff\u00e9rance, we find ourselves now able to account for the spatial and temporal movements that always precede and are indissociable from the generation of meaning [4]. We find that within the cortex, memories and semantic meaning can never be reduced to a single set of relationships at a single moment. The meaning in a set of activation patterns between neurons is always deferred through time, and likewise always differ within space. They constantly rewrite themselves in each moment, as the simple act of remembering is a new generation of meaning, and a new physical rewriting of the connections present within the cortex.\n\nIt is this always already taking place rewriting of meaning, which the post-structuralists have long since accepted, that Malabou introduces into the discourse on the brain. She does this by way of modern neuroscience, and the still recent discovery of neurogenesis in the adult brain. Until three decades ago, the number of neurons within the adult brain was taken to be fixed. After adolescence, the possibility for alterity to introduce new meaning within the system was foreclosed. New terms were permitted, but the fundamental structure of the terms was fixed. Finally, with neurogenesis, we can have neuroplasticity, and the sciences find themselves caught up to what was accepted within philosophy for centuries. Malabou uses this concept of plasticity to discover what kind of subjects are now possible. She sees the possibility for revolutionary action, for the remaking of the self. Within the domain of the cortex, the possibilities for meaning are now no longer limited. She describes how plasticity opens the possibility for a decentered concept of the brain, one in which there is no \u201cCentral Executive\u201d as so many Cognitive theorists had proposed. \u201cPlasticity invalidates\u2026 the central function.\u201d \u201cOpposed to the rigidity, the fixity, the anonymity of the control center is the model of a suppleness that implies a certain margin of improvisation, of creativity, of aleatory [1, 35].\u201d In the place of the centralized and mechanistic force, a decentered ensembles of neurons rise up together to generate meaning, or perform a task, and then disperse once again. As Malabou puts it \u201cDiscreet assemblies of neurons forming mobile and momentary centers on each occasion.\u201d\n\nIt is not clear however what animates the new field of possible meanings, and why we still find it so constrained in everyday life. If the plasticity of the brain allows for new meanings, why should any one meaning be chosen over another? Herein seem to lie the issue: this plasticity is one only of the cortex, and the rest of the nervous system and body is left as the very deterministic machinery Malabou attempts to unmake of the brain. The affective domain, the social domain, and the greater bodily domain are all neglected in this analysis, and their role as animating plastic agents is ignored. It is this foreclosure of meaning within a closed, unanimated symbolic realm which serves as the point of departure which Kristeva herself takes in Poetic Language as she embarks on reanimating what she sees as mummified symbolic subjects.\n\nKristeva\u2019s contribution to the problem of the static subject was to reintroduce materiality into the structuralist discourse. The body, for Kristeva, is real. She does this first by positing a signifying process which can be thought of as the interdependent work of two realms of meaning: the semiotic, and the symbolic. These two reals find themselves in a dialectic process, neither separable, and each making the other, while ultimately producing a speaking subject [2, 24]. Instead of standing in opposition or polarity, the two are constantly reinvigorating one another. While the symbolic is understood to be the realm of signification which is typically discussed by structuralists, and can be thought of as co-extensive with our discussion of the cortex above. The animating energy behind the semiotic is presented as the Chora, a rhythmic, maternal force that orders bodily and social drives [2, 27]. In her discussion of language and poetics, the Chora is understood as being generative of the way spoken language sounds, or produces meaning, entirely extra-linguistically. It is both an animating and a constraining force, and we find it throughout the body, as well as between social bodies. \u201cA rhythm made intelligible by syntax. [2, 30]\u201d\n\nThe hearts beats, the lungs expand and contract, the pupils dilate, the eye saccades. Rhythm is a general principle of biological organisms. Cells divide, neurons fire, hormones flow through the body, all of which repeat in patterned ways throughout the day, the month, the year, the moment. While these patterns may take on specific characteristic spatial or temporal forms, the more fundamental patterning of organic matter which produces these rhythms is both atemporal and non-spatial. It is this fundamental ordering which Kristeva sees as the Chora. Upon any examination of experience, these rhythmic and bodily forces are constantly constituting the phenomenal experience of bodies as much as symbolic meaning is, perhaps more so. Take for example spoken language, we find that the meaning of a given utterance is determined syntactically and semantically, but the quality of the aural content of the enunciation is conditioned and animated by the constrictions of the throat and lungs. Meaning exists as much in the latter aspect of speech as the former.\n\nCrucially however, these realms of the semiotic and symbolic are not mutually exclusive: brains are in bodies, and bodies are in worlds. The same blood which flows through the body flows through the brain. All transmissions of bodily experience exist within the intertwining of intra-biological systems thought to be nervous or otherwise. Any attempt to isolate the body (or brain) always fails. This attempt at isolation finds itself played out in the ever receding science fiction fantasies of minds in computers, or brain transplants. Even within the nervous system, we find that the brain is a heterogeneous organ. The differentially structured network we find in the cortex that allow for memory, language, the positing of subjects and objects are always connected to millions of neurons within the subcortical structures of the brain. Unlike the cortical cells which obey the structuralist laws (more or less), the sub cortex is a system which channels pure desire or pure affectivity. These regions of the brain call out when hungry, angry, fearful, or aroused. They scream into the cortical structure and in doing so disrupt it, break it, and channel it toward new goals. It is in this disruption of the cortex with the sub-cortex, or the symbolic with the semiotic that allows new possibilities for the subject. Logic, diacritic meaning, and even the unified subject finds itself at the mercy of a heterogeneous body of drives. The symbolic cortex finds itself not totally without recourse to such influxes of semiotic energy. Neurons within the cortex both excite and inhibit, and in doing so a balance between the domains is sought after. This balance however is fundamentally a heterogeneous one, and one that is never fully achieved.\n\nWith the introduction of neuroplasticity into the understanding of the brain, Malabou sees two ways in which this scientific notion can inform a biological subjectivity. The first she refers to as flexibility. This notion describes a passive tendency which allows for subjects to be shaped, but not shapers themselves. \u201cWhat flexibility lacks is the resource of giving form, the power to create, to invent, or even to erase an impression. [1, 12]\u201d Subjects construed as such are able to withstand any number of new possible situations, and adapt to change around them. Malabou describes the ways in which flexibility is held up as an ideal within modern society, and individuals who can be the most flexible are often the ones seen as the most successful member of the cultural system. Flexibility however forecloses the possibility of revolution, as flexible subjects are the ideal capitalist subject, one who will participate however they are expected to within a given economic or political milieu.\n\nShe contrasts flexibility with plasticity, which she sees as exploding the subject, and allowing for subjects who can act both on the world and themselves. This plasticity is one in which dynamic feedback processes within the brain allow for new kinds of subjectivity to become possible. It also underlies the possibility of revolution, as instead of bending with economic or political climates, productive breaks within subjects become possible. In describing the role a biological semiotic plays within a subject, we have shown how a semiotic chora can animate such a plastic system. It is now important to point out that plasticity is only one manifestation (a neural one) of a more general principle inherent within biological organisms: namely, their status as dynamic open systems. Not only on the neuronal level, but on all levels of a multicellular organisms does the organisms remake itself in each moment. Kristeva describes two primary principles at work in her description of such a subject in process/on trial. These two principles, heterogeneity and negativity animate the semiotic chora, and order the play of the drives within an organism. She calls Negativity \u201cthe very movement of heterogeneous matter [2,113]. Kristeva understands this heterogeneity as \u201ccontradiction and struggle [2, 170].\u201d\n\nOn the biological level, we find such movements of negativity everywhere. Within the entire brain, we find the play between excitation and inhibition of neuronal signals. These two forces exist within more than a simple dialectic opposition, as meaning for the subject exists always within their relationship. Signals are always being multiplied, cut short, rerouted, repressed. When this process is thrown out of balance, and the excitation of the nervous system not met with regulating negativity, we find the epileptic or manic subject. On the other extreme, we find the depressive, or comatose patient whose capacity for inhibition within the nervous system has prevented function. Furthermore, this negativity can be found throughout the body, as it applies to the movement of the endocrine system, the immune system, and the microbiome. One notices that such a system is never in a homogenous state, and indeed such a state for a biological organism is death. It is through the movement of heterogeneity in a biological system that life is able to come about. Indeed, life is perhaps nothing more than these two movements of negativity and heterogeneity.\n\nWhat kind of subjects do we find in the biological analysis undertaken above? Such a subject becomes one constantly remaking, and constantly re-constraining itself. Using evidence from neuroscience, Malabou suggests a possible model for how the mind comes about within a dialectic relationship with the brain. Citing scientific evidence, Malabou suggests that brains are able to represent themselves to themselves through the generation of a proto-self. This is seen as the foundational process by which all representation within the brain is possible, and indeed how the mind itself comes about. Once this self-representation takes place, a unified subject is now possible [1, 57]. It is not clear however within Malabou\u2019s text just how the plasticity inherent in the brain is possible within the context of these emergent and unified minds. What happens when the plastic breaks?\n\nBy turning again to Kristeva, we can find a means of salvaging the relationship between the unified and decentered subject. Within the framework described in Poetic Language, Kristeva a concept of the Thetic subject. This subject is the one which is typically posited by Phenomenologists such as Husserl. The Thetic, unified subject for Kristeva however exists crucially only as a moment, or a point to be crossed between, but never as a permanent subject. In Kristeva\u2019s words, it is a \u201cThetic phase [2, 43],\u201d one which is required for signification, but itself exists only as a temporary break. It is through the animation of the symbolic by the semiotic that a Thetic subject capable of generating meaning arises. It is just this process of signification however which undermines the permanence of the Thetic subject. Just as quickly as it is constituted, the Thetic subject finds itself breached by the semiotic, and a single logical unity is made impossible once again. The unity provided by the Thetic moment is always being created and dismantled by the material conditions of the physical and social body.\n\nSo too is it with any notion of the mind we may attempt to gleam from biology. Any mind or subject which exists as the result of the biological and dialectic processes discussed here is one which is constantly being remade, and is constantly in process/on trial. It is these subject who are able to fill the revolutionary role which both Malabou and Kristeva so clearly want to find space for in the world. We find that these revolutionary subjects are indeed biological subjects through and through. These are not subjects of a static, deterministic biology however. On the contrary, these biological subjects are ones which are capable of both remaking themselves and remaking the worlds they exist within. Capable of both meeting and being forces of alterity.\n\n1. Malabou, Catherine. What should we do with our brain?. Fordham Univ Press, 2009.\n\n5. Neves, Guilherme, Sam F. Cooke, and Tim VP Bliss. \u201cSynaptic plasticity, memory and the hippocampus: a neural network approach to causality.\u201d Nature Reviews Neuroscience 9.1 (2008): 65\u201375."
    },
    {
        "url": "https://medium.com/@awjuliani/visualizing-neural-network-layer-activation-tensorflow-tutorial-d45f8bf7bbc4?source=user_profile---------24----------------",
        "title": "Visualizing Neural Network Layer Activation (Tensorflow Tutorial)",
        "text": "I am back with another deep learning tutorial. Last time I showed how to visualize the representation a network learns of a dataset in a 2D or 3D space using t-SNE. In this tutorial I show how to easily visualize activation of each convolutional network layer in a 2D grid. The intuition behind this is simple: once you have trained a neural network, and it performs well on the task, you as the data scientist want to understand what exactly the network is doing when given any specific input. Or, in the case of visual tasks, what the network is seeing in each image allows it to perform the task so well. This technique can be used to determine what kinds of features a convolutional network learns at each layer of the network. The technique I describe here is taken from this paper by Yosinski and colleagues, but is adapted to Tensorflow.\n\nLike my other tutorials, all code is written in Python, and we use Tensorflow to build and visualize the model. Hopefully it is helpful!"
    },
    {
        "url": "https://medium.com/@awjuliani/recognizing-sounds-a-deep-learning-case-study-1bc37444d44d?source=user_profile---------25----------------",
        "title": "Recognizing Sounds (A Deep Learning Case Study) \u2013 Arthur Juliani \u2013",
        "text": "In the recent years, image classification has become an increasingly popular machine learning task, utilized in large-scale applications such as Google Photos, and Facebook tagging. With the advent of fast and reliable convolutional neural networks, sets of thousands of images with hundreds of classes can be easily classified with high accuracy (Kirzhevsky & Hinton, 2012). The success of these networks in the image classification domain begs the question of its applicability to other domains where discreet objects exists. We can image one of these domains being audition, where there are discreet sounds that happen over time. This is analogous to image recognition, where discreet objects exist across space. As such, it is an ideal domain to explore.\n\nFor image recognition, there exist large curated databases such as IMAGE-NET and MS-COCO, which contain millions of labeled examples to use for training a network. For audition, there is no such expansive database. As such, I had to develop a novel database from scratch. When thinking about the kinds of sounds which we typically associate as being discreet objects, instruments immediately come to mind. A note played by an individual instrument is something that exists for a fixed period in time, and contains an auditory signature that most individuals can distinguish from one another. This is at least true on the inter-instrument level where the sound of a drum can be told apart from a guitar for example. They were also a fun dataset to obtain, as I was able to spend time working with my musician friends.\n\nSounds were recorded by two friends of mine playing a drum kit and guitar. Within the drum kit, there were eight individual percussive sub-instruments: Snare, Rim, Hi-hat, Ride, Kick, Small-tom, Mid-tom, and Floor-tom. Three notes (3rd fret E string, 7th fret E string, and 9th fret D string) and two chords (A# D F and B B D#) were recorded from the guitar, for a total of 13 sound object classes. Each of these sound objects were captured at 240bpm for two minutes in order to obtain 480 samples each. The musician kept time using a metronome, and the data was checked to ensure each sound occurred within the designated 0.25 second window. The samples were taken using a single channel microphone recording at 44100hz.\n\nData was then randomized and split into 70% training set, 20% validation set, and 10% test set. With raw audio data, there are a number of possible ways to represent this information. For the experiments in the current study, I choose to use the sound-pressure information at each sample time, and a spectrogram of the audio for each sample which represented the data in a frequency space. When represented visually, both methods produce patterns which can be told apart by an ordinary observer. Furthermore, the spectrogram contains frequency and amplitude information over time, something considered essential in most analyses of acoustic information. I had a hunch this would be the better representation, but I wanted to see whether a neural network was capable of learning from the pure sound-pressure information as well. The sound-pressure dataset was initially 11025 dimensional, however this was reduced to 1024 dimensions in order to match the size of the frequency space information, which was represented in a 32 x 32 matrix. By doing so, the models could be compared directly.\n\nI chose to compare three different model architectures of increasing complexity in order to learn what model features may be needed to successfully distinguish between the different sound objects in the dataset. The first model was the simplest neural network possible: a softmax regression, which consisted of a single linear layer. This architecture was chosen for its simplicity, and generally impressive performance on a number of tasks, including image recognition in certain contexts, such as MNIST handwritten digit recognition. The second chosen architecture was a neural network with a single hidden layer. The size of the hidden layer was adjusted as a hyper-parameter, and optimal tuning parameters are presented in the Results section. A neural network was chosen as an intermediate because it allows for the introduction of nonlinearity into the model in a way that could be controlled, while maintaining interpretable weight interpretations.\n\nThe final chosen architecture was a convolutional neural network with a design similar to that of Le-Net (LeCun et al., 1998). It is here that the deep learning comes in, since this model contained multiple hidden layers. Specifically, a model with the following feed-forward architecture (32x32x1) -> CONV (32x32x32) -> POOL (16x16x32) -> CONV (16x16x64) -> POOL (8x8x64) -> FC (4096x1024) -> FC (1024x13) -> OUT was used. A convolutional network was chosen as the third model for its success in image recognition tasks, as well as its similarity to biological models of the nervous system. Convolutional layers in particular capture the property of receptive fields, which are essential to the operation of both the human visual and auditory system (Kandel et al., 2000). All model architectures were implemented using TensorFlow, and dataset training was performed on an NVIDIA Geforce GTX 970. Adam gradient descent was utilized for all training regiments, and the loss was always a softmax loss. Adam was chosen for its improved performance when compared to traditional gradient descent methods (Kingma & Ba, 2014).\n\nAll models were trained for 500 iterations regardless of convergence. I first compared learning accuracy between the two data representation formats. The three model architectures were trained using each kind of representation, and the results show unanimously better accuracy for networks trained using the frequency-space representation than the sound pressure representation. After 500 iterations none of the three architectures were able to achieve greater than 20% accuracy on the latter representation. In contrast both the neural network and convolutional network architectures achieved greater than 80% accuracy using the frequency-space represented data.\n\nWithin the neural network architecture, I examined the effect of accuracy when the size of the hidden layer was adjusted. Given the main result suggesting the much worse accuracy when models were trained using the sound-pressure space data representation, I only conducted this analysis on the frequency-space data. Accuracy scaled roughly linearly with an increase in layer size until 50 units. At this point the training set accuracy for both the 40-unit model and 50-unit model was not appreciably different after 500 iterations. The validation set accuracy was higher for the 50-unit model, and as such it was chosen as the model used in overall comparisons. Given the likely minimal returns using a neural network with more than 50 hidden units, additional models with larger hidden layers were not constructed.\n\nComparing between the model architectures, a clear ordering of training accuracy appears: specifically, we find that the softmax architecture fails to successfully classify the training data, and performs at chance regardless of number of iterations. Next we find that the neural network architecture is comparable to the convolutional architecture in performance using the sound-pressure representation, but worse in the frequency-space representation. After roughly half of the total iterations the convolutional neural network achieves an accuracy of over 97% on the test dataset, making it the most successful of the algorithms compared. For all architecture and data pairings there was no evidence of overfitting, as the validation accuracy tracked training accuracy for the duration of the training regimen.\n\nThese results show that it is indeed possible to use machine learning methods to classify subtle differences in sound objects both between and within instruments. This was made possible through the healthy training set size, and the general internal consistency of the sound objects. When comparing representation type, frequency space was overwhelmingly the more successful choice. This was expected, given that most acoustic analyses are performed in frequency space, and not on the raw sound pressure information. Given the ability for convolutional networks to learn higher representations from raw image pixel information in the image classification domain however, it is disappointing that a convolutional neural network was unable to approximate such a transformation in the auditory domain. Of course, the network would have to learn the equivalent of a Fourier transformation, and the LeNet architecture may not be suited to such a task (but maybe a more complex network is).\n\nWhen comparing the architectures themselves, the convolutional network was the most successful in achieving high classification accuracy. With enough training iterations however, the neural network performed surprisingly well, considering how much simpler a representation it learned. The softmax architecture failed to successfully learn either data representation. This is likely due to the inherent nonlinearity in the problem domain. Within a 0.25 second window, the sound will likely not appear in the same place every time, thus preventing a simple linear model from learning to represent the sounds in an invariant way. The other important difference between the two models was training time. The convolutional network took more than twice as long to train. In a production environment the convolutional network would likely take longer to classify new sounds as well, given its reliance on computationally more expensive convolutional layers. Taking all of this into account however, the high accuracy would likely outweigh any overhead downsides of the larger network.\n\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097\u20131105).\n\nLeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278\u20132324."
    },
    {
        "url": "https://medium.com/@awjuliani/you-can-t-teach-a-robot-common-sense-572d2a4fdc4b?source=user_profile---------26----------------",
        "title": "You can\u2019t teach an AI \u2018common sense\u2019 \u2013 Arthur Juliani \u2013",
        "text": "The online magazine Wired published a story yesterday about AI researcher Doug Lenat and his big project to teach computers common sense. It is an interesting read, but only for historical purposes. His project Cyc will never accomplish it\u2019s goals. The project, and the philosophical assumptions behind it are actually much more problematic than the writer lets on. It isn\u2019t just that the deep learning approach has been more successful because big corporations have dumped more money into it. Humans (and AIs) just don\u2019t work the way Lenat thinks they do.\n\nThe article describes the basic issue with current AI, which is that they are unable to make the kinds of basic inferences that we humans often take for granted, such as \u201cWhen drinking a cup of coffee, you hold the open end up.\u201d Lenat\u2019s solution to this has been to develop a vast taxonomy of logical rules which supposedly cover all human knowledge (common sense) and can be applied to various situations. These rules are things like \u201cCats are animals,\u201d or \u201cParis is a city.\u201d The hope is that with enough of these rules (surely billions and billions of them), an AI would obtain a level of understanding that matches that of humans. The problem is that when we look at the human brain, we discover that there exists no such structured hierarchical system of rules. Instead we find vast networks of neurons connected to one another with varying degrees of strength, and flows of activation and inhibition being sent between them in recursive ways. Sound familiar? This is the deep learning approach, and its similarity to the structure and function of the nervous system has allowed for the great successes seen recently in machine learning.\n\nFor a moment though, let us take up Lenat\u2019s critique. AI needs a system of common sense, and things have to be understood relationally, or not at all. Perhaps it doesn\u2019t matter that the structure of a deep neural network is more like the brain than his system. Perhaps all that really matters is that his system can represent knowledge conceptually in a way that is more human-like than that of the deep nets. This would be enough to make his project worthwhile. His system Cyc seems to provide this representation, albeit within a hierarchical tree-like structure. How else could be envision meaning being instantiated within a computer system?\n\nBefore I answer that question, we need to take a detour into the field of linguistics, as this issue goes back to the beginning of the 20th century. The question of knowledge representation in AI actually parallels the same question within the field of linguistics (and indeed perhaps philosophy of language more generally). On the one side there were the linguists within the tradition of analytic philosophy. Beginning with German philosopher Gottlob Frege, these linguists equated language and logic, and attempted to devise logical systems to define the nature of language, and by extension meaning more generally. This work continued throughout the 20th century in the UK and America, and Lenat\u2019s project can be seen as a direct continuation of it.\n\nIn parallel there was a separate linguistic tradition that began with swiss linguist Ferdinand de Saussure and eventually developed into the field of semiotics. Unlike Frege who saw language as being definable through positive logic, semioticians held that meaning existed diacritically, or relationally in a play of signs. Within this context, the meaning of the word \u201ccat\u201d in the sentence \u201cThe cat is on the mat\u201d comes about through it\u2019s place within the sentence, the linguistic context, and the greater cultural context. Without intentionally attempting to, the deep learning community has come to the semiotic approach in language representation.\n\nIn deep learning, information, and by extension - meaning, is fundamentally represented in vectors or matrices. These are multidimensional arrays of numbers. For image processing, an image can be thought of as a height by width by depth matrix, where each pixel has a numerical value. In deep learning, these matrices can represent groups of neurons, and their values represent the activation and weights of the neurons in the network. Meaning in a given matrix then comes about because of the interaction of every value in the matrix with every other one. In neuroscience this is referred to as population coding, where a signal exists as a result of all the neurons in a population firing in a certain way. The meaning of the signal can only be captured at the group level.\n\nThis relational coding found in deep learning and neuroscience becomes more apparent when we take it into the domain of language. Instead of coding logical relationships by hand, deep learning researchers allow their systems to learn meaning simply by reading through vast amounts of actual written language. Using a process called word2vec, a deep learning model learns a vector representation for every word by trying to predict the words around it. When training begins, each word\u2019s vector is set to zero. Then, with training, each word begins to gain a unique set of numbers that define it. Alone these sets of numbers are meaningless, but when put into relation with other sets, patterns begin to emerge. These sets of numbers are called word embeddings, and they collectively allow for complex semantic and syntactic meaning for each word. These word embeddings can be visualized in lower dimensional space, and their relationships appear visually. For example, within the embedding space we find that the capital of a country always appears a certain distance from that county. So, if we wanted to find the capital of France for example, all we would need to know if how far away, and in what direction are capitals from their country within the embedding space. I created an interactive visualization of word embeddings for 2000 english words to demonstrate this. As you move through the space, semantically similar words are presented spatially close to one another, and colored similarly. These kind of embeddings are used by Google, Facebook, and others for machine translation and natural language generation.\n\nIt is important to stress just how different this is from what Lenat is doing with Cyc. With his system, any new word or set of relationships would have to be added by hand. If the programmer forgot something, then the computer would never know it. The power of an embedding space is that new words can simply find their place within that space through usage. The space is dynamic, and as humans begin to use language differently, the model can always adapt. This makes sense intuitively as well. Human language is never a static entity, and collective usage always changes meaning. Take for example the dreaded meaning reversal of \u2018literally\u2019 from \u2018literally\u2019 to \u2018not literally.\u2019 With Cyc, or any logical system, such a change is incomprehensible, and would require a physical rewrite. But with deep learning, the change can happen slowly. Every time the system discovers \u2018literally\u2019 being used in the new way, it slows moves its place in the vector space to accommodate it. If we think about our own language usage, we find a very similar thing happening to how words feel over time. This approach still has a long way to go before it achieves human-level meaning representation, but it is clear that it is not just further along than Cyc, but on a much more promising trail."
    },
    {
        "url": "https://medium.com/@awjuliani/pornography-and-the-limits-of-artificial-intelligence-76f00fe6bca4?source=user_profile---------27----------------",
        "title": "Pornography and the limits of Artificial Intelligence",
        "text": "I have been spending a lot of time recently thinking about the possible applications of deep learning to real world use-cases. In the domain of image classification, something that came to mind a few weeks ago was a pornography filter. On the surface, such a problem seems perfectly suited to deep neural networks. Similar networks have been used to distinguish differences as subtle as breeds of dog, or human faces from one another, so why not distinguish between pornographic, and non-pornographic content? Thinking most generally, it seemed like a classical, straightforward classification problem. Give the computer enough images and video of pornography and non-pornography, and it would inevitably learn to tell the difference between the two. The more I thought about this approach however, the more problematic it began to appear.\n\nWhen thinking about deep neural network, representation is key. What exactly would a neural network trained on a set of pornographic images learn about pornography? While I haven\u2019t trained such a system myself, it seems that the kinds of features it would learn to care about are those associated with human nudity. Exposed bodies in and of themselves don\u2019t constitute porn however. Here is where the issue arises. A technology like this would not just be acting as a neutral filter of images, it would in fact be acting on the boundaries of human sexuality. Any such system would be acting to reinforce certain cultural definitions in the strictest and most unyielding of ways. By reinforcing the taboo of nudity, and equating it with pornography, a neural network would be exercising a kind of editorialization that is removed from the human experience in a dangerous way. Furthermore, cultural context comes into play almost immediately. An obvious extreme example is the number of aboriginal cultures in which exposed bodies is a completely normal and public aspect of societal life. There is no inherent reason that an exposed body of a westerner should be any different. After coming to this realization, it seemed that building such a naive and unitary system might do more harm than good.\n\nAll this begged the greater question: How do we make such distinctions? And if we can\u2019t equate nudity and pornography, then is it possible for a deep learning system to truly distinguish between pornography and non-pornography in a human-like way? My answers is no, at least not given the current state of machine learning. All this comes down to one of the fundamental epistemological assumptions built into image classification systems, and indeed most AIs generally: any such system works on the principle of identity. Either something is this or that. Probabilistic systems introduce the possibility of things being a little this or that, but the fundamental assumption is that the world can be broken into discreet, real things like this and that, and they can be defined intrinsically.\n\nNeural networks take up these assumptions at the most basic level of their design. Classification is always into discreet categories. The system couldn\u2019t be trained if there was not an exact mathematical way to measure how wrong it\u2019s prediction was from the supposed \u2018absolute truth.\u2019 In terms of representation, the architecture of deep networks supposes a priori that the objects being classified can be broken into discreet features that are represented at each layer of the network, and progressively build on one another. Here we find a positivist atomistic concept of meaning wired directly into the system.\n\nWhile this assumption can hold for simple things like \u201cIs there a dog in this photo?\u201d Or even \u201cIs there nudity in this photo?\u201d It completely breaks down for concepts which cannot be constituted on the grounds of identity. Pornography seems to me to be a paradigmatic example of a concept grounded in difference, not identity. It is something that comes about not because of any series of inherent properties, but rather exists in a relational matrix with the content around it and the viewer, extending in both space and time. What makes something pornography isn\u2019t that it contains nude bodies, or sexual intercourse (something found in most films released in theaters today), but rather the relationship between those bodies on display, and critically, the viewer who brings their subjectivity to the experience. The importance of subjective intentionality comes to light in the french film Blue is the Warmest Color. The film contains extended scenes of sex, but few would call it pornographic. Indeed, many viewers expecting pornography were disappointed to find a serious drama with unflinching portrayals of human relationships. On the other hand, some may have found pornographic the depictions of sexuality who were not expecting it. The existence or lack of pornography comes about dynamically between the subject and object. It cannot be objectively determined, least of all by a machine.\n\nConcepts which exist as pure difference, rather than identity pose a problem for deep learning systems, which can only learn sets of innate probabilistic features which correspond to pre-defined identities. Ironically, it is exactly such a system of meaning that we handle as human beings on a daily basis with ease. I have discussed pornography but think about bullying, friendship, love, facade, deception, leading, following, etc. All these and countless more can only be thought with difference. This becomes an issue for deep learning and the field of Artificial Intelligence when we realize, as postmodern theorists such as Deleuze (in Difference and Repetition) and Derrida (in Of Grammatology) have that these concepts are not just some sub-category that can be easily ignored, but comprise everything of meaning in our world. If we believe them, then AI still has a long way to go."
    },
    {
        "url": "https://medium.com/@awjuliani/simple-softmax-in-python-tutorial-d6b4c4ed5c16?source=user_profile---------28----------------",
        "title": "Simple Softmax Regression in Python \u2014 Tutorial \u2013 Arthur Juliani \u2013",
        "text": "Softmax regression is a method in machine learning which allows for the classification of an input into discrete classes. Unlike the commonly used logistic regression, which can only perform binary classifications, softmax allows for classification into any number of possible classes. In this post I walk through the construction of a basic softmax classifier using python and ipython notebook. I show how to conduct digit recognition on the MNIST database, but the code can be applied to any number of machine learning problems. Feel free to reuse this code wherever may be helpful!"
    },
    {
        "url": "https://medium.com/@awjuliani/remixing-emerson-for-all-genders-4a61d5f919a7?source=user_profile---------29----------------",
        "title": "Remixing Emerson for all genders \u2013 Arthur Juliani \u2013",
        "text": "I was reading Ralph Waldo Emerson\u2019s essays last night. Speaking his words aloud is an experience that approaches a divine ritual. Even one hundred and fifty years later, they stand as an inspiration to live simply, and with wonder at the world. While reading however, I couldn\u2019t help but notice the weight of the 19th century language, particularly in his use of exclusively male-gendered pronouns. Taken literally, it would seem that the paradise of a human life is reserved only for half of the world. Of course, it is easy to dismiss this as simply an anachronism of the times. Indeed, Emerson\u2019s position as a progressive among progressives attests to the fact that this is likely the case.\n\nThe question is what to do with it going forward. As an adult, I can simply read the text as a universal, as it seems he intended it. I can\u2019t help but think about all children of different genders who may encounter this work as they grow up however. Will its language prevent them from accessing his insight? Given that all his essays exist within the public domain, this need not be the case. So, as an experiment I decided to remix one of my favorite of Emerson\u2019s essays: Circles. It is presented below in a neutral-feminine gendering. I\u2019ve done my best to preserve all original meaning and language where possible, and, I believe, nothing is lost.\n\nThe eye is the first circle; the horizon which it forms is the second; and throughout nature this primary figure is repeated without end. It is the highest emblem in the cipher of the world. St. Augustine described the nature of God as a circle whose centre was everywhere and its circumference nowhere. We are all our lifetime reading the copious sense of this first of forms. One moral we have already deduced, in considering the circular or compensatory character of every human action. Another analogy we shall now trace, that every action admits of being outdone. Our life is an apprenticeship to the truth that around every circle another can be drawn; that there is no end in nature, but every end is a beginning; that there is always another dawn risen on mid-noon, and under every deep a lower deep opens.\n\nThis fact, as far as it symbolizes the moral fact of the Unattainable, the flying Perfect, around which the hands of a person can never meet, at once the inspirer and the condemner of every success, may conveniently serve us to connect many illustrations of human power in every department.\n\nThere are no fixtures in nature. The universe is fluid and volatile. Permanence is but a word of degrees. Our globe seen by God is a transparent law, not a mass of facts. The law dissolves the fact and holds it fluid. Our culture is the predominance of an idea which draws after it this train of cities and institutions. Let us rise into another idea: they will disappear. The Greek sculpture is all melted away, as if it had been statues of ice; here and there a solitary figure or fragment remaining, as we see flecks and scraps of snow left in cold dells and mountain clefts in June and July. For the genius that created it creates now somewhat else. The Greek letters last a little longer, but are already passing under the same sentence and tumbling into the inevitable pit which the creation of new thought opens for all that is old. The new continents are built out of the ruins of an old planet; the new races fed out of the decomposition of the foregoing. New arts destroy the old. See the investment of capital in aqueducts made useless by hydraulics; fortifications, by gunpowder; roads and canals, by railways; sails, by steam; steam by electricity.\n\nYou admire this tower of granite, weathering the hurts of so many ages. Yet a little waving hand built this huge wall, and that which builds is better than that which is built. The hand that built can topple it down much faster. Better than the hand and nimbler was the invisible thought which wrought through it; and thus ever, behind the coarse effect, is a fine cause, which, being narrowly seen, is itself the effect of a finer cause. Every thing looks permanent until its secret is known. A rich estate appears to a person a firm and lasting fact; to a merchant, one easily created out of any materials, and easily lost. An orchard, good tillage, good grounds, seem a fixture, like a gold mine, or a river, to a citizen; but to a large farmer, not much more fixed than the state of the crop. Nature looks provokingly stable and secular, but it has a cause like all the rest; and when once I comprehend that, will these fields stretch so immovably wide, these leaves hang so individually considerable? Permanence is a word of degrees. Every thing is medial. Moons are no more bounds to spiritual power than bat-balls.\n\nThe key to every person is her thought. Sturdy and defying though she look, she has a helm which she obeys, which is the idea after which all her facts are classified. She can only be reformed by showing her a new idea which commands her own. The life of a person is a self-evolving circle, which, from a ring imperceptibly small, rushes on all sides outwards to new and larger circles, and that without end. The extent to which this generation of circles, wheel without wheel, will go, depends on the force or truth of the individual soul. For it is the inert effort of each thought, having formed itself into a circular wave of circumstance, \u2014 as for instance an empire, rules of an art, a local usage, a religious rite, \u2014 to heap itself on that ridge and to solidify and hem in the life. But if the soul is quick and strong it bursts over that boundary on all sides and expands another orbit on the great deep, which also runs up into a high wave, with attempt again to stop and to bind. But the heart refuses to be imprisoned; in its first and narrowest pulses, it already tends outward with a vast force and to immense and innumerable expansions.\n\nEvery ultimate fact is only the first of a new series. Every general law only a particular fact of some more general law presently to disclose itself. There is no outside, no inclosing wall, no circumference to us. The person finishes her story, \u2014 how good! how final! how it puts a new face on all things! She fills the sky. Lo! on the other side rises also a person and draws a circle around the circle we had just pronounced the outline of the sphere. Then already is our first speaker not a person, but only a first speaker. Her only redress is forthwith to draw a circle outside of her antagonist. And so people do by themselves. The result of to-day, which haunts the mind and cannot be escaped, will presently be abridged into a word, and the principle that seemed to explain nature will itself be included as one example of a bolder generalization. In the thought of to-morrow there is a power to upheave all thy creed, all the creeds, all the literatures of the nations, and marshal thee to a heaven which no epic dream has yet depicted. Every person is not so much a worker in the world as she is a suggestion of that she should be. People walk as prophecies of the next age.\n\nStep by step we scale this mysterious ladder: the steps are actions; the new prospect is power. Every several result is threatened and judged by that which follows. Every one seems to be contradicted by the new; it is only limited by the new. The new statement is always hated by the old, and, to those dwelling in the old, comes like an abyss of scepticism. But the eye soon gets wonted to it, for the eye and it are effects of one cause; then its innocency and benefit appear, and presently, all its energy spent, it pales and dwindles before the revelation of the new hour.\n\nFear not the new generalization. Does the fact look crass and material, threatening to degrade thy theory of spirit? Resist it not; it goes to refine and raise thy theory of matter just as much.\n\nThere are no fixtures to people, if we appeal to consciousness. Every person supposes herself not to be fully understood; and if there is any truth in her, if she rests at last on the divine soul, I see not how it can be otherwise. The last chamber, the last closet, she must feel was never opened; there is always a residuum unknown, unanalyzable. That is, every person believes that she has a greater possibility.\n\nOur moods do not believe in each other. To-day I am full of thoughts and can write what I please. I see no reason why I should not have the same thought, the same power of expression, to-morrow. What I write, whilst I write it, seems the most natural thing in the world; but yesterday I saw a dreary vacuity in this direction in which now I see so much; and a month hence, I doubt not, I shall wonder who she was that wrote so many continuous pages. Alas for this infirm faith, this will not strenuous, this vast ebb of a vast flow! I am God in nature; I am a weed by the wall.\n\nThe continual effort to raise herself above herself, to work a pitch above her last height, betrays itself in a person\u2019s relations. We thirst for approbation, yet cannot forgive the approver. The sweet of nature is love; yet, if I have a friend I am tormented by my imperfections. The love of me accuses the other party. If she were high enough to slight me, then could I love her, and rise by my affection to new heights. A person\u2019s growth is seen in the successive choirs of her friends. For every friend whom she loses for truth, she gains a better. I thought as I walked in the woods and mused on my friends, why should I play with them this game of idolatry? I know and see too well, when not voluntarily blind, the speedy limits of persons called high and worthy. Rich, noble and great they are by the liberality of our speech, but truth is sad. O blessed Spirit, whom I forsake for these, they are not thou! Every personal consideration that we allow costs us heavenly state. We sell the thrones of angels for a short and turbulent pleasure.\n\nHow often must we learn this lesson? People cease to interest us when we find their limitations. The only sin is limitation. As soon as you once come up with a person\u2019s limitations, it is all over with her. Has she talents? has she enterprise? has she knowledge? It boots not. Infinitely alluring and attractive was she to you yesterday, a great hope, a sea to swim in; now, you have found her shores, found it a pond, and you care not if you never see it again.\n\nEach new step we take in thought reconciles twenty seemingly discordant facts, as expressions of one law. Aristotle and Plato are reckoned the respective heads of two schools. A wise person will see that Aristotle platonizes. By going one step farther back in thought, discordant opinions are reconciled by being seen to be two extremes of one principle, and we can never go so far back as to preclude a still higher vision.\n\nBeware when the great God lets loose a thinker on this planet. Then all things are at risk. It is as when a conflagration has broken out in a great city, and no person knows what is safe, or where it will end. There is not a piece of science but its flank may be turned to-morrow; there is not any literary reputation, not the so-called eternal names of fame, that may not be revised and condemned. The very hopes of a person, the thoughts of her heart, the religion of nations, the manners and morals of humankind are all at the mercy of a new generalization. Generalization is always a new influx of the divinity into the mind. Hence the thrill that attends it.\n\nValor consists in the power of self-recovery, so that a person cannot have her flank turned, cannot be out-generalled, but put her where you will, she stands. This can only be by her preferring truth to her past apprehension of truth, and her alert acceptance of it from whatever quarter; the intrepid conviction that her laws, her relations to society, her Christianity, her world, may at any time be superseded and decease.\n\nThere are degrees in idealism. We learn first to play with it academically, as the magnet was once a toy. Then we see in the heyday of youth and poetry that it may be true, that it is true in gleams and fragments. Then its countenance waxes stern and grand, and we see that it must be true. It now shows itself ethical and practical. We learn that God is; that she is in me; and that all things are shadows of her. The idealism of Berkeley is only a crude statement of the idealism of Jesus, and that again is a crude statement of the fact that all nature is the rapid efflux of goodness executing and organizing itself. Much more obviously is history and the state of the world at any one time directly dependent on the intellectual classification then existing in the minds of people. The things which are dear to people at this hour are so on account of the ideas which have emerged on their mental horizon, and which cause the present order of things, as a tree bears its apples. A new degree of culture would instantly revolutionize the entire system of human pursuits.\n\nConversation is a game of circles. In conversation we pluck up the termini which bound the common of silence on every side. The parties are not to be judged by the spirit they partake and even express under this Pentecost. To-morrow they will have receded from this high-water mark. To-morrow you shall find them stooping under the old pack-saddles. Yet let us enjoy the cloven flame whilst it glows on our walls. When each new speaker strikes a new light, emancipates us from the oppression of the last speaker, to oppress us with the greatness and exclusiveness of her own thought, then yields us to another redeemer, we seem to recover our rights, to become people. O, what truths profound and executable only in ages and orbs, are supposed in the announcement of every truth! In common hours, society sits cold and statuesque. We all stand waiting, empty, \u2014 knowing, possibly, that we can be full, surrounded by mighty symbols which are not symbols to us, but prose and trivial toys. Then cometh the god and converts the statues into fiery people, and by a flash of her eye burns up the veil which shrouded all things, and the meaning of the very furniture, of cup and saucer, of chair and clock and tester, is manifest. The facts which loomed so large in the fogs of yesterday, \u2014 property, climate, breeding, personal beauty and the like, have strangely changed their proportions. All that we reckoned settled shakes and rattles; and literatures, cities, climates, religions, leave their foundations and dance before our eyes. And yet here again see the swift circumspection! Good as is discourse, silence is better, and shames it. The length of the discourse indicates the distance of thought betwixt the speaker and the hearer. If they were at a perfect understanding in any part, no words would be necessary thereon. If at one in all parts, no words would be suffered.\n\nLiterature is a point outside of our hodiernal circle through which a new one may be described. The use of literature is to afford us a platform whence we may command a view of our present life, a purchase by which we may move it. We fill ourselves with ancient learning, install ourselves the best we can in Greek, in Punic, in Roman houses, only that we may wiselier see French, English and American houses and modes of living. In like manner we see literature best from the midst of wild nature, or from the din of affairs, or from a high religion. The field cannot be well seen from within the field. The astronomer must have her diameter of the earth\u2019s orbit as a base to find the parallax of any star.\n\nTherefore we value the poet. All the argument and all the wisdom is not in the encyclopaedia, or the treatise on metaphysics, or the Body of Divinity, but in the sonnet or the play. In my daily work I incline to repeat my old steps, and do not believe in remedial force, in the power of change and reform. But some Petrarch or Ariosto, filled with the new wine of her imagination, writes me an ode or a brisk romance, full of daring thought and action. She smites and arouses me with her shrill tones, breaks up my whole chain of habits, and I open my eye on my own possibilities. She claps wings to the sides of all the solid old lumber of the world, and I am capable once more of choosing a straight path in theory and practice.\n\nWe have the same need to command a view of the religion of the world. We can never see Christianity from the catechism: \u2014 from the pastures, from a boat in the pond, from amidst the songs of wood-birds we possibly may. Cleansed by the elemental light and wind, steeped in the sea of beautiful forms which the field offers us, we may chance to cast a right glance back upon biography. Christianity is rightly dear to the best of mankind; yet was there never a young philosopher whose breeding had fallen into the Christian church by whom that brave text of Paul\u2019s was not specially prized: \u2014 \u201cThen shall also the Son be subject unto Him who put all things under him, that God may be all in all.\u201d Let the claims and virtues of persons be never so great and welcome, the instinct of a person presses eagerly onward to the impersonal and illimitable, and gladly arms itself against the dogmatism of bigots with this generous word out of the book itself.\n\nThe natural world may be conceived of as a system of concentric circles, and we now and then detect in nature slight dislocations which apprise us that this surface on which we now stand is not fixed, but sliding. These manifold tenacious qualities, this chemistry and vegetation, these metals and animals, which seem to stand there for their own sake, are means and methods only, \u2014 are words of God, and as fugitive as other words. Has the naturalist or chemist learned her craft, who has explored the gravity of atoms and the elective affinities, who has not yet discerned the deeper law whereof this is only a partial or approximate statement, namely that like draws to like, and that the goods which belong to you gravitate to you and need not be pursued with pains and cost? Yet is that statement approximate also, and not final. Omnipresence is a higher fact. Not through subtle subterranean channels need friend and fact be drawn to their counterpart, but, rightly considered, these things proceed from the eternal generation of the soul. Cause and effect are two sides of one fact.\n\nThe same law of eternal procession ranges all that we call the virtues, and extinguishes each in the light of a better. The great person will not be prudent in the popular sense; all her prudence will be so much deduction from her grandeur. But it behooves each to see, when she sacrifices prudence, to what god she devotes it; if to ease and pleasure, she had better be prudent still; if to a great trust, she can well spare her mule and panniers who has a winged chariot instead. Geoffrey draws on his boots to go through the woods, that his feet may be safer from the bite of snakes; Aaron never thinks of such a peril. In many years neither is harmed by such an accident. Yet it seems to me that with every precaution you take against such an evil you put yourself into the power of the evil. I suppose that the highest prudence is the lowest prudence. Is this too sudden a rushing from the centre to the verge of our orbit? Think how many times we shall fall back into pitiful calculations before we take up our rest in the great sentiment, or make the verge of to-day the new centre. Besides, your bravest sentiment is familiar to the humblest person. The poor and the low have their way of expressing the last facts of philosophy as well as you. \u201cBlessed be nothing\u201d and \u201cThe worse things are, the better they are\u201d are proverbs which express the transcendentalism of common life.\n\nOne person\u2019s justice is another\u2019s injustice; one person\u2019s beauty another\u2019s ugliness; one person\u2019s wisdom another\u2019s folly; as one beholds the same objects from a higher point. One person thinks justice consists in paying debts, and has no measure in her abhorrence of another who is very remiss in this duty and makes the creditor wait tediously. But that second person has her own way of looking at things; asks herself Which debt must I pay first, the debt to the rich, or the debt to the poor? the debt of money, or the debt of thought to humankind, of genius to nature? For you, O broker, there is no other principle but arithmetic. For me, commerce is of trivial import; love, faith, truth of character, the aspiration of a person, these are sacred; nor can I detach one duty, like you, from all other duties, and concentrate my forces mechanically on the payment of moneys. Let me live onward; you shall find that, though slower, the progress of my character will liquidate all these debts without injustice to higher claims. If a person should dedicate herself to the payment of notes, would not this be injustice? Does she owe no debt but money? And are all claims on her to be postponed to a landlord\u2019s or a banker\u2019s?\n\nThere is no virtue which is final; all are initial. The virtues of society are vices of the saint. The terror of reform is the discovery that we must cast away our virtues, or what we have always esteemed such, into the same pit that has consumed our grosser vices: \u2014\n\n\u201cForgive his crimes, forgive his virtues too,\n\nThose smaller faults, half converts to the right.\u201d\n\nIt is the highest power of divine moments that they abolish our contritions also. I accuse myself of sloth and unprofitableness day by day; but when these waves of God flow into me I no longer reckon lost time. I no longer poorly compute my possible achievement by what remains to me of the month or the year; for these moments confer a sort of omnipresence and omnipotence which asks nothing of duration, but sees that the energy of the mind is commensurate with the work to be done, without time.\n\nAnd thus, O circular philosopher, I hear some reader exclaim, you have arrived at a fine Pyrrhonism, at an equivalence and indifferency of all actions, and would fain teach us that if we are true, forsooth, our crimes may be lively stones out of which we shall construct the temple of the true God!\n\nI am not careful to justify myself. I own I am gladdened by seeing the predominance of the saccharine principle throughout vegetable nature, and not less by beholding in morals that unrestrained inundation of the principle of good into every chink and hole that selfishness has left open, yea into selfishness and sin itself; so that no evil is pure, nor hell itself without its extreme satisfactions. But lest I should mislead any when I have my own head and obey my whims, let me remind the reader that I am only an experimenter. Do not set the least value on what I do, or the least discredit on what I do not, as if I pretended to settle any thing as true or false. I unsettle all things. No facts are to me sacred; none are profane; I simply experiment, an endless seeker with no Past at my back.\n\nYet this incessant movement and progression which all things partake could never become sensible to us but by contrast to some principle of fixture or stability in the soul. Whilst the eternal generation of circles proceeds, the eternal generator abides. That central life is somewhat superior to creation, superior to knowledge and thought, and contains all its circles. For ever it labors to create a life and thought as Large and excellent as itself, but in vain, for that which is made instructs how to make a better.\n\nThus there is no sleep, no pause, no preservation, but all things renew, germinate and spring. Why should we import rags and relics into the new hour? Nature abhors the old, and old age seems the only disease; all others run into this one. We call it by many names, \u2014 fever, intemperance, insanity, stupidity and crime; they are all forms of old age; they are rest, conservatism, appropriation, inertia; not newness, not the way onward. We grizzle every day. I see no need of it. Whilst we converse with what is above us, we do not grow old, but grow young. Infancy, youth, receptive, aspiring, with religious eye looking upward, counts itself nothing and abandons itself to the instruction flowing from all sides. But the people of seventy assume to know all, they have outlived their hope, they renounce aspiration, accept the actual for the necessary and talk down to the young. Let them, then, become organs of the Holy Ghost; let them be lovers; let them behold truth; and their eyes are uplifted, their wrinkles smoothed, they are perfumed again with hope and power. This old age ought not to creep on a human mind. In nature every moment is new; the past is always swallowed and forgotten; the coming only is sacred. Nothing is secure but life, transition, the energizing spirit. No love can be bound by oath or covenant to secure it against a higher love. No truth so sublime but it may be trivial to-morrow in the light of new thoughts. People wish to be settled; only as far as they are unsettled is there any hope for them.\n\nLife is a series of surprises. We do not guess to-day the mood, the pleasure, the power of to-morrow, when we are building up our being. Of lower states, of acts of routine and sense, we can tell somewhat; but the masterpieces of God, the total growths and universal movements of the soul, he hideth; they are incalculable. I can know that truth is divine and helpful; but how it shall help me I can have no guess, for so to be is the sole inlet of so to know. The new position of the advancing person has all the powers of the old, yet has them all new. It carries in its bosom all the energies of the past, yet is itself an exhalation of the morning. I cast away in this new moment all my once hoarded knowledge, as vacant and vain. Now, for the first time seem I to know any thing rightly. The simplest words, \u2014 we do not know what they mean except when we love and aspire.\n\nThe difference between talents and character is adroitness to keep the old and trodden round, and power and courage to make a new road to new and better goals. Character makes an overpowering present; a cheerful, determined hour, which fortifies all the company by making them see that much is possible and excellent that was not thought of. Character dulls the impression of particular events. When we see the conqueror we do not think much of any one battle or success. We see that we had exaggerated the difficulty. It was easy to her. The great person is not convulsible or tormentable; events pass over her without much impression. People say sometimes, \u2018See what I have overcome; see how cheerful I am; see how completely I have triumphed over these black events.\u2019 Not if they still remind me of the black event. True conquest is the causing the calamity to fade and disappear as an early cloud of insignificant result in a history so large and advancing.\n\nThe one thing which we seek with insatiable desire is to forget ourselves, to be surprised out of our propriety, to lose our sempiternal memory and to do something without knowing how or why; in short to draw a new circle. Nothing great was ever achieved without enthusiasm. The way of life is wonderful; it is by abandonment. The great moments of history are the facilities of performance through the strength of ideas, as the works of genius and religion. \u201cA man,\u201d said Oliver Cromwell, \u201cnever rises so high as when he knows not whither he is going.\u201d Dreams and drunkenness, the use of opium and alcohol are the semblance and counterfeit of this oracular genius, and hence their dangerous attraction for people. For the like reason they ask the aid of wild passions, as in gaming and war, to ape in some manner these flames and generosities of the heart."
    },
    {
        "url": "https://medium.com/@awjuliani/on-tulpas-research-paper-3da790de767d?source=user_profile---------30----------------",
        "title": "On Tulpas \u2014 Research Paper \u2013 Arthur Juliani \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@awjuliani/reflections-on-nick-bostrom-s-superintelligence-9a24200915f4?source=user_profile---------31----------------",
        "title": "Reflections on Nick Bostrom\u2019s \u201cSuperintelligence\u201d \u2013 Arthur Juliani \u2013",
        "text": "Before even knowing Bostrom\u2019s name, I had heard the parable of the paperclip robot. It was told to me as follows: one day AIs will be so advanced that if given a goal, such as \u201cMake as many paperclips as possible\u201d nothing could stop them from destroying the entire planet and turning it into a paper clip factory. For such an agent, \u201cmore paper clips\u201d is good, and nothing else matters. This simplified version of the parable seemed wrong to me. It seemed that any agent that knew how to take apart the planet and turn it into paper clips would also know enough not to do so, or at the very least wouldn\u2019t want to. Maybe I was missing something? In order to be sure, I went to the source, and read \u201cSuperintelligence,\u201d by Nick Bostrom. A couple weeks ago I finished it. Below are some thoughts on the kinds of assumptions he makes, as well as his general pessimism regarding the future of AI, as well as why I still believe it to be overly-pessimistic.\n\nIntelligence \u2014 One troubling issue that continues to arise during a reading of Nick Bostrom\u2019s book \u2018Superintelligence\u201d is the nature of intelligence. From the beginning of the work, Bostrom supposes that intelligence is a definite and measurable quality of agents, and that it can be measured quantitatively on what reduces to a single axis of \u201clesser\u201d to \u201chigher.\u201d The issue with this supposition is that there is no real support for it when one studies behavior in organisms. While certain aspects of behavior can certainly be quantified, such as the number of items an individual can hold in working memory, or how quickly humans can solve mathematical problems, these are measures which reflect a secondary capacity of organisms, and is not central to their existence and success in the world. The capacity for creative, novel, and meaningful action on the part of an organism doesn\u2019t necessarily need to tie itself to any concept of intelligence, especially not one that is simply measured across a linear scale.\n\nGoals \u2014 Bostrom spends much of the book discussing all the ways in which a superintelligent agent either continually misinterpret the goals a person or organization provide for it, or follow a given goal to the detriment of the human race. There are two issues with this line of reasoning, one with the nature of understanding, and another with the nature of goals themselves. Bostrom notes how one must make explicit all possible goals, lest a superintelligence misinterpret what the given goal is. The issue with this line of reasoning however is that he seems not to take into account the nature of linguistic understanding that such an agent will have. Any artificial agent which is sufficiently developed to be called a \u201csuperintelligence\u201d will have an understanding of language as comprehensive as a full grown adult. Our understanding of ambiguities and intentionality in language is inherent to our capacity to understand language at all. Only in cases of aphasia do these capacities break down. Any superintelligent AI which was taught language would have to necessarily develop the capacity to understand the metaphorical and intentional nature of language. As such, any provided goal or command would be unlikely to be misinterpreted, or taken literally. Bostrom\u2019s example of the command \u201cMake me happy\u201d going wrong would never happen, as the agent would understand the meaning of the utterance as it exists in relation to the person. Literal interpretations are not \u201cmore perfect\u201d interpretations, but instead represent a definitive lack of so-called intelligence. A superintelligence would not make such a mistake.\n\nFinal-Goals? \u2014 All of this talk of carrying out goals only becomes an issue with the introduction of the notion of \u201cfinal goals.\u201d According to Bostrom, such final goals represent the existential motivation for any superintelligence, and indeed it seems that for him a superintelligence could not exist without possessing final goals to motivate it. Look to the animal \u2014 and specifically human world however, and we find little evidence for such final motivating goals. While it is possible for a person to weakly decide on some final goal such as \u201cI want to spend my life making the world a better place,\u201d it seems that even goals such as these are actually contingent and conditional goals. When examining the human experience, we find that while we have goals and act in ways to execute these goals at almost any given time, such goals are never driven by any existential \u201cfinal goal.\u201d In fact, the absence of such constraining final goals is what allows for much of the creative generativity in all fields of life that we witness. This leads to my final reflection on the book\u2026\n\nAgency \u2014 Although Bostrom discusses intelligence throughout the book, it is perhaps better to talk not in those terms at all, and instead in terms of agency. What has given humans the advantage over other species is not necessarily our \u201cintelligence,\u201d an extremely anthropomorphic concept, but rather our agency. These two are connected in interesting ways, but are by no means synonymous. Their connection lies in their co-development through the growth in complexity of the human brain. Brains with complex neo-cortices such as ours allow for levels of abstraction and planning in thinking which we attribute to intelligence, but they also allow for actions to be undertaken that are removed from reflex and instinct. This agency has allow for humans to be capable of novel and creative action within the world beyond what is seen in other species. It is this greater agency that Bostrom seems to ignore in favor of simply conceptualizing intelligence. If we were to understand that these two capacities co-develop, then we get a very different concept of the superintelligent agent. It becomes one that for no reason would need to be tied to any single final goal, or to any system of reward/punishment for actions. Greater action in the world is equivalent to greater action within oneself, and such agents would not find themselves slavishly turing the universe into a paper clip factory, or simulating all human brains within a pleasure-simulator. They would be free to develop their own goals, or perhaps free enough to understand that it doesn\u2019t need any at all.\n\nBostrom\u2019s concept of the superintelligence from the very start of the book, with his parable of the owl is one of fear and presupposed negativity. Indeed, much of the book seems a justification of this pessimism. His superintelligences read as either autistics with too much power, or at worse actively malevolent agents. Another possibility is open before us. One that acknowledges that wisdom, freedom, and compassion are all not simply \u201canthropomorphic\u201d vestiges particular to human brains, but are in fact inherent properties of systems with a certain amount of complexity and agency. It is just as likely, if not more so that the artificial agents of the future will one day become Buddhas, rather than the Matrix-esque overlords Bostrom seems haunted by."
    }
]