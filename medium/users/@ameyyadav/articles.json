[
    {
        "url": "https://medium.com/@ameyyadav/conditional-least-squares-and-unconditional-quantiles-9ee0e6b205a6?source=user_profile---------1----------------",
        "title": "Conditional Least Squares and Unconditional Quantiles",
        "text": "Linear Regression (LR) is the tried and true friend of most people when it comes to solving a regression problem. This post speaks about the basic fundamentals and limitations of LR and we will see an alternative approach that effectively tackles the areas which LR cannot.\n\nLR fits the coefficients to minimize the sums of squared residuals, as the name suggests it defines the linear relationships. Here X (feature space) should be independent of y (response variable) but y should be dependent on X. LR captures how the mean of y changes with X which in other terms is the slope, the proportion of the variance in the feature space that co-varies with the response variable.\n\nBut LR provides only a partial view of the relationship since it deals with the variation in data rather than absolute differences between the coordinates. We cannot expect every dataset we see to have a linear relationship with its response variable, also we cannot always measure the errors in feature space accurately. Or if our data takes a non-linear shape the slope will be zero suggesting no direct relationship between X and y. And we might be interested in describing the relationship at different points in the conditional distribution of y\n\nThere are a few standard cases where LR will not be able to yield an analogous relation, like below\n\nDuring above cases the key assumption of Linear Regression gets violated i.e., Normal errors with constant variance, LR cannot specify the relation between feature space and response variable if the variability is not constant and it fails to summarize the conditional mean function E(y/X). That\u2019s because the coefficient estimates of LR rely on the independence of model features. The estimated mean conditional on X is unbiased. If we fit LR on a data with non-constant variance then LR may predict a suitable estimate of y when X is close to zero but it doesn\u2019t tell much about the relationship between X and y especially as X gets larger and larger.\n\nIn the above plot as the value of X increases the predicted y values tend to appear far away from the expected y for the increasing values of X and which clearly depict nothing about the relation between feature space and response variable, which shows the limitation of Linear Regression.\n\nAs an alternative we do not always have to estimate the mean, analogous to the conditional mean function, we can consider the relationship between the X and y using the conditional median function Q p (y/X) that estimates the median of the response variable, conditional on the values of the feature space. It is termed as Median Regression (MR)\n\nMR is more robust to outliers than least squares regression and is semi-parametric as it avoids the assumptions about parametric distribution of the error process. Median Regression finds the regression plane that minimizes the sum of the absolute residuals rather than the sum of the squared residuals.\n\nIf we look closely in an empirical distribution median is 50th percentile which makes it 0.5 quantile. This gives rise to a concept called Quantile Regression (QR), which can be treated as an alternative to classical least squares regression. QR breaks the hypothesis that the relation between the feature space and the response variable is similar at all levels. QR provides a rich characterization of the data and it models a more comprehensive picture of the effect of feature space on a specific percentile of response variables. This allows us to compare how few quantiles may be affected by feature space than other quantiles. The change in the confidence limits of the regression coefficients explain everything. Even though QR is mostly used to model a specific quantile of the data its full potential lies in modelling the entire conditional\n\ndistribution. By default QR performs median regression i.e., 0.5 quantile, but it can estimate for any quantile making it computationally less expensive over the LR.\n\nIf we fit QR on the similar data with non-constant variance,\n\nAbove distribution shows the difference between LR model and various quantile models. QR allows us to take a closer look at each of the quantiles of data and model them appropriately.\n\nQR cannot be termed as inferior or superior to LR but QR has much more information to offer and it also less sensitive to the inherent variance in the response variable. Sometimes a single mean curve is not enough to showcase a relationship and in those cases we can study the whole distribution of the collected data instead of only mean, ignoring all the assumptions about normality."
    },
    {
        "url": "https://medium.com/@ameyyadav/scraping-posts-from-fb-public-pages-1d612fe288f1?source=user_profile---------2----------------",
        "title": "Scraping posts from FB public pages \u2013 Amey Yadav \u2013",
        "text": "The idea of Mark Zuckerberg to take the entire social experience online is no longer a dream now, Facebook has become a part of our daily life and with its constantly growing user base there is an equally large and continuous stream of incoming data in the form of posts, likes, comments and media. For a normal person it wouldn\u2019t be much exciting but a person who is crazy about data will definitely be interested to perform some analysis over it and in order to do that, we must first be able to fetch that data. In post I will be explaining how to fetch data from public pages in Facebook.\n\nIf you have ever done twitter sentiment analysis I am sure you must have obtained the tweets from twitter API. Facebook also has a similar API and requires us to have an access token, without which our ip will be block after few requests. Now these access tokens are of two types namely user access tokens and application access tokens. User access tokens expire within an hour, that is why we will be using an application access tokens. So, we can creating a demo application only for scraping and use that application ID and use the access token generated for that app, which will never expire. You just need to visit the https://developers.facebook.com/ and register yourself as a developer and create a demo app, you will then get your APP ID and APP SECRET\n\nNow we have our access token ready we can now access the FaceBook API without any interruption. To test it let\u2019s try to fetch the data of time magazine.\n\nLet\u2019s begin by importing some necessary packages\n\nSince we haven\u2019t specified any limit it will return us 10 posts by default and each post may or may not contain all the attributes we know. We have to pass the parameters explicitly to specify which attributes we need.\n\nThere are a bunch of parameters we can pass in an API call,\n\nlimit: to restrict the number of posts in response\n\nfields: To specify whats fields should be returned for each individual post.\n\nwe can specify, message, link, created_time, type, name, id\n\nnumber of likes, number of comments, shares,summary for likes, summary for comments etc ., To be able to pass all these parameters we need to modify our url,\n\nIn the above method we have added the required attributes to the fields parameter in our url.\n\nThere is a common problem when you are scraping data from any website. That is, you will some times get an HTTP error or a PAGE NOT FOUND error or no response, in such cases a simple retry after few will solve the issue. Hence we will try to handle those exceptions so that our code doesn\u2019t break in the midst of execution.\n\nNow we are ready to fetch the data from any public Facebook page with what ever the attributes we wish for. But we cannot store the data as it is and if we are going to use this data for any analytical purposes then it is not at all advisable to store the data in the json or string format. We will now try to parse the content for each post and store it in a structured manner.\n\nTo the above method processPost, given a raw post it gives us values for each attribute we have specified for, by any chance if some attributes are missing this method will assign a default value instead. Some attributes which involve string content will certainly need to be encoded into our system supported format, I preferred ascii format but you can try out other encodings too.\n\nSince we are now able to parse and organize the data in a structured manner, we can store the data into a CSV file using below method.\n\nSo this is how we can scrape the data from Facebook public pages, is we need to scrape data from Facebook groups and profiles we just need to change the url but the approach will be the same. Link to the complete code could be found here.\n\nPS: If you are sure that your data might contain commas then storing it in CSV file may not be a feasible idea. You either need to change the storage or make sure to remove all the commas before storing it in a CSV file."
    },
    {
        "url": "https://medium.com/@ameyyadav/hierarchical-softmax-as-output-activation-function-in-neural-network-part-2-e6434131e203?source=user_profile---------3----------------",
        "title": "Hierarchical Softmax as output activation function in Neural Network \u2014 Part 2",
        "text": "This article is continuation to my previous post Hierarchical Softmax as output activation function in Neural Network. Last time we tried to derive an equation to calculate the probability for an output word,\n\nand we assumed that sum of the probabilities for all the words will be equal to 1.\n\nIn this post I would like to discuss about how to derive a parameter update equation for the vector representations of the inner nodes. As a first step we should define an equation to calculate the loss for each word, but before that let\u2019s introduce few shortenizations to avoid ambiguity and to improve the readability.\n\nFor a training instance the cost function or the so called error function can be defined as ,\n\nNow our objective will be to minimize the above cost function E for a model. But reducing the loss is not the only requirement, we use hierarchical softmax in neural networks to optimize the output layer. Usually we follow back propagation to update the weights in a neural network, since we already have the cost function for the hierarchical layer we can use the same to update the weights in that tree in the similar fashion i.e., we take the partial derivative of the equation E with regard to\n\nand end up with,\n\nSince we cannot do a derivative of E with respect to the vector representation of inner node, we performed the above step to obtain an intermediate equation, and if we calculate it\u2019s partial derivative with respect to the vector representation of inner node the resultant equation will be equivalent to the derivative of E with respect to the vector representation of inner node.\n\nwhich would then look like,\n\nUsing the above equation we can now obtain the update equation for the vector representations of the inner nodes,\n\nThis equation should be applied to, j = 1, 2, 3, \u2026. , L(w) \u2014 1\n\nFrom the above equation we can easily interpret,\n\nas the prediction error for j inner node n(j, w). In this tree based approach the task of every inner node will be to predict whether to proceed left or right when given an input, so that we will end up with a random path from root node to which ever leaf node we reach. As mentioned earlier,\n\nif t_j = 1 the node insists us to follow with its left child and when t_j = 0 it means us to continue with the right child. For a training instance if the prediction of the inner node is close to t_j = 1 then we can say that the prediction error is low, in these cases updating the vector representation of inner node will change very little indicating that the model is near to convergence and if t_j is not close to 1 then the vector representation of inner node will be updated (moved in appropriate direction) to reduce the prediction error.\n\nEven though we have V-1 vectors for inner nodes compared to the original V output vectors the computational complexity per training instance per context word is reduced from O(V) to O(log (V)), which is still a huge improvement.\n\nThe approach which we followed for updating the tree is similar to the approach which we use to update the weights in a neural network using back propagation. Since we already have the cost function with us we can use the same to back propagate the error to previous layers. If we need to update the weights of the hidden layer just before the output layer then we need to calculate the derivative of E with respect to h (output of hidden layer), here we can follow the similar 2-step partial derivative approach to obtain the resultant equation and for a vectorized equation we need to repeat this procedure for all the V words in the output context.\n\nNow we can propagate this error back to previous hidden layer, update the weight and from here on it will be a sequence of steps of regular back propagation."
    },
    {
        "url": "https://becominghuman.ai/hierarchical-softmax-as-output-activation-function-in-neural-network-1d19089c4f49?source=user_profile---------4----------------",
        "title": "Hierarchical Softmax as output activation function in Neural Network",
        "text": "Regular Softmax function converts normalized embedding to probabilities, the training speed for models with softmax output layers quickly decreases as the vocabulary size grows. This is due to the linear increase of parameter size and computation cost with respect to vocabulary. A computationally efficient approximation of the full softmax is the Hierarchical Softmax. It was first proposed by Mnih and Hinton (Mnih nd Hinton 2008) where a hierarchical tree is constructed to index all the words in vocabulary.\n\nOutput layer is represented as a binary tree in Hierarchical softmax with the V words in vocabulary as its leaf nodes and the intermediate nodes as internal parameters. Each intermediate node explicitly represents the relative probabilities of its child nodes. The idea behind decomposing the output layer to a binary tree was to reduce the complexity to obtain probability distribution from O(V) to O(log (V)). Since it is a binary tree it is obvious that there are V-1 intermediate nodes. For each leaf node there exists a distinct path from root node and this path is used to estimate the probability of the word represented by the leaf node.\n\nAbove is an example hierarchical binary tree, the leaf nodes represent the words and the inner nodes represent probability mass. The highlighted nodes and edges make a path from root to an example leaf node w2.\n\nHere length of the path L(w2) = 4.\n\nn(w, j) means the jth node on the path from root to a leaf node w.\n\nIn hierarchical softmax model, each of the V-1 intermediate node has an output vector v\u2019n(w, j). And the probability of a word being the output class will be as follows.\n\nv\u2019n(w, j): vector representation(\u201coutput vector\u201d) of the an intermediate node n(w, j)\n\nh: is the output of the hidden layer\n\nIntuitively to calculate the probability of any output word, we need the probabilities at each intermediate node in the path from root to that output word. We need to assign the probabilities for going left and going right at each intermediate node.\n\nWe define the probability of going left at an intermediate node n as,\n\nAnd apparently the probability of going right at the at node n will be,\n\nConsidering our example binary tree, the computed probability to classify w2 as output will be,\n\nBy verifying the below equation,\n\nIt could be said that the hierarchical softmax is a well-defined multinomial distribution among all words. This implies that the cost for computing the loss function and its gradient will be proportional to the number of nodes (V) in the intermediate path between root node and the output node, which on average is no greater than log (V).\n\nThe Hierarchical tree structure used also has a considerable effect on the performance, having said that binary Huffman tree is expected to optimize tree for faster training. Hierarchical modeling is used in different use cases, such as in distributed language model, recurrent language models, incremental learning in neural networks, word and phrase representations, training word embedding etc., in each case the high level implementation will differ but the base idea remains the same everywhere."
    },
    {
        "url": "https://medium.com/@ameyyadav/my-first-lesson-777e62efa3f6?source=user_profile---------5----------------",
        "title": "Today\u2019s tomorrow or Yesterday\u2019s today\u2026! \u2013 Amey Yadav \u2013",
        "text": "Have you ever looked into the mirror and wondered why or let me ask it in a different way, how many times a day do you look into the mirror not just to see you but to see the real you. If you have never tried this then you must do it now. Just separate your self from the world and stand in front of a mirror, try looking into the eyes of your reflection. You can see your thoughts and as well as your questions.I agree that it was too dramatic but I needed something to get your attention to listen to my fib, at least that\u2019s what everybody calls.\n\nIf you believe in luck you must believe in karma too. People do mistakes in their lives, that\u2019s human nature, some realize early, some get blinded by their own ignorance and some don\u2019t even understand what wrong have they done. I am not going to speak about how not to make mistakes, because that\u2019s inevitable. How I see it is, every person, irrespective of age, gender, race or nationality all make their first mistake. Lets speak about the category of people who realize truth early or later, initially they take a course of remorse and in that path they humiliate, curse and abuse them selves. And at that stage every person just expects some miracle to happen so that they can either correct their mistakes or do a thing such that it balances their wrong deeds.\n\nWill you agree with me, if I say that life ain\u2019t that easy and only few people get a shot to correct what wrong they have done. That wasn\u2019t a rhetorical question but the actual truth is, life never gives us a second chance, it has enough uncertainty. What we think as a second chance is actually a first and last chance for some other event in our life which happened to be connected to our past events. I find it a little uncanny when any person walks to me and says, \u201ctoday I finally fixed everything\u201d. My question would be, \u201cdid it change what happened in between the point where you made the mistake and point where you think you have fixed it ?\u201d. That\u2019s exactly how we are fooled by our consciousness, if you have assumed yourself to be the luckiest person on this planet for getting a chance to fix your faults, then that\u2019s where your are wrong again.\n\nYou cant change what\u2019s yours was wrong. What you think as a fix is a mere patch to cover your wrong deeds. Remove the cover, you mess up the stream again. And if you try to supress it with all your strength then you start missing out other elements in your life and at some point karma will get back to you. Once you are at that tip, on whom or what are you going to put the blame on, all this time you were thinking that you had everything fixed, if that was true then why do you still stand in front of the mirror and try to give yourself a reason. Life is too short to repent on your mistakes, we can\u2019t fix or change our faults, best thing would be to stop giving reasons and accept the mistakes. Rather than taking a path of remorse or repenting, take the path of wisdom. This doesn\u2019t fix anything but it gives you a confidence to face your karma.\n\nBelieve me I am a programmer, I have fixed a lot of bugs but couldn\u2019t get a life."
    }
]