[
    {
        "url": "https://towardsdatascience.com/model-agnostic-meta-learning-maml-8a245d9bc4ac?source=user_profile---------1----------------",
        "title": "What is Model-Agnostic Meta-learning (MAML) ? \u2013",
        "text": "Artificial intelligence is trying to learn how to learn from the way humans learn. We can quickly and easily recognize a new object from just seeing one or few pictures of it, or even from only reading about it without having ever seen it before. We can learn quickly a new skill as well as master many different tasks. This seems easy for the human intelligence but for machines, it is quite a challenge to overcome. Berkeley AI Research Lab published a research paper introducing Model-Agnostic Meta-learning MAML which is a simple solution, yet so powerful and game-changing in meta-learning (or learning to learn). The objective of this article is to give you a clear understanding of this approach.\n\nDeep learning has a great success in mastering one task using a large dataset. But, what we really want to achieve is few shot-meta learning which is an algorithm that trains a neural network to learn many different tasks using only a small data per task.\n\nIn meta-learning, there is a meta-learner and a learner. The meta-learner (or the agent) trains the learner (or the model) on a training set that contains a large number of different tasks. In this stage of meta-learning, the model will acquire a prior experience from training and will learn the common features representations of all the tasks. Then, whenever, there is a new task to learn, the model with its prior experience will be fine-tuned using the small amount of the new training data brought by that task. But we don\u2019t want to start from a random initialization of its parameters because if we do so, it will not converge to a good performance after only a few updates on each task.\n\nModel-Agnostic Meta-Learning (MAML) provides a good initialization of a model\u2019s parameters to achieve an optimal fast learning on a new task with only a small number of gradient steps while avoiding overfitting that may happen when using a small dataset.\n\nIn the diagram above, \u03b8 is the model\u2019s parameters and the bold black line is the meta-learning phase. When we have, for example, 3 different new tasks 1, 2 and 3, a gradient step is taken for each task (the gray lines). We can see that the parameters \u03b8 are close to all the 3 optimal parameters of task 1, 2, and 3 which makes \u03b8 the best parameters initialization that can quickly adapt to different new tasks. As a result, only a small change in the parameters \u03b8 will lead to an optimal minimization of the loss function of any task.\n\nThere is no better way to understand MAMl than its algorithm:\n\nIn meta-training, all the tasks are treated as training examples p(T). So, we start with randomly choosing the parameters \u03b8, and we enter the first loop (while) that takes a batch of tasks from p(T). And for each task from that batch, we train the model f using K examples of that task (k-shot learning). Then, we get the feedback of its loss function, and test it on new example test set to improve the model\u2019s parameters. If we use one gradient descent update, then the adapted parameters for that task are:\n\nThe step size or learning rate \u03b1 is a hyperparameter.\n\nThe test error on the batch of tasks is the training error of the meta-learning process. And here is the meta-objective:\n\nBefore, we move to the next batch of tasks, we update the parameters of the model \u03b8 using Stochastic Gradient Descent SGD because we have batches here. The parameters of the model \u03b8 are updated as follows:\n\nAs we can see, the meta-gradient update contains a gradient through a gradient which can be computed using the Hessian-vector product.\n\nThen, we repeat the same process until we train all the batches.\n\nMAML can work on any model trained with a gradient descent including fully connected and convolutional networks, and it can be used with a variety of loss functions. What is really important, is its compatibility with different domains such as few-shot regression, image classification, and reinforcement learning, where it was able to outperform a number of existing approaches without expanding the parameters number.\n\nThe most popular approach to meta-learning is using a Recurrent Neural Network RNN as a meta-learner to train another model. For instance, (Ravi & Larochelle, 2017) proposed an LSTM based meta-learner to train a classifier in few-shot learning to get the appropriate parameters and a general initialization of these parameters. However, according to (Finn & Levine, 2017 ) written by the same authors of the MAMl paper, MAML is able to approximate any learning algorithm compared to recurrent learners.The MAML initializations are more resilient to overfitting to small datasets, and they are also more effective when the model is dealing with new tasks that were not in the training set.\n\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.\n\nRavi, Sachin and Larochelle, Hugo. Optimization as a model for few-shot learning. (ICLR), 2017.\n\nChelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. arXiv preprint arXiv:1710.11622, 2017."
    },
    {
        "url": "https://towardsdatascience.com/artistic-style-transfer-b7566a216431?source=user_profile---------2----------------",
        "title": "Artistic Style Transfer \u2013",
        "text": "This article is about Artistic Style Transfer or you can call it Neural Style Transfer too. It is interesting to know that deep learning can make some magical things with images. So, I\u2019ll try to give you a better understanding of this concept and how it works.\n\nWouldn\u2019t you like to have your picture painted by your favorite painter of all time? Sounds impossible to bring the artist back to life in case he or she is dead, but there is a quite good solution to transfer the painter\u2019s style to your favorite picture, as a result, it will look like it was painted in the style of the same painter.\n\nThe image below shows that we can take any picture (e.g. a dog) and any artwork (here we have Starry Night of Vincent van Gogh) and we can transfer the chosen style to the content of the first picture automatically.\n\nBasically, we\u2019re going to take two images as inputs :\n\nThe output image is the generated image that, at the end, will have the content of the content image and the style of the style image.\n\nWe\u2019re going to use a pre-trained Convolutional Neural Network such as VGG-Network because CNN is the right choice for image processing. Also, it allows us to extract separately the content and the style of an image, and that\u2019s exactly what we want. So, we\u2019ll pass the two images through VGG and we initialize the image to be generated at a random image.\n\nIn our model, information is so important and by using Max Pooling in CNN, we are throwing away a large number of pixel values of the previous layer and we are keeping only the highest values. So it is better to use Average Pooling because at least it uses all the data in order to get an average.\n\nOur objective here is to get only the content of the input image without texture or style and it can be done by getting the CNN layer that stores all raw activations that correspond only to the content of the image. It is better to get a higher layer, because in CNN, first layers are quite similar to the original image. However, as we move up to higher layers, we start to throw away much information about the raw pixel values and keep only semantic concepts.\n\nIn order to get the representations of the style image, we are going to compute the correlations between different types of neurons in the network using the Gram Matrix.\n\nSo, how does Gram Matrix work?\n\nLet\u2019s get the convolutional features of the style image at some layer of the network. As it shows below, we\u2019ll get a convolutional feature of volume C by H by W ( Channel by Height by Width). In other words, it\u2019s an H by W spacial grid and at each point of it, there is a dimensional feature vector.\n\nWe pick out two of these different feature columns (e.g. the pink and the blue dimensional vectors), then, we compute the outer product between them. As a result, it will give us a C by C matrix that has information about which features in that feature map tend to activate together at those two specific spatial positions.\n\nWe repeat the same procedure with all different pairs of feature vectors from all points in the H by W grid and averaged them all out to throw away all spatial information that was in this feature volume.\n\nNow, that we know how the gram matrix works, how can we compute it?\n\nThe loss function in style transfer is the content loss function plus the style loss function.\n\nIt is the squared-error loss between the feature representation of the original image and the feature representation of the generated image.\n\nWe apply the content loss at one layer.\n\nFirst, we minimize the mean-squared distance between the style representation (gram matrix) of the style image and the style representation of the output image in one layer l.\n\nSecond, we apply the style loss function on many different layers to get the total style loss:\n\nWe can change the hyperparameters to control how much we want to match the content versus how much we want to match the style.\n\nIn Style transfer learning, we are going to use a deterministic optimizer l-bfgs instead of Stochastic Gradient Descent or Adam because:\n\nAll the equations in this article are from the paper Gatys, Ecker, and Bethge, \u201cA Neural Algorithm of Artistic Style\u201d, arXiv, 2015"
    },
    {
        "url": "https://towardsdatascience.com/one-shot-learning-face-recognition-using-siamese-neural-network-a13dcf739e?source=user_profile---------3----------------",
        "title": "One-Shot Learning: Face Recognition using Siamese Neural Network",
        "text": "This article is about One-shot learning especially Siamese Neural Network using the example of Face Recognition. I\u2019m going to share with you what I learned about it from the paper FaceNet: A Unified Embedding for Face Recognition and Clustering and from deeplearning.ai. This way, you may save more time to go deeper into this topic if you are more interested in it. So let\u2019s get started!\n\nIn order to understand the reason why we have one-shot learning, we need to talk about deep learning and data. Normally, in deep learning, we need a large amount of data and the more we have, the better the results get. However, it will be more convenient to learn only from few data because not all of us are rich in terms of how much data we have.\n\nAlso, the brain doesn\u2019t need thousands of pictures of the same object in order to be able to recognize it. But let\u2019s not talk about the brain analogy because it is far more complicated and powerful, and many things are involved in our process of learning and memorization such as feelings, prior knowledge, and interactions, etc.\n\nThe idea here is that we need to learn an object class from only a few data and that\u2019s what One-shot learning algorithm is.\n\nIn face recognition systems, we want to be able to recognize a person\u2019s identity by just feeding one picture of that person\u2019s face to the system. And, in case, it fails to recognize the picture, it means that this person\u2019s image is not stored in the system\u2019s database.\n\nTo solve this problem, we cannot use only a convolutional neural network for two reasons: 1) CNN doesn\u2019t work on a small training set. 2) It is not convenient to retrain the model every time we add a picture of a new person to the system. However, we can use Siamese neural network for face recognition.\n\nSiamese neural network has the objective to find how similar two comparable things are (e.g. signature verification, face recognition..). This network has two identical subnetworks, which both have the same parameters and weights.\n\nThe image above is a good example of face recognition using Siamese network architecture from deeplearning.ai. As you can see, the first subnetwork\u2019s input is an image, followed by a sequence of convolutional, pooling, fully connected layers and finally a feature vector (We are not going to use a softmax function for classification). The last vector f(x1) is the encoding of the input x1. Then, we do the same thing for the image x2, by feeding it to the second subnetwork which is totally identical to the first one to get a different encoding f(x2) of the input x2.\n\nTo compare the two images x1 and x2, we compute the distance d between their encoding f(x1) and f(x2). If it is less than a threshold (a hyperparameter), it means that the two pictures are the same person, if not, they are two different persons.\n\nAnd this is working for any two images xi and xj.\n\nWe can apply gradient descent on a triplet loss function which is simply a loss function using three images: an anchor image A, a positive image P(same person as the anchor), as well as a negative image N (different person than the anchor). So, we want the distance d(A, P) between the encoding of the anchor and the encoding of the positive example to be less than or equal to the distance d(A, N) between the encoding of the anchor and the encoding of the negative example. In other words, we want pictures of the same person to be close to each other, and pictures of different persons to be far from each other.\n\nThe problem here is that the model can learn to make the same encoding for different images, which means that distances will be zero, and unfortunately, it will satisfy the triplet loss function. For this reason, we are adding a margin alpha (hyperparameter), to prevent this from happening, and to always have a gap between A and P versus A and N.\n\nThe max means as long as d(A, P) \u2014 d(A, N)+ alpha is less than or equal to zero, the loss L(A, P, N) is zero, but if it is greater than zero, the loss will be positive, and the function will try to minimize it to zero or less than zero.\n\nThe Cost function is the sum of all individual losses on different triplets from all the training set.\n\nThe training set should contain multiple pictures of the same person to have the pairs A and P, then once the model is trained, we\u2019ll be able to recognize a person with only one picture.\n\nIf we choose them randomly, it will be so easy to satisfy the constraint of the loss function because the distance is going to be most of the time so large. And the gradient descent will not learn much from the training set. For this reason, we need to find A, P, and N so that A and P are so close to N. Our objective is to make it harder to train the model to push the gradient descent to learn more."
    },
    {
        "url": "https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6?source=user_profile---------4----------------",
        "title": "Clustering using K-means algorithm \u2013",
        "text": "This article explains K-means algorithm in an easy way. I\u2019d like to start with an example to understand the objective of this powerful technique in machine learning before getting into the algorithm, which is quite simple.\n\nSo imagine you have a set of numerical data of cancer tumors in 4 different stages from 1 to 4, and you need to study all the tumors in each stage. However, you have no idea how to identify the tumors that are at the same stage because nobody had the time to label the entire set of features (most data in the world are unlabeled). In this case, you need K-means algorithm because it works on unlabeled numerical data and it will automatically and quickly group them together into 4 clusters.\n\nFor this example, we chose k=4 because, we already know, we have 4 tumors\u2019 stages, but if we want to cluster them based on their structure, growth speed, or growth type, then maybe k will be different than 4.\n\nIf you don\u2019t know how many groups you want, it\u2019s problematical, because K-means needs a specific number k of clusters in order to use it. So, the first lesson, whenever, you have to optimize and solve a problem, you should know your data and on what basis you want to group them. Then, you will be able to determine the number of clusters you need.\n\nBut, most of the time, we really have no idea what the right number of clusters is, so no worries, there is a solution for it, that we will discuss it later in this post.\n\nAs, you can see, k-means algorithm is composed of 3 steps:\n\nThe first thing k-means does, is randomly choose K examples (data points) from the dataset (the 4 green points) as initial centroids and that\u2019s simply because it does not know yet where the center of each cluster is. (a centroid is the center of a cluster).\n\nThen, all the data points that are the closest (similar) to a centroid will create a cluster. If we\u2019re using the Euclidean distance between data points and every centroid, a straight line is drawn between two centroids, then a perpendicular bisector (boundary line) divides this line into two clusters.\n\nNow, we have new clusters, that need centers. A centroid\u2019s new value is going to be the mean of all the examples in a cluster.\n\nWe\u2019ll keep repeating step 2 and 3 until the centroids stop moving, in other words, K-means algorithm is converged.\n\nK-means is a fast and efficient method, because the complexity of one iteration is k*n*d where k (number of clusters), n (number of examples), and d (time of computing the Euclidian distance between 2 points).\n\nIn case, it is not clear, we try different values of k, we evaluate them and we choose the best k value.\n\nDissimilarity(C) is the sum of all the variabilities of k clusters\n\nVariability is the sum of all Euclidean distances between the centroid and each example in the cluster.\n\nOr you can take a small subset of your data, apply hierarchical clustering on it (it\u2019s a slow clustering algorithm) to get an understanding of the data structure before choosing k by hand.\n\nChoosing poorly the random initial centroids will take longer to converge or get stuck on local optima which may result in bad clustering. in the picture above, the blue and red stars are unlucky centroids.\n\nThere are two solutions:"
    },
    {
        "url": "https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c?source=user_profile---------5----------------",
        "title": "Batch normalization in Neural Networks \u2013",
        "text": "This article explains batch normalization in a simple way. I wrote this article after what I learned from Fast.ai and deeplearning.ai. I will start with why we need it, how it works, then how to include it in pre-trained networks such as VGG.\n\nWe normalize the input layer by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, why not do the same thing also for the values in the hidden layers, that are changing all the time, and get 10 times or more improvement in the training speed.\n\nBatch normalization reduces the amount by what the hidden unit values shift around (covariance shift). To explain covariance shift, let\u2019s have a deep network on cat detection. We train our data on only black cats\u2019 images. So, if we now try to apply this network to data with colored cats, it is obvious; we\u2019re not going to do well. The training set and the prediction set are both cats\u2019 images but they differ a little bit. In other words, if an algorithm learned some X to Y mapping, and if the distribution of X changes, then we might need to retrain the learning algorithm by trying to align the distribution of X with the distribution of Y. ( Deeplearning.ai: Why Does Batch Norm Work? (C2W3L06))\n\nAlso, batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.\n\nTo increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.\n\nHowever, after this shift/scale of activation outputs by some randomly initialized parameters, the weights in the next layer are no longer optimal. SGD ( Stochastic gradient descent) undoes this normalization if it\u2019s a way for it to minimize the loss function.\n\nConsequently, batch normalization adds two trainable parameters to each layer, so the normalized output is multiplied by a \u201cstandard deviation\u201d parameter (gamma) and add a \u201cmean\u201d parameter (beta). In other words, batch normalization lets SGD do the denormalization by changing only these two weights for each activation, instead of losing the stability of the network by changing all the weights.\n\nVGG doesn\u2019t have a batch norm layer in it because batch normalization didn\u2019t exist before VGG. If we train it with it from the start, the pre-trained weight will benefit from the normalization of the activations. So adding a batch norm layer actually improves ImageNet, which is cool. You can add it to dense layers, and also to convolutional layers.\n\nIf we insert a batch norm in a pre-trained network, it will change the pre-trained weights, because it will subtract the mean and divide by the standard deviation for the activation layers and we don\u2019t want that to happen because we need those pre-trained weights to stay the same. So, what we need to do is to insert a batch norm layer and figure out gamma and beta in order to undo the outputs change.\n\nTo summarize everything, you can think about batch normalization as doing preprocessing at every layer of the network."
    },
    {
        "url": "https://towardsdatascience.com/simple-explanation-of-semi-supervised-learning-and-pseudo-labeling-c2218e8c769b?source=user_profile---------6----------------",
        "title": "Simple explanation of Semi-Supervised Learning and Pseudo Labeling",
        "text": "Semi-supervised learning uses the unlabeled data to gain more understanding of the population structure in general. Let\u2019s take the Kaggle State farm challenge as an example to show how important is semi-Supervised Learning. If you check its data set, you\u2019re going to find a large test set of 80,000 images, but there are only 20,000 images in the training set. In other words, we learn features only from a small training set because it is labeled, and we don\u2019t take advantage of the test set that contains a lot of valuable information because its images are unlabeled. As a result, we should find a way to learn also from the large unlabeled data.\n\nSo let\u2019s have a look at the picture below, the white and the black points are labeled data and the grey points are unlabeled. However, from the picture, we can tell that the unlabeled points are somehow able to give us some valuable information that will help us know more about the structure of the data.\n\nPseudo Labeling is a simple and an efficient method to do semi-supervised learning. It can combine almost all neural network models and training methods (Pseudo-Label). Here is an example of the steps to follow if you want to learn from your unlabeled data too:\n\nThis method will make the error decreases and it will improve the model by better learning the general structure.\n\nBut, how can I know the proportion of true labels and pseudo-labels in each batch? In other words, how much do I make it a mix of training vs pseudo? The general rule of thumb is to have 1/4\u20131/3 of your batches be pseudo-labeled.\n\nCan I use pseudo-labeling on pseudo-labeling? Yes\n\nHow do I know when to stop changing a model? We still don\u2019t know how to create optimal architectures nor when to stop messing with a model, So just keep trying."
    },
    {
        "url": "https://towardsdatascience.com/some-tricks-learned-from-kaggle-statefarm-competition-8419e032d1f1?source=user_profile---------7----------------",
        "title": "Some tricks learned from Kaggle StateFarm Competition",
        "text": "This article shows some handy tricks that I learned from the solution of the State Farm Challenge that was made by Fast.ai. You can find the code here.\n\nWe all know that we should always normalize our inputs (training set) by manually calculating the average, standard deviation of the input and subtracting it all out. However, there is a handy trick in Keras that will do exactly that for you. Every time you want to create a Keras model, simply, start with a batchnorm layer. BatchNormalization normalizes the activations of the previous layer at each batch. When using this layer as the first layer in a model, you should add the keyword argument input_shape.\n\nWe have to flatten the input into a single vector before using a Dense layer.\n\nAlways, use model.summary() to get information about the representation of your model, and that will help you understand it more and make good decisions.\n\nTo get the best out of the cross-entropy loss function, we use clipping. So we give an interval to numpy.clip. And, the values outside the interval are clipped to the interval edges. For example, if the interval is [0, 1], values smaller than 0 become 0, and values larger than 1 become 1.\n\nIt\u2019s a good practice to start with a sample of the entire training set, tune the hyperparameters (dropout rate, learning rate\u2026), test and modify different architectures. The benefit of using a sample first before using the whole dataset is to reduce computational time and see quickly which hyperparameters and design work well.\n\nYou may follow these steps:\n\nHere is an example in Keras:\n\nWhen we have overfitting, we should not immediately add dropouts or other regulations. However, it is recommended to follow these ordered steps while checking the overfitting:\n\n1- Add more data: if you still have more data to use, you should add it, but for example, if you\u2019re working on a kaggle competition, this step is irrelevant.\n\n2- Use data augmentation: is the process of creating additional synthetic data, by reasonably modifying your data. For instance, if your input data is images, you should not vertical flipping images of houses, because an upside down house is simply not common. Here are some types of data augmentation that you can do with images: flipping, rotation, cropping, zooming, panning, and minor color changes.\n\nSo, you should try each type at a time, and try different levels of it on a sample with enough validation set, maybe go for (4 or 5 levels, it\u2019s up to the time that you want to spend on looking for modifications that work). And finally, combine the types, each one with the best parameters, all together.\n\nLet\u2019s take look at some examples:( after each modification we run the model to see the accuracy)\n\n3- Use architectures that generalize well\n\n4- Add regularization: Regularization attempts to prevent overfitting and make the model performs well not just on the training data, but also on new inputs. There are different types of regularizations such as dropout, data augmentation, early stopping. If we are working with a sample, we should not use regularization because we cannot know how much regularization we need until we use the entire data set. And, we know that if we add more data, we need less regularization. So, if we use the whole dataset, we can add dropouts and run the model to see if there is any improvement.\n\nDropout: You may put dropouts in your dense layers, and if you want you can put small amounts of dropout in the convolutional layers as well. In the VGG model, they put 50% dropout after each of its dense layers, and that doesn\u2019t seem like a bad practice.\n\nIf you see improvement, don\u2019t give up yet and stop running your model, try to decrease the learning rate and run more few epochs, it will keep getting better and better.\n\nIt\u2019s where at least one class is under-represented relative to others. for instance, you create a classification model and get high accuracy, however, you discover that 90% of the data belongs to one class. You can learn more about imbalanced classes here."
    },
    {
        "url": "https://towardsdatascience.com/what-i-learned-about-convnet-from-fast-ai-lesson-4-ef0e4539f839?source=user_profile---------8----------------",
        "title": "What I learned about Convnet from Fast.ai lesson 4 \u2013",
        "text": "I wrote this post to share some of what I have learned about Convolutional Neural Networks from lesson 4 of @fastdotai.\n\nSoftmax and Sigmoid have the same functionality which is computing probabilities to determine the target class. Softmax is used for multi-classification and Sigmoid is used for binary classification.\n\nWe use Max pooling for two reasons:\n\nThe answer is nobody knows! Some people are against using Max pool and others aren\u2019t.\n\nYes, for example, you have a 3\u00d73 filter.When doing convolutions, instead of doing a convolution over every set of 9 pixels, skip a pixel each time you do a convolution.\n\nNo, It is not. CNN can be used for any type of data with consistent ordering such as audio, image, or some kind of consistent time series, and max pooling can be used for any kind of CNN.\n\nIf you want to know the derivative of a function, and maybe, you don\u2019t have time to figure it out. You can use WolframAlpha.com where you can type your formula and get your derivative back.\n\nWe are concerned with the differentiability of all the layers. And here\u2019s why. From the chain rule, a derivative of a function of a function is equal to the product of the derivatives of those functions. And, we know that the loss function consists of a function of a function, of a function, etc. For example, a Categorical Cross Entropy Loss is applied to a SoftMax applied to a ReLU applied to a Dense Layer, applied to a Max pooling, applied to a ReLU applied to Convolutions, etc., etc. And, in order to calculate the derivative of this loss function with respect to the inputs, we have to calculate the derivative of every layer with respect to its inputs and multiply them all together. And this is what\u2019s called backpropagation.\n\nSaddle points or \u201cshallow valleys\u201d are flat regions. They are critical points that are not maximum or minimum. At a saddle point, the gradient of the loss function often becomes very small in one or more axes, and this is what we want from the gradient descent when approaching a minima, however, with a saddle point, there is no minima. As a result, the gradient descent may take time to escape this flat region.\n\nMomentum helps to speed the optimization process toward the minimum of the loss and get out of saddle points by adding a running average of previous gradients and use that average instead of the current batch of data. It forces the gradient descent toward the correct direction to the loss by making the convergence faster and by reducing oscillations.\n\nThe learning rate is how quickly or how slowly a network updates old parameters for new ones. By default, the learning rate is held constant, however, this way may cause some issues such as:\n\nNo, but it would mean that every parameter has its own learning rate.\n\nAdaGrad (for Adaptive Gradient) is an optimization method that takes the average of the root sum of squares of all the previously calculated gradients for a parameter. As a result, if a parameter has a low gradient, the algorithm will barely modify its learning rate, and if a parameter has a high gradient, it will decrease it.\n\nThere is one problem with AdaGrad, a parameter will no longer learn any further, because it will be updated by a small amount and that is due to the fact that the division parameter is always increasing, and the learning rate is always decreasing.\n\nRMSprop ( for Root Mean Square Propagation) divides the overall learning rate by the square root of the sum of squares of the updated gradients for a parameter. It uses an exponentially weighted moving average of previous gradients. Consequently, newer values contribute more than old values.\n\nYes, it does. Adam (for Adaptive Moment Estimation) multiplies the learning rate by the recent weighted moving average of the gradients (the momentum), but also divides by the recent moving average of the root of the squares of the derivative (RMSprop)."
    },
    {
        "url": "https://medium.com/@phidaouss/convolutional-neural-networks-cnn-or-convnets-d7c688b0a207?source=user_profile---------9----------------",
        "title": "Convolutional Neural Networks (CNN, or ConvNets) \u2013 Firdaouss Doukkali \u2013",
        "text": "Convolutional Neural networks allow computers to see, in other words, Convnets are used to recognize images by transforming the original image through layers to a class scores. CNN was inspired by the visual cortex. Every time we see something, a series of layers of neurons gets activated, and each layer will detect a set of features such as lines, edges. The high level of layers will detect more complex features in order to recognize what we saw.\n\nThis article will present my brief notes about the elements that constitute Convolutional Neural Networks.\n\nConvNet has two parts: feature learning (Conv, Relu,and Pool) and classification(FC and softmax).\n\nThe objective of a Conv layer is to extract features of the input volume.\n\nA part of the image is connected to the next Conv layer because if all the pixels of the input is connected to the Conv layer, It will be too computationally expensive. So we are going to apply dot products between a receptive field and a filter on all the dimensions. The outcome of this operation is a single integer of the output volume (feature map). Then we slide the filter over the next receptive field of the same input image by a Stride and compute again the dot products between the new receptive field and the same filter. We repeat this process until we go through the entire input image. The output is going to be the input for the next layer.\n\nFilter, Kernel, or Feature Detector is a small matrix used for features detection. A typical filter on the first layer of a ConvNet might have a size [5x5x3].\n\nConvolved Feature, Activation Map or Feature Map is the output volume formed by sliding the filter over the image and computing the dot product.\n\nReceptive field is a local region of the input volume that has the same size as the filter.\n\nDepth is the number of filters.\n\nDepth column (or fibre) is the set of neurons that are all pointing to the same receptive field.\n\nStride has the objective of producing smaller output volumes spatially. For example, if a stride=2, the filter will shift by the amount of 2 pixels as it convolves around the input volume. Normally, we set the stride in a way that the output volume is an integer and not a fraction. Common stride: 1 or 2 (Smaller strides work better in practice), uncommon stride: 3 or more.\n\nZero-padding adds zeros around the outside of the input volume so that the convolutions end up with the same number of outputs as inputs. If we don\u2019t use padding the information at the borders will be lost after each Conv layer, which will reduce the size of the volumes as well as the performance.\n\nHow to compute the output volume[W2xH2xD2]?\n\nWhat is the output volume of the first Convolutional Layer of Krizhevsky et al. architecture that won the ImageNet challenge in 2012?\n\nParameter Sharing (shared weights): We think that if a feature is useful it will also be useful to look for it everywhere in the image. However, sometimes, it is weird to share the same weights in some cases. For example, in a training data that contains faces centered, we don\u2019t have to look for eyes in the bottom or the top of the picture.\n\nDilation is a new hyperparameter introduced to the Conv layer. dilation is filters with spaces between its cells. for example, we have one dimension filter W of size 3 and an input X:\n\nReLU Layer applies an elementwise activation function max(0,x), which turns negative values to zeros (thresholding at zero). This layer does not change the size of the volume and there are no hyperparameters.\n\nPool Layer performs a function to reduce the spatial dimensions of the input, and the computational complexity of our model. And it also controls overfitting. It operates independently on every depth slice of the input. There are different functions such as Max pooling, average pooling, or L2-norm pooling. However, Max pooling is the most used type of pooling which only takes the most important part (the value of the brightest pixel) of the input volume.\n\nExample of a Max pooling with 2x2 filter and stride = 2. So, for each of the windows, max pooling takes the max value of the 4 pixels.\n\n- Pool layer doesn\u2019t have parameters (the weights and biases of the neurons), and no zero padding, but it has two hyperparameters: Filter (F) and Stride (S). More generally, having the input W1\u00d7H1\u00d7D1, the pooling layer produces a volume of size W2\u00d7H2\u00d7D2 where:\n\nA common form of a Max pooling is filters of size 2x2 applied with a stride of 2. The Pooling sizes with larger filters are too destructive and they usually lead to worse performance.\n\nMany people don\u2019t like using a pooling layer because it throws away information and they replace it by a Conv layer with increased stride once in a while.\n\nFully connected layers connect every neuron in one layer to every neuron in another layer. The last fully-connected layer uses a softmax activation function for classifying the generated features of the input image into various classes based on the training dataset."
    },
    {
        "url": "https://medium.com/@phidaouss/https-medium-com-phidaouss-kaggle-cli-6c31519365db?source=user_profile---------10----------------",
        "title": "Kaggle-Cli \u2013 Firdaouss Doukkali \u2013",
        "text": "Purpose: It is generally easier to use command line interface than navigating Kaggle website, especially if you are using Linux or you want to directly download data to your AWS instance\u2026 Please make sure to accept the rules of the competition on Kaggle website to avoid some common issues with Kaggle-cli. (no command line here, it is through the browser)\n\nTo download data from Kaggle website using Kaggle-cli:\n\nkg download \u2013u \u2018username\u2019 \u2013p \u2018password\u2019 \u2013c \u2018competition name\u2019\n\nThen after kg config, you can install data, only by typing:\n\nTo know more about Kaggle-cli, you may check its GitHub link https://github.com/floydwch/kaggle-cli"
    }
]