[
    {
        "url": "https://medium.com/@Synced/ai-e-mail-assistants-streamline-meeting-scheduling-9705f5f8dd7a?source=user_profile---------1----------------",
        "title": "AI E-Mail Assistants Streamline Meeting Scheduling \u2013 Synced \u2013",
        "text": "Emailing back and forth to book meetings can be a huge waste of time, especially when people are in different time zones, don\u2019t know each other\u2019s availability, or schedules change, etc.\n\nIn a Doodle survey of 1500 professionals, respondents needed an average of 4.75 hours to arrange nine weekly meetings with seven parties. A personal assistant could handle that, but for that would be cost prohibitive for most. People need an easy and affordable solution to scheduling meetings by email.\n\nWith all the progress being made in Artificial Intelligence (AI) and Natural Language Processing (NLP), how about building an AI powered virtual assistant? The idea of intelligent personal assistants and AI scheduling appeared more than years ago, but early research never took off: AI algorithms were not strong; there was a lack of quality training data; and the whole scheduling problem remained too complex. It took years, but now several promising AI meeting scheduling startups and products are emerging.\n\nClara Labs is a San Francisco startup that created the industry-leading Clara AI assistant, which can automatically schedule meeting times and places based on attendants\u2019 preferences. Users simply \u201cCc\u201d Clara on meeting invites, and she sorts it out. If Clara can\u2019t, one of the company\u2019s human staff members will step in. Clara Labs raised $12 million in two funding rounds, with a US$7 million series A led by Basis Set Ventures and Slack Fund in July 2017.\n\nThe New York based startup\u2019s AI personal assistants Amy and Andrew have meeting scheduling workflows similar to Clara Labs. X.ai however offers a free basic plan, with features similar to Clara\u2019s US$99/mo essential package. X.ai was founded in 2014 and has attracted US$32.3 million in funding.\n\nThe Calendar.help project started years ago, and enables Microsoft digital assistant Cortana to arrange meetings and schedule tasks. In 2016, Microsoft also acquired Genee, an end-to-end scheduling tool that integrates with calendar apps and email providers. As a Microsoft product this solution can naturally integrate with Office 365. Calendar.help values performance over speed and automation, with a workflow that brings both algorithms and human assistants into the scheduling loop.\n\nJulie Desk is a French startup that has raised US$3.69 million, while Evie.ai is a Singapore based startup. Both were founded in 2014. Their AI email assistants Julie and Evie are X.ai and Clara Labs\u2019 overseas competition.\n\nFin is a San Francisco based startup founded in 2015. Its powerful personal assistant leverages AI techniques in speech recognition and natural language processing with human assistants, and can call, text, email, schedule, research, book, and even make purchases.\n\nWill AI and robots take over from secretaries and personal assistants? Not yet. \u201cDespite advances in natural language processing, extracting information from free-text is still error-prone and algorithms often make mistakes that seem trivial to humans\u201d, says a Microsoft researcher. This is why most of the startups we surveyed still involve humans in their AI-powered products.\n\nAssistant systems are also not yet scaling up. Clara Labs\u2019 median virtual assistant email response time is a sluggish 44 minutes, which raises concerns about how the system will perform if hundreds of thousands of people use it at same time. Moreover, vendor contracts and policy and procedure standards need to be developed to address security and privacy issues. We can expect to see improvements in AI assistants\u2019 control functions and automation activities to address these operational risks.\n\nThere is no doubt that future email assistant systems will have fewer humans in the loop, process messages faster, and be more accurate. They may also integrate with more platforms, such as team collaboration tool Slack or social media app WeChat. Tomorrow\u2019s conversational AI assistants will be more functional and push deeper into enterprise. They may also become stenographers, make meeting contents searchable, follow up on action items, or highlight key moments of a telephone call. This may sounds futuristic, but researchers are already working on it!"
    },
    {
        "url": "https://medium.com/@Synced/can-ai-reinvent-the-tv-7dd56372425f?source=user_profile---------2----------------",
        "title": "Can AI Reinvent the TV? \u2013 Synced \u2013",
        "text": "Not to be outdone by Skyworth, other major global TV makers are jumping on the AI TV bandwagon. At this year\u2019s Consumer Electronics Show Samsung Electronics introduced an AI-applied 85-inch 8K QLED TV, while LG integrated its AI system ThinQ into its new 4K OLED and Super UHD TVs. At February\u2019s Appliance & Electronics World Expo in Shanghai, TCL showcased its X5 TV within facial recognition technique, and Hisense launched its VIDAA AI TV system.\n\nSkyworth believes the AI TV will help them bounce back. The company\u2019s stock price surged by 18 percent after the Baidu agreement was announced.\n\nWhile the term \u201cAI TV\u201d may suggest a fully intelligent, Skynet-like TV bot, it rather refers to incorporating machine learning techniques such as facial recognition, voice-controlled assistants, personalized content suggestion, sentiment analysis, etc into the product. AI TV also sounds a lot fancier than Smart TV.\n\nThe TV has come a long way. Wood cabinets with monochrome cathode ray tubes evolved into HD flat colour screens with remote controls; while channel choices expanded from a handful to thousands. The device\u2019s latest incarnations include so-called Smart TVs, and the latest buzz in the consumer electronics community: the \u201cAI TV\u201d of the future.\n\nA Chinese TV manufacturer for more than 30 years, Skyworth is going through rough times. The company\u2019s domestic TV sales fell 16 percent in 2017, while its global TV sales slipped three percent, part of an industry-wide trend.\n\nLeading television manufacturer Skyworth has signed a strategic cooperation agreement with Chinese internet giant Baidu. In the deal announced March 17, Baidu will invest CN\u00a51.01 billion (US$159.7 million) in Skyworth\u2019s Smart TV unit Coocaa; while its AI assistant system DuerOS will be integrated into Skyworth\u2019s agenda-setting Super AI TV.\n\nAI TV\u2019s voice-based interface takes the remote control out of the loop. Jing Kun is Baidu Project Manager of DuerOS, a Mandarin-based equivalent of Alexa. He told Synced he believes the TV is one of the most promising application areas for AI assistants. \u201cWhile people are watching a TV, it\u2019s difficult for them to interact with it. An intelligent voice interaction may be the way to push the user experience to the next level,\u201d says Jing.\n\nMost major TV manufacturers lack the R&D capabilities to build their own voice interaction system from scratch. As a result, they seek joint development agreements with AI tech providers such as Google, Amazon, or Chinese tech giants. LG\u2019s new TVs have added Google Assistant and Alexa. Google Assistant is also in Sony\u2019s X900F. Samsung is integrating its AI assistant Bixby \u2014 already available in its phones \u2014 into TVs as well.\n\nAnother use of AI is to better personalize content recommendations. Though Hulu, Netflix, Amazon Prime and other content providers have their own AI recommendation engines, AI TVs can search across all platforms for actors, titles, genres and other preferences, and tune content recommendations to suit family members.\n\nAlex Haslam, a cord-cutting expert at Utah-based HowtoWatch.com, told Synced, \u201cToday\u2019s viewers value choice, which is why streaming services like Netflix are so popular. Your TV being able to adapt to those choices will enhance the experience.\u201d Scott Amyx, Managing Partner at Amyx Ventures, takes it further: \u201cAI will eventually understand that streamlining preferences vary by day, time, mood, family member and who they are with.\u201d\n\nToday\u2019s AI TVs will know what content was watched, when and for how long, and this data can be sold to advertisers.\n\nBut with front-facing cameras and sophisticated algorithms, the AI TV could also know who is watching, and whether for example they appear engaged, or look away during commercials, or even doze off. Amyx told Synced he has spoken with researchers and advertising executives who are continually seeking new ways to get the attention of viewers. \u201cFor example, through loud sounds that bring them back to the ad on the screen, a certain pitch of voice, etc.\u201d\n\nTVs capturing user behavior data has raised concerns about privacy and even risks. Consumer Reports recently discovered that millions of Smart TVs might have security vulnerabilities. Hackers could for example remotely change channels, play offensive content, or crank up the volume. The report suggest Samsung, TCL and other TV makers using the Roku Smart TV platform, as well as streaming devices such as the Roku Ultra, may be affected.\n\nIt\u2019s still early to say whether AI TVs will live up to expectations. At present the hype may be greater than the performance. According to the China Electronic Chamber of Commerce and JD Appliance 2017 White Paper on Artificial Intelligence Television, \u201cSome AI TVs can only achieve simple speech interaction, and lack AI-specific \u2018cognitive-judgment-decision making\u2019 capabilities.\u201d\n\nManufacturers believe AI will soon be essential in TVs. Says Skyworth Founder Huang Hongsheng, \u201cIn the past, the remote control changed how watchers interact with televisions, so they didn\u2019t have to walk across the room to press a button. Today\u2019s AI can open another new path of convenience and communication, by making a television that \u2018gets to know you\u2019.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/cambricon-unveils-its-first-ai-chip-for-cloud-computing-d3f7acdb4076?source=user_profile---------3----------------",
        "title": "Cambricon Unveils its First AI Chip for Cloud Computing",
        "text": "Cambricon Unveils its First AI Chip for Cloud Computing\n\nCambricon today unveiled its Cambricon 1M chip for edge computing, and the MLU100, the first in a new chip series for cloud computing.\n\nCambricon 1M is the company\u2019s third generation AI chip for edge devices. With its TSMC 7nm technology, the AI chip provides efficiency of 5 TOPS/Watt for 8-bit computing. The 1M chip is available in 2, 4, and 8 TOPS versions to support a range of AI applications.\n\nLike the company\u2019s 1H and 1A chips, Cambricon 1M supports deep learning models such as CNN, RNN, SOM; and supports SVM, k-NN, k-Means, decision tree and other algorithms. This is also the world\u2019s first AI processor supporting local machine learning training.\n\nCambricon MLU100 is the first generation of Cambricon\u2019s new product series supporting cloud computing. MLU100 adopts Cambricon\u2019s latest MLUv01 architecture and TSMC 16nm technology. This processor has the capability of providing 128 TFLOPS/166.4 TFLOPS in balance mode and high-performance mode respectively.\n\nCambricon and its partners demonstrated several use cases: Lenovo\u2019s new ThinkSystem SR650 server is based on the MLU100, as is Sugon\u2019s new PHANERON series of products. iFlytek has also announced a collaboration with Cambricon."
    },
    {
        "url": "https://medium.com/syncedreview/ubtech-robotics-gets-us-820-million-funding-becomes-the-worlds-most-valuable-ai-startup-c25cd357e87e?source=user_profile---------4----------------",
        "title": "UBTECH Robotics Gets US$820 Million Funding; Becomes the World\u2019s Most Valuable AI Startup",
        "text": "UBTECH Robotics Gets US$820 Million Funding; Becomes the World\u2019s Most Valuable AI Startup\n\nChinese AI and humanoid robotic company UBTECH Robotics today announced a staggering US$820 million in Series C funding. With its new estimated value of US$5 billion, Shenzhen City based UBTECH becomes the world\u2019s most valuable AI startup.\n\nInternet giant Tencent led the funding with a US$120 million investment. It is believed UBTECH\u2019s capabilities in robot design and manufacturing will strengthen Tencent\u2019s AI products. Last year, Tencent reportedly pumped US$40 million into UBTECH. Also joining the funding are the Industrial and Commercial Bank of China, Haier, Minsheng Securities, Telstra, CDHFund, and others.\n\nUBTECH Founder and CEO Zhou Jian told Synced that \u201cthis round of financing will be mainly used for strengthening R&D capabilities, facilitating marketing and brand development, and attracting top-tier talents.\u201d\n\nFounded in 2012, UBTECH aims to \u201cbring a robot into every home, and truly integrate intelligent robots into the daily lives of everyone creating a more intelligent way of life.\u201d The company\u2019s Alpha 1S robot holds the Guinness World Record for \u201cmost robots dancing simultaneously.\u201d A video of UBTECH robodog Jimu dancing and licking paws at the 2018 CCTV Spring Festival Gala went viral. Also in the UBTECH family are Cruzr, an intelligent service robot; the voice-activated, video-enabled companion Lynx; and a Star Wars First Order Stormtrooper robot. The company even produces a BuilderBots Kit for children who want to build robots.\n\nWhile the global robotic market is competitive, UBTECH has an advantage with its technological development of humanoid robot servos and motion controlled gait algorithms. UBTECH is a global leader in robotic joint and body structure manufacturing.\n\nZhou says UBTECH\u2019s sales goal for 2018 will exceed CN\u00a52 billion. He also unveiled the company\u2019s roadmap for the next four years:\n\nZhou\u2019s vision for robots goes beyond the crunching of algorithms and whirring of servos. Rather he stresses the relationship of such machines to human beings. \u201cIn UBTECH, we insist on viewing the robot as a member of a future family.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/ml-community-pledges-to-boycott-natures-new-paywalled-journal-6429d9d9a800?source=user_profile---------5----------------",
        "title": "ML Community Pledges to Boycott Nature\u2019s New Paywalled Journal",
        "text": "Thousands in the machine learning community say they will boycott Nature\u2019s paywalled Machine Intelligence Journal, which is set for a January 2019 release. So far, 2482 researchers have signed a petition pledging to not submit any work to the journal or review or edit any of its papers.\n\nThe key objection to the new publication is its subscription-based revenue model, wherein the journal will charge a paper submission fee and individual and institutional subscription fees.\n\nThis, of course, touches a nerve in the machine learning research community, which prides itself on an open sharing culture. It has been easy for the public to access ML papers, codes, and in some cases even datasets. Up-to-date papers from major conferences such as NIPS and CVPR can also be easily shared. ArXiv is sometimes criticized for its relatively loose review mechanism, but its huge volume of papers can nevertheless be accessed by a global readership free of cost.\n\nLeading the boycott call are deep learning pioneers Geoffrey Hinton, Yann LeCun and Yoshua Bengio; research leads at Google, Facebook, Amazon and IBM; and academics from MIT, Stanford, CMU, and Oxford. OSU Professor Emeritus Thomas G. Dietterich, who initiated the petition, was Executive Director of the journal Machine Learning from 1992\u20131998, before the publication became the open-access Journal of Machine Learning Research (JMLR) in 2001.\n\nNature announced plans for Machine Intelligence last November, as a new online-only publication that would cover the \u201cbest research across the field of artificial intelligence\u201d with relevant reviews and commentaries. All editors are internal staff working under Chief Editor Liesbeth Venema.\n\nOpen access in academic publications is an ongoing issue. Research access in areas such as biology, neuroscience, psychology, and social sciences can be extremely costly, even for wealthy institutions like Harvard.\n\nThe academic community has been battling publishers for open access for a decade. Sci-Hub, created by Alexandra Elbakyan in 2011, has an archive of over 64.5 million papers available for direct download. Reddit founder Aaron Swartz\u2019s attempts embroiled him in a legal fight with Jstor and MIT that ended in tragedy.\n\nOutside the machine learning community, research papers in other AI subfields such as robotics and hardware chips are difficult for non-specialists to obtain. There are also labs like Boston Dynamics that can live outside the \u201cpublish-or-perish\u201d curse.\n\nThe machine learning community is rebelling against a publishing orthodoxy it regards as an impediment to progress. In a Reddit discussion thread, one researcher posted that he would judge a paper based on its ability to spread and instruct further research, rather than focusing on the publisher\u2019s prestige."
    },
    {
        "url": "https://medium.com/syncedreview/focus-on-ai-at-facebook-f8-6dbe7e04b565?source=user_profile---------6----------------",
        "title": "Focus on AI at Facebook F8; PyTorch 1.0 Released (Updated)",
        "text": "Facebook generally uses its F8 Developer Conference to introduce new platform features. This year however there was a distinct emphasis on \u201cAI,\u201d with the term mentioned more than ever in the event\u2019s keynote speech.\n\nFacebook Founder and CEO Mark Zuckerberg unveiled the company\u2019s strategic AI roadmap, pushing R&D in vision technologies, unsupervised learning, natural language processing, reinforcement learning, generative networks, and AI developer tools. Facebook also announced several AI-related updates to elevate the user experience.\n\nRussian bots, fake news and the Cambridge Analytical scandal have hit Facebook hard. The seminal social media platform \u2014 where billions of users share photos and videos of their lives with family and friends, and exchange thoughts and opinions on events of the day \u2014 is increasingly revealing a dark side, as a malignant tool for spreading false information, instigating hatred, and even influencing elections.\n\nFacebook wants to fight back. Zuckerberg told the F8 audience the company is now using AI algorithms to detect \u201cspammers who just want money,\u201d \u201cfake accounts created by bad actors,\u201d and \u201creal people who are sharing fake information.\u201d\n\nFacebook has previously suggested they were deploying cutting-edge AI to recognize fake news and bot-generated accounts. In 2016, AI pioneer and then Facebook Director of AI Research Yann LeCun said AI could be used to identify fake news or violence in live video content on the site. In 2017, Facebook announced that they would use AI to detect terrorism-related posts.\n\nAI researchers are divided on whether machine learning algorithms can effectively detect fake news. Dean Pomerleau is a research scientist and entrepreneur who helped organize a crowdsourcing challenge to develop machine learning solutions to combat fake news. He told The Verge that in his opinion AI couldn\u2019t fix the fake news problem. On the other hand, Aaron Edell, CEO of AI company Machine Box, claims to have produced a machine learning algorithm that can detect fake news with higher than 95% accuracy.\n\nZuckerberg faced questions about Facebook\u2019s data privacy practices at a congressional hearing last month, where he estimated the company would need five to ten years to build an effective AI-powered fake news detection system that leaves humans out of the loop.\n\nThe Instagram Explore page is a powerful feature that recommends posts based on personal interests and tastes. Users can browse content across topics of interest, even from accounts they don\u2019t follow.\n\nFacebook Data Science & Analytic Manager Tamar Shapiro announced at F8 that Facebook will revamp Instagram Explore to better organize its recommended content into different topic channels, and this new Explore page will be powered by AI.\n\n\u201cIn order to deliver cutting-edge experience, we are augmenting AI with content classification and curation signals from our community,\u201d said Shapiro.\n\nShapiro also introduced a new AI-powered \u201cbullying comment filter,\u201d which can hide content that disturbs or upsets users. Last year, Instagram launched an offensive comment filter, which can automatically hide comments it deems \u201cdivisive\u201d or \u201ctoxic.\u201d\n\nM Translation expands on M Suggestion, a pop-up feature launched last year that suggests relevant content and capabilities. Now, when users receive a Marketplace message in a language different from their default, M Translation can translate the message into their default language.\n\nFacebook smart speakers to sell only outside the US?\n\nRivals Google, Amazon, and Microsoft have already jumped on the smart speaker bandwagon, as have major Chinese tech companies. Now Facebook is rumoured to be readying their own line of smart speakers, although Zuckerberg was tight-lipped about the plan at F8.\n\nMultiple media reports suggest Facebook will launch two devices this July in overseas markets. The unusual marketing plan is said to be due to Facebook\u2019s slipping trustworthiness among users in the US. The smart speaker is expected to be equipped with a touchscreen and camera, and will be powered by the text-based chatbot Facebook Messenger bot, M, which will likely get an upgrade to voice assistant.\n\nOn F8\u2019s second day, Facebook announced PyTorch 1.0, the latest version of its open-source AI software framework that guides and supports researchers from research stages to deployment of trained models for various AI applications. \n\n \n\nVP of Facebook Infrastructure Bill Jia said \u201cPyTorch 1.0 takes the modular, production-oriented capabilities from Caffe2 and ONNX and combines them with PyTorch\u2019s existing flexible, research-focused design.\u201d \n\n \n\nFacebook is pushing the combined PyTorch \u2014 Caffe2 framework. Last month, the Caffe2 Github page introductory \u201creadme\u201d document was suddenly replaced with a link: \u201cSource code now lives in the PyTorch repository,\u201d which enabled Caffe2 users to directly check Caffe2 code in PyTorch.\n\n \n\nSince its release in October 2016, PyTorch has become a preferred machine learning framework for AI researchers due to its flexibility. Over half of Facebook AI projects run on PyTorch. PyTorch 1.0 will be available to beta users later this summer.\n\n \n\n Facebook also announced the open-sourcing of many AI tools, including Translate, a PyTorch Language Library for neural machine translation, and ELF OpenGo, an AI bot based on the ELF (extensive, lightweight and flexible) platform for training gameplay AI with reinforcement learning. \n\n \n\n Facebook Chief AI Scientist Yann LeCun tweeted that the ELF OpenGo bot \u201chas attained professional level in two weeks of training and has won 15 games against 4 top professional human players.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/accutarbio-advances-ai-in-drug-discovery-e28c50abdec3?source=user_profile---------7----------------",
        "title": "AccutarBio Advances AI in Drug Discovery \u2013 SyncedReview \u2013",
        "text": "In December 2017 Chinese pharma tech startup AccutarBio raised US$15 million from IDG Capital, YITU Tech, and ZhenFund. This was one of the country\u2019s largest pharma tech funding rounds, and signaled AI\u2019s strong promise and potential in early-stage drug discovery.\n\n\u201cDrug discovery is never subjective,\u201d AccutarBio CEO and Co-founder Dr. Jie Fan tells Synced. \u201cA person may have a good knowledge of thousands of types of drugs. But there is no way they know ten thousand or even one hundred thousand drugs. In contrast, machines can do a much better job than humans.\u201d Dr. Fan also believes that hybrid AI can accelerate drug discovery for targeted therapy and provide cancer patients with further alternative treatment plans.\n\nDr. Fan graduated from Fudan University with a BA and received his Master\u2019s in Biostatistics from the University of California Berkeley in 2004. He completed his PhD under Dr. Nikola Pavletich, studying the crystal structure of DNA binding proteins. Fan then worked as a postdoc researcher under 1999 Nobel Prize laureate G\u00fcnter Blobel, focusing on the structural analysis of nuclear pore complexes (NPCs), which are large protein complexes that allow transport of molecules across the nuclear membrane.\n\nAccutarBio continues to widely collaborate with academics for cross-interdisciplinary research, and the company has deployed labs in both Shanghai and New York.\n\nIn the research paper Chemi-net: a Graph Convolutional Network for Accurate Drug Property Prediction, AccutarBio extended the use of traditional statistical learning methods to create a multi-layer DNN architecture called \u201cChemi-Net\u201d, which can predict ADME (absorption, distribution, metabolism, and excretion) properties of molecular compounds. ADME study is crucial and should be done at an early stage in drug discovery as it helps researchers understand the transport of molecules in organisms and can efficiently eliminate weak drug candidates, increasing success rate in drug trials and shortening drug discovery timelines.\n\nChemi-Net was tested on 5 ADME endpoints \u2014 human microsomal clearance, human CYP450 inhibition, aqueous equilibrium solubility, pregnane X receptor induction, and bioavailability \u2014 with 13 industrial grade datasets selected for predictive model development, involving more than 250,000 data points. Both single-task and multi-task Chemi-Net exhibited dramatic predictive accuracy improvements over benchmarks. Researchers expect Chemi-Net\u2019s significantly increased ADME prediction accuracy to greatly accelerate efficiency and success rates in drug discovery.\n\nFollowing on the success of their AI-powered drug discovery model, AccutarBio plans to further develop and promote Chemi-Net with the aim of revolutionizing traditional experiment-based and experience-based drug development.\n\nAI has demonstrated its abilities in healthcare applications in medical and pharmaceutical fields. The tech\u2019s capability for \u2018learning\u2019 meaningful features from large datasets has been widely used in clinical situations involving computer-aided image detection and diagnosis (e.g., assisted image-based early cancer screening), implementation and maintenance of electronic health records, and continuous patient monitoring.\n\nAccutarBio strategic investor YITU Tech says \u201cAsymmetric information in multidisciplinary fields has brought huge barriers to technology development. We hope our knowledge and work in AI will be able to assist AccutarBio and make great advances in biology. We believe artificial intelligence will make significant difference on the current status of biological research and biopharmaceuticals through our in-depth cooperation with AccutarBio. And collaboration with YITU for Medical AI-powered research in clinical applications will bring more profound value.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-350-kph-self-driving-train-gets-ready-to-hit-the-rails-2301e7753811?source=user_profile---------8----------------",
        "title": "China\u2019s 350 kph Self-Driving Train Gets Ready to Hit the Rails",
        "text": "On April 29th, the Assets Supervision and Administration Commission (SASAC) of China\u2019s State Council announced that China Railway Signal & Communication Corp (CRSC) had completed lab testing of their high-speed rail self-driving system (C3 + ATO), and is preparing for field testing. The system will be deployed in Fuxing Hao trains, which are the world\u2019s fastest with an operating speed of 350 km per hour.\n\nThe new self-driving high-speed rail system integrates various technologies, including cloud computing, IoT, artificial intelligence, big data, etc. The rail system infrastructure will include facial recognition for check-in, robot porters and other intelligent services.\n\nCRSC is a world-leading rail signal and communication service provider. It owns the world\u2019s biggest rail technology lab, and developed the Chinese Train Control System (CTCS).\n\nCRSC says they will complete assembling the smart train by the end of 2018, finish rail tests and adjustments by early 2019, and then officially open the \u201cBeijing-Zhangjiakou Intelligent Rail Line\u201d in late 2019, enabling passengers to complete the over 200 km journey in just 50 minutes."
    },
    {
        "url": "https://medium.com/syncedreview/iclr-2018-kicks-off-in-vancouver-f3a99bab70e0?source=user_profile---------9----------------",
        "title": "ICLR 2018 Kicks Off in Vancouver \u2013 SyncedReview \u2013",
        "text": "AI researchers are gathering in Vancouver, Canada for the sixth annual ICLR (International Conference on Learning Representations). The event starts today and runs to May 3rd at the Vancouver Convention Centre.\n\nCompared to NIPS, ICML, and ACL, the ICLR is a relative newcomer. Founded in 2013 by deep learning mavericks Yoshua Bengio and Yann LeCun, the conference has become an important destination for AI researchers.\n\nProgram chair Yann LeCun tweeted that \u201cattendance at ICLR doubles every year. It\u2019s up to 2000 this year. It merely doubles because attendance is capped. The number of submissions also doubled from last year. [Google Researcher] Tara Sainath is opening the conference.\u201d\n\nThis year 337 out of 935 paper submissions were accepted, with best paper honours going to On the convergence of Adam and Beyond, Spherical CNNs and Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments.\n\nA statistical analysis by Arthur Pajot shows Google leading both the paper submission and acceptance charts, ahead of academic institutions Berkeley, Stanford, CMU, UofT, and ETH; and corporate research teams from Facebook, Microsoft and IBM. Google is dispatching 130 researchers to the conference and pitching in US$40,000 as a platinum sponsor.\n\nThe most prolific individual submittors are Yoshua Bengio, who submitted 18 papers, (9 rejected, 7 posters, 2 workshops), and UC Berkeley\u2019s Pieter Abbeel with 12 submissions (4 rejected, 4 posters, 2 workshops, 2 orals). The top-submitting countries are the United States, United Kingdom, China, Canada, and Germany.\n\nA paper keyword analysis conducted by Pau Rodr\u00edguez suggests \u201cmeta-learning, exploration, model compression, adversarial examples, variational inference are the hottest topics this year. For instance, 85% of the papers with the keyword exploration were accepted, while classification and CNN only show 12% acceptance rate.\u201d\n\nThe ICLR is known for popularizing the open review system designed by Andrew McCallum from UMass Amherst. Papers submitted to the conference are published on http://OpenReview.net, publicly reviewed and archived on the site."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-apr-w-4-apr-w-5-c31ec014c9ba?source=user_profile---------10----------------",
        "title": "AI Biweekly: 10 Bits from Apr W 4 \u2014 Apr W 5 \u2013 SyncedReview \u2013",
        "text": "Google introduces starter kits designed to help people learn and experiment with AI solutions. The AIY kits (a pun on DIY/do-it-yourself) and are aimed at students, who can use for example the Voice Kit to develop a voice-controlled speaker, or the Vision Kit for tasks like object detection, facial detection etc. Each kit comes with a Raspberry Pi Zero WH board.\n\nMicrosoft users can now run translations without an Internet connection, thanks to a new app offering modified neural network translations. The offline translation quality is high, although not as good as online translation backed by AI on the cloud. Offline translation is available for Arabic, Chinese-Simplified, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish and Thai. More will languages will be added later.\n\nApril 19th \u2014 Dubai Uses AI to Turn Waste Into Energy\n\nAs part of its \u201cVision of the Future City, Today\u201d initiative, Dubai Municipality launches its AI-empowered Wasteniser project, which sorts solid waste by type using AI tech, and incinerates materials at optimal temperatures to produce good ash that can be used in the production of green concrete. The energy generated goes to the local electrical grid.\n\nWith Amazon\u2019s new Alexa Blueprints, users can create their own Alexa skills without any coding knowledge. Alexa Blueprints comes with 20+ templates, including Fun & Games, At Home, Storyteller, and Learning & Knowledge. Although these user-created skills will not be available on the Alexa Skills Store, they will appear on a \u201cSkills You\u2019ve Made\u201d page on the Blueprint website. Alexa Blueprints gives Amazon an edge over Apple\u2019s HomePod and Google Home, which do not offer such custom skill creation opportunities. The service is currently only available in the US.\n\nGoogle publishes research showing quick detection of cancer cells using a new Augmented Reality Microscope technology. Google believes the AI-backed ARM will \u201caccelerate and democratize the adoption of deep learning tools for pathologists around the world.\u201d The ARM is compatible with most current devices and can be easily retrofitted.\n\nApril 23rd \u2014 NVIDIA and DEEPCORE Team Up to Boost AI Startups in Japan\n\nNVIDIA announces a partnership with SoftBank-owned startup incubator DEEPCORE, which will use NVIDIA\u2019s AI computing platform to expand its support for startups and promote AI research in Japan. NVIDIA will provide technical training and industry advice for DEEPCORE customers and incubator members via its Deep Learning Institute.\n\nAsahi Shuzo is applying Fujitsu AI technology in a trial project to help visualize and optimize their sake brewing process. The Fujitsu Human Centric AI Zinrai system aims to \u201csystematize the experience and know-how of each employee\u201d to widen and improve real-time monitoring of the sake production process. Fujitsu built the predictive AI model based on the sake brewing company\u2019s historical data.\n\nAlibaba announces collaborations with automakers Daimler, Audi, and Volvo to use its voice assistant Tmall Genie in their cars to monitor battery level, mileage, engine status, etc., and allow remote management via voice command of car status including doors, windows and air conditioning etc. \u201cCars are an environment, alongside the home and the office, where individuals spend a significant amount of time, and which through connectivity can become an important part of life,\u201d said Lijuan Chen, head of Alibaba AI Labs, which created the Genie.\n\nWeb service company IFTTT announces US$24 million in funding from Salesforce, IBM, and others. The investment will be used to hire talents, develop enterprise business and IoT services, and address problems in the market. \u201cEvery business is undergoing a dramatic transformation into a digital service,\u201d says IFTTT CEO Linden Tibbets. \u201cWe could not be more excited to partner with our new investors, our passionate users, and every business working hard to become a service.\u201d\n\nTesla\u2019s ex-autopilot head Jim Keller leaves the autonomous driving company to lead Intel\u2019s silicon engineering team, where his research will focus on SoC development and integration. \u201cJim is one of the most respected microarchitecture design visionaries in the industry, and the latest example of top technical talent to join Intel,\u201d said Dr. Murthy Renduchintala, Intel\u2019s chief engineering officer and group president of the Technology, Systems Architecture & Client Group (TSCG)."
    },
    {
        "url": "https://medium.com/syncedreview/amazon-patents-provide-clues-to-new-home-robot-dedbafe0e1b7?source=user_profile---------11----------------",
        "title": "Amazon Patents Provide Clues to New Home Robot \u2013 SyncedReview \u2013",
        "text": "The robotics community is abuzz with speculation following reports that Amazon\u2019s R&D arm Lab126 is hiring senior researchers for a home robot-producing project codenamed \u201cVesta.\u201d The company is also said to be recruiting employees who will test the robot in their homes later this year.\n\nNo one knows if Amazon envisions an enhanced roomba or a precocious R2D2 V2.0. Company Founder and CEO Jeff Bezos was recently spotted at a tech conference walking Spotmini, the Boston Dynamics robotic dog known for autonomously opening doors: Might the bot take the form of a pet?\n\nOne thing is certain: Amazon will cram Alexa into the robot. The company\u2019s star virtual assistant has evolved into a know-it-all that can play music and audiobooks, compile to-do lists, set alarms, stream podcasts, and provide real-time weather, traffic, news and other personalized information. Although most people only use their virtual assistants for music streaming, controlling smart lights, timers and weather forecasts, according to an IFTTT survey, Alexa is backed by so much cutting-edge AI tech that she\u2019s become too smart for just a smart speaker.\n\nWith Alexa as the brains, the bot may be more humanoid, both in physical form and in personalization and communication scope and style. Sarah Osentoski, co-founder of the robotics company Mayfield that makes companion robot Kuri, told Wired, \u201cWhen you have something that\u2019s talking to you and that\u2019s driving around your house you start to expect a lot more. You start to expect the intelligence of a 3-year-old or a 5-year-old.\u201d\n\nThat could be why Amazon is scrambling to update Alexa with more powerful capabilities. At the recent World Wide Web Conference in Lyon, France, the Head of the Alexa Brain group Ruhi Sarikaya said Alexa will soon launch three new features: an enhanced memory system that remembers everything users say, a more natural conversational system, and automatic activation of over 40,000 third-party skills.\n\nTo recognize its owner and family members, navigate rooms and hallways and so on, Amazon\u2019s new robot will certainly require robust computer vision skills. In 2016, Amazon acquired Orbeus, a Silicon Valley startup that specializes in image recognition. Amazon hired nearly all of Orbeus\u2019 technical staff, and picked up its facial recognition system and scene recognition system patents.\n\nLeveraging facial recognition, the robot will be able to capture images and record videos automatically. Amazon has patented approaches for creating high quality images with less blur and noise. The bot for example might be put in charge of photographing a birthday party, or compiling family photo and video albums.\n\nThe new robot might also employ multimodal dialogues to better watch, listen, speak, and physically interact with humans. Dr. Zhou Yu, an assistant professor at University of California in Davis, told Synced she is enabling intelligent assistants such as Siri and Alexa to recognize actions such as facial expressions and give appropriate responses. Since 2016, Amazon has supported her research with US$100,000 in annual funding.\n\nThe Amazon home bot will be mobile, and navigate autonomously with cameras. It will need to recognize and track its user or users in the environment. Some of this tech can be borrowed from the Echo Smart Speaker, which uses real time sound data to determine a user\u2019s physical position. Amazon\u2019s robot could push that technique by incorporating dynamic visual data.\n\nLab126 has been granted several patents for localization techniques which can determine where a user is. One patent describes a system that utilizes a sound location technique to estimate an audio-based sound source position, and then pinpoints the user\u2019s position from analysis of optical images or depth maps generated by multiple sensors, such as LiDar and structured light.\n\nAlthough the patent was originally filed for a mysterious Lab126 project \u2014 rumoured to be a since-abandoned in-home augmented reality entertainment system \u2014 Lab126 is likely to apply this technique in the home robot.\n\nWhile Amazon has prioritized voice commands in human-machine interfaces, their home bot will have to go beyond that, and watch for and respond to physical cues from the user, such as hand gestures.\n\nAmazon has already been granted patents for hand signal detection. One is for a room computing system that uses sensors to detect and respond to hand poses. To detect a hand pose, an observed pose is compared to a hand pose dataset. When you wave goodbye to your Amazon robot as you head to work in the morning, will it wave back? It will if you want it to.\n\nIn any case the home bot will understand you are leaving, and will get on with its day, which may involve assigned tasks such as security monitoring. The new Amazon Key service allows in-home delivery and secure home access for guests. An interior security camera connects with Amazon Key, and it would seem natural to apply that task to the robot.\n\nGo outside to run errands\n\nAutonomous vehicles are a heated innovation area attracting interest from tech companies like Amazon, which this year patented an autonomous ground vehicle (AGV) that can leave its home, pick up a package from a depot and bring it back.\n\nWill the Amazon robot have this capability? It is possible. Tye Michael Brady and Ethan Zane Evans, the researchers behind the technique, point out \u201cthe AGVs may be owned by individual users and/or may service a group of users in a given area (e.g. in an apartment building, neighborhood, etc.).\u201d When the robot knows a delivery truck is approaching, it will navigate autonomously to meet the truck at the pickup point.\n\nYour AGV bot could even meet up with other neighbourhood AGVs to return a rake or borrow a cup of sugar.\n\nThe new Amazon home bot will likely be a wheeled, mobile, Alexa-based robot with an array of cameras and possibly other sensors. Will it have arms and suction grippers to do household chores? Possible, but Amazon seems to be aiming for multiple capabilities across a wide range of functions, not a housekeeper like \u201cAndrew\u201d in the film Bicentennial Man. The potential is huge and the possibilities endless.\n\nOnly time will tell whether Bezos and Lab126\u2019s creation will be a rolling Echo or a game-changing, \u201cwow\u201d product."
    },
    {
        "url": "https://medium.com/syncedreview/the-past-present-future-of-biotech-in-boston-150980e4860c?source=user_profile---------12----------------",
        "title": "The Past, Present & Future of Biotech in Boston \u2013 SyncedReview \u2013",
        "text": "If you are familiar with the biotech industry, you know Cambridge. The small city at the center of the Great Boston Area hosts over 1,000 biotechnology-related companies. Most of these companies cluster around Kendall Square, the same block as the Massachusetts Institute of Technology (MIT).\n\nOver the past decade the number of biotech jobs in Boston has jumped by 37%. [1] With the rapid rise in the application of Artificial Intelligence, and given the promising future of bioinformatics and computational biology, the local biotech industry finds itself in the midst of a technological revolution.\n\nThe biotech industry in Boston can be traced back to the 1970s, when molecular biology was in its golden age. The idea of \u201cplaying with genes\u201d however made many uncomfortable at the time. Cambridge City Council held a hearing on DNA experiments, and granted permission to Biogen, a new, local company founded by MIT Professor Phillip Sharp. Biogen was the first US firm to get the green light for genetic engineering. [2]\n\nBio-pharmaceutical companies quickly poured into Kendall Square, creating a well-rounded, self-vitalizing biotech ecosystem and building a global centre for biotechnology.\n\nOne key factor in Boston\u2019s rise in biotechnology is the area\u2019s academic resources, which are second to none.\n\nAlong with traditional biomedical schools such as Harvard Medical School and MIT Whitehead Institute for Biomedical Research, there are also a number of interdisciplinary programs combining biology and other informatic engineering subjects, such as the MIT Computational and Systems Biology Initiative (CSBi) and the Department of Biostatistics at Harvard. Other universities such as Tufts also have their own bioinformatics research groups.\n\nBoston\u2019s many universities are a talent pipeline, and most of the area\u2019s biotech company founders are graduates or professors from Harvard, MIT or other top universities. [4]\n\nBesides laboratories in universities, Boston\u2019s biotech industry is also backed by labs in hospitals and large pharmaceutical companies.\n\nBoston has three respected medical schools, two pharmaceutical schools, and three general hospitals. Having top hospitals not only provides more research facilities, but also more disease study opportunities. It is extremely difficult for example to do clinical testing and study on certain rare diseases which appear early in life and can claim the lives of 30% of patients before age five. The Boston Children\u2019s Hospital International Health Services division treats young patients from over 100 countries every year, and these treatment cases can inform the biotech research ecosystem.\n\nLarge medical companies are becoming more dependent on smaller biotech companies in the research field. As rising costs, patent cliffs, and other factors eat into pharmaceutical industry profits, many big medical companies are turning to Boston\u2019s innovative biotech startups to provide high-quality R&D results at lower costs. By the end of 2017, pharma giants Genzyme, Novartis, Pfizer, and Baxter had all established presences in Boston largely for this reason.\n\nCapital is an essential element in building a vibrant industry ecosystem, and as biotech has grown so has the money behind it. General venture capital and business companies are involved, along with investors specifically targeting biotechnologies.\n\nFidelity Biosciences, with US$2 trillion in assets under management, was one of the earliest venture capital companies to focus on life sciences, specifically \u201cBiopharmaceuticals, MedTech, and Healthcare IT/Services in a stage-agnostic fashion.\u201d [5] Other capital and consulting firms in this market include Flagship Ventures, MPM Capital, Locust Walk Partners, Voisin Consulting, and Fuld & Co.\n\nWhat\u2019s more, Biotech is an area where government can support businesses. Last year Massachusetts Governor Charlie Baker announced plans to invest US$500 million over the next five years in the Massachusetts Life Sciences Initiative. Baker\u2019s predecessor, Gov. Deval Patrick, had launched a US$1 billion initial investment in biotech back in 2008. [6]\n\nRespected biotechnology media company FierceBiotech publishes an annual \u201cFierce 15\u201d list of the world\u2019s most promising biotech companies, which it believes can make future breakthroughs. Cambridge area companies have taken about one-third of the spots on the list over the last five years, an achievement no other city has even approached.\n\nWhat are the most popular areas in biotech industry today? Gene editing is undoubtedly one, especially with the discovery of the CRISPR/Cas9 Method in 2011.\n\nCRISPR, for Clustered Regularly Interspaced Short Palindromic Repeats [7], is a family of DNA sequences in bacteria that contains snippets of DNA from viruses that have attacked the bacterium. These sequences play a key role in a bacterial defence system, and form the basis of a genome editing technology known as CRISPR/Cas9, which allows permanent modification of genes within organisms. [8]\n\nWith its huge application value in genome engineering, gene knockdown/activation, disease models, biomedicine and more, CRISPR technology has fostered many innovative Boston startups. Zhang Feng is the MIT Professor who first successfully applied CRISPR/cas9 in eukaryotic cells, and secured a patent for this method in 2017. [9]\n\nZhang co-founded Editas Medicine, one of the leading Boston companies focused on this technology. Another is CRISPR Therapeutics, which was co-founded by Emmanuelle Charpentier, another seminal figure in the field.\n\nIn 2015, Editas Medicine received US$120 million in Round B funding. Investors included Flagship Ventures (15.6%), Third Rock Ventures (15.6%), Polaris Venture Partners (15.6%), and bng0 under Bill Gates (9%). [10] In 2016, Editas Medicine became the first CRISPR gene editing company to make an IPO. Editas plans to start testing CRISPR in treating blindness, which would be the first instance of editing human genomes using CRISPR.\n\nOther promising players in the gene editing field include Intellia and Bluebird Bio \u2014 whose stock price has risen sixfold since it went public.\n\nThe biotech industry in Boston is booming, while facing potential challenges.\n\nLife sciences research involves a heavy time commitment in the design and execution of experiments, and this can send costs skyrocketing.\n\nEditas Medicine for example remains stuck in the preparation phase for the blindness treatment plan it announced in early 2015, although it confirmed investments of up to US$20 million at its 2016 IPO. This situation is normal in the industry. The success of a biotech company thus depends not only on its technology, but also on the confidence of the capital behind it, which will support it through such delays.\n\nSilicon Valley is another challenge for Boston. The two regions are virtually neck-and-neck in the biotech race, but as the industry starts relying more on software, wearable devices, and big data analysis, etc, this could tilt the contest in favour of Silicon Valley, which has a definite advantage in these tech areas."
    },
    {
        "url": "https://medium.com/syncedreview/pigs-cows-cockroaches-on-the-ai-animal-farm-58280d62f34f?source=user_profile---------13----------------",
        "title": "Pigs, Cows & Cockroaches on the AI Animal Farm \u2013 SyncedReview \u2013",
        "text": "Farming is becoming a data-centric business powered by artificial intelligence. China\u2019s big tech firms are using neural network-backed computer vision, wearable devices, and predictive analytics algorithms to reimagine pig, chicken, cow, goose, and cockroach farming.\n\nThe SCMP reports that Gooddoctor Pharmaceutical Group is using AI to cultivate up to six billion cockroaches per year in China\u2019s southeast Sichuan province for medical uses. The operation has generated US$684 million in revenue and is backed by AI algorithms which collect and analyze up to 80 indexes of data, catering to the roaches\u2019 humidity, temperature, and food requirements. AI also keenly monitors and stimulates the roaches\u2019 growth and breeding rates.\n\nOver the past year, Sichuan pig farming corporation Dekon Group and pig feed supplier Tequ Group have been working in partnership with Alibaba Cloud. By 2020, Dekon Group will breed up to 10 million pigs per year. The AI-backed computer vision and voice recognition systems can recognize pigs via numbers tattooed on their flanks and monitor vulnerable piglets for squeals of distress.\n\nAlibaba competitor JD.com meanwhile has launched an AI chicken breeding project wherein each chicken wears a fitness tracker around the ankle. JD.com says it will buy the birds back at triple the price once they walk one million steps. JD Finance CEO Shengqiang Wang says the company wants to rebuild the entire farmhouse infrastructure, monitoring food intake, defecation, and other physiological conditions.\n\nPoking fun at the trend on April Fool\u2019s Day, Tencent announced the \u201cgrand opening\u201d of a purported geese production facility in the mountains of Guizhou. In Chinese, \u201cgoose\u201d (\u9e45) is written one hanzi character away from Tencent\u2019s flagship mascot penguin (\u4f01\u9e45). A Tencent spokesperson claimed the company was starting a pilot goose farming project to explore the potential of \u201cAgriculture + AI + Internet Smart Retail.\u201d The company said the Guizhou operation would be located in excavated mountain caves, begin with 5,000 geese and scale up to 200,000. To further play on the hanzi pun, Tencent promised netizens it was considering adding swans (\u5929\u9e45) and of course, penguins (\u4f01\u9e45).\n\nArtificial intelligence is certainly revamping the animal farming industry, with more and more technology companies hopping onboard. Animal farming is no longer a difficult job plagued with sanitation problems. AI may provide more wholesome and sustainable solutions for this inevitable trend of mass production.\n\nIn Japan, Osaka University\u2019s intelligent cow breeding system can detect contagious viral disease in livestock with up to 99% accuracy. The system is being adopted for cowhouses with automatic milking machines and feeding robots, and several Japanese dairy farms are using it along with wearable devices to fine-tune milking and feeding and provide real-time updates.\n\nAt the same time, computer vision or data manipulating software portals are just small nodes in the bigger IoT makeover of food production. While farmers may be initially skeptical of all these new-fangled cameras, fitbits, and smartphone apps, the AI farming wave is not likely to recede, rather it may completely change the farming status quo."
    },
    {
        "url": "https://medium.com/syncedreview/google-boosting-its-ai-research-in-tokyo-608f8ba11c9?source=user_profile---------14----------------",
        "title": "Google Boosting its AI Research in Tokyo \u2013 SyncedReview \u2013",
        "text": "Google is looking to expand its AI research activities in the Japanese capital. The company\u2019s deep learning and AI research team Google Brain yesterday posted a Tokyo job listing seeking talented experts to participate in cutting edge research on machine learning.\n\nApplicants will work on real-world problems involving AI, data mining, natural language processing, hardware and software performance analysis, improving compilers for mobile platforms, as well as core search and much more.\n\nMinimum qualifications for the position are:\n\nGoogle Chief of AI division and Head of Google Brain Jeff Dean tweeted, \u201cHappy to see our #GoogleAI efforts expanding with Google Brain now having a research presence in Tokyo.\u201d\n\nGoogle Brain was initiated in 2011 as \u201cGoogle X,\u201d a project focused on building a large-scale deep learning software system. Its founding members \u2014 Dean, Google Researcher Greg Corrado, and Stanford University professor Andrew Ng \u2014 successfully built a neural network powered by 16,000 computer processors, which was trained to recognize cats in YouTube videos. The project ended up doing so well that it was upgraded into \u201cGoogle Brain\u201d with a mission to improve people\u2019s lives by making machines smarter. Google Brain has since attracted top-tier researchers such as Dr. Geoffrey Hinton, who developed back propagation; and Ian Goodfellow, who created generative adversarial networks (GANs).\n\nGoogle Brain is aggressively pursuing AI talents outside the US. In 2016, Google opened a Zurich research unit focused on machine learning, the digital assistant inside its Allo Chat app, autonomous driving efforts, and improvements to Google\u2019s search engine.\n\nGoogle opened its first office in Japan back in 2001. Headquartered in Tokyo\u2019s Roppongi Hills complex, Google Japan has since grown to a team of over 1,300."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-train-100-000-ai-talents-in-three-years-850ab9c1cc01?source=user_profile---------15----------------",
        "title": "Baidu to Train 100,000 AI Talents in Three Years \u2013 SyncedReview \u2013",
        "text": "At a tech conference in Beijing yesterday, Baidu Vice President Yaqin Zhang plotted a bold course for the search engine giant. In response to a growing shortage of AI talents, Zhang announced that Baidu will train over 100,000 AI talents with expertise in engineering and product development over the next three years.\n\nTrainees can learn AI, data analysis, and cloud computing skills on the Cloud Intelligence College (\u4e91\u667a\u5b66\u9662), an education platform launched by Baidu last year that offers online and offline courses, and awards specialized certificates to graduates.\n\n\u201cOur mission, on one hand, is to improve the employment competitiveness of trainees, and on the other hand, to ramp up companies\u2019 product R&D capabilities,\u201d says Zhang.\n\nChina is facing a shortage of five million AI talents. While the country is an AI business deployment leader with well-financed startups, its AI educational infrastructure is lagging. The US has six times more AI education institutions. Meanwhile, high demand has sent AI engineer salaries skyrocketing in China.\n\nEarlier this month China\u2019s Ministry of Education issued an action plan aimed at energizing Chinese universities\u2019 capabilities in AI technological innovation, talent cultivation, and global cooperation. The action plan also pledges to educate 500 teachers and 5,000 students over the next five years in a joint effort with Sinovation Venture and Peking University."
    },
    {
        "url": "https://medium.com/syncedreview/chip-guru-jim-keller-leaves-tesla-for-intel-34affc4a6490?source=user_profile---------16----------------",
        "title": "Chip Guru Jim Keller Leaves Tesla for Intel \u2013 SyncedReview \u2013",
        "text": "Last December, Tesla ditched its chip partner Nvidia and announced that they would make their own in-vehicle chips. The Head of Tesla\u2019s Autopilot Hardware Team Jim Keller, best known for his CPU design at AMD and Apple, was supposed to help the company make that happen.\n\nToday, however, Tesla announced Keller had left: \u201cToday is Jim Keller\u2019s last day at Tesla, where he has overseen low-voltage hardware, Autopilot software and infotainment. Prior to joining Tesla, Jim\u2019s core passion was microprocessor engineering and he\u2019s now joining a company where he\u2019ll be able to once again focus on this exclusively. We appreciate his contributions to Tesla and wish him the best.\u201d\n\nPrior to joining Tesla in January 2016, Keller served as Corporate Vice President and Chief Architect at AMD. He earned his reputation as a CPU architect master by leading design on the AMD K8 microarchitecture and Apple A4/A5 processors which empower the iPhone.\n\nKeller\u2019s departure is the latest in a series of setbacks for Tesla. The company is struggling with Model 3 production hiccups, and Moody\u2019s recently downgraded Tesla\u2019s credit rating due these delays; and an Apple engineer died last month when his Tesla Model X, with Autopilot, crashed in Mountain View, California. About the only good news for Tesla is that their stock price has not yet been seriously affected.\n\nKeller\u2019s next stop is Intel, where he will be a senior vice president and lead the company\u2019s silicon engineering, which encompasses system-on-chip (SoC) development and integration.\n\n\u201cThe world will be a very different place in the next decade as a result of where computing is headed. I am excited to join the Intel team to build the future of CPUs, GPUs, accelerators and other products for the data-centric computing era,\u201d says Keller."
    },
    {
        "url": "https://medium.com/syncedreview/liulishuos-ai-app-is-teaching-english-to-70-million-people-31d4fb38a799?source=user_profile---------17----------------",
        "title": "Liulishuo\u2019s AI App Is Teaching English to 70 Million People",
        "text": "\u201cLiulishuo is the AI English teacher on your phone. You don\u2019t need to know how it works, yet it helps you learn English more efficiently than a human teacher,\u201d says Yi Wang, Founder and CEO of Liulishuo \u2014 a Beijing-based \u201cAI + language\u201d company whose name translates as \u201cspeak fluently.\u201d Liulishuo hosts the world\u2019s largest speech bank of Chinese speakers speaking English.\n\nAfter obtaining a PhD in computer science from Princeton, Wang worked as a product manager at Google in California. He returned to China in the early 2010s and found many of his friends asking similar questions: \u201cHow do I learn English?\u201d; \u201cWhy is it that I pay so much money for lessons and fail to keep up?\u201d; \u201cShould I watch more American TV shows to learn to speak naturally?\u201d\n\nWang wondered whether people might be able to learn English by speaking with their phones. At the time, AI and online education were much less developed. In 2012 Wang launched Liulishuo with Hui Lin, a Google coworker specializing in voice recognition and machine learning.\n\nLaunched six months ago, the company\u2019s flagship app is now teaching English with personalized and adaptive methods based on deep learning to some 70 million registered users in 175 countries. It was selected as an Apple Store \u201cBest App of the Year,\u201d and Apple VP Philip Schiller and his team visited the company\u2019s offices. Liulishuo is the only Chinese Education company to make the CB Insights AI 100 list.\n\nSynced recently spoke with Liulishuo Yi Wang about his company and AI language learning.\n\nLiulishuo has the world\u2019s largest database of Chinese people speaking English. Based on the database we created a Chinese-speaking-English recognition engine with the highest accuracy and an evaluation engine which provides users with ratings and feedback.\n\nInitially, we used speech recognition to evaluate the user\u2019s verbal skills. Now our engine can perform a full range of verbal language assessments, and we have a separate engine that can correct writing.\n\nWe have a special function tackling the IELTS exams, which tests users on their pronunciation, grammar, vocabulary, and fluency. The scoring algorithms have passed the Turing Test. The variation in score between our AI and a human examiner is lower than between two human examiners. We offer users individualized suggestions to improve their English using their scores in the four areas.\n\nWe launched the world\u2019s first AI English teacher platform in July 2016. Ten million users have already completed our ranking exam, with completion time ranging from 5 minutes to 20 minutes depending on proficiency.\n\nAfter completing the exam, the system selects a starting level for each user. Users improve their English through the immersive learning of scenes from images and animations, without any subtitles or translations. They must attempt to understand the scenario, which is followed by suggested practices, which is then followed by more progressive scenarios. This back and forth is highly effective in improving the user\u2019s English language skills.\n\nWe have the data to prove our effectiveness. ETS TOEFL testing has proven our AI teacher can improve learning efficiency by three times. For example, if it used to require 100 hours of learning to reach a certain level in the CEFR standard, we would only need about 36. Regardless of product and service, we\u2019re the only English learning organization that publishes efficiencies of this caliber, and that really excites me.\n\nBefore launching the product, we asked some American English speakers to record audio for us to cold start the engine. We also collected limited data from Chinese speakers audio recordings via crowdsourcing.\n\nBut since the launch of our product, our users have provided us with massive amounts of incoming data in different skill levels and accents. This data was recorded by users reading what\u2019s displayed on their screen, and so it is also labeled. We effectively received all this data for free from users practicing their English.\n\nIn order to train our model, we invited experts to label our data for us, for example for IELTS we asked IELTS examiners to label our data.\n\nWe have two types of content. The free content is English conversations written by professional writers, on top of User-Generated Content (UGC) and Professionally Generated Content (PGC). We also have the most active English learning community in China, and many of our short videos are contributed by these learners.\n\nThe paid content is created by our own team. We hired Philip Lance Knowles, who has previously proposed Recursive Hierarchical Recognition Theory and other breakthroughs in language learning theory based on cognitive science, as our expert consultant and created our content based on Lance\u2019s theory. Of course, our customized learning material is different from writing a textbook, where all students learn in the same sequence.\n\nThis is the general direction, and we are doing some early stage testing.\n\nThese headphones can benefit let\u2019s say seniors traveling in another country. But I think they won\u2019t be able to replace the language learning market. Firstly, the Ministry of Education will not remove English from the curriculum just because we have translation machines. Secondly, learning a new language isn\u2019t just about translation, translation is just application.\n\nLearning a new language is about learning to communicate with others, and there are cultural contexts one must also learn to understand in order to learn real communication. In the process of learning a new language, there\u2019s a sense of accomplishment, in which the user builds confidence and challenges themselves. Thus we see the social function of language learning, as many people learn to make new friends. There are people who make friends and even find their partner on our platform. In this sense, you can\u2019t equal language learning with translation.\n\nWe want the learning process to be customized and highly effective. To achieve these two goals, we believe that data-driven AI is the key. We\u2019ve only taken the first step in exploring AI teachers. They aren\u2019t intelligent enough just yet, and the learning experience has many areas that can still be optimized. We are working hard at solving these problems.\n\nPeople have limited understanding of how our conscience and brain actually work. We are working with many experts in neuroscience and education, such as the Dean of Education Faculty at Stanford University and a Professor of Neurology at Yale University, in hopes of bringing in new research results. Our platform is also useful for their research because we have large amounts of user data that helps them create new learning models. We have set up an education and AI lab in the Bay Area, hoping to attract top experts in AI, education, and cognitive science, in order to help us create the most intelligent, most efficient AI English teacher in the world.\n\nThe explosive growth of the mobile internet since 2012 has turned mobile device usage into the new way of life. I saw this as an immense opportunity, but I realized that if I only developed small apps focusing on weather, calendar, camera etc, it would be a challenge to make them profitable. Therefore we thought about combining the mobile internet with traditional industries. The markets must be large, with good user paying habits, and room for improving efficiency. We researched applications in finance, health, and education, finally deciding to go with education.\n\nWe set forth to create an easy-to-use product. The first week Liulishuo was available, it was recommended by the Apple Store in mainland China, Hong Kong, Taiwan and Japan, and quickly became one of their \u201cBest Apps of the Year.\u201d Apple\u2019s Senior VP of Worldwide Marketing soon toured our company. This shows that our product-centered strategy is being rewarded.\n\nThe second turning point was the transformation of Liulishuo from a tools App into a community App. Building a community increased user stickiness and activity and created a broad learning environment.\n\nThe third point was in 2014, when we made a strategic decision to create an educational research team, to involve education professionals from a content perspective. Before this, we were purely an internet company. We decided to put a heavy focus on the essence of education and personalization of content. If we had not made that shift, we would just be a marginalized tech company.\n\nIn 2014 we worked with a foreign company and tested two learning techniques which used word games to practice speaking. But they weren\u2019t very successful. From a gameplay perspective, they weren\u2019t as fun as normal games; while from a learning perspective, they weren\u2019t very effective.\n\nDuring the second half of 2014 we wanted to create a textbook product. Our first instinct was to license the best textbooks from top publishers such as Pearson, Cambridge, or Oxford. After we got to know them a bit better, we realized that these textbooks were written and designed for traditional, offline classroom scenarios and were not effective for users lounging on their couch learning with a smartphone. Therefore, we started to work on our own educational research team and spent two years developing the AI teacher. This was a strategic change, and looking back it was the right thing to do.\n\nWe are seeing three historical opportunities. Firstly, learning is now digitized. In the past we did our homework on paper, whereas today 100% of user learning, actions, and interactions are digitized. This is a huge leap forward and the only way to make it possible to use AI. If you\u2019re not digitized and have no structured data, it will be impossible to talk of AI, right?\n\nSecondly, I think we\u2019re experiencing a historic leap from teacher-centric to student-centric learning. There were many more students than teachers in the industrial era, but many students are now practicing one-on-one with a language tutor. However, this is a transition phase because the so-called \u201cone-on-one\u201d is still not necessarily centered around the student, as teachers may not understand the needs of the student and create suitable teaching plans.\n\nLiulishuo\u2019s AI teacher is not human, it is a system that relies on user interactions to make decisions. Through deep learning and other AI technologies, it selects only relevant content from a huge library and recommends it based on the student\u2019s level. I think the pace of review and practice frequency based on an individual student\u2019s needs, strengths and weaknesses is the ultimate student-centric learning experience.\n\nThird, from a business perspective, we believe that we must develop a results-oriented business model to replace a process-oriented one. In a language training institution for example, if you learn for 100 hours and yet still see no improvement, the institution won\u2019t be responsible as it has delivered its service by selling you the teacher\u2019s time. Hence, these traditional training institutions are just wholesalers of teachers\u2019 time. We think this situation will eventually change, and educators will get paid according to the results achieved by each student.\n\nOur paid product works exactly this way, it does not charge based on instructional hours, but instead, provides users with a buffet. For just CN\u00a599 a month, users can spend as much time as they like there. A diligent student can learn at a much faster pace, absorbing all that they can. Our paid users on average spend five hours or more learning on our App each week. Who spends five hours a week learning a new skill anymore as an adult? This shows our product is really effective.\n\nI hope that in the next two to three years Liulishuo can assemble a leading team of researchers and product designers with full-stack development capabilities, dedicated to applying AI to education. As for long-term plans, I hope that in the next decade, we can become a global leader in education."
    },
    {
        "url": "https://medium.com/syncedreview/pytorch-releases-major-update-now-officially-supports-windows-2426c9f29d2d?source=user_profile---------18----------------",
        "title": "PyTorch Releases Major Update, Now Officially Supports Windows",
        "text": "PyTorch, an open source machine learning library for Python, today announced the release of PyTorch 0.4.0 with Windows support. \n\n \n\n PyTorch can now be installed on Windows OS via Conda or Pip command line. The new version also merges Tensor and Variable, which means torch.autograd.Variable and torch.Tensor are now in the same class; and unifies the return 0-dimensional vector of size, which makes it more similar to NumPy features. Also, a set of more flexible context managers has replaced the volatile flag.\n\n \n\n As a Facebook-backed open source package released in October 2016, PyTorch has been very well-received in the developer community, and has more than 14.4k stars on GitHub (Google-backed TensorFlow has 97.4K stars, and Amazon-backed Apache MXNet has 13.7K stars on GitHub). It can leverage the capability of GPU, speed up computing for AI tasks, provide GPU-friendly NumPy functions, and robustly support Tensor. \n\n \n\n Detailed update content:\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/new-petuum-cmu-paper-identifies-statistical-correlation-among-deep-generative-models-1d9afc5abb87?source=user_profile---------19----------------",
        "title": "New Petuum & CMU Paper Identifies Statistical Correlation Among Deep Generative Models",
        "text": "On April 17th, researchers from Carnegie Mellon University and Petuum, a Pittsburgh-based CMU spinoff focused on artificial intelligence platforms jointly published On Unifying Deep Generative Models. The paper introduces a high-level theoretical connection between various deep generative models, particularly Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). It has been accepted as a 2018 ICLR Conference Paper.\n\n \n\n The paper\u2019s researchers suggest that GAN and VAE lack a unified statistical connection due to their distinct generative parameter learning paradigms. Researchers derived a new GAN formula that has many similarities with VAE, which could spark innovations in R&D of GANs and VAEs, and help researchers discover new common rules of machine intelligence that were previously undetected.\n\nAccording to the original post, many advantages can be achieved by this unified statistical view:\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/mckinsey-report-ai-promises-added-value-of-up-to-us-5-8-trillion-80cc0043ebf6?source=user_profile---------20----------------",
        "title": "McKinsey Report: AI Promises Added Value of Up to US$5.8 Trillion",
        "text": "McKinsey Report: AI Promises Added Value of Up to US$5.8 Trillion\n\nAlthough artificial intelligence has triumphed over the world\u2019s best human players at Go and Poker, and outperformed humans at imaging analysis and speech recognition, few are aware of the actual dollar value that AI techniques can bring to industries such as travel, retail, transport & logistics.\n\nThe McKinsey Global Institute this month released the report Notes From the AI Frontier Insights From Hundreds of Use Cases. The 36-page discussion paper surveys cutting-edge machine learning algorithms, and discusses how they can be integrated or transformed into practical applications across 19 selected industries.\n\nAI can potentially create US$3.5\u20135.8 trillion in annual value\n\nThe report defines AI as deep learning techniques based on artificial neural networks, such as feed forward neural networks, recurrent neural networks (RNN), and convolutional neural networks (CNN). These algorithms have grown from fledgling research subjects to mature techniques in real world use. Advanced AI techniques such as generative-adversarial-networks (GANs) and reinforcement learning are not within the scope of the report.\n\nIn the 19 industries studied, AI\u2019s potential annual value was between US$3.5 trillion and US$5.8 trillion. Retail is the industry expected to be most impacted by AI at US$0.4\u20130.8 trillion, followed by travel (US$0.3\u20130.5 trillion), and transport & logistics (US$0.4\u20130.5 trillion). Marketing & sales, and supply-chain management & manufacturing are sectors where AI can help companies grow US$1.2\u20132.6 trillion in annual revenue.\n\nAI can increase value up to 128 percent over traditional analytic techniques\n\nThe report says AI is more likely to improve performance over other analytic tools in 69 percent of the use cases McKinsey studied.\n\nThe industries with most potential incremental value benefit from AI compared to analytical techniques are travel (128%), transport & logistics (89%), and retail (87%). Industries at the bottom of the list are insurance (38%), advanced electronics/semiconductors (36%), and aerospace & space (30%).\n\nSixteen percent of the report\u2019s use cases are new applications developed by AI techniques, for example a smart customer service assistant in retail or medical imaging detection in healthcare.\n\nGetting accurate labeled data to train AI models is challenging\n\nLeveraging AI algorithms requires a large amount of clean and labeled data. The textbook Deep Learning, written by Google Researcher Ian Goodfellow and Head of the Montreal Institute for Learning Algorithms (MILA) Yoshua Bengio, suggests that a deep-learning algorithm can achieve acceptable performance by training with 5,000 labeled examples per category. If a model is expected to match or even exceed human level performance, it has to be trained with a dataset of at least 10 million labeled examples.\n\nCollecting large-scale datasets is challenging, particularly in industries such as healthcare where there is not so much available or useable data. Also vexing is data processing, including data cleansing and labeling, which is a difficult and time-consuming engineering problem. While advanced techniques such as reinforcement learning and GANs can effectively simulate data for academic research, they are not mature enough for wider implementation.\n\nMeanwhile, AI still has other limitations that need solutions. Interpretability, also referred to as \u201cblack box\u201d problem, means that even scientists cannot explain how an AI arrives at a decision. Google researchers recently attempted to create a step by step visualization of the process involved in a computer recognizing an object.\n\nThink twice before you embrace AI\n\nMcKinsey analysts suggest that AI is an elusive proposition for many companies as it remains unclear whether injecting huge investments in AI is worth the potential value the tech promises. There is also the concern that any careless technical executions could cause unpredictable, expensive or grave consequences, especially in sensitive fields like healthcare or legal systems.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-in-the-media-and-entertainment-industry-1ad4b2b701b8?source=user_profile---------21----------------",
        "title": "AI in the Media and Entertainment Industry \u2013 SyncedReview \u2013",
        "text": "AI in the Media and Entertainment Industry\n\nThe Media and Entertainment industry is a cornerstone of contemporary human culture, delivering films, TV shows, advertisements and more in a multitude of languages across a wide variety of devices. A PwC report predicts total M&E revenue will reach US$2.2 trillion in the next three years. The industry\u2019s growth rate however has lagged, and slipped by 0.2% in 2017, prompting many companies to turn to AI technologies to boost business.\n\nWith the breakthroughs in machine learning, many intelligent products have made the leap from sci-fi movies to the home. Superhero Ironman\u2019s virtual helper JARVIS (Just A Rather Very Intelligent System) for example is echoed in smart assistants such as Alexa and Google Assistant, which may not catch criminals but can perform a range of practical tasks via connected household devices. NVIDIA meanwhile used VR technology to create a Holodeck similar to one in the sci-fi series Star Trek.\n\nAI technologies are also being applied in filming, visual design, post production etc.\n\nCurrent AI applications in the M&E industry are mainly in four categories: Marketing and Advertising, Service Comprehension, Search and Classification, and Experience Innovation.\n\nThe marketing and advertising sector includes visual design, film promotion and advertising. A machine learning algorithm trained with data such as text, stills and video segments can extract language, objects and concepts from its training resources and suggest marketing and advertising solutions to improve efficiency. Such a system can work as an assistant or even a content creator.\n\nAlibaba\u2019s Luban is an AI designer that can create banners thousands of time faster than a human designer. On China\u2019s online shopping extravaganza \u201cSingles Day\u201d in 2016 Luban generated some 8000 different banner designs per second and 170 million banners in total. The record output would of course be impossible for human designers to process in one day. On Singles Day 2017 Luban raised its one-day record to a staggering 400 million banners.\n\nIBM used their AI system Watson to help 20th Century Fox create a trailer for the horror movie \u201cMorgan.\u201d The research group trained the AI system to analyze and classify input \u201cmoments\u201d from visual, audio, and other composition elements in 100 horror movies to learn what kind of \u201cmoments\u201d should appear in a standard horror movie trailer. Watson needed just 24 hours to create a six-minute movie trailer that may have taken a human professionals weeks to produce.\n\nAlbert Intelligence Marketing\u2019s AI marketing platform accelerates the marketing process using predictive analytics, machine learning, NLP and computer vision technology. The Albert platform can perform audience targeting, customer solution making and generate autonomous campaign management strategies. Albert says companies using the platform report a 183% improvement in customer transaction rate and 600% higher conversation efficiency.\n\nAs user experience personalization becomes more important for the industry, companies are using AI to create personalized services for billions of customers. These include for example recommending content that fits users\u2019 personal tastes while they are browsing a video site or shopping online; and optimizing video definition and fluency for users with different internet speeds and bandwidth.\n\nNetflix\u2019s content recommendation got a boost in May 2016 when the company launched Meson, an intelligent workflow management and scheduling application. This AI system automatically manages the various machine learning pipelines that provide video recommendations. According to the Netflix 2016 annual report, there are 93 million global users streaming over 125 million hours of TV shows and movies per day on the platform. Predicting which shows will attract users\u2019 interest is a key component of the Netflix model.\n\nAI technology is also being applied to optimize video fluency and definition. Slow Internet connections and bandwidth limits can be a problem for streaming services in developing nations and for mobile device users. Netflix collaborated with the University of Southern California and the University of Nantes in France to develop a new machine learning methodology called Dynamic Optimizer which can compress video without degrading image quality to ensure a smooth and high quality streaming experience for its customers, whether they are in India or in Japan.\n\nThe Internet hosts countless media works. Video, audio and text can all be transformed into a digital copies which can be stored and spread so easily that it is getting increasingly difficult for people to find exactly what they want online. AI is helping optimize the accuracy of search results. Computer vision technologies meanwhile are also enabling content producers to better manage visual content and accelerate the media production process.\n\nAdvancements in machine learning technology have enabled Google to augment the world\u2019s leading search engine in multiple ways. One is in image searching. Rather than typing in keywords and checking returned images, users can upload a sample picture to Google Image, which uses image recognition technology to identify image features and search for similar pictures. Another advanced application involves selective link-building. Google applies AI to position ads appropriately \u2014 for example so a cat food ad appears in a pet-related website, but a bacon cheeseburger promotion will not appear on a site for vegetarians.\n\nClarifAI is an AI startup focusing on computer vision technology which partnered with Vintage Cloud to deploy AI on a film digitalization platform. By using ClarifAI\u2019s computer vision API, Vintage Cloud successfully accelerated the progress of movie content classification and categorization. It used to require dozens of hours for humans to recognize and manually classify objects in a movie. AI can do a better job in much less time.\n\nIn the past, papers and books were the main medium for words and images. The introduction of film and TV brought us into the dynamic new world of moving pictures. Now, AI is heralding a new age of immersive experience for visual content. This technology includes Virtual Reality (VR) and Augmented Reality (AR). With machine learning algorithms and computer vision technologies, developers can build complex and holographic scenes within a pair of goggles. This opens up a brand new market.\n\nVR gaming is one of the first areas that comes to mind, and this is where companies like HTC Vive, Samsung Gear VR, Oculus Rift, etc. are focusing their efforts. Various type of headsets have been introduced. Combined with motion sensing games, VR gaming innovation has become a hot market that shows no sign of slowing down.\n\nIntel is now getting into the immersive experience industry. With the application of deep learning and computer vision technology, Intel has become a visual content provider emphasizing Virtual Reality content. Supported by AI algorithms, Intel True VR Technology can perform every piece of a scene with pixels in three-dimensions.\n\nUsing the tech, fans can also watch sports in holographic view. Intel demonstrated this in their widely viewed VR game broadcast of the NFL 2018 Super Bowl. Intel partnered with the International Olympic Committee to broadcast the 2018 Winter Olympic Games as 360-degree video content. With a VR headset or even just a smartphone, fans and families could experience the action from the POV of an athlete.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/tomaso-poggio-on-deep-learning-representation-optimization-and-generalization-66bb8c8e524f?source=user_profile---------22----------------",
        "title": "Tomaso Poggio on Deep Learning Representation, Optimization, and Generalization",
        "text": "Those outside academia may know Tomaso Poggio through his students, DeepMind Founder Demis Hassabis and Mobileye Founder Amnon Shashua. The former built the celebrated AI Go champion AlphaGo, while the latter has installed copilot systems in more than 15 million vehicles worldwide, and produced the world\u2019s first L2 autonomous driving system in a car.\n\nWhile Poggio the teacher has taught some extraordinary leaders in AI, Poggio the scientist is renowned for his theory of deep learning, presented in papers with self-explanatory names: Theory of Deep Learning I, II and III.\n\nHe is a Professor in the Department of Brain and Cognitive Sciences, an investigator at the McGovern Institute for Brain Research, a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Director of the Center for Biological and Computational Learning at MIT and the Center for Brains, Minds, and Machines.\n\nPoggio\u2019s research focuses on three deep learning problems: 1) Representation: Why are deep neural networks better than shallow ones? 2) Optimization: Why is SGD (Stochastic Gradient Descent) good at finding minima and what are good minima? 3) Generalization: Why is it that we don\u2019t have to worry about overfitting despite overparameterization?\n\nPoggio uses mathematics to explain each problem before inductively working out the theory.\n\nPoggio and mathematician Steve Smale co-authored a 2002 paper that summarized classical learning theories on neural networks with one hidden layer. \u201cClassical theory tells us to use one layer networks, while we find that the brain using many layers,\u201d recalls Poggio.\n\nBoth deep and single-layer networks can approximate continuous functions. This was one reason why research in the 80s focused on simpler single-layer networks.\n\nThe problem occurs in the dimensionality of single-layer networks. In order to represent a complicated function, a single-layer network would require more units than the number of atoms in the universe. Mathematically, this is called \u201cthe curse of dimensionality,\u201d wherein the number of parameters goes up exponentially corresponding to function dimensionality.\n\nMathematicians make assumptions about function smoothness in order to escape the curse of dimensionality. Yet deep learning offers a different approach that uses compositional functions. The units that deep networks require to approximate a compositional function share a linear relationship with function dimensionality.\n\nDeep learning works beautifully for datasets that are compositional in nature, such as images and voice samples. Images can be broken down into related snippets of details, while voice samples can be converted into meaningful phonemes. For an image classification task, there\u2019s no need to look at pixels that are further apart, the model simply observes each small bit and combines them. The neural network escapes the curse of dimensionality by using a very small number of parameters.\n\nIf the target is a function made up of functions with a smaller number of variables, then a deep network can approximate it with a number of units that is linear in dimensionality no matter how big the function is.\n\nKnowing that compositional functions work well with deep networks is far from enough. \u201cFor a computer scientist or a mathematician, can we say something more about compositional functions beyond the fact that they\u2019re compositional? Can we characterize them to get a better understanding of neural networks? This is an interesting open question,\u201d says Poggio.\n\nDeep networks have far more parameters than the number of examples in the training set.\n\nThe CIFAR dataset has 60,000 examples, and we use networks with millions of weights to process it. This is a typical case of overparameterization. We can make a hypothesis to simplify the matter: if one replaces nonlinear neurons in the deep network with univariate polynomials, then getting zero training errors on CIFAR means solving 60,000 polynomial equations. We now have infinite sets of solutions according to B\u00e9zout\u2019s theorem, which ends up becoming the dataset\u2019s infinite global minima.\n\nThus overparameterization guarantees lots of global minima that form flat valleys in the loss space. As SGD is known for its preference for flat valleys, there is a high probability that SGD will find the global minima for neural networks.\n\nPoggio\u2019s work showed that a combination of overparameterization and SGD simplifies the optimization of neural networks.\n\nOverparameterization is good news for optimization, but a nightmare when it comes to generalization. Test errors go down but then up again in classic machine learning, which is called \u201coverfitting.\u201d Yet in deep learning, overfitting is not reported, and so the test error rate goes down and stays there.\n\nWhy is this the case? Poggio likens this to a \u201cchemical reaction\u201d that occurs when classification tasks are mixed with a specific type of loss functions called cross entropy.\n\nAlthough we can use 0\u20131 loss to evaluate error rates, we need an alternative approach when it comes to loss function. Take handwritten digit classifiers as an example, the last step for the neural network is to turn a softmax into a \u201chardmax\u201d, in other words, a class. Thus, even if we only have a bad model that is only 30% sure that the \u201c1\u201d we show it is a \u201c1\u201d, as long as 30% is the highest among the given 10 possibilities, the model will classify the image correctly. Of course no one would be satisfied with a 30% success model. The model needs further optimization, which can\u2019t be done using a 0\u20131 loss.\n\nIn case of cross entropy, as long as the model is not 100% certain, one can continue to optimize it by calculating another gradient, and use backpropagation for fine-tuning. On a side note, the favourable property of using cross entropy as the loss function and 0\u20131 loss as error metrics is that, even when cross entropy is overfitted, the 0\u20131 loss will work just fine. A few months ago, University of Chicago researcher Nathan Srebro and his colleagues proved this for a special case of linear networks with separable datasets.\n\n\u201cOn top of [Srebro\u2019s work], we\u2019ve shown that using differential equations from dynamical system theory will make a deep network behave like a linear network near a global minimum. We can use the Srebro result to say the same thing about deep learning, even if a deep neural network classifier has an overfitting cross entropy, the classifier wouldn\u2019t overfit,\u201d says Poggio.\n\nThis property of cross entropy is shared with loss functions such as exponential loss, but not with simpler ones like the least square error. Why is this the case? Poggio says this remains an unsolved question.\n\nPoggio says his opinion on the shape of minima and their corresponding generalization capabilities has changed recently: \u201cPeople said in papers that flatness is good for generalization. I also said something like this a year ago, but I don\u2019t think this is true anymore.\u201d\n\n\u201cI don\u2019t see a direct relation between flatness and generalization. Generalization relies on properties like choosing classification as the task, choosing cross entropy as the loss function but not flatness. There\u2019s a paper of which two out of four authors are Bengios, which proves that even sharp minima can generalize because you can change the weights in different layers to make it sharp without changing the input-output relation of the network.\u201d\n\nPoggio also doesn\u2019t think it\u2019s possible for a flat minimum to exist, at least not for neural networks that are polynomial in nature.\n\nLearning deep learning theory can be enlightening, but engineers working on applications ask the question: How can theoretical research work help me train my model?\n\nThe No Free Lunch Theorem tells us that two learning algorithms are equal when no prior information is offered for distribution. For any algorithms A and B, there are as many distributions in which algorithm A outperforms B, as distributions in which B outperforms A.\n\nPoggio utilizes the theorem to propose that in machine learning, no algorithm can work the best for every problem. \u201cTheory tells you about the average case and the worst case, what you should or should not do to avoid bad things. But it can\u2019t advise you on the best thing to do for any particular case.\u201d\n\nPoggio suggests engineers who employ deep learning models be careful of overfitting, \u201cOne lesson to learn from the past few decades of machine learning is that when you don\u2019t have enough data, then after many trials, the state-of-the-art method is usually overfitting. It\u2019s not because people have peeped at the validation set, it\u2019s just that the community of researchers has tried too many different algorithms.\u201d\n\n\u201cI\u2019m a physicist by origin. When I was in school, the rule of thumb was that if you have a model or a set of equations with n parameters, you need at least 2n data points. If you want to do something statistical, the recommendation was to have 10n data points. Nowadays people use 300,000 parameters for datasets of any size. The arguments we make like \u2018deep learning models tend not to overfit\u2019 is only true for classification tasks with nice datasets, so people should be more careful about that.\u201d\n\nHumans don\u2019t need millions of pieces of labeled data to learn, thanks to prior knowledge carried in our genes. \u201cThere is not a simple answer for how many priors we need for a model. There are only situations where we know the minimum priors needed to make predictions.\u201d\n\nPoggio uses regression as an example, \u201cIf I want to reconstruct a curve from points, I can\u2019t do anything unless I have all the points. Continuity is essential but not enough, the least I need is something like smoothness. In the end, it\u2019s a tradeoff between how strong the priors are and how much data you need.\u201d\n\nMIT has a tradition of knitting together deep learning and neuroscience, so what is Poggio\u2019s view on learning from the human brain?\n\n\u201cI think it\u2019s unlikely, but not impossible, that things like backpropagation can be done biologically, given what we know about neurons and signal processing. What I think is impossible is labeling everything.\u201d\n\nHow our brain gets around labeling is an interesting question. Poggio assumes that our visual system for example is pre-trained to \u201ccolour-fill\u201d an image. It receives the colour information but only gives black, grey, and white signals to the visual cortex. You do not need an oracle to tell you what the real colour is, your brain hides this part of the information, so that \u201ccolours are measured but not given to the [brain] network,\u201d explains Poggio.\n\n\u201cThe hope is that if you train a network to predict colour or the next image, can this network do other things easier? Can it learn to recognize objects with less data?\u201d asks Poggio. \u201cThese are open questions that, once we get the answers, the whole deep learning community would benefit from.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-test-self-driving-food-delivery-with-meituan-eb3916f3ac83?source=user_profile---------23----------------",
        "title": "Baidu to Test Self-driving Food Delivery with Meituan",
        "text": "Xiongan City is getting a new food delivery service that requires no interaction with humans.\n\nChinese media is reporting that Baidu will launch a series of self-driving delivery tests in cooperation with Meituan-Dianping, a group-buying service for local food and retail businesses. The first tests will take place in Xiongan, about 100 kilometres southwest of Beijing.\n\nAn insider familiar with the matter said testing was slated to begin on May 1, however Xiongan is still building the required infrastructure, and so the official announcement will be postponed.\n\nAutonomous vehicles can not only cut labor costs in the food-delivery business, but can also for example reduce security issues for batch deliveries to restricted locations such as construction sites.\n\nBaidu is leading the development of China\u2019s autonomous driving technologies. Last June, the search engine giant introduced its autonomous driving platform Apollo, which is billed as \u201cthe Android of the auto industry.\u201d Apollo aims at democratizing autonomous driving with open-source data and code. Last month, Baidu became the first company granted special license plates for autonomous vehicle testing on public roads in Beijing.\n\nAlso on Thursday, Baidu released the latest iteration of its Apollo platform, Apollo 2.5, which now supports autonomous driving on geo-fenced highways. Baidu also announced the new Apollo Automotive Cybersecurity Lab, which aims to advance safety in mobility.\n\nMeituan set up a project team on self-driving delivery R&D in 2016. Last year the team was promoted to a business division with a staff of over 200.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-national-ai-team-gets-busy-46f226d7d054?source=user_profile---------24----------------",
        "title": "China\u2019s \u201cNational AI Team\u201d Gets Busy \u2013 SyncedReview \u2013",
        "text": "Words often have different meanings in the business world. A \u201cUnicorn\u201d for example is not a magical horse with a horn projecting from its forehead, but rather a startup that\u2019s valued at more than a billion dollars.\n\nA phrase that\u2019s cropping up a lot in Chinese media these days is \u201cNational AI Team (\u4eba\u5de5\u667a\u80fd\u56fd\u5bb6\u961f).\u201d While this may suggest a squad of scientists heading off to some sort of AI Olympics, it rather refers to a new class of AI companies that are either backed by national institutes or closely integrated into government-funded programs.\n\nWhy are they calling themselves the National AI Team?\n\nCloudWalk, a spinoff from Chongqing Institutes of Green and Intelligent Technology (CIGIT) of the Chinese Academy of Sciences (CAS), is a typical National AI Team member. Founded in 2015, the company specializes in computer vision and machine learning.\n\nIn China, core industries such as public security, banking, political communication, civil aviation, and urban transport are being overhauled through the concerted application of various AI techniques. Security bureaus for example want object tracking systems to help spot criminal activity and hunt fugitives, while banks are using facial recognition algorithms to provide customers with almost-instant access their bank accounts. The government owns or controls most of these industries, and they are rigorous in selecting their technical partners.\n\n\u201cIf a bank\u2019s system goes down for two hours, its governor will have a problem; if it doesn\u2019t work out in four hours, he will write a report; if it doesn\u2019t work out in eight hours, it\u2019s going to be a serious incident, the bank\u2019s rating is bound to drop, and the bank may even be closed,\u201d CloudWalk Founder and CEO Xi Zhou (\u5468\u66e6) told Synced.\n\nWhile a private company\u2019s best way to attract government clients is robust technology or products, being on the National AI Team can also give it an advantage in core industries. Over 100 banks have adopted CloudWalk\u2019s facial recognition technique, making it the largest provider of the tech to China\u2019s banking industry. The company also develops and deploys facial-recognition-enabled surveillance cameras in over 80 percent of domestic airports.\n\n\u201cI was not aware of this advantage until I founded CloudWalk,\u201d says Zhou. \u201cObviously the state-owned banks and public security departments have more faith in our technologies and products because we come from the Chinese Academy of Sciences.\u201d\n\nWhat if a startup does not enjoy a relationship with a state-owned institute? They might also look for investment from a state-owned venture. China\u2019s computer vision startup Face++ announced financing of US$460 million last November led by the China State-Owned Capital Venture Investment Fund, while the Guangzhou Municipal Government injected US$300 million into CloudWalk\u2019s Series B funding round.\n\nThe Chinese Government owns a large-scale database, which is one of the most important components for developing AI applications and products. Li Xu (\u5f90\u7acb) is the CEO of SenseTime, a company that uses deep learning to replicate tasks performed by the human visual system. In an interview with Quartz, Xu boasted that his company has a database of over two billion images. Much of that data comes from various government agencies.\n\nSenseTime raised US$410 million last June, including a B2 round led by Sailing Capital, whose major shareholder is the large state-owned financial holding company Shanghai International Group. SenseTime is now the world\u2019s most valued AI startup. The injection of the state-owned capital is expected to help SenseTime secure more government contracts. At present, 30 percent of SenseTime\u2019s clients are government-related.\n\nWhile National AI Team status may be an \u201cexpress lane\u201d to government deals, the role also comes with responsibilities. Last year China issued an ambitious policy blueprint calling for the nation to become the world\u2019s primary AI innovation center and aiming to build a domestic industry worth some US$150 billion by 2030. The government is counting on its National AI Team to put that plan into practice.\n\nOver the past five years, the world has seen an explosion of activity in deep learning in both academia and industry all around the world. While AI is energizing industries, the lack of industry-wide standards for research and development is creating a number of problems: How to correctly access users\u2019 data without violating privacy? How to qualify a dataset for use in training AI algorithms? How to ensure the safety and precision of an AI application?\n\nCarlos E. Perez, author of the book Artificial Intuition and Founder of Intuition Machine, stressed the need to standardize the best practices of developing deep learning in his medium blog. \u201cIn conventional software development, we have a more mature conceptual framework that has evolved over time. Deep Learning introduces new kinds of requirements, so we need to understand what these are and what standardize the class of tools needed.\u201d\n\nThe Chinese Government is pushing hard for AI standardization. Last year it released its Standardization of AI Helps Industry Development along with a blueprint for further AI integration with the economy. This year it followed up with the Artificial Intelligence Standardization White Paper.\n\nIn March 2017 the National Development and Reform Commission launched the Public Service Platform for Basic Resources (\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u8d44\u6e90\u516c\u5171\u670d\u52a1\u5e73\u53f0) with a heavy focus on AI. This platform will be built over the next three years and aims to deliver:\n\nProminent National AI Team players Baidu, Tencent, iFlytek, and CloudWalk were designated to take the lead on the Platform\u2019s development.\n\nChina\u2019s voice technology giant iFlytek was founded by alumni of the University of Science and Technology of China, a national research university, while Baidu and Tencent have dominated the country\u2019s tech scene for years.\n\nCloudWalk spokesperson Xiaolong Fu told Synced, \u201cAI is closely related with data and privacy, so National AI Team members can help ensure information security. Meanwhile, we also want to ensure that our intellectual property remains in our own hands.\u201d\n\nChina\u2019s quest to develop cutting-edge AI technologies provides a world of opportunities for companies on the National AI Team, particularly with the increasing number of government initiatives and state-run projects in the field. The Team may also spur development and deployment of improved AI resources for academic, industry and public use.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/tencents-smart-speaker-tingting-brings-wechat-to-the-entire-family-e0787eba8391?source=user_profile---------25----------------",
        "title": "Tencent\u2019s Smart Speaker Tingting Brings WeChat to the Entire Family",
        "text": "Tencent\u2019s WeChat is like the Chinese equivalent of WhatsApp + Facebook. Each day hundreds of millions of users press the WeChat Hold to Talk button to send voice messages.\n\nNow Tencent wants to free up its users\u2019 hands. The Chinese Internet giant today officially launched its first-ever smart speaker, Tencent Tingting (\u817e\u8baf\u542c\u542c), which can access users\u2019 WeChat accounts and send or receive voice messages. Moreover, Tencent Tingting can automatically convert text messages to voice format and read them out to you.\n\nThe speaker has a Mandarin interface, and is specially designed to accommodate the elderly and children, who either have no phone or find it difficult to use one. The company says it has tailored Tingting\u2019s speech recognition algorithms to better recognize the particularities of speech in these age demographics.\n\nMany Chinese households have three generations living under the same roof. Tingting aims to create a family-friendly communication environment where grandparents won\u2019t have to put on their reading glasses if they get a text and toddlers can easily talk with their parents remotely. Tingting users need not have a WeChat account.\n\n\u201cWhen my grandpa was speaking to Tingting, I didn\u2019t even know what he said, but the speaker understood,\u201d says Tencent Tingting Chief Products Officer Chaoqin Wang. \u201cWhen the speaker replied to him, he said \u2018Oh, not bad. It\u2019s amazing\u2019. I could feel that the old man was very pleased that he was understood by a modern Internet product.\u201d\n\nThe sleek and cylindrical Tingting comes in black or lime. It has an amusing wake-up word, \u201c9420\u201d (Jiu Si Er Ling), which sounds like \u201cI just love you\u201d in Mandarin. A self-adaptive tuning function can adjust volume based on environment.\n\nTencent boasts that Tingting outperforms its competition in acoustic quality. Music Critic Lihua Su appreciated the full sound, which he said didn\u2019t come from a \u201cpoint,\u201d but rather an \u201carea.\u201d \u201cThe unique speaker design makes anywhere in a room a \u2018sweet spot\u2019,\u201d he told Synced.\n\nTingting features an unplugged design with lithium batteries that can power it for up to 16 hours, five hours in WIFI mode, or six hours in Bluetooth mode. This was a bold choice by Tencent, as integrating batteries with smart speakers raises many technical challenges, including how to compress the algorithms to lower power consumption.\n\nThe speaker\u2019s content library is stuffed with more than 20 million original songs, one million audio stories for children, and one hundred million hours of other audio content. \u201cFor a speaker, content is at the core of customer needs,\u201d says Wang.\n\nTencent\u2019s announcement signals the company\u2019s entry into China\u2019s heated smart speaker market, where Baidu\u2019s Raven H, Alibaba\u2019s Tmall Genie and hundreds of competitors are all vying for the vanguard.\n\nTingting is sold on JD.com at \u00a5699(US$100), which is slightly more expensive than competitors like Xiaomi\u2019s Mi AI speaker Mi at CN\u00a5299 (US$45), and Tmall Genie at CN\u00a5499 (US$80). Genie even went on sale at the rock-bottom price of CN\u00a599 (US$15) during China\u2019s November 11 \u201cSingles Day\u201d online shopping spree.\n\nTencent bills Tingting as a powerful smart assistant with compelling features and superior performance, and it looks and sounds the part. Although Tencent hasn\u2019t yet built much of a reputation in the hardware market, insiders suggest the company views smart speakers as a serious long-term business, and there will be more Tencent smart speakers coming up.\n\nSynced is covering the story and will update readers on Tingting\u2019s performance in the market."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-ups-its-chip-game-with-ali-npu-6dbc469deeb7?source=user_profile---------26----------------",
        "title": "Alibaba Ups Its Chip Game With Ali-NPU; Acquires Chipmaker C-Sky Microsystems (Updated)",
        "text": "Alibaba\u2019s R&D arm DAMO Academy announced yesterday that it is developing a new neural network chip called Ali-NPU for AI inferencing in the field of image processing, machine learning, etc.\n\nAli-NPU is expected to be some 40 times more cost-effective than conventional chips. Alibaba Researcher Yang Jiao says this chip\u2019s performance will be 10 times better than mainstream CPU and GPU architecture AI chips currently on the market, with only half the manufacturing cost and power consumption. By self-developing AI chips tailored to its own needs, Alibaba will reduce its dependence on other chip companies.\n\nIn 2015, Alibaba cooperated with Hangzhou C-SKY Microsystems to develop the Yun On Chip (YoC). In November 2016, Alibaba and Tencent invested US$23 million in California chip company Barefoot Networks.\n\nDAMO Academy is an R&D institute for fundamental and disruptive technology research established by Alibaba in November 2017. It has a dozen top-notch international scientists working in quantum computing, machine learning, network security, visual computing, natural language processing, chip technology, and embedded technology. Alibaba is planning to inject US$15 billion into this project over the next three years.\n\nAlibaba announced yesterday that it has acquired chipmaker C-Sky Microsystems in a bid to strengthen its chip-making R&D capability. Financial terms of the deal were not disclosed.\n\nFounded in 2011, Hangzhou-based C-Sky Microsystems is an integrated circuit design company dedicated to 32-bit, high-performance and low-power embedded CPU, with chip architecture licensing as its core business. The company\u2019s embedded CK-CPU chip software and hardware platform provides customers with CPU IP core, System-on-Chips (SoC) design and development platforms.\n\nC-Sky Microsystems bills itself as \u201cthe only embedded CPU volume provider in China with its own instruction set architecture.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/mits-josh-tenenbaum-on-intuitive-physics-psychology-in-ai-99690db3480?source=user_profile---------27----------------",
        "title": "MIT\u2019s Josh Tenenbaum on Intuitive Physics & Psychology in AI",
        "text": "When Bayesian Cognitive Scientist Josh Tenenbaum recently told a packed University of Toronto lecture hall that \u201cintelligence is not just about pattern recognition,\u201d the significance of the statement was not lost on the audience.\n\n \n\nThe university is the birthplace of the \u201cHinton School of Neural Networks\u201d, where Geoffrey Hinton redesigned the neural network approach to AI by coding a synthesized biological circuitry of neurons into artificial models that excelled in pattern recognition, trailblazing a new path for the development of artificial intelligence.\n\n \n\nFollowing an introduction by Hinton, Tenenbaum quipped about the old days, when Hinton \u201cwas willing to be called a cognitive scientist.\u201d This drew laughter from the audience, considering the latter\u2019s eminence in the field of computer science. As the principal AI investigator at MIT\u2019s Center for Brain, Minds, and Machines (CBMM), Center for Brain and Cognitive Sciences (BCS), and Computer Science and Artificial Intelligence Laboratory (CSAIL), Tenenbaum is widely respected for his interdisciplinary research in cognitive science and AI. \n\n \n\nBreakthroughs in AI and deep learning have prompted neuroscientists such as Dan Yamins from the Stanford NeuroAILab to rethink the structure of the ventral stream \u2014 the object and visual recognition part of the brain. This cross-pollination is hardly a surprise, considering that early publications on neural networks frequently appear in journals like Psychological Review, Cognitive Science, and Nature, as the field moved forward from the 50s\u2019 single-layer perceptron to Kunihiko Fukushima\u2019s neocognitron in the 80s, to Yan LeCun\u2019s widely-used deep convolutional neural networks of today.\n\nHowever, if humans hope to develop artificial general intelligence, data-munching, pattern-seeking deep neural networks may not be the best approach. Might Bayesian networks, causal models, and predictive coding work better? Or a symbol manipulation engine modeled after logic, lambda calculus, and programming languages be the route to pursue? Tenenbaum wants to steer the research wheel to cognitive science and look for the answer there.\n\nHuman common sense involves the understanding of physical objects, intentional agents and their interactions, which Tenenbaum believes can be explained through intuitive theories. This \u201cabstract system of knowledge\u201d is based on physics (eg. forces, masses) and psychology (eg. desires, beliefs, plans).\n\n \n\nSuch intuitions are present even in young infants, bridging perception, language, and action planning capabilities. A 2011 study by Erno Teglas models the physics reasoning capability of 12-month-olds, while Elizabeth Spelke\u2019s pursued a similar research course in her paper psychological inferencing capability in 10-months-old.\n\nHow do we use computation models to reverse-engineer intuitive theories and teach an AI to evolve based on these principals? Tenenbaum suggests tackling the problem by using a new class of programming language called Probabilistic Programs, which is a compound of symbolic language, probabilistic inference, hierarchical inference (learning to learn), and neural networks.\n\nIn collaboration with Tenenbaum\u2019s group at MIT, DeepMind researcher Peter Battaglia is working on \u201ca realistic model of physics that can estimate physical properties and predict probable futures,\u201d to quote from the paper he co-authored with Tenenbaum, Computational Models of Intuitive Physics. Based on Bayesian inferencing, this model makes predictions in simulated 3D scenarios based on real-life statics, dynamics, forces, collisions, and friction. \n\n \n\nFacebook AI proposed PhysNet in 2016, a neural network that predicted whether a stack of blocks would fall. The network excelled in predicting outcome and estimating block falling trajectories, discerning them based on color. Tenenbaum says that for a prediction involving 2\u20134 cubes, PhysNet required over 200k training scenarios.\n\nIn the joint research program Learning Physics from Dynamic Scenes, developed with Stanford\u2019s Noah Goodman, Tenenbaum proposes a hierarchical Bayesian framework, working with probabilistic programs to model intuitive physical theories. The project trains the model on inferring physics scenarios in varying time periods, with different physical laws and properties at play. Inferring properties include mass, charge, friction, elasticity, and resistance.\n\n \n\nIn order to model intuitive psychology, researchers use a model wherein the agent considers its desires and its beliefs about the environment, which enables planning and then actions. Jara-Ettinger and Julian Schulz at Yale call this the \u201cnaive utility calculus.\u201d Co-authored with Tenenbaum, Jara-Ettinger\u2019s 2017 paper in Nature proposes a Bayesian theory of mind (BToM) model that infers an actor\u2019s beliefs, desires, and percepts from how they move in the local environment.\n\nTenenbaum lectures at the nexus of AI, cognitive science, and neuroscience. As he notes in the 2017 paper Building Machines That Learn and Think Like People co-authored with Brenden Lake, Tomer Ullman and Sam Gershman, the most immediate task for AI is to \u201ca) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and \u00a9 harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations.\u201d \n\n \n\nResearch in intuitive physics and psychology are especially promising in the field of robotics. A robot that knows intuitive physics can navigate the environment and perform nuanced actions such as carrying a cup of coffee, grasping a party balloon, and so on. Meanwhile, a robot that knows intuitive psychology by heart could observe, for example, a child pointing at cotton candy while crying as its parent shakes head, \u201cno,\u201d and would be able to correctly infer both humans\u2019 intentions.\n\n \n\n Tennenbaum is aiming for an AI that more completely understands the physical and psychological landscapes it will exist in. Such machines may also allow us to deepen our own understanding of intelligence.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-gears-up-for-self-driving-road-tests-6d603b45682e?source=user_profile---------28----------------",
        "title": "Alibaba Gears Up for Self-Driving Road Tests \u2013 SyncedReview \u2013",
        "text": "China\u2019s self-driving industry has just welcomed another major player: Alibaba Group is developing Level 4 autonomous driving technology and is ready to test its self-driving cars on public roads, Chinese media reported today.\n\nThe e-commerce giant has assigned its Chief Scientist and former Nanyang Technological University in Singapore Associate Professor Gang Wang to lead the project. Alibaba is looking to add more than 50 self-driving experts and specialists to the project to maximize its R&D capability, according to an Alibaba insider.\n\nAlibaba is also ready to conduct its first road test with a converted Lincoln MKZ, a model widely used by self-driving companies such as Baidu, NVIDIA, and many startups. The vehicle will test L4 autonomous driving capabilities, a level defined as \u201cfully autonomous\u201d in that the car can be operated without a human driver in most circumstances.\n\nAlibaba\u2019s engagement in self-driving technology began last year when it formed a partnership with Chinese domestic automakers SAIC Motor and Dongfeng Peugeot-Citro\u00ebn, and equipped the cars with its upgraded operating system AliOS.\n\nAlibaba\u2019s ambitious move represents a direct challenge against China\u2019s other internet giants Baidu and Tencent, which have been making substantial progress in the field of self-driving cars for several years. Baidu leads the race with its autonomous driving platform Apollo, which the company bills as \u201cthe Android of the auto industry.\u201d Just three weeks ago, Baidu received approval from the Beijing Municipal Government for testing its self-driving cars on Beijing roads. Tencent, meanwhile, reportedly tested one of its autonomous vehicles on Beijing highways earlier this month.\n\nWhile Uber and Tesla accidents and scandals in the US may have weakened public confidence in self-driving vehicles, Alibaba\u2019s move into the field is another sign that China remains dedicated to encouraging development and implementation of the technology.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-apr-w-2-apr-w-3-a101f46dd5ef?source=user_profile---------29----------------",
        "title": "AI Biweekly: 10 Bits from Apr W 2 \u2014 Apr W 3 \u2013 SyncedReview \u2013",
        "text": "IBM announces a four-year program in collaboration with Calgary-based Natural Resources Solutions Center to help oil and gas companies with sustainability and efficiency. IBM\u2019s AI platform Watson will digest raw data, structure it and come up with insights for oil and gas companies.\n\nAmazon adds a musical upgrade to Alexa\u2019s \u201croutine\u201d function, which enables users to combine multiple actions such into one command \u2014 for example to turn on both the coffee maker and kitchen lights in the morning. Alexa can now play music, podcast or radio shows as part of a routine, and users can also control audio output on the device. The move brings Alexa up to par with Google Assistant, which already integrates radio, podcasts and music.\n\nMontreal-based incubator Element AI will open a new office in Toronto where it will do R&D and work with local businesses that want to integrate AI into their operations and the Ontario Provincial Government. Element AI\u2019s July 2017 fundraising round raised a record US$137.5 million. This will be Element AI\u2019s fifth office, and the company is considering opening more in Asia in the near future.\n\nCalifornia\u2019s Public Utilities Commission has introduced a proposal to allow autonomous vehicles without a backup driver to transport passengers on state roads. The California Department of Motor Vehicles had already approved autonomous driving tests without a backup driver starting this month. The new regulations would allow members of the public to ride in such vehicles, and will be voted on next month.\n\nApril 10th \u2014 Microsoft Collaborates with C3 IoT to Accelerate AI in Enterprise\n\nMicrosoft and software company C3 IoT have announced a partnership to accelerate cutting-edge business level AI and IoT application development in areas such as AI predictive maintenance, dynamic inventory optimization, precision healthcare, and CRM. The co-marketing and co-selling deal also includes a co-development strategy on Microsoft\u2019s Azure cloud platform.\n\nQualcomm announces its own IoT and AI-optimized system on a chip (SoC) platform. The new platform is aimed at computer vision IoT applications such as security cameras, wearable cameras, and smart displays.\n\nAirFusion Wind is a cloud-based workflow and AI-based analysis platform that can identify and classify wind turbine asset damage. AirFusion Wind transforms inspection images from drones, ground-based sensors, and other image capture tools into data that can be used to evaluate conditions and reduce costs.\n\nApril 11th \u2014 NVIDIA Collaborates with Canon Medical Systems to Accelerate Deep Learning in Healthcare\n\nNVIDIA announces a collaboration with Canon Medical Systems to develop research infrastructure to support deep learning technology in the healthcare industry. The partnership will focus on deploying deep learning and big data analytics to support early detection and assisted diagnosis. Canon Medical Systems is the largest medical systems supplier in Japan, and will use the NVIDIA DGX system to process the massive data generated by its platform.\n\nGoogle\u2019s natural language processing and synthesis research just got fun. The company has uploaded two playful and creative interactive web experiments based on its word-association systems. These are offshoots of a new search option that allows users to directly ask the system questions instead of searching for specific words, titles, or authors, etc. The results are not perfect, but the system provides a useful and flexible new query function.\n\nApril 13th \u2014 Facebook Uses AI to Predict and Sell User Preferences\n\nThe Intercept creates a stir when it publishes apparently confidential internal Facebook documents that describe how AI can build predictive models of user behaviour that Facebook can sell to brands, a process some argue is unethical. The Intercept article argues that although Facebook says these algorithms are used to improve the user experience, in fact they are largely used to make money from advertisers.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/iclr-2018s-best-papers-variant-adam-spherical-cnns-and-meta-learning-6b48dca83e8b?source=user_profile---------30----------------",
        "title": "ICLR 2018\u2019s Best Papers: Variant Adam, Spherical CNNs, and Meta-Learning",
        "text": "Leading machine learning conference International Conference on Learning Representations (ICLR) has named its best research papers of the last year: On the convergence of Adam and Beyond, Spherical CNNs, and Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments.\n\nLaunched in 2013, the ICLR has grown to a world-class conference for machine learning researchers and engineers. ICRL 2018 received 935 papers \u2014 double last year\u2019s total \u2014 and 337 papers were accepted.\n\nIn On the Convergence of Adam and Beyond, Google New York proposes the new variant Adam, a gradient descent optimization algorithm introduced in ICLR 2015.\n\nGradient Descent is one of the most popular algorithm types for optimizing neural networks, but struggles with a convergence issue in non-convex settings that makes optimizations ineffectual. The Google paper suggests a new Adam algorithm which it says fixes the problem and improves the empirical performance.\n\nResearchers at the University of Amsterdam proposed Spherical Convolutional Neural Networks (CNNs) which can analyze spherical images, a technique in wide demand for drones, robots, autonomous cars, molecular regression problems, and global weather and climate modelling. The paper demonstrates that spherical CNNs can be efficiently applied to 3D model recognition and atomization energy regression.\n\nIn Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments, top academic institutes CMU, UMass Amherst, OpenAI, and UC Berkeley jointly developed a simple gradient-based meta-learning algorithm that can adapt to dynamically changing and adversarial environments.\n\nOver the past few years, reinforcement learning (RL) has successfully enabled machines to outperform humans in tasks ranging from Atari video games to the ancient and complex Chinese board game Go. However, the AI technique is not adaptable to non-stationary environments, for example a multiplayer game with high randomness. Meta-learning, the so-called learning-to-learn method, can compensate for RL\u2019s weakness.\n\nThis paper also introduced a new multi-agent competitive environment, RoboSumo, for more effective training of meta-learning algorithms.\n\nICLR 2018 runs April 30 to May 3 at the Vancouver Convention Centre in Vancouver, Canada.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/how-ai-can-speed-up-drug-discovery-3c7f01654625?source=user_profile---------31----------------",
        "title": "How AI Can Speed Up Drug Discovery \u2013 SyncedReview \u2013",
        "text": "The US FDA defines five steps for the development of a new drug: discovery and development, preclinical research, clinical research, FDA review, and FDA post-market safety monitoring.[1] The Tufts Center for the Study of Drug Development estimates the average cost of developing a new drug at US$2.55 billion, with the process potentially taking more than 10 years.[2]\n\nThe first step, drug discovery, typically involves one of four scenarios: finding new insights into a disease, finding possible effects of a drug by testing molecular compounds, repurposing existing drugs, or manipulating genetic materials. At the drug discovery stage, thousands of compounds may be potential candidates for development as a medication.[3] They need to go through a series of tests and only a small number will advance to further research.\n\nTo accelerate drug discovery and reduce the costs of drug development, pharmaceutical companies are introducing AI technologies such as machine learning and deep learning into the processes.\n\nDrug discovery is a data-driven environment with a massive of data such as high-resolution medical images, genomic profiles, metabolites, molecular structures, and biological information.[4] This information is published in papers and journals, however it can be a challenge for researchers to keep up with it. AI can use machine learning and deep learning to correlate, assimilate, and connect existing data more rapidly in order to help discover patterns in the data pools. By reviewing scientific research papers, AI can make connections that provide possible hypotheses for drug discovery.\n\nFinding new compounds for a medicine is difficult because the possible combinations are countless. Such research requires medical data on genes, proteins, metabolites, molecular structures, and biological information.[7] Processing this huge amount of information can be a very time-consuming task. Pharmaceutical companies are discovering that AI techniques such as deep learning algorithms can process the same information much faster.\n\nDrug repurposing, also known as drug repositioning or therapeutic switching, is the application of known drugs and compounds to treat new indications.[11] One advantage of drug repositioning is that most of the repositioned drugs have already passed a series of tests, and so have less risk of unexpected toxicity or side effects. With the help of machine learning algorithms, pharmaceutical companies can repurpose drugs faster and at lower costs than developing new drugs.\n\nManipulating genetic materials for drug discovery is also known as personalized medicine or precision medicine. This kind of drug discovery can be more effective in treatment because it is based on individual health data paired with predictive analytics.[14] In order to efficiently gather, analyze, store, and trace a person\u2019s detailed information, especially when the data is huge and unstructured, pharmaceutical companies use deep learning, machine learning, or computer vision.\n\nWe know that artificial intelligence can be applied in drug discovery to make the process faster. There are also areas where artificial intelligence can help in drug development. Clinical trials, for example, are currently classified into five phases and usually require more than three thousand test subjects to proceed from phase one to phase three.[17] Most pharmaceutical companies use recruitment firms to find clinical trial subjects by examining individual medical records.[18] This task takes time and the efficacy is low. Companies can use machine learning to train a model that includes age, sex, treatment history, and current health status to build an inclusion/exclusion criteria that will speed up this aspect of clinical trials research.\n\nMoreover, AI can help test the side effects or toxicities of candidate drugs. Cyclica, a Canadian startup, uses a suite of computational algorithms to evaluate and predict how drugs might interact with the human body.[19] This kind of testing helps pharmaceutical companies identify a drug candidate\u2019s side effects before clinical trials, so companies can make corrective adjustments in advance.\n\nDrug discovery can benefit from machine learning, deep learning, and computer modeling. However, there is also the potential for introduction of biases from unbalanced data, which might cause errors or discrimination while AI is training the neural networks.[8] A research team from Insilico Medicine discovered that accuracy could become unstable unless the neural network had been trained using diverse datasets. The range, quantity and quality of input data is therefore a key factor for further implementation of AI in drug discovery.\n\n[4] AI Provides New Insights for Accelerated Drug Development: https://blogs.sap.com/2017/11/02/ai-provides-new-insights-for-accelerated-drug-development/\n\n[5] AI in Pharma and Biomedicine \u2014 Analysis of the Top 5 Global Drug Companies: https://www.techemergence.com/ai-in-pharma-and-biomedicine/\n\n[6] What if AI could take your research to the next level?: http://benevolent.ai/blog/benevolentai/what-if-ai-could-take-your-research-to-the-next-level/\n\n[8] Artificial Intelligence: will it change the way drugs are discovered?: https://www.pharmaceutical-journal.com/news-and-analysis/features/artificial-intelligence-will-it-change-the-way-drugs-are-discovered/20204085.article\n\n[12] IBM, Teva to Use A.I. for Drug Repurposing Program: https://www.rdmag.com/article/2016/10/ibm-teva-use-ai-drug-repurposing-program\n\n[13] How Pharmaceutical And Biotech Companies Go About Applying Artificial Intelligence in R&D: https://www.biopharmatrend.com/post/34-biopharmas-hunt-for-artificial-intelligence-who-does-what/?lipi=urn:li:page:d_flagship3_feed%3BhAZ67GlARQyGqnLUgoHuzA\n\n[18] MEET THE COMPANY TRYING TO DEMOCRATIZE CLINICAL TRIALS WITH AI: https://www.wired.com/story/meet-the-company-trying-to-democratize-clinical-trials-with-ai/\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-yitu-battles-for-position-in-huge-facial-recognition-market-8887bcc87234?source=user_profile---------32----------------",
        "title": "China\u2019s Yitu Battles for Position in Huge Facial Recognition Market",
        "text": "If you deposit a cheque or withdraw cash from an ATM in China you\u2019ll no longer need a debit card to do so. New facial-recognition ATMs now enable almost-instant access your bank accounts.\n\nThe tech has been deployed on over 12000 ATMs nationwide by Yitu Technology, a Chinese AI startup specialized in computer vision and machine learning. ATMs are just one facet of Yitu, whose big plan is to put smart eyes on all human-machine interfaces.\n\nFounded by two Chinese AI experts in 2012, the company is particularly competent in recognizing human faces and vehicles. Such capability appeals to China\u2019s public security departments. In 2015, Suzhou local police captured a burglar in just ten minutes, using Yitu\u2019s technique to pinpoint the targeted car among hundreds in a surveillance video.\n\nThe case quickly sent a shockwave across the country. Now, over 30 provincial and 150 municipal public security departments have adopted Yitu\u2019s technique for identifying thefts and hunting fugitives.\n\nThe China Security Industry Network (21csp.com) projects China\u2019s security surveillance market value at CNY\u00a5752 billion (US$120 billion) in 2018. That\u2019s a big pie, and everyone wants a piece.\n\nIn an interview with Synced, Yitu Senior Researcher Shuang Wu said the company can accurately enable facial recognition in just one second, with a large database of over one billion facial photographs.\n\n\u201cYitu\u2019s technology has been well-accepted by many China\u2019s crucial ports and organizations as Yitu shows a superior performance even when it comes to racial spectrum, which is usually a difficulty in facial recognition,\u201d says Wu.\n\nYitu\u2019s advance in facial recognition technique quickly won global accolades as the company scored highest under the \u201cidentification accuracy\u201d category in a competition run by the US National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (IARPA) in November 2017.\n\nIn recent years Yitu has been expanding the use of its vision technologies to applications in smart cities for example. Partnering with Alibaba last year, Yitu energized Hangzhou\u2019s urban intersections by modelling vehicle behaviour and predicting traffic flow, speeding up traffic flow by 11 percent.\n\nWhile Yitu has consolidated its business in the security surveillance over the years, the company has also been plotting a bold course in healthcare, a traditionally conservative field that is now increasingly driven by technology. \u201cWe believe AI\u2019s potential in healthcare is unquestionably promising,\u201d says Wu.\n\nStruggling with air pollution, China has seen a sharp rise in lung cancer cases over the past 15 years. The country recorded nearly 4.3 million new cancer patients in 2015, 730,000 of whom had lung cancer, accounting for nearly 36 percent of the global total.\n\nYitu launched a smart medical imaging platform in early 2017 for lung cancer early detection \u2014 applying deep learning techniques to locate lung nodules, a type of small tissue mass in the lung that appear as round and white shadows on a CT scan. By analyzing size, shape and location, the system can help doctors compare a current imaging scan with historical scans to track changes in condition. It then creates a structured diagnosis report according to the imaging results, which is delivered to the doctor.\n\nZhejiang Provincial People\u2019s Hospital first deployed Yitu\u2019s system, and hospital clients have now grown to over 20. \u201cOur stats tell us the adoption rate of our diagnosis report is about 92 percent,\u201d says Wu.\n\nWu stressed that AI+healthcare is not to be treated as a short-term venture. \u201cPublic security for example has a clear threshold, and what you need to do is to reach a certain percentage of accuracy. Healthcare, however, is a bit more complicated. You have to keep talking with doctors and ask what and how to improve your product. While it is easy to make a demo, building trust in healthcare is a long-term challenge,\u201d says Wu.\n\nYitu\u2019s fast tech development and rapid rise attracted interest from venture capital. In 2017 the company raised CNY\u00a5380 million (US$60 million) in a Series C funding round led by Hillhouse Capital Group. Last month, Chinese media reported that Yitu had raised a new round funding, bringing the company\u2019s value to an estimated CNY\u00a515 billion (US$2.4 billion). The company has not officially confirmed the news yet.\n\nYitu now has a crew of over 500 in China, Singapore and Silicon Valley.\n\nThis yearlong series of huge funding rounds is providing Yitu with the resources it needs to develop new AI-powered products beyond machine vision, such as those based on natural language processing (NLP). Yitu recently created an NLP-based medical reference database that can efficiently convert patients\u2019 medical records into organized and usable data.\n\n\u201cFrom a technical point of view, deep learning reinforces the inner relationship between different academic areas, or you could say the threshold for developing NLP by a computer vision company is lower. On the other hand, NLP is something that any AI company should invest in,\u201d says Wu.\n\nYitu is one of four billion-dollar computer vision unicorns in China, along with CloudWalk, Face++, and SenseTime \u2014 the three-year-old startup that just became the world\u2019s highest valued AI company. They are all vying for the vanguard in marketplaces such as robotics, finance, security surveillance, transportations, mobile device, and AR/VR.\n\nThe fierce competition between the four companies does not allow Yitu time to relax \u2014 the company must continue growing. Wu says this is an exciting time for Yitu: \u201cThere are surprises every day. A lot of things happen beyond the plan, and somehow they achieve good success, bringing great challenges as well as opportunities for everyone.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-prepares-for-ai-talent-shortage-e66d0a3a0de2?source=user_profile---------33----------------",
        "title": "China Prepares for AI Talent Shortage \u2013 SyncedReview \u2013",
        "text": "Chinese State media People\u2019s Daily recently reported the country is facing a shortage of five million AI talents.\n\nA separate LinkedIn survey revealed that while there are some 1.9 million AI engineers worldwide, about one million reside in the US while China is home to just 50,000. And as the talent level increases, the gap only widens \u2014 of the 208 AAAI fellowships granted over the past 27 years, only 4 went to Chinese nationals.\n\nAlthough China is an AI business deployment leader with well-financed startups such as the US$4.5 billion AI unicorn SenseTime, the country\u2019s AI educational infrastructure is lagging. The US has six times more AI education institutions.\n\nMeanwhile, high demand has sent AI engineer salaries skyrocketing in China. IDG Capital\u2019s 2017 Internet Unicorn Salary Report shows compensation for top AI talent is 55 percent higher than average ICT industry employee salaries, 90 percent higher at intermediate positions, and 110 percent higher at junior positions.\n\nAI laboratories established through joint ventures between corporations and universities, such as the iFlytek and Aispeech labs at Shanghai Jiaotong University, currently provide top-level AI education opportunities. Such partnerships grant well-rounded tutelage to students and facilitate talent transfer. Dr. Cheng-Lin Liu from Chinese Academy of Sciences says \u201cPhD students can convert their researchers into direct products or services upon graduation.\u201d\n\nThe AI talent shortage also provides an opportunity for online education providers. Andrew Ng\u2019s deeplearing.ai and Udacity are penetrating the China market, providing coaching in fields of machine learning, deep learning, NLP, computer vision, Python and so on. Chinese competitors 51CTO, CSDN, and Netease also provide AI video tutorials. Last year, iFlytek launched its online \u201cAI University,\u201d offering speech recognition and synthesis expertise and entrepreneurship coaching.\n\nSome employers however are wary of the online training trend, believing AI-related research and engineering skills must be built on years of formal learning and research, and cannot be instilled through relatively short online courses.\n\nTo tackle the problem, the Ministry of Education this week announced ambitious goalsfor the coming decade: establish a set of 100 \u201cAI+X\u201d specialization categories by 2020 in the disciplinary fields of math, physics, biology, psychology, sociology, law, and other related professional fields. The action plan will also compile 50 seminal teaching materials, 50 state-level online open courses, and open 50 additional AI teaching and R&D centres over the coming decade.\n\nThe Affiliated Elementary School of Peking University has begun introducing primary students to genetic algorithms and neural networks using easily explainable graphics and games. This is part of a larger initiative to promote STEM and AI courses in elementary and secondary schools. The Tongzhou District Experimental Primary School has added 75 AI-related courses, including a winter bootcamp to teach students patent filing procedures for robotics, to \u201chelp them build up awareness for intellectual property protection,\u201d explains class teacher Zhang Li.\n\nThis month the Ministry of Education, Sinovation AI Lab, and Peking University jointly announced the Global AI Talent Training Program for Chinese Universities, pledging to educate 500 teachers and 5,000 students over the next five years. Participating staff undergo strict screening and must presently be teaching CS, preferably in an affiliated AI institute.\n\nIn the eastern city of Nanjing, computer vision firm Seetatech is giving middle school students weekly 70-minute demo lessons. \u201cFirstly we help students get a theoretical glimpse of AI, machine learning and their practical applications, then we teach them about object and facial detection, instructing them to build their own AI detection algorithms,\u201d says an onsite Seetech employee.\n\nNanjing University opened one of the first AI institutes in China this March, and is currently seeking AI researchers, offering a base annual salary of US$60k, a housing subsidy of close to US$200k, and over US$300k in research funding as the starter package.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-oks-self-driving-tests-on-public-roads-d73216d0c243?source=user_profile---------34----------------",
        "title": "China OKs Self-driving Tests on Public Roads \u2013 SyncedReview \u2013",
        "text": "New Chinese regulations will permit self-driving vehicle testing on public roads across the country.\n\nThe Chinese Ministry of Industry and Information Technology, Ministry of Public Security, and Ministry of Transport yesterday jointly issued the Intelligent Connected Vehicle Road Test Management Standards (Trial) (\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u9053\u8def\u6d4b\u8bd5\u7ba1\u7406\u89c4\u8303\u8bd5\u884c) notice, which introduces regulations for testing self-driving cars on public roads nationwide. The policies exclude \u201clow speed\u201d vehicles and motorcycles, and will take effect on May 1.\n\nThe new regulations aim to facilitate the development of self-driving technology through the wide deployment of public road tests. Chinese media is reporting that the country also plans to make its transportation infrastructure more self-driving-friendly, according to an Officer from the Ministry of Transportation.\n\nThis year the Shanghai, Beijing, and Chongqing municipal governments introduced their own self-driving regulations; and China\u2019s first special autonomous driving licence plates were issued to tech giant Baidu and electric vehicle startup NIO.\n\nThe new regulations require the presence in the test vehicle of a safety driver with at least three years driving experience and a clean record over the past 12 months. Testing companies are responsible for training their safety drivers, providing support in emergency situations on the road, and assume full liability for the vehicle\u2019s actions during testing.\n\nTest vehicles must be new and able to switch from self-driving mode to manual operation efficiently, safely and instantly. They require the capability for real-time monitoring, recording, and storing of vehicle status, control mode information (self-driving or manual), and location, direction and speed. Vehicles must also record environment perception and response data, headlight and turn signal status, and provide 360-degree video monitoring.\n\nTesting companies must meet all technical requirements and provide a detailed test plan and insurance of CNY\u00a55 million for each test vehicle. Once an application is approved, the company will receive an Intelligent Connected Vehicle Road Test Notice (\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u8def\u6d4b\u8bd5\u901a\u77e5\u4e66), and can then apply for a temporary test license plate from the local Traffic Management Department of Public Security (\u516c\u5b89\u673a\u5173\u4ea4\u901a\u7ba1\u7406\u90e8\u95e8).\n\nTest vehicles must adhere to their submitted test plans. If a test vehicle experiences a severe accident or serious violation, the relevant supervision department (\u4e3b\u7ba1\u90e8\u95e8) can revoke the temporary test license plate. The testing company must submit a summary report one month after each test ends and full test reports every six months.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/berkeley-researchers-create-virtual-acrobat-81427228fb50?source=user_profile---------35----------------",
        "title": "Berkeley Researchers Create Virtual Acrobat \u2013 SyncedReview \u2013",
        "text": "Simulated robots can now spin-kick like a karate expert or backflip like an acrobat. The Berkeley Artificial Intelligence Research (BAIR) Lab yesterday proposed DeepMimic, a Reinforcement Learning (RL) technique that enables simulated characters to regenerate highly dynamic physical movements learned from data collected from human subjects. BAIR is a top-tier research lab focused on computer vision, machine learning, natural language processing, and robotics.\n\n \n\nRL methods have been shown to be applicable to a diverse suite of robotic tasks, particularly motion control problems. A typical RL includes a policy function that consists of all action selections that machines can do, and a value function that returns a low or high reward each time a machine takes an action. Machines can self-learn skills by leveraging the reward. The epoch-making Go computer AlphaGo produced by DeepMind is grounded on the same technique.\n\n \n\nHowever, virtual characters trained with deep RL can exhibit abnormal behaviours such as jittering, asymmetric gaits, or excessive movement of limbs.\n\n \n\nBAIR\u2019s new paper DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skill introduces a policy function that collects challenging skills such as locomotion, acrobatics, martial arts, and dancing.\n\n \n\nBAIR next initialises a character to a state sampled randomly, a method known as Reference State Initialization (RSI). The character can learn skills from any state of moves, such as the inflection point of a flip, and RSI can allow the character to know which states will result in high rewards even before it has acquired the proficiency to reach those states.\n\n \n\nBy connecting RSI with Early Termination (ET), a standard practice for RL researchers to stop simulations that lead to failure, BAIR researchers ensured that a substantial proportion of the dataset consists of samples close to the reference trajectory. Without ET, the character may flail or fall, but will not learn to flip.\n\n \n\nThe research shows that the character can learn over 24 skills, with movements nearly indistinguishable from the human reference subjects. BAIR also says its technique is simpler and produces better results than the current leading motion imitation method, Generative Adversarial Imitation Learning (GAIL).\n\n \n\nBAIR hopes the new research will facilitate the development of more dynamic motor skills for both simulated characters and robots in the real world.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-puts-education-focus-on-ai-plans-50-ai-research-centres-by-2020-5589c35ba701?source=user_profile---------36----------------",
        "title": "China Puts Education Focus on AI; Plans 50 AI Research Centres By 2020",
        "text": "China\u2019s Ministry of Education last week issued its AI Innovation Action Plan for Colleges and Universities to the education departments at all levels and institutions of higher education. This action plan aims to advance China\u2019s universities to world frontiers in science and technology; energize their capabilities in AI technological innovation, talent cultivation, and global cooperation; and provide strategic support for the development of next-generation AI in China.\n\nThe plan sets three goals for the next 12 years:\n\nThe plan further proposes establishing a set of one hundred \u201cAI+X\u201d specialization categories by 2020, with specific goals to:\n\nChina sees AI as the state\u2019s innovation focus, which will spark technological breakthroughs and stroke national pride. In July 2017, the State Council issued the New Generation Artificial Intelligence Development Plan and listed \u201caccelerating the education of top-notch AI talents\u201d as a primary task.\n\nLast week, China\u2019s Ministry of Education, Sinovation Ventures, and Peking University jointly launched the country\u2019s first university program \u2014 Global AI Talent Training Program for Chinese Universities \u2014 to educate over 500 teachers and 5,000 students with AI expertise within the next five years.\n\nThe latest action plan can be viewed as a supplement to this program, with the same ultimate goal of putting the State Council\u2019s Development Plan into practice and making China a world leader in the field of AI by 2030.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/onsite-from-re-work-finance-summit-in-london-deep-learning-trading-6521912aa599?source=user_profile---------37----------------",
        "title": "Onsite From RE\u2022WORK Finance Summit in London \u2014 Deep Learning & Trading",
        "text": "Financial markets are becoming a new proving ground for deep learning. The AI technique has already achieved remarkable success in image recognition, speech detection and sentiment analysis, and is believed to be well-suited for dealing with financial data.\n\nAt last month\u2019s RE\u2022WORK Deep Learning in Finance Summit in London, leading AI industry practitioners and academics from prestigious universities discussed their research, provided insights on business trends and real-life AI applications, and addressed current challenges facing the AI industry as a whole.\n\nSynced visited the summit to explore how deep learning techniques, such as neural networks and LSTM, can be applied to proprietary trading. This article focuses on Filippo Scopel\u2019s presentation Learning to Trade and Dr. Luigi Troiano\u2019s Supporting Trading in Financial Markets using Deep Learning Tools.\n\nScopel is a machine intelligence engineer at Merantix, a Berlin research lab building AI ventures. Scopel\u2019s research speciality is mathematical modelling and algorithm development with applications in finance and other fields. For the past year, he has been training deep learning models for time series prediction on financial microstructure data.\n\nScopel discussed Merantix\u2019s use of deep neural networks to predict price movements with short-term samples of under five minutes, and how these predictions can be transferred into trading strategies and then trading algorithms.\n\nDeep learning models are trained to make predictions by absorbing raw data such as past prices, conducting data computation, and generating results. Due to the complexity of the hidden layers within such neural networks, it is impossible to make an exact interpretation of how those predictions were generated.\n\nPredictions will be turned into specific trading strategies regarding bid-asks and particular time horizons. A decision to buy, hold, or sell is then generated and executed by the algorithm.\n\nAlthough the Merantix predictions can be more accurate than traditional methodologies, Scopel pointed out the system is far from infallible. Even with the right predictions, Merantix can still record losses due to the characteristics of high-trading frequency. What he means is transaction costs, and full bid-ask spreads can erode the potential profitability, sometimes resulting in losses.\n\nDr. Troiano is an Assistant Professor of AI, Machine Learning and Data Science at the University of Sannio. He began his presentation by identifying areas within finance where AI and particularly deep learning could be applied, including reshaping the analysis space, searching complex patterns in data, and trading robotisation. Data filtering removes noise to enable better quantitative analyses, and complex correlations and co-occurrences in large datasets can be identified to generate improved trading strategies. Automation reduces costs and improves the long-term effects of hedge funds and proprietary trading firms.\n\nIn forecasting the variability of prices, also referred to as volatility, Dr. Troiano said long short-term memory (LSTM) networks \u2014 a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies \u2014 work best when volatility is extremely high, which is a beneficial finding since such periods also provide high-profit opportunities.\n\nDr. Troiano then discussed his research into algorithmic trading using technical indicators such as the moving average convergence divergence (MACD) with deep learning techniques. A deep learning system can construct trading strategies after observing historical prices and indicator values instead of being explicitly programmed to execute those strategies. What this means is that AI has the potential to learn, or more appropriately, invent trading effective strategies.\n\nHowever, Dr. Troiano stressed that his research is still at its preliminary stage, and only certain deep learning techniques have been tried. More research is needed to determine how neural networks can improve on old-fashioned methods.\n\nOverall, it is clear that deep learning can accelerate the performance of tasks such as short-term future price predictions and proprietary trading strategies, but the AI technique has not yet reached maturity for full application in the trading market. The industry needs to be patient before widely employing neural networks to make market predictions.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-photographs-chinese-jaywalkers-shames-them-on-public-screens-ad0a301a46a6?source=user_profile---------38----------------",
        "title": "AI Photographs Chinese Jaywalkers; Shames Them on Public Screens",
        "text": "China\u2019s traffic police are using AI to tackle \u201cChinese-style jaywalking\u201d at major urban intersections. Facial recognition cameras take a 15-second video and four snapshots of pedestrians crossing on a red light. Pictures are matched with photo IDs in the police database, and violators can have their headshots along with family name and partially obscured citizen ID and registration address displayed on large roadside screens.\n\nSince deploying the system in April 2017, Shenzhen traffic police have caught over 13,930 jaywalking pedestrians and non-motorized vehicles. Jaywalkers are fined up to CNY\u00a520 (US$3) and are subject to traffic rule refresher courses or community service at road intersections.\n\n\u201cSince the new technology has been adopted, jaywalking cases have been reduced from 200 to 20 each day at the major intersection of Jing\u2019Shi and Shun\u2019Geng roads. Fewer people are crossing roads during red lights,\u201d said Li Yong, a Traffic Police Officer in the eastern city of Ji\u2019Nan.\n\nChinese-style jaywalking is a social nuisance. According to Ji\u2019Nan city statistics, barging pedestrians and non-motorized vehicles account for 16 and 33 percent of traffic accidents per year respectively. As ever-broadening Chinese roads can now have 10 lanes at urban intersections, ignoring traffic signals is extremely dangerous.\n\nTraffic authorities also plan to build a social credit system wherein jaywalkers will start receiving text messages or Weibo notifications. Traffic police will record the number of violations, and a certain threshold will affect the offender\u2019s social scores, which may limit their ability to borrow from banks.\n\nSome may ask whether this AI is saving lives or infringing on personal privacy? Li Yi, a research fellow at the Shanghai Academy of Social Sciences, says the public display of offenders\u2019 photos and partial personal information may prove to be effective in reducing pedestrian accidents and injuries, \u201chowever, we always need to find a balance between law enforcement and privacy protection.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-sensetime-scores-us-600-million-in-funding-to-become-the-world-s-most-valued-ai-startup-a505d2fa9c01?source=user_profile---------39----------------",
        "title": "China\u2019s SenseTime Scores US$600 Million in Funding to Become the World\u2019s Most Valued AI Startup",
        "text": "China\u2019s computer vision company SenseTime today announced it had raised a staggering US$600 million in Series C funding, setting a world record for an AI company and bringing its value to an estimated US$4.5 billion to make it the world\u2019s most valued AI startup.\n\nChina\u2019s e-commerce giant Alibaba Group led the funding, joined by Temasek Holdings and Suning Corporation. Says Alibaba Group Vice Chairman Cai Chongxin, \u201cSenseTime\u2018s research capabilities in deep learning and computer vision are impressive. Alibaba looks forward to partnering with SenseTime to inspire more innovations and create value for society.\u201d\n\nFounded in 2014 by Dr. Xiao\u2019ou Tang, a Professor of Information Engineering at the Chinese University of Hong Kong (CUHK), SenseTime uses deep learning in the development of computer vision to replicate tasks performed by the human visual system.\n\nSenseTime Co-Founder and CEO Xu Li told Synced the company set up an R&D team of 200 scientists during its first two years. The investment quickly paid off, as the team came up with an advanced deep learning framework and a cutting-edge 1207-layer neural network in 2016.\n\nSenseTime\u2019s superior AI technologies were soon transformed into a number of marketable software solutions: SensePose to synchronize users\u2019 movements with virtual figures in real-time videos; SenseVideo to recognize the positions and attributes of humans, vehicles and other entities from video input; and SenseFace to detect humans faces in a millisecond.\n\nThese technologies can be applied to multiple industries, including automotive, finance, mobile internet, robotics, security, and smartphones, and helped SenseTime score mega-clients such as China Mobile, China UnionPay, Huawei Technologies Co, Xiaomi, JD.com, and an important strategic partnership with chip giant NVIDIA.\n\nIn late 2016 SenseTime raised US$120 million in funding led by Beijing-based CDH Investments, Dalian Wanda Group, IDG Capital Partners and Star VC. Some six months later the company closed its Series B funding with US$410 million from CDH and Sailing Capital, which propelled it onto CB Insights\u2019 2017 technology unicorn list.\n\nSenseTime has about 700 staff, with 120 researchers who hold doctoral degrees. According to a November Reuters report the company is preparing for an IPO.\n\nXu says the latest funding round will not only strengthen the company\u2019s advantages, but bring more business opportunities: \u201cThe Series C financing will help SenseTime apply its core technology to more industries, expand the business landscape by cooperating with partners globally, and connect the upstream and downstream industry.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/simulations-pave-the-road-for-self-driving-technologies-78b696227383?source=user_profile---------40----------------",
        "title": "Simulations Pave the Road for Self-Driving Technologies",
        "text": "The Encyclopaedia Britannica defines a computer simulation as \u201cthe use of a computer to represent the dynamic responses of one system by the behaviour of another system modeled after it.\u201d Airline pilots train on simulators, while in the automotive industry driving simulations are used to optimize ride experience and improve engine performance. With the emergence of autonomous vehicle technologies, the development and deployment of effective self-driving simulations has become an industry priority.\n\nSelf-driving simulations collect data to improve autonomous vehicles\u2019 algorithm training capability, sensor accuracy and road data quality. It\u2019s been proposed that autonomous vehicles should log 18 billion kilometers (11 billion miles) of road test data to reach an acceptable safety threshold. This would however require a fleet of 100 vehicles running on roadways 24/7 at a constant speed of 40 kph (25 mph) for 5 years. Self-driving simulations are an ideal solution, enabling continuous and unlimited data collection in the digital environment with relatively low operational costs.\n\nSelf-driving simulations have advantages in milage data collection efficiency, road condition dataset diversity, and sensor corresponding data accuracy.\n\nSelf-driving simulations can boost the speed of data collection to reach mileage accumulation targets while reducing fleet operation costs. Waymo and Baidu have made use of self-driving simulation to speed up self-driving development.\n\nSelf-driving simulations can also add more uncertainty to a dataset to increase the responsiveness of the system. They can produce a variety of scenarios to test and improve vehicle performance under different conditions, for instance in severe weather, heavy traffic environments, and various distinct scenarios. Truevison.ai and AirSim are two leading solutions in this field.\n\nSensors are the eyes of the self-driving car. Multiple sensors can increase the accuracy of input data and protect the vehicle from misidentification or malfunction. Self-driving simulations provide multiple signals simultaneously, such as camera, LiDAR, radar and more. A given object in the virtual environment will thus be detected by different sensors, and these signals will validate each other to increase accuracy. RightHook and Cognata are two simulators providing multiple self-driving sensors.\n\nThe limitations of self-driving simulations should not be overlooked. Currently, there are two main weaknesses: lack of emergency situation scenarios, and potential consequences of differences between real and simulated data.\n\nEmergency situations are still hard to simulate as each real world accident is unique. Although vehicles can learn general driving operations through simulation, it is impossible to predict every single emergency situation. For example the May 7th, 2016 fatal accident involving a Tesla on autopilot occured because the system failed to distinguish a white truck against a bright sky. Although this is not a rare scenario in the real world, the simulator had not covered it.\n\nThis differences between real and simulated data is another issue that could negatively affect system performance, as the full consequences of these differences remain unclear. Engineers are hard pressed to determine what type of data leads to an accident due to unexplainable features of the algorithms. For example, real world pedestrians with disparate clothing and posture profiles cannot all be reflected in the simulator, and this might reduce a self-driving vehicle\u2019s ability to identify pedestrians.\n\nIn order to overcome these drawbacks, it is important to simulate more scenarios to make abnormal data traceable. SynCity is a Dutch company developing an advanced simulator that aims to present a wider range of scenarios, including other vehicle misbehaviour and various emergency situations, in order to optimize algorithms.\n\nIn the words of Toyota Research Institute Chief Executive Gill Pratt, \u201cSimulation is a tremendous thing.\u201d The correct understanding and prudent application of self-driving simulations are essential to safely accelerating R&D in the autonomous vehicle industry.\n\nReference:"
    },
    {
        "url": "https://medium.com/syncedreview/ai-powered-hockey-analytics-a-game-changer-8534b2e263aa?source=user_profile---------41----------------",
        "title": "AI-Powered Hockey Analytics: A Game Changer \u2013 SyncedReview \u2013",
        "text": "Analytics are all the rage in professional sports. The concept can be traced back to American statistician Bill James, who introduced his \u201cSabermetrics\u201d method for in-game baseball analysis in the 1970s. When the NBA\u2019s Golden State Warriors decided to favour three-pointers over two-point shots in 2016, the winning strategy sent a shockwave through professional basketball. This was a \u201cdata-driven decision\u201d based on higher score probability, explains Alex Martynov.\n\nMartynov is the 24-year old founder of ICEBERG, a Canadian startup using AI algorithms in sports analytics with a focus on ice hockey. Three years ago, Alex shared the idea of an AI sports analytics company with his investor father, who helped him kickstart the idea with $25,000. Not much, but it was enough for Alex to gather programmer friends in Toronto and Moscow and put together a working prototype.\n\nICEBERG installs a set of three FLIR thermal cameras around the rink before the start of each game. The video has lower resolution than an iPhone recording, but provides the constant full-ice view the company\u2019s algorithms require, as broadcast feeds typically leave 50 percent or more of the ice surface and players out of frame.\n\nArtificial neural networks are trained to recognize all moving entities on the ice surface: 12 players grouped by jersey color, on-ice officials, and a small black puck that can reach speeds of 160 kph (100 mph). Computer vision algorithms previously trained on a dataset containing 10,000 variations of numbers from all angles can identify each player by jersey number.\n\nBy tracking player and puck coordinates 10 times per second, a sixty-minute game will generate one million data points. The algorithm matches individual player coordinates with those of the puck to record their passes, body checks, giveaways and takeaways, shots, and goals. Typically, about 7\u20139 percent of all shots result in goals, and variance here predicts higher or lower goal probability.\n\nICEBERG\u2019s AI tracks a total of 500 different metrics which correspond to player and team behavior, and the company sometimes finds statistical nuances that are counterintuitive to hardcore fans or watchful coaches.\n\nIn a match between favorite Canada and underdog Switzerland, Iceberg\u2019s AI found that Switzerland skated 1.7 more kilometers and were 5\u201310 centimeters closer to the puck in micro-episodes. The Swiss also had longer puck possession and generated 2.48 more expected goals (xG). Switzerland\u2019s superior metrics should deliver a win seven times out of ten. But Canada won the game 3\u20130. Why?\n\nMartynov explains that \u201cabout 40 percent of all game outcomes is luck, but the other 60 percent can be predicted, which is what we are trying to do \u2014 predicting what isn\u2019t random. Our clients can play five games and lose five times in a row, but data will show that they could\u2019ve won every time. The coach will call our analyst, and we tell them, \u2018calm down, it\u2019s just the variation, you will get back to the mean, if you continue playing like this you will win five games in a row\u2019.\u201d\n\nICEBERG uses NVIDIA\u2019s GPU and marketing expertise and Microsoft Azure\u2019s cloud storage. The company also participates in NVIDIA\u2019s Inception Program.\n\nPortal subscription fee ranges from US$400 \u2014 $800 per game. If a team plays 60 games in a season that\u2019s approximately US$30,000. Clients receive a report the morning after each game and can access detailed game numbers from the portal. ICEBERG also has on-call analysts to answer clients\u2019 questions.\n\nFinding clients can be a long process of convincing the coach, the manager, and the owner. ICEBERG\u2019s deal with Austria\u2019s Red Bull Salzburg required four months of negotiation. \u201cThere are coaches who are confused, asking \u2018why do I need this?\u2019\u201d says Martynov. \u201cWe are not trying to replace the coach or the manager, but give teams an edge. It makes hockey more intellectual.\u201d\n\nThere are also cases like Swedish teams V\u00e4xj\u00f6 Lakers and F\u00e4rjestad BK, who signed contracts in five minutes. The competitive edge of data analytics is too good to be ignored.\n\n\u201cCurrently, We have a market share of 5\u20137% of global professional hockey teams. But it\u2019s not moving as fast as I would like,\u201d says Martynov, \u201cWe want to go into the soccer market after this. If you get two percent of the soccer market, that\u2019s approximately the same as the entire hockey market. We started in the niche market, but hockey is also a very complicated sport where players skate fast, collide often, change every minute, not to mention the puck is very small. Technically, it\u2019s easy to downgrade from hockey into other sports.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-gtc-2018-peeks-inside-the-gpu-powered-world-3366891680fe?source=user_profile---------42----------------",
        "title": "NVIDIA GTC 2018 Peeks Inside the GPU-Powered World \u2013 SyncedReview \u2013",
        "text": "In 1993, on his 30th birthday, Jensen Huang gave himself the best present ever by founding NVIDIA, the graphic-processor company where he still serves as CEO. Huang and his NVIDIA team pioneered the graphics processing chip (GPU) in 1999, revolutionizing the visual performance of device displays. But even Huang never dreamt that his GPU would one day become a driving force in the arena of artificial intelligence (AI).\n\nIn 2011 at Stanford University, Andrew Ng, one of the top minds in AI, discovered that a dozen NVIDIA GPUs could perform as well as 2,000 CPUs in training deep learning models. A GPU contains thousands of cores and is capable of processing thousands of threads simultaneously. This parallel architecture makes GPU extremely powerful in large-scale but straightforward data computation.\n\nOther top academic institutes and laboratories quickly followed Ng and deployed GPUs for deep learning research with the belief that GPU could eliminate bottlenecks in computing capabilities that had vexed AI researchers for years and spark technological breakthroughs.\n\nNVIDIA quickly recognized this trend and pivoted its strategy to become an AI computing company. Over the last six years, the Santa Clara chipmaker has been pushing the limits of GPU architecture, releasing a cutting edge GPU every one or two years to empower computing-hungry applications and products. Over the last five years, the number of developers with expertise in GPU has grown tenfold; CUDA downloads five times; and total GPU flops of the top 50 systems 15 times.\n\nSince 2009, NVIDIA has been hosting the annual GPU Technology Conference (GTC), which showcases company releases and provides an exhibition venue for GPU-based innovations. GTC attendance has soared from 2000 attendees in 2012 to the 8,500 developers, buyers and innovators who went to Santa Clara, California for GTC 2018 last month.\n\nThe role and deployment of GPUs is changing dramatically, and they are creating significant new market opportunities. Synced visited GTC 2018 to explore a world based on GPUs.\n\nTech giants usually have a capital investment arm \u2014 such as Google Ventures or Microsoft Ventures \u2014 to fund AI startups. NVIDIA GPU Ventures invests in and nurtures next-generation companies built on GPU. A number of NVIDIA portfolio companies have become high-profile AI startups: H2O.ai, Element.ai, and Drive.ai.\n\nAt GTC 2018, a large black semi truck parked outside the San Jose McEnery Convention Center became a centre of attention. The prototype was from TuSimple, a leading Chinese autonomous-driving truck company founded by entrepreneurs Mo Chen and Dr Xiaodi Hou from the California Institute of Technology.\n\nThe TuSimple semi operates on an accelerator fusion of NVIDIA GPUs \u2014 including GTX 1080Ti, NVIDIA Drive PX and Jetson TX \u2014 to process huge amounts amount of data.\n\nDrive PX is NVIDIA\u2019s first in-vehicle supercomputer, introduced in 2015. As computing needs for self-driving vehicles have ramped up, NVIDIA has developing powerful vehicle-specific processors to help the company stay ahead of the race. NVIDIA recently successively unveiled the advanced Drive PX Pegasus and what it billed as the world\u2019s most powerful in-vehicle System-on-Chip (SoC), Xavier.\n\nLast June, TuSimple completed a 200-mile high-automated test drive from San Diego to Yuma, Arizona. The company\u2019s fast tech development attracted interest from NVIDIA GPU Ventures, which joined a group of investors led by Chinese social media company Sina putting more than US$20 million into TuSimple last August.\n\nNVIDIA is not just writing checks. In June 2016, the company introduced a virtual incubator, NVIDIA Inception Program, to nurture AI startups. In 18 months over 2000 companies have applied to the program, and only 70 have so far been accepted. NVIDIA also hosts the Inception Competition at GTC, where its portfolio companies compete for NVIDIA Inception Awards and US$1.5 million in prize money.\n\nIsraeli cybersecurity startup Deep Instinct was one of the winners last year. The company uses a GPU-based neural network and CUDA to achieve 99 percent detection rates, compared with about 80 percent detection from conventional cybersecurity software. Last June, NVIDIA pumped US$10 million into Deep Instinct. \u201cNVIDIA introduced us to their strategic accounts which are now our customers, which has been very helpful,\u201d said a Deep Instinct representative at GTC 2018.\n\nOn this year\u2019s final pitch day, robotic arm company Kinema Systems won the NVIDIA Inception Award and took home US$375,000. The company\u2019s flagship product is the AI-powered industrial robotic vacuum grabber Kinema Pick, which runs on GTX 1060 and NVIDIA embedded AI computing device Jetson TX2.\n\nKinema Founder and CEO Sachin Chitta says NVIDIA provides portfolio companies with \u201cspecial treats,\u201d such as a discount on NVIDIA hardware, a training course conducted by NVIDIA experts, and not least a chance to appear at GTC: \u201cThis conference provides a great opportunity for exposure. We have met many customers and investors who showed a huge interest in our products.\u201d\n\nTech giants believe AI can reimagine conventional diagnostic methodologies in medical health, increasing accuracy and reducing costs. IBM has been using slides to train deep neural networks to detect tumours since 2016. Google has successfully produced a tumour probability prediction heat map algorithm whose localisation score reached 89 percent, significantly outperforming pathologists\u2019 average of 73 percent.\n\nThe medical health field was not widely addressed in the last few GTCs, but NVIDIA is now putting more effort into this area.\n\nAt GTC 2018 NVIDIA unveiled Project Clara \u2014 a medical imaging supercomputer deployed on its cloud platform. Clara is designed to transform standard medical images such as X-rays, ultrasound scans, CTs, MRIs, PETs, and mammograms into high-resolution cinematic renderings. Because deploying GPUs in every clinic and hospital would not be cost-effective, Clara provides its users with high-performance cloud-based services.\n\nNVIDIA added medical health talks and panels at GTC 2018, and invited renowned professionals to represent their research achievements based on GPUs.\n\nThomas Fuchs is the Founder and CEO of New York startup Paige.ai, founded this January to fight cancer with AI. The company has access to a dataset of 25 million pathology images and financial support from Breyer Capital, which led a US$25 million Series A Funding Round.\n\nIn an interview with Synced, Fuchs said he believed the time was right to build Paige.ai because the requirements were all in place: qualified devices, extensive collection of medical images, and full-fledged deep learning algorithms.\n\nMost importantly, GPU advancement drives deep learning development with an unprecedented scale of medical image data. Paige.ai has now built a high performance compute cluster with hundreds of NVIDIA GPUs.\n\nGPUs will not replace the central processing unit (CPU), which is still extremely powerful in performing the necessary arithmetic, logical, control and input/output (I/O) operations for example for personal computers. However, GPU are better than CPU in particular tasks: they are unquestionably the best chip for image processing in gaming, and their parallel architecture happens to be very well-suited for deep learning.\n\nA few innovative startups have discovered that GPU acceleration can also deliver better performance than CPU in databases, particularly in repetitive operations on large amounts of data.\n\nOnline Transaction Processing Databases (OLTP) primarily handle day-to-day transactions for example for banks. Oracle is the dominant vendor here, accounting for nearly 50 percent of the market share. Online Analytical Processing (OLAP) databases meanwhile are designed to handle complex analysis of large volumes of data, and consequently many AI applications are now running on OLAP databases. The estimated market size for OLAP databases is US$22.8 billion in 2020, a lucrative emerging market for startups to target.\n\nGPU database companies first appeared in 2016. Silicon Valley pioneers Kinetica and MapD have raised US$50 and US$37 million respectively; and Israel\u2019s SQream US$31.5 million. Last year, Chinese database leader Zilliz raised an undisclosed amount (reportedly RMB\uffe5100 million or US$16 million).\n\nSays Zilliz Founder and CEO Chao Xie, \u201cUsing a GPU to run a database can be traced back to 15 years ago in academia. But it didn\u2019t work as the top software has long been constrained by the underlying hardware. Now, NVIDIA and other chipmakers are building an infrastructure for GPUs, helping developers to reduce the threshold of developing GPU-based applications.\u201d\n\nZilliz\u2019s GTC 2018 booth showcased the company\u2019s cutting-edge GPU database, which it says can increase the performance of data processing up to 100 times over CPUs; reduce hardware cost by 10 times; and lower data centre operation and energy costs by up to 95 percent.\n\nThe SQream booth was a few meters from Zilliz\u2019s. Founded in 2010, SQream spent their six years conducting research and building databases, and launched its commercial product in 2016.\n\n\u201cIf you have tens of hundreds of terabytes [data], and you try to do some database operation on what we called Massive Parallel Processing (MPP) databases, sometimes the query goes up to 30 minutes to an hour. When you put data into the SQream instead of MPP, the query goes down to five minutes,\u201d says SQream Senior Solutions Architect Arnon Shimoni.\n\nAs AI applications thrive with complex neural networks and large datasets, researchers and developers can of course invest in GPU clusters \u2014 but another option is to purchase on-demand services on the cloud for GPU-intensive tasks.\n\nThis premise is growing the market for GPU-as-a-Service (GaaS) solutions, which is set to exceed US$5 billion by 2024, according to a new research report by Global Market Insights.\n\nMajor cloud service vendors AWS, Google Cloud, and Microsoft Azure have been hosting NVIDIA GPU-equipped virtual machines for their cloud machine learning services for some time. Google began incorporation of NVIDIA GPU in its cloud computing centres back in November 2016, just months after AWS announced a new Elastic Compute Cloud (EC2) instance type, dubbed P2, which leverages NVIDIA GPUs.\n\n\u201cGaaS will be used for augmented reality, but will also able to handle massively parallel complex app problems like encryption (or decryption), weather forecasting, business intelligence graphical displays, big data comparisons,\u201d writes Jack Gold, founder and principal analyst at analyst firm J. Gold Associates.\n\nAlthough a few cloud services giants will dominate the market for GaaS, small and medium enterprises still have a chance to take a piece of the pie.\n\nCirrascale Cloud Services is a San Diego-based company that enables researchers and data scientists to attach GPU acceleration to a wide range of tasks over the network. Although its service is similar to AWS or Google Cloud, the company appeals to users who train models for weeks or even months at a time by offering a 35 percent lower price point and 35 percent faster speed compared to AWS.\n\nSays Cirrascale \u200eExecutive Account Manager Andrew Kruszewski, \u201cWe realized that there was a market, people [researchers] would come and test on the system, and they wouldn\u2019t want to get off. They also didn\u2019t want to own the equipment because NVIDIA changes their GPU so often.\u201d\n\nLaunched just three years ago, Cirrascale\u2019s cloud services have been growing so quickly that the company had to cut other divisions. Last year, its design and manufacturing business was sold to BOXX technologies.\n\nOver the last two years, NVIDIA\u2019s stock price has skyrocketed thanks to the rapidly increasing role AI is playing in the company\u2019s revenue growth. Full-year revenue of 2017 was US$9.71 billion, up 41 percent from a year earlier, and its discrete GPU market share increased to 72.8% during the third quarter of 2017.\n\nHowever, rivals Google and Intel are catching up, and developing their own AI chips to challenge NVIDIA. This February, Google announced that its Tensor Processing Unit (TPU) \u2014 a custom chip that powers neural network computations \u2014 will be available in beta for researchers and developers on the Google Cloud Platform.\n\nLast year, Google boasted that its TPUs were 15 to 30 times faster than contemporary GPUs and CPUs in inferencing, and delivered a 30\u201380 times improvement in TOPS/Watt measure. In machine learning training, TPU are more powerful in performance (180 vs. 120 TFLOPS) and two times larger in memory capacity (64 GB vs. 32 GB of memory) than NVIDIA\u2019s top GPU Tesla V100.\n\nJoe Pelissier, Distinguished Engineer at Cisco Systems, says that a serious challenger may even replace NVIDIA in the next three to five years.\n\n\u201cThe type of mathematics for machine learning you basically need to be able to do is multiplication. Everything else is at least two orders of magnitude less significant. So you can imagine Silicon Valley, there are a lot of folks saying \u2018hey, if i take out a lot of functionality that GPU has, and only leave the stuff it needs for machine learning, I can either make it cheaper, or I can put more cores in it, or a combination of both\u2019,\u201d says Pelissier.\n\nMany experts believe that while NVIDIA GPU were not initially created for AI, they are now embedded in it and cannot be easily replaced.\n\nBrett Newman, VP marketing and customer engagement at compute hardware company Microway, says NVIDIA has done a good job building the ecosystem. \u201cThey are making software tools better applied for deep learning training. And they are making developer friendly things like Digits [Deep Learning GPU Training System]. I think that stuff is setting them up for success that will persist into the long-term.\u201d\n\nHuang quipped at GTC 2018 that \u201cNVIDIA is still a small company with only 10,000 employees.\u201d But he is too humble. It\u2019s no small achievement to have built NVIDIA from a birthday present into a US$150 billion chip giant in 25 years. And now, the company\u2019s GPU have become the muscle powering AI research and innovation. It\u2019s been an incredible journey for NVIDIA, one that will continue to empower AI long into the future."
    },
    {
        "url": "https://medium.com/syncedreview/lawyee-has-digitized-40-million-chinese-court-records-a53f02fa5061?source=user_profile---------43----------------",
        "title": "Lawyee Has Digitized 40 Million Chinese Court Records",
        "text": "Lawyee is a Peking University spinoff founded in 2003 that was one of the first companies specializing in building searchable legal databases and digitizing IT systems for Chinese courts and law firms.\n\nWhen Lawyee started the Internet\u2019s legal landscape was barren and computer technologies underdeveloped. Things fast-tracked however in 2014 when China\u2019s Supreme People\u2019s Court made publishing court decisions on an official online archivemandatory within seven days of entry into force, excluding politically-sensitive, private or youth crime cases.\n\n\u201cLawyee became the contractor for building this database,\u201d says company General Vice Manager Chen Hao, \u201cand we accumulated several million documents by the end of 2014.\u201d This number skyrocketed to 40 million in mid-2018, and page views exceeded 14 billion. Lawyee also runs a case law database which Chinese law schools can access for an annual subscription fee of few thousand US dollars.\n\nResponding to recent advances in AI, Chen explains that \u201cAI evolved out of decades of statistics research. Legal researchers have also used statistics for a long time, and there are many machine learning algorithms on SPSS Statistics. Since 2006 deep learning has upgraded machine perception.\u201d\n\n\u201cHowever there aren\u2019t many mature commercial applications at least in areas of law. For both common and continental legal systems, technology so far has only tackled vertical problems, but it\u2019s not enough to reinvent the legal industry. Now there are startups building applications on top of IBM Watson that may prove successful.\u201d\n\nBuilding on strong database resources, AI can generate legal documents and perform compliance review of contracts, quality control of documents, and analysis of legal risks, business guidelines, and so on.\n\nOne of Lawyee\u2019s current AI projects is identifying the core issues in a legal document. Lawyee\u2019s AI trained on more than 30 million data samples can currently identify core arguments in about 70 out of 100 documents, and be exactly on-target with 60.\n\nChen says the hard part is human-labelling larger quantities of data. \u201cWhen data samples rise from 100 to 10k, people can no longer handle the repetitive work, not to mention when data size goes up to 300k plus.\u201d\n\nAnother challenging task is finding benchmarks for algorithm performance, which is crucial for measuring accuracy. \u201cThere are no benchmark datasets such as ImageNet for image or SQuAD for NLP in the legal space. Labelling them is very expensive, thus impossible for normal companies. For accuracy, we proclaim that certain software can go up to 70\u201380 percent. Customers are willing to pay more if other options on the market don\u2019t work as well,\u201d says Chen.\n\nThe Chinese government directive mandating legal document digitalization has engendered a number of Beijng legaltech startups, most notably AI-powered legal consulting companies Lvpin Technology and Itslaw, along with natural language processing solution providers Fa\u2019gou\u2019gou and most recently deepcurious.ai."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-ministry-of-education-sinovation-and-peking-u-join-hands-to-train-ai-talents-f2d21ce3387e?source=user_profile---------44----------------",
        "title": "Chinese Ministry of Education, Sinovation, and Peking U Join Hands to Train AI Talents",
        "text": "In Beijing today, China\u2019s Ministry of Education, Sinovation AI Lab, and Peking University jointly announced the Global AI Talent Training Program for Chinese Universities. The program will include tutorships from Turing Award winner John E. Hopcroft, Deep Learning pioneer Geoffrey Hinton, and Sinovation CEO Kai-Fu Lee.\n\nThe program has its origins in a February meeting between Kai-Fu Lee, officials from China\u2019s Ministry of Education, and computer scientists from more than 50 Chinese universities.\n\nThere are two components, the DeeCamp Student Boot Camp and a Teacher Training Program. In the explorational phase, the program will select 100 teachers and 300 students from top CS-heavy Chinese high schools to test teaching methods. The bigger plan is to educate at least 500 teachers and 5,000 students over five years.\n\nIn addition, the Ministry of Education plans to set up the China-US University AI Talents Cooperation Training Program to provide Chinese students with AI scholarships for US colleges and universities.\n\nIn July 2017, the State Council issued the New Generation Artificial Intelligence Development Plan and listed \u201caccelerating the education of top-notch AI talents\u201d as a primary task, emphasizing the importance of \u201cimproving the artificial intelligence education system, strengthening talent reserve and echelon building, and occupying the AI highland in China.\u201d\n\nToday\u2019s announcement provides a framework with the international experience and expertise required to take on that task."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-mar-w-4-apr-w-1-caec29bd494?source=user_profile---------45----------------",
        "title": "AI Biweekly: 10 Bits from Mar W 4 \u2014 Apr W 1 \u2013 SyncedReview \u2013",
        "text": "Now social and behavioral scientists can use the TuringBox platform to study artificial intelligence algorithms. AI contributors can upload existing and novel algorithms for review, gaining a reputation in their community. AI examiners meanwhile develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.\n\nAmper Music is the world\u2019s first AI-powered music composer, enabling users to create and customize music. The company raised an additional US$4 million in seed round funding, which will help double the number of US Amper Music employees and help the company expand internationally.\n\nMarch 22th \u2014 Google Assistant Lets You Send and Receive Money on Phones\n\nGoogle Phone users can now send and receive money with Google Pay for free via Google Assistant. This feature is currently only available on new phones due to security concerns. It will be offered on Google Home smart speakers in the coming months.\n\nMarch 27th \u2014 Toshiba Plans to Transform Itself With AI and IoT\n\nToshiba introduces a high-level digital transformation strategy for its business, with a focus on both AI and IoT solution development. Toshiba believes the strategy will maximize customer value through its various ecosystem partners.\n\nWaymo partners with Jaguar to build a 20,000-electric-car fleet with fully autonomous driving capabilities in the next two years. The cars will have sufficient battery life to drive all day on one charge.\n\nMarch 27th \u2014 Nvidia Teams up with ARM to Develop Deep Learning for IoT Devices\n\nAt GTC 2018, NVIDIA and ARM announce a partnership to integrate open source NVIDIA Deep Learning Accelerator architecture into ARM\u2019s Project Trillium platform. This collaboration aims to simplify the product integration process for IoT chip companies.\n\nHitachi\u2019s new AI Technology service analyzes product, inventory, demand and sales information on warehouse management systems (WMS) and learns effective measures based on evaluation results. Hitachi says the service can boost product picking efficiency by 16 percent.\n\nMitsubishi Hitachi Power Systems (MHPS) announces new investments in innovative technologies with digital tools to deliver solutions to the power industry. MHPS plans to use AI technology, battery storage, and geothermal renewable power to achieve better energy efficiency.\n\nApple announces that its health record products are now being used by 40 health systems and 300 hospitals. It has traditionally been time-consuming for patients to obtain their health records, and Apple\u2019s health records products streamline the process. Apple is planning to make the products available to all iOS users.\n\nMicrosoft France pledges US$30 million over three years to further France\u2019s artificial intelligence development. The AI Impact group aims for AI with a positive impact on environment, transport, and health in France. The AI Skills program meanwhile will train 400,000 people over three years and aims to create 3,000 new jobs in the French digital ecosystem."
    },
    {
        "url": "https://medium.com/syncedreview/caffe2-merges-with-pytorch-a89c70ad9eb7?source=user_profile---------46----------------",
        "title": "Caffe2 Merges With PyTorch \u2013 SyncedReview \u2013",
        "text": "Facebook operates two flagship open source machine learning frameworks \u2014 Caffe2 and PyTorch. Their incompatibility, however, made it difficult to transform a PyTorch-defined model into Caffe2 or vice versa. Facebook is doing something about that.\n\nLast Friday the Caffe2 Github page introductory \u201creadme\u201d document was suddenly replaced with a bold link: \u201cSource code now lives in the PyTorch repository.\u201d What this meant was that Caffe2 users could now directly check Caffe2 code in PyTorch.\n\nFacebook AI Researcher and Caffe creator Yangqing Jia says Facebook decided to merge Caffe2 into PyTorch \u201cbecause this will incur minimal overhead for the Python user community.\u201d\n\nNothing changes for a PyTorch user, says Facebook AI Researcher Soumith Chintala. \u201cPyTorch is installed, shipped and used exactly how it is done today. Your code will not break. This is development and backend work. If you are not a core-developer, this issue is not even that relevant to you.\u201d\n\nAlthough most frameworks are similar to certain degrees, each has its unique characteristics. Sharing repository and development infrastructure between different machine learning frameworks can compensate for each one\u2019s shortcomings.\n\nSince its release in October 2016, PyTorch has become a preferred machine learning framework for many AI researchers due to its research flexibility. Over half of Facebook AI projects run on PyTorch. Meanwhile, Caffe 2, launched in April 2017, is more developer-friendly than PyTorch for AI model deployment on IOs, Android and Raspberry Pi devices.\n\nLast September, Facebook and Microsoft announced their Open Neural Network Exchange (ONNX), an open source project that helps researchers to convert models between frameworks. The merging of Caffe2 and PyTorch is a logical next step in this strategy.\n\nThe merging also ups the stakes in Facebook\u2019s challenge to the dominant machine learning framework, TensorFlow.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinese-video-surveillance-giant-hikvision-to-opensource-its-ai-technology-7e68412a14ec?source=user_profile---------47----------------",
        "title": "Chinese Video Surveillance Giant Hikvision to Opensource its AI Technology",
        "text": "At 2018 AI Cloud Ecological International Summit in Hangzhou on Mar. 28, global video surveillance manufacturing leader Hikvision announced it would open access to its AI technology, notably AI Cloud.\n\nHikvision AI Cloud is a distributed structure incorporating cloud computing and edge computing. Launched last year, it can extend an AI algorithm from the cloud centre to an edge network of on-premises video recorders and servers, and further to edge devices, such as security cameras.\n\nHikvision has deployed AI Cloud in more than 30 Chinese provinces, providing AI-empowered solutions for emergency management, maintenance, urban operation, traffic management, business intelligence, etc. Hikvision\u2019s facial recognition system at an intersection in Suqian, Jiangsu, decreased red-light-running violations by over 90 percent.\n\nHikvision CEO Hu Yangzhong says the company will open an AI development platform for application developers. At the same time, AI Cloud will also merge algorithms from other AI companies.\n\n\u201cThe industry should jointly promote the development and application of AI in the security camera industry,\u201d says Hu.\n\nHikvision will also launch an open training system providing transfer learning and augmented learning capabilities; AI services on EZVIZ, its video-service application designed for consumer markets; and data labelling and sharing services.\n\nFounded in 2001, Hikvision is dedicated to improving video surveillance and video analysis technology, and providing surveillance products and solutions. The company accounted for 21.4 percent of the global market in CCTV and video surveillance in 2016, and now leads the security surveillance market with an estimated value of US$63 billion.\n\nHikvision\u2019s surveillance system has integrated AI chips with frontend cameras, and developed data analysis systems on the backend. As a result, it is an essential supplier for China\u2019s Skynet, a real-time surveillance program for public security. Last year BBC reporter John Sudworth agreed to be tracked by the system, which required just seven minutes to locate and \u201capprehend\u201d him.\n\nNot to be outdone by Hikvision, China\u2019s leading tech companies are also building open-sourced AI platforms and sharing data access, tools, and backend codes with developers. Baidu last year opened its smart assistant platform DuerOS and autonomous driving platform Apollo. By January 2018, DuerOS had activated more than 50 million smart devices, with over 10 million active devices per month.\n\nIFlytek, a leading Chinese AI company known for its voice technology, plotted a bold course last year: \u201cProject 1024\u201d will package CN\uffe51.024 billion into a developer fund, build 1024 professional teams, and support 1024 AI projects."
    },
    {
        "url": "https://medium.com/syncedreview/the-new-age-of-discovery-space-exploration-and-machine-learning-64883f7dc7f9?source=user_profile---------48----------------",
        "title": "The New Age of Discovery: Space Exploration and Machine Learning",
        "text": "The Age of Discovery began in the 15th century, when Europeans built their first oceangoing vessels and set out to explore the world. Whether motivated by political, economic or cultural factors, human exploration has traditionally been driven by technological progress.\n\nRocket booster technology developed during World War II enabled the first generation of spaceflight in the mid 20th-century, when the Soviet Union and the United States launched artificial satellites and interplanetary probes. As humans step up to deep space exploration, artificial intelligence technologies are expected to play a huge role.\n\nThe \u201clearning\u201d part of machine learning refers to an algorithm\u2019s ability to find patterns in data to self-improve the machine\u2019s outcomes, ie to use existing data to predict unknowns. Machine learning already has applications in banking, healthcare, aviation, and so on, and the technology is expected to power future space exploration as it can handle huge data volumes, find patterns in planet image datasets, and predict spaceship condition.\n\nThe role of machine learning in space exploration can be roughly divided into data transmission, visual data analytics, navigation, and rocket landing.\n\nSpacecraft and satellites operating in deep space can generate huge amounts of data due to the complexity of their research missions. Because of the different rotations and orbits of their host planets, these massive data packets must be transmitted to earth during specific windows of opportunity. The lag meanwhile will depend on Earth\u2019s light year distance from the spacecraft\u2019s host planet and may be months or even years. Moreover if a data packet transmission is unsuccessful, the data may be permanently lost if it was overwritten with new data in the onboard memory.\n\nMachine learning enables a \u201csmart\u201d method to manage the distant planet to Earth data transmission problem. The outer space machine learning application MEXAR2 (\u2018Mars Express AI Tool) was introduced in 2005 at Italy\u2019s Institute for Cognitive Science and Technology (ISTC-CNR). The onboard learning algorithm can leverage historical data to remove superfluous data and pinpoint the download schedule to optimize data packet transmission. This outer data transmission technique is already being used by NASA and others in their space research programs.\n\nA usual early step in deep space exploration is planet condition and environment analysis. Satellites and space telescopes have already collected a large amount data for example for target planet Mars. Images are the major data source, while the major challenge is how to identify and read the right information from the images. Machine learning has become an effective technique for solving this problem.\n\nThe NASA Frontier Development Lab and top-tier technologies companies such as IBM and Microsoft are collaborating on machine learning as a solution for solar storm damage detection, targeting a target planet\u2019s \u2018space weather\u2019 through magnetosphere and atmosphere measurement. The technique can also be used for resource discovery and to identify suitable planet landing sites.\n\nAnother field where machine learning can improve current technology is in relative spacecraft and satellite motion control. Each control action selected for spaceships or satellites requires considering and processing geometric and kinematical location information in an extremely short timeframe. As outer space missions become increasingly frequent and complex and spacecraft get further from Earth, there will be growing demand for fast and self-adjusting machine-learning based navigation capabilities. The field could include orbit adjustment, autonomous navigation, and space station docking.\n\nThe NASA Jet Propulsion Laboratory (JPL) is already involved in the above research field, and machine learning has emerged as a key technique for measurement and adjustment of a spacecraft\u2019s motion with different orbital parameters. This allows the spacecraft to self-adjust for example orbit and velocity, and can support ground navigation systems to control a spacecraft\u2019s flight path, engine power and orbital position. A spacecraft\u2019s onboard machine learning algorithm also has the potential to perform autonomous navigation in deep space.\n\nRecent research in landing spacecraft has focused on developing algorithms that increase the level of autonomy for air and space systems. Some of the major issues for spaceship or rocket landings include vacuum stage, software errors, guidance and sensor problems etc. Machine learning and computer vision are the core optimization and evaluation techniques for successful landings.\n\nThe SpaceX Falcon 9\u2019s successful landing at Cape Canaveral Air Force Station in 2015 demonstrated machine learning and computer vision\u2019s power to transform space exploration. SpaceX used a convex optimization algorithm to determine the best way to land the rocket, with real-time computer vision data aiding route prediction. These advanced machine learning applications enabled the first reusable rocket in space exploration history \u2014 a feat scientists regard as essential in developing deep space exploration.\n\nDespite the challenges, machine learning will, or must, play a vital role in the coming age of space exploration.\n\nWith the help of advanced machine learning based terrain classifiers and path planning algorithms, NASA built a Mars Rover which can navigate long distances on the planet\u2019s complex surface. Mars may be humans\u2019 current target but the red planet will not be our final destination. There are reports that NASA will deploy robotic machine learning based probes to Jupiter\u2019s moon Europa to search for life, and NASA engineer Hiro Ono says autonomous spacecraft are in the design phase.\n\nSoon, spacecraft may operate using only artificial intelligence and machine learning algorithms. As in the past, it is technological innovations that will enable humans to go \u201cwhere no man has gone before.\u201d For the immediate future, those innovations will continue to emerge from machine learning."
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-ceo-says-fgpa-is-not-the-right-answer-for-accelerating-ai-83c810969edd?source=user_profile---------49----------------",
        "title": "NVIDIA CEO Says \u201cFGPA is Not the Right Answer\u201d for Accelerating AI",
        "text": "Accelerating resource-hungry AI applications demands chip performance beyond what mere CPU or GPU can deliver, prompting researchers to turn to sophisticated Application-specific Integrated Circuits (ASIC) and Field Programmable Gate Arrays (FPGA). Chip giant NVIDIA Founder and CEO Jensen Huang created a bit of a stir at yesterday\u2019s GPU Technology Conference in Santa Clara, USA, when he appeared to dis one of these chips\u2019 appropriateness for autonomous vehicle system development: \u201cFPGA is not the right answer,\u201d he said.\n\n\u201cFPGA is really for prototyping. If you want the [self-driving] car to be perfect, I would build myself an ASIC because self-driving cars deserve it,\u201d says Huang.\n\nFPGAs are logic chips best known for their programmability, which gives engineers the flexibility to configure an FPGA for example as a micro-control unit today, and use the same FPGA as an audio codec tomorrow. ASICs meanwhile are custom chips with little or limited programmability. Because FPGAs are more versatile, chip makers can streamline their operations by developing FPGAs rather than ASICs. However FPGAs are both more expensive and less powerful than ASICs.\n\n\u201cWhen you want to build something for cars, you should have a very large concentrated group of expert engineers design the chip one time and sell it to everyone, instead of a hundred random groups of different levels of capability and expertise build their own chips,\u201d says Huang.\n\nNVIDIA has never been impressed with FPGA. Chief Scientist Bill Dally once said \u201cif you want to solve a problem and you are willing to devote a lot of engineering time, just develop the ASIC directly. I don\u2019t think the FPGA is competitive.\u201d\n\nNVIDIA has been developing ASIC for years and has traditionally kept their tech under wraps. At last year\u2019s GTC however the company decided to share the architecture of their Deep Learning Accelerator (DLA) \u2014 an ASIC for deep learning inferencing \u2014 on the open-source codebase Github.\n\nNVIDIA recently announced an agreement with British chip IP company Arm to integrate DLA architecture into Arm\u2019s new Project Trillium platform, which hastens the development of AI inferencing accelerators. As 90 percent of AI-enabled devices shipped today are based on architecture developed by Arm, NVIDIA\u2019s DLA is expected to be deployed on billions of mobile, consumer electronics, and the Internet of Things (IoT) devices.\n\nHowever even as NVIDIA snubs FPGA, rivals like Intel are ramping up efforts to develop and deploy them. In 2015 Intel acquired top US manufacturer of programmable logic devices Altera in an all-cash transaction estimated at US$16.7 billion. Intel has since developed a CPU+FPGA hybrid chip for deep learning inference on the cloud.\n\nIntel also introduced its Movidius Myriad X Vision Processing Unit (VPU), a system-on-chip (SoC) used for vision devices such as smart cameras, augmented reality headsets and drones. The Myriad X is shipped with a dedicated Neural Compute Engine (NCE) for running deep neural networks at high speed and low power in real time at the edge. With NCE, the Myriad X can reach one trillion operations per second in deep learning inferencing.\n\nMeanwhile, the world\u2019s leading supplier of programmable logic devices Xilinx is competing with Intel\u2019s Altera in the FPGA market. While Intel dominates the server chip market, Xilinx has the technology lead, helping the company win orders from large cloud customers.\n\nChinese startup DeepPhi last year garnered US$40 million in funding \u2014 led by Xilinx \u2014 to develop its Deep-Learning Processing Units (DPU), which include both FPGA chips and ASIC chips."
    },
    {
        "url": "https://medium.com/syncedreview/france-pumps-1-5-billion-into-ai-in-bid-to-catch-up-2470a7fc132b?source=user_profile---------50----------------",
        "title": "France Pumps \u20ac1.5 Billion into AI in Bid to Catch Up",
        "text": "France is pledging \u20ac1.5 billion to hasten the development of its fledgling AI ecosystem. French President Emmanuel Macron made the commitment this morning at the Artificial Intelligence Summit held at the College de France Research Center. The event included industry discussions featuring top-notch researchers and high-level ministers such as Secretary of State for Digital Mounir Mahjoubi. France also announced partnerships with DeepMind, Samsung, and Fujitsu as part of a grand strategy to build Paris into a global AI hub.\n\nA Summit highlight was the release of the 152-page report \u201cAI for Humanity,\u201d written by President Macron\u2019s star technology advisor, Fields Medal-winning mathematician C\u00e9dric Villani.\n\nVillani accepted government appointment just six months ago, and has quickly pulled together \u201cMission Villani,\u201d composed of machine learning researchers and members of Europe\u2019s Digital Advisory Council. The team interviewed 350 industry leaders to help form \u201cune strat\u00e9gie national port\u00e9e par le plus hautes autorit\u00e9s et la d\u00e9cliner en feuille de route concr\u00e8te\u201d \u2014 \u201ca national strategy carried by the highest authorities and transformed into a concrete roadmap.\u201d\n\nThe strategy promises to comply with European Union\u2019s data privacy policies, namely the General Data Protection Regulation (GDPR), which will take effect this May.\n\nBack in 2017, the French government released the 200-page document France Intelligence Artificelle, detailing over 50 policy proposals and placing the number of French AI startups at over 270.\n\nIn the global AI competition, France trails neighbours Germany and UK, and all lag far behind leaders USA and China. President Macron incorporated the nation\u2019s innovation challenge his election campaign, calling on France to become a pro-Europe \u201cstartup nation.\u201d France has clustered talents from EU\u2019s robotics industries, the Human Brain Project, and FET projects, and accelerated industry development with initiatives like La French Tech.\n\nGoogle has already helped over 230,000 French students improve their digital skills and is building four \u201cLes Ateliers Num\u00e9riques\u201d Google Hubs to provide free digital training to the French public. The company is adding 1,000 employees to its sprawling Paris office.\n\nGoogle\u2019s AI research subsidiary, London-based DeepMind said it will open a Paris lab with 15 researchers led by vernacular AI scientist Remis Munos.\n\nFacebook says it will put \u20ac10 million into its French research center over the next five years and double the number of AI research scientists to 100 by 2022. This is Facebook\u2019s biggest investment in France since the Station F Startup Campus in Paris.\n\nKorean consumer electronics giant Samsung will open its third-biggest AI R&D center in Paris. Led by former Apple Siri Chief Luc Julia, the center will house 100 researchers. Corporate President and CSO Young Sohn said today that Samsung sees France\u2019s strong competitive edge in mathematics and physics as a fertile environment for developing AI talents.\n\nJapan\u2019s Fujitsu is intensifying the current expansion of its Paris AI center, pledging US$61 million over the next five years, while Microsoft is spending US$30 million to open an AI school in France.\n\nPresident Macron sent out 32 tweets from the event under the hashtag #AIforhumanity and #ChooseFrance."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-to-connect-10-billion-devices-in-5-years-d2b2d6a678be?source=user_profile---------51----------------",
        "title": "Alibaba to Connect 10 Billion Devices in 5 Years \u2013 SyncedReview \u2013",
        "text": "In a world of increasingly connected humans, the new frontier is devices and the Internet of Things (IoT). Alibaba Cloud wants to secure its position in this rapidly growing market by connecting ten billion devices over the next five years.\n\nPresident of Alibaba Cloud Computing and Corporate Senior Vice-President Hu Xiaoming announced the ambitious plan today at the Computing Conference Summit in Shenzhen, where he compared AI to the human brain, cloud computing to the heart, and the IoT to the sensorium.\n\n\u201cThe Internet\u2019s first phase was digitizing human activities such as shopping, social, and entertainment. This formed the booming market of four billion Internet users,\u201d explains Hu. \u201cThe second half is digitizing the physical world of cars, forests, rivers, factories\u2026 Even trash cans are being connected to the internet! And this will be a profound technological change, a new productivity revolution.\u201d\n\nMarket research firm IDC predicts that by 2020 there will be 50 billion networked terminal devices in the world, storing about 50 percent of all data. Alibaba has been vying for the IoT market since all the way back in 2014 when it formed its Smart Living Group. In 2017 the company fast-tracked IoT development with a dedicated IoT Group, partnering with 200 companies to develop the industry standard ICA Alliance.\n\nIn 2017 August development of the first IoT-dedicated town, Hong\u2019Shan Township, began in partnership with the Wuxi Gaoxin District Government. The town has sensors tracking fire hazards and water pipe leakage, a system that auto adjusts and saves energy on street lamps, and advertising billboards that can flash warnings in case of emergency.\n\nAlibaba Cloud has also signed contracts with Suzhou City on an \u201ceconomic brain\u201d project, Chongqing city on public transportation projects, Suzhou and Shanghai for real-estate management.\n\nAnother component of Alibaba\u2019s IoT strategy are Internet cars powered by its AliOS operating system. There are 500,000 SAIC Motor, Dongfeng Peugeot-Citro\u00ebn, and Ford automobiles on the road connected with AliOS.\n\nAlibaba Cloud IoT says it will focus on providing an open, convenient IoT platform augmented with strong AI capabilities, while at the same streamlining the collaborative computing of cloud, edge, and end devices."
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-unveils-ai-computing-beasts-worlds-largest-gpu-two-petaflop-supercomputer-65dca0cbaf59?source=user_profile---------52----------------",
        "title": "NVIDIA Unveils AI Computing Beasts: World\u2019s Largest GPU & Two Petaflop Supercomputer",
        "text": "Five years ago a University of Toronto team led by Dr. Geoffrey Hinton used two GPUs to train the image recognition model AlexNet in a record time of six days \u2014 and GPUs have been powering AI research ever since. However, as researchers take on more challenging tasks, they need more compute power. NVIDIA Founder and CEO Jensen Huang believes he has the answer: \u201cThe world needs a gigantic GPU.\u201d\n\n \n\n At the GPU Technology Conference in Santa Clara, USA today, Huang unveiled the world\u2019s largest GPU \u2014 a binary beast packed with 16 Tesla V100 with doubled memory 32 GB, 81920 CUDA Cores, 2,000 TFLOPS Tensor Cores, and a bandwidth of 300 GB/Seconds between each GPUs.\n\n \n\n It\u2019s like looking under the hood of a muscle car. And it\u2019s an AI researcher\u2019s dream.\n\n \n\n Incorporating 16 GPUs in a single machine raised huge technical challenges in GPU interconnectivity. NVIDIA developed a new GPU interconnect fabric, NVSwitch, an upgrade on NVIDIA NVLink that delivers bandwidth five times higher than the best PCIe switch, enabling systems with higher GPU hyperconnectivity.\n\n \n\n NVIDIA announced that its gigantic GPU is now integrated into the DGX-2, the company\u2019s latest supercomputer for offices and data centers. DGX-2 is the world\u2019s first system to deliver performance of two PFLOPS, has 512GB HBM2 of memory, energy consumption of 10,000 watts, and 1.5TB system memory. The DGX-2 can train AlexNet in just 18 minutes, 500 times faster than the Hinton team in 2012.\n\n \n\n The DGX-2 is aimed at general academic institutions or established enterprises who demand substantial computing in AI research. It will go on sale in quarter three for US$399,000. Huang joked in his keynote speech: \u201cThe more GPUs you buy, the more money you save!\u201d\n\n \n\n The release of new GPU and DGX-2 is expected to consolidate NVIDIA\u2019s data center business, which doubled to US$2 billion in annual revenue in 2017 to become the company\u2019s second largest revenue source. Last December, NVIDIA controversially prohibited the deployment of its consumer-side GPU GeForce series in data centers. This was believed to be a measure to defend the company\u2019s own data center business. \n\n \n\n Also announced today was Clara, a datacenter medical imaging supercomputer for researchers to train models on reconstructing 3D images, detecting brain tumors, and cinematic rendering.\n\n \n\n NVIDIA enhanced its cloud platform NVIDIA GPU Cloud with the release of TensorRT 4.0, the company\u2019s latest high-performance deep learning inference optimizer. TensorRT 4.0 can accelerate AI applications, such as image recognition, speech synthesis, and natural language processing, and reduce data center power consumption by 70%. It incorporates with today\u2019s most widely used AI open source framework, Google TensorFlow 1.7.\n\n \n\n The NVIDIA GPU Cloud also added Kubernetes, a portable, extensible open-source platform for managing containerized workloads and services. Launched by Google in 2014, Kubernetes can help the NVIDIA GPU Cloud manage computing resources, particularly data centers on the cloud, in a cluster orchestration. This enables portability across infrastructure providers.\n\n \n\n Amazon Web Services, Google Cloud Platform, AliCloud, and Oracle Cloud users can access the NVIDIA GPU Cloud. \n\n \n\n NVIDIA is sending a message to the AI community: Its \u201cgigantic GPU\u201d will save researchers time training AI models so they can put more time into AI innovation. While Intel and Google have been catching up in the AI computing market in recent years, NVIDIA\u2019s new product announcements are expected to ramp up its market share in the critical data center business and dramatically expand its influence on the cloud."
    },
    {
        "url": "https://medium.com/syncedreview/the-yolov3-object-detection-network-is-fast-fcceae0ab650?source=user_profile---------53----------------",
        "title": "The YOLOv3 Object Detection Network Is Fast! \u2013 SyncedReview \u2013",
        "text": "YOLO creators Joseph Redmon and Ali Farhadi from the University of Washington on March 25 released YOLOv3, an upgraded version of their fast object detection network, now available on Github.\n\nAt 320 x 320, YOLOv3 runs in 22 ms at 28.2 mAP, as accurate but three times faster than SSD. It also runs almost four times faster than RetinaNet, achieving 57.9 AP50 in 51 ms on a Pascal Titan X.\n\nThe first generation of YOLO was published on arXiv in June 2015. The model framed objects separated by bounding boxes and associated class probabilities to treat them as a regression problem. A base YOLO model could detect images in real-time at 45 frames per second, while Fast YOLO was capable of processing 155 frames per second, while still outperforming other real-time detectors.\n\nIn 2016 Redmon and Farhadi developed YOLO9000, which could detect up to 9,000 object categories using the improved YOLOv2 model. At 67 frames per second, the detector scored 76.8 mAP on the visual object classes challenge VOOC 2007, beating methods such as Faster RCNN. The model was also trained to detect unlabelled objects.\n\nThe new YOLOv3 follows on YOLO9000\u2019s methodology and predicts bounding boxes using dimension clusters as anchor boxes. It then guesses an objectness score for each bounding box using logistic regression. The model next predicts boxes at three different scales, extracting features from these scales using a similar concept to feature pyramid networks. Redmon uses a hybrid approach to perform feature extraction, building on former YOLOv2, Darknet-19 and residual networks. The new network, Darketnet-53, is significantly larger and has 53 convolutional layers.\n\nWhen the duo ran YOLOv3 on Microsoft\u2019s COCO Dataset it performed on par with RetinaNet and SSD variants, indicating the model\u2019s strength at fitting boxes to objects. However when the IOU threshold raises the model struggles to align boxes perfectly with objects. Redmon and Farhadi say the model does not work well on average AP between 0.5 and 0.95 IOU metric, but performs very well on a threshold metric of 0.5 IOU. It also performs better with small objects than with large objects.\n\nOn a side note, it\u2019s worth mentioning that Redmon and Farhadi\u2019s paper is not only a step forward in object detection, it\u2019s also peppered with humour. Andrej Karpathy retweeted that the paper \u201creads like good stand up comedy.\u201d\n\nAli Farhadi is the Associate Professor of Computer Science and Engineering at the University of Washington. He also leads Project Plato \u2014 which uses computer vision to extracting visual knowledge \u2014 at the Allen Institute of Artificial Intelligence. His student Joseph Redmon is the YOLO paper\u2019s first author. Redman\u2019s personal website is called Survival Strategies for the Robot Rebellion."
    },
    {
        "url": "https://medium.com/syncedreview/crown-prince-sheikh-hamdan-launches-new-round-of-ai-programmes-in-dubai-967c41e36dae?source=user_profile---------54----------------",
        "title": "Crown Prince Sheikh Hamdan Launches New Round of AI Programmes in Dubai",
        "text": "Crown Prince and Chairman of the Board of Trustees of the Dubai Future Foundation Sheikh Hamdan officially opened the 4th edition of the Dubai Future Accelerator (DFA) on March 24th at Dubai\u2019s Emirates Towers. The program aims to boost tech development across the United Arab Emirates.\n\nIn 2016, Hamdan set a goal of achieving 25% autonomous transportation in the UAE by 2030, positioning the country of nine million as a regional reader in AI. This year\u2019s theme \u2014 Take Part in Creating the Future \u2014 matches 12 government entities with private sector partners. The teams will collaborate for nine weeks to solve public challenges using frontier technologies.\n\nThe DFA\u2019s AI deployment is both varied and wide. The Dubai Police will use statistical AI to support their decision-making processes, with a goal of cutting the crime rate by 25 percent by 2021. The Dubai Municipality will use AI and blockchain solutions to improve public infrastructure in areas of pest control, food safety, and health. Dubai\u2019s Department of Economic Development will use AI in optimizing control and inspection produces.\n\nEtisalat Telecommunications will adopt AR and AI to streamline operations and customer support, with the target of 90 percent service automation by 2021. Competitor Du telecommunications meanwhile believes AI and machine learning can upgrade its corporate support services by 30 percent.\n\nOther government departments such as the Dubai Electricity and Water Authority, Smart Dubai, the General Directorate of Residency and Foreigners Affairs, and the Knowledge and Human Development authorities will target challenges in urban environment, education, IoT operations, solar energy, and various explorational smart systems.\n\nThe UAE will host the Middle East\u2019s biggest AI fair this year. \u201cWorld AI Show\u201d will run April 11\u201312 in Dubai before moving to Singapore, Mumbai, and Paris. The AI market in the United Arab Emirates is expected to reach $50 billion by 2025.\n\nLast year, 27-year-old Emirati Omar bin Sultan Al-Olama was named the world\u2019s first-ever Minister of Artificial Intelligence. Al-Olama\u2019s previous contributions to his nation\u2019s top-level strategies include UAE Centennial 2071 and the UAE Strategy for the Fourth Industrial Revolution. In a CNBC interview Al Olama said \u201cIn ten years we will be the capital of AI in service and government. I also think we will be a hub for AI in the region.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/tracking-the-transforming-ai-chip-market-bac117359459?source=user_profile---------55----------------",
        "title": "Tracking the Transforming AI Chip Market \u2013 SyncedReview \u2013",
        "text": "Embedded AI can transform a tabletop speaker into a personal assistant; give a robot brains and dexterity; and turn a smartphone into a smart camera, music player, or game console. Traditional processors, however, lack the computational power to support many of these intelligent features. Chipmakers, startups, and capital are taking this opportunity to the market.\n\nAccording to a Gartner report, the chip market\u2019s total revenue hit US$400 billion in 2017, and the figure is expected to exceed US$459 billion in 2018. Traditional chip makers are putting an increasing focus on AI chip development, venture capital is pumping significant investments into the market, and AI chip startups are emerging.\n\nCPU (Central Processing Units) are a chip designed for general computing purpose, emphasizing calculation and logic control functions. They are strong in processing single complex computing sequential tasks, but poor in large-scale data computation.[2]\n\nGPU (Graphics Processing Units) were originally designed for image processing but have been successfully adopted for AI. A GPU contains thousands of cores and is capable of processing thousands of threads simultaneously. This parallel computing design makes GPU extremely powerful in large-scale data computation.[3]\n\nFPGA (Field Programmable Gate Arrays) are programmable logic chips. This type of processor is powerful in processing small-scale but intensive data access. In addition, FPGA chips allows users to program the circuit path through its tiny logic block, to handle any kind of digital function.[2] [4]\n\nASIC (Application-Specific Integrated Circuit) are highly customized chips tailored to provide superior performance in specific applications. However, a customized ASIC is not alterable once put into production.[5]\n\nOthers chip types such as Neuromorphic Processing Units (NPU) \u2014 which have architecture mimicking that of the human brain \u2014 have the potential to become mainstream in the future but are still at early stages of development.\n\nAI Chips, also known as AI accelerators, are processors for AI-related computing tasks. Machine learning technology places great demands on computing power for training algorithms and running applications, which traditional computing hardware cannot provide. As a result the demand for specialized AI chips is growing rapidly. [6] AI Chips can be divided into three major application areas: training, inference on the cloud, and inference on edge devices.\n\nTraining is a process wherein algorithms analyze data, learn from it and finally obtain the intelligence to respond to real-world events. Trillions of data samples are analyzed by the algorithm during this training process. Chip makers must not only ramp up processor performance, but also provide an entire ecosystem \u2014 including hardware, framework and other supportive tools \u2014 to enable developers to shorten their AI technology development processes [6]. Given these challenges, it\u2019s major companies like NVIDIA and Google who are thriving in this space.\n\nNVIDIA is the leader in training. When developers discovered GPU\u2019s parallel computing architecture could accelerate the deep learning training process this brought a significant advantage to GPU giant NVIDIA. By seizing the opportunity, NVIDIA transformed itself into an AI computing company and developed a new GPU architecture, Volta, which emphasizes deep learning acceleration. NVIDIA \u2018s GPU have been widely adopted for training machine learning algorithms, and the company now holds a virtual monopoly in the hardware training market.\n\nGoogle is another big player in this market. Based on the achievements of AlphaGo and the millions of users on its cloud service, Google has strong potential in the training market. The company has developed its own TPU (Tensor Processing Units) to compete with NVIDIA. TPUs are a type of ASIC designed exclusively for deep learning and Google\u2019s TensorFlow framework. Google says its TPU can provide 180 teraflops of floating-point performance, which is six times better than NVIDIA\u2019s latest data center GPU Tesla V100. [7] [8]\n\nA developed machine learning model for AI application areas such as image recognition or machine translation usually comes with high complexity, and the required inference is too compute-intensive to be deployed on edge devices. Therefore, inference on cloud becomes necessary for the deployment of many AI applications. And when an app is being used by thousands of people simultaneously, the cloud server also requires robust capability to meet inference demands. In such cases, FPGAs are the top choice for cloud companies. [9] This type of processor is good at low-latency streaming and computing-intensive tasks. In addition, FPGAs provide a flexibility which allows cloud companies to modify the chips. Traditional chip makers, cloud service providers, and startups are all developing FPGA solutions.\n\nIntel is one of the major players developing heterogeneous computing technology. By acquiring chip maker Altera, Intel boosted its FPGA technology expertise and developed a CPU+FPGA hybrid chip for deep learning inference on cloud. By utilizing the advantages of both processor types, this hybrid chip provides computing power, high memory bandwidth, and low latency. This technology has been adopted by Microsoft to accelerate its Azure Cloud Service.\n\nChinese tech giant Tencent is an example of cloud service providers developing FPGA solutions to support inference on cloud. Tencent developed China\u2019s first \u201cFPGA Cloud Computing\u201d service for its cloud service Cloud Virtual Machine. Compared to a CPU-based cloud server, the FPGA integrated CVM provides better computing power to support HPC application and deep learning development. [6] Accessing FPGA on cloud also eliminates the need to purchase hardware, reducing the cost of developing AI application. Tencent also supports third-party AI application development for commercial use.\n\nDeePhi Tech is a startup focused on inference on cloud. The company garnered US$40 million in funding to develop its DPU (Deep-Learning Processing Units, an FPGA based ASIC) platform. With the DNNDK (Deep Neural Network Development Kit), DeePhi Tech aims to provide a one-stop service for development and deployment of deep learning technologies. DeePhi co-founder Dr. Song Hang is a respected AI researcher who proposed a methodology called \u201cDeep Compression\u201d to reduce model scale, workload and power consumption in order to improve deep learning efficiency. [10] This methodology has been adopted by chip giants such as Intel and NVIDIA.\n\nInternet connections may not always be stable, and the cloud cannot accommodate all computing loads for AI innovations. Therefore future edge devices will require more independence in their inference features. Smartphones, drones, robots, VR and AR immersive experience devices, self-driving cars and so on all require specific AI hardware support. Moreover, breakthroughs in recent years have reduced chip volume, enabling embedding on almost any device, making inference on edge more viable.[11] To meet the demand for different devices, numerous startups are producing their own ASIC. Large chip makers are also adding AI supportive features to their processors.\n\nLeading Chinese phone and processor producer Huawei is boosting the performance of their SoC by integrating AI chips. In collaboration with chip startup Cambricon, Huawei adopted a NPU (Neural Processing Unit, a type of ASIC from Cambricon) to advance its SoC Kirin 970 for its flagship smartphone Mate 10. [12] This integration enhances the the phone\u2019s camera\u2019s image processing features.\n\nChinese startup WestWell Lab\u2019s DeepSouth neural processors are ASIC which simulate human brain neurons. The company created a DeepSouth-based brain simulator that can be used to accelerate medical devices supporting research in Parkinson\u2019s, Alzheimer\u2019s, and neural impairment.\n\nHorizon Robotics is another startup concentrating on embedded artificial intelligence. The company has developed two types of ASIC to support different AI applications. Sunrise series processors are for face recognition and video analytics solutions in smart cameras. Journey series processors are for self-driving cars, and provide real-time detection and recognition processing capacity in eight categories.[13]\n\nAI is far from maturity, and as the AI innovation ecosystem continues to develop the chip market will fluctuate. With the possibility of new frameworks emerging for algorithm development, current leaders in the training hardware market may face new competition. The inference on the cloud market is also still growing, and competition between cloud service providers will intensify as more AI applications are developed. The inference on edge market meanwhile is an arena with both big companies and startups.\n\nGiven the ever-increasing demands of AI applications, we can expect to see more collaborations between chipmakers and developers. Artificial intelligence has already had a significant impact on the chip market, a trend that will continue into the foreseeable future.\n\n[1] Chip market to top $400 billion in 2017, says Gartner: http://www.eenewseurope.com/news/chip-market-top-400-billion-2017-says-gartner-0\n\n[2] \u8be6\u7ec6\u5206\u6790\u4eba\u5de5\u667a\u80fd\u82af\u7247 CPU/GPU/FPGA\u6709\u4f55\u5dee\u5f02?: http://www.sohu.com/a/131606094_470053\n\n[3] What\u2019s the Difference Between a CPU and a GPU?: https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/\n\n[4] Difference Between FPGA and CPLD: http://www.differencebetween.net/technology/difference-between-fpga-and-cpld/\n\n[5] ASIC and SoC: https://www.eetimes.com/author.asp?doc_id=1285201\n\n[6] \u4e00\u6587\u770b\u61c2\u4eba\u5de5\u667a\u80fd\u82af\u7247\u7684\u4ea7\u4e1a\u751f\u6001\u53ca\u7ade\u4e89\u683c\u5c40: https://www.leiphone.com/news/201709/uuJFzAxdoBY7bzEL.html\n\n[7] CPUs, GPUs, and Now AI Chips: http://www.electronicdesign.com/industrial/cpus-gpus-and-now-ai-chips\n\n[8] Quantifying the performance of the TPU, our first machine learning chip: https://cloudplatform.googleblog.com/2017/04/quantifying-the-performance-of-the-TPU-our-first-machine-learning-chip.html\n\n[9] \u6df1\u5ea6\u5b66\u4e60\u7684\u4e09\u79cd\u786c\u4ef6\u65b9\u6848\uff1aASIC\uff0cFPGA\uff0cGPU\uff1b\u4f60\u66f4\u770b\u597d\uff1f: http://www.sohu.com/a/123176776_463982\n\n[10] \u65af\u5766\u798f\u535a\u58eb\u97e9\u677e\u6bd5\u4e1a\u8bba\u6587\uff1a\u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u6548\u65b9\u6cd5\u4e0e\u786c\u4ef6: https://zhuanlan.zhihu.com/p/30211134\n\n[11] A brief guide to mobile AI chips: https://www.theverge.com/2017/10/19/16502538/mobile-ai-chips-apple-google-huawei-qualcomm\n\n[12] Huawei unveils Kirin 970 chipset with AI: http://www.zdnet.com/article/huawei-unveils-kirin-970-chipset-with-ai/\n\n[13] \u60f3\u6210\u4e3aAI\u9886\u57df\u7684\u82f1\u7279\u5c14\uff0c\u5730\u5e73\u7ebf\u53d1\u5e03\u4e24\u6b3e\u7ec8\u7aef\u89c6\u89c9\u82af\u7247: https://www.jiqizhixin.com/articles/2017-12-21-4\n\n[14] NVIDIA launched Volta GPU computing architecture to bring speed in AI inference and training, as well as for accelerating HPC and graphics workloads.: https://nvidianews.nvidia.com/news/nvidia-launches-revolutionary-volta-gpu-platform-fueling-next-era-of-ai-and-high-performance-computing\n\n[15] Google introduced its TPU (Tensor Processing Units) that accelerates the TensorFlow framework in machine learning.: https://cloud.google.com/tpu/\n\n[16] IBM and U.S. AFRL announced the collaboration on a brain-inspired supercomputing system.: https://www-03.ibm.com/press/us/en/pressrelease/52657.wss\n\n[17] Microsoft is working on AI chips across its different devices, top exec says: https://www.cnbc.com/2017/11/01/microsoft-working-on-ai-chips-across-different-devices-top-exec-says.html\n\n[18] Huawei launched Kirin 970 \u2014 the new glagship SoC with AI capabilities: https://www.androidauthority.com/huawei-announces-kirin-970-797788/\n\n[19] Intel is buying Movidius, a startup that makes vision chips for drones and virtual reality: https://www.recode.net/2016/9/6/12810246/intel-buying-movidius\n\n[20] Report: Amazon working on its own AI chips for Echo devices: https://mashable.com/2018/02/12/amazon-echo-ai-chip/#VFOj98mEfqqy\n\n[21] CB Insight: www.cbinsights.com\n\n[22] Crunchbase: www.crunchbase.com\n\n[23] Hupogu: http://www.hupogu.com"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-mar-w-2-mar-w-3-7c5e00204f5d?source=user_profile---------56----------------",
        "title": "AI Biweekly: 10 Bits from Mar W 2 \u2014 Mar W 3 \u2013 SyncedReview \u2013",
        "text": "March 6th \u2014 Google Builds AI Drone Image Analysis System for the Pentagon\n\nGoogle is collaborating with the United States Department of Defense to create an AI system for identifying drone footage. The system will leverage machine learning and computer vision technologies to collect and categorize drone footage more efficiently.\n\nMarch 7th \u2014 Microsoft Announces an AI Platform for Developers in Windows 10\n\nMicrosoft is adding a new AI platform to Windows 10 which includes or enables various AI-based applications such as Photos app for intelligent video creation, Hello app for face recognition log in, and voice assistant Cortana for question answering.\n\nMarch 7th \u2014 Microsoft and Intel Enable Deep Learning Inference Vision Processing Units (VPU)\n\nMicrosoft announces Intel\u2019s Movidius Myriad X VPU hardware will be included in the Windows ML server. Windows ML can pre-train machine learning data models and process developers\u2019 own deep learning tasks within the OS interface. With the added power of Intel\u2019s VPU, Windows ML will be able to run specific deep neural network AI tasks at higher speeds while using less power.\n\nMarch 9th \u2014 Telenor Enhances AI and IoT Laboratory Research\n\nTelenor has opened a new IoT laboratory research unit in Trondheim, Norway. The company will invest over \u20ac5.2 million in the new lab, which will mainly collaborate with its existing AI lab on Narrowband IoT technology research and developer portal enhancement.\n\nMarch 9th \u2014 Waymo and Google Launch an Autonomous Truck Pilot Project in Atlanta\n\nWaymo and Google announce a pilot project for self-driving truck testing in Atlanta, USA. During this project the team will not only test the capabilities of their autonomous truck but also the automated logistics in allocating loads, connecting shippers, factories, and distribution centres.\n\nMarch 13th \u2014 Google Makes Music with Machine Learning\n\nUnder its Magenta project, Google introduces its NSynth (Neural Synthesizer), which uses machine learning algorithms and deep neural networks to create new sounds for electronic music. The prototype can create over 100,000 sounds based on only 16 source inputs.\n\nMarch 14th \u2014 Huawei Introduces Blockchain Stress Test Project\n\nHuawei introduces its Caliper Project, a method for testing blockchain stress performance which can help developers and engineers evaluate their blockchain applications in a controlled environment. Performance results may include success rate of the transaction, speed of transaction, and hardware resource performance.\n\nMarch 14th \u2014 Alexa Now Able to Play AI-Generated Songs\n\nAlexa\u2019s latest skill is the ability to play AI-generated songs. The new DeepMusic feature uses a deep recurrent neural network to collect music samples, and leverages an algorithm to create melodies and songs without human input.\n\nMarch 15th \u2014 Microsoft\u2019s AI Translation System Performs at Human Expert Level\n\nMicrosoft builds a Chinese to English translation system with natural language processing and machine learning techniques that can translate Mandarin to English at a level matching human translators. Microsoft based its results on blind comparisons by bilingual humans between its system\u2019s outputs and those of human translators.\n\nMarch 15th \u2014 Google Ventures Invests in AI Chip Start-Up Sambanova\n\nGoogle Ventures\u2019 parent company Alphabet pours US$56 million in Series A funding into startup SambaNova. SambaNova builds computer chipsets and applications for artificial intelligence and data analytics. SambaNova CEO Rodrigo Liang says the company is also interested in public cloud services."
    },
    {
        "url": "https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7?source=user_profile---------57----------------",
        "title": "Facebook AI Proposes Group Normalization Alternative to Batch Normalization",
        "text": "As Facebook struggles with fallout from the Cambridge Analytica scandal, its research arm today delivered a welcome bit of good news in deep learning. Research Engineer Dr. Yuxin Wu and Research Scientist Dr. Kaiming He proposed a new Group Normalization (GN) technique they say can accelerate deep neural network training with small batch sizes.\n\nAlthough deep learning thrives with complex neural networks and large datasets, training a model requires much time and power. This has prompted AI researchers to rethink the normalization techniques they use to reduce training costs.\n\nFacebook AI Research had already taken a few steps forward. Last June, it proposed an accurate, large minibatch SGD technique that can train ResNet50 with a minibatch size of 8192 on 256 GPUs in only one hour, while matching small minibatch accuracy.\n\nThe mainstream normalization technique for almost all convolutional neural networks today is Batch Normalization (BN), which has been widely adopted in the development of deep learning. Proposed by Google in 2015, BN can not only accelerate a model\u2019s converging speed, but also alleviate problems such as Gradient Dispersion in the deep neural network, making it easier to train models.\n\nDr. Wu and Dr. He however argue in their paper Group Normalization that normalizing with batch size has limitations, as BN cannot ensure the model accuracy rate when the batch size becomes smaller. As a result, researchers today are normalizing with large batches, which is very memory intensive, and are avoiding using limited memory to explore higher-capacity models.\n\nDr. Wu and Dr. He believe their new GN technique is a simple but effective alternative to BN. Specifically, GN divides channels \u2014 also referred to as feature maps that look like 3D chunks of data \u2014 into groups and normalizes the features within each group. GN only exploits the layer dimensions, and its computation is independent of batch sizes.\n\nThe idea of GN was inspired by many classical image features like SIFT and HOG, which involve group-wise normalization. The paper states, \u201cFor example, a HOG vector is the outcome of several spatial cells where each cell is represented by a normalized orientation histogram.\u201d\n\nThe paper reports that GN had a 10.6% lower error rate than its BN counterpart for ResNet-50 in ImageNet with a batch size of 2 samples; and matched BN performance while outperforming other normalization techniques with a regular batch size. It is worth noting that Dr. He is the main contributor to the development of ResNet (Deep Residual Network).\n\nGN also outperformed BN on other neural networks, such as Mask R-CNN for COCO object detection and segmentation, and 3D convolutional networks for Kinetics video classification.\n\nGN is not the first attempt to replace BN. Layer Normalization (LN), proposed in 2016 by a University of Toronto team led by Dr. Geoffrey Hinton; and Instance Normalization (IN), proposed by Russian and UK researchers, are also alternatives for normalizing batch dimensions. While LN and IN are effective for training sequential models such as RNN/LSTM or generative models such as GANs, GN appears to present a better result in visual recognition."
    },
    {
        "url": "https://medium.com/syncedreview/mapbox-is-mapping-the-future-9d4cf4753bea?source=user_profile---------58----------------",
        "title": "Mapbox Is Mapping the Future \u2013 SyncedReview \u2013",
        "text": "When Google Maps debuted in 2005 the technology amazed users, who could locate an address and even visit the spot using archived \u201cStreet View\u201d captures. The next generation of digital maps for autonomous driving and augmented reality (AR) will require much more \u2014 for example that the states and positions of rendered real world objects be both precise and up-to-the-minute.\n\nThese new challenges and opportunities have prompted mapping companies to reimagine digital mapping technology.\n\nMapbox, a San Francisco-based digital mapping startup, released its Mapbox AR at the Mobile World Conference 2018 in Barcelona. The AR platform comprises a suite of tools, a low-level AR core kit, and a framework. Users can access interactive maps with animated dynamic directions on mobile devices.\n\nFounded in 2011, Mapbox offers clients development tools for mapping their apps and website, including a featured Photoshop-like product, Mapbox Studio, that enables developers to access Mapbox\u2019s visualization layers and make maps with their own data. Mapbox business model focuses on visualizing for example high-density weather radar information; delivering business analytics; and logistics.\n\nWhile Google is unquestionably a dominant player in the digital mapping industry, Mapbox is carving out a niche. Their Road Network collects anonymous data information from 300 million users of apps such as Airbnb, Instacart, Snap, and others with embedded Mapbox SDK. This data can extrapolate detailed patterns, for example, of real-time traffic and environmental status.\n\nDave Cole, a Mapbox founding member and VP of Business Operations and Strategy told Synced that data is the key to his company\u2019s success. \u201cWe have a network now that\u2019s collecting over 220 million miles of anonymous data every single day that goes back into the map and it gets better.\u201d\n\nMapping for autonomous driving is a high-growth sector estimated to open up a US$20 billion market by 2050, according to Goldman Sachs. While breakthroughs in self-driving technologies have thus far come from sensors such as LiDars and Cameras, the important role of maps cannot be overlooked. In the future, maps may even become more important than sensors.\n\n\u201cFor example, when driving on a multi-lane road, the onboard sensor may be unable to detect the road after a turn because of road-side obstructions. Once you have the lane-level positioning and real-time road updates enabled by HD maps, you can slow down and switch to a safe lane in advance to prevent an accident,\u201d says Xudong Cao, Founder and CEO of China\u2019s self-driving startup Momenta.ai.\n\nMapping companies are vying for the vanguard in autonomous driving integration. Dutch mapping firm TomTom is a leader in HD map production, and has attracted investments and partnerships deals from Ford, GM, Fiat Chrysler.\n\nMapbox made its initial move into autonomous driving in 2016 with Mapbox Drive \u2014 now Mapbox Automotive \u2014 a package of developer tools that enabled automakers to customize HD maps and turn-by-turn navigation. Developers can access API offered by Mapbox Automotive and build out specific embedded dash navigation, as well as 3D rendered visualization, to give drivers enhanced environmental context.\n\nMapbox Automotive has also built a Global Lane Network that aggregates sensor data collected by cameras, LiDars and GPS from other self-driving partners and distributes the information.\n\n\u201cThink about a lot of autonomous companies that have invested in computer vision or LiDar. They are generating all this data on the vehicles, but they need a network to actually distribute it back so as one vehicle collects, they can refine the map and make it available for general use for the whole fleet of vehicles,\u201d says Cole.\n\nTo maintain a map that accurately keeps up with the world as it changes, Mapbox researchers are leveraging AI technologies for processing data and turning it into, for example, predictive traffic profiles based on density of people; and using Receptive Field Networks (RFNet) or You-Only-Look-Once (YOLO) algorithms for segmentation and object recognition.\n\nLast year Mapbox purchased Mapdata, an AI mapping startup that uses deep neural networks to improve computer vision and AR. The Belarus-based company is now a satellite R&D team devoted to merging front-facing cameras and navigation.\n\nMapbox has a team of 350 and has thus far accumulated investments of US$227 million. As of February 2018, the number of registered developers on the Mapbox platform topped one million, a milestone for a technology platform.\n\nWhile 60 percent of its business is with US developers, Mapbox is broadening its global operations, particularly in China. This month the former Head of Uber Business Development in Asia-Pacific (APAC) Andy Lee joined Mapbox to lead its APAC expansion.\n\nLee told Synced that because Mapbox is already partnered with Chinese companies such as Alibaba\u2019s trip service Feizhu, it is in a position to help Western companies enter China, and its Chinese partners go global.\n\n\u201cWhat we have done is helpful for many companies who are trying to figure out how to enter China. Building maps in China is a challenge of being able to deliver experiences on a very large scale, not just like hundreds of thousands but sometimes hundreds of millions of users on a daily basis. There are very interesting engineering problems,\u201d says Lee.\n\nWhen Cole spoke to us he shared a story about his youthful hobby of sketching building and drawing maps. Coincidentally, the young Lee loved to carry paper maps his mother gave him and study streets \u201cin a very nerdy way.\u201d\n\n\u201cThe experience has totally changed now because I have a smartphone with a map and travel or dining or lifestyle apps. My early childhood interest in maps is my career now, which is helping developers continue that journey, and that gets me really excited,\u201d says Lee."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-begin-testing-self-driving-cars-on-beijing-roads-c3dd9dd242c0?source=user_profile---------59----------------",
        "title": "Baidu to Begin Testing Self-Driving Cars on Beijing Roads",
        "text": "Uber\u2019s fatal self-driving car accident continues to send repercussions through the autonomous vehicle industry, and has fomented public doubt concerning the technology\u2019s safety. In response, Uber halted all its road tests for self-driving cars. And so it came as somewhat unexpected news when on March 22nd the Beijing Municipal Government gave tech unicorn Baidu the green light to test driverless cars on the city\u2019s public roads.\n\nBaidu is the first company to be granted special license plates for autonomous vehicle testing on public roads in the Chinese capital.\n\nIn December 2017 Beijing issued China\u2019s first self-driving vehicle policies in the official statements \u201cGuiding Opinions on Accelerating the Road Test of Autonomous Vehicles (Test Version)\u201d and \u201cAutonomous Driving Vehicle Road Test Management Regulations (Test Version), which detailed basic thresholds for testing autonomous vehicles on public roads, mandating for example that test vehicles must not drive on public roads, carry US$800k in insurance, be assessed and approved by the municipality\u2019s management agency at closed test sites, exclude passengers that are not testing engineers, and that no more than five vehicles can participate in a single test.\n\nSubsequent regulations such as \u201cStandards and Methods for Assessing Autonomous Vehicles\u2019 Road Test Capability (Test Version),\u201d \u201cRoad Test Requirements for Autonomous Vehicles at Closed Test Sites,\u201d and \u201cRoad Prerequisites for Autonomous Vehicles Road Tests\u201d introduced more detailed clauses.\n\nThe safety regulations in these documents state that autonomous vehicles tested on public routes must complete 5,000km training at designated closed testing sites, be able to comply with traffic regulations, and have a sound response plan prepared in case of emergency. Test engineers must have over three years driving experience, complete 50 hours of operational training, and be ready to take control of the vehicle at any time. Unless specified, testing vehicles are required to avoid peak traffic times and bad weather. Tests can only be conducted on routes outside Beijing\u2019s 5th ring road.\n\nBaidu is China\u2019s leading tech company in the self-driving space. In February it released the self-driving dataset ApolloSpace, which is about 10 times larger than any other existing open-source dataset. Baidu\u2019s platform Apollo grants developers access to a complete set of service solutions and open-source codes and enables software engineers to convert a Lincoln MKZ into a self-driving vehicle in about 48 hours. Apollo has also joined the UC Berkeley DeepDrive (BDD) Industry Consortium, led by Professor Darrell.\n\nAlthough Beijing is being relatively prudent in its approach to self-driving vehicles on public roads, the Baidu permits are a huge step forward for autonomous research. Shanghai, Chongqing, and Shenzhen are also making relevant policies in this space. Interested readers can follow up here: Global Survey of Autonomous Vehicle Regulations."
    },
    {
        "url": "https://medium.com/syncedreview/my-other-lawyer-is-a-robot-lawgeex-automates-contract-review-eef4e2247114?source=user_profile---------60----------------",
        "title": "\u201cMy Other Lawyer is a Robot\u201d \u2014 LawGeex Automates Contract Review",
        "text": "Artificial Intelligence is already helping doctors examine MRI scans, travel agents book flight tickets, and accountants keep books. But the legal industry has been somewhat stubborn when it comes to AI. \u201cThe assumption is that as long as it\u2019s read by humans it\u2019s fine, but there\u2019s no ground for this in any other industry in the world,\u201d says Shmuli Goldberg, VP of Marketing at LawGeex.\n\nFounded in 2014 by international business lawyer Noory Bechor and machine learning guru Ilan Admon, LawGeex is a Tel Aviv-based legal tech startup focused on contract review. It closed US$7 million in Series A funding last March, principally from Indeed.com owner Recruit Holdings, Lool Ventures, and LionBird. The company has a team of close to 50 in its Tel Aviv R&D units and New York sales office.\n\nThis February, LawGeex created a stir in the legal industry when its algorithms outperformed human lawyers at contract review in a study conducted by Stanford, Duke University, and the University of South California.\n\nResearchers asked 20 experienced lawyers and the AI to examine five previously unseen contracts composed of 153 legal clauses copied from standard Non-Disclosure Agreements (NDA). The AI used just 26 seconds to complete its analysis with an accuracy rate of 94 percent. Human lawyers meanwhile required nearly 92 minutes to achieve an average accuracy rate of 85 percent.\n\nA Fortune 1000 company can deal with up to 40,000 contracts annually. Many companies are dissatisfied with the existing organizational contract review process, which is an essential but laborious task. Confidentiality agreements, in particular, take a week or even longer to review.\n\nGoldberg says the LawGeex AI can cut the contract review cycle from a week or a month down to one hour, and reduce the time spent by a lawyer reviewing the contract from hours to minutes \u2014 adding that law firms which bill by the hour are not as interested in automated efficiency. LawGeex can also review NDAs, purchase orders, service agreements, software license, sales and other types of contracts with a consistency, says Goldberg, that tired office-trapped lawyers on a Friday evening cannot match.\n\nThe ideal LawGeex client is an in-house legal departments of medium to large-sized enterprises with over a thousand contracts per year. The client will first making clear their precedents and definition of terms. The AI, pre-trained to understand what they mean, will then identify pertinent elements in the contract and make sure these align with the client\u2019s specifications.\n\nThe default AI application for document review is natural language processing (NLP). However, LawGeex discovered that NLP performed poorly in contract review due to legalese. Although natural language can have multiple interpretations, legalese is specific and rigid.\n\nTo cope with this problem LawGeex developed two new AI algorithms. A Legalese Language Processing (LLP) algorithm first trains the neural network with as many contracts as possible. The AI learns to understand terms such as \u201cnon-compete\u201d and \u201cdisclosure,\u201d and identify terminology-relevant clauses in the contract. A Legalese Language Understanding (LLU) algorithm works on top of the LLP, translating the legalese into legal concepts to help the AI understand unfamiliar clauses.\n\nThe hardest part of building the process, says Goldberg, was finding half a million contracts as training datasets, as there are no standard learning or training sets in the legal world \u2014 such as ImageNet for computer vision or MNIST for handwritten digits. LawGeex enlisted the help of experienced lawyers from top US law firms and spent three years hunting for shared and disclosed contracts.\n\nGoldberg says what separates LawGeex from automated contract service providers such as Legalsifter and Kiva Systems is that \u201cthey are all based on the same premise, if I have thousands of contracts and want to find one thing, they are the best for that. Whereas our tool looks at one contract and answers the question \u2018can I sign this?\u2019 When someone comes to the lawyer, hands them the NDA and asks if he can sign this, our tool opens the NDA up, looks the potential hundreds of issues, highlights what\u2019s relevant and irrelevant and passes it onto the lawyer.\u201d\n\n\u201cThis isn\u2019t a new technology,\u201d says Goldberg, \u201cand we are not saying that AI is taking over the legal world, we are saying the opposite: AI has taken over the legal world.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/2017-turing-award-goes-to-computer-chip-pioneers-2c126b97c8e5?source=user_profile---------61----------------",
        "title": "2017 Turing Award Goes to Computer Chip Pioneers \u2013 SyncedReview \u2013",
        "text": "The Association for Computing Machinery (ACM) today announced John L. Hennessy, former Stanford University President and Chairman of the Board of Alphabet; and David A. Patterson, retired Professor at University of California, Berkeley, as winners of the 2017 Turing Award for their groundbreaking approach to computer architecture design and evaluation.\n\nThe Turing Award is the Nobel Prize of computer science, named after British mathematician Alan M. Turing, who laid the mathematical foundations for and defined the limits of modern computing. The award is sponsored by Google and carries a prize of US$1 million.\n\nHennessy and Patterson proposed a systematic, quantified approach to building faster and more energy-efficient Reduced Instruction Set Computer (RISC) microprocessors, which has been widely adopted by academia and industries. Today, RISC processors account for 99 percent of the more than 16 billion microprocessors produced each year and are used in smartphones, tablets and other embedded devices.\n\nHennessy and Patterson are also the co-authors of Computer Architecture: A Quantitative Approach, an influential 1990 textbook that has contributed to improved microprocessor design across the computer architecture community.\n\nIn the textbook the pair encourage architects to tailor their system designs to accommodate different memory and computing demands. The textbook also prompted considerations of energy consumption, heat dissipation, and off-chip communications, widening computer architecture\u2019s traditional focus from computational power alone.\n\n\u201cHennessy and Patterson\u2019s contributions to energy-efficient RISC-based processors have helped make possible the mobile and IoT revolutions. At the same time, their seminal textbook has advanced the pace of innovation across the industry over the past 25 years by influencing generations of engineers and computer designers,\u201d said ACM President Vicki L. Hanson.\n\nHennessy and Patterson have received numerous honours, including the ACM-IEEE CS Eckert-Mauchly Award and the IEEE John von Neumann Medal. They are fellows of the ACM and IEEE and members of the National Academy of Engineering and the National Academy of Sciences."
    },
    {
        "url": "https://medium.com/syncedreview/huawei-introduces-ai-development-board-hikey-970-763ac996b29a?source=user_profile---------62----------------",
        "title": "Huawei Introduces AI Development Board HiKey 970 \u2013 SyncedReview \u2013",
        "text": "At the Linaro Connect Hong Kong 2018 Developer Conference today, Chinese tech giant Huawei unveiled its latest development board, HiKey 970, which is dedicated to AI-powered devices and applications.\n\nHuawei\u2019s HiKey series are single board Linux computers that developers can use to write and test applications. They run on Huawei\u2019s smartphone SoC (system on a chip), and their development is supported by Linaro, an engineering organization that works on free and open-source software.\n\nHiKey 970 is based on Huawei\u2019s Kirin 970, the world\u2019s first AI processor for smartphones, with four ARM Cortex-A73 and four ARM Cortex-A53 cores, 6GB of LPDDR4 memory, the latest generation 12-core Mali G72MP12 graphic processors, and a Neural Processing Unit dedicated to AI acceleration. The Kirin 970 is up to 25 times faster and 50 times more energy efficient than traditional processors.\n\nHuawei has been a leader in the global telecommunications equipment market since 2012, and released the groundbreaking Kirin 970 SoC last September. The company\u2019s Kirin 970-equipped flagship smartphone Mate 10 is the first-ever mobile phone to ship with AI hardware.\n\nHiKey 970 supports a variety of interfaces including an AI stack, the Huawei HiAI computing architecture, and popular neural network frameworks. Developers can leverage HiKey 970 for easier and more efficient AI development in robots, smart cities, deep learning algorithms, etc.\n\nTo help free AI application developers from concerns regarding costs, distribution/promotion, and IP issues, Huawei improved HiKey 970\u2019s design and introduced features such as a multi-application model, support for machine learning frameworks, comprehensive documentation, rich and efficient APIs, a rapid-start source code, and so on.\n\nHiKey 970 will compete with the UK\u2019s Raspberry Pi boards in this market, and will go on sale in mid-April."
    },
    {
        "url": "https://medium.com/syncedreview/where-is-autonomous-driving-headed-after-the-fatal-uber-accident-in-arizona-130c893a35b6?source=user_profile---------63----------------",
        "title": "Where is Autonomous Driving Headed After the Fatal Uber Accident in Arizona?",
        "text": "An Uber self-driving SUV struck and killed a female pedestrian Sunday evening in Tempe, Arizona. The first known autonomous vehicle-related pedestrian death on a public road stunned the AI community and raised public concerns on autonomous driving safety.\n\nTempe police reported that the accident took place around 10 p.m. local time. The Uber vehicle was in autonomous mode, with a human safety driver at the wheel. The weather in Tempe Sunday night was clear and dry. The US National Transportation Safety Board (NTSB) said that it was joining the accident investigation.\n\nUber immediately suspended all its self-driving testing in North American cities. The company had launched its first self-driving road tests in Arizona only a few months ago.\n\nArizona is a hotbed of self-driving technologies and there are over 600 self-driving cars on the state\u2019s public roads. The state legislature has for years been cultivating an AV-friendly testing environment that rivals California\u2019s. Two weeks ago Arizona gave the green light for public road testing without human drivers in the vehicle.\n\nThere has been no official word yet on whether Arizona regulators will reconsider their relatively accommodating autonomous vehicle testing policies in the aftermath of Sunday\u2019s fatal accident.\n\nUber CEO Dara Khosrowshahi said Sunday\u2019s accident was \u201cincredibly sad news\u2026 We\u2019re thinking of the victim\u2019s family as we work with local law enforcement to understand what happened.\u201d\n\nIt\u2019s been a yearlong series of scandals for Uber, including a sexual harassment class action suit, the former CEO\u2019s departure, and an exodus of core executives. Last month the ride-hailing giant paid Waymo US$245 million in stock to settle a self-driving technology IP infringement lawsuit.\n\nGoogle Cloud Chief Scientist of AI/ML Fei-Fei Li tweeted \u201cA fatal self-driving car accident\u2026 This is what AI is: it\u2019s deeply impactful to human lives; and it takes all of us to work on it to make it safe, fair and benevolent.\u201d\n\nDirector of Microsoft Research Labs Eric Horvitz called for increased diligence in self-driving safety in the wake of the accident, and stressed that \u201cself-driving cars will be held to a higher standard. Automation will almost certainly bring down numbers of deaths on the roads.\u201d\n\nSelf-driving vehicles have logged millions of miles on public roads with strong safety records. \u201cThe probability of having an accident is 50 percent lower if you have Autopilot on,\u201d says Tesla CEO Elon Musk. When things do go wrong, however, they make headlines. In 2016, the driver of a Tesla in semi-autonomous driving mode was killed after colliding with a truck on a Florida highway. Although the driver had reportedly ignored at least seven safety warnings, the accident severely hit Tesla\u2019s credibility in the semi-autonomous driving market.\n\nLast year in Las Vegas a self-driving bus was involved in a crash with a delivery truck, only two hours after it made its debut. No injuries were reported at the scene. While technically the bus was not responsible for the accident \u2014 and the delivery truck driver was cited by police \u2014 passengers on the smart bus complained that it was not intelligent enough to move out of harm\u2019s way as the truck slowly approached.\n\nInsiders are saying the Arizona accident is the most serious setback yet for self-driving vehicles, coming at a critical time when the technology was transitioning from research and development to operation and deployment.\n\nEarlier this year, Waymo bought thousands of autonomous-capable Chrysler Pacifica Hybrids. The Alphabet-owned company has tested its self-driving cars in 25 cities across the US. American car maker Ford meanwhile is testing the capability of its own self-driving technology with delivery tasks in some US states.\n\nMany questions remained unanswered: How will the regrettable accident affect autonomous driving R&D and Startups? Will there be a public backlash against autonomous vehicles? Will lawmakers tighten regulations on self-driving cars on public roads?\n\nSynced is covering the story and will continue to update readers with the latest news."
    },
    {
        "url": "https://medium.com/syncedreview/googles-musicvae-is-a-machine-learning-mozart-eb0e44c790d5?source=user_profile---------64----------------",
        "title": "Google\u2019s MusicVAE Is a Machine Learning Mozart \u2013 SyncedReview \u2013",
        "text": "Google has announced the release of MusicVAE, a machine learning model that makes composing musical scores as easy as mixing paint on a palette. A breakthrough from Google Brain\u2019s Magenta Project, MusicVAE generates and morphs melodies to output multi-instrumental passages optimized for expression, realism and smoothness which sound convincingly like human-composed music.\n\n \n\nWhile breakthroughs in AI technologies have thus far tended to emerge from research into industry solutions, Magenta is exploring AI\u2019s potential in the creative spaces that differentiate humans from machines. Launched in 2016, Magenta uses deep learning and reinforcement learning algorithms to explore art and music and has introduced a number of research tools, including NSynth, a music synthesizer; and SketchRNN, an online neural network-based interactive doodling experiment.\n\n \n\nTeaching a machine to create a standardized method for blending different musical elements is not easy. Google researchers turned to Variational Auto-Encoders (VAE), a widely-used generative model that has yielded state-of-the-art machine learning results in image generation and reinforcement learning since 2013. \n\n \n\nVAEs work in an encoder-decoder structure where the encoder represents the variation in a high-dimensional dataset with a lower-dimensional code, and the decoder morphs the variation in a neural network to create an output. The model can be refined and tuned by comparing the input and output. \n\n \n\nGoogle researchers had already applied the technique to SketchRNN, and have now brought the same infrastructure to MusicVAE. Because musical elements are typically more complicated than sketches, Google researchers developed a novel hierarchical decoder for MusicVAE that is capable of generating long-term structure from individual latent codes.\n\n \n\nGoogle last Thursday released a Tensorflow implementation of MusicVAE and a JavaScript library with pre-trained MusicVAE models to help coders, composers and researchers build tools. \n\n \n\nSeveral Google engineers have already handcrafted applications based on MusicVAE. Melody Mixer is an interface created by Google\u2019s Creative Lab that allows users to generate interpolations between short melody loops. Latent Loops, from Google\u2019s Pie Shop, can generate a palette of melodic loops by sketching on a matrix. \n\n \n\nDemos and music samples generated by MusicVAE are already popping up on social media. \u201cThis MusicVAE thing is absurdly cool. The interpolated (and randomly generated) melodies/songs sound *real*, like they were composed, not generated,\u201d tweeted Alexander Huth, an Assistant Professor in Computer Science and Neuroscience at UT Austin. \n\n \n\nThe Magenta team stresses that MusicVAE and their other smart tools are meant as collaborative tools to \u201callow artists and musicians to extend (not replace!) their processes.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/african-fintech-startups-are-revolutionizing-banking-b726a1b4ccfe?source=user_profile---------65----------------",
        "title": "African Fintech Startups Are Revolutionizing Banking",
        "text": "Africa is a vast continent with diverse economies and a total population of over 1 billion people living in 54 independent countries spread over 30 million square kilometres. By the end of 2017, there were more than 300 fintech startups across the continent. Disrupt Africa\u2019s Finnovating for Africa: Exploring the African Fintech Startup Ecosystem Report 2017 concludes that African fintech startups\u2019 growth since 2015 has been nothing short of tremendous.\n\nThe record shows over US$100 million in fintech funding has been secured across the continent over the last two years, with South Africa receiving 34.2 percent of the total and Nigeria following closely with 34 percent. South Africa has the most fintech startups with 94, followed by Nigeria with 74 and Kenya 56. In this report, we identify the factors and the drivers behind the growth.\n\nFintech has made payments and remittances more convenient across the continent. Most traditional banks are located in cities and commercial areas, making them difficult to access from remote areas. Tanzania, for example, has about 50 million people sparsely distributed across an area nearly four times the size of the United Kingdom. In Nigeria, banks used to be packed with customers queueing to pay their utility and cable TV bills, school fees and so on. It could take hours to make a simple transaction.\n\nIn 2012, a cashless policy was introduced by the Central Bank of Nigeria to curb excess handling of cash and reduce the volume of money in circulation. The policy has facilitated many Nigerian fintech startups\u2019 market penetration and expansion.\n\nCustomer transportation costs, waiting times and the loss risks attached to cash have been eliminated by the smartphone-based fintech services provided by these startups. About 100 of Africa\u2019s fintech startups are focused on streamlining money transfers. According to Tayo Oviosu, Founder & CEO of Nigerian mobile payment platform Paga, \u201cNigerian banks have traditionally not focused on retail. Paga has built the single largest network of financial access points in Nigeria. We are going to leverage that to deliver financial services to the mass market\u201d.\n\nOne of the significant drivers of African fintech startups is high confidence in the market. Early fintech startups demonstrated that the market is strong, and the growth trend has continued, attracting Silicon Valley-based accelerators. Fintech startup funding is presently one of the most attractive investments on the continent. In 2017, over 30% of the US$195 million in VC funding raised by Africa startups went to the fintech sector.\n\nSafaricom\u2019s M-Pesa mobile money service has had a great impact in Kenya, and Nigeria\u2019s Paga, South Africa\u2019s Zoona, Kenya\u2019s BitPesa and others across the continent are garnering increased funding as investors become more confident.\n\nSince the inception of fintech, there have been dramatic changes in the continent\u2019s traditional banking system. Banks and financial institutions are under pressure to match the innovative solutions and services being offered by fintech startups which have reached millions of people who have mobile phones but not bank accounts.\n\nNow, banks and financial institutions are introducing a variety of strategies and tactics to invest in, acquire or collaborate with fintech startups. This is a trend that\u2019s expected to continue.\n\nMore Africans Connected to the Internet\n\nNigeria, South Africa, Egypt, Ethiopia and Kenya are among the most significant mobile markets in Africa. Although 80 million Nigerians \u2014 47 percent of the population \u2014 do not have bank accounts, 142 million Nigerians have mobile network access and 92 million are internet users, according to the Nigerian Communications Commission (NCC).\n\nThe penetration of mobile phones and the internet has enabled fintech to influence how financial services and products are developed and delivered, as more Africans plug into digital financial services in Nigeria and across the continent.\n\nKenya\u2019s M-Pesa is being used by more than half of the country\u2019s adult population, and has recorded transactions worth more than half the country\u2019s GDP since its debut. Similar success stories have been told by the likes of South African startup Zoona, Nigeria\u2019s Paga and others.\n\nA World Bank report notes that Nigeria, like many countries in sub-Sahara Africa, has a growing population that lacks easy access to traditional financial services, and fintech innovators are filling the vacuum by connecting these people.\n\nThis differs from the situation in advanced economies with strong financial institutions, where fintech startups are cast instead as disrupting the traditional banking industry. For example, China\u2019s Wechat is a popular messaging app with a wallet feature that enables users to send and receive money, make payments and so on from within the Wechat app, without connecting to a bank account for many transactions.\n\nAmong the factors responsible for African fintech startup growth are strong competition and loose regulations. Presently across the continent, there are few or no strict regulations compared to advanced economies. Startups can operate relatively tax-free free and with less government interference, leaving them to chart their courses and develop their products with little regulatory interference.\n\nThere is also increasing integration from service providers, many of which are partnering with fintech startups to make transactions more convenient for customers. For example, cable TV companies such as DSTV, HiTV, and TSTV have introduced fintech alternatives to their traditional bank payment models.\n\nAs more banks and financial institutions acquire or partner with fintech startups, the trend is being seen not so much as financial industry competition but as an industry reinvention that has improved financial services companies\u2019 profiles, reach, and products and services; and is beneficial to banks, startups and customers alike.\n\nFintech has thus become one of the most vibrant investment options in the African tech space.\n\nIn recent years, African fintech startups have been outperforming banks in delivering digital financial services. Iyin Aboyeji, Chief Executive of digital payment technology startup Flutterwave, describes fintech as a fundamental element that will drive the digital economy in Africa over the coming years. Investors and companies are expected to get even more involved in African fintech markets because the opportunities are tremendous."
    },
    {
        "url": "https://medium.com/syncedreview/yu-zheng-on-jd-coms-smart-city-ambitions-265253d51746?source=user_profile---------66----------------",
        "title": "Yu Zheng on JD.com\u2019s Smart City Ambitions \u2013 SyncedReview \u2013",
        "text": "Dr. Zheng joined JD Finance last month as Vice President and Chief Data Scientist, Urban Computing Business Unit President, and Urban Computing Lab Director. JD Finance is the fintech arm of JD.com, China\u2019s largest e-commerce platform by revenue.\n\nSynced recently spoke with Dr. Zheng on JD Finance\u2019s entry into Urban Computing.\n\nFintech is composed of two keywords: finance and technology. Many people think of JD Finance as a fintech company, but this is not accurate: we provide empowering technology for the finance industry.\n\nJD Finance\u2019s business model is B2B2C. To illustrate this, let\u2019s say we provide a better risk control model for banks, which in turn give better loan services for customers. In this equation JD Finance is the first B (business), the bank is the second B (business), and the end customer is C (customer).\n\nWe can also replace the middle B (business) with G (government), providing governing institutes the proper technology to serve its people. In other words, urban computing broadens JD Finance\u2019s existing businesses. The Chinese Government\u2019s invitation to JDF to get into these urban computing areas such as transportation and environment will change the public\u2019s view of our company.\n\nThat is confidential for the time being, we have some upcoming announcements. But I can tell you that we are building sub-divisions focusing on environment and transportation control, and the teams will be big.\n\nSmart commerce. People\u2019s impression of urban computing is environment, transportation, and various types of city planning. But in fact commerce also plays a big role in this panorama, for example in business site selection, real estate assessment, and helping banks with risk management.\n\nFor example, if a company seeks a bank loan to build a casino, then the bank will conduct risk assessment on the project itself, whereas in the past, the bank\u2019s approach may have been to just assess the company\u2019s own credit qualifications such as bad debt ratios, creditworthiness, and so on.\n\nCompanies with good qualifications may undertake risky projects by factoring in local development and consumption levels, which are only reflected in forms of data. Criteria such as local spending index, travel method, urban infrastructure including power networks and transportation all contribute to the decision making process.\n\nUrban computing uses diverse spatio-temporal data to do calculations such as analysis, prediction, causal analysis, and anomaly detection for a given scenario.\n\nWe have a lot of data. JD.com has nearly 300 million active users, as shown in our latest financial report. Our huge datasets are composed of product info, user transaction data, and logistic data. Financial figures like wealth management, payment and consumption also contribute to datasets.\n\nSufficient data quantity can accurately reflect a city\u2019s economic well-being. Logistics data for example maps the commercial flow of an area and its business relations with surrounding areas. We have data in abundance which is rare and good.\n\nUrban computing relies on spatio-temporal data, which is neither video, image, nor text. It has its own data management methods and AI algorithms. In other words, you can\u2019t solve the problems at hand by throwing in a CNN or LSTM alone. Spatio-temporal attributes, including time trends, periods, and proximity, spatial distance, and spatial gradation are characteristics that cannot be grasped by commonly used algorithms.\n\nThere are also multiple data sources. For instance, the casino case we mentioned above demands the use of POI, road network data points, plus a lot of data such as environmental and spending, which jointly predict future changes.\n\nMultivariate data fusion is a difficult, and it is also a relatively new discipline and research direction in machine learning. How can data from different fields determine that 1+1 is greater than 2? This is very difficult.\n\nAt the same time, urban computing is not a simple cloud computing problem. Cloud computing platforms can\u2019t support such spatio-temporal data. The data structure query method of spatio-temporal data, as well as the multi-data fusion and indexing mechanism just described, do not yet exist.\n\nThe cloud service providers currently on the market are not suitable for urban computing. Service providers must undergo a special technical build up, in order to manage, analyze, and tap into spatio-temporal big data, and form a dynamic closed loop. It is very difficult, and the threshold is also very high.\n\nLet me give you a concrete example: traffic light control is more difficult to tackle than AlphaGo. AlphaGo faces a 19\u00d719 grid, and the states on each grid are only black, white, or empty.\n\nYet there are tens of thousands of intersections of traffic lights in Beijing, and the status and actions of each intersection have more possibilities. Traffic may be flowing at 40 km/h, 45km/h or 30 km/h; signal light timing may be 30 seconds for the red light or 20 seconds for the green light. All of these are changing continuously. We are also missing data. Roads with no pedestrians, cars, or sensors do not give us any data. The road is also an open system. One man, or even a dog crossing the road will alter the traffic state.\n\nTherefore, urban problems have a large state space, large movement space, and an open system. This is certainly much more difficult to solve than problems on a Go board.\n\nLet me give another example. In urban population flow forecasting, we divide cities into a number of grids, and we want to predict how many people will enter and exit in each grid.\n\nThe flow of people in a grid is related not only to how many people have entered and exited in the previous hour, but also to how many people are moving in and out of neighboring grids. You also want to loop in areas that are far away, taking into consideration for that when big events happen, people will emerge from subway exits. If you only rely on local changes around the grid, you can\u2019t predict bigger things happening, say tragic incidents such as a stampede.\n\nIt\u2019s data and team. Urban computing relies on good databases and data resources. Everyone thinks that governments have a lot of data, but this is not the case. In many cases, they need industry data to support their own decision-making processes and solve problems.\n\nAlso important is the team. People say AI is a talent war, but it\u2019s definitely not a war of numbers. A team of one hundred is not necessarily better than a team of ten. But there are many times when you can\u2019t solve a problem yourself and so rely on others for inspiration. If we are stuck with missing critical data, perhaps an \u201ca-ha!\u201d moment from a teammate will provide alternative measures. This is what the word \u201ctalent\u201d stands for.\n\nA good engineer can work on ten projects at the same time, while a mediocre team might use one hundred people without any results. We realize the importance of talent, and so we have heavily invested in pooling top-notch talents.\n\nPrior to joining JD Finance, Dr. Zheng led the urban computing team at Microsoft Research Asia, publishing profusely in this area; while also holding the positions of Chair Professor of Shanghai Jiaotong University, Guest Professor of Hong Kong University of Science and Technology, and Editor-in-Chief of ACM TIST."
    }
]