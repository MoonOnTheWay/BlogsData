[
    {
        "url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0?source=user_profile---------1----------------",
        "title": "The fall of RNN / LSTM \u2013",
        "text": "We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. Now it is time to drop them!\n\nIt is the year 2014 and LSTM and RNN make a great come-back from the dead. We all read Colah\u2019s blog and Karpathy\u2019s ode to RNN. But we were all young and unexperienced. For a few years this was the way to solve sequence learning, sequence translation (seq2seq), which also resulted in amazing results in speech to text comprehension and the raise of Siri, Cortana, Google voice assistant, Alexa. Also let us not forget machine translation, which resulted in the ability to translate documents into different languages or neural machine translation, but also translate images into text, text into images, and captioning video, and \u2026 well you got the idea.\n\nThen in the following years (2015\u201316) came ResNet and Attention. One could then better understand that LSTM were a clever bypass technique. Also attention showed that MLP network could be replaced by averaging networks influenced by a context vector. More on this later.\n\nIt only took 2 more years, but today we can definitely say:\n\nBut do not take our words for it, also see evidence that Attention based networks are used more and more by Google, Facebook, Salesforce, to name a few. All these companies have replaced RNN and variants for attention based models, and it is just the beginning. RNN have the days counted in all applications, because they require more resources to train and run than attention-based models. See this post for more info.\n\nRemember RNN and LSTM and derivatives use mainly sequential processing over time. See the horizontal arrow in the diagram below:\n\nThis arrow means that long-term information has to sequentially travel through all cells before getting to the present processing cell. This means it can be easily corrupted by being multiplied many time by small numbers < 0. This is the cause of vanishing gradients.\n\nTo the rescue, came the LSTM module, which today can be seen as multiple switch gates, and a bit like ResNet it can bypass units and thus remember for longer time steps. LSTM thus have a way to remove some of the vanishing gradients problems.\n\nBut not all of it, as you can see from the figure above. Still we have a sequential path from older past cells to the current one. In fact the path is now even more complicated, because it has additive and forget branches attached to it. No question LSTM and GRU and derivatives are able to learn a lot of longer term information! See results here; but they can remember sequences of 100s, not 1000s or 10,000s or more.\n\nAnd one issue of RNN is that they are not hardware friendly. Let me explain: it takes a lot of resources we do not have to train these network fast. Also it takes much resources to run these model in the cloud, and given that the demand for speech-to-text is growing rapidly, the cloud is not scalable. We will need to process at the edge, right into the Amazon Echo! See note below for more details.\n\nIf sequential processing is to be avoided, then we can find units that \u201clook-ahead\u201d or better \u201clook-back\u201d, since most of the time we deal with real-time causal data where we know the past and want to affect future decisions. Not so in translating sentences, or analyzing recorded videos, for example, where we have all data and can reason on it more time. Such look-back/ahead units are neural attention modules, which we previously explained here.\n\nTo the rescue, and combining multiple neural attention modules, comes the \u201chierarchical neural attention encoder\u201d, shown in the figure below:\n\nA better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector Ct.\n\nNotice there is a hierarchy of attention modules here, very similar to the hierarchy of neural networks. This is also similar to Temporal convolutional network (TCN), reported in Note 3 below.\n\nIn the hierarchical neural attention encoder multiple layers of attention can look at a small portion of recent past, say 100 vectors, while layers above can look at 100 of these attention modules, effectively integrating the information of 100 x 100 vectors. This extends the ability of the hierarchical neural attention encoder to 10,000 past vectors.\n\nBut more importantly look at the length of the path needed to propagate a representation vector to the output of the network: in hierarchical networks it is proportional to log(N) where N are the number of hierarchy layers. This is in contrast to the T steps that a RNN needs to do, where T is the maximum length of the sequence to be remembered, and T >> N.\n\nThis architecture is similar to a neural Turing machine, but lets the neural network decide what is read out from memory via attention. This means an actual neural network will decide which vectors from the past are important for future decisions.\n\nBut what about storing to memory? The architecture above stores all previous representation in memory, unlike neural Turning machines. This can be rather inefficient: think about storing the representation of every frame in a video \u2014 most times the representation vector does not change frame-to-frame, so we really are storing too much of the same! What can we do is add another unit to prevent correlated data to be stored. For example by not storing vectors too similar to previously stored ones. But this is really a hack, the best would be to be let the application guide what vectors should be saved or not. This is the focus of current research studies. Stay tuned for more information.\n\nTell your friends! It is very surprising to us to see so many companies still use RNN/LSTM for speech to text, many unaware that these networks are so inefficient and not scalable. Please tell them about this post.\n\nAbout training RNN/LSTM: RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions. In short, LSTM require 4 linear layer (MLP layer) per cell to run at and for each sequence time-step. Linear layers require large amounts of memory bandwidth to be computed, in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units. And it is easy to add more computational units, but hard to add more memory bandwidth (note enough lines on a chip, long wires from processors to memory, etc). As a result, RNN/LSTM and variants are not a good match for hardware acceleration, and we talked about this issue before here and here. A solution will be compute in memory-devices like the ones we work on at FWDNXT.\n\nNote 1: Hierarchical neural attention is similar to the ideas in WaveNet. But instead of a convolutional neural network we use hierarchical attention modules. Also: Hierarchical neural attention can be also bi-directional.\n\nNote 2: RNN and LSTM are memory-bandwidth limited problems (see this for details). The processing unit(s) need as much memory bandwidth as the number of operations/s they can provide, making it impossible to fully utilize them! The external bandwidth is never going to be enough, and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth. The best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory, or that can be re-used for multiple computation per byte transferred (high arithmetic intensity).\n\nNote 3: Here is a paper comparing CNN to RNN. Temporal convolutional network (TCN) \u201coutperform canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory\u201d.\n\nNote 4: Related to this topic, is the fact that we know little of how our human brain learns and remembers sequences. \u201cWe often learn and recall long sequences in smaller segments, such as a phone number 858 534 22 30 memorized as four segments. Behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks\u201d \u2014 these chunks remind me of small convolutional or attention like networks on smaller sequences, that then are hierarchically strung together like in the hierarchical neural attention encoder and Temporal convolutional network (TCN). More studies make me think that working memory is similar to RNN networks that uses recurrent real neuron networks, and their capacity is very low. On the other hand both the cortex and hippocampus give us the ability to remember really long sequences of steps (like: where did I park my car at airport 5 days ago), suggesting that more parallel pathways may be involved to recall long sequences, where attention mechanism gate important chunks and force hops in parts of the sequence that is not relevant to the final goal or task.\n\nNote 5: The above evidence shows we do not read sequentially, in fact we interpret characters, words and sentences as a group. An attention-based or convolutional module perceives the sequence and projects a representation in our mind. We would not be misreading this if we processed this information sequentially! We would stop and notice the inconsistencies!\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/tracking-with-deep-networks-91e0f2c1cc20?source=user_profile---------2----------------",
        "title": "Tracking with deep networks \u2013 Eugenio Culurciello \u2013",
        "text": "Tracking is the process of locating a user selected object in different frames as it moves around the scene. It has a variety of uses such as human-computer interactions, gesture recognition, driver assistance systems, security monitoring, medical imaging and agricultural automations. There has been extensive studies for tracking during the last four decades and many different tracking algorithms have been proposed. However, all these trackers are limited to simple scenarios such as no occlusion, illumination or appearance change and no complex object motion. On the other hand we have such perfect tracker examples: humans and animals!! The human visual system object tracking performance is currently unsurpassed by engineered systems, thus our research tries to take inspiration and reverse-engineer the known principles of cortical processing during visual tracking.Inspired by recent \ufb01ndings on shallow feature extractors of the visual cortex, we postulate that simple tracking processes are based on a shallow neural network that can identify quickly similarities between object features repeated in time. We propose an algorithm that can track and extract motion of an object based on the similarity between local features observed in subsequent frames. The local features are initially de\ufb01ned as a bounding box that de\ufb01nes the object to track.\n\nThe SMR tracker achieved the state-of-the-art performance on the TLD [1] dataset as presented in Table 2. See the SMR Paperto learn more about it!! Download the code and try yourself!\n\nFigure 1 shows snopshots from videos and Table 1 lists the properties. Detection is considered to be correct if its overlap with ground truth bounding box is larger than 25% .\n\nFigure 1 : Snapshots from the sequences with the objects marked by the bounding box [1]\n\nNOTE: this is an old post from our research in 2012"
    },
    {
        "url": "https://medium.com/@culurciello/you-are-the-product-2ae2e66384bb?source=user_profile---------3----------------",
        "title": "You are the product \u2013 Eugenio Culurciello \u2013",
        "text": "Social networks and mind games: ethical concerns and ideas for proper usage\n\nSocial network that want to push us advertisement need to learn more about us, what we like and what we do not like. They will push products, and now even news, and they will influence us on our decisions. As some colleagues say, \u201cwe are the product\u201d.\n\n\u201cBeing the product\u201d is good if you want to get more of what you like. Social networks will read your personality and give you more of what you seek, and maybe do not know it exists. It may help you to find more news and products you care about. This is beneficial as it can save you time from searching or learning something new.\n\nBut in order to push you information, social networks build a model of you, based on your data. A statistical model of what you like and do not like. They can thus show you more products and news, and you will be tipped to consume more of such product and information \u2014 there are many studies on this. You may spend more, and maybe be pushed a bad products, usually not, as products come with reviews and feedback from other humans.\n\nNews, on the other hand, cannot be tried and evaluated like a product. We consume them instantly from titles and short sentences in social networks, sometimes even bypassing the actual content of a larger set of information.\n\nWe instead rely on our network to trust the information, and we often absorb it whethere it is real or not. We just do nto spend the time to check every fact, and we are getting a lot of facts with social network, more than we can consume and more that we can evaluate and validate.\n\nIdeally the social network only passes us true and verified information. But how can it do that? Humans are required to validate information, and humans have bias.\n\nAll media have human bias, and it is the culture of a media channel that decides what the bias is and how much bias is added.\n\nFake information created for the purpose of manipulating people\u2019s mind is unethical. But what is the difference between extreme bias and fabrication of information? How can we draw the line? We all know the may be one truth, but we also all know that there are many points of view of the same truth. Each one of us adds a bias. A pedestrian was hit by a car. Was it the pedestrian fault to walk at night with no lights? Or was it the car driver fault not noticing the pedestrian?\n\nWe have all been fooled by fake news, often because we did not read all the information, or we did not verify its source, or because it told us something we wanted to read. As this reference says:\n\nRegardless of the motive of why fake news is instilled in social network, we consumers need to pay more attention to sources and spend more time validating facts. The social network cannot possibly validate facts, because there is too much information flowing through the platform and not enough automated tools to do this. We yet do not have machine intelligence to verify news and information sources reliably. Social networks and all us free-speech advocates do not want, of course, to filter information, so we cannot afford to have imperfect machine intelligence tools to filter information. We need these tools to be at least a bit better than human-based filtering.\n\nYour social network also biases you towards people like you, removing you with potentially enriching dialogue with people that are not like your circle and have different ideas from you. Unlike the real world in social network you can carefully draft your circles, and this be more prone to this potential negative effect of living in isolated bubbles with less communication.\n\nYour data is thus important to you and social network to give you more of what you seek, but it also has to be protected, so we do not give social network and other parties the ability to control your mind.\n\nAs many others predict, these problems will only increase. One reason is that\n\nMany people like myself and my group work on intelligent algorithms to mine and analyze data. And the machine intelligence revolution has just started, yet our tools are most likely very crude. Often deep learning algorithms based on neural networks have provided super-human abilities in focused tasks, examples: one, two, three, etc. But in other tasks, as for example behavior analysis, video understanding, action recognition, understanding of text and video \u2014 we are still in infancy.\n\nAs algorithms and intelligent learning machines become more powerful, placed in the hands of the wrong people, they will be able to create fake content, images, videos that will be hard for us humans to validate and verify. As intelligent algorithms have more information about us, they will be able to create better and better model to fool us. They will play a reinforcement game with us, trying on purpose to fool us and drive our decisions. We may not be in control of our own future. We may be brainwashed just like we can be from a fellow human con artist. Or even worse, these algorithm will evolve super-human ability to control our minds.\n\nIf you want to use social media and protect your mind from brainwashing, we suggest:"
    },
    {
        "url": "https://towardsdatascience.com/artificial-intelligence-ai-in-2018-and-beyond-e06f05167f9c?source=user_profile---------4----------------",
        "title": "Artificial Intelligence, AI in 2018 and beyond \u2013",
        "text": "These are my opinions on where deep neural network and machine learning is headed in the larger field of artificial intelligence, and how we can get more and more sophisticated machines that can help us in our daily routines.\n\nPlease note that these are not predictions of forecasts, but more a detailed analysis of the trajectory of the fields, the trends and the technical needs we have to achieve useful artificial intelligence.\n\nNot all machine learning is targeting artificial intelligences, and there are low-hanging fruits, which we will examine here also.\n\nThe goal of the field is to achieve human and super-human abilities in machines that can help us in every-day lives. Autonomous vehicles, smart homes, artificial assistants, security cameras are a first target. Home cooking and cleaning robots are a second target, together with surveillance drones and robots. Another one is assistants on mobile devices or always-on assistants. Another is full-time companion assistants that can hear and see what we experience in our life. One ultimate goal is a fully autonomous synthetic entity that can behave at or beyond human level performance in everyday tasks.\n\nSee more about these goals here, and here, and here.\n\nSoftware is defined here as neural networks architectures trained with an optimization algorithm to solve a specific task.\n\nToday neural networks are the de-facto tool for learning to solve tasks that involve learning supervised to categorize from a large dataset.\n\nBut this is not artificial intelligence, which requires acting in the real world often learning without supervision and from experiences never seen before, often combining previous knowledge in disparate circumstances to solve the current challenge.\n\nNeural network architectures \u2014 when the field boomed, a few years back, we often said it had the advantage to learn the parameters of an algorithms automatically from data, and as such was superior to hand-crafted features. But we conveniently forgot to mention one little detail\u2026 the neural network architecture that is at the foundation of training to solve a specific task is not learned from data! In fact it is still designed by hand. Hand-crafted from experience, and it is currently one of the major limitations of the field. There is research in this direction: here and here (for example), but much more is needed. Neural network architectures are the fundamental core of learning algorithms. Even if our learning algorithms are capable of mastering a new task, if the neural network is not correct, they will not be able to. The problem on learning neural network architecture from data is that it currently takes too long to experiment with multiple architectures on a large dataset. One has to try training multiple architectures from scratch and see which one works best. Well this is exactly the time-consuming trial-and-error procedure we are using today! We ought to overcome this limitation and put more brain-power on this very important issue.\n\nUnsupervised learning \u2014we cannot always be there for our neural networks, guiding them at every stop of their lives and every experience. We cannot afford to correct them at every instance, and provide feedback on their performance. We have our lives to live! But that is exactly what we do today with supervised neural networks: we offer help at every instance to make them perform correctly. Instead humans learn from just a handful of examples, and can self-correct and learn more complex data in a continuous fashion. We have talked about unsupervised learning extensively here.\n\nPredictive neural networks \u2014 A major limitation of current neural networks is that they do not possess one of the most important features of human brains: their predictive power. One major theory about how the human brain work is by constantly making predictions: predictive coding. If you think about it, we experience it every day. As you lift an object that you thought was light but turned out heavy. It surprises you, because as you approached to pick it up, you have predicted how it was going to affect you and your body, or your environment in overall.\n\nPrediction allows not only to understand the world, but also to know when we do not, and when we should learn. In fact we save information about things we do not know and surprise us, so next time they will not! And cognitive abilities are clearly linked to our attention mechanism in the brain: our innate ability to forego of 99.9% of our sensory inputs, only to focus on the very important data for our survival \u2014 where is the threat and where do we run to to avoid it. Or, in the modern world, where is my cell-phone as we walk out the door in a rush.\n\nBuilding predictive neural networks is at the core of interacting with the real world, and acting in a complex environment. As such this is the core network for any work in reinforcement learning. See more below.\n\nWe have talked extensively about the topic of predictive neural networks, and were one of the pioneering groups to study them and create them. For more details on predictive neural networks, see here, and here, and here.\n\nLimitations of current neural networks \u2014 We have talked about before on the limitation of neural networks as they are today. Cannot predict, reason on content, and have temporal instabilities \u2014 we need a new kind of neural networks that you can about read here.\n\nNeural Network Capsules are one approach to solve the limitation of current neural networks. We reviewed them here. We argue here that Capsules have to be extended with a few additional features:\n\nContinuous learning \u2014 this is important because neural networks need to continue to learn new data-points continuously for their life. Current neural networks are not able to learn new data without being re-trained from scratch at every instance. Neural networks need to be able to self-assess the need of new training and the fact that they do know something. This is also needed to perform in real-life and for reinforcement learning tasks, where we want to teach machines to do new tasks without forgetting older ones.\n\nFor more detail, see this excellent blog post by Vincenzo Lomonaco.\n\nTransfer learning \u2014 or how do we have these algorithms learn on their own by watching videos, just like we do when we want to learn how to cook something new? That is an ability that requires all the components we listed above, and also is important for reinforcement learning. Now you can really train your machine to do what you want by just giving an example, the same way we humans do every!\n\nReinforcement learning \u2014 this is the holy grail of deep neural network research: teach machines how to learn to act in an environment, the real world! This requires self-learning, continuous learning, predictive power, and a lot more we do not know. There is much work in the field of reinforcement learning, but to the author it is really only scratching the surface of the problem, still millions of miles away from it. We already talked about this here.\n\nReinforcement learning is often referred as the \u201ccherry on the cake\u201d, meaning that it is just minor training on top of a plastic synthetic brain. But how can we get a \u201cgeneric\u201d brain that then solve all problems easily? It is a chicken-in-the-egg problem! Today to solve reinforcement learning problems, one by one, we use standard neural networks:\n\nBoth these components are obvious solutions to the problem, and currently are clearly wrong, but that is what everyone uses because they are some of the available building blocks. As such results are unimpressive: yes we can learn to play video-games from scratch, and master fully-observable games like chess and go, but I do not need to tell you that is nothing compared to solving problems in a complex world. Imagine an AI that can play Horizon Zero Dawn better than humans\u2026 I want to see that!\n\nBut this is what we want. Machine that can operate like us.\n\nOur proposal for reinforcement learning work is detailed here. It uses a predictive neural network that can operate continuously and an associative memory to store recent experiences.\n\nNo more recurrent neural networks \u2014 recurrent neural network (RNN) have their days counted. RNN are particularly bad at parallelizing for training and also slow even on special custom machines, due to their very high memory bandwidth usage \u2014 as such they are memory-bandwidth-bound, rather than computation-bound, see here for more details. Attention based neural network are more efficient and faster to train and deploy, and they suffer much less from scalability in training and deployment. Attention in neural network has the potential to really revolutionize a lot of architectures, yet it has not been as recognized as it should. The combination of associative memories and attention is at the heart of the next wave of neural network advancements.\n\nAttention has already showed to be able to learn sequences as well as RNNs and at up to 100x less computation! Who can ignore that?\n\nWe recognize that attention based neural network are going to slowly supplant speech recognition based on RNN, and also find their ways in reinforcement learning architecture and AI in general.\n\nLocalization of information in categorization neural networks \u2014 We have talked about how we can localize and detect key-points in images and video extensively here. This is practically a solved problem, that will be embedded in future neural network architectures.\n\nHardware for deep learning is at the core of progress. Let us now forget that the rapid expansion of deep learning in 2008\u20132012 and in the recent years is mainly due to hardware:\n\nAnd we have talked about hardware extensively before. But we need to give you a recent update! Last 1\u20132 years saw a boom in the are of machine learning hardware, and in particular on the one targeting deep neural networks. We have significant experience here, and we are FWDNXT, the makers of SnowFlake: deep neural network accelerator.\n\nThere are several companies working in this space: NVIDIA (obviously), Intel, Nervana, Movidius, Bitmain, Cambricon, Cerebras, DeePhi, Google, Graphcore, Groq, Huawei, ARM, Wave Computing. All are developing custom high-performance micro-chips that will be able to train and run deep neural networks.\n\nThe key is to provide the lowest power and the highest measured performance while computing recent useful neural networks operations, not raw theoretical operations per seconds \u2014 as many claim to do.\n\nBut few people in the field understand how hardware can really change machine learning, neural networks and AI in general. And few understand what is important in micro-chips and how to develop them.\n\nHere is our list:\n\nAbout neuromorphic neural networks hardware, please see here.\n\nWe talked briefly about applications in the Goals section above, but we really need to go into details here. How is AI and neural network going to get into our daily life?\n\nHere is our list:\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!\n\nFor interesting additional reading, please see:"
    },
    {
        "url": "https://medium.com/@culurciello/2017-an-year-in-review-7b341dbf5880?source=user_profile---------5----------------",
        "title": "2017: an year in review \u2013 Eugenio Culurciello \u2013",
        "text": "How we can raise again from the lowest lows in 2018 and later~\n\nI tried to spend a great deal of time trying to understand what made this year so utterly dark. As a person ages it tends to have more negative outlooks for the world, but this is not what made this year one of the worst for me and many of us. This year was not just strange, it just showed us how human nature and human limitations can resurface again to bring the worst in all of us.\n\nThese are some of the trends I noticed:\n\nBut nobody likes negativity, so we ought to find way to look forward to a better future:\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/learning-and-performing-in-the-real-world-7e53eb46d9c3?source=user_profile---------6----------------",
        "title": "Learning and performing in the real world \u2013",
        "text": "Learning and performing in the real world\n\nIn order to interact with a complex environment, living entities need to produce a \u201ccorrect\u201d sequence of actions to achieve delayed future rewards. These living entities, or actors, can sense the environment and produce actions in response of a sequence of states of both the environment and agent previous history. See figure:\n\nThe scenario where we want to learn to perform some tasks, is:\n\nThe actions propel the actor closer to multiple goals, where achieving even one goal may take an undefined number of steps. The agent need to learn the correct sequence of model states and actions that lead to a delayed reward by interacting with the world and performing online learning.\n\nRecent work in artificial intelligence (AI) uses reinforcement learning algorithms to model the behavior of agents using sparse and sporadic rewards. Recent advances in this field allowed to train agents to perform in complex artificial environment and even surpass human abilities in the same contexts [Mnih2013, -2016]. Some exciting recent results in the field are learning language while performing RL tasks \u2014 see here.\n\nTo learn to perform in the real world, given a sequence of states s we want to obtain a sequence of actions a that maximize a reward, or probability of winning.\n\nFundamental needs (suppose we do not know rules of games, just a list of possible actions we can perform):\n\nThere are major differences in board games like chess, go etc, where the entire game is fully observable, and real-world scenarios, like self-driving robot-cars, where only part of the environment is visible.\n\nThe AlphaZero paper show an elegant approach to fully-observable games.\n\nOne (AZ1): a function f (neural network) based on parameters theta outputs probability of action a and value of action v (a prediction of the value of making a move).\n\nTwo (AZ2): a predictive model that can evaluate the outcome of different moves and self-play (play scenarios in your head). A Monte Carlo Tree Search in AlphaZero. These games outcomes, move probabilities a and value v are then used to update the function f.\n\nThese two ingredients make for a very simple and elegant way to learn to play these table games, and also learn without human support in a relatively short time.\n\nBut what about non-fully observable games? Like a 1st person shooter (Doom)? Or an autonomous car learning to drive?\n\nA- Only a portion of the environment is visible, and thus predicting the outcome of actions and next states is a much harder problem.\n\nB- There are too many possible options to search. The search tree is too vast. We can use AZ2, but we need to be smart and fast about which options we evaluate, as there are too many and we only have limited time to decide our next action!\n\nC- There may not be another player to compete with. The agent here has to compete with himself, or with his own predictions, getting intermediate rewards from its ability to foresee events or not.\n\nUnfortunately, and despite efforts in the recent years, RL algorithms only work in small artificial scenarios, and do not extend to more complex or real-life environment. The reason is that currently the bulk of the parameters of these systems are trained with time-consuming reinforcement- learning based on excessively sparse reward. In real-world scenarios, the model becomes very large (with many parameters) and almost impossible to train in short time-frames.\n\nWhat we need is to be able to train a model brain to be familiar with the environment, and be able to predict actions and events combinations. With this pre-trained model brain, reinforcement learning classifier for achieving specific goals, and with a small set of trainable parameters, are much easier to train using sparse rewards. We need a pre-trained neural network that can process at least visual inputs. It needs to be trained on video sequences, and needs to be able to provide prediction of future representation of the inputs.\n\nIn order to surpass these limitations, we are studying a synthetic model brain with the following characteristics:\n\nOn the left, you can see a proposal for a neural network that can understand the world and act in it. This network uses video and can predict future representation based on a combination of the state s and its own associative memory. A multi-head attention can recall memories and combine them with state s to predict the best action to take.\n\nThis model is able to perform well in multiple conditions:\n\nHow do we train this synthetic brain?\n\nWe argued in the past for a new kind of neural networks, which have showed to be more effective in learning RL tasks.\n\nWe also like the predictive capabilities of Capsules, which does not require a ground-truth representation, but is able to predict next layer outputs based on the previous layer.\n\nNote 1: this is a great post on why RL does or does not work well and the issues associated with it. I agree a lot of RL these days is highly inefficient and there is no transfer-learning success story. This is why we should push approaches to pre-train networks, curriculum learning and breaking the task into many simpler tasks each with simple rewards. One solution to all problems is hard if your search space is so large!\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/predictive-neural-networks-for-reinforcement-learning-490738725839?source=user_profile---------7----------------",
        "title": "Predictive neural networks for reinforcement learning",
        "text": "We used predictive neural network like CortexNet to show that they can speed up reinforcement learning. We used VizDoom rocket basic scenario.\n\nWe collected videos of 500 episodes of human game play, and we pre-trained a predictive neural network on those videos.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/mlreview/deep-neural-network-capsules-137be2877d44?source=user_profile---------8----------------",
        "title": "Deep Neural Network Capsules \u2013 ML Review \u2013",
        "text": "A recent paper on Capsules has many important insights for revolutionizing learning in Deep Neural Networks. Let us see why!\n\nHere is a list of important points in the paper and video lecture:\n\nHere is a picture of CapsNet, the neural network architecture using Capsules. The interesting dynamic routing occurs between PrimaryCaps and DigitCaps.\n\nDynamic routing is implemented with two main transformation as reported in these equations (2 in paper). U are the outputs of Capsules in the layer below, and S are outputs from Capsules on layer above. U_hat is a prediction of what the output from a Capsule j above would be given the input from the Capsule i in layer below. This is very interesting as an instantiation of predictive neural networks.\n\nW_ij is a matrix of weights (like a linear layer) going from all capsules from one layer to the next. Notice there are as many W matrices as i*j. c_ij is another matrix that combines the contribution of lower layer capsules into the next layer output. Coefficients c_ij are computed with the dynamic routing algorithms described in the paper. The important point is that this is done by computing the agreement between the real output of next layer v and the prediction h_hat: b_ij \u2190 b_ij + u\u02c6_j|i * v_j\n\nNote 1: We like this paper because Capsules agrees with a lot of the work and thoughts we had in previous years, and that we named \u201cClustering Learning\u201d. See our previous publications here:\n\nhttps://lnkd.in/dnSgjJU\n\nhttps://lnkd.in/dmNbuVs\n\nNote 3: great blog post on this part I and part II.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d?source=user_profile---------9----------------",
        "title": "Neural Networks Building Blocks \u2013 Eugenio Culurciello \u2013",
        "text": "Neural networks are made of smaller modules or building blocks, similarly to atoms in matter and logic gates in digital circuits.\n\nOnce you know what the blocks are, you can combine them to solve a variety of problems. Many new building blocks are added constantly, as smart minds discover them\n\nWhat are these blocks? How can we use them? Follow me\u2026\n\nWeights implement the product of an input i with a weight value w, to produce an output o. Seems easy, but addition and multiplications are at the hearth of neural networks.\n\nThey sum up some inputs i and use a non-linearity (added as separate block) to output (o) a non-liner version of the sum.\n\nNeurons use weights w to take inputs and scale them, before summing the values.\n\nIdentity layers just pass the input to the output. Seem pretty bare, but they are an essential block of neural networks.\n\nSee below why. You will not be disappointed!\n\nThey take the inner value of a neuron and transform it with a non-liner function to produce a neuron output.\n\nThe most popular are: ReLU, tang, sigmoid.\n\nThese layers are an array of neurons. Each take multiple inputs and produce multiple outputs.\n\nThe input and output number is arbitrary and of uncorrelated length.\n\nThese building blocks are basically an array of linear combinations of inputs scaled by weights. Weights multiply inputs with an array of weight values, and they are usually learned with a learning algorithm.\n\nLinear layers do not come with non-linearities, you will have to add it after each layer. If you stack multiple layers, you will need a non-linearity between them, or they will all collapse to a single linear layer.\n\nThey are exactly like a linear layer, but each output neuron is connected to a locally constrained group of input neurons.\n\nThis group is often called receptive-field, borrowing the name from neuroscience.\n\nConvolutions can be performed in 1D, 2D, 3D\u2026 etc.\n\nThese basic block takes advantage of the local features of data, which are correlated in some kind of inputs. If the inputs are correlated, then it makes more sense to look at a group of inputs rather than a single value like in a linear layer. Linear layer can be thought of a convolutions with 1 value per filters.\n\nRecurrent neural networks are a special kind of linear layer, where the output of each neuron is fed back ad additional input of the neuron, together with actual input.\n\nYou can think of RNN as a combination of the input at instant t and the state/output of the same neuron at time t-1.\n\nRNN can remember sequences of values, since they can recall previous outputs, which again are linear combinations of their inputs.\n\nAttention modules are simply a gating function for memories. If you want to specify which values in an array should be passed through attention, you use a linear layer to gate each input by some weighting function.\n\nAttention modules can be soft when the weights are real-valued and the inputs are thus multiplied by values. Attention is hard when weight are binary, and inputs are either 0 or passing through. Outputs are also called attention head outputs. More info: nice blog post and another.\n\nThis is basically just a multi-dimensional array. The table can be of any size and any dimensions. It can also be composed of multiple banks or heads. Generally memory have a write function writes to all locations and reads from all location. An attention-like module can focus reading and writing to specific locations. See attention module below. More info: nice blog post.\n\nResidual modules are a simple combination of layers and pass-through layers or Identity layers (ah-ah! told you they were important!).\n\nThey became famous in ResNet: https://arxiv.org/abs/1512.03385, and they have revolutionized neural networks since.\n\nThese units use an RNN, residual modules and multiple Attention modules to gate inputs, outputs and state values (and more) of an RNN, to produce augmented RNN that can remember longer sequence in the future. More info and figures: great post and nice blog post.\n\nNow the hard part: how do you combine these modules to make neural networks that can actually solve interesting problems in the world?\n\nSo far it has been human smart minds that architected these neural network topologies.\n\nBut why, did you not tell me that neural network were all about learning from data? And yet the most important things or all, the network architecture is still designed by hand? What?\n\nPS1: I do wish some smart minds can help me to find automatic ways to learn neural network architectures from data and the problem they need to solve.\n\nIf we can do this, we can also create machines that can build their own circuits topologies, and self-evolve. This would be a blast. And my work would be done. Ah, I can finally enjoy life!\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/segmenting-localizing-and-counting-object-instances-in-an-image-878805fef7fc?source=user_profile---------10----------------",
        "title": "Segmenting, localizing and counting object instances in an image",
        "text": "When we visually perceive the world, we may get a large amount of data. If you take a picture with a modern camera it is > 4 Million pixels and several megabytes of data.\n\nBut really in a picture or scene there is little interesting data we humans consume. It is task dependent, but for example in a scene we look for other animals and humans, their location, their actions. We may look for faces to gage emotions, or intensity and gravity of actions to understand the situation in the overall scene.\n\nWhen driving, we look for traversable road, behavior of other vehicles, pedestrians and moving objects, and pay attention to traffic signs, lights and road markings.\n\nIn most cases, we look for a handful of objects, their x,y,z position, and reject the vast majority of what we call background. Background is anything our task does not require to attend to. People can be background if we are looking for our keys.\n\nSometimes we also need to count, and be able to tell how many objects of one kind are present, and where they are.\n\nIn most cases we look at a scene and want to get this information:\n\nWe may also want to get more tailed information on a second glance, for example facial key-points, position of skeletal key-points in a human figure, and more. An example:\n\nWe will now review how this can be done with neural networks and deep learning algorithms.\n\nWe should understand that human vision works on multiple passes on the visual scene. This means we recursively observe the visual scene in waves, first to get the most crude content in the minimum time, for time sensitive tasks. Then we may glance again and again to find more and more details, for precision tasks. For example in a driving situation we want to know if we are on the road and if there are obstacles. We look at rough features for a fast response. We are not interested in the color or make/model of the car we are about to hit. We just need to brake fast. But if we are looking for specific person in a crowd, we will find people first, and then find their face, and then study their face with multiple glances.\n\nNeural network need not follow the rules and ways of the human brain, but generally it is a good idea to do so in the first iteration of algorithms.\n\nNow, if you run a neural network designed to categorize objects in a large image, you will get several maps at the output. These maps contain the probability of the objects presence in multiple location. But because categorization neural network want to reduce a large amount of pixels to a small amount of data (categorize), then they also lose the ability to precisely localize object instances \u2014 to some extent. See example below:\n\nNote that the output you get is \u201cfor free\u201d meaning we do need to run any other algorithms beside the neural network to find localization probabilities. The resolution of the output map is usually low, and depends on the neural network , its input trained eye size, and the input image size. Usually this is rough, but for many tasks it is enough. What this does not give you is precise instance segmentation of all objects, and precise boundaries.\n\nTo get the most precise boundaries, we use segmentation neural networks, such as our LinkNet, here modified to detect many different kinds of image key-points and bounding boxes.:\n\nThese kind of neural networks are Generative Ladder Networks that use an encoder as a categorization network and a decoder to be able to provide precise localization and image segmentation on the input image plane.\n\nThis kind of network gives the best performance for simultaneously identifying, categorizing and localizing any kind of objects.\n\nHere are results we can obtain with Generative Ladder Networks:\n\nGenerative ladder networks are not very computationally heavy, because the encoder is a standard neural network, and can be designed to be efficient, like eNet or LinkNet. The decoder is an upsampling neural network that can be made asymettrically fast and computationally inexpensive, such as in eNet, or use bypass layers like LinkNet for increased precision.\n\nBypass layers are used to inform the decoder at each layer on how to aggregate features at multiple scales for better scene segmentation. Since the encoder layers downsample the image data in some layers, the encoder has to upsample the neural maps at each layer according to the features found in the encoder.\n\nWe have been arguing and showing for many years that Generative Ladder Networks like LinkNet provide the back-bone for categorization, precise localization with segmentation. Segmentation provides much refined localization in an image, and also provides better training examples for neural networks. The reason is that precise boundary group objects features together more efficiently than imprecise boundaries like bounding boxes. It is obvious to notice that a bounding box will contain a lot of pixel of the background or other categories. Training a neural network with such erroneous labels will decrease the power of categorization of the network, since the background information will confuse its training. We recommend NOT TO USE bounding box.\n\nIn the past the literature has been littered with approaches using bounding boxes, with very inefficient use of neural networks and even poor understanding of the way they work and can be used with parsimony. A list of sub-optimal methods is here: Yolo, SSD Single Shot Multi-Box Detector, R-CNN. A review and comparison of these inferior methods is here \u2014 we note that SSD is the only method that at least tries to use neural network as pyramids of scales to regress bounding boxes.\n\nA list of reasons why these methods are sub-par:\n\nThe recent work from: Focal Loss for Dense Object Detection is more insightful, as it shows that Generative Ladder Networks can be seen as the basic framework that should drive future neural network designs for instance categorization, localization (see Note 1).\n\nBut how can we use networks like LinkNet to perform bounding box regression, key-point detections, and instance counting? This can be done by attaching subnetworks at the output of each each decoder layer as done here and here. These subnetwork require minimal networks and small classifier to be fast and efficient. The design of these networks needs to be performed by experienced neural network architecture engineers. See our recent work here, where we show how one single neural network like LinkNet can perform all tasks mentioned.\n\nNote 1: a recent tutorial on methods for localization, segmentation and Instance-level Visual Recognition also makes a point that model like LinkNet are a common framework for object detection. They call Generative Ladder Networks as: Feature Pyramid Network (FPN). They recognize Generative Ladder Networks have an intrinsic pyramid of scales built in by the encoder downsampling. They also recognize the decoder can upsample images to better localization, segmentation and more tasks.\n\nNote 2: it is not a good idea to try to identify actions from a single image. Actions live in a video space. An image may give you an idea of an action, as it may identify a key frame that relates to an action, but it is not a substitute for the sequence learning require to accurately categorize actions. Do not use these techniques on single frames to categorize actions. You will not get accurate results. Use video-based neural network like CortexNet or similar.\n\nNote 3: Segmentation labels are more laborious to obtain than bounding boxes. It is easier to label an image with rough bounding boxes that to precisely draw contour of all objects manually. This is one reason for the long life of inferior techniques like bounding boxes, dictated by the availability of more and large datasets with bounding boxes. But there are recent techniques that can help segmenting images, albeit maybe not as precisely as human labeling, but that can produce at least a first pass in segmenting a large number of image automatically. See this work (Learning Features by Watching Objects Move) and this as references.\n\nNote 4: The encoder network for Generative Ladder Networks needs to be efficiently designed for realistic performance in real applications. One cannot use a segmentation neural network that takes 1 second to process one frame. Yet most results in the literature are focused on obtaining the best accuracy only. We argue the best metric is accuracy/inference time as reported here. This was the key design for our eNet and LinkNet. Several paper still use VGG as input network, which is the most inefficient model to date.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/adversarial-predictive-networks-3aa7026d53d2?source=user_profile---------11----------------",
        "title": "Adversarial predictive networks \u2013",
        "text": "We wanted to test the idea that predictive neural network like CortexNet would benefit from simultaneously training to:\n\n2- use adversarial training to distinguish between real frames in a videos and the ones generated by the network\n\nWe call this Adversarial Predictive Training. The idea is to improve on the capability of CortexNet to pre-train on more unlabeled data and then use only small amounts of labeled data.\n\nWe modified our new kind of neural network in this way:\n\nwe added a classifier C to predict fake vs real generated frame. Our network discriminator or encoder D is like a standard multi-layer neural network, and so we place the classifier C after all green layers. The blue generative or decoder layer instead are useful to reconstruct future images or representation. More details is here.\n\nIn order to train we use the following steps:\n\n1- we train the network to predict next frames just as in a regular CortexNet\n\n2- once the network can generate decent future frames, we train classifier C to predict whether the input was a real next frame or a generated next frame\n\nWe run steps 1,2 in sequence at every training step.\n\nThese are the G and D blocks in detail:\n\nWe use batch normalization between conv and ReLU.\n\nResults: using the KTH dataset, we pre-train the network with the adversarial predictive method described above, and then used 10% of the data to train the classifier C by freezing the network of the generator.\n\nCase 1: we trained the entire network supervised on the 10% of data. We get a max testing accuracy of 60.43%\n\nCase 2: we used Adversarial Predictive training (fake/real and also predict next frames) on 100% of data, then fine tuned the network on 10% of data. We get a max testing accuracy of 53.33%\n\nCase 3: we used predictive training only (predict next frames, as the original CortexNet) on 100% of data, then fine tuned the network on 10% of data. We get a max testing accuracy of 71.98%\n\nConclusion: as you can see we expected the Case 2 to be better than the case 3. But it did not happen: (53 vs 71% \u2014 case 2 vs 3). Our conclusion is that Adversarial Predictive training produces a conflict between training the classifier on classification of fake/real and the predictive capabilities of the entire network.\n\nPre-training for 33% (62 vs 79% \u2014 case 2 vs 3) and 50% (71% vs 81% \u2014 case 2 vs 3) of the data, instead of only 10% did not change the situation, and using more data defeats the purpose of pre-training on unlabeled data anyway\u2026\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/memory-attention-sequences-37456d271992?source=user_profile---------12----------------",
        "title": "Memory, attention, sequences \u2013",
        "text": "We have seen the rise and success of categorization neural networks. The next big step in neural network is to make sense of complex spatio-temporal data coming from observing and interacting with the real world. We talked before about the new wave of neural networks that operate in this space.\n\nBut how can we use these network to learn complex tasks in the real world? For example, how can I tell my advanced vacuum cleaning robot: \u201cRoomby: you forgot to vacuum the spot under the red couch in the living room!\u201d and get an proper response?\n\nFor this to happen, we need to parse spatio-temporal information with attention mechanism, so we can understand complex instruction and how they relate to our environment.\n\nLet us consider a couple of example applications: summarization of text or video. Consider this text:\n\nIn order to answer to the question: \u201cwho offered a slice of apple?\u201d we need to focus on \u201capple\u201d, \u201capple owner\u201d, \u201cgive\u201d words and concepts. The rest of the story is not relevant. These part of the story need our attention.\n\nA similar situation occurs in video summarization, where a long video can be summarized in a small set of sequences of frames where important actions are performed, and which again require our attention. Imagine you are looking for the car keys, or your shoes, you would focus on different part of the video, and the scene. For each action and for each goal, we need attention to focus on the important data, and ignore the rest.\n\nWell if you think about it, summarization and a focused set of data is important for every temporal sequence, be it translation of a document, or action recognition in video, or the combination of a sentence description of a task and the execution in an environment.\n\nAttention is then one of the most important components of neural networks adept to understand sequences, be it a video sequence, an action sequence in real life, or a sequence of inputs, like voice or text or any other data. It is no wonder that our brain implements attention at many levels, in order to select only the important information to process, and eliminate the overwhelming amount of background information that is not needed for the task at hand.\n\nA great review of attention in neural network is given here. I report here some important diagrams as a reference:\n\nBoth the last 2 figures above implement \u201csoft\u201d attention. Hard attention is implemented by randomly picking one of the inputs y_i with probability s_i. This is a rougher choice than the averaging of soft attention. Soft attention use is preferred because it can be trained with back-propagation.\n\nAttention and memory systems are also described here with nice visualizations.\n\nAttention can come at a cost, as they mention in this article, but in reality this cost can be minimized by hierarchical attention modules, such as the ones implemented here.\n\nIt can do so by stacking multiple layer of attention modules, and with an architecture such as this one:\n\nMultiple attention modules can be used in parallel in a multi-head attention:\n\nIn RNN, time is encoded in the sequence, as inputs and outputs flow one at a time. In a feed-forward neural network, time needs to be represented to preserve the positional encoding. In these attention-driven networks, time is encoded as an added extra input, a sine wave. It is basically a signal that is added to inputs and outputs to represent the passing of time. Notice here the biological parallel with brain waves and neural oscillations.\n\nIf you read the paper to Table 2, you will see these network can save 2\u20133 orders of magnitude of operations! That is some serious saving!\n\nI believe this attention-based network will slowly supplant RNN in many application of neural networks.\n\nOne important piece of work that is also interesting is Fast Weights. This work implements a neural associative memory \u2014 this is a kind of short-term memory that sits in between neural weight (long-term) and recurrent weights (very-fast weights based on input activities). Fast Weights implements a kind of memory that is similar to the neural attention mechanism seen above, where we compare current inputs to a set of stored previous inputs. This is basically what happens in the \u2018Attention model with dot-products\u2019 digram seen above.\n\nIn Fast Weights the input x(t) is the context used to compare to previously stored values h in the figure below.\n\nIf you read the paper you can see that this kind of neural network associative memory can outperform RNN and LSTM networks again, in the same way that attention can.\n\nI thinks this is again mounting evidence that many of the task currently performed by RNN today, could be replaced by less computationally expensive (not to mention using less memory bandwidth and parameters) algorithms like this one.\n\nPlease also look at: Attentive recurrent comparators, which also combine attention and recurrent layers to understand the details of a learning unit.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/uncertainty-in-deep-learning-and-neural-network-i-do-not-feel-that-way-f11ceca4341a?source=user_profile---------13----------------",
        "title": "Uncertainty in deep learning and neural network? I do not feel that way!",
        "text": "Note: this is an old Blogger post from Thursday, June 5, 2014\n\nAs you might know, neural networks are often in the news these days, with many success stories.\n\nNeural networks are now the state-of-the-art algorithm to understand complex sensory data such as images, videos, speech, audio, voice, music, etc.\n\nNeural networks recently got rebranded under the name Deep Learning (DL) or deep neural networks.\n\nIn 2012 they made the news when they outperformed by more than 10% any other algorithm in a industry-standard image dataset:\n\nThey also had similar improvements in speech recognition, up to 20%:\n\nAnd in many other tasks. Recently getting at the level of human performance in familiar face identification:\n\nYet industries and investors are wary.\n\nThey often see new algorithms come and go, almost on a year-to-year basis.\n\nThey say: \u201dwhy do a start-up on deep learning or neural networks? What happens when a better algorithm comes up and swipes you off your tablet?\u201d\n\nIt is a legitimate doubt, however I am certain we should not worry about this anymore.\n\nNeural Networks are here to stay for many years.\n\nThere are 3 strong reasons for this:\n\nIn 2012, when deep neural network algorithms proved to be the best algorithm for understanding complex sensory data10\u201320%, instead of the 1\u20132% typical year by year improvement.\n\nThis is a big difference in algorithm performance in complex data. I have not seen such large improvement in all my career.\n\nImagine a 100-meter dash athlete run in 7.5 seconds, beating everyone else by 2 full seconds when a typical record was usually just 0.1 seconds better before. Wouldn\u2019t you be surprised? \u201cAlmost unreal!\u201d\n\nThe human brain is a large neural network. Deep learning provides large neural network models inspired by biological neural systems, such as the human brain.\n\nThe human brain is the best \u201cprocessor\u201d of complex sensory data in the known universe. We can understand images, videos, voice, sound like no computer can today!\n\nA model of the human brain, such as the artificial neural networks used in deep learning can scale to human performance, as datasets and network topologies improve. An example is Facebook DeepFace mentioned above.\n\nAs we deepen our knowledge of the neural topology of our brain and improve the artificial neural network models, we can reach human performance in many more tasks. And this is happening now: every few months I withness a new result in this direction.\n\nWe have seen many algorithms in the past that cannot scale to the complexity of sensory data. They can do well for a few years, then they disappeared under a new wave of \u201cnew\u201d algorithms. While this has happened, neural networks relentlessly continued to evolve, sometimes outside the limelight, for more than 60 years. We owe this progress to many smart colleagues and researchers.\n\nBut one difference that sets aside neural network from other techniques is that they are a very scalable model. Their ability to understand data can grow with the size of the model and also with the data available to train these models. By adding layers of neurons, different connections and connection topologies, more input data in the form of space and time, the models can provide ever-incrementing abilities to understand the complex data they are trained with, and new data of the same kind.\n\nDeep Learning and neural networks are here to stay. For 10 years at least, or as long as it will take us to get to human performance levels in the understanding of raw sensory data.\n\nAnd if you want to invest and push the future of technology, this is it!\n\nWhy should you believe me? \u201cThis is just your opinion!\u201d, \u201c\u201dWho are you anyway?\u201d\n\nI am a professor and an inventor, an engineer and a scientist. I want my work to change the world. I met US President Obama because of the success of my research. I teach college-level computational neuroscience, micro-chip design, computer-architecture, machine and deep learning, to name a few.\n\nI have 20+ years of experience in the design of neuromorphic systems. I have seen neural networks in analog microchips, digital microchips, computer code, and theory. I have seen many examples of artificial neural systems that work and the ones that do not, both in the hardware and software domains. I have designed many systems myself, tested code and algorithms in first person, not just through the work of others.\n\nMy goal is to help humanity with my knowledge of technology and science. And this is the way to do it now, with deep neural networks!\n\nI am not in love with the algorithm, I love what works and can scale. Deep neural networks work and scale \u2014 right now.\n\nI have seen a lot of things and I have the experience to spend my life on important goals for humanity as a whole. I would not work in this area if I was not strongly convinced of the power and potential of neural networks.\n\nBelieve me now or let time convince you, at the expense of losing a big opportunity.\n\nWhile you decide, all my colleagues and I will continue every day to push the envelope of technology, breaking the barriers that are impeding current computers from understanding images, videos, speech, audio and any complex sensory data.\n\nIt is our choice, our destiny.\n\nAnd it is inevitable.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/will-artificial-intelligence-take-over-the-universe-33e214ef29a9?source=user_profile---------14----------------",
        "title": "Will artificial intelligence take over the universe?",
        "text": "Note: this is an old Blogger post from Sunday, July 19, 2015\n\nRecently some branches of artificial intelligence (AI) have made tremendous progress on low-level cognitive abilities. In particular on low-level understanding of complex data, such as raw images, video streams, audio and voice streams. Some examples are:\n\nI must confess, that although I have been working in the area of artificial neural networks since the beginning of my PhD (almost 20 years), I was surprised by how fast the field is moving. For example, I was really left astonished by the recent neural networks that understand image content and also can learn language models (any language!) to create transcription of images [3].\n\nI was not alone, many colleagues shared with me similar feelings.\n\nLast year, I read \u201cOur Final Invention\u201d, a book by James Barrat. This book describes scenarios in which artificial intelligence would cause the end of the Human Era. It is a good idea to read, at least partly, this book. Recently famous individuals working on technology commented on the future of AI and humans on media channels, creating an echo of worries. AI, they say, will become more intelligent than us human beings, and take over our world. Some of these people include Elon Musk, Stephen Hawkins, Bill Gates, and others [4].\n\nWhen I first approached the book by Barrat, I was skeptical. Mostly aware of the limitation of our algorithms and techniques, I was one of the people that dismissed the issues as being part of an improbable distant future. But it made me think more about the issue. The real questions are:\n\n- is AI capable one day to control the universe we know\n\n- what can we do to prevent AI from considering us a pest or a nuisance, as we consider insects or other lower life forms?\n\n- what is the future of man and machines? Co-exist, replace, diverge?\n\nThe answer: \u201cwe are far from having AI with any capability to take over the world\u201d does not help the debate. It is just a way to delay answering these question, or bypassing the issue altogether. Even if it will take long, if AI reaches those levels of intelligence, then what?\n\nIn the recent months, a large amount of article commented on the doom of AI. You can find most of them with a simple web search. Most of them do not really address the issues, or try to answer the questions, or propose any solution. They mostly feed on ignorance and create fear, the usual way that media seeks attention!\n\nMany people comment on the issue: some are afraid and think of negative outcomes, while other are less worried, or not at all. Some commentators mock robots taking over the world as highly improbable and far-fetched [5]. Some even mock the recent DARPA Robotics Grand Challenge, with his slow and clumsy robots that would represent the current state-of-the-art [6].\n\nIt is true that current AI is still not as capable as we would like it to be. Robots are not able to understand the environment around themselves, especially in real-time, and they cannot even help us on tedious mundane tasks like loading a dish washer, cleaning floors, doing laundry, folding laundry. There are examples of very specific robots that can do some of this, but extremely slowly and executing instructions in a code, not really understanding what they do and how they do it, or how they can do it better. We are still far from that.\n\nBut we will get there! And sooner that most people think.\n\nSo what about those question on AI taking over the world?\n\nI think we should answer some of these. We can create some scenarios. Many scenarios are in Barrat\u2019s book, and other similar books and recent articles.\n\nThis is my view:\n\n- Will AI become more intelligent than humans?\n\nYes, with 100% probability in my view! Within 10\u201315 years, maybe.\n\n- Can we stop AI from becoming more intelligent than us?\n\nI think we cannot. It is our mission in this world to improve human health and the human condition. Progress continues moving forward. Machines that can cure us, make us live longer, send us farther and faster through space, perform our tedious tasks, free us from hard labor, fight our wars, explore and colonize space, etc. \u2014 all needs to progress!\n\nThe byproduct is that these machines will have to become more and more intelligent and also learn to decide more and more on their own.\n\n- What will it do once it is intelligent?\n\nIt might explore the universe, and find purposes of its own. We cannot know for sure.\n\n- What will it do with us?\n\nThat is the tough question! If a much more intelligent and capable being comes to our planet what would it do with us? I really do not know, it depends on his goals. But one thing is for sure: our world will have to change; and we will be exploited in some form. We can think in similar ways for an intelligent and highly capable AI.\n\nBUT let\u2019s not start running away in screams:\n\nThe interaction between the human race and evolved machines will be a mutually beneficial one.\n\nWhy? Because an intelligent AI will recognize that humans have still a lot to give to machines. Our intelligence, although inferior to some advanced AI or beings, it still valuable. It is part of an evolution of our world that took billions of years. If our world is to be part of the AI future, then we will be part of it also. Our knowledge will be an essential component. In fact remember: we are the ones who will create the AI (if it is meant to come from this our world!).\n\nAnd so what will happen?\n\nMachines will find a way to interface to our brains. All of our brains.\n\nThe holy grail in biomedical research is still humans\u2019 most sought goal: to live forever!\n\nWe are in fact trying to find ways to read our brains and be able to download / upload all that defines us as individuals into machines. That way, saved from our decaying bodies, we can live forever, in any form we choose. AI will be able to know everything about the most evolved species on the planet, and all that it discovered in the recent millenia.\n\nAI and humans will fuse together into a single super-being [7].\n\nAll of human brains will be read and installed into the AI system memory. This way AI will use all that we are, we have created and have evolved to be. It is the ultimate exploitation, a combination of powers, a union that will fulfill our human lifelong dream: to extend our existence beyond our biological bodily limitations.\n\nI think of this as natural evolution: we continue to extend our native abilities via machines and engineered systems, so we can surpass the limitations of our physical bodies and minds.\n\nThis is really what evolution has been all about since the beginning of time.\n\nHumans and all their creations will evolve into a singular element that combines all of their knowledge, ideal, goals to fulfill them faster and better together [7].\n\n- Is this what we want?\n\nEvolution does not take into account individual thoughts, or the hopes and plans of one of many species, it operates on a global scale to maximize the use of the resources and extend the species into the future. It is our destiny, and one we cannot escape.\n\nAll this is inevitable.\n\nThere is no right or wrong, and maybe not all of us like this. But this is my prediction. Take it as is: the small opinion of a small man.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/my-story-with-deep-learning-and-neural-networks-part-i-a751c7d148c?source=user_profile---------15----------------",
        "title": "my story with Deep Learning and Neural Networks \u2014 part I",
        "text": "I started working on Deep Learning from 2008, about this time of the year. I was lucky enough to have Yann LeCun as mentor, and one of his talented students: Clement Farabet. Together we set up to make convolutional neural network accelerators. It was just before the field was called \u201cDeep Learning\u201d. But let us zoom out a bit more:\n\nSince my PhD, during the years 1998\u20132004, I was working in the area of Neuromorphic Engineering. I was advised by some of the best in the field: Andreas Andreou, my super-talented PhD Advisor, Ralph-Etienne-Cummings \u2014 also a close advisor, Gert Cauwenberghs \u2014 a third close colleague, and bit indirectly also by Kwabena Boahen \u2014 one of the most famous of the \u201cNeuromorphs\u201d. I was basically standing on the shoulders of giants. I owe them the rest of the my life, interest. And you are reading this memoire because of them!\n\nAnyway, Neuromorphic Engineering is the field that takes inspiration from Biology to create engineered system that can execute all the tasks that biology is best at, like understanding complex data: images, videos, learn, etc.\n\nBiology, or maybe just \u201cneural networks\u201d.\n\nZooming back even further, during my BS at the University of Trieste Italy, I was studying models of the human visual system. I then learned how complex the human brain is, and I started back then caressing the idea of devoting my life to replicating the human brain into synthetic systems. And it was a good life goal, coupled with the fact that by replicating something, one really has to understand how it all works! During my PhD studies 1998\u20132004, understanding neural networks became my life goal. At the time I was connected to various luminaries in the field of electrical engineering, robotics, neuroscience, psychology, from the Neuromorphic Engineering international research group. It really widen my interests, to be able to listen to all the scientific problems in all these seemingly unconnected fields.\n\nBut later I learned more and more that everything is connected. It is like a giant brain. Like a giant neural network! more about this will be discussed later.\n\nSome of Neuromorphic engineering goals were to replicate the human ability to understand the environment. Humans do this mostly visually, after all vision can extend your \u201csensing sphere\u201d the furthest, further than touch, or your body, and even further than the sound your ears can sense. But all of our senses gives the incredible ability to survive in the environment. The world humans lived in was a lot less safe only a few thousands years ago\u2026 In the years 1998\u20132004, during my PhD, I worked on many artificial eyes, or special image sensors capable of getting the right visual information at the right time, possibly compressing the enormous amount of visual data.\n\nAt the time I was interested in working with the industry, but it all seemed so far away from my goals and from the devices I was creating. Industrial production and research on image sensors of the time were all about pushing better cell-phone cameras. Not unlike today. And it seemed so incremental to only think about adding more pixels and increasing speed. But, yes, it was and it is a revolution. I was glad to be connected to the best researchers in the world innovating image sensors, such as Eric Fossum and Gunhee Han, Jun Ohta, to name a few.\n\nAt the time the best neuromorphic image sensors and vision system were created by some of my fellow students and advisors at Johns Hopkins: the talented Jacob Vogelstein, Viktor Gruev, and most notably Shih-Chi Liu and Tobi Delbruck. For example, the high dynamic vision sensor and silicon retina by Tobi was one of the best designed sensors.\n\nBut since about 2003, I became interested more and more in \u201cwhat is in the picture?\u201d, rather than just taking pretty photos, even with our fancy neuromorphic cameras! The reason was it was hard to really squeeze more computation into image sensors. The inevitability of 2D chip manufacturing was keeping us confined in a 2D world. Visionaries like my advisor Andreas knew then that we needed to go 3D, a recent trend in sensors and memories!\n\nAt the time, if you wanted to do more in vision, designing better image sensors was really not the way to go. I was also not impressed with Computer Vision in general. I did not like or appreciate trying to hand-engineer solution to all vision problems and trying to break it down into parts like it was historically done by gestalt psychology and math-oriented computer scientists. It all seemed to me it can be summarized in the (in)famous goal of the MIT paper from 1966, to completely reproduce human vision in one summer project. Such goal, to-date, it has proven a difficult task to tackle!\n\nI was instead attracted to the way the human brain solves the problem.\n\nThe human visual system \u2014 the best vision system in the universe, to my eyes! At least the best to my human eyes, in its ability to perform so many tasks, and feed our on the intelligence of our brains. Often I thought the reason our brain are so developed is mostly because of our visual system, or the co-evolution of both. Here! Food for debate. The human visual system is what lets us move about the environment with ease. I really wanted to reproduce it, to use it in robots and machines, and make computers see in a way similar to how humans see.\n\nThis is now the goal of my life. And in order to do that, to delve into the problem of intelligence, of language, and of technical evolution. All these topics seem to fuse together now. I do not know the details, but some futuristic pictures are clear in my mind. More will be told.\n\nZooming back to 2006\u20137, one of my research goals was to create a neural system that can reproduce the ability of our vision to recognize objects. Then I was truly inspired by Thomas Serre work and thesis, led by the famous Tomaso Poggio. Their MIT works was a true inspiration, and with Shoushun Chen we worked hard at reproducing some of their model in hardware.\n\nThen, one day day in early 2008, I was talking to our distinguished colleague Bernabe Linares-Barranco, who casually mentioned the work of Yann LeCun. And Yann just moved to NYU, and I was then at Yale, and he kindly agreed to come and present a seminar. And what an inspiration he was. He had the most simple model of neurons an hardware developer was asking for! And ways to train large systems! It had \u201clearning\u201d, a concept that escaped my PhD experience, although my close friends and colleagues Roman Genov and Shantanu Chakrabartty were working on Support-Vector Machines and neural networks back in 1998\u20132005. It all made sense to me right away.\n\nSince the year 2008, we started to work on artificial hardware based on programmable logic devices (Xilinx FPGA) to implement deep neural network in hardware. We were very lucky to work with Clement Farabet, who introduced us to Torch, and made our lab later become one of the tool developers. Torch made it easy to create neural network and train them in software. It was a big step forward from previous CNN libraries, which were less hackable and understandable.\n\nOver the next few years, from 2008\u20132011 we worked with Clement Farabet on custom hardware, until he moved to work for his own software company, then acquired in 2014 by Twitter (congratulations!). In 2011 our lab (e-Lab) moved to Purdue university and we started to train a new generation of machine learning experts: Jonghoon Jin, Aysegul Dundar, Alfredo Canziani, and more hardware experts, such as Vinayak Gokhale. Berin Martini continued to perfect the hardware system over the last few years, until Vinayak Gokhale invented a completely new architecture in 2014\u20132015. Our machine learning experts and myself worked on visual tracking, then in unsupervised learning with Clustering Learning and k-means clustering techniques, and also on compilers for our custom hardware machine. We wrote a lot of software for Torch7, and we maintained demonstration code and software well before Caffe and other tools came into life. In the Summer of 2013, all e-Lab worked together to port our hardware system into Xilinx Zynq devices, true embedded systems combining AMR cores and FPGA fabric. We developed low-level code to talk to the device in Linux Device Drivers, and then implemented an entire hardware system and software compiler. We called this system nn-X and presented it for the first time at NIPS 2014, where our hardware demonstration received a wave of attention and recognition in the media.\n\nAt the same time, it was clear we were experts in a key future technology, Deep Learning, and I decided to fund the company Teradeep, in order to commercialize our hardware devices in programmable devices, and also transition them to custom microchips. The grand goal was to make all devices be able to see and perceive the world just like humans do.\n\nThe goal was to create a specialized microchip to run neural networks on a battery powered device, like a cell-phone. And on data-center servers, and on systems for autonomous cars.\n\nIt was early days, most industry did not know what Deep Learning was, it all just seemed \u201canother one of those algorithms\u201d to them. The vision was well alive in our hearts, but was hard to incept into many company officers. It would daunt them later that they missed the starting gun. \u2014 END OF PART I \u2014 FOOTNOTE: 1: This is my story. I hope it can help others to get perspective. Through someone else\u2019s experience, if that is even possible.\n\n2: Massive thanks also go to the Office of Naval Research, ONR and Thomas McKenna, who funded a lot of our research activities."
    },
    {
        "url": "https://towardsdatascience.com/can-we-copy-the-brain-9ddbff5e0dde?source=user_profile---------16----------------",
        "title": "Can We Copy the Brain? \u2013",
        "text": "A review of IEEE Spectrum Special Report: Can We Copy the Brain?\n\nThe IEEE Spectrum this month has a story on synthetic brains. In this article I will review the story and comment on the status of the quest: replicating the human brain in synthetic systems. This article is about neuroscience, neuromorphic, artificial neural networks, deep learning, computing hardware in biology and synthetic, and how all of these come together in the the human grand challenge of creating a synthetic brain at or above human level.\n\nWhy We Should Copy the Brain: we should do this because we want to create intelligent machines that can do the work for us. In order to do our work, machine will have to live in our environment, have senses similar to our own, and be able to accomplish the same kind of tasks. It does not stop here: machine can do more and better than we can most tasks, simply as we do better than other life forms. And we would like them to do things we cannot do, and do better the things we can do. It is called progress, and we need to do this to bypass biological evolution and speed it up. The article has a good summary of what this will be, and what machines will do for us. More comments below in section PS1. For jobs, see PS3.\n\nIn the Future, Machines Will Borrow Our Brain\u2019s Best Tricks: the human brain is one of the most efficient computing machines we know of. In that sense, it is the best \u201cbrain\u201d of the know universe (known to men). And what is a brain? It is a computer that allows us to live our life in our environment. What is life? Oh well, maybe for now, let\u2019s just say our life is devoted to procreate, ensure the best for our offsprings, promote future generation and their success, preserve the best environment for all this to happen (or are we now?).\n\nAnd today us humans are trying to build artificial brains, inspired by our own. And slowly, in the recent years (many more articles and reviews online!), artificial neural networks and deep learning have slowly eroded many gaps between computers and human abilities. It is only inevitable that they will become more and more like a person, because we are actually building them with that goal in mind! We want them to do things for us, as driving our car, providing customer service, being a perfect digital assistants, reading your mind and predicting your needs. And also pervade every instrument and sensor in the world, so they can better assist us to get the right information at the right time, sometimes without us asking for it.\n\nBut building a synthetic brain does not mean we need to copy our own. In fact, that is not our goal at all, our goal is to make an even better one! Our brain is made of cells and biological tissues, and our synthetic brains are made of silicon and wires. The physics of these media is not the same, and thus we only take inspiration from the brain algorithms, as we build better and larger synthetic hardware, one step at the time. We are already dreaming of neural networks that can create computing architectures by themselves. The same way that a neural network trains its weights, it could also train to eliminate the last human input in all this: learn to create the neural network model definition!\n\nEven if we wanted to limit ourselves to creating a clone of our brain, it will still rapidly evolve beyond our capabilities, as one of the goals of building it, is to continuously learn new knowledge and improve behavior. It is thus inevitable we will end up with a \u201cbetter\u201d brain than ours, possibly so much better, we cannot even imagine it. Maybe like our brain is compared to the one of an insect \u2014 and more. There may be no limit to how intelligent and knowledgeable a creature can become.\n\nThe Brain as Computer: Bad at Math, Good at Everything Else: us humans have studied neural networks for a long time now. And we have been studying our brains for a long time also. But we still do not know how we can predict what is going to happen by just looking at a scene, something we do every moment of our life. We still do not know how we learn new concepts, how we add them to what we already know, how we use the past to predict the future, and how we recognize complex spatio-temporal data, such as recognizing actions in the real world. See this for a summary. We also do not know how to best interact with a real or simulated world, or how we learn to interact with the world.\n\nWe may not know all this. But we are making strides. We started by learning to recognize objects (and faces, and road-scenes). We then learned to categorize and create sequences (including speech, text comprehension, language translation, image to text translation, image to image translation, and many more). We are still trying to learn how to learn without a lot of labeled data (unsupervised learning). And we started playing video games, first simple ones, then difficult ones, now very complex ones. It is only a matter of time that AI algorithms will learn about mechanics and physics of our world.\n\nAnd we got really good at it, better than humans in all these tasks \u2014 or some! And we are not planning to stop until we have robots that can do common tasks for us: cook, clean, wash dishes, fold laundry, talk to us (Alexa, Siri, Cortana, etc), understand our feelings and emotions, and many more tasks commonly associated with human intellect and abilities! But how do we get there? We have been very good at having neural networks categorize things, now we need them to predict. Learn long sequences of events, categorizing long sequence of events. And since there are an infinite number of possible events, we cannot train an AI with examples, we do not have all the examples, so we need it to learn on its own. And the best theories of how our brains learns to do this, is by predicting the future constantly, so it knows to ignore all unimportant and previously-seen events, but at the same time know if some event is new. Unsupervised and self-supervised learning will be important components. More here.\n\nNote also that much of this deep learning progress did not come out of neuroscience or psychology, the same way that making good batteries did not come out of alchemy.\n\nComputing hardware is mentioned in this article also, stating that conventional computers may not be as good as some neuromorphic computer approach. We comment on this topic here. There will sure be more efficient hardware coming about, hardware that possibly will be able to run the latest and greatest deep learning algorithms, such as our work here. And it may have neuromorphic components, such as spiking networks and asynchronous communication of sparse data, but it is not this day. This day neuromorphic hardware has yet to run anything similar to the great successes of deep learning algorithms, such as the Google/Baidu speech recognition on mobile phones, Google text translation on the wild on your phone, or tagging of your images in the cloud. These are tangible results we have today, and they use deep learning and they use back-propagation on labeled datasets, and they use conventional digital hardware, soon to be specialized hardware.\n\nWhat Intelligent Machines Need to Learn From the Neocortex: well this is a duh moment. Jeff Hawkins has written a very exciting book, only a decade ago or so. It hugely inspired all my students, our e-Lab, and myself, to work on synthetic brains, and take inspiration from what we know and can measure of the human brain.\n\nBut since then artificial neural networks and deep learning have stolen his thunder. Of course we need to learn to rewire. Of course we are learning a sparse representation. All deep neural networks do this \u2014 see PS2. And of course we need to learn to act in an environment (embodiment), we already do this by learning to play video games and drive cars.\n\nBut of course it does not say how to tackle real-world tasks, because Numenta is still stuck in its peculiar business model where it does not help itself and it does not help the community. It would be better to listen to the community, share its success and fund smart people and startups. Maybe Jeff does think he alone can solve all this and is better than anyone else. We all are victims of this egocentric behavior\u2026\n\nI have to add I agree with Jeff\u2019s frustration on how categorization-centric deep learning algorithms fail to tackle more complex tasks. We have wrote about this here. But as you can read in this link, we are all working in this area, and there will very soon be much advancement, as there has been in categorization tasks. Rest assured of that! Jeff says: \u201cAs I consider the future, I worry that we are not aiming high enough\u201d. If Jeff and Numenta would join, we will all be faster and better off, and retarget our aims.\n\nAI Designers Find Inspiration in Rat Brains: here we get to the culprit of all the problems in brain/cognition/intelligence: studying the brain. I spent more than 10 years trying to build better neuroscience instrumentation, with the goal of helping neuroscientists understand how human perceive the visual world. See this, slide 16-on. This is at a time where people are still poking neurons with 1 or few wires, and making only limited progress at the topics I was mostly interested in: understanding how neural networks are wired, encode information, and build higher-level representation of the real world. Why do I care about this? Because knowing how the brain does some of this would allow us to build a synthetic brain faster, as we would apply principles from biology first, rather that trying to figure out things with trials and errors. And bear in mind biology got there by trial and error anyway, in billions of years of evolution\u2026\n\nWith time, I grew increasingly frustrated with the progress of brain studies and neuroscience, because:\n\nWorking with artificial neural networks allows to surpass many of these limitations, while keeping a variable degree of loyalty to biological principles. Artificial neural networks can be designed on a computer simulations, can run really fast, and can be used today for practical tasks. This is basically what deep learning has been doing in the last 5\u201310 years. Also these systems are fully observable: we know exactly how neuron work and what response they give and what the inputs were in all conditions. We also know in full detail how they got trained to perform in a specific manner.\n\nBut the question is: what biological principle are important to follow? While we have no answer to this questions, we can definitely conclude that if an artificial neural network can solve a practical task, it will be important, regardless of whether it perfectly mimics or not a biological counter-part. Studying a 1mm\u00b3 of cortex and hoping that we can get an idea of how the brain works and learn is ill-founded. We may get much data and details, but all of this can be discarded as the only underlying working principle is of importance. For example: we do not need to know where every molecule or drop of water is in a stream, all we need to know is where it is going on average and what is the average stream size. And for testing these underlying models, we can use our ideas or simulations, or even better, design a system that can design a synthetic system for us. We do not need to reverse-engineer every aspect of a piece of tissue, as it has little relevance to its underlying algorithms and operating principle. The same way that we do not need to know how our ear and vocal cord works, in order to send voice all over the world with radio cell-phones, surpassing the capabilities of any biological entities on this planet. Same can be said for airplane wings.\n\nThe article says: \u201cAI is not built on \u201cneural networks\u201d similar to those in the brain? They use an overly simplified neuron model. The differences are many, and they matter\u201d. This claim has been uttered many times to claim biology has some special property that we do not know of, and that we cannot make any progress without knowing it. This is plain non-sense, as we have made plenty of progress without knowing all the details, in fact this may prove the details are not important. There has not been a single piece of evidence showing that if we add some \u201cdetail\u201d to the simple artificial neuron of deep learning the system can improve in performance. There is no evidence because all neuromorphic system operate on toy models and data, and are not able to scale to the success of deep learning. So no comparison can be made to date. One day this may be the case, but I assure you your \u201cdetail\u201d can simply be encoded as more neurons and more layers or a simple rewiring.\n\nReading this article on Spectrum reminds me that the situation has not changed in the last 5\u201310 years and that we still do not know how much of the brain works and we still do not have the tools to investigate this. There is much information to back this claim, and there have been two large initiative on studying the brain both in USA and Europe, both with very limited success. I am not negative about this field, I am just stating my observations here. I hope some smart minds come and invent new tools, as I have tried and may have failed, for now. But please let us stop poking the brain with a few wires and keep claiming that EEG will one day solve all our problems. I would be a very happy person if we could make some strides in the neuroscience area, but I think the current way of doing things, basic research, goals and tools have to be reset and redesigned.\n\nAnd maybe that is why in this IEEE Spectrum article all neuroscientists say it will take hundreds of years to reach human-level AI, while all researchers in AI say it will take 20\u201350 years. Because in neuroscience so far, there has been little progress towards explaining neural networks, while in AI / deep learning, the progress occurs on a daily basis. I call the the AI/neuroscience divide, which will only grow.\n\nNeuromorphic Chips Are Destined for Deep Learning \u2014 or Obscurity and We Could Build an Artificial Brain Right Now: this we already talked about extensively in this post.\n\nWhy Rat-Brained Robots Are So Good at Navigating Unfamiliar Terrain: note we implemented a similar system here. All it has to do it to remember the view from a certain location. Since navigation is a basic need for anything that moves in an environments, it can be implemented as the representation of space from a certain point of view. Knowing where you are means recalling a certain view.\n\nCan We Quantify Machine Consciousness? Consciousness is just a model of the world and of yourself. Like seeing yourself from another person\u2019s perspective, but fully aware of your feelings and senses. Any brain with enough computational power will be able to do this, whether in silicon or in biology. As we go and build more and more complex artificial neural networks, we see the same pattern: at the end a small number of neurons encode \u201cexperiences\u201d of all kinds. Now they may encode objects and classes, but they can also encode rewards and punishment, and soon long sequences such as actions and deferred reward. These are no different from human experiences, even as far as neuroscience can tell us.\n\nI do not believe consciousness is a prerogative of humans, and any artificial brain large enough will be able to create a model of itself and the environments. In fact, we have already argued this has to happen to be able to predict what happens in an environment and the results of our actions. I do not think very highly of this piece, and I would rather see the authors use their intellect to help us create usable system, rather that conjecture about philosophy and things anyone can say anything about.\n\nThe fixation that bothers me the most is: humans are special, human brain is special, nothing will ever compare, nothing can be better. Well this is all nonsense. In evolutionary history, we see this happening all the time. We have evolved from single cell beings into what we are, and even though we are more capable of other life forms, it does not mean we will always be, or we have a special property. Our DNA is basically the same of many other beings on this planet! We keep evolving, and we are lucky we are now at the top, but it is not mean to last. Evolution goes on, with or without you.\n\nAnd our evolution on this planet is using biological materials. Nothing says that we cannot build intelligence out of rocks and metals, or out of fire and smoke. In fact, it may have already happened, but our limited senses may prevents us from seeing it. It is not very open minded to think humans are special in any way, in fact if you look around, you will see human at some point in the future will be what cockroaches are to us now.\n\nPS1: Why should we copy the brain, part II: Many people think the human brain and human race is special, but if you look at the evolutionary tree, we can see the only advantage we have is that we are at the top of the pyramid. But nothing else. Other lower life forms share a lot with us: the same destiny we would have when we will face an higher intelligence. We may be visited by higher intelligence, or we may create it, as I think will happen in a very short time-frame. Once we create something more intelligence that us, as J\u00fcrgen Schmidhuber says in the article, this intelligence will most probably want to explore and understand the universe we live in, non confined by a biological body and it short (relatively) time-frame.\n\nBut before a true super-human AI will come along, we will try to replicate many other human abilities in computer programs, such as drawing, making art, painting, sculpting, making music, etc. And while at beginning synthetic art may tend to be only as good as humans, it will eventually evolve to a higher form, maybe one that we will never comprehend.\n\nAn interesting movie about this topic is Transcendence. I like how the AI in this movie merges with humanity for the greater good.\n\nPS2: if you have a neuron with 10000 inputs and only 100 are active, this is a sparse signal. If you have a neuron with 100 inputs all active this is not sparse, but the data communicated is the same, hence the efficiency of the system.\n\nPS3: If you worry about the machines taking all our jobs, do not worry! We will not need jobs in a world where machine do all our work, we can just play and have fun! And we do not want the jobs that machines will do anyway, as we are trying to get rid of jobs that are dangerous and boring and spend more time ending up like the humans in Wall-e.\n\nFor more about all you read here: see this video.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/e-lab-produtions-1997-2013-72485faaab24?source=user_profile---------17----------------",
        "title": "e-Lab produtions 1997\u20132013 \u2013 Eugenio Culurciello \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@culurciello/linknet-neural-networks-for-autonomous-driving-de0798145235?source=user_profile---------18----------------",
        "title": "LinkNet: neural networks for autonomous driving \u2013 Eugenio Culurciello \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://towardsdatascience.com/neuromorphic-and-spiking-or-not-894a836dc3b3?source=user_profile---------19----------------",
        "title": "Neuromorphic and Deep Neural Networks \u2013",
        "text": "Disclaimer: I have been working on analog and mixed-signal neuromorphic micro-chips during my PhD, and in the last 10 years have switched to Deep Learning and fully digital neural networks. For a complete list of our works, see here. An older publication from regarding this topic is here (from our group).\n\nNeuromorphic or standard digital for computing neural networks: which one is better? This is a long question to answer. Standard digital neural networks are the kind we see in Deep Learning, with all their success. They compute using digital values of 64-bits and lower, all in standard digital hardware.\n\nThere are many flavors of neuromorphic systems and hardware, which originated largely by the pioneering work of Carver Mead (his book cover on right). They most typically use 1-bit per value, as in spiking neural networks. They can compute in analog or in digital compute units. The main idea is that neurons are free-running and independent units, that are stimulated by the communication of spikes in the form of 1-bit pulses. Neuromorphic neurons can be both complex digital counters or simpler analog integrators.\n\nCommunication is asynchronous and complex. Also it is a lot more energy efficient and noise-resistant to use digital communication, as you can see below.\n\nNeuromorphic hardware in digital was designed by IBM (TrueNorth), and in analog by many research groups (Boahen, Hasler, our lab, etc.). The best system uses analog computation and digital communication:\n\nUsing conventional digital hardware to simulate neuromorphic hardware (SpiNNaker) is clearly inefficient, because we are using 32 bits hardware to simulate 1-bit hardware.\n\nNeural networks require multiply and add operations. The product of an M*M matrix by a M vector require M\u00b2 multiplication and additions, or 2*M\u00b2 operations. They also compute by convolutions, or artificial receptive fields, which basically are identical to matrix-vector multiplications, only that you multiply multiple values (filters) for each output data-point.\n\nNeuromorphic system compute the same operations in a different way, rather than using B bit words for activations and weights, they use 1-bit spike communication from neuron to neuron. Neurons can send spikes on positive and negative lines to activate more or less neurons connected to them. Neuromorphic neurons integrate inputs in a digital counter or an analog integrator, and when a certain threshold is reached, they themselves fire another spike to communicate to other neurons.\n\nNeuron arrays and neural activations: Neurons are organized in an array, and store activations in a memory inside each neuron. This is a problem, because neural network can be large and require many neurons. This makes neuromorphic micro-chip large because neurons cannot be re-used like we do in conventional digital systems. If one wants to re-use neuron arrays for multiple computations in the same sequence, then one has to use analog memories to store neural activations, or convert them to digital and store them into conventional digital memories. Neuromorphic problem: both these options are currently prohibitive. More below in: \u201cAnalog versus digital memories\u201d.\n\nComputing a neural network layer in neuromorphic arrays requires adding the weight value W to the neuron activations. This is simple: no multiplications. A digital multiplier uses L (~30\u2013300) transistors per bit (lower bound: 12 transistors per flip-flop, 2 ff, one xor gate to multiply), while an analog may require just a handful transistor (1 to multiply, ~5\u201310 to bias circuit) to multiply ~5 bits (limited by noise) \u2014 this all depends on how weights are implemented. Neuromorphic computation wins big time by using much less power.\n\nHow are weight implemented? They can be digital or analog. Usually they are digital as analog memories fade during operation and would require large circuitry per weight to avoid leaking. Digital weight add an mount of activation proportional to their digital value.\n\nWhere are these weights stored? Locally there is limited storage capacity per neuron, but if that is not enough one has to resort to an external digital memory. More below in: \u201cAnalog versus digital memories\u201d.\n\nNeuromorphic system use asynchronous communication of spikes in a clock-free design. This allows to save power by only communicating when needed, and not at the tick of a unrelated clock. But this also complicates system design and communication infrastructure. More below in: \u201cSystem design\u201d.\n\nNote: in order to get values in and out of a neuron array, we need to convert digital numbers (such as 8-bit pixels in an image) into spike trains. This can be done by converting the intensity value into a frequency of pulses. Ideally this conversion is only performed at input data and output of array.\n\nNeural network computation requires computing the product of an M*M matrix by a M vector. M is typically in the range of 128\u20134096.\n\nSince a processor would have to compute multiple such operation in a sequence, it will need to swap matrices, relying to an external memory for storage. This requires a digital memory, since values are digital numbers.\n\nAn analog processor would have to convert values from analog to digital and back to perform operations on digital memories. There needs to be an AD/DA at each port from/to memory. These devices need to operate at memory speed, usually on the order of 1GHz, which is at the limit of current converters and requires very large amount of power.\n\nConverting back and forth AD/DA at each layer is prohibitive. Therefore an analog neural network would have to perform multiple layers in a row in analog mode, which is noisy and degrades signals. Also this would require hardware for multiple layers, N layers by M*M hardware multiplier blocks need to be available. This requires large micro-chip silicon area / size.\n\nIn contrast, digital hardware can easily store and retrieve from digital memory, and hardware can be re-used, requiring less micro-chip silicon area / size. In addition, there is no over-head of data conversion.\n\nNeuromorphic spiking networks use clock-less design, and asynchronous digital design. Few engineers are trained to design such systems.\n\nAnalog requires even more expertise and even fewer engineers are available.\n\nNeuromorphic system tend to be more complicated to design than conventional digital ones. They are more error prone, require more design iterations, non-standard tools and micro-chip process variations.\n\nSome system can be superior in sparse computation domains, where instead of computing the entire 2*M\u00b2 operations, they just need to compute a fraction of it. Note: in deep neural networks, inputs are not sparse (images, sounds), but can be sparsified by hidden layers. Many values are zero or very close, and can be highly quantized.\n\nLet us assume here we may have no sparsity in the first 1\u20132 layers, and up to 100-x sparsity in some hidden layers.\n\nThere is no difference between conventional digital system and neuromorphic system, as both can perform sparse operations. We do need hardware that supports sparse matrix multiplications (not currently implemented in many deep learning frameworks and hardware)!\n\nNeuromorphic systems are still hard to design. To scale, they require analog memories or fast and low power AD/DA converters, which are currently not available.\n\nBut they can offer large energy savings is compute is in analog and communication uses inter-event timing.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/a-new-kind-of-deep-neural-networks-749bcde19108?source=user_profile---------20----------------",
        "title": "A new kind of deep neural networks \u2013",
        "text": "There is a new wave of deep neural networks coming. They are the evolution of feed-forward models, that we previously analyzed in detail.\n\nThe new kind of neural networks are an evolution of the initial feed-forward model of LeNet5 / AlexNet and derivatives, and include more sophisticated by-pass schemes than ResNet / Inception. These feedforward neural networks are also called encoders, as they compress and encode images into smaller representation vectors.\n\nThe new wave of neural networks have two important new features:\n\nIt turns out conventional feed-forward neural networks have a lot of limitations:\n\n1- cannot localize precisely: due to downsampling and loss of spatial resolution in higher layers, localization of features / objects / classes is impaired\n\n2- cannot reason about the scene: because they compress an image into a short representation code, they lose information on how the image is composed, and how parts of the image or scene are spatially arranged\n\n2- have temporal instabilities: since they are trained on still images, they did not learn smooth spatio-temporal transformations of objects motion in space. They can recognize categories of objects in some image, but not others, and are very sensitive to adversarial noise and perturbations\n\n3- cannot predict: since they do not use temporal information, feed-forward neural networks provide a new representation code at each frame, only based on the current input, but cannot predict what will happen in the next few frames (note: with exceptions, not trained on videos)\n\nTo surpass these limitation, we need a new kind of network that can project back a learned representation into the input image space, and also that can be trained on temporally coherent sequences of images: we need to train on videos.\n\nThis is a list of advanced features that these new networks can provide:\n\nLet us now examine the details and implementations of these new networks, as follows.\n\nThese models use an encoder and a decoder pair to segment images into parts and objects. Examples are: ENet, SegNet, Unet, DenseNets, ladder networks, and many more.\n\nD modules are standard feed-forward layers. G modules are generative modules, similar to standard feed-forward layers but with de-convolution and upsampling. They also use residual-like connections \u201cres\u201d to connect the representation at each encoder layer to the one of decoder layers. This forces the representation of generative layers to be modulated by the feed-forward representation, and thus have a stronger ability to localize and parse the scene into objects and parts. \u201cx\u201d is the input image and \u201cy\u201d is the output segmentation at the same time step.\n\nThese network can perform segmentation, scene-parsing and precise localization, but do not operate in the temporal domain, and have no memory of the past frames.\n\nThe encoder to decoder by-pass at each layer recently helped these network to achieve state-of-the-art performance.\n\nOne of the newest deep neural network architectures adds recursion to generative ladder networks. These are Recursive and Generative ladder networks (REGEL, we call them CortexNet models), and they are one of the most complex deep neural network models to date, at least for image analysis.\n\nHere is a 3-layer model of one network we currently use:\n\nD and G modules are practically identical to the ones in Generative ladder networks described above. These network adds a recurrent path \u201ct-1\u201d from each G module to the respective D module in the same layer.\n\nThese networks take a sequence of frames from a video as input x[t], and at each time-step they predict the next frame in the video y[t+1], which is close to x[t+1], if the prediction is accurate.\n\nSince this network can measure the error between the prediction and the actual next frame, it knows when it is able to predict an input or not. If not, it can activate incremental learning, something feed-forward networks are not able to do. It is thus able to perform inherent online learning.\n\nWe think this is a very important features of machine learning that is a prerogative of predictive neural networks. Without this feature, network are not able to provide a true predictive confidence signals, and are not able to perform effective incremental learning.\n\nThese networks are still under study. Our advice: keep and eye on them!\n\nRecursive generative networks are one possible predictive model. Alternatively, the predictive coding computational neuroscience model can provide prediction capabilities and be arranged as hierarchical deep neural networks.\n\nHere is an example of a 2-layer model:\n\nRao and Ballard model and Friston implementation compute an error \u201ce\u201d at each layer between \u201cA\u201d modules (similar to D modules above ladder networks) and \u201cR/Ay\u201d modules (similar to G modules of above ladder networks). This error \u201ce\u201d represents, at each layer, the ability of the network to predict the representation. Error \u201ce\u201d is then forwarded as input to the next layer. \u201cR\u201d is a convolutional RNN/LST module, and \u201cAy\u201d is similar to \u201cA\u201d module. \u201cR\u201d and \u201cAy\u201d can be also combined into a single recurrent module. In the first layer \u201cx\u201d is the input frame.\n\nThe problem with this model is that this network is very different from standard feed-forward neural networks. It does not create a hierarchical representation at higher layer that create combination of features of lower layers, rather, these predictive network compute the representation of residual errors of previous layers.\n\nAs such, they are a bit reminiscent of residual feed-forward networks, but in practice forcing these networks to forward errors does not lead them to learn an effective hierarchical representation at higher layers. As such, they are not able to effectively perform other tasks based on upper-layer representations, such as categorization, segmentation, action recognition. more experiments are needed to present these limitations.\n\nThis model has been implemented in PredNet by Bill Lotter and David Cox. A similar model is also from Brain Corporation.\n\nSpratling predictive coding model projects the representation y to upper layers, not the error \u201ce\u201d, as was performed in Friston models above. This make this network model more compatible with hierarchical feedforward deep neural networks and avoid learning moments of errors in the upper layers.\n\nHere is an example of a 2-layer model:\n\nThis model can be essentially rewritten and simplified to the Recurrent Generative ladder model we have seen above. This is because \u201cR\u201d and \u201cAy\u201d can be combined into a single recurrent module.\n\nGenerative Adversarial Networks (GAN) are a very popular model that is able to learn to generate samples from a distribution of data. The new networks model presented here are superior to GAN because:\n\nModels like CortexNet are reminiscent of Pixel recurrent networks and its various implementations (PixelCNN, Pixel CNN++, WaveNet, etc). These model aim at modeling the distribution of input data: (\u201cOur aim is to estimate a distribution over natural images that can be used to tractably compute the likelihood of [data] and to generate new ones.\u201d). They only focus on generating new realistic data samples, but have not shown to learn representations for real-life tasks. These models are also very slow in inference.\n\nA paper on this topic is here. CortexNet is still under study and evaluation. For example the most recent PredNet paper presents a comparison of predictive coding and ladder networks, where PredNet wins on some tasks. PredNet was used to perform orientation-invariant face classification, using higher layer representation. Also it can predict steering angles in a dataset, but mostly using simple motion filter from the first layer of the network. This tasks does not require a hierarchical decomposition of features.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/intuitionmachine/navigating-the-unsupervised-learning-landscape-951bd5842df9?source=user_profile---------21----------------",
        "title": "Navigating the Unsupervised Learning Landscape \u2013 Intuition Machine \u2013",
        "text": "Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with little data. Very little data.\n\nToday Deep Learning models are trained on large supervised datasets. Meaning that for each data, there is a corresponding label. In the case of the popular ImageNet dataset, there are 1M images labeled by humans. 1000 images for each of the 1000 classes. It can take some effort to create such dataset, many months of work. Imagine now creating a dataset with 1M classes. Imagine having to label each frame of a video dataset, with 100M frames. This is not scalable.\n\nNow, think about how you got trained when you were very little. Yes you got some supervision, but when your parents told you that is a \u201ccat\u201d they would not tell you \u201ccat\u201d every split second you were looking at a cat for the rest of your life! That is what supervised learning is today: I tell you over and over what a \u201ccat\u201d is, maybe 1M times. Then your Deep Learning model gets it.\n\nIdeally, we would like to have a model that behaves more like our brain. That needs just a few labels here and there to make sense of the multitude of classes of the world. And with classes I mean objects classes, action classes, environment classes, object parts classes, and the list goes on and on.\n\nAs you will see in this review, the most successful models are the ones that predict future representation of a video. One issue that many of these techniques have, and are trying to resolve, is that training for a good overall representation needs to be performed on videos, rather than still images. This is the only way to apply the learned representation to real-life tasks.\n\nThe main goal of unsupervised learning research is to pre-train a model (called \u201cdiscriminator\u201d or \u201cencoder\u201d) network to be used for other tasks. The encoder features should be general enough to be used in a categorization tasks: for example to train on ImageNet and provide good results, as close as possible as supervised models.\n\nUp to date, supervised models always perform better than unsupervised pre-trained models. That is because the supervision allows to model to better encode the characteristics of the dataset. But supervision can also be decremental if the model is then applied to other tasks. In this regards, the hope is that unsupervised training can provide more general features for learning to perform any tasks.\n\nIf real-life applications are the targets, as in autonomous driving, action recognition, object detection and recognition in live feeds, then these algorithms need to be trained on video data.\n\nOriginated largely from Bruno Olshausen and David Field in 1996. This paper showed that coding theory can be applied to the receptive field in the visual cortex. They showed that the primary visual vortex (V1) in our brain uses principles of sparsity to create a minimal set of base functions that can be also used to reconstruct the input image.\n\nYann LeCun group also worked a lot in this area. In this page you can see a great animation of how sparse filters V1-like are learned.\n\nStacked-auto encoders are also used, by repeating the process of training greedily layer by layer.\n\nOne technique uses k-means clustering to learn filters at multiple layers.\n\nOur group named this technique: Clustering Learning, Clustering Connections and Convolutional Clustering, which very recently achieved very good results on the popular STL-10 unsupervised dataset.\n\nOur work in this area was developed independently to the work of Adam Coates and Andrew Ng.\n\nRestricted Boltzmann machines (RBMs), deep Boltzmann machines (DBMs), Deep belief networks (DBNs) have been notoriously hard to train because of the numerical difficulties of solving their partition function. As such they have not been used widely to solve problems.\n\nGenerative models try to create a categorization (discriminator or encoder) network and a model that generates images (generative model) at the same time. This method originated from the pioneering work of Ian Goodfellow and Yoshua Bengio. Here is a great and recent summary of GAN by Ian.\n\nThe generative adversarial model by Alec Radford, Luke Metz, Soumith Chintala named DCGAN instantiates one such model that got really awesome results.\n\nA good explanation of this model is here. See this system diagram:\n\nThe DCGAN discriminator is designed to tell if an input image is real, or coming from the dataset, or fake, coming from the generator. The generator takes a random noise vector (say 1024 values) at the input and generates an image.\n\nIn the DCGAN, the generator network is:\n\nwhile the discriminator is a standard neural network. See code below for details.\n\nThe key is to train both networks in parallel while not completely overfitting and thus copying the dataset. The learned features need to generalize to unseen examples, so learning the dataset would not be of use.\n\nTraining code of DCGAN in Torch7 is also provided, which allow for great experimentation.\n\nAfter both generator and discriminator network are trained, one can use both. The main goal was to train a nice discriminator network to be used for other tasks, for example categorization on other datasets. The generator can be used to generate images out of random vectors. These images have very interesting properties. First of all, they offer smooth transitions from the input space. See an example here, where they show the images produced by moving between 9 random input vectors:\n\nAlso the input vector space offers mathematical properties, showing the learned features are organized by similarity:\n\nThe smooth space learned by the generator suggests that the discriminator also has similar properties, making it a great general feature extractor for encoding images. This should help the typical problem of CNN trained in discontinuous image datasets, where adversarial noise makes them fail.\n\nA recent update to the GAN training, provided a 21% error rate on CIFAR-10 with only 1000 labeled samples.\n\nA recent paper on infoGAN was able to produce very sharp images with image features that can be dis-entangled and have more interesting meaning. They, however, did not report the performance of the learned features in a task or dataset for comparison.\n\nInteresting summary on generative models are also here and here.\n\nAnother very interesting example is given here where the authors use generative adversarial training to learn to produce images out of textual descriptions. See this example:\n\nWhat I appreciate the most about this work is that the network is using the textual description as input to the generator, as opposed to random vectors, and can thus control accurately the output of the generator. A picture of the network model is here:\n\nThese models learn directly from unlabeled data, by devising unsupervised learning tasks that do not require labels, and learning aims at solving the task.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles is a clever trick. The author break the image into a puzzle and train a deep neural network to solve the puzzle. The resulting network has one of the highest performance of pre-trained networks.\n\nUnsupervised learning of visual representations from image patches and locality is also a clever trick. Here they take two patches of the same image closely located. These patches are statistically of the same object. A third patch is taken from a random picture and location, statistically not of the same object as the other 2 patches. Then a deep neural network is trained to discriminate between 2 patches of same object or different objects. The resulting network has one of the highest performance of pre-trained networks.\n\nUnsupervised learning of visual representations from stereo image reconstructions takes a stereo image, say the left frame, and reconstruct the right frame. Albeit this work was not aimed at unsupervised learning, it can be! This method also generates interesting 3D movies form stills.\n\nUnsupervised Learning of Visual Representations using surrogate categories uses patches of images to create a very large number of surrogate classes. These image patches are then augmented, and then used to train a supervised network based on the augmented surrogate classes. This gives one of the best results in unsupervised feature learning.\n\nUnsupervised Learning of Visual Representations using Videos uses an encoder-decoder LSTM pair. The encoder LSTM runs through a sequence of video frames to generate an internal representation. This representation is then decoded through another LSTM to produce a target sequence. To make this unsupervised, one way is to predict the same sequence as the input. Another way is to predict future frames.\n\nAnother paper (MIT: Vondrick, Torralba) using videos with very compelling results is here. The great idea behind this work is to predict the representation of future frames from a video input. This is an elegant approach. The model used is here:\n\nOne problem of this technique is that a standard neural network trained on static frames is used to interpret a video. Such networks does not learn the temporal dynamics of video and the smooth transformation of objects moving in space. Thus we argue this network is ill-suited to predict future representations in video.\n\nTo overcome this issue, our group is creating a large video dataset eVDS created to train new network models (recursive and feed-back) directly on video data.\n\nPredictive deep neural networks are models that are designed to predict future representations of the future.\n\nPredNet is a network designed to predict future frames in video. See great examples here: https://coxlab.github.io/prednet/\n\nPredNet is a very clever neural network model that in our opinion will have a major role in the future of neural networks. PredNet learns a neural representation that extends beyond the single frames of supervised CNN.\n\nPredNet combine bio-inspired bi-directional [models of the human brain] (https://papers.nips.cc/paper/1083-unsupervised-pixel-prediction.pdf). It uses predictive coding and using [feedback connections in neural models] (http://arxiv.org/abs/1608.03425). This is the PredNet module and example of 2 stacked layer:\n\nThis model also has the following advantages:\n\nOne issue with PredNet is that predicting the future input frame is a relatively easy task some simple motion-based filters at the first layer. In our experiments PredNet this learns to do a great job at re-constructing an input frame, but higher layer do not learn a good representations. In our experiments, higher layer in fact are not able to solve simple tasks like categorization.\n\nPredicting the future frame is in fact not necessary. What we would like to do is to predict future representations of next frames, just like Carl Vondrick does.\n\nA very interesting model is the PVM from BrainCorporation. This model aims at capturing bidirectional and recurrent connections in the brain, and also provide local layer-wise training.\n\nThis model is very interesting because it offers connectivity similar to new network models (recursive and feed-back connections). These connections provide temporal information and generative abilities.\n\nIt is also interesting because it can be trained locally, with each PVM unit trying to predict its future output, and adjusting only local weights appropriately. This is quite different from the way deep neural network are trained today: back-propagating errors throughput the entire network.\n\nThe PVM unit is given below. It has inputs from multiple lower layers, to which it also provides feedback, and lateral connections. The feedback is time-delayed to form recurrent loops.\n\nMore details about the PVM system are given here.\n\nThis recent paper (April 2017) trains unsupervised models by looking at motion of objects in videos. Motion is extracted as optical flow, and used as a segmentation mask for moving objects. Even though optical flow signal does not give anywhere close to a good segmentation mask, the averaging effect of training on a large data allow the resulting network to do a good job. Examples below.\n\nThis work is very exciting because it is follows neuroscience theories of how the visual cortex develops by learning to segment moving objects.\n\nIs your to make.\n\nUnsupervised training is very much an open topic, where you can make a large contribution by:\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/snowflake-ae51c238ead6?source=user_profile---------22----------------",
        "title": "Snowflake \u2013 Eugenio Culurciello \u2013",
        "text": "Deep convolutional neural networks (CNNs) are the deep learning model of choice for performing object detection, classification, semantic segmentation and natural language processing tasks. CNNs require billions of operations to process a frame. This computational complexity, combined with the inherent parallelism of the convolution operation make CNNs an excellent target for custom accelerators. However, when optimizing for different CNN hierarchies and data access patterns, it is difficult for custom accelerators to achieve close to 100% computational efficiency. Our team at Purdue University has designed Snowflake, a scalable and efficient accelerator that is agnostic to CNN workloads, and was designed with the primary goal of optimizing computational efficiency.\n\nSnowflake is able to achieve a computational efficiency of over 91% on entire modern CNN models, and 99% on some individual layers. Implemented on a Xilinx Zynq XC7Z045 SoC is capable of achieving a peak throughput of 128 G-ops/s and a measured throughput of 100 frames per second and 120 G- ops/s on the AlexNet CNN model, 36 frames per second and 116 G-ops/s on the GoogLeNet CNN model and 17 frames per second and 122G-ops/s on the ResNet-50 CNN model. To the best of our knowledge, Snowflake is the only implemented system capable of achieving over 91% efficiency on modern CNNs and the only implemented system with GoogLeNet and ResNet as part of the benchmark suite.\n\nHere are some of Snowflake results and performance:\n\nAverage performance on AlexNet is above 94%.\n\nThe figure above shows the size of weights and maps data moved to and from memory by the Snowflake system. The left axis is for the maps and the right axis is for the weights. The numbers written above each point are the bandwidth in GB/s. AlexNet requires an average bandwidth of 1.53 GB/s.\n\nAverage performance on GoogleNet is above 91%.\n\nAverage performance on ResNet-50 is above 95%.\n\nBelow is a comparison with recent literature. Snowflake is the most efficient deep neural network accelerator to date."
    },
    {
        "url": "https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5?source=user_profile---------23----------------",
        "title": "Computation and memory bandwidth in deep neural networks",
        "text": "Memory bandwidth and data re-use in deep neural network computation can be estimated with a few simple simulations and calculations.\n\nDeep neural network computation requires the use of weight data and input data. Weights are neural network parameters, and input data (maps, activations) is the data you want to process from one layer to the next.\n\nIn general, if a computation re-uses data, it will require less memory bandwidth. Re-use can be accomplished by:\n\nIf there is no input or weight data re-use, then the bandwidth is at a maximum for a given application. Let us give some examples:\n\nLinear layers: here a weight matrix of M by M is used to process a vector of M values with b bits. Total data transferred is: b(M+M\u00b2) or ~bM\u00b2\n\nIf the linear layer is used only for one vector, it will require to send the entire M\u00b2 matrix of weights as computation occurs. If your system has T operations/second of performance, then the time to perform the computation is TbM\u00b2. Given than bandwidth BW = total data transferred / time, in case of linear layers BW = T.\n\nThis means that if your system has 128 G-ops/s of performance, you will need a bandwidth of more than 128 GB/s to perform the operation at full system efficiency (provided, of course that the system can do this!).\n\nOf course if you have multiple inputs for the same linear layer (multiple vectors that need to be multiplied by the same matrix) then: BW = T/B, where B is the number of vectors or Batch.\n\nConvolutions: for convolution operation, the bandwidth requirements are usually lower, as an input map data can be used in several convolution operation in parallel, and convolution weights are relatively small.\n\nFor example: a 13 x 13 pixel map in a 3x3 convolution operation from 192 input maps to 192 output maps (as, for example, in Alexnet layer 3) requires: ~4MB weight data and ~0.1MB input data from memory. This may require about 3.2 GB/s to be performed on a 128 G-ops/s system with ~99% efficiency (SnowFlake Spring 2017 version). The bandwidth usage is low is because the same input data is used to compute 192 outputs, albeit with different small weight matrices.\n\nRNN: memory bandwidth for recurrent neural networks is one of the highest. Deep Speech 2 system or similar use 4 RNN layers of 400 size (see here and here). Each layer uses the equivalent of 3 linear-layer-like matrix multiplications in a GRU model. During inference the input batch is only 1 or a small number, and thus running these neural network requires the highest amount of memory bandwidth, so high it usually it is not possible to fully utilize even efficient hardware at full utilization.\n\nIn order to visualize these concepts, refer to this figure:"
    },
    {
        "url": "https://aboveintelligent.com/artificial-intelligence-past-present-future-cb954fc7da3b?source=user_profile---------24----------------",
        "title": "Artificial Intelligence: past, present, future \u2013",
        "text": "Deep Learning and neural networks are quickly becoming a dominant unified algorithm to extract information from unstructured data. Using standard gradient-descent learning algorithms and scalable models, they have been a revolution in image understanding, video summarization, captioning, scene understanding, speech recognition, text translation, and many more success stories coming on a almost daily basis! We will discuss how we can progress the field into more general artificial intelligence, so that these algorithms can continue to solve problems for us and take on more and more complex applications.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/our-gpu-machines-6d14a28511b8?source=user_profile---------25----------------",
        "title": "Our GPU machines \u2013 Eugenio Culurciello \u2013",
        "text": "Specs 4-GPU Machine for deep learning \u2014 that we use for Deep Learning work and research!\n\nTotal: ~2000$ without GPUs. About 1/2\u20131/4 of the price of: https://lambdal.com/deep-learning-devbox\n\n6.1) Primary Storage \u2014 Samsung 950 PRO 512 GB- We need a large and fast primary disk to store the data and feed the GPUs.\n\n6.2) Secondary storage \u2014 HGST 4TB \u2014 We have the two of these in each mcahine\n\n8) GPU \u2014 GTX 1080 or Titan X latest models \u2014 4 of these.\n\nApproximate cost: about $6000 with Titan X GPUs, $4000 with GTX 1080 or below.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/hardware-for-deep-learning-8d9b03df41a?source=user_profile---------26----------------",
        "title": "Hardware for Deep Learning \u2013",
        "text": "Deep Learning\u2019s recent success is unstoppable. From categorizing objects in images and speech recognition, to captioning images, understanding visual scenes, summarizing videos, translate language, paint, even produce images, speech, sounds and music!\n\nThe results are amazing, and so the demand will rise. Imagine you are Google or Facebook or Twitter: after you find a way to \u201cread\u201d the content of images and videos to make a better model of your users, what they like, what they talk about, what they recommend, and what they share. What would you do? You would probably like to do more of it!\n\nMaybe you run a version of ResNet / Xception / denseNet to classify user images into thousands of categories. And if you are one of the internet giants, you have lots of servers and server farms, so ideally you want to run Deep Learning algorithms on this existing infrastructure. And it works for a while\u2026 until you realize those servers that you were using to parse text, now have to do > 1 Million times the operations that have to do before to run categorization of single images. And data from use trickles down faster and faster. 300 hours of video for each minute of real life!.\n\nServer farms consume a lot of power and if we need to use 1M more infrastructure to process images and videos, we will need to either build a lot of power plants, or use more efficient ways to do Deep Learning in the cloud. An power is hard to come by, so we better take the efficiency route going forward.\n\nBut data centers are only one of the areas where we need more optimized microchips and hardware for Deep Learning solutions. In an autonomous car it may be ok to place a 1000 Watt computing system (albeit that will also use battery/fuel), but in many other applications, power is a hard limit. Think drones, robots, cell-phones, tablets and other mobile devices. These all need to run on a few watts of power budget, if not below 1 W.\n\nAnd there are a lot of consumer products, like smart cameras, and augmented reality goggle and devices, that also need to consume little power, and may not want to use cloud computing solutions for privacy issues.\n\nAnd with our homes becoming smarter and smarter one can see that many devices will need to use Deep Learning applications, collect and process data on a continuous basis.\n\nSo we need new hardware, one that is more efficient than Intel-Xeon-powered servers. An Intel server CPU may consume 100\u2013150 W and may also need a large system with cooling to support the performance.\n\nWhat are other options?\n\nGPUs are processors designed to generate polygon-based computer graphics. In the recent years, given the sophistication and need for realism of recent computer games and graphic engines, GPUs have accumulated large processing powers. NVIDIA is leading the game, producing processors with several thousand cores designed to compute with almost 100% efficiency. Turns out these processors are also well suited to perform the computation of neural networks, matrix multiplications. Notice matrix-vector multiplications are considered \u201cembarrassingly parallel\u201d because they can be parallelized with easy algorithm scaling (they lack the branching and thus do away with little cache misses).\n\nThe Titan X is one favorite workhorse for training Deep Learning models. With more than 3500 cores, it can deliver more than 11 Tera-flops. More information on tested performance is here.\n\nThe fight between Intel CPUs and NVIDIA GPUs favored the latter because of the large amount of cores of GPUs (~3500 vs 16 of an Intel Xeon, or 32 of Xeon-Phi), offsetting the 2\u20133 faster speed of CPU clocks. The GPUs cores are streamlined version of the more complex (branch prediction and pipelined) CPU cores, but having so many of them enables higher level of parallelism and thus more performance.\n\nAt this time GPUs are the norm in training Deep Learning systems, be that convolutional / CNN or recurrent neural networks / RNN. They can train on large batches of images of 128 or 256 images at once in just a few milliseconds. But they consume ~250 W and require a full PC to support them, with additional 150 W of power. No less than 400 W may go into a high-performance GPU system.\n\nThis is not an option for augmented reality goggles, drones, cell-phones, mobile devices, and small robots. And even in a future consumer-grade autonomous car this power budget is not acceptable.\n\nNVIDIA is working hard on more power-efficient devices, such as the Tegra TX1 and TX2 (12 W and ~100 G-flops/s TX1 performance on deep neural nets, more the TX2) and the more powerful Drive PX (250 W, like a Titan X).\n\nNotice also that in the case of autonomous cars, and smart cameras, where live video is necessary, image batching is not possible, as video has to be processed in real time for timely responses.\n\nIn general GPUs deliver ~5 G-flops/s per W of power. We need to do better than this if we want mobile systems to deploy deep learning solutions!\n\nModern FPGA devices such as the ones from Xilinx are the Lego of electronics. One can build entire custom microprocessors and complex heterogenous systems using their circuits as building blocks. And in the recent years, FPGA started sporting more and more multiply-accumulate computing blocks. These DSP blocks, as they are called, can perform multiplications, and can be arrayed together to perform many of them in parallel.\n\nWe have been working for more than 10 years on using FPGA for neural networks. Our work started from initial pioneering work from Yann LeCun team at NYU, and in particular of Clement Farabet. Our collaborative work produced NeuFlow a complex data-flow processor for running neural networks.\n\nDuring the years 2011 to early 2015, we perfected a completely new design called nn-X. This work was lead by Berin Martini and Vinayak Gokhale (from our lab). The system delivered up to 200 G-ops/s on a 4 W budget, effectively 50 G-ops/s/W, or almost 10x more than GPUs.\n\nSystems with similar design constrains will also be limited in performance.\n\nWhat is needed is a system with data cache that can use arbitrary groups of DPS units to effectively use close to 100% of the resources. One such system is Microsoft Catapult and our own SnowFlake accelerator with close to 100% utilization.\n\nMicrosoft uses Altera devices to achieve record performance in executing deep neural networks. Unfortunately this is not a commercial system, but rather one of Microsoft data-center assets, and thus is not yet available to the public. The Chinese Internet giant Baidu also followed route.\n\nQualcomm, AMD, ARM, Intel, NVIDIA, are all working hard on integrating custom microchips into their existing solutions. Nervana and Movidius (now both at Intel)have or are developing integrated solutions. SoC can provide ~10x better performance than FPGA system on the same technology node, and more in some specific architectures. As the power of SoC and processors becomes lower and lower, the differentiation will come from new integrated memory systems, and the efficient use of bandwidth to external memory. In this area 3D memory integrated as systems-on-a-package are a way to decrease power by at least 10x.\n\nDSPs have been around for a long time, and were born to perform matrix arithmetics. But to date, no DSP has really provided any useful performance or device that can compete with GPUs. Why is that? The main reason is the number of cores. DSP were mainly used for telecommunication systems, and did not need to have more than 16 or 32 cores. Their workload just did not need it. Instead GPU workloads kept increasing more and more in the recent 10\u201315 years, thus requiring more and more cores. At the end, since the year 2006 or so, NVIDIA GPUs were superior in performance to DSPs.\n\nTexas Instruments continue to develop them, but we have not seen any competitive performance out of these. And many DSP have also been supplanted by FPGAs.\n\nQualcomm use DSPs in their SoC, and they provide acceleration, but there is no enough detail to date to compare them to other solutions.\n\nIs for us to make. Stay interested, focused and active!\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://towardsdatascience.com/neural-network-architectures-156e5bad51ba?source=user_profile---------27----------------",
        "title": "Neural Network Architectures \u2013",
        "text": "Deep neural networks and Deep Learning are powerful and popular algorithms. And a lot of their success lays in the careful design of the neural network architecture.\n\nI wanted to revisit the history of neural network design in the last few years and in the context of Deep Learning.\n\nFor a more in-depth analysis and comparison of all the networks reported here, please see our recent article. One representative figure from this article is here:\n\nReporting top-1 one-crop accuracy versus amount of operations required for a single forward pass in multiple popular neural network architectures.\n\nIt is the year 1994, and this is one of the very first convolutional neural networks, and what propelled the field of Deep Learning. This pioneering work by Yann LeCun was named LeNet5 after many previous successful iterations since the year 1988!\n\nThe LeNet5 architecture was fundamental, in particular the insight that image features are distributed across the entire image, and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters. At the time there was no GPU to help training, and even CPUs were slow. Therefore being able to save parameters and computation was a key advantage. This is in contrast to using each pixel as a separate input of a large multi-layer neural network. LeNet5 explained that those should not be used in the first layer, because images are highly spatially correlated, and using individual pixel of the image as separate input features would not take advantage of these correlations.\n\nLeNet5 features can be summarized as:\n\nIn overall this network was the origin of much of the recent architectures, and a true inspiration for many people in the field.\n\nIn the years from 1998 to 2010 neural network were in incubation. Most people did not notice their increasing power, while many other researchers slowly progressed. More and more data was available because of the rise of cell-phone cameras and cheap digital cameras. And computing power was on the rise, CPUs were becoming faster, and GPUs became a general-purpose computing tool. Both of these trends made neural network progress, albeit at a slow rate. Both data and computing power made the tasks that neural networks tackled more and more interesting. And then it became clear\u2026\n\nIn 2010 Dan Claudiu Ciresan and Jurgen Schmidhuber published one of the very fist implementations of GPU Neural nets. This implementation had both forward and backward implemented on a a NVIDIA GTX 280 graphic processor of an up to 9 layers neural network.\n\nIn 2012, Alex Krizhevsky released AlexNet which was a deeper and much wider version of the LeNet and won by a large margin the difficult ImageNet competition.\n\nAlexNet scaled the insights of LeNet into a much larger neural network that could be used to learn much more complex objects and object hierarchies. The contribution of this work were:\n\nAt the time GPU offered a much larger number of cores than CPUs, and allowed 10x faster training time, which in turn allowed to use larger datasets and also bigger images.\n\nThe success of AlexNet started a small revolution. Convolutional neural network were now the workhorse of Deep Learning, which became the new name for \u201clarge neural networks that can now solve useful tasks\u201d.\n\nIn December 2013 the NYU lab from Yann LeCun came up with Overfeat, which is a derivative of AlexNet. The article also proposed learning bounding boxes, which later gave rise to many other papers on the same topic. I believe it is better to learn to segment objects rather than learn artificial bounding boxes.\n\nThe VGG networks from Oxford were the first to use much smaller 3\u00d73 filters in each convolutional layers and also combined them as a sequence of convolutions.\n\nThis seems to be contrary to the principles of LeNet, where large convolutions were used to capture similar features in an image. Instead of the 9\u00d79 or 11\u00d711 filters of AlexNet, filters started to become smaller, too dangerously close to the infamous 1\u00d71 convolutions that LeNet wanted to avoid, at least on the first layers of the network. But the great advantage of VGG was the insight that multiple 3\u00d73 convolution in sequence can emulate the effect of larger receptive fields, for examples 5\u00d75 and 7\u00d77. These ideas will be also used in more recent network architectures as Inception and ResNet.\n\nThe VGG networks uses multiple 3x3 convolutional layers to represent complex features. Notice blocks 3, 4, 5 of VGG-E: 256\u00d7256 and 512\u00d7512 3\u00d73 filters are used multiple times in sequence to extract more complex features and the combination of such features. This is effectively like having large 512\u00d7512 classifiers with 3 layers, which are convolutional! This obviously amounts to a massive number of parameters, and also learning power. But training of these network was difficult, and had to be split into smaller networks with layers added one by one. All this because of the lack of strong ways to regularize the model, or to somehow restrict the massive search space promoted by the large amount of parameters.\n\nVGG used large feature sizes in many layers and thus inference was quite costly at run-time. Reducing the number of features, as done in Inception bottlenecks, will save some of the computational cost.\n\nNetwork-in-network (NiN) had the great and simple insight of using 1x1 convolutions to provide more combinational power to the features of a convolutional layers.\n\nThe NiN architecture used spatial MLP layers after each convolution, in order to better combine features before another layer. Again one can think the 1x1 convolutions are against the original principles of LeNet, but really they instead help to combine convolutional features in a better way, which is not possible by simply stacking more convolutional layers. This is different from using raw pixels as input to the next layer. Here 1\u00d71 convolution are used to spatially combine features across features maps after convolution, so they effectively use very few parameters, shared across all pixels of these features!\n\nThe power of MLP can greatly increase the effectiveness of individual convolutional features by combining them into more complex groups. This idea will be later used in most recent architectures as ResNet and Inception and derivatives.\n\nNiN also used an average pooling layer as part of the last classifier, another practice that will become common. This was done to average the response of the network to multiple are of the input image before classification.\n\nChristian Szegedy from Google begun a quest aimed at reducing the computational burden of deep neural networks, and devised the GoogLeNet the first Inception architecture.\n\nBy now, Fall 2014, deep learning models were becoming extermely useful in categorizing the content of images and video frames. Most skeptics had given in that Deep Learning and neural nets came back to stay this time. Given the usefulness of these techniques, the internet giants like Google were very interested in efficient and large deployments of architectures on their server farms.\n\nChristian thought a lot about ways to reduce the computational burden of deep neural nets while obtaining state-of-art performance (on ImageNet, for example). Or be able to keep the computational cost the same, while offering improved performance.\n\nHe and his team came up with the Inception module:\n\nwhich at a first glance is basically the parallel combination of 1\u00d71, 3\u00d73, and 5\u00d75 convolutional filters. But the great insight of the inception module was the use of 1\u00d71 convolutional blocks (NiN) to reduce the number of features before the expensive parallel blocks. This is commonly referred as \u201cbottleneck\u201d. This deserves its own section to explain: see \u201cbottleneck layer\u201d section below.\n\nGoogLeNet used a stem without inception modules as initial layers, and an average pooling plus softmax classifier similar to NiN. This classifier is also extremely low number of operations, compared to the ones of AlexNet and VGG. This also contributed to a very efficient network design.\n\nInspired by NiN, the bottleneck layer of Inception was reducing the number of features, and thus operations, at each layer, so the inference time could be kept low. Before passing data to the expensive convolution modules, the number of features was reduce by, say, 4 times. This led to large savings in computational cost, and the success of this architecture.\n\nLet\u2019s examine this in detail. Let\u2019s say you have 256 features coming in, and 256 coming out, and let\u2019s say the Inception layer only performs 3x3 convolutions. That is 256x256 x 3x3 convolutions that have to be performed (589,000s multiply-accumulate, or MAC operations). That may be more than the computational budget we have, say, to run this layer in 0.5 milli-seconds on a Google Server. Instead of doing this, we decide to reduce the number of features that will have to be convolved, say to 64 or 256/4. In this case, we first perform 256 -> 64 1\u00d71 convolutions, then 64 convolution on all Inception branches, and then we use again a 1x1 convolution from 64 -> 256 features back again. The operations are now:\n\nFor a total of about 70,000 versus the almost 600,000 we had before. Almost 10x less operations!\n\nAnd although we are doing less operations, we are not losing generality in this layer. In fact the bottleneck layers have been proven to perform at state-of-art on the ImageNet dataset, for example, and will be also used in later architectures such as ResNet.\n\nThe reason for the success is that the input features are correlated, and thus redundancy can be removed by combining them appropriately with the 1x1 convolutions. Then, after convolution with a smaller number of features, they can be expanded again into meaningful combination for the next layer.\n\nChristian and his team are very efficient researchers. In February 2015 Batch-normalized Inception was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values. This corresponds to \u201cwhitening\u201d the data, and thus making all the neural maps have responses in the same range, and with zero mean. This helps training as the next layer does not have to learn offsets in the input data, and can focus on how to best combine features.\n\nIn December 2015 they released a new version of the Inception modules and the corresponding architecture This article better explains the original GoogLeNet architecture, giving a lot more detail on the design choices. A list of the original ideas are:\n\nInception still uses a pooling layer plus softmax as final classifier.\n\nThe revolution then came in December 2015, at about the same time as Inception v3. ResNet have a simple ideas: feed the output of two successive convolutional layer AND also bypass the input to the next layers!\n\nThis is similar to older ideas like this one. But here they bypass TWO layers and are applied to large scales. Bypassing after 2 layers is a key intuition, as bypassing a single layer did not give much improvements. By 2 layers can be thought as a small classifier, or a Network-In-Network!\n\nThis is also the very first time that a network of > hundred, even 1000 layers was trained.\n\nResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck:\n\nThis layer reduces the number of features at each layer by first using a 1x1 convolution with a smaller output (usually 1/4 of the input), and then a 3x3 layer, and then again a 1x1 convolution to a larger number of features. Like in the case of Inception modules, this allows to keep the computation low, while providing rich combination of features. See \u201cbottleneck layer\u201d section after \u201cGoogLeNet and Inception\u201d.\n\nResNet uses a fairly simple initial layers at the input (stem): a 7x7 conv layer followed with a pool of 2. Contrast this to more complex and less intuitive stems as in Inception V3, V4.\n\nResNet also uses a pooling layer plus softmax as final classifier.\n\nAdditional insights about the ResNet architecture are appearing every day:\n\nAnd Christian and team are at it again with a new version of Inception.\n\nThe Inception module after the stem is rather similar to Inception V3:\n\nThey also combined the Inception module with the ResNet module:\n\nThis time though the solution is, in my opinion, less elegant and more complex, but also full of less transparent heuristics. It is hard to understand the choices and it is also hard for the authors to justify them.\n\nIn this regard the prize for a clean and simple network that can be easily understood and modified now goes to ResNet.\n\nSqueezeNet has been recently released. It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.\n\nOur team set up to combine all the features of the recent architectures into a very efficient and light-weight network that uses very few parameters and computation to achieve state-of-the-art results. This network architecture is dubbed ENet, and was designed by Adam Paszke. We have used it to perform pixel-wise labeling and scene-parsing. Here are some videos of ENet in action. These videos are not part of the training dataset.\n\nThe technical report on ENet is available here. ENet is a encoder plus decoder network. The encoder is a regular CNN design for categorization, while the decoder is a upsampling network designed to propagate the categories back into the original image size for segmentation. This worked used only neural networks, and no other algorithm to perform image segmentation.\n\nAs you can see in this figure ENet has the highest accuracy per parameter used of any neural network out there!\n\nENet was designed to use the minimum number of resources possible from the start. As such it achieves such a small footprint that both encoder and decoder network together only occupies 0.7 MB with fp16 precision. Even at this small size, ENet is similar or above other pure neural network solutions in accuracy of segmentation.\n\nA systematic evaluation of CNN modules has been presented. The found out that is advantageous to use:\n\n\u2022 use ELU non-linearity without batchnorm or ReLU with it.\n\n\u2022 use a sum of the average and max pooling layers.\n\n\u2022 use mini-batch size around 128 or 256. If this is too big for your GPU, decrease the learning rate proportionally to the batch size.\n\n\u2022 use fully-connected layers as convolutional and average the predictions for the final decision.\n\n\u2022 when investing in increasing training set size, check if a plateau has not been reach. \u2022 cleanliness of the data is more important then the size.\n\n\u2022 if you cannot increase the input image size, reduce the stride in the con- sequent layers, it has roughly the same effect.\n\n\u2022 if your network has a complex and highly optimized architecture, like e.g. GoogLeNet, be careful with modifications.\n\nXception improves on the inception module and architecture with a simple and more elegant architecture that is as effective as ResNet and Inception V4.\n\nThe Xception module is presented here:\n\nThis network can be anyone\u2019s favorite given the simplicity and elegance of the architecture, presented here:\n\nThe architecture has 36 convolutional stages, making it close in similarity to a ResNet-34. But the model and code is as simple as ResNet and much more comprehensible than Inception V4.\n\nA Torch7 implementation of this network is available here An implementation in Keras/TF is availble here.\n\nIt is interesting to note that the recent Xception architecture was also inspired by our work on separable convolutional filters.\n\nA new MobileNets architecture is also available since April 2017. This architecture uses separable convolutions to reduce the number of parameters. The separate convolution is the same as Xception above. Now the claim of the paper is that there is a great reduction in parameters \u2014 about 1/2 in case of FaceNet, as reported in the paper. Here is the complete model architecture:\n\nUnfortunately, we have tested this network in actual application and found it to be abysmally slow on a batch of 1 on a Titan Xp GPU. Look at a comparison here of inference time per image:\n\nClearly this is not a contender in fast inference! It may reduce the parameters and size of network on disk, but is not usable.\n\nFractalNet uses a recursive architecture, that was not tested on ImageNet, and is a derivative or the more general ResNet.\n\nWe believe that crafting neural network architectures is of paramount importance for the progress of the Deep Learning field. Our group highly recommends reading carefully and understanding all the papers in this post.\n\nBut one could now wonder why we have to spend so much time in crafting architectures, and why instead we do not use data to tell us what to use, and how to combine modules. This would be nice, but now it is work in progress. Some initial interesting results are here.\n\nNote also that here we mostly talked about architectures for computer vision. Similarly neural network architectures developed in other areas, and it is interesting to study the evolution of architectures for all other tasks also.\n\nIf you are interested in a comparison of neural network architecture and computational performance, see our recent paper.\n\nThis post was inspired by discussions with Abhishek Chaurasia, Adam Paszke, Sangpil Kim, Alfredo Canziani and others in our e-Lab at Purdue University.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/@culurciello/shopper-artificial-intelligence-to-help-you-shop-d252bd995f78?source=user_profile---------28----------------",
        "title": "Shopper: artificial intelligence to help you shop \u2013 Eugenio Culurciello \u2013",
        "text": "At FWDNXT we want to get devices to work for you, not be their slave.\n\nWe dream about creating user interfaces that predict your needs and help you to get information at the right time and place without having to navigate to devices, apps, menus.\n\nMeet Shopper: this is artificial intelligence to help you shop. It uses bio-inspired localization algorithms to figure out where you are in a store, and what products are around you. It can then automatically tell you to pick up items in your shopping list without the need for you to pull your phone, find app, find list \u2014 which is the tedious process i go through every week when I enter a supermarket.\n\nCan devices do more for you? With augmented reality and scene parsing and recognition abilities, we believe they can!"
    }
]