[
    {
        "url": "https://medium.com/@madrugado/advances-in-nlp-in-2017-part-ii-d8da391a3f01?source=user_profile---------1----------------",
        "title": "Advances in NLP in 2017 (part II) \u2013 Valentin Malykh \u2013",
        "text": "The first part is devoted to the main fresh idea in our field \u2014 Transformer. If you are interested in Feed-Forward networks disrupting RNNs\u2019 monopoly, welcome to the first part.\n\nThe next part of this article is devoted to task which seemed impossible even a few years back: unsupervised MT. The papers which will be discussed are:\n\nThe last paper seems to not belong here. But as they say \u2014 do not judge book by its cover. All three papers have the same main idea behind. In a nutshell it could be expressed like this: we have two auto-encoders for different text sources (i.e. different languages, or texts of different styles) and we just switch the decoder parts for them. How does it work? Let\u2019s try to sort it out.\n\nThe auto-encoder is an encoder-decoder where decoder decodes to original space. This means that input and output belong to the same language (or style). So if we have some text, we train an encoder to produce a vector containing all the info needed by decoder to reconstruct the original sentence. In the ideal case the reconstructed sentence will be just the same. But in many cases it is not exactly so. We need to somehow measure the similarity between the source and reconstructed sentences. And the machine translation field has an answer for such a challenge. There is a standard formula which is used to measure the similarity, it is called BLEU.\n\nAs you may guess it is not differentiable, so some other way to train our translator is needed. For auto-encoder this could be a standard cross-entropy, but it\u2019s not enough for translation. Let\u2019s skip this for now and go on.\n\nOK, now we have a tool to build our auto-encoder. The next thing which we need to do is to train two of them: one for source language (style), and another one for target language. And also we also need them crossed over: make the decoder for the source language restore encoded target strings, and vice versa.\n\nHere is the tricky part: in auto-encoder (or any encoder-decoder) in the middle we have a so-called hidden representation \u2014 a vector in some high-dimensional space. And if we want two our auto-encoders to be compatible, we need their hidden representations to be in the same space. How it is reachable? Through some additional loss for these two auto-encoders. This loss comes from a discriminator, which in its turn refers us to GANs.\n\nIn our case discriminator (L_adv on the figure) has to tell where the input comes from \u2014target or source language (for machine translation), target or source style (for style transfer task). On the figure above we can see two auto-encoders represented as separate blocks \u2014 encoders and decoders. They have a link between them in the middle, where the discriminator is placed. Training two auto-encoders in parallel with such additional loss leads the model to making hidden representations in both encoders similar (upper part of the figure), so the rest is clear \u2014 just replace the original decoder with its counterpart from the neighbouring auto-encoder (lower part of the figure) and enjoy your model translating from one language to another.\n\nAll three papers have this idea behind the scene, with specific details in each case. The explanation above comes mostly from Unsupervised Machine Translation Using Monolingual Corpora Only, so I should also mention the previous work of the same authors, since its results are used in their work in question:\n\nThe idea of this work is simple and brilliant:\n\nLet\u2019s say that we have word embeddings for two different languages. (Suppose that we work with texts from the same domain, say news or fiction books.) We can assume that the vocabularies of news corpora in a pair of languages are close: for the majority of words in the source corpus we can find their translations in the target news corpus \u2014 like president, taxation or ecology will be definitely presented in the news on both languages. So why we can\u2019t we just juxtapose these words and pull one vector space on another? And we actually can. Even more, we can find a function to transform the whole space (and the dots i.e. vectors for words in it) to some other space, where these dots will be placed on dots in another space. In this work the authors show that this could be done in an unsupervised manner which means we have no need in explicit dictionary.\n\nThe Style-Transfer from Non-Parallel Text by Cross-Alignment paper is placed in this section since languages could be treated just like different styles of text and authors mention it in the paper. Also this work is interesting because they have their code published.\n\nThis section is really close to the previous one, but still a little bit different. The works in question here:\n\nIn the first paper we have a different approach to style transfer which is closer to controllable generation, so it is placed here instead of previous section where belongs its double. The idea of controllable generation could be illustrated by this figure:\n\nHere we see again the auto-encoder on text, but with additional information: the hidden representation of input (here it will be sense) is enriched by additional feature vector. This feature vector is encoding some specific properties of text, e.g. sentiment or grammar tense.\n\nAs you can see on the picture, we also have a discriminator in addition to auto-encoder. There could be even more than one discriminators if we have multiple properties encoded at once. So at the end we have a composite loss function \u2014 the reconstruction loss from auto-encoder and an additional loss for specified text features. Therefore, reconstruction loss here has somewhat different meaning \u2014 it represents only sense of a sentence, not its features we force to be explicit.\n\nAnd the last but not least section. It is devoted again to speed of computation. Despite the fact that in the first part we discussed ground-breaking newcomers from Fully-Connected nets, all the field to this day works on Recurrent networks. And another well-known fact is that RNNs are much slower than CNNs. Or aren\u2019t they? To answer this question, let\u2019s explore this paper:\n\nI think that authors of this work tried to answer the question: why RNNs are so slow? What makes them be like that? And they found the key: RNNs are sequential, this is in their nature. But what if we could do as little of sequentionality as possible? Let\u2019s say that (almost) everything is independent of its previous state. Then we could process all the inputs in parallel. So our task is to throw out all unneeded dependencies on previous states. And that is where it comes to:\n\nAs you can see, only two equations depend on previous state. And this equations work with vectors, not the matrices. All the heavy mathematics could be done independently in parallel. And at the end we just add few multiplications to handle the sequential nature of data. This setup proves to be great, check it out yourself:\n\nThe Simple Recurrent Unit (SRU) speed is approaching that of CNN!\n\nWe can see that in 2017 the field has its own disruptors, like Transformer, and breakthroughs like Unsupervised Machine Translation, and also \u2014 the for common good the fast RNNs (which are faster than FasterRNNs, it you know what I mean). I\u2019m looking into 2018 with aspiration of a new breakthroughs and advances in ways I still cannot imagine."
    },
    {
        "url": "https://medium.com/@madrugado/advances-in-nlp-in-2017-b00e927fcc57?source=user_profile---------2----------------",
        "title": "Advances in NLP in 2017 \u2013 Valentin Malykh \u2013",
        "text": "Disclaimer: First of all I need to say that all these trends and advances are only my point of view, other people could have other perspectives.\n\nIn 2017 A.D. I could spot two main trends:\n\nThis article is mostly about first one, for the second trend see the consecutive article. It also features speedup, which seems to be inevitable nowadays anyway.\n\nThis already famous paper marked the second coming of Feed-Forward networks to NLP. This work is from Google and some famous researchers like Jakob Uszkoreit and \u0141ukasz Kaiser. The idea behind Transformer architecture featured in article is simple and brilliant: let\u2019s forget about recurrency and all that stuff and just try to use attention to do the job. And this idea has actually worked!\n\nBut first lets remember that all current state of the art neural machine translation architectures work on recurrent networks. These networks intuitively are really suit for natural language processing tasks like machine translation, since they have explicit memory they keep during inference. This feature has the obvious bright side, but it also has accompanying dark side: since we keep the memory of what has been done before, we need to process the data only in that particular consecutive order. As a result the processing of the whole data is slow (e.g. in comparison to CNNs), and this exact issue address the authors.\n\nHence the Transformer architecture is Feed-Forward without any recurrency. Here, what they use instead to do the whole job is attention.\n\nLet\u2019s first refresh the standard Bahdanau\u2019s approach to attention:\n\nThe idea of attention is that we need to focus on some relevant input in encoder to do better decoding. In the simplest case the relevance is defined as similarity of specific input to current output. This similarity in its turn could be defined as sum of some inputs with weights, where the weights are summing up to 1, and the biggest weight is corresponding to most relevant input.\n\nIn the figure, we could see the already classic Dzmitry Bahdanau\u2019s approach: we have one input \u2014 the hidden states of encoder (h\u2019s) and some coefficient to sum this hidden states with (a\u2019s). These coefficients are not preset, they are generated from some other input different from encoder hidden states.\n\nIn contrast the authors of paper in question suggested so-called self-attention on the input data. The term \u201cself\u201d in the name refers to idea that attention is applied to the data on which is it being computed, in contrast to the standard approach where one uses some additional input to produce attention on the given input.\n\nFurthermore this self-attention is named Multi-Head since it makes the same operation multiple times in parallel. This feature could be compared to convolutional filters if you\u2019re looking for analog, i.e. each head has been focusing on different places in input. The other main feature of the attention from the paper is usage of three inputs (instead of two in standard approach). As you can see in the figure, at first we compute \u201csub-attention\u201d on Q (query) and K (key) and after that combining this with V (value) from the input. This feature refers us to the notion of memory, which an attention actually is.\n\nAside the main feature of this architecture there are two secondary, yet significant features:\n\nPositional encoding \u2014 as we remember, whole architecture of the model is feed-forward, so there is no notion of sequence inside the network. To inject the knowledge of time sequences in the network the positional encoding was proposed. For me the usage of trigonometric functions (sines & cosines), which form position of word in the document, isn\u2019t that obvious in this capacity, but it is working: this embedding in combination with actual word embedding (e.g. above mentioned word2vec) brings the sense of a word and its relative position to our network.\n\nMasked attention \u2014 simple yet important feature: again, since there is no notion of sequence inside the network, we need to somehow filter the propositions of network for future words which are actually unavailable when we do the decoding. So as you may have spotted on the picture of attention, there is a place for mask, which figuratively speaking crosses out the words which position if in the future to the current one.\n\nAll these features allowed this architecture to not only work, but even improve the state of the art in machine translation.\n\nThe latter feature was unsatisfying for the authors of this paper, written by Richard Socher\u2019s group from Salesforce Research. So this masked attention for decoder was just not good enough for them in terms of speedup got from the parallel encoder, and they decided to take the next step: \u201cWhy can\u2019t we make a parallel decoder, if we already have a parallel encoder?\u201d That is only my speculation, but I bet the authors had a similar question in their minds. And they have found a way to solve the issue.\n\nThey called it Non-Autoregressive Decoding and the whole architecture Non-Autoregressive Transformer, which means, that now not a single word is dependent on another one. This is an exaggeration, but not that big, after all. The idea here is that the encoder in this architecture produces so-called fertility rate for each word it sees. This fertility rate is used to generate the actual translation for each word, based only on word itself. This could be thought of like we have a standard alignment matrix for machine translation:\n\nAs you can see, some of the words could refer to more than one word, and some seem to not refer to any word in particular. Thus the fertility rate just slices this matrix into pieces where each piece is for specific word in the source language.\n\nTherefore we have the fertility, but this is not enough for the wholly parallel decoding. As you can see we need some more attention layers \u2014 positional attention (which refers us again to positional encoding) and inter-attention, which replaced masked attention from original Transformer.\n\nUnfortunately, giving such a boost in speed (up to 8x in some cases), the Non-Autoregressive Decoder takes a few points of BLEU in return. So there is room for improvement!\n\nIn the next part we\u2019ll discuss other important works, considering unsupervised approaches, in the first place."
    },
    {
        "url": "https://medium.com/@madrugado/what-are-the-dialog-systems-or-something-about-eliza-9aefb551eaaa?source=user_profile---------3----------------",
        "title": "What are the Dialog Systems, or Something about Eliza",
        "text": "It seems that dialog systems have been around from the dawn of our age. A good example of them is Eliza \u2014 a chatbot system born in the 1960s. This is a psychotherapist bot, whose interface resembles the interface of Apollo mission, and it\u2019s not a coincidence. Let\u2019s talk more about dialog systems, but first \u2014 our guest star Eliza:\n\nNowadays you meet dialog systems virtually everywhere \u2014 when calling to a bank you will first hear a pleasant voice of a so-called auto-informer system saying \u201cdial 1 for credit card\u201d, when driving to a new place you are given directions by your navigator software. These along with Siri from Apple or Microsoft Cortana are all dialog systems.\n\nWhy are they so popular? Probably because a conversation is a natural way for people to get information.\n\nWe need to talk about classification of dialog systems to better understand how they are built. There are two main criteria which we\u2019ll use to describe dialog systems:\n\nTask-oriented systems are created to solve a particular problem: find the information requested by a user, accomplish a task. Therefore, there always is a point when the system has done what it was asked to do (or failed and run out of options) and the conversation can be finished. They are opposed to general-purpose systems whose goal is conversation itself and they aim at making it enjoyable.\n\nClosed-domain systems are systems which are capable of accomplishing a particular task (or a small set of tasks) in a narrow domain, e.g. ordering a taxi, finding information on flights, etc. On the other hand, open-domain systems are not limited to one domain, they are meant to be omni-purpose: e.g. Siri is supposed to do anything that can be done by an iPhone.\n\nOpen Domain systems are significantly more difficult to implement, then Closed Domain ones, the system needs to \u201cknow\u201d more. Let\u2019s have a look at a few examples:\n\nLet\u2019s start with simplest dialog systems \u2014 auto-informers. They are for sure closed-domain and task-oriented. They even barely have a dialog \u2014 you can only take part in a conversation by dialing numbers on your phone.\n\nThe other example is ELIZA , which is also closed-domain: it interprets all the phrases in a psychological way (which resembles real psychologists a bit, don\u2019t you think?..) and is general-purpose in a way that it has no explicit goal in chat.\n\nNext thing in the list is Char-RNN from Andrej Karpathy. It is here as an example of simplest chatbots. Char-RNN itself is a neural language model which can continue a string of text fed to it as an input. You can use it as a chatbot, for example, if you train it on subtitles to series or movies, so it learns how to \u201canswer\u201d to your line like characters from the movies do. Based on that one could say that CharRNN is open-domain (it can talk in any topic) and general-purpose ( there is no notion of goal in such dialog). The only issue with this model is that it is also has no notion of dialog, phrase or even word. It just does the only thing it knows \u2014 continues a string of text.\n\nThere are only two items on the list left: the ConvAI and so-called True AI. Why is it explicitly stated that AI is \u201ctrue\u201d? That\u2019s because nowadays everything is called AI and you need to set apart say auto-informers from full-glory Artificial Intelligence. It can keep up with a dialog on any topic. And not only keep up, but also lead a conversation. As you know, there is no such system at the moment, but one could try to build a system as close to this as possible. ConvAI is one of such efforts.\n\nThe last item undiscussed here is ConvAI, and it\u2019s time to talk about it. ConvAI stands for Conversational Artificial Intelligence, this is a name of the Conversational Intelligence Challenge (convai.io), which will be held at NIPS this year, a week from today. The idea of this challenge is to make dialog system which can discuss some text with a human. This is challenge for teams providing their dialog systems to chat with volunteers.\n\nBut back to the theme. Since the ConvAI is a Challenge one should decide somehow, who is the winner there. And this is actually not that simple question. Due to standard metrics of textual comparison, used e.g. in machine translation competitions, like BLEU, ROUGE, etc. don\u2019t suit here.\n\nAs you can see on the picture, BLEU score is not even slightly correlated with human judgements. And at the same time humans have quite strong correlation between themselves, which means that one person\u2019s opinion about the quality of an answer usually matches those of other people.\n\nIf you\u2019re not familiar with such a metrics, let's describe them. Metrics like BLEU are made for comparing strings. They are higher for pairs of strings which have more common words and phrases.\n\nLet's consider an example. Say we have a question to a dialog system: \u201cTell me something about elephants.\" The possible system's answer could be: \u201cThese are huge animals with big ears\u201d. But if the known answer is \u201cElephants are mammals which live in Africa and India, their weight can reach 3 tons\u201d, then the suggested answer will be scored poorly despite the fact that it is totally correct according to common sense.\n\nDue to such high variation in answers we cannot score dialog systems for their answers directly. And, more importantly, since we have no other scoring for general-purpose systems, we cannot independently score them. We need some other way to score at least goal-oriented systems. And there is such a metric, which is pretty straightforward - it is called Task Completion Rate:\n\nTask completion rate is the percentage of successful runs of a dialog system. A successful run is a conversation where a system succeeded in accomplishing its goal: for goal-oriented systems goal is usually pre-defined, general-purpose systems we usually consider user satisfaction to be the goal.\n\nTask Completion is judged by humans, in our case they decide if the conversation was satisfactory, engaging, and so on. Therefore, we could compare dialog systems based on a number of successful dialogs they had. Successful dialog here is the one which achieved its goal. For the ConvAI we\u2019ve decided to make conversation about some piece of text, for example a Wikipedia paragraph. After finishing the conversation, a person is asked to leave her/his feedback on the quality of conversation. The trick here is that we don\u2019t inform the person on the nature of her/his counterpart: that could be either AI (a bot from one of participating teams) or a fellow human being. Based on this assessment we could rank dialog systems from dummy Char-RNN bot to real human intelligence.\n\nAs mentioned above you could donate your dialogs to community, so we collect the dataset of human-human and human-bot dialogs. The first part of the dataset is already available here. After the end of our challenge the rest of the data will be also make public.\n\nOf cause, our challenge is not the first dialog systems challenge (although it is unique in some features) with a dataset. One convenient list of dialog corpora is published in [2]. The other example of dialog systems challenge and dataset is DSTC which stands for Dialog State Tracking Challenge. This year it is held for the 6th time. All the data from it is available from here. What is Dialog State Tracking? The idea of dialog state refers us to semantic frame \u2014 the restricted dialog information representation, so the dialog could be considered successful if whole frame is filled with knowledge from dialog. As it could be seen from the definition this challenge is for closed domain systems. So despite that this challenge is really the closest to ConvAI, but the unique feature of ConvAI is open domainness of proposed conversations.\n\nIf you\u2019re attending NIPS, don\u2019t miss a chance to have some fun chatting with bots and humans and donate your dialogs with chatbots to create more robust conversational AI. Also we should mention that the challenge is co-organized by Yoshua Bengio, who has no need in presentation, and also Alan Black and Alex Rudnicky from Carnegie-Mellon University, who have extensive experience in research and development in dialog systems.\n\nIn the end we\u2019d like to invite you volunteer in ConvAI once more, bots (and fellow humans!) are waiting for you. You could start chatting right now!"
    }
]