[
    {
        "url": "https://medium.com/ai-saturdays/the-journey-so-far-ai-saturdays-lagos-15a143382180?source=---------0",
        "title": "The Journey So Far \u2014 AI Saturdays Lagos \u2013 AI Saturdays \u2013",
        "text": "I had the privileged to co-organize AI Saturdays ( an initiative to help people be a kickass in AI by going through course materials and building AI projects ) in my local community, Lagos Nigeria where we met every Saturday from 6th of January till 21st of April making it a total of 16 meet-ups (10am \u2014 5pm).\n\nThe curriculum followed was drafted out by the brilliant people of nurture.ai. We made some modifications to match our audiences\u2019 level of experience.\n\nThroughout the period of AI6 Lagos\u2019s first cohort we have managed to document and contribute 13 articles to AI6 Medium publication. We hope people who are thinking of organizing similar study group can read and be inspired enough to start.\n\n\ud83d\udd17 The Brain and The Model\n\n\ud83d\udd17 May The Tensor Flow with YouWeek10\n\nWe really didn\u2019t think we would be able to pull this meet-ups off because of all the things we couldn\u2019t stop thinking might go wrong but once we were able to sort out the venue, every other things felt do-able.\n\nUsually, our checklist for each week are\n\nSomehow we end up not sending in feedback form as intended and not making as much contribution to the AI6 forums where all the interesting conversations happen :(\n\nThe sessions in AI6 curriculum is in 3 parts\n\nThe most difficult part to stay consistent with was \u2014 ehm, you probably guessed it. Number 3. We had some struggles with 1 as well because the contents were for Intermediate and not for beginners. This made it difficult for most of the members to comfortably sit though a 3 hrs course without losing interest in it.\n\nFor these two session, what we eventually did after observing the pattern for a few weeks was to skip through the videos and Femi giving a gist of what that week\u2019s topic was all about.\n\nWe also noticed that even though the materials are meant to be reviewed before the class, we all know how the week sort-of disappear right in front of you and wholla it\u2019s Saturday again.\n\nSo, most of our members hardly go through the contents which makes it harder for them because they\u2019re being exposed to the content for the first time. That begged for a retaining strategy \ud83d\ude03 we didn\u2019t wan\u2019t our members disappearing because they were bored or couldn\u2019t keep up.\n\nThe retaining strategy was forming groups named after 5 notable deep learning frameworks\n\nThe challenge was to understand your framework and explain it to the class along with your group member. I\u2019m proud to say that their presentations exceeded what we had in mind. So proud of all the group members for going extra miles for their presentations.\n\nHowever, we are excited to announce our bi-monthly meet-up starting on 12th of May where we will focus on:\n\nTo all AI6 Lagos graduates \ud83d\ude01 Thanks for your grit. We hope you have equipped yourselves with enough tools to start kicking some asses in AI. Thanks for making each week worth looking forward to.\n\nThank you Innocent Amadi for your amazing work with FB Dev Circle Lagos, for building a community of amazing developers and helping other community like ourself grow though your support.\n\nThank you Vesper. Thank you Intel.\n\nTo all the amazing communities in Nigeria, thanks for all your hard-work \ud83d\udc9b\n\nFinally, a huge thanks goes to Nurture.AI for this amazing opportunity.\n\nAzeez Oluwafemi\u2019s closing speech for AI6 Lagos first cohort (6th of Jan \u2014 21st of April)\n\nIt\u2019s easy to read an interesting biography of how some genius built an awesome technology or of how a very young and smart person built the first quantum teleportation system.\n\nFew readers are aware of the influence of the \u201ccommunity factor\u201d contributing to such success stories. If you\u2019re very conversant with history, you would realize the impact of a lively intellectual community in breeding such kind of success story. Albert Einstein\u2019s success story is a result of a long history of a massive community of intellectuals in Germany. It was this kind of community idea that gave birth to places like Institute of Advanced studies at Princeton, the construction of the first atomic bomb through the Manhattan project (A community of scientist and mathematicians) led by Robert Oppenheimer was also a result.\n\nWe want to build the future of our nation and our continent by extension through intellectual communities like AI Saturdays Lagos."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-nervana-neon-the-fastest-framework-alive-77e69385ed78?source=---------1",
        "title": "AISaturdayLagos:Nervana Neon, the fastest framework alive",
        "text": "How this article is Structured\n\nNervana Neon is a modern deep learning framework created by Nervana Systems, an artificial intelligence software company based in the U.S. The company provides a full-stack software-as-a-service platform called Nervana Cloud that enables businesses to develop custom deep learning software. On August 9, 2016, it was acquired by Intel for an estimated $408 million.\n\nFor one to run the Nervana Neon framework, there are certain requirements to be met. Neon runs on Python 2.7 or Python 3.4+ and supports Linux and Mac OS X machines ONLY. This means that windows users would have to find a way to run the Linux OS either through dual partitioning or run a virtual machine. Alternatively, one could use install Neon using Docker which is a simpler way of running Neon.\n\nBefore the Neon framework can work, you need to have the latest versions of python-pip (Tool to install python dependencies), python-virtualenv (*) (Allows creation of isolated environments), libhdf5-dev (Enables loading of hdf5 formats), libyaml-dev (Parses YAML format inputs), pkg-con g (Retrieves information about installed libraries). Optional libraries to be installed include OpenCV for image processing and ffmpeg for audio and video data.\n\nFirst, we configure and activate a new conda environment for neon:\n\nNext, we have to activate neon on the Anaconda environment:\n\nThen we run git clone to download neon from the Nervana github repo:\n\nRunning make sysinstall causes Neon to install the dependencies in your virtual environment\u2019s python folder.\n\nIf you would prefer having a containerized installation of neon and its dependencies, the open source community has contributed the following Docker images:\n\nWith the virtual environment activated, we can test our model by running this line of code on the terminal;\n\nNext, we are going to discuss on the Neon workflow, explaining how each section of the Nervana Neon framework work. These sections include:\n\nNeon features highly optimized CPU (MKL) and GPU computational backends for fast matrix operations, which is the main reason for its speed. Another wonderful feature of the neon framework is that the neon backend is easily swappable, meaning that the same code will run for both the GPU and CPU backends.\n\nTo generate an MKL backend, we call:\n\nThen we store it in a variable:\n\nNote: The difference between specifying a \u201cCPU\u201d backend and an \u201cMKL CPU\u201d backend is the fact that the Intel\u2019s Math\u2019s Kernel Library (MKL) backend is highly optimized for fast matrix operations.\n\nThere are two components to working with data in neon:\n\nBut amongst the two listed, NervanaDataIterator is the most common one, and that was what we used throughout our exploration.\n\nWhen using the NervanaDataIterator component, there are conditions to consider which are:\n\nTo specify the architecture of a model, we can create a network by concatenating layers in a list:\n\nFrom the code, nout is the output of the specified layer, init is the variable holding the Gaussian random values set during forward pass, and the activation function used is the Rectlin activation function (also called ReLU) in other frameworks.\n\nThe MNiST dataset is a popular dataset with the following characteristics:\n\nTo load the dataset, we compute the following lines of code:\n\nTo load the CIFAR10 dataset, we repeat the same procedures for the MNIST dataset, only few differences:\n\nThis dataset uses precomputed CNN image features and caption sentences.\n\nTo checkout and test the MNIST and CIFAR10 datasets we tried out, you can visit the GitHub link below.\n\nThe difference between the code here and the one on the Nervana neon GitHub page is the fact that we modified some codes so it will be compatible with Python 3, since it was initially written on Python 2.\n\nNeon supports convenience functions for evaluating performance using custom metrics. Here we measure the misclassification rate on the held-out test set.\n\nAfter running the training set, we got a misclassification error of 2.8% which not too awesome but good enough to make correct predictions.\n\nNext, we download a new digit image from the web and use our trained model to recognize the digit. We first download the image and scale it to the 28x28 pixels that our model expects.\n\nWe then forward pass through the model and examine the output of the model for this image.\n\nWe can now compute the misclassification on the test set to see how well we did using a learning rate of 0.1 and 9 Epochs. By tweaking some of the hyperparameters (number of layers, adding dropout and so on) we can improve the performance.\n\nAfter the training, we got a misclassification error of 38.8%, which is not really bad. So we decided to change some parameters like the learning rate from 0.1 to 0.05, the number of Epochs from 9 to 60 and then 150.\n\nFrom the images, it is obvious that as we increased the number of Epochs, the performance increased, although the difference wasn\u2019t really much.\n\nWe then went ahead to grab a new image from the internet and classified it through our network.\n\nAfter downloading the image, we create a dataset with this image for inference and get model outputs on the inference data.\n\nAfter testing the network with several images, the network made a lot of wrong predictions like classifying an airplane as a deer, a dog as a cat, and many more. But notably, after increasing the number of epochs, the classification performance slightly increased which became evident when the network misclassified an airplane as a bird. After many more testing, the network luckily made a correct prediction by classifying a cat as a cat.\n\nFrom the above charts, we can see that Neon is the poorest with respect to tutorials and learning materials, GitHub interests, GitHub contributors and Industrial usage. This is due to the fact that the framework is new to the market. But I am certain that this is something Intel is working on at the moment.\n\nIntel is trying to make Neon the number one framework for robotics, so obviously, in few years to come, Neon will be mostly used by those in the field of robotics.\n\nAlso due to Neon\u2019s high speed, it is the best for making research and testing.\n\nIn conclusion, even as this post is centered around the MNIST and CIFAR10 dataset, there is more to the Nervana neon framework like transfer learning, fine tuning, creating custom datasets and so on.\n\nAlso, we can\u2019t ignore the fact that Neon is a new framework, and it needs time to catch up with the likes of competitors like TensorFlow and Pytorch.\n\nUltimately, this was a team effort from #TeamNeon, and we hope to achieve greater things together."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-lagosaihack-8479b3f53169?source=---------2",
        "title": "AISaturdayLagos: #LagosAIHack \u2013 AI Saturdays \u2013",
        "text": "Who would have guessed that AI6 Lagos would be partnering with Lagos Women in Machine Learning and Data Science to organize the first AI Hackathon in Nigeria \ud83d\ude32\n\nWe held Lagos AI Hackathon on 7th and 8th of April 2018 with a total of 50 participants, 6 females and 44 males. We had a total of 100 registrants on the attending.io page.\n\nThe theme of the hack was focused on deep learning and the challenge was to classify food image data. We chose food because AI6 Lagos is currently working on a project called ChowNet. ChowNet is set to be an ImageNet for local delicacy.\n\nThis project was born because of the tweet below \ud83d\ude04\n\nFor the hackathon, we had 16643 food images grouped in 11 major food categories. Each team(5 maximum which the team ensured was a mix of novice, beginners, intermediate and expert) was expected to choose one of the popular deep learning frameworks such as Tensorflow, Pytorch, Keras or Neon to solve the challenge. We had a kaggle InClass competition page created for the teams to monitor their submission on the leaderboard.\n\nThe hackathon started on Saturday 7th of April at 1pm, AI6 Lagos held her normal AI Saturdays Lectures from 11am \u2014 12:30pm after which we started the registration process for the hackathon. The hackathon was called to an end at 6pm on day 2 with each team members presentation about how they approach the problem, why they chose that approach and the challenges they faced.\n\n2. GPU \u2014 thanks to the free GPU provided by Google Colab which some of the groups judiciously leveraged to train their model even though most of them ran into runtime memory error \ud83d\ude36\n\n3. Deserting team members \ud83d\ude20 who came on Saturdays but left their team hanging on Sunday"
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-karessing-deep-learning-with-keras-1e9b96d2d013?source=---------3",
        "title": "AISaturdaylagos: \u201cKaressing\u201d Deep Learning with Keras",
        "text": "How this article is structured:\n\nIt has been four weeks of shedding light on some deep learning APIs/libraries at AISaturdays Lagos and just before you make up your mind for or against deep learning especially after hanging out last week with granny Theano, permit me to sweep your feet off the ground with Keras.\n\nImagine driving interstate in 2018 with a Ford Model T and a Ford Mustang 2018 model. Just to be \u201cfair\u201d, you are driving on an international no speed-limit day (just keep imagining and don\u2019t ask google if such a day exists)\n\nI believe, all things being equal, you will get to your destination and if you chose Model T, I can assume you will get to your destination looking happy like the couple below. Living happily ever or never after? considering it a waste of computing power to design a model to answer this, let us just say time will tell.\n\nBut as regards being \u201cfair\u201d, we are in the information age where thousands of researchers, like us \u2014 Keras team of AISaturdays, are looking at solving same problems that we face as human beings i.e. automation across all works of life \ud83d\ude0e\n\nThe advent of cloud services are beginning to bridge the gap of the inaccessibility to scare resources such as computing resources \u2014 Tensor Processing Unit (TPU), Graphical Processing Unit (GPU), etc. Also, we get recognized for what we publish or produce and the faster we do, the more we make living easier and cozy for humanity \u2014 imagine self driving cars with zero records of accidents or a receptionist or bank teller that would not give you \u201cattitude\u201d. Hence a call to simplify a researcher\u2019s life through an easy to use and less technical library for deep learning implementation.\n\nLadies and Gentlemen readers, I present to you (drums rolling)\n\nThe name \u201cKeras\u201d means either Horn (Greek) or Ivory (Latin). Can hear someone asking the reason for the name, though I am not Chollet, it refers to where dream spirits arrive on earth either through a gate of ivory for those that deceive men with false visions or a gate of horn for those who announce the future that will come to pass. Either one you believe, the future is nearer than we once thought. Need not figure out the gate I come from because I am not a dream spirit though I announce the future.\n\nKeras is a high level NN API that takes away the stress of trying to deal with low-level operations such as tensor differentiation, manipulation (reshaping, dot, elementwise operations, etc), etc. Imagine having to understand how each part of your car engine functions before you can be allowed to commence your driving classes, frustrating? Sure, just an understatement, right?\n\nKeras was developed to enable researchers focus on experimentation as the key to doing good research is being able to transform one\u2019s ideas to results with least possible delay or bottleneck through users\u2019 friendliness, modularity and extensibility.\n\nIt supports Convolutional and Recurrent Neural Networks. It runs on Central Processing Unit (CPU) and Graphical Processing Unit (GPU). I believe it will run on Google\u2019s Tensor Processing Unit (TPU) soon because it sits on Tensorflow. It has stronger adoption in both industry and research community. If you use Netflix, Uber, Yelp, Square among others, then have been interacting with Keras.\n\nKeras Models are easily deployed across greater range of platforms that other deep learning frameworks. On iOS using CoreML, Android using TensorFlow Android rumtime, in browser vis GPU-accelerated JavaScript runtimes such as keras.js and WnDNN, on Google Cloud via TensorFlow-Serving, on Raspberry PI, etc.\n\nKeras runs better on linux and installing it requires a system with:\n\nAlternatively you can use cloud services like Amazon Web Services, Google Collaboratory \u2014 this is free, Microsoft Azure, etc.\n\nTo install Keras on your local system on a tensorflow backend, after installing Anaconda, you have to create an environment using:\n\nUsing Keras to implement deep learning reminds me of the concept of playing with bricks from Lego, a danish company that produces toys consisting of mainly interlocking plastic bricks. First you have a problem you intend to solve using CNN like image classification or RNN like text or genome sequencing or both problem just like a child trying to describe his or her imagination to her friend.\n\nFirst step is to define your training data. Just as a child sorts through the various pieces of Lego bricks, one has to define the training dataset as tensors (input and target) as data used in NN are represented in tensors.\n\nTensors are data containers in NN varying from 0 dimensional (0D) tensors (scalars) to 5D tensors for holding video frames.\n\nSecondly, define a network of layers or model. Just as a child decides on a plan and commences the construction of the conceived imagination, Keras offers you the option of building your model using either Sequential API or Functional API.\n\nSequential API requires you putting a layer over another; a brick over another to build a model, which is the most common architecture while Functional API provides you the option to build an arbitrary architecture model that either requires shared layer (a mean of reusing a layer or model like a function in any programming language), multiple inputs (text and image) or multiple outputs (captioning an image). A simple model using a sequential API is shown below\n\nThirdly, configure the learning process by choosing a loss function, optimiser and other metrics to monitor the learning process of the model. This is as simple as writing:\n\nLastly, iterate on your training data by calling the fit() method as shown below:\n\nFrom our Lego view, you have the below. That this might be complex for a child? Never underestimate the power of imagination.\n\nBelow is a cheat sheet for Keras for www.datacamp.com."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-ancestral-intelligence-ai-with-granny-theano-fc70ea2e6a7c?source=---------4",
        "title": "AISaturdayLagos: Ancestral Intelligence (AI) with Granny Theano",
        "text": "How this article is structured:\n\nAs we all know, the first step to attacking a Deep Learning library is its documentation. Luckily, Theano documentation has a pile of information in its documentation page useful for any Theano beginner.\n\nWe started studying Theano\u2019s documentation and it was really tedious and boring. At page 50/644 of the documentation, we asked ourselves \u201cwhat the heck are we doing with this library?\u201d.\n\nHere is the link if you want to read https://media.readthedocs.org/pdf/theano/latest/theano.pdf ( knock yourselves out \ud83d\ude1c ).\n\nThe documentation made rumors of Theano being only for academia. It ensures that its users have a strong mathematical thought process.\n\nTheano is a combination of the best features of Numpy and Sci-Py kits. It was built to understand the machine level process to implementing AI algorithms such that a typical Theano compiler can produce C and C++ code. Little wonder its name is associated with the Greek mathematician.\n\nThese are the basic requirements to have Theano installed on your machine.\n\nYou should also note the following:\n\nWe had enough of the documentation and were set to get our hands dirty and also after successfully installing the library on our machine, we went ahead to try out some of its functions such as its data types, shared variables, gradient calculation, updates, theano.function. One funny thing about Theano is its numerous data types. This is actually interesting, but we seemed not to find its use in application.\n\nTo the main activity, we tried out the \u201chello world\u2019 of image recognition; the MINST dataset. We implemented a logistic regression model on Theano and trained it using the MINST dataset.\n\nWith a built-up morale, we went ahead to implement a simple neural network using the LeNet architecture as described in its documentation and the library worked fine without errors. We got an accuracy of 87%.\n\nBelow is the link to the source code. (Sure be kind to drop comments on how to better this)\n\nAt this point, we felt we already had Theano in our palms. We confidently left the pond and headed for the ocean in search of a wild fish. We decided to implement VGG16 on Theano.\n\nStill left with some morale to spend, we decided to build a VGG model from scratch (how awesome is that) and train it with ?? dataset. The coding was quite easy and we expected it to work fine. But instead, the results were cryptic errors which had no pointer to its cause.\n\nWe tried to debug the error for hours until we finally gave up on Theano as a whole.\n\n\u25cf Theano is actually a nice library to try out but it is definitely not a library to specialise on.\n\n\u25cf Theano is more academia suited than for industry\n\n\u25cf In as much as it one of the founding libraries for ML and DL practice, better libraries which are more concise, less stressful and require less coding have emerged e.g TensorFlow, Pytorch etc. so, use them instead.\n\nUltimately, this was a team effort \ud83d\udcaa and this experience is only the beginning of bigger things to come from #TeamTheano (we are definitely dropping Theano as a framework but sticking with the name though)."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-may-the-tensor-flow-with-you-5cdcaad1ddc3?source=---------5",
        "title": "AISaturdayLagos: May the Tensor Flow with you\u2026 \u2013 AI Saturdays \u2013",
        "text": "After the team Wakanda, I mean Team Torch Panther or was it team PyTorch razzle dazzled the AI Saturday Lagos community with its pythonic and dynamic neural networking ways, it was time for Tensors to take center stage with its elegant flow and high performance due to its C++ back-end and \u201clazy evaluation\u201d.\n\nTo learn more of how you can flow with the tensors, keep reading \u2014 we\u2019ve got you covered.\n\nThis Jupyter notebook would walk you through how to classify handwritten digits using the opensource Google Brain\u2019stensorflow python library.\n\nFurther, only the low level tensorflow API would be used to demonstrate how to recognize images of digits(0 through 9) with over 99% accuracy.\n\nTensorFlow is a deep learning library designed by Google Brain Team within Google\u2019s Machine Intelligence research organization.\n\nThings in TensorFlow works like tensor. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.\n\nTensorFlow computations are represented as dataflow graphs, i.e, Nodes are the operations (ops) and Edges are the Tensor (multidimensional arrays)\n\nTensorFlow has High level Apis and Low level Apis, we decided to use the Low level Api because of the following reasons:\n\nWe\u2019d be using the Google TensorFlow python library. Pre-installation steps include:\n\nUsed to yield results of the built computational graph\n\nThis is data from the Mixed National Institute of Standards and Technology (MNIST) representing digitized handwritten digits.\n\nTo know how well our network is doing, we use the cross_entropy function to measure our loss between trained and actual/test(never seen by our neural network before) input data.\n\nYou can view the full code here\n\nAfter the computational graph yields all its values, the session is then closed\n\nOur experience with Tensorflow was overrall positive. Some challenges and guides to overcome them include\n\nThis challenge becomes more and more apparent the deeper the network. Having a deeper network (.e.g. GoogleNet ) with more parameters and layers for example can lead to situations where the shapes don\u2019t match. This is exacerbated by the fact that the final result is not known until after the static computational graph is run.\n\nBuilding the computational graph step by step from small one layer onwards and testing continuously helped avoid this issue.\n\nThe very speed of tensorflow comes from its \u201clazy evaluation\u201d of a computational graph. However, when we were building the neural network and wanted to having branching logic based on various criteria(e.e.g loss, different layer performance .etc.), it was not straightforward.\n\nTensorflow now has an API to do control flow but it ends up not being pythonic and adds an additional layer of complexity to the neural network logic.\n\nThanks to:\n\n1. Ejiro Onose, an engineering student at UNILAG for putting together the presentation.\n\n2. Tella Babatunde for wrangling the MNIST dataset into a ~99.16% accurate digit classifier\n\n3. Ibrahim Gbadegeshin for the noteworthy presentation\n\n4. Juwe C. Raphael, Tunde Osborne for making it possible\n\n5. Yours truly, todun, with the jupyter notebook for fielding questions and aggregating our effort"
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-the-torch-panther-cdec328c125b?source=---------6",
        "title": "AISaturdayLagos: The Torch Panther \u2013 AI Saturdays \u2013",
        "text": "PyTorch is an open source deep learning framework for python. This open source framework was based upon another deep learning library named Torch. Surprisingly, this framework was not created with python rather it was designed with a scripting language named LuaJIT.\n\nPyTorch was initially released on October, 2016 which followed with its stable release (version: 0.3.1) in 14th February, 2018. It was primarily developed by Facebook\u2019s artificial-intelligence research group. The framework\u2019s popularity grew sporadically after its stable release because it was created using python as its scripting language. Also, it is built with the concept of dynamic computational graph in mind.\n\nPyTorch puts Python FIRST which makes it suitable for industry experts and academic researchers for its simplicity and rapid prototyping. Also, Fastai library and Uber\u2019s \u201cPyro\u201d software for probabilistic programming are built upon it.\n\nOur team were basically beginners when we tried this experiment out of curiosity. We implemented Convolutional Neural Networks(CNNs), which are deep learning architecture inspired by the brain\u2019s biological processes. So everything we did was by the book with a little ingenuity.\n\nWe decided to compare an earlier version of neural networks called LeNet-5 with a modern version called ResNet on the CIFAR-10 dataset. We decided to try the most basic network model on the CIFAR-10 dataset which was the LeNet-5 model.\n\nThe LeNet model was developed in 1988 by LeCun for handwritten number recognition used by banks for cheque checking.The LeNet model is a 7-level convolutional neural network that employed the use of one or more convolutional layers before adjoining to the fully connected linear layers where the outputs are collated and compared. In our experiment, we made use of 2 convolutional layers, 2 max pooling layer and 3 fully connected linear layers.\n\nThe design of our Convolutional Neural Network (CNN) based on LeNet follows the approach below:-\n\nPyTorch has special in built packages in order to assist us in designing our desired network model. Some of these notable features include: torch.autograd, torch.nn and torch.nn.functional. These features help us in situations like computing gradients, performing gradient descent, declaring neural networks as well as selecting our desired activation functions etc.\n\nAfter designing our neural network, we declared our criterion and optimizers. These functions are vital in computing our gradients, performing back propagation as well as updating our weight since the LeNet model works with this flow.\n\nWe trained the network for 2 epochs as well as its performance over the dataset. This process was carried out on a Local system (CPU) with the class \u201ccar\u201d having the highest accuracy. The loss rapidly in the first epoch but its rapid fall decreased in the second epoch.\n\nWe concluded that the results collated and compiled were good but we felt that the accuracy could go higher and the loss lower.\n\nOne way this could be achieved was using a deeper neural network than LeNet-5. So we decided to apply the ResNet (Residual Network) model on our dataset.\n\nWe decided to go with the ResNet-50 model not because we thought it was the best, but because it was a model we understood and this allow us to fine -tune our dataset.\n\nAfter fine-tuning layers of the skeletal ResNet-50 Model, the results were better compared to results gotten from the LeNet-5 Model. The class \u201ccar\u201d had a higher accuracy of 83% while the lowest loss value dropped by 0.161 over 2 epochs compared to the previous result.\n\nFrom the experiments performed above, we discovered that the deeper the network the better the accuracy. But this conclusion is clouded by how deep the network should be. In general, it can be deduced that the ResNet model provides a better accuracy and lower loss values than LeNet-5 Model.\n\nTherefore, we proceeded to try out this same experiment with our custom dataset. We also discovered that the results could be better by increasing the number of epochs but this might not work at in few cases.\n\nOur team decided to be a bit adventurous by embarking on our own Wakanda \ud83d\ude45 conquest. We created a custom image dataset (120 images) of two main characters in the movie namely \u201cOkoye\u201d and \u201cShuri\u201d. We implemented transfer learning using the dataset. But we had to ask ourselves this question what is transfer learning?\n\nTransfer learning involves applying knowledge from a tested and trusted source to a new problem/situation. In practice, models are rarely trained from scratch because such processes are computation-intensive, power -consuming as well as a waste of processing time. So, pre-trained models are considered suitable for this experiment.\n\nIn this experiment, we will be comparing two transfer learning techniques: Fine-tuning and Feature-Extraction.\n\nHere, we initialized our network with a pre-trained network (ResNet-18) which has been trained with Imagenet dataset. We are basically transferring the knowledge such as edge detection or color blob detection used in Imagenet to our network. Then, the model was trained with the custom dataset.\n\nIn this case, we froze the weights of all the network leaving the last fully-connected layer. We altered some features and replaced it with our own fully connected layer with 2 classes and trained only that layer. In our experiment, we replaced the final fully connected layer with our new fully connected layers with our 2 classes (Shuri and Okoye).\n\nThe accuracy of the fine-tuned model was about 95% compare to the 90% of feature extraction.\n\nOne of the challenges, we encountered was training the model. We naively trained the model on convection laptop CPU, which took a lot of time before switching the Google cloud Engine which took rough 25 minutes (lifesaver).\n\nGreat care should be taken when selecting your images as you\u2019ve all noticed we did not use pictures of \u201cPrince T\u2019 Challa\u201d because most images of him conflict with the antagonist of the movie (Erik Killmonger). This was done in order to avoid multi-labelling which can prove to be a pain during computation. The image format is another challenge that should be considered PyTorch will work better with images of the same format.\n\nAnnotation of any custom dataset is very important as it will help you organize your images. We learnt this the hard way.\n\nOvercoming all these challenges seemed impossible for novices like us but by addressing the necessities required for the experiment we got our desired results.\n\nIn summary, this project was an eye-opener and a worthwhile exercise.\n\nYou can watch the presentation below and view our slide here"
    },
    {
        "url": "https://medium.com/ai-saturdays/ai-saturdays-uyo-persistence-patience-and-perspiration-make-unbeatable-combination-for-success-55850e3dd154?source=---------7",
        "title": "AI Saturdays Uyo: Persistence, Patience and Perspiration Make Unbeatable Combination For Success.",
        "text": "We had our second meetup on 17th February 2017. People came in numbers 1,2,3,4\u2026\u2026\u2026\u20265, although, not all that signed up turn-up. At this point we did not wait for crowd again because \u201ctime wait for no man\u201d.\n\nWe started by 10:00 am and ended by 4:00 pm all the session was taken. 10:00 am to 11:00 am was used to quickly review last meetup lesson and welcome new participants who joined us.\n\nFast.ai (practical deep learning for coders lesson 2 was taken) from 11:00am-12:30pm. where we get to know more about the model, learning rate, epoch, loss validation, how to fine tune a network and lastly how to setup aws (amazon web service) thanks to Jeremy Howard.\n\nWe quickly moved to convolutional neural network for visual recognition lecture one where we learnt about what the course is all about that is, computer version: the study of visual data. We also learnt the history of computer version and general overview of the course. Fill free to check out the video here. This session lasted for one hour thirty minute .from 12:30pm-2:00pm.\n\nThirty minute was use for break. During this thirty minute we decided to put some participants who where not familiar through on Git. A big thanks to Onuwa Nnachi Isaac.\n\nSession three, Reinforcement learning by David Silver lecture one which appeared to be the most interesting session to majority of the participant . The video briefly introduce what a Reinforcement learning is, problems within reinforcement learning agent, the reinforcement learning problems. It also cover some reinforcement learning key words like environment, agent, reward , action and observation.\n\nWe are staying with three session for now and may introduce some letter depending on the participant interest. I will be very self-fish if I fall to appreciate nature.ai for this beautiful opportunity and my fellow ambassador for their time. Wils Wilson and Jude Ben.\n\nUltimate guide to setting up a Google Cloud machine for fast.ai version 2"
    },
    {
        "url": "https://medium.com/ai-saturdays/ai-saturdays-uyo-chapter-meetup-1-bfa96658d2a2?source=---------8",
        "title": "AI Saturdays Uyo Chapter: #meetup 1 \u2013 AI Saturdays \u2013",
        "text": "We held our first super, exciting meetup on the 3rd Feb. 2018.\n\nAlthough the turn up was low, the few people that came around were really excited about the meetup. It was more of an introductory part as most of them were not familiar with the phrase Artificial Intelligence (AI).\n\nWe took time to give an intuitive talk on AI, AI tool box .That is, the basic software needed to get started with AI. Topics like jupyter notebook, anaconda etc were covered in details. This software was also given to some of the participant who did not have it.\n\nThe aim of the section was to carry along those who have no experienced at all with AI. This section lasted for one hour thirty minutes and was facilitated by Wils Wilson.\n\nSession one, Practical Deep Learning for Coders by fast.ai was also introduced. We watched the videos together which briefly gave an overview of everything that will be done through out the whole course.\n\nWe also learnt how to build a simple convolutional neural network that classify between dog and cat using fast.ai framework. At this point the frame work was not fully understood but we believe with time it will be understood.This was all we did in our first meetup as we were expecting more participant in our next meet and also did not want it to be boring.\n\nIn conclusion the participant where very exited and also very thankful to nature.ai for the beautiful opportunity to learn for free. I will like to say thank you to Jude Ben and Wils Wilson my fellow ambassador who also make the day any the entire AI Saturdays Uyo chapter a success. Thank you for your time."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-the-brain-and-the-model-2874814eb0de?source=---------9",
        "title": "AISaturdayLagos: The Brain And The Model \u2013 AI Saturdays \u2013",
        "text": "THE BRAIN AND THE MODEL Terminologies and Concepts that Relates Deep Learning with the Brain One of the factors that made Deep Learning frustrating for me was the numerous new and strange terminologies associated with it. Over time, with the help of some lectures and learning, some analogies came up in my head that helped relate Deep Learning with the brain. A model is basically an equation that takes in input and produces a result (output). Models try to predict real life occurrences. Deep Learning models are just like any other models. I guess we can relate the human brain to a model. Different brains perform different functions. For example, Isaac Newton\u2019s brain is good at producing intuitions based on observation, a traffic wards brain is good at controlling traffic, and so on. Permit me to refer to the deep learning model as DL-brain.\ud83d\ude01 The idea of a network is just an interaction of different nodes; like Facebook Network, Electrical Network and Internet Network. As you must have figured out, the word Biological Neural Network has to do with the unique structure and working of the brains. Since, Deep Learning is trying to create models that perform just like the brain, it is fair enough to borrow the word neural; so we have the word Neural Networks for deep learning too. The network activity involves the transfers information from node to node through neurons (biological or artificial) and the different ways these networks are connected is called its architecture.\n\nWhat happens at the node (source: giphy.com) This is basically a large chunk of data we want the DL-brain to understand (learn). You will agree with me that a new born baby\u2019s brain is unlearned and will only perform its predefined functions like blinking, crying, smiling etc; until the baby is exposed to the different kinds of information (data-set) in the world. Similarly, our DL-brain has to be exposed to different information (data-sets). This process of exposing the DL-brain (model) to different data-set is called training; yeah, just like training a child in school or at home.\n\nBut how does the training take place in our DL brain? At the first ever sight of a dog (say a Doberman), the human brain gets some information of what a dog looks like, but might fail to identify a Caucasian as a dog when it sees one later. If the brain gets access to other varieties of dogs, it figures out different features that makes a dog a dog. This information is usually stored somewhere in the brain (memory); and I guess this memory is called weight in our DL-brain. Remember our DL-brain formula (model)? The variable \u2018W\u2019 is actually the weight. It is that one value that has the precious information that makes the DL-brain label images correctly, identify sound and recognize speech when it comes across one. In other words, weight holds specific details (information) of specific features in the data-sets that it came across.\n\nFor the maths guys \u2014 It\u2019s just that parameter that we tweak in a model during parametric studies. So, when we say \u201ctraining a model\u201d, we basically mean getting the right values for the weight. Once a good weight has been gotten, we can say that the DL-brain has learnt the data. But how do these weight in the DL-brain get this information?"
    },
    {
        "url": "https://medium.com/ai-saturdays/ai-saturday-bangalore-chapter-trash-classifier-tutorial-8e6b3c6860c5",
        "title": "AI Saturday-Bangalore Chapter: Trash Classifier Tutorial",
        "text": "Trash Segregation is one of the challenging process in order to effectively recycle Garbage. It is always easy to recycle a well segregated waste than mixed ones.\n\nAs a Co-organiser of Nuture.AI\u2019s AI Saturdays Meetup in Bangalore, I thought it would be a great toy experiment to build a trash classifier based on this dataset using Fast.ai Library (Thanks a Ton !! Jeremy Howard for the course and library). We had shared this task as assignment in the meetup for fellow attendees to put their learning into action.\n\nThis post serves as Tutorial/Walk through on how to use fast.ai library to solve the problem.\n\nThe Dataset can be Downloaded From : Link or Direct Link\n\nUnderstanding the data set: It consists of 2527 images :\n\nThe SVM achieved better results than the Neural Network. It achieved an accuracy of 63% using a 70/30 train/test data split. After running 50 epochs of training with a 0.0075 learning rate and batch size of 25 on our neural network with a 70/30 train/test split, we achieved a testing accuracy of 27% (and 23% training accuracy). Link\n\nKey Steps to Be Followed :"
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-basic-overview-of-cnn-cd354470e2bb",
        "title": "AISaturdayLagos: Basic Overview of CNN \u2013 AI Saturdays \u2013",
        "text": "Convolutional Neural Network is a class of deep neural network that is used for Computer Vision or analyzing visual imagery.\n\nComputers read images as pixels and it is expressed as matrix (NxNx3) \u2014 (height by width by depth). Images makes use of three channels (rgb), so that is why we have a depth of 3.\n\nThe Convolutional Layer makes use of a set of learnable filters. A filter is used to detect the presence of specific features or patterns present in the original image (input). It is usually expressed as a matrix (MxMx3), with a smaller dimension but the same depth as the input file.\n\nThis filter is convolved (slided) across the width and height of the input file, and a dot product is computed to give an activation map.\n\nDifferent filters which detect different features are convolved on the input file and a set of activation maps is outputted which is passed to the next layer in the CNN.\n\nThere is a formular which is used in determining the dimension of the activation maps:\n\nActivation function is a node that is put at the end of or in between Neural Networks. They help to decide if the neuron would fire or not.\n\nWe have different types of activation functions just as the figure above, but for this post, myfocus will be on Rectified Linear Unit (ReLU).\n\nReLU function is the most widely used activation function in neural networks today. One of the greatest advantage ReLU has over other activation functions is that it does not activate all neurons at the same time. From the image for ReLU function above, we\u2019ll notice that it converts all negative inputs to zero and the neuron does not get activated. This makes it very computational efficient as few neurons are activated per time. It does not saturate at the positive region. In practice, ReLU converges six times faster than tanh and sigmoid activation functions.\n\nSome disadvantage ReLU presents is that it is saturated at the negative region, meaning that the gradient at that region is zero. With the gradient equal to zero, during back propagation all the weights will not updated, to fix this, we use Leaky ReLU. Also, ReLU functions are not zero-centered. This means that for it to get to its optimal point, it will have to use a zig-zag path which may be longer.\n\nThe Pooling layer can be seen between Convolution layers in a CNN architecture. This layer basically reduces the amount of parameters and computation in the network, controlling overfitting by progressively reducing the spatial size of the network.\n\nThere are two operations in this layer; Average pooling and Maximum pooling. Only Max-pooling will be discussed in this post.\n\nMax-pooling, like the name states; will take out only the maximum from pool. This is actually done with the use of filters sliding through the input; and at every stride, the maximum parameter is taken out and the rest is dropped. This actually down-samples the network.\n\nUnlike the convolution layer, the pooling layer does not alter the depth of the network, the depth dimension remains unchanged.\n\nFormular for the output after Max-pooling:\n\nIn this layer, the neurons have complete connection to all the activations from the previous layers. Their activations can hence be computed with a matrix multiplication followed by a bias offset. This is the last phase for a CNN network.\n\nThe Convolutional Neural Network is actually made up of hidden layers and fully-connected layer(s)."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-classification-of-nigeria-currency-notes-using-fastai-framework-2fdcedc174e0",
        "title": "AISaturdayLagos: Classification of Nigeria Currency Notes Using Fastai Framework",
        "text": "The fast.ai course has proven to be one of the fastest learning option for beginners who want to build deep neural networks with good results in image classification. The course begins with coding and gradually goes dipper into the theory of deep-learning (i.e. bottom-top learning method).\n\nAfter day-three at the Saturday-AI class by Nurture.AI in Lagos Nigeria, I decided to try out the fast.ai library with the procedures laid-out in the course; a simple image classification task using the Nigerian Naira as subject.\n\nI started off by gathering random images of the naira denominations (N5, N10, N20 \u2026 N500, N1000).\n\nAfter gathering sufficient images from google search engine, I then categorized the images in to \u2018train\u2019, \u2018test\u2019, and \u2018valid\u2019 folders.\n\nFor this practice, I had 32 training set and 15 valid set for each category of the Naira currency. I also had 57 test set for the practice.\n\nThe resnet34 algorithm was used for the classification with a learning rate of 0.01 and 3 epochs."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-leveraging-on-google-colab-313bab053603",
        "title": "AISaturdayLagos: Leveraging on Google Colab \u2013 AI Saturdays \u2013",
        "text": "Learning \u201cDeep Learning\u201d is hard, and Nurture.AI\u2019s AI Saturdays initiative is out to make it easier. However, one big barrier for newbies which we\u2019ve discovered over the weeks is the need for a Graphic Processing Unit to use for the course.\n\nIf you don\u2019t have a PC with a good GPU then you need to rent one of the available Cloud Compute services, e.g. AWS or Google Cloud, using a tutorial like this.\n\nHowever, this process of setting up your cloud environment could be a tedious process for any newbie to wrap his/her head around because this process takes time and effort, and also requires a debit/credit card.\n\nSome of the participants do not have a valid credit card and thus are unable to claim the free $300 credit Google has given out and for those who were able to set up successfully, they sometimes run at risk of forgetting to shut down their machine which usually is as a result of fluctuating Internet connection.\n\nGoogle has graciously opened up their research tool Google Colab for this purpose, and they now provide free virtual machines for you to use: with about 12GB RAM and 50GB hard drive space, and TensorFlow is pre-installed! \u2014 Yay!\n\nThis means you don\u2019t need to go through the stress of setting up a Cloud VM to get started with deep learning. Google Colab tool is a Jupyter notebook-based system integrated with Google Drive.\n\nNote that when you connect to a GPU-based VM runtime, you are given a maximum of 12 hours at a time on the VM. Although you will be able to connect to another VM after the 12 hours, you lose access to the previous VM instance \u2014 meaning that you lose all data setups you have on it that you haven\u2019t saved to Google Drive. Therefore, you can\u2019t run a 24-hour training script with Colab unless you decide to take the occasional snapshots and save it to Drive manually. However, I don\u2019t think this is a beginners problem.\n\nIf you opt for Cloud computing, when you are using Google Colab (or any Cloud computing tbh) you get to save yourself data costs, compared to if you have to download datasets and install deep learning libraries on your personal PC. There are datasets that are up to 10GB and more out there. Using Google Colab\u2019s VM you can download them faster and work with them directly on the VM without having to worry about the speed and costs of data/internet connection and the hard drive space they\u2019d consume on your local PC or the cloud VM but the real catch here is \u2014 it\u2019s really Free!\n\nTo get started, I have created this Colab file to set up fastai on your new GPU-based VM, following this post. When you are just getting started or connecting to a new VM, all you need is to run the file on the VM (before doing other stuff that may require the fastai library) \u2014 Cool yeah?\n\nThe Colab file also provides basic information on the current VM at the end of the script.\n\nFor me, I have a 12GB RAM system, 49GB hard drive space and I am the root user.\n\nAfter running the script successfully you can go ahead to import your Jupyter notebooks into the VM and start working with them."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-recap-on-week-3-b463396f2140",
        "title": "AISaturdayLagos: Recap on Week 3 \u2013 AI Saturdays \u2013",
        "text": "Here\u2019s a recap of what we learnt in week 3.\n\nWe virtually attended Fast.ai Lesson 2. Jeremy Howard did a recap of last week\u2019s lecture, after which he talked about how to choose learning rates, he explained what learning rates are and why they\u2019re crucial to how accurate your model will be. Some key takeaway concept are cosine annealing, differential learning rate annealing, data augmentation, test time augmentation and stochastic gradient descent with restarts.\n\nAccording Jeremy Howard, these are easy steps to train a world-class image classifier using fast.ai framework\n\nJustin Johnson did a brief overview of last week\u2019s lecture on linear classifiers where he explained how for every pixel in the input image, there is some matrix weight W telling us how much does the pixel influence the labelled class. He talked about how to determine a matrix W that we can apply to our input to give us a loss of approximately zero. He explained two different loss functions and\n\nGoing down this line of thought, He further explained why these functions aren\u2019t enough to determine how accurate our model is because they tend to generalize on just the training dataset thereby performing poorly on the test sets. This is where a term comes to rescue.\n\nThe key takeaway concepts are and He wrapped up the class with a discussion on what feature extraction is, why it should be considered and popular methods to perform this.\n\nThe class started with us watching a video of an interview session with Prof. Helmut B\u0151lcskei where he talked about why it\u2019s important to understand the theoretical aspects of deep learning and it\u2019s underlying mathematics.\n\nAzeez Oluwafemi led the discussion part of this session where he briefly went through the papers on harmonic analysis of CNNs. He explained the findings from the papers which has to do with Translational Invariance. He talked about how a good CNN has to be translationally invariant (meaning that you can recognize an object as an object, even when its appearance varies in some way). He explained that this Translational Invariance can be achieved because of the pooling layer which helps us reduce the dimensionality of our input layer, which in turn reduces compute power. He gave a side note that the translational invariance discussed is vertical and that the early layers of a CNN are still very covariant. Invariance increases with depth of the network.\n\nWe concluded this session by grouping the class for projects \u2014 into 10 per group."
    },
    {
        "url": "https://medium.com/ai-saturdays/aisaturdaylagos-recap-on-week-2-8bf253802796",
        "title": "AISaturdayLagos: Recap on Week 2 \u2013 AI Saturdays \u2013",
        "text": "We held week 2 of AI Saturdays Lagos on the 13th of January, 2018. People came in masses \u2014 1, 10, 20, \u2026 80.\n\nWe were able to ensure a large percent of the class had set up their google cloud instance while the remaining shared with them.\n\nWe watched videos by Jeremy Howard on making neural networks uncool again. It was a practical session so we had to do a code walk-through. The first code session involved trying out an binary image classification challenge from kaggle called \u201cCats and Dogs\u201d without knowing the details of the architecture yet. So a resnet35 architecture was used at high level with details implemented in fast.ai framework which was also implemented in Pytorch. Students were challenged with the suggestion on how they could tweek the code to build their own binary classification project.\n\nWe also watched the Lecture 1 video of CS231n taught by Prof. FeiFei Li, Justin Johnson and Serena Yeung. For the first lecture, Prof FeiFei gave a brief history of Computer Vision and an overview of CS231n.\n\nShe talked about the challenges of vision and why it is important for computers to develop vision. She related this to building algorithms.\n\nLecture 2 was taught by Justin Johnson, a soft introduction to Image Classification, the motivation, example and challenges. He further discussed algorithms that can be used to solve the problem of Image Classification \u2014 Nearest Neighbor and Linear Classification.\n\nDue to the nature of the complexity of the papers treated and in an effort to make the information accessible to enthusiast, we decided to make it a discussion class. That way people are forced to explain what they read and not just throw equations around. Those who wanted to know the details could however study and help provide interesting and reasonable intuitions for other students to understand. It ended up becoming one of the most interesting classes."
    },
    {
        "url": "https://medium.com/ai-saturdays/ripples-of-the-wave-of-change-95178e728d0b",
        "title": "Ripples of the Wave of Change \u2013 AI Saturdays \u2013",
        "text": "Little drops of water make a mighty ocean\u2026 Little by little the ant makes its colony a thousand times its size. It\u2019s amazing how something so small can make a colony the size of an average human-being\u2026From the ant I learnt that a small beginning isn\u2019t what it seems to be\u2026 rather it\u2019s a disguised success. \u2014 Jakpinky\n\nNigeria has been a country of many labels; sometimes derogatory. The name \u201cNigeria\u201d leaves no better image in the mind of most people but corruption, poverty, developing nation, sleeping giant of Africa, to mention a few. While this perception still holds for a lot of people, there is a new breed of younger generation trying to find a meaning, expression and solution to problems not just local but global. There have been many of such efforts but this write-up zooms highlights just one of such, a dream that hatched on Saturday, 6th January 2018. Before now, there has been meet-ups that were quarterly but one-off as regards each meeting but this idea is about continuously building on subsequent meetups for 16th weeks to build and empower the Artificial Intelligence (AI) community in Nigeria alongside with the rest of the world, #AISaturdays.\n\nA step back to my past, I have heard of AI since my undergrad years, just about when the popular movie I, Robot staring Will Smith, was released. I\u2019ve always been a fan of sci-fi movies, so it wasn\u2019t surprising that a passion was sparked in me to learn, understand and transform the world through the development of AI products. Even though Sci-fi movies AI aren\u2019t exactly what real-life AI is \u2014 you know what I mean :-)\n\nIt was thereon that my AI ripples first started forming, thinking about it, this sounds like an irony considering the fact that I actually studied Computer Science in school but trust me, none of my Q Basic, FORTran, AI lectures spiked enough interest in me like that movie did.\n\nI guess I left my head in the clouds :(\n\nBless you! I, Robot \u2014 now I\u2019m a fan for life :)\n\nFast forwards few years after my undergrad studies, my passion for AI was rekindled with my participation in the #DataScienceNigeria\u2019s bootcamp in October 2017 and this was further reinforced by a chat from a friend that directed me to Nurture.AI\u2019s AI Saturdays. I longed for it to start and here we are :).\n\nOn the start of this fateful day, it appeared like the word got out to just a few as the organizers initially planned for 20 persons. However, as the lessons began, more people started coming in as the hours advanced. The first day ended with about 67 persons ranging from students to AI enthusiasts.\n\nOne of the thing that fascinated me about the study group was how linear algebra could be simplified into an interesting, short yet condescend concepts. I was literally in a \u201cmathematics class\u201d for hours without calculating or solving any problems yet I understood the lessons far better than when I was in school\n\nwhere I was more bothered in solving exercises than understanding its concepts or applications. Okay, to be honest, I think school cheated me :( Nobody told me Matrices could be used to represent basically everything \u2014 sort-of.\n\nEssence of Linear Algebra blew my mind, the lessons were concise yet illuminating mathematical concepts one learnt without earlier possessing that in-depth understanding of its applications or how it relates to the natural world.\n\nAsides from the lessons on Linear Algebra, We set up google cloud (because it\u2019s kinda free :)) using a tutorial written by How Khang, we then dived into some introduction to Python Programming Language and Numpy, a package in Python that helps in easier scientific computing on Matrices. (see links below)\n\nThe puzzles are starting to fit together in my mind. The ripples have begun forming and in the next 16th weeks, change is imminent."
    },
    {
        "url": "https://medium.com/ai-saturdays/david-silver-rl-course-lecture-1-notes-c7f895ab45b2",
        "title": "David Silver RL Course: Lecture 1 Notes \u2013 AI Saturdays \u2013",
        "text": "Reinforcement Learning: It sits at the Intersection of many fields of Science. It\u2019s the science of Decision making, a method to understand optimum decisions.\n\nSo it\u2019s really commo to a lot of branches, and is a general approach to solving the Reward based problems.\n\n(Informal) Reward Hypothesis: All goals can be described by the maximisation of expected cummalative reward.\n\nAll rewards can be weighed out based on the actions after comparisions. Thus all rewards can be decomposed into a scale of comparisions and hence a Scalar reward must be ultimately derived.\n\nBy Definition, Goal may be intermediate or a Final goal or Time Based, etcetra.\n\nFirst step is understanding the Reward Signal.\n\nWe control the brain here-Brain is the agent.\n\nAt every step, the agent receives observations which are generated by the environment and the agent itself influences the environment by making actions.\n\nThe Machine Learning problem of RL is related to the stream of data coming from the trail and error interaction.\n\nThe Stream of Experience, Sequence of Observation, Actions and rewards.\n\nState: It\u2019s a summary of the information to determine the next action. It captures the history to determine all that should happen next.\n\nNote: For a multi-agent problem, an agent can consider other agents as part of the Environment.\n\nAn information systems contains all useful information from History.\n\nMarkov Property: A state is Markov if and only if: Probability of the next state, given your current state is the same as all of the previous states. In other words, only current state is determining the next state and the history is not relavant.\n\nIn other words, If Markov property holds. The Future is independent of the History, given the Present. Since the state characterises everything about the past.\n\nAnother definition: State is a sufficient statistics of the Future.\n\nAnd the entire history is also a Markov state. (Not a useful one)\n\nAn RL Agent may (or may not) include on of these:\n\nIt\u2019s a map from state from action. Determines what the agent will do if it\u2019s in a state.\n\nIt\u2019s a prediction of expected future reward. We chose between actions by deciding to go for the highest rewards, an estimate of this is obtained by the Value function.\n\nValue function depends on the way in which we are behaving, it depends on the policy. It gives the reward if we follow an action, thus helps in optimize our behaviour.\n\nGamma: Discounting. It affects if we care about current/later states. It decides the horizon for evaluating the future. (Horizon-how far along do we need to calculate outcomes of our actions).\n\nIt\u2019s used to learn the environment, predicts what the environment will do next. It isn\u2019t necessary to create a model of the environment. But it\u2019s useful when we do.\n\nIt can be divide into two states:\n\nWe catgorize our agents based on which of the above three concepts, it follows. Say, if we have a value based agent: if it has a value function and a policy is implicit.\n\nPolicy based: maintains a data structure of the every state without storing the value function.\n\nActor Critic: Combines both the policy and also the value function.\n\nSo RL Problems can be categorized as:\n\nThere are two problems when it comes to sequential decision making.\n\nExploration: Chosing to give up some known reward, in order to find more about the environment.\n\nThere is an Exploration Vs Exploitation Tradeoff.\n\nPrediction: An estimate of the future, given the current policy\n\nIn RL, we need to evaluate all our policies to find out the best one."
    },
    {
        "url": "https://medium.com/ai-saturdays/the-windows-guide-to-setting-up-a-google-cloud-instance-for-the-fast-ai-course-fe2d61f098ca",
        "title": "The Windows guide to setting up a Google Cloud instance for the fast.ai course",
        "text": "This is a quick guide for the Windows users who are attending AI Saturdays and need a cloud instance with GPU to run the fast.ai notebooks.\n\nRun the interactive installer with all the default options (ensure option to install Python is checked) until installation is complete.\n\nLog in to your Google account and select the project you created in Step 1. Select \u2018Y\u2019 to configure Google Compute Engine and check your zone.\n\nRight-click on the page, \u201cSave as\u2026\u201d and select a folder on your machine. But before you hit Save, add double quotes \u201c \u201d around the file name and choose Save as type: All Files.\n\nGo to the folder where you downloaded the script. Select the file, right-click and choose \u201cRun with PowerShell\u201d. The PowerShell terminal will launch and create a Google Cloud instance.\n\nThere should now be two new files in the folder. Open the file fastai-instance-<#>-commands.txt and copy the command for connecting to your VM instance and paste it into a new PowerShell terminal (to launch, click the Windows Start Menu and type PowerShell).\n\nA PuTTY terminal should launch and connect you to the VM instance.\n\nReconnect and in the VM terminal, type jupyter notebook and hit Enter.\n\nCopy the IP address, paste it into your browser with an added :8888 behind.\n\nFor the token, copy the long string of characters shown after the \u201c=\u201d .\n\nAnd we are good to go! Happy training!"
    },
    {
        "url": "https://medium.com/ai-saturdays/how-to-read-academic-papers-without-freaking-out-3f7ef43a070f",
        "title": "How to Read Academic Papers without Freaking Out \u2013 AI Saturdays \u2013",
        "text": "When you think about a paper in that perspective it becomes an absurd expectation for any person to be able to read a paper, and subsequently be able to have an in-depth understanding of what the author is trying to convey. Unless you already are an expert of course.\n\nThere is an abundance of academic papers to be read right now, whether it\u2019s for some novel method in Artificial Intelligence or a whitepaper of an ICO (not necessarily an academic paper but presented in the same way), reading academic papers can be quite the headache, especially for the uninitiated.\n\nTo implement papers, we first have to able to read them efficiently. For the most part, people aren\u2019t always taught how to best approach academic papers. As a result of this a lot of time and effort goes to waste, and reading academic papers has sometimes become a chore, even for those who have spent many years doing it already.\n\nSo before you dive head first into exploring the world of emergent research, I\u2019ll share a few tips and tricks that I find useful when reading papers.\n\nReading an academic paper is an entirely different process than compared to reading a blog post or news articles. It is often better to not read the sections in the order that they are presented, but you would also have to go through it multiple times. If you\u2019re just starting out, it\u2019s likely that you will have to spend a lot of time on a single paper (I took a few hours when I started), but you\u2019ll get faster over time for sure. :)\n\nMost research papers will be divided into the following sections: Abstract, Introduction, Methods, Results, and Conclusions/Interpretations/Discussion, References. It really depends on what the subject area is and which journal it\u2019s published too. Some papers come with supplementary material or appendices that complement the existing sections.\n\nIf you intend to become an avid reader, you will have to learn to be able to extract the maximum amount of information with the least effort and time expended.\n\nTo do that, there are a few sections of a paper you should head to first:\n\nIf you read these 3 sections first, you will find that you\u2019ll be able to have a better sense of what\u2019s going on in the paper. Especially in the technically heavier parts like the Methods section. You\u2019ll also be able to decide if this paper is something that is truly relevant to what you are looking for.\n\nRead the Methods section only after you\u2019ve determined that the problem statement, methods taken to solve it and the findings are relevant. Else you might be lost in all the technical jargon that they often include.\n\nThe Methods section is more often than not the most technical part of a paper. It describes the procedures that the authors take to solve the problem. Be prepared to go through this section multiple times, especially if it\u2019s a math heavy piece. Oh and don\u2019t be afraid of losing track of all the mathematical symbols, it\u2019s perfectly normal.\n\nIt\u2019s true. Even after reading hundreds of papers myself, I still wouldn\u2019t be able to understand everything in one go.\n\nWhen you encounter something you don\u2019t understand, there are 2 things you can do:\n\nIf you\u2019re new to the subject area, it\u2019s also a good idea to read papers from the references. As you read more and more papers from a particular subject area, you\u2019ll start to realize that a few common titles would appear more than once. They might be a little dated, but these are often defining papers that introduce the best way to do something.\n\nFinding and reading these papers often boost your understanding of the subject as a whole much more than compared to randomly picking papers to read.\n\nThere are a few phrases that you should always be on the lookout for. To my knowledge there are 2 that are important \u2014 \u201ckey contribution\u201d and \u201csignificant\u201d\n\nThe phrase \u201ckey contribution\u201d often indicates a finding that the paper discovers that captures the entire essence of the paper. If present, finding these phrases alone is enough to give you a brief overview of the paper\u2019s work.\n\n\u201cSignificant\u201d on the other hand indicates something of statistical importance. It\u2019s often a key result from the paper.\n\nAs an academic, it is important that you be inquisitive and curious about the things authors declare in their papers. Do not merely take the words you read as truth and accept them. Pick up the habit of questioning them, ask why this method was used and not another, put yourself in the author\u2019s perspective and figure out their train of thought.\n\nThese are all habits that will naturally groom you into an efficient reader of academic papers, as well as a solid scientist. \ud83d\udc4d\n\nAbove all else, prioritize the speed at which you read. Lingering on a single paper only cuts you off from reading other papers that will yield more insights.\n\nPrioritize efficiency. Don\u2019t waste your time and effort on papers that are irrelevant. There\u2019s way too many out there for anyone to ever hope to read all. You have to learn to be objective about your topics.\n\nAnd lastly, prioritize the value you can derive from any paper. Your goal is to learn, and if you can\u2019t derive any value from it you should move on to something else that will. After all, the world needs scientists like you to make the world a better place. \ud83d\udc4a"
    },
    {
        "url": "https://medium.com/ai-saturdays/reinforcement-learning-part-0-bd755c9cb895",
        "title": "Reinforcement Learning Part 0 \u2013 AI Saturdays \u2013",
        "text": "We are super excited to announce an upcoming Reinforcement Learning Series!\n\nThe series will be in the form of a deep dive into the code with explanations and walkthroughs along side.\n\nThe series will take you through a few concepts in the most beginner appealing way that we can put up.\n\nYou will receive a full Mathematically and Programmatically sound answer in the series, but here is a fun one to begin with.\n\nImagine you\u2019re in a Bakery and have been told to bake a Delicious cake by your Supervisor.\n\nYour Supervisor is rather a strict person who leaves you to discover the best recipe. However, since the Supervisor hates you, she will thrash you every time you bake a bad cake (Probably not the best place to work at).\n\nNow, you\u2019re a smart kid! You start out an experiment. You keep a track of your Performance and the taste of every attempt.\n\nYour end goal is to impress your Supervisor (maximise your reward). You start out as an inexperienced person. You play around the Bakery (Your environment) and keep trying until you finally impress your Supervisor (Reward)\n\nYou start out by adding Salt, by burning down a few things and get Thrashed every time you do so (Receive a penalty) and since you\u2019re smart, you make sure you don\u2019t do this again (Keep a track of previous moves).\n\nIn the end you finally get \u2018Trained\u2019 once you\u2019ve baked the Best Cake and received your highest goal.\n\nSo this is how RL works."
    },
    {
        "url": "https://medium.com/ai-saturdays/geek-board-92f16f87f660",
        "title": "Geek Board \u2013 AI Saturdays \u2013",
        "text": "This idea as proposed by Sanyam Bhutani and as refined by the AISaturdays Team is:\n\nWe plan to promote Learning to the Max and so we are creating a Geek Leaderboard.\n\nThe idea is to distribute Swag points across various cities to people that do cool stuff. We hope to promote some (actually a lot) friendly competition across the cities.\n\nThe Leaderboard will be updated every Friday, the links will be updated on this post, Sanyam Bhutani will take care of allocating the points to everyone and updating the Leaderboard. The board will feature top 5 Geeks from every city.\n\nThe points will be allocated universally regardless of how \u2018beginner\u2019 your project/work is. We hope to promote equal growth amongs beginners and experts by doing this. That said, if you create something amazing, you will have a special mention in the Leaderboard but points will be the same to keep the beginners motivated.\n\nThe Leaderboard will be Updated weekly and all the submissions from every week will be linked to this post which also will be updated weekly.\n\nNote: All of the Projects, Blogs will be featured in every Weekly Leaderboard.\n\nThe notes will be Prepared by Sanyam Bhutani and will be published on Medium, the MarkDown files will be available in our Official Github account, feel free to contribute there and the Notes will be updated on Medium as well by Sanyam.\n\nSpread the word on Twitter with #AISaturdaysGeek when you make a submission."
    },
    {
        "url": "https://medium.com/ai-saturdays/basic-tutorials-part-5-14b177d9bef8",
        "title": "Basic Tutorials Part 5 \u2013 AI Saturdays \u2013",
        "text": "Follow along with this tutorial using the code\n\nIn this post we discuss the Python Programming basics.\n\nAn overview of why we want to use the langauge has been given in Part 0 of this series.\n\nThis post shall serve as an introductory-crash course to Python.\n\nHere is the accompanying Notebook, the code portions discussed here and in the Notebook will be with bits and pieces left for the reader to figure out. We expect a more active participation in learning. Do leave a comment below if you feel anything is missing or have any doubts.\n\nWe suggest you use Jupyter notebooks (Details were discussed in the early posts)\n\nNotice the indents. Python uses indentation instead of using braces to mark the bodies.\n\nTuples can be created by () braces\n\nLists are created by enclosing within [] braces\n\nLists can be roughly linked to deques in their functionality\n\nDictionaries store values in the form of Key Value Pairs\n\nList comprehensions are a neat trick to collapse several lines of codes into one"
    },
    {
        "url": "https://medium.com/ai-saturdays/basic-tutorials-part-4-80723a0aea5d",
        "title": "Basic Tutorials Part 4 \u2013 AI Saturdays \u2013",
        "text": "Welcome to this tutorial, Part 4 of the series on using Jupyter notebooks.\n\nAs a seasoned practitioner, you might want to demonstrate your code. Your insights and the techniques used.\n\nThe notebook is a web application that allows you to combine explanatory text, math equations, code, and visualizations all in one easily sharable document.\n\nNotebooks have quickly become an essential tool when working with data. You\u2019ll find them being used for data cleaning and exploration, visualization, machine learning, and analysis.\n\nThese support one of my favourite philosophies of Literate programming. To qoute Donald Knuth\n\nJupyter notebooks grew out of the IPython project started by Fernando Perez.\n\nIPython is an interactive shell, similar to the normal Python shell but with great features like syntax highlighting and one of my favourite-code completion (For the note, I\u2019m not lazy, I\u2019m a relaxed typer)\n\nThe central point is the notebook server. You connect to the server through your browser and the notebook works as a web app. The code is sent through the server to the kernel. The kernel runs the code and sends it back to the server, then any output is rendered back in the browser.\n\nThe notebooks are saved as .ipynb files, in a JSON format.\n\nJupyter isn\u2019t Python exclusive! The new name Jupyter comes from the combination of Julia, Python, and R. The basic working is the same for any given language. Just the kernel running everything in the background changes.\n\nNote: Since the Notebooks do not have to be on the same machine, we will use to render notebooks on our machine while using a cloud vendor\u2019s offering.\n\nWhen in the environment\n\nYou\u2019ll see a little box outlined in green. This is called a cell. Cells are where you write and run your code.\n\nJuputer also supports Markdown. In the toolbar, click \u201cCode\u201d to change it to Markdown and back. The little play button runs the cell, and the up and down arrows move cells up and down. Notice, the colour is now Blue.\n\nNow there is a lot of things in the toolbar, feel free to play around a bit and familiarize yourself. Here are a few:\n\nTo activate a cell, press enter when it is highlighted.\n\nNow, in my defence I\u2019m not lazy, the touchpad is just too far down on Macbook. So here are a few commands that I think are common:\n\nThese will often become intuitive and you will find the faster rather than searching for these commands via the GUI\n\nThese are few of the shorcuts that are worthy of a quick mention. There many more mentioned in our Git Repo\n\nTo get a list of all commands,\n\nMarkdown is the widely used format to create web documents. This post is written in Markdown. So it\u2019s worth to mention a few of the Syntaxes that are common.\n\nYou may want to leverage this when you create a notebook for showcasing your models/results.\n\nThis is a basic overview of the funtionalities offered by Jupyter notebooks, this is by no means an exhaustive list."
    },
    {
        "url": "https://medium.com/ai-saturdays/basic-tutorials-part-3-4962731e808e",
        "title": "Basic Tutorials Part 3 \u2013 AI Saturdays \u2013",
        "text": "Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux.\n\nConda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.\n\nAnother point worth mentioning is, Open Source projects share their \u2018source\u2019 code; which needs to be compiled everytime you want to use it. However, compiling a huge library can be tedious and time taking. Conda provides precompiled libraries to be downloaded whenever you need to install something new. So you just have to download it and can dive right in!\n\nA detailed tutorial of using Conda and Jupyter notebooks will be shared in Part-1 of this series.\n\nPackage managers are used to install libraries and other software on your computer. Pip is the default package manager for Python libraries.\n\nConda is similar to pip except that the available packages are focused around data science while pip is for general use.\n\nHowever, conda is not Python specific like pip is, it can also install non-Python packages but it does include all the Python packages and supports pip.\n\nWhile creating a Project, you will require various libraries and dependencies. Some projects will require a certain set of libraries which will work only with a given version of a set of other libraries, but at the same point you might want to work on a different set of projects.\n\nTo help with this, Conda creates separate \u2018environments\u2019. A environment X with a set of libraries is independent and unaffected by another environment Y. Thus you can work on your given projects without worrying about \u2018breaking\u2019 the requirements everytime you install something-when you do, conda ensures that the \u2018environment\u2019 works in cohesion by changing other libraries.\n\nConda is the package of Anaconda. If you\u2019ve used virtual env, pip: it includes both of these functionalities along with a few extra.\n\nWe will use conda because we\u2019re geeks. Just kidding, conda will be required when you need to access a cloud instance since you won\u2019t have access to GUI. Plus, it\u2019s easier to type 1 command than click a few buttons (Okay, maybe I wasn\u2019t kidding about the geekiness).\n\nAnaconda navigator which serves as a GUI to the Conda package and includes it is supported on Linux, OS X and Windows.\n\nYou can download the Navigator from Here\n\nSelect your OS and follow the Steps. These are pretty basic and have been omitted. If you\u2019re stuck anywhere or need help, please ask us in the comments below.\n\nThe commands below are to serve as a syntax. We\u2019ve create a Github repository for you to use. Feel free to use the code from there\n\nGithub repository link (Link to be updated soon)\n\nTo use an environment:\n\nNotice that your prompt has a (envname) before it.\n\nTo get out of an environment:\n\nYour project has a certain set of required libraries in order to work. When sharing your project, you share your requirements file so that one can create an environment directly from these.\n\nThis creates a YAML file that contains the list of dependencies of the project.\n\nThis creates a new environment with the name as is \u2014 inside the YAML file.\n\nThis is by no means an exhaustive list of the commands. These are meant to serve for an introductory purposes.\n\nCheckout the Conda user guides if you want to learn more.\n\nHere is another interesting read Common Myths and Misconceptions"
    },
    {
        "url": "https://medium.com/ai-saturdays/basic-tutorials-part-2-ee778e5926cf",
        "title": "Basic Tutorials Part 2 \u2013 AI Saturdays \u2013",
        "text": "A version control system (or VCS) provides an automatic way to track changes in software projects, giving creators the power to view previous versions of files and directories, develop speculative features without disrupting the main development, securely back up the project and its history, and collaborate easily and conveniently with others.\n\nThe most common way of using git is via the Command Line interface. We will demonstrate this because we are geeks! (Er, it\u2019s needed when you\u2019re working remotely)\n\nTo get help\n\nIf you get something like\n\nGit Works as shown: Unstage refers to making the file \u2018staged\u2019 from being untracked (In English, the file is added to our repository in the eyes of Git)\n\nUntracked: Changes are not tracked Unstaged: These aren\u2019t a part of Git\u2019s repository.\n\nReturns the status of the Repository\n\nThis adds the File FileName to the staged area\n\nAdds everything in our repository to git\n\nCommit: When you make changes in a repo, these need to be commited. By design, Git requires every commit to include a commit message describing the purpose of the commit. Typically, this takes the form of a single line, usually limited to around 72 characters, with an optional longer message if desired\n\nTo see a record of your commits\n\nWhen you have made changes in your repository, that have not been commited yet.\n\nThis shows the differences between the previous commit and the current one. (For all you Linux geeks, it runs the diff command in the background)\n\nWhen someone wants to submit changes to your repository, they submit a pull request. You review these changes and then \u2018Merge\u2019 the request into your repo.\n\nWell, generally speaking, your Repository will be available for anyone to view once you put it on GitHub. But Collaborators are people that have direct access to the repo. Consider them the administrators.\n\nDifferent collaborators work on the code simultaneously. To avoid hinderence, everyone works on a branch. A branch is the copy made of the code at point of time, which is used to work upon in a manner to keep other branches unaffected.\n\nMaster branch contains the main code for your Project\n\nA site designed to facilitate collaboration with Git repositories and it\u2019s free to use.\n\nSign up for a GitHub account Here.\n\nNow for all purposes of managing a local repository, I recommend you start with Github Desktop which provides a neat GUI to interact with GitHub and has a set of neat tutorials to get you started.\n\nThis is a fairly advanced topics to share amongst complete beginners and is skipped over for a later post in the series. (Or an update)\n\nThis Blog post might feel a little incomplete-that is because it\u2019s been created in a way to force you to explore the Software. Git is a software to be used and explored, hence that part is left out as an intensive excercise to the reader.\n\nThat being said, if you have any doubt: Do leave a comment below and we\u2019d be happy to help resolve it."
    },
    {
        "url": "https://medium.com/ai-saturdays/basic-tutorials-part-1-fc384b2327a2",
        "title": "Basic Tutorials Part 1 \u2013 AI Saturdays \u2013",
        "text": "These terms are usually used to refer to a Black and white window where people (geeks) usually type in code and work with.\n\nBASH: Bourne Again Shell, is what runs your commands when you type them.\n\nTerminal: The windows is called Terminal, is where you type the commands.\n\nGiven these reasons, we chose to use Terminal for our work.\n\nKnow that this is by no means an exhaustive list and is meant to provide an introductory walkthrough.\n\nThere are many more things to be explored. Feel free to ask for more resources or doubts in the comments below.\n\nThe $ is the prompt. Which means that the terminal is ready and is waiting for your commands to start.\n\ncd=Change directory, by changing your directory. You are changing your present working directory.\n\nThese are the absolute basic commands that should get you started. Feel free to explore more and ask in the comments below for help."
    },
    {
        "url": "https://medium.com/ai-saturdays/basic-tutorials-part-0-b84ee51da37f",
        "title": "Basic Tutorials Part 0 \u2013 AI Saturdays \u2013",
        "text": "We will follow a bunch of libraries in the few courses that we shall follow and this post intends to provide an overview of the same and justification for using these. If you have a different viewpoint or want to question us on any of these points, please leave a comment below and we\u2019d love to chat about it.\n\nThat said, we will probably stick to these options because after extensive exploration, we have concluded that these are the best options for us. But we\u2019re open to any suggestions and places for improvements!\n\nA version control system (or VCS) provides an automatic way to track changes in software projects, giving creators the power to view previous versions of files and directories, develop speculative features without disrupting the main development, securely back up the project and its history, and collaborate easily and conveniently with others. In addition, using version control also makes deploying production websites and web applications much easier.\n\nAt a certain point, you might end up breaking your project and you might want to do a little version control. Git to the rescue!\n\nIt\u2019s a place to brag about repositories by showcasing them and at the same point a place to host the code to allow contributors to add to the Project.\n\nNow many of us here would be beginners and might pick a GUI over a command line, but it turns it that command line can be better than GUI. Some functionalities are just faster done with Command Line, and when you use a cloud service. You will need to use a Command Line to perform actions on your machine.\n\nMost of the commands are run in BASH which is a shell that runs your commands in the background.\n\nA terminal is the interface to the BASH environment on your machine. When you use a terminal, you use the interface to access the BASH environment on your machine\n\nAnaconda is another Open Source project that is the most used amongs the Data Science world. We will chiefly be using Conda and Jupyter Notebooks.\n\nPackage, dependency and environment management for any language \u2014 Python, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN\n\nConda is an open source package management system and environment management system that runs on Windows, macOS and Linux.\n\nConda quickly installs, runs and updates packages and their dependencies. Conda easily creates, saves, loads and switches between environments on your local computer. It was created for Python programs, but it can package and distribute software for any language.\n\nWhile creating a Project, you will require various libraries and dependencies. Some projects will require a certain set of libraries which will work only with a given version of a set of other libraries, but at the same point you might want to work on a different set of projects.\n\nTo help with this, Conda creates separate \u2018environments\u2019. A environment X with a set of libraries is independent and unaffected by another environment Y. Thus you can work on your given projects without worrying about \u2018breaking\u2019 the requirements everytime you install something-when you do, conda ensures that the \u2018environment\u2019 works in cohesion by changing other libraries.\n\nAnother point worth mentioning is, Open Source projects share their \u2018source\u2019 code; which needs to be compiled everytime you want to use it. However, compiling a huge library can be tedious and time taking. Conda provides precompiled libraries to be downloaded whenever you need to install something new. So you just have to download it and can dive right in!\n\nA detailed tutorial of using Conda and Jupyter notebooks will be shared in these series.\n\nPython is the intensely used by Deep learning practitioners to cutting edge researchers.\n\nIt\u2019s an Open Source langauge that has gained tremendous fame in the recent years\n\nThis post serves as a basic overview, hence many technical details will be skipped here.\n\nThe reasons why we want to use this are:\n\nThis series will feature an introduction to Python programming and using it\u2019s two most important libraries for Data Science:"
    },
    {
        "url": "https://medium.com/ai-saturdays/cloud-setup-tutorial-part-0-53d42dd4c733",
        "title": "Cloud Setup Tutorial Part-0 \u2013 AI Saturdays \u2013",
        "text": "Deep Learning is extremely Computation intensive. GPU-Graphic Processing Units seem to perform really well with these computations. Reason: GPUs were orignally devoloped for gaming, which involves a lot of Matrix computations. Deep learning involves a lot of Matrix computations too and recently the GPUs have shown tremendous advances and this has resulted in these being used for Deep Learning.\n\nUsually, GPU resources are not required in our everyday life and we do not have latops or \u2018rigs\u2019 (Desktops) that can leverage this amount of power. So chances are you do not have a GPU at all or have one that would take too long to \u2018train\u2019 or work with Deep learning Models. An easy hassle free alternative is using Cloud Compute for Deep learning.\n\nHassle free? You don\u2019t have to go through the pain of setting every \u2018Library\u2019 and Dependency and you get to jump right in and start out with the lessons.\n\nAnd to start out, it\u2019s pretty cheap. You can get access at 0.6$ an hour (with decent power). Which is way less than buying a \u2018rig\u2019 right away.\n\nThere are a huge number of options that are available now. I will mention a few that I think are good. Know that there are a lot of more options that this blog wouldn\u2019t cover, feel free to discuss about them in the comments below!\n\nThis is roughly in an order of the easiest to setup and in increasing difficulty\n\nCrestle is the simplest interface to play with, once you login you have an option to either start with a GPU or CPU. Then you\u2019re taken directly to the Jupyter notebook\n\nAll of DL libraries installed. No need for any setting up, no using terminals to login (which is initmidating when you start).\n\nThere is access to a terminal still if you want to add/download datasets and install other libraries.\n\nThe pricing doesn\u2019t have many options since you don\u2019t get any. It\u2019s GPU or CPU. The GPUs are Tesla K-80 for you Benchmarking geeks.\n\nPaperspace also offers a clean an intuitive interface along with a few options for the GPU computing. Plus another cool feature is that launching an instance takes you directly to the \u2018Desktop\u2019 view. You get to control the GUI.\n\nYou can always use a terminal to SSH and use the interface.\n\nThat said, the downside of Paperspace is that they have servers in three areas, and the GUI feature might require a good internet connection. (Good is subjective. In my case, I use a 512KBPS connection and can\u2019t use this feature at all, the internet options in your areas ideally would not be this bad)\n\nGoogle Cloud Platform: One of the Big players in the space. The coolest part is, they are offering 300$ for signing up (with a validity of 12 months)\n\nI encourage you to check out our AI Researcher, James Lee\u2019s blog post about it Here\n\nIt\u2019s a walk through of setting up a google cloud computing instance with a 500gb SSD, a 3.75gb ram Broadwell CPU and a Nvidia Tesla K80 GPU. All of this can be done for free at the start, with some details. Repeating these here is unncessary but please feel free to comment below if you to discuss anything.\n\nAWS is arguably the Number 1 cloud service being used right now. Not just in academia, in Industry as well.\n\nThis series will include a walkthrough of setting up and launching instances in the following parts.\n\nAWS offers tonnes of options for all purposes- storage, CPU Power, GPU power, it also hosts the fastest P3 instances in various regions which are really good for huge Deep learning models.\n\nThe costs range from 0.5$ onwards (per hour) for GPU compute. AWS offers some credits to Students as well (If you sign up via a Student ID).\n\nI will skip the details of AWS since we will demonstrate setting up an AWS Instance in this series.\n\nFeel free to ask for help with any other platforms too! Drop a comment below.\n\nThis Blogpost was intended to give a brief intro and has skipped over a lot of details, since many of the Pricing and technical details are highly dynamic. Using a certain vendor is truly a personal choice. I prefer Crestle and AWS, I suggest you should play with all of these options and pick your poison. At a certain point when you are sure you want to pursue the field, a Local server setup will seem to be more viable option. Setting up a Local server will be included in this series.\n\nThis is by no means an exhaustive and complete list. There are many other options to choose from:"
    },
    {
        "url": "https://medium.com/ai-saturdays/aws-tutorial-part-2-61c82f8a5731",
        "title": "AWS Tutorial Part-2 \u2013 AI Saturdays \u2013",
        "text": "In this post, we explore spot instances. Spot instances are an amazing way to cutting down your AWS bills by upto 90%!\n\nWhy Cheap? Well Amazon tends to leave its free servers around for usage of Spot instances; as a user you place a \u2018bet\u2019 on the pricing that you want for the instance. If someone outbids your price or if the load on AWS goes up. Boof! Your instance is gone!\n\nOne more point, unlike normal instances-spot instances do not let you retain your data after stopping the instance. There is no option of stopping it infact, you can only terminate the instance.\n\nNow assuming we are good with the disclaimers and need to cut our costs, lets proceed:\n\nAssumption: You\u2019ve gone through the Part 1 and have a valid AWS account with a access to GPU Compute permissions (you have applied for increase in compute power and have access to >1 according to the limits)\n\nRequesting: So how this works is, you place a \u2018bet\u2019 on the pricing you want and AWS grants you the access based on availability. Now based on the load on AWS servers and the pricing you have requested, you may have to wait for a while.\n\nKey Pair: We use Secure Shell Login or SSH to login to our instances, the security is ensured by A Key-Pair which gives you access to the instance"
    },
    {
        "url": "https://medium.com/ai-saturdays/aws-setup-tutorial-part-1-f3ee3c77fe3d",
        "title": "AWS Setup Tutorial: Part 1 \u2013 AI Saturdays \u2013",
        "text": "AWS or Amazon Web Services are a bunch of Cloud services that are provided by Amazon.\n\nThere a bunch of options to choose from for Deep learning, I prefer AWS the most because of the huge number of options that it offers.\n\nNow you might have to wait for a while untill you hear from Amazon, once your limit increase has been approved\n\nAMI is an \u2018Image\u2019 of the OS with everything installed. Consider a snapshot of the states of OS after installing all the software. It can save you the pain of setting it all up, you can dive right into working.\n\nKey Pair: We use Secure Shell Login or SSH to login to our instances, the security is ensured by A Key-Pair which gives you access to the instance"
    }
]