[
    {
        "url": "https://medium.com/@Synced/ai-e-mail-assistants-streamline-meeting-scheduling-9705f5f8dd7a?source=user_profile---------1----------------",
        "title": "AI E-Mail Assistants Streamline Meeting Scheduling \u2013 Synced \u2013",
        "text": "Emailing back and forth to book meetings can be a huge waste of time, especially when people are in different time zones, don\u2019t know each other\u2019s availability, or schedules change, etc.\n\nIn a Doodle survey of 1500 professionals, respondents needed an average of 4.75 hours to arrange nine weekly meetings with seven parties. A personal assistant could handle that, but for that would be cost prohibitive for most. People need an easy and affordable solution to scheduling meetings by email.\n\nWith all the progress being made in Artificial Intelligence (AI) and Natural Language Processing (NLP), how about building an AI powered virtual assistant? The idea of intelligent personal assistants and AI scheduling appeared more than years ago, but early research never took off: AI algorithms were not strong; there was a lack of quality training data; and the whole scheduling problem remained too complex. It took years, but now several promising AI meeting scheduling startups and products are emerging.\n\nClara Labs is a San Francisco startup that created the industry-leading Clara AI assistant, which can automatically schedule meeting times and places based on attendants\u2019 preferences. Users simply \u201cCc\u201d Clara on meeting invites, and she sorts it out. If Clara can\u2019t, one of the company\u2019s human staff members will step in. Clara Labs raised $12 million in two funding rounds, with a US$7 million series A led by Basis Set Ventures and Slack Fund in July 2017.\n\nThe New York based startup\u2019s AI personal assistants Amy and Andrew have meeting scheduling workflows similar to Clara Labs. X.ai however offers a free basic plan, with features similar to Clara\u2019s US$99/mo essential package. X.ai was founded in 2014 and has attracted US$32.3 million in funding.\n\nThe Calendar.help project started years ago, and enables Microsoft digital assistant Cortana to arrange meetings and schedule tasks. In 2016, Microsoft also acquired Genee, an end-to-end scheduling tool that integrates with calendar apps and email providers. As a Microsoft product this solution can naturally integrate with Office 365. Calendar.help values performance over speed and automation, with a workflow that brings both algorithms and human assistants into the scheduling loop.\n\nJulie Desk is a French startup that has raised US$3.69 million, while Evie.ai is a Singapore based startup. Both were founded in 2014. Their AI email assistants Julie and Evie are X.ai and Clara Labs\u2019 overseas competition.\n\nFin is a San Francisco based startup founded in 2015. Its powerful personal assistant leverages AI techniques in speech recognition and natural language processing with human assistants, and can call, text, email, schedule, research, book, and even make purchases.\n\nWill AI and robots take over from secretaries and personal assistants? Not yet. \u201cDespite advances in natural language processing, extracting information from free-text is still error-prone and algorithms often make mistakes that seem trivial to humans\u201d, says a Microsoft researcher. This is why most of the startups we surveyed still involve humans in their AI-powered products.\n\nAssistant systems are also not yet scaling up. Clara Labs\u2019 median virtual assistant email response time is a sluggish 44 minutes, which raises concerns about how the system will perform if hundreds of thousands of people use it at same time. Moreover, vendor contracts and policy and procedure standards need to be developed to address security and privacy issues. We can expect to see improvements in AI assistants\u2019 control functions and automation activities to address these operational risks.\n\nThere is no doubt that future email assistant systems will have fewer humans in the loop, process messages faster, and be more accurate. They may also integrate with more platforms, such as team collaboration tool Slack or social media app WeChat. Tomorrow\u2019s conversational AI assistants will be more functional and push deeper into enterprise. They may also become stenographers, make meeting contents searchable, follow up on action items, or highlight key moments of a telephone call. This may sounds futuristic, but researchers are already working on it!"
    },
    {
        "url": "https://medium.com/@Synced/can-ai-reinvent-the-tv-7dd56372425f?source=user_profile---------2----------------",
        "title": "Can AI Reinvent the TV? \u2013 Synced \u2013",
        "text": "Not to be outdone by Skyworth, other major global TV makers are jumping on the AI TV bandwagon. At this year\u2019s Consumer Electronics Show Samsung Electronics introduced an AI-applied 85-inch 8K QLED TV, while LG integrated its AI system ThinQ into its new 4K OLED and Super UHD TVs. At February\u2019s Appliance & Electronics World Expo in Shanghai, TCL showcased its X5 TV within facial recognition technique, and Hisense launched its VIDAA AI TV system.\n\nSkyworth believes the AI TV will help them bounce back. The company\u2019s stock price surged by 18 percent after the Baidu agreement was announced.\n\nWhile the term \u201cAI TV\u201d may suggest a fully intelligent, Skynet-like TV bot, it rather refers to incorporating machine learning techniques such as facial recognition, voice-controlled assistants, personalized content suggestion, sentiment analysis, etc into the product. AI TV also sounds a lot fancier than Smart TV.\n\nThe TV has come a long way. Wood cabinets with monochrome cathode ray tubes evolved into HD flat colour screens with remote controls; while channel choices expanded from a handful to thousands. The device\u2019s latest incarnations include so-called Smart TVs, and the latest buzz in the consumer electronics community: the \u201cAI TV\u201d of the future.\n\nA Chinese TV manufacturer for more than 30 years, Skyworth is going through rough times. The company\u2019s domestic TV sales fell 16 percent in 2017, while its global TV sales slipped three percent, part of an industry-wide trend.\n\nLeading television manufacturer Skyworth has signed a strategic cooperation agreement with Chinese internet giant Baidu. In the deal announced March 17, Baidu will invest CN\u00a51.01 billion (US$159.7 million) in Skyworth\u2019s Smart TV unit Coocaa; while its AI assistant system DuerOS will be integrated into Skyworth\u2019s agenda-setting Super AI TV.\n\nAI TV\u2019s voice-based interface takes the remote control out of the loop. Jing Kun is Baidu Project Manager of DuerOS, a Mandarin-based equivalent of Alexa. He told Synced he believes the TV is one of the most promising application areas for AI assistants. \u201cWhile people are watching a TV, it\u2019s difficult for them to interact with it. An intelligent voice interaction may be the way to push the user experience to the next level,\u201d says Jing.\n\nMost major TV manufacturers lack the R&D capabilities to build their own voice interaction system from scratch. As a result, they seek joint development agreements with AI tech providers such as Google, Amazon, or Chinese tech giants. LG\u2019s new TVs have added Google Assistant and Alexa. Google Assistant is also in Sony\u2019s X900F. Samsung is integrating its AI assistant Bixby \u2014 already available in its phones \u2014 into TVs as well.\n\nAnother use of AI is to better personalize content recommendations. Though Hulu, Netflix, Amazon Prime and other content providers have their own AI recommendation engines, AI TVs can search across all platforms for actors, titles, genres and other preferences, and tune content recommendations to suit family members.\n\nAlex Haslam, a cord-cutting expert at Utah-based HowtoWatch.com, told Synced, \u201cToday\u2019s viewers value choice, which is why streaming services like Netflix are so popular. Your TV being able to adapt to those choices will enhance the experience.\u201d Scott Amyx, Managing Partner at Amyx Ventures, takes it further: \u201cAI will eventually understand that streamlining preferences vary by day, time, mood, family member and who they are with.\u201d\n\nToday\u2019s AI TVs will know what content was watched, when and for how long, and this data can be sold to advertisers.\n\nBut with front-facing cameras and sophisticated algorithms, the AI TV could also know who is watching, and whether for example they appear engaged, or look away during commercials, or even doze off. Amyx told Synced he has spoken with researchers and advertising executives who are continually seeking new ways to get the attention of viewers. \u201cFor example, through loud sounds that bring them back to the ad on the screen, a certain pitch of voice, etc.\u201d\n\nTVs capturing user behavior data has raised concerns about privacy and even risks. Consumer Reports recently discovered that millions of Smart TVs might have security vulnerabilities. Hackers could for example remotely change channels, play offensive content, or crank up the volume. The report suggest Samsung, TCL and other TV makers using the Roku Smart TV platform, as well as streaming devices such as the Roku Ultra, may be affected.\n\nIt\u2019s still early to say whether AI TVs will live up to expectations. At present the hype may be greater than the performance. According to the China Electronic Chamber of Commerce and JD Appliance 2017 White Paper on Artificial Intelligence Television, \u201cSome AI TVs can only achieve simple speech interaction, and lack AI-specific \u2018cognitive-judgment-decision making\u2019 capabilities.\u201d\n\nManufacturers believe AI will soon be essential in TVs. Says Skyworth Founder Huang Hongsheng, \u201cIn the past, the remote control changed how watchers interact with televisions, so they didn\u2019t have to walk across the room to press a button. Today\u2019s AI can open another new path of convenience and communication, by making a television that \u2018gets to know you\u2019.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/cambricon-unveils-its-first-ai-chip-for-cloud-computing-d3f7acdb4076?source=user_profile---------3----------------",
        "title": "Cambricon Unveils its First AI Chip for Cloud Computing",
        "text": "Cambricon Unveils its First AI Chip for Cloud Computing\n\nCambricon today unveiled its Cambricon 1M chip for edge computing, and the MLU100, the first in a new chip series for cloud computing.\n\nCambricon 1M is the company\u2019s third generation AI chip for edge devices. With its TSMC 7nm technology, the AI chip provides efficiency of 5 TOPS/Watt for 8-bit computing. The 1M chip is available in 2, 4, and 8 TOPS versions to support a range of AI applications.\n\nLike the company\u2019s 1H and 1A chips, Cambricon 1M supports deep learning models such as CNN, RNN, SOM; and supports SVM, k-NN, k-Means, decision tree and other algorithms. This is also the world\u2019s first AI processor supporting local machine learning training.\n\nCambricon MLU100 is the first generation of Cambricon\u2019s new product series supporting cloud computing. MLU100 adopts Cambricon\u2019s latest MLUv01 architecture and TSMC 16nm technology. This processor has the capability of providing 128 TFLOPS/166.4 TFLOPS in balance mode and high-performance mode respectively.\n\nCambricon and its partners demonstrated several use cases: Lenovo\u2019s new ThinkSystem SR650 server is based on the MLU100, as is Sugon\u2019s new PHANERON series of products. iFlytek has also announced a collaboration with Cambricon."
    },
    {
        "url": "https://medium.com/syncedreview/ubtech-robotics-gets-us-820-million-funding-becomes-the-worlds-most-valuable-ai-startup-c25cd357e87e?source=user_profile---------4----------------",
        "title": "UBTECH Robotics Gets US$820 Million Funding; Becomes the World\u2019s Most Valuable AI Startup",
        "text": "UBTECH Robotics Gets US$820 Million Funding; Becomes the World\u2019s Most Valuable AI Startup\n\nChinese AI and humanoid robotic company UBTECH Robotics today announced a staggering US$820 million in Series C funding. With its new estimated value of US$5 billion, Shenzhen City based UBTECH becomes the world\u2019s most valuable AI startup.\n\nInternet giant Tencent led the funding with a US$120 million investment. It is believed UBTECH\u2019s capabilities in robot design and manufacturing will strengthen Tencent\u2019s AI products. Last year, Tencent reportedly pumped US$40 million into UBTECH. Also joining the funding are the Industrial and Commercial Bank of China, Haier, Minsheng Securities, Telstra, CDHFund, and others.\n\nUBTECH Founder and CEO Zhou Jian told Synced that \u201cthis round of financing will be mainly used for strengthening R&D capabilities, facilitating marketing and brand development, and attracting top-tier talents.\u201d\n\nFounded in 2012, UBTECH aims to \u201cbring a robot into every home, and truly integrate intelligent robots into the daily lives of everyone creating a more intelligent way of life.\u201d The company\u2019s Alpha 1S robot holds the Guinness World Record for \u201cmost robots dancing simultaneously.\u201d A video of UBTECH robodog Jimu dancing and licking paws at the 2018 CCTV Spring Festival Gala went viral. Also in the UBTECH family are Cruzr, an intelligent service robot; the voice-activated, video-enabled companion Lynx; and a Star Wars First Order Stormtrooper robot. The company even produces a BuilderBots Kit for children who want to build robots.\n\nWhile the global robotic market is competitive, UBTECH has an advantage with its technological development of humanoid robot servos and motion controlled gait algorithms. UBTECH is a global leader in robotic joint and body structure manufacturing.\n\nZhou says UBTECH\u2019s sales goal for 2018 will exceed CN\u00a52 billion. He also unveiled the company\u2019s roadmap for the next four years:\n\nZhou\u2019s vision for robots goes beyond the crunching of algorithms and whirring of servos. Rather he stresses the relationship of such machines to human beings. \u201cIn UBTECH, we insist on viewing the robot as a member of a future family.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/ml-community-pledges-to-boycott-natures-new-paywalled-journal-6429d9d9a800?source=user_profile---------5----------------",
        "title": "ML Community Pledges to Boycott Nature\u2019s New Paywalled Journal",
        "text": "Thousands in the machine learning community say they will boycott Nature\u2019s paywalled Machine Intelligence Journal, which is set for a January 2019 release. So far, 2482 researchers have signed a petition pledging to not submit any work to the journal or review or edit any of its papers.\n\nThe key objection to the new publication is its subscription-based revenue model, wherein the journal will charge a paper submission fee and individual and institutional subscription fees.\n\nThis, of course, touches a nerve in the machine learning research community, which prides itself on an open sharing culture. It has been easy for the public to access ML papers, codes, and in some cases even datasets. Up-to-date papers from major conferences such as NIPS and CVPR can also be easily shared. ArXiv is sometimes criticized for its relatively loose review mechanism, but its huge volume of papers can nevertheless be accessed by a global readership free of cost.\n\nLeading the boycott call are deep learning pioneers Geoffrey Hinton, Yann LeCun and Yoshua Bengio; research leads at Google, Facebook, Amazon and IBM; and academics from MIT, Stanford, CMU, and Oxford. OSU Professor Emeritus Thomas G. Dietterich, who initiated the petition, was Executive Director of the journal Machine Learning from 1992\u20131998, before the publication became the open-access Journal of Machine Learning Research (JMLR) in 2001.\n\nNature announced plans for Machine Intelligence last November, as a new online-only publication that would cover the \u201cbest research across the field of artificial intelligence\u201d with relevant reviews and commentaries. All editors are internal staff working under Chief Editor Liesbeth Venema.\n\nOpen access in academic publications is an ongoing issue. Research access in areas such as biology, neuroscience, psychology, and social sciences can be extremely costly, even for wealthy institutions like Harvard.\n\nThe academic community has been battling publishers for open access for a decade. Sci-Hub, created by Alexandra Elbakyan in 2011, has an archive of over 64.5 million papers available for direct download. Reddit founder Aaron Swartz\u2019s attempts embroiled him in a legal fight with Jstor and MIT that ended in tragedy.\n\nOutside the machine learning community, research papers in other AI subfields such as robotics and hardware chips are difficult for non-specialists to obtain. There are also labs like Boston Dynamics that can live outside the \u201cpublish-or-perish\u201d curse.\n\nThe machine learning community is rebelling against a publishing orthodoxy it regards as an impediment to progress. In a Reddit discussion thread, one researcher posted that he would judge a paper based on its ability to spread and instruct further research, rather than focusing on the publisher\u2019s prestige."
    },
    {
        "url": "https://medium.com/syncedreview/focus-on-ai-at-facebook-f8-6dbe7e04b565?source=user_profile---------6----------------",
        "title": "Focus on AI at Facebook F8; PyTorch 1.0 Released (Updated)",
        "text": "Facebook generally uses its F8 Developer Conference to introduce new platform features. This year however there was a distinct emphasis on \u201cAI,\u201d with the term mentioned more than ever in the event\u2019s keynote speech.\n\nFacebook Founder and CEO Mark Zuckerberg unveiled the company\u2019s strategic AI roadmap, pushing R&D in vision technologies, unsupervised learning, natural language processing, reinforcement learning, generative networks, and AI developer tools. Facebook also announced several AI-related updates to elevate the user experience.\n\nRussian bots, fake news and the Cambridge Analytical scandal have hit Facebook hard. The seminal social media platform \u2014 where billions of users share photos and videos of their lives with family and friends, and exchange thoughts and opinions on events of the day \u2014 is increasingly revealing a dark side, as a malignant tool for spreading false information, instigating hatred, and even influencing elections.\n\nFacebook wants to fight back. Zuckerberg told the F8 audience the company is now using AI algorithms to detect \u201cspammers who just want money,\u201d \u201cfake accounts created by bad actors,\u201d and \u201creal people who are sharing fake information.\u201d\n\nFacebook has previously suggested they were deploying cutting-edge AI to recognize fake news and bot-generated accounts. In 2016, AI pioneer and then Facebook Director of AI Research Yann LeCun said AI could be used to identify fake news or violence in live video content on the site. In 2017, Facebook announced that they would use AI to detect terrorism-related posts.\n\nAI researchers are divided on whether machine learning algorithms can effectively detect fake news. Dean Pomerleau is a research scientist and entrepreneur who helped organize a crowdsourcing challenge to develop machine learning solutions to combat fake news. He told The Verge that in his opinion AI couldn\u2019t fix the fake news problem. On the other hand, Aaron Edell, CEO of AI company Machine Box, claims to have produced a machine learning algorithm that can detect fake news with higher than 95% accuracy.\n\nZuckerberg faced questions about Facebook\u2019s data privacy practices at a congressional hearing last month, where he estimated the company would need five to ten years to build an effective AI-powered fake news detection system that leaves humans out of the loop.\n\nThe Instagram Explore page is a powerful feature that recommends posts based on personal interests and tastes. Users can browse content across topics of interest, even from accounts they don\u2019t follow.\n\nFacebook Data Science & Analytic Manager Tamar Shapiro announced at F8 that Facebook will revamp Instagram Explore to better organize its recommended content into different topic channels, and this new Explore page will be powered by AI.\n\n\u201cIn order to deliver cutting-edge experience, we are augmenting AI with content classification and curation signals from our community,\u201d said Shapiro.\n\nShapiro also introduced a new AI-powered \u201cbullying comment filter,\u201d which can hide content that disturbs or upsets users. Last year, Instagram launched an offensive comment filter, which can automatically hide comments it deems \u201cdivisive\u201d or \u201ctoxic.\u201d\n\nM Translation expands on M Suggestion, a pop-up feature launched last year that suggests relevant content and capabilities. Now, when users receive a Marketplace message in a language different from their default, M Translation can translate the message into their default language.\n\nFacebook smart speakers to sell only outside the US?\n\nRivals Google, Amazon, and Microsoft have already jumped on the smart speaker bandwagon, as have major Chinese tech companies. Now Facebook is rumoured to be readying their own line of smart speakers, although Zuckerberg was tight-lipped about the plan at F8.\n\nMultiple media reports suggest Facebook will launch two devices this July in overseas markets. The unusual marketing plan is said to be due to Facebook\u2019s slipping trustworthiness among users in the US. The smart speaker is expected to be equipped with a touchscreen and camera, and will be powered by the text-based chatbot Facebook Messenger bot, M, which will likely get an upgrade to voice assistant.\n\nOn F8\u2019s second day, Facebook announced PyTorch 1.0, the latest version of its open-source AI software framework that guides and supports researchers from research stages to deployment of trained models for various AI applications. \n\n \n\nVP of Facebook Infrastructure Bill Jia said \u201cPyTorch 1.0 takes the modular, production-oriented capabilities from Caffe2 and ONNX and combines them with PyTorch\u2019s existing flexible, research-focused design.\u201d \n\n \n\nFacebook is pushing the combined PyTorch \u2014 Caffe2 framework. Last month, the Caffe2 Github page introductory \u201creadme\u201d document was suddenly replaced with a link: \u201cSource code now lives in the PyTorch repository,\u201d which enabled Caffe2 users to directly check Caffe2 code in PyTorch.\n\n \n\nSince its release in October 2016, PyTorch has become a preferred machine learning framework for AI researchers due to its flexibility. Over half of Facebook AI projects run on PyTorch. PyTorch 1.0 will be available to beta users later this summer.\n\n \n\n Facebook also announced the open-sourcing of many AI tools, including Translate, a PyTorch Language Library for neural machine translation, and ELF OpenGo, an AI bot based on the ELF (extensive, lightweight and flexible) platform for training gameplay AI with reinforcement learning. \n\n \n\n Facebook Chief AI Scientist Yann LeCun tweeted that the ELF OpenGo bot \u201chas attained professional level in two weeks of training and has won 15 games against 4 top professional human players.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/accutarbio-advances-ai-in-drug-discovery-e28c50abdec3?source=user_profile---------7----------------",
        "title": "AccutarBio Advances AI in Drug Discovery \u2013 SyncedReview \u2013",
        "text": "In December 2017 Chinese pharma tech startup AccutarBio raised US$15 million from IDG Capital, YITU Tech, and ZhenFund. This was one of the country\u2019s largest pharma tech funding rounds, and signaled AI\u2019s strong promise and potential in early-stage drug discovery.\n\n\u201cDrug discovery is never subjective,\u201d AccutarBio CEO and Co-founder Dr. Jie Fan tells Synced. \u201cA person may have a good knowledge of thousands of types of drugs. But there is no way they know ten thousand or even one hundred thousand drugs. In contrast, machines can do a much better job than humans.\u201d Dr. Fan also believes that hybrid AI can accelerate drug discovery for targeted therapy and provide cancer patients with further alternative treatment plans.\n\nDr. Fan graduated from Fudan University with a BA and received his Master\u2019s in Biostatistics from the University of California Berkeley in 2004. He completed his PhD under Dr. Nikola Pavletich, studying the crystal structure of DNA binding proteins. Fan then worked as a postdoc researcher under 1999 Nobel Prize laureate G\u00fcnter Blobel, focusing on the structural analysis of nuclear pore complexes (NPCs), which are large protein complexes that allow transport of molecules across the nuclear membrane.\n\nAccutarBio continues to widely collaborate with academics for cross-interdisciplinary research, and the company has deployed labs in both Shanghai and New York.\n\nIn the research paper Chemi-net: a Graph Convolutional Network for Accurate Drug Property Prediction, AccutarBio extended the use of traditional statistical learning methods to create a multi-layer DNN architecture called \u201cChemi-Net\u201d, which can predict ADME (absorption, distribution, metabolism, and excretion) properties of molecular compounds. ADME study is crucial and should be done at an early stage in drug discovery as it helps researchers understand the transport of molecules in organisms and can efficiently eliminate weak drug candidates, increasing success rate in drug trials and shortening drug discovery timelines.\n\nChemi-Net was tested on 5 ADME endpoints \u2014 human microsomal clearance, human CYP450 inhibition, aqueous equilibrium solubility, pregnane X receptor induction, and bioavailability \u2014 with 13 industrial grade datasets selected for predictive model development, involving more than 250,000 data points. Both single-task and multi-task Chemi-Net exhibited dramatic predictive accuracy improvements over benchmarks. Researchers expect Chemi-Net\u2019s significantly increased ADME prediction accuracy to greatly accelerate efficiency and success rates in drug discovery.\n\nFollowing on the success of their AI-powered drug discovery model, AccutarBio plans to further develop and promote Chemi-Net with the aim of revolutionizing traditional experiment-based and experience-based drug development.\n\nAI has demonstrated its abilities in healthcare applications in medical and pharmaceutical fields. The tech\u2019s capability for \u2018learning\u2019 meaningful features from large datasets has been widely used in clinical situations involving computer-aided image detection and diagnosis (e.g., assisted image-based early cancer screening), implementation and maintenance of electronic health records, and continuous patient monitoring.\n\nAccutarBio strategic investor YITU Tech says \u201cAsymmetric information in multidisciplinary fields has brought huge barriers to technology development. We hope our knowledge and work in AI will be able to assist AccutarBio and make great advances in biology. We believe artificial intelligence will make significant difference on the current status of biological research and biopharmaceuticals through our in-depth cooperation with AccutarBio. And collaboration with YITU for Medical AI-powered research in clinical applications will bring more profound value.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-350-kph-self-driving-train-gets-ready-to-hit-the-rails-2301e7753811?source=user_profile---------8----------------",
        "title": "China\u2019s 350 kph Self-Driving Train Gets Ready to Hit the Rails",
        "text": "On April 29th, the Assets Supervision and Administration Commission (SASAC) of China\u2019s State Council announced that China Railway Signal & Communication Corp (CRSC) had completed lab testing of their high-speed rail self-driving system (C3 + ATO), and is preparing for field testing. The system will be deployed in Fuxing Hao trains, which are the world\u2019s fastest with an operating speed of 350 km per hour.\n\nThe new self-driving high-speed rail system integrates various technologies, including cloud computing, IoT, artificial intelligence, big data, etc. The rail system infrastructure will include facial recognition for check-in, robot porters and other intelligent services.\n\nCRSC is a world-leading rail signal and communication service provider. It owns the world\u2019s biggest rail technology lab, and developed the Chinese Train Control System (CTCS).\n\nCRSC says they will complete assembling the smart train by the end of 2018, finish rail tests and adjustments by early 2019, and then officially open the \u201cBeijing-Zhangjiakou Intelligent Rail Line\u201d in late 2019, enabling passengers to complete the over 200 km journey in just 50 minutes."
    },
    {
        "url": "https://medium.com/syncedreview/iclr-2018-kicks-off-in-vancouver-f3a99bab70e0?source=user_profile---------9----------------",
        "title": "ICLR 2018 Kicks Off in Vancouver \u2013 SyncedReview \u2013",
        "text": "AI researchers are gathering in Vancouver, Canada for the sixth annual ICLR (International Conference on Learning Representations). The event starts today and runs to May 3rd at the Vancouver Convention Centre.\n\nCompared to NIPS, ICML, and ACL, the ICLR is a relative newcomer. Founded in 2013 by deep learning mavericks Yoshua Bengio and Yann LeCun, the conference has become an important destination for AI researchers.\n\nProgram chair Yann LeCun tweeted that \u201cattendance at ICLR doubles every year. It\u2019s up to 2000 this year. It merely doubles because attendance is capped. The number of submissions also doubled from last year. [Google Researcher] Tara Sainath is opening the conference.\u201d\n\nThis year 337 out of 935 paper submissions were accepted, with best paper honours going to On the convergence of Adam and Beyond, Spherical CNNs and Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments.\n\nA statistical analysis by Arthur Pajot shows Google leading both the paper submission and acceptance charts, ahead of academic institutions Berkeley, Stanford, CMU, UofT, and ETH; and corporate research teams from Facebook, Microsoft and IBM. Google is dispatching 130 researchers to the conference and pitching in US$40,000 as a platinum sponsor.\n\nThe most prolific individual submittors are Yoshua Bengio, who submitted 18 papers, (9 rejected, 7 posters, 2 workshops), and UC Berkeley\u2019s Pieter Abbeel with 12 submissions (4 rejected, 4 posters, 2 workshops, 2 orals). The top-submitting countries are the United States, United Kingdom, China, Canada, and Germany.\n\nA paper keyword analysis conducted by Pau Rodr\u00edguez suggests \u201cmeta-learning, exploration, model compression, adversarial examples, variational inference are the hottest topics this year. For instance, 85% of the papers with the keyword exploration were accepted, while classification and CNN only show 12% acceptance rate.\u201d\n\nThe ICLR is known for popularizing the open review system designed by Andrew McCallum from UMass Amherst. Papers submitted to the conference are published on http://OpenReview.net, publicly reviewed and archived on the site."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-apr-w-4-apr-w-5-c31ec014c9ba?source=user_profile---------10----------------",
        "title": "AI Biweekly: 10 Bits from Apr W 4 \u2014 Apr W 5 \u2013 SyncedReview \u2013",
        "text": "Google introduces starter kits designed to help people learn and experiment with AI solutions. The AIY kits (a pun on DIY/do-it-yourself) and are aimed at students, who can use for example the Voice Kit to develop a voice-controlled speaker, or the Vision Kit for tasks like object detection, facial detection etc. Each kit comes with a Raspberry Pi Zero WH board.\n\nMicrosoft users can now run translations without an Internet connection, thanks to a new app offering modified neural network translations. The offline translation quality is high, although not as good as online translation backed by AI on the cloud. Offline translation is available for Arabic, Chinese-Simplified, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish and Thai. More will languages will be added later.\n\nApril 19th \u2014 Dubai Uses AI to Turn Waste Into Energy\n\nAs part of its \u201cVision of the Future City, Today\u201d initiative, Dubai Municipality launches its AI-empowered Wasteniser project, which sorts solid waste by type using AI tech, and incinerates materials at optimal temperatures to produce good ash that can be used in the production of green concrete. The energy generated goes to the local electrical grid.\n\nWith Amazon\u2019s new Alexa Blueprints, users can create their own Alexa skills without any coding knowledge. Alexa Blueprints comes with 20+ templates, including Fun & Games, At Home, Storyteller, and Learning & Knowledge. Although these user-created skills will not be available on the Alexa Skills Store, they will appear on a \u201cSkills You\u2019ve Made\u201d page on the Blueprint website. Alexa Blueprints gives Amazon an edge over Apple\u2019s HomePod and Google Home, which do not offer such custom skill creation opportunities. The service is currently only available in the US.\n\nGoogle publishes research showing quick detection of cancer cells using a new Augmented Reality Microscope technology. Google believes the AI-backed ARM will \u201caccelerate and democratize the adoption of deep learning tools for pathologists around the world.\u201d The ARM is compatible with most current devices and can be easily retrofitted.\n\nApril 23rd \u2014 NVIDIA and DEEPCORE Team Up to Boost AI Startups in Japan\n\nNVIDIA announces a partnership with SoftBank-owned startup incubator DEEPCORE, which will use NVIDIA\u2019s AI computing platform to expand its support for startups and promote AI research in Japan. NVIDIA will provide technical training and industry advice for DEEPCORE customers and incubator members via its Deep Learning Institute.\n\nAsahi Shuzo is applying Fujitsu AI technology in a trial project to help visualize and optimize their sake brewing process. The Fujitsu Human Centric AI Zinrai system aims to \u201csystematize the experience and know-how of each employee\u201d to widen and improve real-time monitoring of the sake production process. Fujitsu built the predictive AI model based on the sake brewing company\u2019s historical data.\n\nAlibaba announces collaborations with automakers Daimler, Audi, and Volvo to use its voice assistant Tmall Genie in their cars to monitor battery level, mileage, engine status, etc., and allow remote management via voice command of car status including doors, windows and air conditioning etc. \u201cCars are an environment, alongside the home and the office, where individuals spend a significant amount of time, and which through connectivity can become an important part of life,\u201d said Lijuan Chen, head of Alibaba AI Labs, which created the Genie.\n\nWeb service company IFTTT announces US$24 million in funding from Salesforce, IBM, and others. The investment will be used to hire talents, develop enterprise business and IoT services, and address problems in the market. \u201cEvery business is undergoing a dramatic transformation into a digital service,\u201d says IFTTT CEO Linden Tibbets. \u201cWe could not be more excited to partner with our new investors, our passionate users, and every business working hard to become a service.\u201d\n\nTesla\u2019s ex-autopilot head Jim Keller leaves the autonomous driving company to lead Intel\u2019s silicon engineering team, where his research will focus on SoC development and integration. \u201cJim is one of the most respected microarchitecture design visionaries in the industry, and the latest example of top technical talent to join Intel,\u201d said Dr. Murthy Renduchintala, Intel\u2019s chief engineering officer and group president of the Technology, Systems Architecture & Client Group (TSCG)."
    },
    {
        "url": "https://medium.com/syncedreview/amazon-patents-provide-clues-to-new-home-robot-dedbafe0e1b7?source=user_profile---------11----------------",
        "title": "Amazon Patents Provide Clues to New Home Robot \u2013 SyncedReview \u2013",
        "text": "The robotics community is abuzz with speculation following reports that Amazon\u2019s R&D arm Lab126 is hiring senior researchers for a home robot-producing project codenamed \u201cVesta.\u201d The company is also said to be recruiting employees who will test the robot in their homes later this year.\n\nNo one knows if Amazon envisions an enhanced roomba or a precocious R2D2 V2.0. Company Founder and CEO Jeff Bezos was recently spotted at a tech conference walking Spotmini, the Boston Dynamics robotic dog known for autonomously opening doors: Might the bot take the form of a pet?\n\nOne thing is certain: Amazon will cram Alexa into the robot. The company\u2019s star virtual assistant has evolved into a know-it-all that can play music and audiobooks, compile to-do lists, set alarms, stream podcasts, and provide real-time weather, traffic, news and other personalized information. Although most people only use their virtual assistants for music streaming, controlling smart lights, timers and weather forecasts, according to an IFTTT survey, Alexa is backed by so much cutting-edge AI tech that she\u2019s become too smart for just a smart speaker.\n\nWith Alexa as the brains, the bot may be more humanoid, both in physical form and in personalization and communication scope and style. Sarah Osentoski, co-founder of the robotics company Mayfield that makes companion robot Kuri, told Wired, \u201cWhen you have something that\u2019s talking to you and that\u2019s driving around your house you start to expect a lot more. You start to expect the intelligence of a 3-year-old or a 5-year-old.\u201d\n\nThat could be why Amazon is scrambling to update Alexa with more powerful capabilities. At the recent World Wide Web Conference in Lyon, France, the Head of the Alexa Brain group Ruhi Sarikaya said Alexa will soon launch three new features: an enhanced memory system that remembers everything users say, a more natural conversational system, and automatic activation of over 40,000 third-party skills.\n\nTo recognize its owner and family members, navigate rooms and hallways and so on, Amazon\u2019s new robot will certainly require robust computer vision skills. In 2016, Amazon acquired Orbeus, a Silicon Valley startup that specializes in image recognition. Amazon hired nearly all of Orbeus\u2019 technical staff, and picked up its facial recognition system and scene recognition system patents.\n\nLeveraging facial recognition, the robot will be able to capture images and record videos automatically. Amazon has patented approaches for creating high quality images with less blur and noise. The bot for example might be put in charge of photographing a birthday party, or compiling family photo and video albums.\n\nThe new robot might also employ multimodal dialogues to better watch, listen, speak, and physically interact with humans. Dr. Zhou Yu, an assistant professor at University of California in Davis, told Synced she is enabling intelligent assistants such as Siri and Alexa to recognize actions such as facial expressions and give appropriate responses. Since 2016, Amazon has supported her research with US$100,000 in annual funding.\n\nThe Amazon home bot will be mobile, and navigate autonomously with cameras. It will need to recognize and track its user or users in the environment. Some of this tech can be borrowed from the Echo Smart Speaker, which uses real time sound data to determine a user\u2019s physical position. Amazon\u2019s robot could push that technique by incorporating dynamic visual data.\n\nLab126 has been granted several patents for localization techniques which can determine where a user is. One patent describes a system that utilizes a sound location technique to estimate an audio-based sound source position, and then pinpoints the user\u2019s position from analysis of optical images or depth maps generated by multiple sensors, such as LiDar and structured light.\n\nAlthough the patent was originally filed for a mysterious Lab126 project \u2014 rumoured to be a since-abandoned in-home augmented reality entertainment system \u2014 Lab126 is likely to apply this technique in the home robot.\n\nWhile Amazon has prioritized voice commands in human-machine interfaces, their home bot will have to go beyond that, and watch for and respond to physical cues from the user, such as hand gestures.\n\nAmazon has already been granted patents for hand signal detection. One is for a room computing system that uses sensors to detect and respond to hand poses. To detect a hand pose, an observed pose is compared to a hand pose dataset. When you wave goodbye to your Amazon robot as you head to work in the morning, will it wave back? It will if you want it to.\n\nIn any case the home bot will understand you are leaving, and will get on with its day, which may involve assigned tasks such as security monitoring. The new Amazon Key service allows in-home delivery and secure home access for guests. An interior security camera connects with Amazon Key, and it would seem natural to apply that task to the robot.\n\nGo outside to run errands\n\nAutonomous vehicles are a heated innovation area attracting interest from tech companies like Amazon, which this year patented an autonomous ground vehicle (AGV) that can leave its home, pick up a package from a depot and bring it back.\n\nWill the Amazon robot have this capability? It is possible. Tye Michael Brady and Ethan Zane Evans, the researchers behind the technique, point out \u201cthe AGVs may be owned by individual users and/or may service a group of users in a given area (e.g. in an apartment building, neighborhood, etc.).\u201d When the robot knows a delivery truck is approaching, it will navigate autonomously to meet the truck at the pickup point.\n\nYour AGV bot could even meet up with other neighbourhood AGVs to return a rake or borrow a cup of sugar.\n\nThe new Amazon home bot will likely be a wheeled, mobile, Alexa-based robot with an array of cameras and possibly other sensors. Will it have arms and suction grippers to do household chores? Possible, but Amazon seems to be aiming for multiple capabilities across a wide range of functions, not a housekeeper like \u201cAndrew\u201d in the film Bicentennial Man. The potential is huge and the possibilities endless.\n\nOnly time will tell whether Bezos and Lab126\u2019s creation will be a rolling Echo or a game-changing, \u201cwow\u201d product."
    },
    {
        "url": "https://medium.com/syncedreview/the-past-present-future-of-biotech-in-boston-150980e4860c?source=user_profile---------12----------------",
        "title": "The Past, Present & Future of Biotech in Boston \u2013 SyncedReview \u2013",
        "text": "If you are familiar with the biotech industry, you know Cambridge. The small city at the center of the Great Boston Area hosts over 1,000 biotechnology-related companies. Most of these companies cluster around Kendall Square, the same block as the Massachusetts Institute of Technology (MIT).\n\nOver the past decade the number of biotech jobs in Boston has jumped by 37%. [1] With the rapid rise in the application of Artificial Intelligence, and given the promising future of bioinformatics and computational biology, the local biotech industry finds itself in the midst of a technological revolution.\n\nThe biotech industry in Boston can be traced back to the 1970s, when molecular biology was in its golden age. The idea of \u201cplaying with genes\u201d however made many uncomfortable at the time. Cambridge City Council held a hearing on DNA experiments, and granted permission to Biogen, a new, local company founded by MIT Professor Phillip Sharp. Biogen was the first US firm to get the green light for genetic engineering. [2]\n\nBio-pharmaceutical companies quickly poured into Kendall Square, creating a well-rounded, self-vitalizing biotech ecosystem and building a global centre for biotechnology.\n\nOne key factor in Boston\u2019s rise in biotechnology is the area\u2019s academic resources, which are second to none.\n\nAlong with traditional biomedical schools such as Harvard Medical School and MIT Whitehead Institute for Biomedical Research, there are also a number of interdisciplinary programs combining biology and other informatic engineering subjects, such as the MIT Computational and Systems Biology Initiative (CSBi) and the Department of Biostatistics at Harvard. Other universities such as Tufts also have their own bioinformatics research groups.\n\nBoston\u2019s many universities are a talent pipeline, and most of the area\u2019s biotech company founders are graduates or professors from Harvard, MIT or other top universities. [4]\n\nBesides laboratories in universities, Boston\u2019s biotech industry is also backed by labs in hospitals and large pharmaceutical companies.\n\nBoston has three respected medical schools, two pharmaceutical schools, and three general hospitals. Having top hospitals not only provides more research facilities, but also more disease study opportunities. It is extremely difficult for example to do clinical testing and study on certain rare diseases which appear early in life and can claim the lives of 30% of patients before age five. The Boston Children\u2019s Hospital International Health Services division treats young patients from over 100 countries every year, and these treatment cases can inform the biotech research ecosystem.\n\nLarge medical companies are becoming more dependent on smaller biotech companies in the research field. As rising costs, patent cliffs, and other factors eat into pharmaceutical industry profits, many big medical companies are turning to Boston\u2019s innovative biotech startups to provide high-quality R&D results at lower costs. By the end of 2017, pharma giants Genzyme, Novartis, Pfizer, and Baxter had all established presences in Boston largely for this reason.\n\nCapital is an essential element in building a vibrant industry ecosystem, and as biotech has grown so has the money behind it. General venture capital and business companies are involved, along with investors specifically targeting biotechnologies.\n\nFidelity Biosciences, with US$2 trillion in assets under management, was one of the earliest venture capital companies to focus on life sciences, specifically \u201cBiopharmaceuticals, MedTech, and Healthcare IT/Services in a stage-agnostic fashion.\u201d [5] Other capital and consulting firms in this market include Flagship Ventures, MPM Capital, Locust Walk Partners, Voisin Consulting, and Fuld & Co.\n\nWhat\u2019s more, Biotech is an area where government can support businesses. Last year Massachusetts Governor Charlie Baker announced plans to invest US$500 million over the next five years in the Massachusetts Life Sciences Initiative. Baker\u2019s predecessor, Gov. Deval Patrick, had launched a US$1 billion initial investment in biotech back in 2008. [6]\n\nRespected biotechnology media company FierceBiotech publishes an annual \u201cFierce 15\u201d list of the world\u2019s most promising biotech companies, which it believes can make future breakthroughs. Cambridge area companies have taken about one-third of the spots on the list over the last five years, an achievement no other city has even approached.\n\nWhat are the most popular areas in biotech industry today? Gene editing is undoubtedly one, especially with the discovery of the CRISPR/Cas9 Method in 2011.\n\nCRISPR, for Clustered Regularly Interspaced Short Palindromic Repeats [7], is a family of DNA sequences in bacteria that contains snippets of DNA from viruses that have attacked the bacterium. These sequences play a key role in a bacterial defence system, and form the basis of a genome editing technology known as CRISPR/Cas9, which allows permanent modification of genes within organisms. [8]\n\nWith its huge application value in genome engineering, gene knockdown/activation, disease models, biomedicine and more, CRISPR technology has fostered many innovative Boston startups. Zhang Feng is the MIT Professor who first successfully applied CRISPR/cas9 in eukaryotic cells, and secured a patent for this method in 2017. [9]\n\nZhang co-founded Editas Medicine, one of the leading Boston companies focused on this technology. Another is CRISPR Therapeutics, which was co-founded by Emmanuelle Charpentier, another seminal figure in the field.\n\nIn 2015, Editas Medicine received US$120 million in Round B funding. Investors included Flagship Ventures (15.6%), Third Rock Ventures (15.6%), Polaris Venture Partners (15.6%), and bng0 under Bill Gates (9%). [10] In 2016, Editas Medicine became the first CRISPR gene editing company to make an IPO. Editas plans to start testing CRISPR in treating blindness, which would be the first instance of editing human genomes using CRISPR.\n\nOther promising players in the gene editing field include Intellia and Bluebird Bio \u2014 whose stock price has risen sixfold since it went public.\n\nThe biotech industry in Boston is booming, while facing potential challenges.\n\nLife sciences research involves a heavy time commitment in the design and execution of experiments, and this can send costs skyrocketing.\n\nEditas Medicine for example remains stuck in the preparation phase for the blindness treatment plan it announced in early 2015, although it confirmed investments of up to US$20 million at its 2016 IPO. This situation is normal in the industry. The success of a biotech company thus depends not only on its technology, but also on the confidence of the capital behind it, which will support it through such delays.\n\nSilicon Valley is another challenge for Boston. The two regions are virtually neck-and-neck in the biotech race, but as the industry starts relying more on software, wearable devices, and big data analysis, etc, this could tilt the contest in favour of Silicon Valley, which has a definite advantage in these tech areas."
    },
    {
        "url": "https://medium.com/syncedreview/pigs-cows-cockroaches-on-the-ai-animal-farm-58280d62f34f?source=user_profile---------13----------------",
        "title": "Pigs, Cows & Cockroaches on the AI Animal Farm \u2013 SyncedReview \u2013",
        "text": "Farming is becoming a data-centric business powered by artificial intelligence. China\u2019s big tech firms are using neural network-backed computer vision, wearable devices, and predictive analytics algorithms to reimagine pig, chicken, cow, goose, and cockroach farming.\n\nThe SCMP reports that Gooddoctor Pharmaceutical Group is using AI to cultivate up to six billion cockroaches per year in China\u2019s southeast Sichuan province for medical uses. The operation has generated US$684 million in revenue and is backed by AI algorithms which collect and analyze up to 80 indexes of data, catering to the roaches\u2019 humidity, temperature, and food requirements. AI also keenly monitors and stimulates the roaches\u2019 growth and breeding rates.\n\nOver the past year, Sichuan pig farming corporation Dekon Group and pig feed supplier Tequ Group have been working in partnership with Alibaba Cloud. By 2020, Dekon Group will breed up to 10 million pigs per year. The AI-backed computer vision and voice recognition systems can recognize pigs via numbers tattooed on their flanks and monitor vulnerable piglets for squeals of distress.\n\nAlibaba competitor JD.com meanwhile has launched an AI chicken breeding project wherein each chicken wears a fitness tracker around the ankle. JD.com says it will buy the birds back at triple the price once they walk one million steps. JD Finance CEO Shengqiang Wang says the company wants to rebuild the entire farmhouse infrastructure, monitoring food intake, defecation, and other physiological conditions.\n\nPoking fun at the trend on April Fool\u2019s Day, Tencent announced the \u201cgrand opening\u201d of a purported geese production facility in the mountains of Guizhou. In Chinese, \u201cgoose\u201d (\u9e45) is written one hanzi character away from Tencent\u2019s flagship mascot penguin (\u4f01\u9e45). A Tencent spokesperson claimed the company was starting a pilot goose farming project to explore the potential of \u201cAgriculture + AI + Internet Smart Retail.\u201d The company said the Guizhou operation would be located in excavated mountain caves, begin with 5,000 geese and scale up to 200,000. To further play on the hanzi pun, Tencent promised netizens it was considering adding swans (\u5929\u9e45) and of course, penguins (\u4f01\u9e45).\n\nArtificial intelligence is certainly revamping the animal farming industry, with more and more technology companies hopping onboard. Animal farming is no longer a difficult job plagued with sanitation problems. AI may provide more wholesome and sustainable solutions for this inevitable trend of mass production.\n\nIn Japan, Osaka University\u2019s intelligent cow breeding system can detect contagious viral disease in livestock with up to 99% accuracy. The system is being adopted for cowhouses with automatic milking machines and feeding robots, and several Japanese dairy farms are using it along with wearable devices to fine-tune milking and feeding and provide real-time updates.\n\nAt the same time, computer vision or data manipulating software portals are just small nodes in the bigger IoT makeover of food production. While farmers may be initially skeptical of all these new-fangled cameras, fitbits, and smartphone apps, the AI farming wave is not likely to recede, rather it may completely change the farming status quo."
    },
    {
        "url": "https://medium.com/syncedreview/google-boosting-its-ai-research-in-tokyo-608f8ba11c9?source=user_profile---------14----------------",
        "title": "Google Boosting its AI Research in Tokyo \u2013 SyncedReview \u2013",
        "text": "Google is looking to expand its AI research activities in the Japanese capital. The company\u2019s deep learning and AI research team Google Brain yesterday posted a Tokyo job listing seeking talented experts to participate in cutting edge research on machine learning.\n\nApplicants will work on real-world problems involving AI, data mining, natural language processing, hardware and software performance analysis, improving compilers for mobile platforms, as well as core search and much more.\n\nMinimum qualifications for the position are:\n\nGoogle Chief of AI division and Head of Google Brain Jeff Dean tweeted, \u201cHappy to see our #GoogleAI efforts expanding with Google Brain now having a research presence in Tokyo.\u201d\n\nGoogle Brain was initiated in 2011 as \u201cGoogle X,\u201d a project focused on building a large-scale deep learning software system. Its founding members \u2014 Dean, Google Researcher Greg Corrado, and Stanford University professor Andrew Ng \u2014 successfully built a neural network powered by 16,000 computer processors, which was trained to recognize cats in YouTube videos. The project ended up doing so well that it was upgraded into \u201cGoogle Brain\u201d with a mission to improve people\u2019s lives by making machines smarter. Google Brain has since attracted top-tier researchers such as Dr. Geoffrey Hinton, who developed back propagation; and Ian Goodfellow, who created generative adversarial networks (GANs).\n\nGoogle Brain is aggressively pursuing AI talents outside the US. In 2016, Google opened a Zurich research unit focused on machine learning, the digital assistant inside its Allo Chat app, autonomous driving efforts, and improvements to Google\u2019s search engine.\n\nGoogle opened its first office in Japan back in 2001. Headquartered in Tokyo\u2019s Roppongi Hills complex, Google Japan has since grown to a team of over 1,300."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-train-100-000-ai-talents-in-three-years-850ab9c1cc01?source=user_profile---------15----------------",
        "title": "Baidu to Train 100,000 AI Talents in Three Years \u2013 SyncedReview \u2013",
        "text": "At a tech conference in Beijing yesterday, Baidu Vice President Yaqin Zhang plotted a bold course for the search engine giant. In response to a growing shortage of AI talents, Zhang announced that Baidu will train over 100,000 AI talents with expertise in engineering and product development over the next three years.\n\nTrainees can learn AI, data analysis, and cloud computing skills on the Cloud Intelligence College (\u4e91\u667a\u5b66\u9662), an education platform launched by Baidu last year that offers online and offline courses, and awards specialized certificates to graduates.\n\n\u201cOur mission, on one hand, is to improve the employment competitiveness of trainees, and on the other hand, to ramp up companies\u2019 product R&D capabilities,\u201d says Zhang.\n\nChina is facing a shortage of five million AI talents. While the country is an AI business deployment leader with well-financed startups, its AI educational infrastructure is lagging. The US has six times more AI education institutions. Meanwhile, high demand has sent AI engineer salaries skyrocketing in China.\n\nEarlier this month China\u2019s Ministry of Education issued an action plan aimed at energizing Chinese universities\u2019 capabilities in AI technological innovation, talent cultivation, and global cooperation. The action plan also pledges to educate 500 teachers and 5,000 students over the next five years in a joint effort with Sinovation Venture and Peking University."
    },
    {
        "url": "https://medium.com/syncedreview/chip-guru-jim-keller-leaves-tesla-for-intel-34affc4a6490?source=user_profile---------16----------------",
        "title": "Chip Guru Jim Keller Leaves Tesla for Intel \u2013 SyncedReview \u2013",
        "text": "Last December, Tesla ditched its chip partner Nvidia and announced that they would make their own in-vehicle chips. The Head of Tesla\u2019s Autopilot Hardware Team Jim Keller, best known for his CPU design at AMD and Apple, was supposed to help the company make that happen.\n\nToday, however, Tesla announced Keller had left: \u201cToday is Jim Keller\u2019s last day at Tesla, where he has overseen low-voltage hardware, Autopilot software and infotainment. Prior to joining Tesla, Jim\u2019s core passion was microprocessor engineering and he\u2019s now joining a company where he\u2019ll be able to once again focus on this exclusively. We appreciate his contributions to Tesla and wish him the best.\u201d\n\nPrior to joining Tesla in January 2016, Keller served as Corporate Vice President and Chief Architect at AMD. He earned his reputation as a CPU architect master by leading design on the AMD K8 microarchitecture and Apple A4/A5 processors which empower the iPhone.\n\nKeller\u2019s departure is the latest in a series of setbacks for Tesla. The company is struggling with Model 3 production hiccups, and Moody\u2019s recently downgraded Tesla\u2019s credit rating due these delays; and an Apple engineer died last month when his Tesla Model X, with Autopilot, crashed in Mountain View, California. About the only good news for Tesla is that their stock price has not yet been seriously affected.\n\nKeller\u2019s next stop is Intel, where he will be a senior vice president and lead the company\u2019s silicon engineering, which encompasses system-on-chip (SoC) development and integration.\n\n\u201cThe world will be a very different place in the next decade as a result of where computing is headed. I am excited to join the Intel team to build the future of CPUs, GPUs, accelerators and other products for the data-centric computing era,\u201d says Keller."
    },
    {
        "url": "https://medium.com/syncedreview/liulishuos-ai-app-is-teaching-english-to-70-million-people-31d4fb38a799?source=user_profile---------17----------------",
        "title": "Liulishuo\u2019s AI App Is Teaching English to 70 Million People",
        "text": "\u201cLiulishuo is the AI English teacher on your phone. You don\u2019t need to know how it works, yet it helps you learn English more efficiently than a human teacher,\u201d says Yi Wang, Founder and CEO of Liulishuo \u2014 a Beijing-based \u201cAI + language\u201d company whose name translates as \u201cspeak fluently.\u201d Liulishuo hosts the world\u2019s largest speech bank of Chinese speakers speaking English.\n\nAfter obtaining a PhD in computer science from Princeton, Wang worked as a product manager at Google in California. He returned to China in the early 2010s and found many of his friends asking similar questions: \u201cHow do I learn English?\u201d; \u201cWhy is it that I pay so much money for lessons and fail to keep up?\u201d; \u201cShould I watch more American TV shows to learn to speak naturally?\u201d\n\nWang wondered whether people might be able to learn English by speaking with their phones. At the time, AI and online education were much less developed. In 2012 Wang launched Liulishuo with Hui Lin, a Google coworker specializing in voice recognition and machine learning.\n\nLaunched six months ago, the company\u2019s flagship app is now teaching English with personalized and adaptive methods based on deep learning to some 70 million registered users in 175 countries. It was selected as an Apple Store \u201cBest App of the Year,\u201d and Apple VP Philip Schiller and his team visited the company\u2019s offices. Liulishuo is the only Chinese Education company to make the CB Insights AI 100 list.\n\nSynced recently spoke with Liulishuo Yi Wang about his company and AI language learning.\n\nLiulishuo has the world\u2019s largest database of Chinese people speaking English. Based on the database we created a Chinese-speaking-English recognition engine with the highest accuracy and an evaluation engine which provides users with ratings and feedback.\n\nInitially, we used speech recognition to evaluate the user\u2019s verbal skills. Now our engine can perform a full range of verbal language assessments, and we have a separate engine that can correct writing.\n\nWe have a special function tackling the IELTS exams, which tests users on their pronunciation, grammar, vocabulary, and fluency. The scoring algorithms have passed the Turing Test. The variation in score between our AI and a human examiner is lower than between two human examiners. We offer users individualized suggestions to improve their English using their scores in the four areas.\n\nWe launched the world\u2019s first AI English teacher platform in July 2016. Ten million users have already completed our ranking exam, with completion time ranging from 5 minutes to 20 minutes depending on proficiency.\n\nAfter completing the exam, the system selects a starting level for each user. Users improve their English through the immersive learning of scenes from images and animations, without any subtitles or translations. They must attempt to understand the scenario, which is followed by suggested practices, which is then followed by more progressive scenarios. This back and forth is highly effective in improving the user\u2019s English language skills.\n\nWe have the data to prove our effectiveness. ETS TOEFL testing has proven our AI teacher can improve learning efficiency by three times. For example, if it used to require 100 hours of learning to reach a certain level in the CEFR standard, we would only need about 36. Regardless of product and service, we\u2019re the only English learning organization that publishes efficiencies of this caliber, and that really excites me.\n\nBefore launching the product, we asked some American English speakers to record audio for us to cold start the engine. We also collected limited data from Chinese speakers audio recordings via crowdsourcing.\n\nBut since the launch of our product, our users have provided us with massive amounts of incoming data in different skill levels and accents. This data was recorded by users reading what\u2019s displayed on their screen, and so it is also labeled. We effectively received all this data for free from users practicing their English.\n\nIn order to train our model, we invited experts to label our data for us, for example for IELTS we asked IELTS examiners to label our data.\n\nWe have two types of content. The free content is English conversations written by professional writers, on top of User-Generated Content (UGC) and Professionally Generated Content (PGC). We also have the most active English learning community in China, and many of our short videos are contributed by these learners.\n\nThe paid content is created by our own team. We hired Philip Lance Knowles, who has previously proposed Recursive Hierarchical Recognition Theory and other breakthroughs in language learning theory based on cognitive science, as our expert consultant and created our content based on Lance\u2019s theory. Of course, our customized learning material is different from writing a textbook, where all students learn in the same sequence.\n\nThis is the general direction, and we are doing some early stage testing.\n\nThese headphones can benefit let\u2019s say seniors traveling in another country. But I think they won\u2019t be able to replace the language learning market. Firstly, the Ministry of Education will not remove English from the curriculum just because we have translation machines. Secondly, learning a new language isn\u2019t just about translation, translation is just application.\n\nLearning a new language is about learning to communicate with others, and there are cultural contexts one must also learn to understand in order to learn real communication. In the process of learning a new language, there\u2019s a sense of accomplishment, in which the user builds confidence and challenges themselves. Thus we see the social function of language learning, as many people learn to make new friends. There are people who make friends and even find their partner on our platform. In this sense, you can\u2019t equal language learning with translation.\n\nWe want the learning process to be customized and highly effective. To achieve these two goals, we believe that data-driven AI is the key. We\u2019ve only taken the first step in exploring AI teachers. They aren\u2019t intelligent enough just yet, and the learning experience has many areas that can still be optimized. We are working hard at solving these problems.\n\nPeople have limited understanding of how our conscience and brain actually work. We are working with many experts in neuroscience and education, such as the Dean of Education Faculty at Stanford University and a Professor of Neurology at Yale University, in hopes of bringing in new research results. Our platform is also useful for their research because we have large amounts of user data that helps them create new learning models. We have set up an education and AI lab in the Bay Area, hoping to attract top experts in AI, education, and cognitive science, in order to help us create the most intelligent, most efficient AI English teacher in the world.\n\nThe explosive growth of the mobile internet since 2012 has turned mobile device usage into the new way of life. I saw this as an immense opportunity, but I realized that if I only developed small apps focusing on weather, calendar, camera etc, it would be a challenge to make them profitable. Therefore we thought about combining the mobile internet with traditional industries. The markets must be large, with good user paying habits, and room for improving efficiency. We researched applications in finance, health, and education, finally deciding to go with education.\n\nWe set forth to create an easy-to-use product. The first week Liulishuo was available, it was recommended by the Apple Store in mainland China, Hong Kong, Taiwan and Japan, and quickly became one of their \u201cBest Apps of the Year.\u201d Apple\u2019s Senior VP of Worldwide Marketing soon toured our company. This shows that our product-centered strategy is being rewarded.\n\nThe second turning point was the transformation of Liulishuo from a tools App into a community App. Building a community increased user stickiness and activity and created a broad learning environment.\n\nThe third point was in 2014, when we made a strategic decision to create an educational research team, to involve education professionals from a content perspective. Before this, we were purely an internet company. We decided to put a heavy focus on the essence of education and personalization of content. If we had not made that shift, we would just be a marginalized tech company.\n\nIn 2014 we worked with a foreign company and tested two learning techniques which used word games to practice speaking. But they weren\u2019t very successful. From a gameplay perspective, they weren\u2019t as fun as normal games; while from a learning perspective, they weren\u2019t very effective.\n\nDuring the second half of 2014 we wanted to create a textbook product. Our first instinct was to license the best textbooks from top publishers such as Pearson, Cambridge, or Oxford. After we got to know them a bit better, we realized that these textbooks were written and designed for traditional, offline classroom scenarios and were not effective for users lounging on their couch learning with a smartphone. Therefore, we started to work on our own educational research team and spent two years developing the AI teacher. This was a strategic change, and looking back it was the right thing to do.\n\nWe are seeing three historical opportunities. Firstly, learning is now digitized. In the past we did our homework on paper, whereas today 100% of user learning, actions, and interactions are digitized. This is a huge leap forward and the only way to make it possible to use AI. If you\u2019re not digitized and have no structured data, it will be impossible to talk of AI, right?\n\nSecondly, I think we\u2019re experiencing a historic leap from teacher-centric to student-centric learning. There were many more students than teachers in the industrial era, but many students are now practicing one-on-one with a language tutor. However, this is a transition phase because the so-called \u201cone-on-one\u201d is still not necessarily centered around the student, as teachers may not understand the needs of the student and create suitable teaching plans.\n\nLiulishuo\u2019s AI teacher is not human, it is a system that relies on user interactions to make decisions. Through deep learning and other AI technologies, it selects only relevant content from a huge library and recommends it based on the student\u2019s level. I think the pace of review and practice frequency based on an individual student\u2019s needs, strengths and weaknesses is the ultimate student-centric learning experience.\n\nThird, from a business perspective, we believe that we must develop a results-oriented business model to replace a process-oriented one. In a language training institution for example, if you learn for 100 hours and yet still see no improvement, the institution won\u2019t be responsible as it has delivered its service by selling you the teacher\u2019s time. Hence, these traditional training institutions are just wholesalers of teachers\u2019 time. We think this situation will eventually change, and educators will get paid according to the results achieved by each student.\n\nOur paid product works exactly this way, it does not charge based on instructional hours, but instead, provides users with a buffet. For just CN\u00a599 a month, users can spend as much time as they like there. A diligent student can learn at a much faster pace, absorbing all that they can. Our paid users on average spend five hours or more learning on our App each week. Who spends five hours a week learning a new skill anymore as an adult? This shows our product is really effective.\n\nI hope that in the next two to three years Liulishuo can assemble a leading team of researchers and product designers with full-stack development capabilities, dedicated to applying AI to education. As for long-term plans, I hope that in the next decade, we can become a global leader in education."
    },
    {
        "url": "https://medium.com/syncedreview/pytorch-releases-major-update-now-officially-supports-windows-2426c9f29d2d?source=user_profile---------18----------------",
        "title": "PyTorch Releases Major Update, Now Officially Supports Windows",
        "text": "PyTorch, an open source machine learning library for Python, today announced the release of PyTorch 0.4.0 with Windows support. \n\n \n\n PyTorch can now be installed on Windows OS via Conda or Pip command line. The new version also merges Tensor and Variable, which means torch.autograd.Variable and torch.Tensor are now in the same class; and unifies the return 0-dimensional vector of size, which makes it more similar to NumPy features. Also, a set of more flexible context managers has replaced the volatile flag.\n\n \n\n As a Facebook-backed open source package released in October 2016, PyTorch has been very well-received in the developer community, and has more than 14.4k stars on GitHub (Google-backed TensorFlow has 97.4K stars, and Amazon-backed Apache MXNet has 13.7K stars on GitHub). It can leverage the capability of GPU, speed up computing for AI tasks, provide GPU-friendly NumPy functions, and robustly support Tensor. \n\n \n\n Detailed update content:\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/new-petuum-cmu-paper-identifies-statistical-correlation-among-deep-generative-models-1d9afc5abb87?source=user_profile---------19----------------",
        "title": "New Petuum & CMU Paper Identifies Statistical Correlation Among Deep Generative Models",
        "text": "On April 17th, researchers from Carnegie Mellon University and Petuum, a Pittsburgh-based CMU spinoff focused on artificial intelligence platforms jointly published On Unifying Deep Generative Models. The paper introduces a high-level theoretical connection between various deep generative models, particularly Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). It has been accepted as a 2018 ICLR Conference Paper.\n\n \n\n The paper\u2019s researchers suggest that GAN and VAE lack a unified statistical connection due to their distinct generative parameter learning paradigms. Researchers derived a new GAN formula that has many similarities with VAE, which could spark innovations in R&D of GANs and VAEs, and help researchers discover new common rules of machine intelligence that were previously undetected.\n\nAccording to the original post, many advantages can be achieved by this unified statistical view:\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/mckinsey-report-ai-promises-added-value-of-up-to-us-5-8-trillion-80cc0043ebf6?source=user_profile---------20----------------",
        "title": "McKinsey Report: AI Promises Added Value of Up to US$5.8 Trillion",
        "text": "McKinsey Report: AI Promises Added Value of Up to US$5.8 Trillion\n\nAlthough artificial intelligence has triumphed over the world\u2019s best human players at Go and Poker, and outperformed humans at imaging analysis and speech recognition, few are aware of the actual dollar value that AI techniques can bring to industries such as travel, retail, transport & logistics.\n\nThe McKinsey Global Institute this month released the report Notes From the AI Frontier Insights From Hundreds of Use Cases. The 36-page discussion paper surveys cutting-edge machine learning algorithms, and discusses how they can be integrated or transformed into practical applications across 19 selected industries.\n\nAI can potentially create US$3.5\u20135.8 trillion in annual value\n\nThe report defines AI as deep learning techniques based on artificial neural networks, such as feed forward neural networks, recurrent neural networks (RNN), and convolutional neural networks (CNN). These algorithms have grown from fledgling research subjects to mature techniques in real world use. Advanced AI techniques such as generative-adversarial-networks (GANs) and reinforcement learning are not within the scope of the report.\n\nIn the 19 industries studied, AI\u2019s potential annual value was between US$3.5 trillion and US$5.8 trillion. Retail is the industry expected to be most impacted by AI at US$0.4\u20130.8 trillion, followed by travel (US$0.3\u20130.5 trillion), and transport & logistics (US$0.4\u20130.5 trillion). Marketing & sales, and supply-chain management & manufacturing are sectors where AI can help companies grow US$1.2\u20132.6 trillion in annual revenue.\n\nAI can increase value up to 128 percent over traditional analytic techniques\n\nThe report says AI is more likely to improve performance over other analytic tools in 69 percent of the use cases McKinsey studied.\n\nThe industries with most potential incremental value benefit from AI compared to analytical techniques are travel (128%), transport & logistics (89%), and retail (87%). Industries at the bottom of the list are insurance (38%), advanced electronics/semiconductors (36%), and aerospace & space (30%).\n\nSixteen percent of the report\u2019s use cases are new applications developed by AI techniques, for example a smart customer service assistant in retail or medical imaging detection in healthcare.\n\nGetting accurate labeled data to train AI models is challenging\n\nLeveraging AI algorithms requires a large amount of clean and labeled data. The textbook Deep Learning, written by Google Researcher Ian Goodfellow and Head of the Montreal Institute for Learning Algorithms (MILA) Yoshua Bengio, suggests that a deep-learning algorithm can achieve acceptable performance by training with 5,000 labeled examples per category. If a model is expected to match or even exceed human level performance, it has to be trained with a dataset of at least 10 million labeled examples.\n\nCollecting large-scale datasets is challenging, particularly in industries such as healthcare where there is not so much available or useable data. Also vexing is data processing, including data cleansing and labeling, which is a difficult and time-consuming engineering problem. While advanced techniques such as reinforcement learning and GANs can effectively simulate data for academic research, they are not mature enough for wider implementation.\n\nMeanwhile, AI still has other limitations that need solutions. Interpretability, also referred to as \u201cblack box\u201d problem, means that even scientists cannot explain how an AI arrives at a decision. Google researchers recently attempted to create a step by step visualization of the process involved in a computer recognizing an object.\n\nThink twice before you embrace AI\n\nMcKinsey analysts suggest that AI is an elusive proposition for many companies as it remains unclear whether injecting huge investments in AI is worth the potential value the tech promises. There is also the concern that any careless technical executions could cause unpredictable, expensive or grave consequences, especially in sensitive fields like healthcare or legal systems.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-in-the-media-and-entertainment-industry-1ad4b2b701b8?source=user_profile---------21----------------",
        "title": "AI in the Media and Entertainment Industry \u2013 SyncedReview \u2013",
        "text": "AI in the Media and Entertainment Industry\n\nThe Media and Entertainment industry is a cornerstone of contemporary human culture, delivering films, TV shows, advertisements and more in a multitude of languages across a wide variety of devices. A PwC report predicts total M&E revenue will reach US$2.2 trillion in the next three years. The industry\u2019s growth rate however has lagged, and slipped by 0.2% in 2017, prompting many companies to turn to AI technologies to boost business.\n\nWith the breakthroughs in machine learning, many intelligent products have made the leap from sci-fi movies to the home. Superhero Ironman\u2019s virtual helper JARVIS (Just A Rather Very Intelligent System) for example is echoed in smart assistants such as Alexa and Google Assistant, which may not catch criminals but can perform a range of practical tasks via connected household devices. NVIDIA meanwhile used VR technology to create a Holodeck similar to one in the sci-fi series Star Trek.\n\nAI technologies are also being applied in filming, visual design, post production etc.\n\nCurrent AI applications in the M&E industry are mainly in four categories: Marketing and Advertising, Service Comprehension, Search and Classification, and Experience Innovation.\n\nThe marketing and advertising sector includes visual design, film promotion and advertising. A machine learning algorithm trained with data such as text, stills and video segments can extract language, objects and concepts from its training resources and suggest marketing and advertising solutions to improve efficiency. Such a system can work as an assistant or even a content creator.\n\nAlibaba\u2019s Luban is an AI designer that can create banners thousands of time faster than a human designer. On China\u2019s online shopping extravaganza \u201cSingles Day\u201d in 2016 Luban generated some 8000 different banner designs per second and 170 million banners in total. The record output would of course be impossible for human designers to process in one day. On Singles Day 2017 Luban raised its one-day record to a staggering 400 million banners.\n\nIBM used their AI system Watson to help 20th Century Fox create a trailer for the horror movie \u201cMorgan.\u201d The research group trained the AI system to analyze and classify input \u201cmoments\u201d from visual, audio, and other composition elements in 100 horror movies to learn what kind of \u201cmoments\u201d should appear in a standard horror movie trailer. Watson needed just 24 hours to create a six-minute movie trailer that may have taken a human professionals weeks to produce.\n\nAlbert Intelligence Marketing\u2019s AI marketing platform accelerates the marketing process using predictive analytics, machine learning, NLP and computer vision technology. The Albert platform can perform audience targeting, customer solution making and generate autonomous campaign management strategies. Albert says companies using the platform report a 183% improvement in customer transaction rate and 600% higher conversation efficiency.\n\nAs user experience personalization becomes more important for the industry, companies are using AI to create personalized services for billions of customers. These include for example recommending content that fits users\u2019 personal tastes while they are browsing a video site or shopping online; and optimizing video definition and fluency for users with different internet speeds and bandwidth.\n\nNetflix\u2019s content recommendation got a boost in May 2016 when the company launched Meson, an intelligent workflow management and scheduling application. This AI system automatically manages the various machine learning pipelines that provide video recommendations. According to the Netflix 2016 annual report, there are 93 million global users streaming over 125 million hours of TV shows and movies per day on the platform. Predicting which shows will attract users\u2019 interest is a key component of the Netflix model.\n\nAI technology is also being applied to optimize video fluency and definition. Slow Internet connections and bandwidth limits can be a problem for streaming services in developing nations and for mobile device users. Netflix collaborated with the University of Southern California and the University of Nantes in France to develop a new machine learning methodology called Dynamic Optimizer which can compress video without degrading image quality to ensure a smooth and high quality streaming experience for its customers, whether they are in India or in Japan.\n\nThe Internet hosts countless media works. Video, audio and text can all be transformed into a digital copies which can be stored and spread so easily that it is getting increasingly difficult for people to find exactly what they want online. AI is helping optimize the accuracy of search results. Computer vision technologies meanwhile are also enabling content producers to better manage visual content and accelerate the media production process.\n\nAdvancements in machine learning technology have enabled Google to augment the world\u2019s leading search engine in multiple ways. One is in image searching. Rather than typing in keywords and checking returned images, users can upload a sample picture to Google Image, which uses image recognition technology to identify image features and search for similar pictures. Another advanced application involves selective link-building. Google applies AI to position ads appropriately \u2014 for example so a cat food ad appears in a pet-related website, but a bacon cheeseburger promotion will not appear on a site for vegetarians.\n\nClarifAI is an AI startup focusing on computer vision technology which partnered with Vintage Cloud to deploy AI on a film digitalization platform. By using ClarifAI\u2019s computer vision API, Vintage Cloud successfully accelerated the progress of movie content classification and categorization. It used to require dozens of hours for humans to recognize and manually classify objects in a movie. AI can do a better job in much less time.\n\nIn the past, papers and books were the main medium for words and images. The introduction of film and TV brought us into the dynamic new world of moving pictures. Now, AI is heralding a new age of immersive experience for visual content. This technology includes Virtual Reality (VR) and Augmented Reality (AR). With machine learning algorithms and computer vision technologies, developers can build complex and holographic scenes within a pair of goggles. This opens up a brand new market.\n\nVR gaming is one of the first areas that comes to mind, and this is where companies like HTC Vive, Samsung Gear VR, Oculus Rift, etc. are focusing their efforts. Various type of headsets have been introduced. Combined with motion sensing games, VR gaming innovation has become a hot market that shows no sign of slowing down.\n\nIntel is now getting into the immersive experience industry. With the application of deep learning and computer vision technology, Intel has become a visual content provider emphasizing Virtual Reality content. Supported by AI algorithms, Intel True VR Technology can perform every piece of a scene with pixels in three-dimensions.\n\nUsing the tech, fans can also watch sports in holographic view. Intel demonstrated this in their widely viewed VR game broadcast of the NFL 2018 Super Bowl. Intel partnered with the International Olympic Committee to broadcast the 2018 Winter Olympic Games as 360-degree video content. With a VR headset or even just a smartphone, fans and families could experience the action from the POV of an athlete.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/tomaso-poggio-on-deep-learning-representation-optimization-and-generalization-66bb8c8e524f?source=user_profile---------22----------------",
        "title": "Tomaso Poggio on Deep Learning Representation, Optimization, and Generalization",
        "text": "Those outside academia may know Tomaso Poggio through his students, DeepMind Founder Demis Hassabis and Mobileye Founder Amnon Shashua. The former built the celebrated AI Go champion AlphaGo, while the latter has installed copilot systems in more than 15 million vehicles worldwide, and produced the world\u2019s first L2 autonomous driving system in a car.\n\nWhile Poggio the teacher has taught some extraordinary leaders in AI, Poggio the scientist is renowned for his theory of deep learning, presented in papers with self-explanatory names: Theory of Deep Learning I, II and III.\n\nHe is a Professor in the Department of Brain and Cognitive Sciences, an investigator at the McGovern Institute for Brain Research, a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Director of the Center for Biological and Computational Learning at MIT and the Center for Brains, Minds, and Machines.\n\nPoggio\u2019s research focuses on three deep learning problems: 1) Representation: Why are deep neural networks better than shallow ones? 2) Optimization: Why is SGD (Stochastic Gradient Descent) good at finding minima and what are good minima? 3) Generalization: Why is it that we don\u2019t have to worry about overfitting despite overparameterization?\n\nPoggio uses mathematics to explain each problem before inductively working out the theory.\n\nPoggio and mathematician Steve Smale co-authored a 2002 paper that summarized classical learning theories on neural networks with one hidden layer. \u201cClassical theory tells us to use one layer networks, while we find that the brain using many layers,\u201d recalls Poggio.\n\nBoth deep and single-layer networks can approximate continuous functions. This was one reason why research in the 80s focused on simpler single-layer networks.\n\nThe problem occurs in the dimensionality of single-layer networks. In order to represent a complicated function, a single-layer network would require more units than the number of atoms in the universe. Mathematically, this is called \u201cthe curse of dimensionality,\u201d wherein the number of parameters goes up exponentially corresponding to function dimensionality.\n\nMathematicians make assumptions about function smoothness in order to escape the curse of dimensionality. Yet deep learning offers a different approach that uses compositional functions. The units that deep networks require to approximate a compositional function share a linear relationship with function dimensionality.\n\nDeep learning works beautifully for datasets that are compositional in nature, such as images and voice samples. Images can be broken down into related snippets of details, while voice samples can be converted into meaningful phonemes. For an image classification task, there\u2019s no need to look at pixels that are further apart, the model simply observes each small bit and combines them. The neural network escapes the curse of dimensionality by using a very small number of parameters.\n\nIf the target is a function made up of functions with a smaller number of variables, then a deep network can approximate it with a number of units that is linear in dimensionality no matter how big the function is.\n\nKnowing that compositional functions work well with deep networks is far from enough. \u201cFor a computer scientist or a mathematician, can we say something more about compositional functions beyond the fact that they\u2019re compositional? Can we characterize them to get a better understanding of neural networks? This is an interesting open question,\u201d says Poggio.\n\nDeep networks have far more parameters than the number of examples in the training set.\n\nThe CIFAR dataset has 60,000 examples, and we use networks with millions of weights to process it. This is a typical case of overparameterization. We can make a hypothesis to simplify the matter: if one replaces nonlinear neurons in the deep network with univariate polynomials, then getting zero training errors on CIFAR means solving 60,000 polynomial equations. We now have infinite sets of solutions according to B\u00e9zout\u2019s theorem, which ends up becoming the dataset\u2019s infinite global minima.\n\nThus overparameterization guarantees lots of global minima that form flat valleys in the loss space. As SGD is known for its preference for flat valleys, there is a high probability that SGD will find the global minima for neural networks.\n\nPoggio\u2019s work showed that a combination of overparameterization and SGD simplifies the optimization of neural networks.\n\nOverparameterization is good news for optimization, but a nightmare when it comes to generalization. Test errors go down but then up again in classic machine learning, which is called \u201coverfitting.\u201d Yet in deep learning, overfitting is not reported, and so the test error rate goes down and stays there.\n\nWhy is this the case? Poggio likens this to a \u201cchemical reaction\u201d that occurs when classification tasks are mixed with a specific type of loss functions called cross entropy.\n\nAlthough we can use 0\u20131 loss to evaluate error rates, we need an alternative approach when it comes to loss function. Take handwritten digit classifiers as an example, the last step for the neural network is to turn a softmax into a \u201chardmax\u201d, in other words, a class. Thus, even if we only have a bad model that is only 30% sure that the \u201c1\u201d we show it is a \u201c1\u201d, as long as 30% is the highest among the given 10 possibilities, the model will classify the image correctly. Of course no one would be satisfied with a 30% success model. The model needs further optimization, which can\u2019t be done using a 0\u20131 loss.\n\nIn case of cross entropy, as long as the model is not 100% certain, one can continue to optimize it by calculating another gradient, and use backpropagation for fine-tuning. On a side note, the favourable property of using cross entropy as the loss function and 0\u20131 loss as error metrics is that, even when cross entropy is overfitted, the 0\u20131 loss will work just fine. A few months ago, University of Chicago researcher Nathan Srebro and his colleagues proved this for a special case of linear networks with separable datasets.\n\n\u201cOn top of [Srebro\u2019s work], we\u2019ve shown that using differential equations from dynamical system theory will make a deep network behave like a linear network near a global minimum. We can use the Srebro result to say the same thing about deep learning, even if a deep neural network classifier has an overfitting cross entropy, the classifier wouldn\u2019t overfit,\u201d says Poggio.\n\nThis property of cross entropy is shared with loss functions such as exponential loss, but not with simpler ones like the least square error. Why is this the case? Poggio says this remains an unsolved question.\n\nPoggio says his opinion on the shape of minima and their corresponding generalization capabilities has changed recently: \u201cPeople said in papers that flatness is good for generalization. I also said something like this a year ago, but I don\u2019t think this is true anymore.\u201d\n\n\u201cI don\u2019t see a direct relation between flatness and generalization. Generalization relies on properties like choosing classification as the task, choosing cross entropy as the loss function but not flatness. There\u2019s a paper of which two out of four authors are Bengios, which proves that even sharp minima can generalize because you can change the weights in different layers to make it sharp without changing the input-output relation of the network.\u201d\n\nPoggio also doesn\u2019t think it\u2019s possible for a flat minimum to exist, at least not for neural networks that are polynomial in nature.\n\nLearning deep learning theory can be enlightening, but engineers working on applications ask the question: How can theoretical research work help me train my model?\n\nThe No Free Lunch Theorem tells us that two learning algorithms are equal when no prior information is offered for distribution. For any algorithms A and B, there are as many distributions in which algorithm A outperforms B, as distributions in which B outperforms A.\n\nPoggio utilizes the theorem to propose that in machine learning, no algorithm can work the best for every problem. \u201cTheory tells you about the average case and the worst case, what you should or should not do to avoid bad things. But it can\u2019t advise you on the best thing to do for any particular case.\u201d\n\nPoggio suggests engineers who employ deep learning models be careful of overfitting, \u201cOne lesson to learn from the past few decades of machine learning is that when you don\u2019t have enough data, then after many trials, the state-of-the-art method is usually overfitting. It\u2019s not because people have peeped at the validation set, it\u2019s just that the community of researchers has tried too many different algorithms.\u201d\n\n\u201cI\u2019m a physicist by origin. When I was in school, the rule of thumb was that if you have a model or a set of equations with n parameters, you need at least 2n data points. If you want to do something statistical, the recommendation was to have 10n data points. Nowadays people use 300,000 parameters for datasets of any size. The arguments we make like \u2018deep learning models tend not to overfit\u2019 is only true for classification tasks with nice datasets, so people should be more careful about that.\u201d\n\nHumans don\u2019t need millions of pieces of labeled data to learn, thanks to prior knowledge carried in our genes. \u201cThere is not a simple answer for how many priors we need for a model. There are only situations where we know the minimum priors needed to make predictions.\u201d\n\nPoggio uses regression as an example, \u201cIf I want to reconstruct a curve from points, I can\u2019t do anything unless I have all the points. Continuity is essential but not enough, the least I need is something like smoothness. In the end, it\u2019s a tradeoff between how strong the priors are and how much data you need.\u201d\n\nMIT has a tradition of knitting together deep learning and neuroscience, so what is Poggio\u2019s view on learning from the human brain?\n\n\u201cI think it\u2019s unlikely, but not impossible, that things like backpropagation can be done biologically, given what we know about neurons and signal processing. What I think is impossible is labeling everything.\u201d\n\nHow our brain gets around labeling is an interesting question. Poggio assumes that our visual system for example is pre-trained to \u201ccolour-fill\u201d an image. It receives the colour information but only gives black, grey, and white signals to the visual cortex. You do not need an oracle to tell you what the real colour is, your brain hides this part of the information, so that \u201ccolours are measured but not given to the [brain] network,\u201d explains Poggio.\n\n\u201cThe hope is that if you train a network to predict colour or the next image, can this network do other things easier? Can it learn to recognize objects with less data?\u201d asks Poggio. \u201cThese are open questions that, once we get the answers, the whole deep learning community would benefit from.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-test-self-driving-food-delivery-with-meituan-eb3916f3ac83?source=user_profile---------23----------------",
        "title": "Baidu to Test Self-driving Food Delivery with Meituan",
        "text": "Xiongan City is getting a new food delivery service that requires no interaction with humans.\n\nChinese media is reporting that Baidu will launch a series of self-driving delivery tests in cooperation with Meituan-Dianping, a group-buying service for local food and retail businesses. The first tests will take place in Xiongan, about 100 kilometres southwest of Beijing.\n\nAn insider familiar with the matter said testing was slated to begin on May 1, however Xiongan is still building the required infrastructure, and so the official announcement will be postponed.\n\nAutonomous vehicles can not only cut labor costs in the food-delivery business, but can also for example reduce security issues for batch deliveries to restricted locations such as construction sites.\n\nBaidu is leading the development of China\u2019s autonomous driving technologies. Last June, the search engine giant introduced its autonomous driving platform Apollo, which is billed as \u201cthe Android of the auto industry.\u201d Apollo aims at democratizing autonomous driving with open-source data and code. Last month, Baidu became the first company granted special license plates for autonomous vehicle testing on public roads in Beijing.\n\nAlso on Thursday, Baidu released the latest iteration of its Apollo platform, Apollo 2.5, which now supports autonomous driving on geo-fenced highways. Baidu also announced the new Apollo Automotive Cybersecurity Lab, which aims to advance safety in mobility.\n\nMeituan set up a project team on self-driving delivery R&D in 2016. Last year the team was promoted to a business division with a staff of over 200.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-national-ai-team-gets-busy-46f226d7d054?source=user_profile---------24----------------",
        "title": "China\u2019s \u201cNational AI Team\u201d Gets Busy \u2013 SyncedReview \u2013",
        "text": "Words often have different meanings in the business world. A \u201cUnicorn\u201d for example is not a magical horse with a horn projecting from its forehead, but rather a startup that\u2019s valued at more than a billion dollars.\n\nA phrase that\u2019s cropping up a lot in Chinese media these days is \u201cNational AI Team (\u4eba\u5de5\u667a\u80fd\u56fd\u5bb6\u961f).\u201d While this may suggest a squad of scientists heading off to some sort of AI Olympics, it rather refers to a new class of AI companies that are either backed by national institutes or closely integrated into government-funded programs.\n\nWhy are they calling themselves the National AI Team?\n\nCloudWalk, a spinoff from Chongqing Institutes of Green and Intelligent Technology (CIGIT) of the Chinese Academy of Sciences (CAS), is a typical National AI Team member. Founded in 2015, the company specializes in computer vision and machine learning.\n\nIn China, core industries such as public security, banking, political communication, civil aviation, and urban transport are being overhauled through the concerted application of various AI techniques. Security bureaus for example want object tracking systems to help spot criminal activity and hunt fugitives, while banks are using facial recognition algorithms to provide customers with almost-instant access their bank accounts. The government owns or controls most of these industries, and they are rigorous in selecting their technical partners.\n\n\u201cIf a bank\u2019s system goes down for two hours, its governor will have a problem; if it doesn\u2019t work out in four hours, he will write a report; if it doesn\u2019t work out in eight hours, it\u2019s going to be a serious incident, the bank\u2019s rating is bound to drop, and the bank may even be closed,\u201d CloudWalk Founder and CEO Xi Zhou (\u5468\u66e6) told Synced.\n\nWhile a private company\u2019s best way to attract government clients is robust technology or products, being on the National AI Team can also give it an advantage in core industries. Over 100 banks have adopted CloudWalk\u2019s facial recognition technique, making it the largest provider of the tech to China\u2019s banking industry. The company also develops and deploys facial-recognition-enabled surveillance cameras in over 80 percent of domestic airports.\n\n\u201cI was not aware of this advantage until I founded CloudWalk,\u201d says Zhou. \u201cObviously the state-owned banks and public security departments have more faith in our technologies and products because we come from the Chinese Academy of Sciences.\u201d\n\nWhat if a startup does not enjoy a relationship with a state-owned institute? They might also look for investment from a state-owned venture. China\u2019s computer vision startup Face++ announced financing of US$460 million last November led by the China State-Owned Capital Venture Investment Fund, while the Guangzhou Municipal Government injected US$300 million into CloudWalk\u2019s Series B funding round.\n\nThe Chinese Government owns a large-scale database, which is one of the most important components for developing AI applications and products. Li Xu (\u5f90\u7acb) is the CEO of SenseTime, a company that uses deep learning to replicate tasks performed by the human visual system. In an interview with Quartz, Xu boasted that his company has a database of over two billion images. Much of that data comes from various government agencies.\n\nSenseTime raised US$410 million last June, including a B2 round led by Sailing Capital, whose major shareholder is the large state-owned financial holding company Shanghai International Group. SenseTime is now the world\u2019s most valued AI startup. The injection of the state-owned capital is expected to help SenseTime secure more government contracts. At present, 30 percent of SenseTime\u2019s clients are government-related.\n\nWhile National AI Team status may be an \u201cexpress lane\u201d to government deals, the role also comes with responsibilities. Last year China issued an ambitious policy blueprint calling for the nation to become the world\u2019s primary AI innovation center and aiming to build a domestic industry worth some US$150 billion by 2030. The government is counting on its National AI Team to put that plan into practice.\n\nOver the past five years, the world has seen an explosion of activity in deep learning in both academia and industry all around the world. While AI is energizing industries, the lack of industry-wide standards for research and development is creating a number of problems: How to correctly access users\u2019 data without violating privacy? How to qualify a dataset for use in training AI algorithms? How to ensure the safety and precision of an AI application?\n\nCarlos E. Perez, author of the book Artificial Intuition and Founder of Intuition Machine, stressed the need to standardize the best practices of developing deep learning in his medium blog. \u201cIn conventional software development, we have a more mature conceptual framework that has evolved over time. Deep Learning introduces new kinds of requirements, so we need to understand what these are and what standardize the class of tools needed.\u201d\n\nThe Chinese Government is pushing hard for AI standardization. Last year it released its Standardization of AI Helps Industry Development along with a blueprint for further AI integration with the economy. This year it followed up with the Artificial Intelligence Standardization White Paper.\n\nIn March 2017 the National Development and Reform Commission launched the Public Service Platform for Basic Resources (\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u8d44\u6e90\u516c\u5171\u670d\u52a1\u5e73\u53f0) with a heavy focus on AI. This platform will be built over the next three years and aims to deliver:\n\nProminent National AI Team players Baidu, Tencent, iFlytek, and CloudWalk were designated to take the lead on the Platform\u2019s development.\n\nChina\u2019s voice technology giant iFlytek was founded by alumni of the University of Science and Technology of China, a national research university, while Baidu and Tencent have dominated the country\u2019s tech scene for years.\n\nCloudWalk spokesperson Xiaolong Fu told Synced, \u201cAI is closely related with data and privacy, so National AI Team members can help ensure information security. Meanwhile, we also want to ensure that our intellectual property remains in our own hands.\u201d\n\nChina\u2019s quest to develop cutting-edge AI technologies provides a world of opportunities for companies on the National AI Team, particularly with the increasing number of government initiatives and state-run projects in the field. The Team may also spur development and deployment of improved AI resources for academic, industry and public use.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/tencents-smart-speaker-tingting-brings-wechat-to-the-entire-family-e0787eba8391?source=user_profile---------25----------------",
        "title": "Tencent\u2019s Smart Speaker Tingting Brings WeChat to the Entire Family",
        "text": "Tencent\u2019s WeChat is like the Chinese equivalent of WhatsApp + Facebook. Each day hundreds of millions of users press the WeChat Hold to Talk button to send voice messages.\n\nNow Tencent wants to free up its users\u2019 hands. The Chinese Internet giant today officially launched its first-ever smart speaker, Tencent Tingting (\u817e\u8baf\u542c\u542c), which can access users\u2019 WeChat accounts and send or receive voice messages. Moreover, Tencent Tingting can automatically convert text messages to voice format and read them out to you.\n\nThe speaker has a Mandarin interface, and is specially designed to accommodate the elderly and children, who either have no phone or find it difficult to use one. The company says it has tailored Tingting\u2019s speech recognition algorithms to better recognize the particularities of speech in these age demographics.\n\nMany Chinese households have three generations living under the same roof. Tingting aims to create a family-friendly communication environment where grandparents won\u2019t have to put on their reading glasses if they get a text and toddlers can easily talk with their parents remotely. Tingting users need not have a WeChat account.\n\n\u201cWhen my grandpa was speaking to Tingting, I didn\u2019t even know what he said, but the speaker understood,\u201d says Tencent Tingting Chief Products Officer Chaoqin Wang. \u201cWhen the speaker replied to him, he said \u2018Oh, not bad. It\u2019s amazing\u2019. I could feel that the old man was very pleased that he was understood by a modern Internet product.\u201d\n\nThe sleek and cylindrical Tingting comes in black or lime. It has an amusing wake-up word, \u201c9420\u201d (Jiu Si Er Ling), which sounds like \u201cI just love you\u201d in Mandarin. A self-adaptive tuning function can adjust volume based on environment.\n\nTencent boasts that Tingting outperforms its competition in acoustic quality. Music Critic Lihua Su appreciated the full sound, which he said didn\u2019t come from a \u201cpoint,\u201d but rather an \u201carea.\u201d \u201cThe unique speaker design makes anywhere in a room a \u2018sweet spot\u2019,\u201d he told Synced.\n\nTingting features an unplugged design with lithium batteries that can power it for up to 16 hours, five hours in WIFI mode, or six hours in Bluetooth mode. This was a bold choice by Tencent, as integrating batteries with smart speakers raises many technical challenges, including how to compress the algorithms to lower power consumption.\n\nThe speaker\u2019s content library is stuffed with more than 20 million original songs, one million audio stories for children, and one hundred million hours of other audio content. \u201cFor a speaker, content is at the core of customer needs,\u201d says Wang.\n\nTencent\u2019s announcement signals the company\u2019s entry into China\u2019s heated smart speaker market, where Baidu\u2019s Raven H, Alibaba\u2019s Tmall Genie and hundreds of competitors are all vying for the vanguard.\n\nTingting is sold on JD.com at \u00a5699(US$100), which is slightly more expensive than competitors like Xiaomi\u2019s Mi AI speaker Mi at CN\u00a5299 (US$45), and Tmall Genie at CN\u00a5499 (US$80). Genie even went on sale at the rock-bottom price of CN\u00a599 (US$15) during China\u2019s November 11 \u201cSingles Day\u201d online shopping spree.\n\nTencent bills Tingting as a powerful smart assistant with compelling features and superior performance, and it looks and sounds the part. Although Tencent hasn\u2019t yet built much of a reputation in the hardware market, insiders suggest the company views smart speakers as a serious long-term business, and there will be more Tencent smart speakers coming up.\n\nSynced is covering the story and will update readers on Tingting\u2019s performance in the market."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-ups-its-chip-game-with-ali-npu-6dbc469deeb7?source=user_profile---------26----------------",
        "title": "Alibaba Ups Its Chip Game With Ali-NPU; Acquires Chipmaker C-Sky Microsystems (Updated)",
        "text": "Alibaba\u2019s R&D arm DAMO Academy announced yesterday that it is developing a new neural network chip called Ali-NPU for AI inferencing in the field of image processing, machine learning, etc.\n\nAli-NPU is expected to be some 40 times more cost-effective than conventional chips. Alibaba Researcher Yang Jiao says this chip\u2019s performance will be 10 times better than mainstream CPU and GPU architecture AI chips currently on the market, with only half the manufacturing cost and power consumption. By self-developing AI chips tailored to its own needs, Alibaba will reduce its dependence on other chip companies.\n\nIn 2015, Alibaba cooperated with Hangzhou C-SKY Microsystems to develop the Yun On Chip (YoC). In November 2016, Alibaba and Tencent invested US$23 million in California chip company Barefoot Networks.\n\nDAMO Academy is an R&D institute for fundamental and disruptive technology research established by Alibaba in November 2017. It has a dozen top-notch international scientists working in quantum computing, machine learning, network security, visual computing, natural language processing, chip technology, and embedded technology. Alibaba is planning to inject US$15 billion into this project over the next three years.\n\nAlibaba announced yesterday that it has acquired chipmaker C-Sky Microsystems in a bid to strengthen its chip-making R&D capability. Financial terms of the deal were not disclosed.\n\nFounded in 2011, Hangzhou-based C-Sky Microsystems is an integrated circuit design company dedicated to 32-bit, high-performance and low-power embedded CPU, with chip architecture licensing as its core business. The company\u2019s embedded CK-CPU chip software and hardware platform provides customers with CPU IP core, System-on-Chips (SoC) design and development platforms.\n\nC-Sky Microsystems bills itself as \u201cthe only embedded CPU volume provider in China with its own instruction set architecture.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/mits-josh-tenenbaum-on-intuitive-physics-psychology-in-ai-99690db3480?source=user_profile---------27----------------",
        "title": "MIT\u2019s Josh Tenenbaum on Intuitive Physics & Psychology in AI",
        "text": "When Bayesian Cognitive Scientist Josh Tenenbaum recently told a packed University of Toronto lecture hall that \u201cintelligence is not just about pattern recognition,\u201d the significance of the statement was not lost on the audience.\n\n \n\nThe university is the birthplace of the \u201cHinton School of Neural Networks\u201d, where Geoffrey Hinton redesigned the neural network approach to AI by coding a synthesized biological circuitry of neurons into artificial models that excelled in pattern recognition, trailblazing a new path for the development of artificial intelligence.\n\n \n\nFollowing an introduction by Hinton, Tenenbaum quipped about the old days, when Hinton \u201cwas willing to be called a cognitive scientist.\u201d This drew laughter from the audience, considering the latter\u2019s eminence in the field of computer science. As the principal AI investigator at MIT\u2019s Center for Brain, Minds, and Machines (CBMM), Center for Brain and Cognitive Sciences (BCS), and Computer Science and Artificial Intelligence Laboratory (CSAIL), Tenenbaum is widely respected for his interdisciplinary research in cognitive science and AI. \n\n \n\nBreakthroughs in AI and deep learning have prompted neuroscientists such as Dan Yamins from the Stanford NeuroAILab to rethink the structure of the ventral stream \u2014 the object and visual recognition part of the brain. This cross-pollination is hardly a surprise, considering that early publications on neural networks frequently appear in journals like Psychological Review, Cognitive Science, and Nature, as the field moved forward from the 50s\u2019 single-layer perceptron to Kunihiko Fukushima\u2019s neocognitron in the 80s, to Yan LeCun\u2019s widely-used deep convolutional neural networks of today.\n\nHowever, if humans hope to develop artificial general intelligence, data-munching, pattern-seeking deep neural networks may not be the best approach. Might Bayesian networks, causal models, and predictive coding work better? Or a symbol manipulation engine modeled after logic, lambda calculus, and programming languages be the route to pursue? Tenenbaum wants to steer the research wheel to cognitive science and look for the answer there.\n\nHuman common sense involves the understanding of physical objects, intentional agents and their interactions, which Tenenbaum believes can be explained through intuitive theories. This \u201cabstract system of knowledge\u201d is based on physics (eg. forces, masses) and psychology (eg. desires, beliefs, plans).\n\n \n\nSuch intuitions are present even in young infants, bridging perception, language, and action planning capabilities. A 2011 study by Erno Teglas models the physics reasoning capability of 12-month-olds, while Elizabeth Spelke\u2019s pursued a similar research course in her paper psychological inferencing capability in 10-months-old.\n\nHow do we use computation models to reverse-engineer intuitive theories and teach an AI to evolve based on these principals? Tenenbaum suggests tackling the problem by using a new class of programming language called Probabilistic Programs, which is a compound of symbolic language, probabilistic inference, hierarchical inference (learning to learn), and neural networks.\n\nIn collaboration with Tenenbaum\u2019s group at MIT, DeepMind researcher Peter Battaglia is working on \u201ca realistic model of physics that can estimate physical properties and predict probable futures,\u201d to quote from the paper he co-authored with Tenenbaum, Computational Models of Intuitive Physics. Based on Bayesian inferencing, this model makes predictions in simulated 3D scenarios based on real-life statics, dynamics, forces, collisions, and friction. \n\n \n\nFacebook AI proposed PhysNet in 2016, a neural network that predicted whether a stack of blocks would fall. The network excelled in predicting outcome and estimating block falling trajectories, discerning them based on color. Tenenbaum says that for a prediction involving 2\u20134 cubes, PhysNet required over 200k training scenarios.\n\nIn the joint research program Learning Physics from Dynamic Scenes, developed with Stanford\u2019s Noah Goodman, Tenenbaum proposes a hierarchical Bayesian framework, working with probabilistic programs to model intuitive physical theories. The project trains the model on inferring physics scenarios in varying time periods, with different physical laws and properties at play. Inferring properties include mass, charge, friction, elasticity, and resistance.\n\n \n\nIn order to model intuitive psychology, researchers use a model wherein the agent considers its desires and its beliefs about the environment, which enables planning and then actions. Jara-Ettinger and Julian Schulz at Yale call this the \u201cnaive utility calculus.\u201d Co-authored with Tenenbaum, Jara-Ettinger\u2019s 2017 paper in Nature proposes a Bayesian theory of mind (BToM) model that infers an actor\u2019s beliefs, desires, and percepts from how they move in the local environment.\n\nTenenbaum lectures at the nexus of AI, cognitive science, and neuroscience. As he notes in the 2017 paper Building Machines That Learn and Think Like People co-authored with Brenden Lake, Tomer Ullman and Sam Gershman, the most immediate task for AI is to \u201ca) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and \u00a9 harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations.\u201d \n\n \n\nResearch in intuitive physics and psychology are especially promising in the field of robotics. A robot that knows intuitive physics can navigate the environment and perform nuanced actions such as carrying a cup of coffee, grasping a party balloon, and so on. Meanwhile, a robot that knows intuitive psychology by heart could observe, for example, a child pointing at cotton candy while crying as its parent shakes head, \u201cno,\u201d and would be able to correctly infer both humans\u2019 intentions.\n\n \n\n Tennenbaum is aiming for an AI that more completely understands the physical and psychological landscapes it will exist in. Such machines may also allow us to deepen our own understanding of intelligence.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-gears-up-for-self-driving-road-tests-6d603b45682e?source=user_profile---------28----------------",
        "title": "Alibaba Gears Up for Self-Driving Road Tests \u2013 SyncedReview \u2013",
        "text": "China\u2019s self-driving industry has just welcomed another major player: Alibaba Group is developing Level 4 autonomous driving technology and is ready to test its self-driving cars on public roads, Chinese media reported today.\n\nThe e-commerce giant has assigned its Chief Scientist and former Nanyang Technological University in Singapore Associate Professor Gang Wang to lead the project. Alibaba is looking to add more than 50 self-driving experts and specialists to the project to maximize its R&D capability, according to an Alibaba insider.\n\nAlibaba is also ready to conduct its first road test with a converted Lincoln MKZ, a model widely used by self-driving companies such as Baidu, NVIDIA, and many startups. The vehicle will test L4 autonomous driving capabilities, a level defined as \u201cfully autonomous\u201d in that the car can be operated without a human driver in most circumstances.\n\nAlibaba\u2019s engagement in self-driving technology began last year when it formed a partnership with Chinese domestic automakers SAIC Motor and Dongfeng Peugeot-Citro\u00ebn, and equipped the cars with its upgraded operating system AliOS.\n\nAlibaba\u2019s ambitious move represents a direct challenge against China\u2019s other internet giants Baidu and Tencent, which have been making substantial progress in the field of self-driving cars for several years. Baidu leads the race with its autonomous driving platform Apollo, which the company bills as \u201cthe Android of the auto industry.\u201d Just three weeks ago, Baidu received approval from the Beijing Municipal Government for testing its self-driving cars on Beijing roads. Tencent, meanwhile, reportedly tested one of its autonomous vehicles on Beijing highways earlier this month.\n\nWhile Uber and Tesla accidents and scandals in the US may have weakened public confidence in self-driving vehicles, Alibaba\u2019s move into the field is another sign that China remains dedicated to encouraging development and implementation of the technology.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-apr-w-2-apr-w-3-a101f46dd5ef?source=user_profile---------29----------------",
        "title": "AI Biweekly: 10 Bits from Apr W 2 \u2014 Apr W 3 \u2013 SyncedReview \u2013",
        "text": "IBM announces a four-year program in collaboration with Calgary-based Natural Resources Solutions Center to help oil and gas companies with sustainability and efficiency. IBM\u2019s AI platform Watson will digest raw data, structure it and come up with insights for oil and gas companies.\n\nAmazon adds a musical upgrade to Alexa\u2019s \u201croutine\u201d function, which enables users to combine multiple actions such into one command \u2014 for example to turn on both the coffee maker and kitchen lights in the morning. Alexa can now play music, podcast or radio shows as part of a routine, and users can also control audio output on the device. The move brings Alexa up to par with Google Assistant, which already integrates radio, podcasts and music.\n\nMontreal-based incubator Element AI will open a new office in Toronto where it will do R&D and work with local businesses that want to integrate AI into their operations and the Ontario Provincial Government. Element AI\u2019s July 2017 fundraising round raised a record US$137.5 million. This will be Element AI\u2019s fifth office, and the company is considering opening more in Asia in the near future.\n\nCalifornia\u2019s Public Utilities Commission has introduced a proposal to allow autonomous vehicles without a backup driver to transport passengers on state roads. The California Department of Motor Vehicles had already approved autonomous driving tests without a backup driver starting this month. The new regulations would allow members of the public to ride in such vehicles, and will be voted on next month.\n\nApril 10th \u2014 Microsoft Collaborates with C3 IoT to Accelerate AI in Enterprise\n\nMicrosoft and software company C3 IoT have announced a partnership to accelerate cutting-edge business level AI and IoT application development in areas such as AI predictive maintenance, dynamic inventory optimization, precision healthcare, and CRM. The co-marketing and co-selling deal also includes a co-development strategy on Microsoft\u2019s Azure cloud platform.\n\nQualcomm announces its own IoT and AI-optimized system on a chip (SoC) platform. The new platform is aimed at computer vision IoT applications such as security cameras, wearable cameras, and smart displays.\n\nAirFusion Wind is a cloud-based workflow and AI-based analysis platform that can identify and classify wind turbine asset damage. AirFusion Wind transforms inspection images from drones, ground-based sensors, and other image capture tools into data that can be used to evaluate conditions and reduce costs.\n\nApril 11th \u2014 NVIDIA Collaborates with Canon Medical Systems to Accelerate Deep Learning in Healthcare\n\nNVIDIA announces a collaboration with Canon Medical Systems to develop research infrastructure to support deep learning technology in the healthcare industry. The partnership will focus on deploying deep learning and big data analytics to support early detection and assisted diagnosis. Canon Medical Systems is the largest medical systems supplier in Japan, and will use the NVIDIA DGX system to process the massive data generated by its platform.\n\nGoogle\u2019s natural language processing and synthesis research just got fun. The company has uploaded two playful and creative interactive web experiments based on its word-association systems. These are offshoots of a new search option that allows users to directly ask the system questions instead of searching for specific words, titles, or authors, etc. The results are not perfect, but the system provides a useful and flexible new query function.\n\nApril 13th \u2014 Facebook Uses AI to Predict and Sell User Preferences\n\nThe Intercept creates a stir when it publishes apparently confidential internal Facebook documents that describe how AI can build predictive models of user behaviour that Facebook can sell to brands, a process some argue is unethical. The Intercept article argues that although Facebook says these algorithms are used to improve the user experience, in fact they are largely used to make money from advertisers.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/iclr-2018s-best-papers-variant-adam-spherical-cnns-and-meta-learning-6b48dca83e8b?source=user_profile---------30----------------",
        "title": "ICLR 2018\u2019s Best Papers: Variant Adam, Spherical CNNs, and Meta-Learning",
        "text": "Leading machine learning conference International Conference on Learning Representations (ICLR) has named its best research papers of the last year: On the convergence of Adam and Beyond, Spherical CNNs, and Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments.\n\nLaunched in 2013, the ICLR has grown to a world-class conference for machine learning researchers and engineers. ICRL 2018 received 935 papers \u2014 double last year\u2019s total \u2014 and 337 papers were accepted.\n\nIn On the Convergence of Adam and Beyond, Google New York proposes the new variant Adam, a gradient descent optimization algorithm introduced in ICLR 2015.\n\nGradient Descent is one of the most popular algorithm types for optimizing neural networks, but struggles with a convergence issue in non-convex settings that makes optimizations ineffectual. The Google paper suggests a new Adam algorithm which it says fixes the problem and improves the empirical performance.\n\nResearchers at the University of Amsterdam proposed Spherical Convolutional Neural Networks (CNNs) which can analyze spherical images, a technique in wide demand for drones, robots, autonomous cars, molecular regression problems, and global weather and climate modelling. The paper demonstrates that spherical CNNs can be efficiently applied to 3D model recognition and atomization energy regression.\n\nIn Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments, top academic institutes CMU, UMass Amherst, OpenAI, and UC Berkeley jointly developed a simple gradient-based meta-learning algorithm that can adapt to dynamically changing and adversarial environments.\n\nOver the past few years, reinforcement learning (RL) has successfully enabled machines to outperform humans in tasks ranging from Atari video games to the ancient and complex Chinese board game Go. However, the AI technique is not adaptable to non-stationary environments, for example a multiplayer game with high randomness. Meta-learning, the so-called learning-to-learn method, can compensate for RL\u2019s weakness.\n\nThis paper also introduced a new multi-agent competitive environment, RoboSumo, for more effective training of meta-learning algorithms.\n\nICLR 2018 runs April 30 to May 3 at the Vancouver Convention Centre in Vancouver, Canada.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/how-ai-can-speed-up-drug-discovery-3c7f01654625?source=user_profile---------31----------------",
        "title": "How AI Can Speed Up Drug Discovery \u2013 SyncedReview \u2013",
        "text": "The US FDA defines five steps for the development of a new drug: discovery and development, preclinical research, clinical research, FDA review, and FDA post-market safety monitoring.[1] The Tufts Center for the Study of Drug Development estimates the average cost of developing a new drug at US$2.55 billion, with the process potentially taking more than 10 years.[2]\n\nThe first step, drug discovery, typically involves one of four scenarios: finding new insights into a disease, finding possible effects of a drug by testing molecular compounds, repurposing existing drugs, or manipulating genetic materials. At the drug discovery stage, thousands of compounds may be potential candidates for development as a medication.[3] They need to go through a series of tests and only a small number will advance to further research.\n\nTo accelerate drug discovery and reduce the costs of drug development, pharmaceutical companies are introducing AI technologies such as machine learning and deep learning into the processes.\n\nDrug discovery is a data-driven environment with a massive of data such as high-resolution medical images, genomic profiles, metabolites, molecular structures, and biological information.[4] This information is published in papers and journals, however it can be a challenge for researchers to keep up with it. AI can use machine learning and deep learning to correlate, assimilate, and connect existing data more rapidly in order to help discover patterns in the data pools. By reviewing scientific research papers, AI can make connections that provide possible hypotheses for drug discovery.\n\nFinding new compounds for a medicine is difficult because the possible combinations are countless. Such research requires medical data on genes, proteins, metabolites, molecular structures, and biological information.[7] Processing this huge amount of information can be a very time-consuming task. Pharmaceutical companies are discovering that AI techniques such as deep learning algorithms can process the same information much faster.\n\nDrug repurposing, also known as drug repositioning or therapeutic switching, is the application of known drugs and compounds to treat new indications.[11] One advantage of drug repositioning is that most of the repositioned drugs have already passed a series of tests, and so have less risk of unexpected toxicity or side effects. With the help of machine learning algorithms, pharmaceutical companies can repurpose drugs faster and at lower costs than developing new drugs.\n\nManipulating genetic materials for drug discovery is also known as personalized medicine or precision medicine. This kind of drug discovery can be more effective in treatment because it is based on individual health data paired with predictive analytics.[14] In order to efficiently gather, analyze, store, and trace a person\u2019s detailed information, especially when the data is huge and unstructured, pharmaceutical companies use deep learning, machine learning, or computer vision.\n\nWe know that artificial intelligence can be applied in drug discovery to make the process faster. There are also areas where artificial intelligence can help in drug development. Clinical trials, for example, are currently classified into five phases and usually require more than three thousand test subjects to proceed from phase one to phase three.[17] Most pharmaceutical companies use recruitment firms to find clinical trial subjects by examining individual medical records.[18] This task takes time and the efficacy is low. Companies can use machine learning to train a model that includes age, sex, treatment history, and current health status to build an inclusion/exclusion criteria that will speed up this aspect of clinical trials research.\n\nMoreover, AI can help test the side effects or toxicities of candidate drugs. Cyclica, a Canadian startup, uses a suite of computational algorithms to evaluate and predict how drugs might interact with the human body.[19] This kind of testing helps pharmaceutical companies identify a drug candidate\u2019s side effects before clinical trials, so companies can make corrective adjustments in advance.\n\nDrug discovery can benefit from machine learning, deep learning, and computer modeling. However, there is also the potential for introduction of biases from unbalanced data, which might cause errors or discrimination while AI is training the neural networks.[8] A research team from Insilico Medicine discovered that accuracy could become unstable unless the neural network had been trained using diverse datasets. The range, quantity and quality of input data is therefore a key factor for further implementation of AI in drug discovery.\n\n[4] AI Provides New Insights for Accelerated Drug Development: https://blogs.sap.com/2017/11/02/ai-provides-new-insights-for-accelerated-drug-development/\n\n[5] AI in Pharma and Biomedicine \u2014 Analysis of the Top 5 Global Drug Companies: https://www.techemergence.com/ai-in-pharma-and-biomedicine/\n\n[6] What if AI could take your research to the next level?: http://benevolent.ai/blog/benevolentai/what-if-ai-could-take-your-research-to-the-next-level/\n\n[8] Artificial Intelligence: will it change the way drugs are discovered?: https://www.pharmaceutical-journal.com/news-and-analysis/features/artificial-intelligence-will-it-change-the-way-drugs-are-discovered/20204085.article\n\n[12] IBM, Teva to Use A.I. for Drug Repurposing Program: https://www.rdmag.com/article/2016/10/ibm-teva-use-ai-drug-repurposing-program\n\n[13] How Pharmaceutical And Biotech Companies Go About Applying Artificial Intelligence in R&D: https://www.biopharmatrend.com/post/34-biopharmas-hunt-for-artificial-intelligence-who-does-what/?lipi=urn:li:page:d_flagship3_feed%3BhAZ67GlARQyGqnLUgoHuzA\n\n[18] MEET THE COMPANY TRYING TO DEMOCRATIZE CLINICAL TRIALS WITH AI: https://www.wired.com/story/meet-the-company-trying-to-democratize-clinical-trials-with-ai/\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-yitu-battles-for-position-in-huge-facial-recognition-market-8887bcc87234?source=user_profile---------32----------------",
        "title": "China\u2019s Yitu Battles for Position in Huge Facial Recognition Market",
        "text": "If you deposit a cheque or withdraw cash from an ATM in China you\u2019ll no longer need a debit card to do so. New facial-recognition ATMs now enable almost-instant access your bank accounts.\n\nThe tech has been deployed on over 12000 ATMs nationwide by Yitu Technology, a Chinese AI startup specialized in computer vision and machine learning. ATMs are just one facet of Yitu, whose big plan is to put smart eyes on all human-machine interfaces.\n\nFounded by two Chinese AI experts in 2012, the company is particularly competent in recognizing human faces and vehicles. Such capability appeals to China\u2019s public security departments. In 2015, Suzhou local police captured a burglar in just ten minutes, using Yitu\u2019s technique to pinpoint the targeted car among hundreds in a surveillance video.\n\nThe case quickly sent a shockwave across the country. Now, over 30 provincial and 150 municipal public security departments have adopted Yitu\u2019s technique for identifying thefts and hunting fugitives.\n\nThe China Security Industry Network (21csp.com) projects China\u2019s security surveillance market value at CNY\u00a5752 billion (US$120 billion) in 2018. That\u2019s a big pie, and everyone wants a piece.\n\nIn an interview with Synced, Yitu Senior Researcher Shuang Wu said the company can accurately enable facial recognition in just one second, with a large database of over one billion facial photographs.\n\n\u201cYitu\u2019s technology has been well-accepted by many China\u2019s crucial ports and organizations as Yitu shows a superior performance even when it comes to racial spectrum, which is usually a difficulty in facial recognition,\u201d says Wu.\n\nYitu\u2019s advance in facial recognition technique quickly won global accolades as the company scored highest under the \u201cidentification accuracy\u201d category in a competition run by the US National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (IARPA) in November 2017.\n\nIn recent years Yitu has been expanding the use of its vision technologies to applications in smart cities for example. Partnering with Alibaba last year, Yitu energized Hangzhou\u2019s urban intersections by modelling vehicle behaviour and predicting traffic flow, speeding up traffic flow by 11 percent.\n\nWhile Yitu has consolidated its business in the security surveillance over the years, the company has also been plotting a bold course in healthcare, a traditionally conservative field that is now increasingly driven by technology. \u201cWe believe AI\u2019s potential in healthcare is unquestionably promising,\u201d says Wu.\n\nStruggling with air pollution, China has seen a sharp rise in lung cancer cases over the past 15 years. The country recorded nearly 4.3 million new cancer patients in 2015, 730,000 of whom had lung cancer, accounting for nearly 36 percent of the global total.\n\nYitu launched a smart medical imaging platform in early 2017 for lung cancer early detection \u2014 applying deep learning techniques to locate lung nodules, a type of small tissue mass in the lung that appear as round and white shadows on a CT scan. By analyzing size, shape and location, the system can help doctors compare a current imaging scan with historical scans to track changes in condition. It then creates a structured diagnosis report according to the imaging results, which is delivered to the doctor.\n\nZhejiang Provincial People\u2019s Hospital first deployed Yitu\u2019s system, and hospital clients have now grown to over 20. \u201cOur stats tell us the adoption rate of our diagnosis report is about 92 percent,\u201d says Wu.\n\nWu stressed that AI+healthcare is not to be treated as a short-term venture. \u201cPublic security for example has a clear threshold, and what you need to do is to reach a certain percentage of accuracy. Healthcare, however, is a bit more complicated. You have to keep talking with doctors and ask what and how to improve your product. While it is easy to make a demo, building trust in healthcare is a long-term challenge,\u201d says Wu.\n\nYitu\u2019s fast tech development and rapid rise attracted interest from venture capital. In 2017 the company raised CNY\u00a5380 million (US$60 million) in a Series C funding round led by Hillhouse Capital Group. Last month, Chinese media reported that Yitu had raised a new round funding, bringing the company\u2019s value to an estimated CNY\u00a515 billion (US$2.4 billion). The company has not officially confirmed the news yet.\n\nYitu now has a crew of over 500 in China, Singapore and Silicon Valley.\n\nThis yearlong series of huge funding rounds is providing Yitu with the resources it needs to develop new AI-powered products beyond machine vision, such as those based on natural language processing (NLP). Yitu recently created an NLP-based medical reference database that can efficiently convert patients\u2019 medical records into organized and usable data.\n\n\u201cFrom a technical point of view, deep learning reinforces the inner relationship between different academic areas, or you could say the threshold for developing NLP by a computer vision company is lower. On the other hand, NLP is something that any AI company should invest in,\u201d says Wu.\n\nYitu is one of four billion-dollar computer vision unicorns in China, along with CloudWalk, Face++, and SenseTime \u2014 the three-year-old startup that just became the world\u2019s highest valued AI company. They are all vying for the vanguard in marketplaces such as robotics, finance, security surveillance, transportations, mobile device, and AR/VR.\n\nThe fierce competition between the four companies does not allow Yitu time to relax \u2014 the company must continue growing. Wu says this is an exciting time for Yitu: \u201cThere are surprises every day. A lot of things happen beyond the plan, and somehow they achieve good success, bringing great challenges as well as opportunities for everyone.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-prepares-for-ai-talent-shortage-e66d0a3a0de2?source=user_profile---------33----------------",
        "title": "China Prepares for AI Talent Shortage \u2013 SyncedReview \u2013",
        "text": "Chinese State media People\u2019s Daily recently reported the country is facing a shortage of five million AI talents.\n\nA separate LinkedIn survey revealed that while there are some 1.9 million AI engineers worldwide, about one million reside in the US while China is home to just 50,000. And as the talent level increases, the gap only widens \u2014 of the 208 AAAI fellowships granted over the past 27 years, only 4 went to Chinese nationals.\n\nAlthough China is an AI business deployment leader with well-financed startups such as the US$4.5 billion AI unicorn SenseTime, the country\u2019s AI educational infrastructure is lagging. The US has six times more AI education institutions.\n\nMeanwhile, high demand has sent AI engineer salaries skyrocketing in China. IDG Capital\u2019s 2017 Internet Unicorn Salary Report shows compensation for top AI talent is 55 percent higher than average ICT industry employee salaries, 90 percent higher at intermediate positions, and 110 percent higher at junior positions.\n\nAI laboratories established through joint ventures between corporations and universities, such as the iFlytek and Aispeech labs at Shanghai Jiaotong University, currently provide top-level AI education opportunities. Such partnerships grant well-rounded tutelage to students and facilitate talent transfer. Dr. Cheng-Lin Liu from Chinese Academy of Sciences says \u201cPhD students can convert their researchers into direct products or services upon graduation.\u201d\n\nThe AI talent shortage also provides an opportunity for online education providers. Andrew Ng\u2019s deeplearing.ai and Udacity are penetrating the China market, providing coaching in fields of machine learning, deep learning, NLP, computer vision, Python and so on. Chinese competitors 51CTO, CSDN, and Netease also provide AI video tutorials. Last year, iFlytek launched its online \u201cAI University,\u201d offering speech recognition and synthesis expertise and entrepreneurship coaching.\n\nSome employers however are wary of the online training trend, believing AI-related research and engineering skills must be built on years of formal learning and research, and cannot be instilled through relatively short online courses.\n\nTo tackle the problem, the Ministry of Education this week announced ambitious goalsfor the coming decade: establish a set of 100 \u201cAI+X\u201d specialization categories by 2020 in the disciplinary fields of math, physics, biology, psychology, sociology, law, and other related professional fields. The action plan will also compile 50 seminal teaching materials, 50 state-level online open courses, and open 50 additional AI teaching and R&D centres over the coming decade.\n\nThe Affiliated Elementary School of Peking University has begun introducing primary students to genetic algorithms and neural networks using easily explainable graphics and games. This is part of a larger initiative to promote STEM and AI courses in elementary and secondary schools. The Tongzhou District Experimental Primary School has added 75 AI-related courses, including a winter bootcamp to teach students patent filing procedures for robotics, to \u201chelp them build up awareness for intellectual property protection,\u201d explains class teacher Zhang Li.\n\nThis month the Ministry of Education, Sinovation AI Lab, and Peking University jointly announced the Global AI Talent Training Program for Chinese Universities, pledging to educate 500 teachers and 5,000 students over the next five years. Participating staff undergo strict screening and must presently be teaching CS, preferably in an affiliated AI institute.\n\nIn the eastern city of Nanjing, computer vision firm Seetatech is giving middle school students weekly 70-minute demo lessons. \u201cFirstly we help students get a theoretical glimpse of AI, machine learning and their practical applications, then we teach them about object and facial detection, instructing them to build their own AI detection algorithms,\u201d says an onsite Seetech employee.\n\nNanjing University opened one of the first AI institutes in China this March, and is currently seeking AI researchers, offering a base annual salary of US$60k, a housing subsidy of close to US$200k, and over US$300k in research funding as the starter package.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-oks-self-driving-tests-on-public-roads-d73216d0c243?source=user_profile---------34----------------",
        "title": "China OKs Self-driving Tests on Public Roads \u2013 SyncedReview \u2013",
        "text": "New Chinese regulations will permit self-driving vehicle testing on public roads across the country.\n\nThe Chinese Ministry of Industry and Information Technology, Ministry of Public Security, and Ministry of Transport yesterday jointly issued the Intelligent Connected Vehicle Road Test Management Standards (Trial) (\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u9053\u8def\u6d4b\u8bd5\u7ba1\u7406\u89c4\u8303\u8bd5\u884c) notice, which introduces regulations for testing self-driving cars on public roads nationwide. The policies exclude \u201clow speed\u201d vehicles and motorcycles, and will take effect on May 1.\n\nThe new regulations aim to facilitate the development of self-driving technology through the wide deployment of public road tests. Chinese media is reporting that the country also plans to make its transportation infrastructure more self-driving-friendly, according to an Officer from the Ministry of Transportation.\n\nThis year the Shanghai, Beijing, and Chongqing municipal governments introduced their own self-driving regulations; and China\u2019s first special autonomous driving licence plates were issued to tech giant Baidu and electric vehicle startup NIO.\n\nThe new regulations require the presence in the test vehicle of a safety driver with at least three years driving experience and a clean record over the past 12 months. Testing companies are responsible for training their safety drivers, providing support in emergency situations on the road, and assume full liability for the vehicle\u2019s actions during testing.\n\nTest vehicles must be new and able to switch from self-driving mode to manual operation efficiently, safely and instantly. They require the capability for real-time monitoring, recording, and storing of vehicle status, control mode information (self-driving or manual), and location, direction and speed. Vehicles must also record environment perception and response data, headlight and turn signal status, and provide 360-degree video monitoring.\n\nTesting companies must meet all technical requirements and provide a detailed test plan and insurance of CNY\u00a55 million for each test vehicle. Once an application is approved, the company will receive an Intelligent Connected Vehicle Road Test Notice (\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u8def\u6d4b\u8bd5\u901a\u77e5\u4e66), and can then apply for a temporary test license plate from the local Traffic Management Department of Public Security (\u516c\u5b89\u673a\u5173\u4ea4\u901a\u7ba1\u7406\u90e8\u95e8).\n\nTest vehicles must adhere to their submitted test plans. If a test vehicle experiences a severe accident or serious violation, the relevant supervision department (\u4e3b\u7ba1\u90e8\u95e8) can revoke the temporary test license plate. The testing company must submit a summary report one month after each test ends and full test reports every six months.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/berkeley-researchers-create-virtual-acrobat-81427228fb50?source=user_profile---------35----------------",
        "title": "Berkeley Researchers Create Virtual Acrobat \u2013 SyncedReview \u2013",
        "text": "Simulated robots can now spin-kick like a karate expert or backflip like an acrobat. The Berkeley Artificial Intelligence Research (BAIR) Lab yesterday proposed DeepMimic, a Reinforcement Learning (RL) technique that enables simulated characters to regenerate highly dynamic physical movements learned from data collected from human subjects. BAIR is a top-tier research lab focused on computer vision, machine learning, natural language processing, and robotics.\n\n \n\nRL methods have been shown to be applicable to a diverse suite of robotic tasks, particularly motion control problems. A typical RL includes a policy function that consists of all action selections that machines can do, and a value function that returns a low or high reward each time a machine takes an action. Machines can self-learn skills by leveraging the reward. The epoch-making Go computer AlphaGo produced by DeepMind is grounded on the same technique.\n\n \n\nHowever, virtual characters trained with deep RL can exhibit abnormal behaviours such as jittering, asymmetric gaits, or excessive movement of limbs.\n\n \n\nBAIR\u2019s new paper DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skill introduces a policy function that collects challenging skills such as locomotion, acrobatics, martial arts, and dancing.\n\n \n\nBAIR next initialises a character to a state sampled randomly, a method known as Reference State Initialization (RSI). The character can learn skills from any state of moves, such as the inflection point of a flip, and RSI can allow the character to know which states will result in high rewards even before it has acquired the proficiency to reach those states.\n\n \n\nBy connecting RSI with Early Termination (ET), a standard practice for RL researchers to stop simulations that lead to failure, BAIR researchers ensured that a substantial proportion of the dataset consists of samples close to the reference trajectory. Without ET, the character may flail or fall, but will not learn to flip.\n\n \n\nThe research shows that the character can learn over 24 skills, with movements nearly indistinguishable from the human reference subjects. BAIR also says its technique is simpler and produces better results than the current leading motion imitation method, Generative Adversarial Imitation Learning (GAIL).\n\n \n\nBAIR hopes the new research will facilitate the development of more dynamic motor skills for both simulated characters and robots in the real world.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-puts-education-focus-on-ai-plans-50-ai-research-centres-by-2020-5589c35ba701?source=user_profile---------36----------------",
        "title": "China Puts Education Focus on AI; Plans 50 AI Research Centres By 2020",
        "text": "China\u2019s Ministry of Education last week issued its AI Innovation Action Plan for Colleges and Universities to the education departments at all levels and institutions of higher education. This action plan aims to advance China\u2019s universities to world frontiers in science and technology; energize their capabilities in AI technological innovation, talent cultivation, and global cooperation; and provide strategic support for the development of next-generation AI in China.\n\nThe plan sets three goals for the next 12 years:\n\nThe plan further proposes establishing a set of one hundred \u201cAI+X\u201d specialization categories by 2020, with specific goals to:\n\nChina sees AI as the state\u2019s innovation focus, which will spark technological breakthroughs and stroke national pride. In July 2017, the State Council issued the New Generation Artificial Intelligence Development Plan and listed \u201caccelerating the education of top-notch AI talents\u201d as a primary task.\n\nLast week, China\u2019s Ministry of Education, Sinovation Ventures, and Peking University jointly launched the country\u2019s first university program \u2014 Global AI Talent Training Program for Chinese Universities \u2014 to educate over 500 teachers and 5,000 students with AI expertise within the next five years.\n\nThe latest action plan can be viewed as a supplement to this program, with the same ultimate goal of putting the State Council\u2019s Development Plan into practice and making China a world leader in the field of AI by 2030.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/onsite-from-re-work-finance-summit-in-london-deep-learning-trading-6521912aa599?source=user_profile---------37----------------",
        "title": "Onsite From RE\u2022WORK Finance Summit in London \u2014 Deep Learning & Trading",
        "text": "Financial markets are becoming a new proving ground for deep learning. The AI technique has already achieved remarkable success in image recognition, speech detection and sentiment analysis, and is believed to be well-suited for dealing with financial data.\n\nAt last month\u2019s RE\u2022WORK Deep Learning in Finance Summit in London, leading AI industry practitioners and academics from prestigious universities discussed their research, provided insights on business trends and real-life AI applications, and addressed current challenges facing the AI industry as a whole.\n\nSynced visited the summit to explore how deep learning techniques, such as neural networks and LSTM, can be applied to proprietary trading. This article focuses on Filippo Scopel\u2019s presentation Learning to Trade and Dr. Luigi Troiano\u2019s Supporting Trading in Financial Markets using Deep Learning Tools.\n\nScopel is a machine intelligence engineer at Merantix, a Berlin research lab building AI ventures. Scopel\u2019s research speciality is mathematical modelling and algorithm development with applications in finance and other fields. For the past year, he has been training deep learning models for time series prediction on financial microstructure data.\n\nScopel discussed Merantix\u2019s use of deep neural networks to predict price movements with short-term samples of under five minutes, and how these predictions can be transferred into trading strategies and then trading algorithms.\n\nDeep learning models are trained to make predictions by absorbing raw data such as past prices, conducting data computation, and generating results. Due to the complexity of the hidden layers within such neural networks, it is impossible to make an exact interpretation of how those predictions were generated.\n\nPredictions will be turned into specific trading strategies regarding bid-asks and particular time horizons. A decision to buy, hold, or sell is then generated and executed by the algorithm.\n\nAlthough the Merantix predictions can be more accurate than traditional methodologies, Scopel pointed out the system is far from infallible. Even with the right predictions, Merantix can still record losses due to the characteristics of high-trading frequency. What he means is transaction costs, and full bid-ask spreads can erode the potential profitability, sometimes resulting in losses.\n\nDr. Troiano is an Assistant Professor of AI, Machine Learning and Data Science at the University of Sannio. He began his presentation by identifying areas within finance where AI and particularly deep learning could be applied, including reshaping the analysis space, searching complex patterns in data, and trading robotisation. Data filtering removes noise to enable better quantitative analyses, and complex correlations and co-occurrences in large datasets can be identified to generate improved trading strategies. Automation reduces costs and improves the long-term effects of hedge funds and proprietary trading firms.\n\nIn forecasting the variability of prices, also referred to as volatility, Dr. Troiano said long short-term memory (LSTM) networks \u2014 a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies \u2014 work best when volatility is extremely high, which is a beneficial finding since such periods also provide high-profit opportunities.\n\nDr. Troiano then discussed his research into algorithmic trading using technical indicators such as the moving average convergence divergence (MACD) with deep learning techniques. A deep learning system can construct trading strategies after observing historical prices and indicator values instead of being explicitly programmed to execute those strategies. What this means is that AI has the potential to learn, or more appropriately, invent trading effective strategies.\n\nHowever, Dr. Troiano stressed that his research is still at its preliminary stage, and only certain deep learning techniques have been tried. More research is needed to determine how neural networks can improve on old-fashioned methods.\n\nOverall, it is clear that deep learning can accelerate the performance of tasks such as short-term future price predictions and proprietary trading strategies, but the AI technique has not yet reached maturity for full application in the trading market. The industry needs to be patient before widely employing neural networks to make market predictions.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-photographs-chinese-jaywalkers-shames-them-on-public-screens-ad0a301a46a6?source=user_profile---------38----------------",
        "title": "AI Photographs Chinese Jaywalkers; Shames Them on Public Screens",
        "text": "China\u2019s traffic police are using AI to tackle \u201cChinese-style jaywalking\u201d at major urban intersections. Facial recognition cameras take a 15-second video and four snapshots of pedestrians crossing on a red light. Pictures are matched with photo IDs in the police database, and violators can have their headshots along with family name and partially obscured citizen ID and registration address displayed on large roadside screens.\n\nSince deploying the system in April 2017, Shenzhen traffic police have caught over 13,930 jaywalking pedestrians and non-motorized vehicles. Jaywalkers are fined up to CNY\u00a520 (US$3) and are subject to traffic rule refresher courses or community service at road intersections.\n\n\u201cSince the new technology has been adopted, jaywalking cases have been reduced from 200 to 20 each day at the major intersection of Jing\u2019Shi and Shun\u2019Geng roads. Fewer people are crossing roads during red lights,\u201d said Li Yong, a Traffic Police Officer in the eastern city of Ji\u2019Nan.\n\nChinese-style jaywalking is a social nuisance. According to Ji\u2019Nan city statistics, barging pedestrians and non-motorized vehicles account for 16 and 33 percent of traffic accidents per year respectively. As ever-broadening Chinese roads can now have 10 lanes at urban intersections, ignoring traffic signals is extremely dangerous.\n\nTraffic authorities also plan to build a social credit system wherein jaywalkers will start receiving text messages or Weibo notifications. Traffic police will record the number of violations, and a certain threshold will affect the offender\u2019s social scores, which may limit their ability to borrow from banks.\n\nSome may ask whether this AI is saving lives or infringing on personal privacy? Li Yi, a research fellow at the Shanghai Academy of Social Sciences, says the public display of offenders\u2019 photos and partial personal information may prove to be effective in reducing pedestrian accidents and injuries, \u201chowever, we always need to find a balance between law enforcement and privacy protection.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-sensetime-scores-us-600-million-in-funding-to-become-the-world-s-most-valued-ai-startup-a505d2fa9c01?source=user_profile---------39----------------",
        "title": "China\u2019s SenseTime Scores US$600 Million in Funding to Become the World\u2019s Most Valued AI Startup",
        "text": "China\u2019s computer vision company SenseTime today announced it had raised a staggering US$600 million in Series C funding, setting a world record for an AI company and bringing its value to an estimated US$4.5 billion to make it the world\u2019s most valued AI startup.\n\nChina\u2019s e-commerce giant Alibaba Group led the funding, joined by Temasek Holdings and Suning Corporation. Says Alibaba Group Vice Chairman Cai Chongxin, \u201cSenseTime\u2018s research capabilities in deep learning and computer vision are impressive. Alibaba looks forward to partnering with SenseTime to inspire more innovations and create value for society.\u201d\n\nFounded in 2014 by Dr. Xiao\u2019ou Tang, a Professor of Information Engineering at the Chinese University of Hong Kong (CUHK), SenseTime uses deep learning in the development of computer vision to replicate tasks performed by the human visual system.\n\nSenseTime Co-Founder and CEO Xu Li told Synced the company set up an R&D team of 200 scientists during its first two years. The investment quickly paid off, as the team came up with an advanced deep learning framework and a cutting-edge 1207-layer neural network in 2016.\n\nSenseTime\u2019s superior AI technologies were soon transformed into a number of marketable software solutions: SensePose to synchronize users\u2019 movements with virtual figures in real-time videos; SenseVideo to recognize the positions and attributes of humans, vehicles and other entities from video input; and SenseFace to detect humans faces in a millisecond.\n\nThese technologies can be applied to multiple industries, including automotive, finance, mobile internet, robotics, security, and smartphones, and helped SenseTime score mega-clients such as China Mobile, China UnionPay, Huawei Technologies Co, Xiaomi, JD.com, and an important strategic partnership with chip giant NVIDIA.\n\nIn late 2016 SenseTime raised US$120 million in funding led by Beijing-based CDH Investments, Dalian Wanda Group, IDG Capital Partners and Star VC. Some six months later the company closed its Series B funding with US$410 million from CDH and Sailing Capital, which propelled it onto CB Insights\u2019 2017 technology unicorn list.\n\nSenseTime has about 700 staff, with 120 researchers who hold doctoral degrees. According to a November Reuters report the company is preparing for an IPO.\n\nXu says the latest funding round will not only strengthen the company\u2019s advantages, but bring more business opportunities: \u201cThe Series C financing will help SenseTime apply its core technology to more industries, expand the business landscape by cooperating with partners globally, and connect the upstream and downstream industry.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/simulations-pave-the-road-for-self-driving-technologies-78b696227383?source=user_profile---------40----------------",
        "title": "Simulations Pave the Road for Self-Driving Technologies",
        "text": "The Encyclopaedia Britannica defines a computer simulation as \u201cthe use of a computer to represent the dynamic responses of one system by the behaviour of another system modeled after it.\u201d Airline pilots train on simulators, while in the automotive industry driving simulations are used to optimize ride experience and improve engine performance. With the emergence of autonomous vehicle technologies, the development and deployment of effective self-driving simulations has become an industry priority.\n\nSelf-driving simulations collect data to improve autonomous vehicles\u2019 algorithm training capability, sensor accuracy and road data quality. It\u2019s been proposed that autonomous vehicles should log 18 billion kilometers (11 billion miles) of road test data to reach an acceptable safety threshold. This would however require a fleet of 100 vehicles running on roadways 24/7 at a constant speed of 40 kph (25 mph) for 5 years. Self-driving simulations are an ideal solution, enabling continuous and unlimited data collection in the digital environment with relatively low operational costs.\n\nSelf-driving simulations have advantages in milage data collection efficiency, road condition dataset diversity, and sensor corresponding data accuracy.\n\nSelf-driving simulations can boost the speed of data collection to reach mileage accumulation targets while reducing fleet operation costs. Waymo and Baidu have made use of self-driving simulation to speed up self-driving development.\n\nSelf-driving simulations can also add more uncertainty to a dataset to increase the responsiveness of the system. They can produce a variety of scenarios to test and improve vehicle performance under different conditions, for instance in severe weather, heavy traffic environments, and various distinct scenarios. Truevison.ai and AirSim are two leading solutions in this field.\n\nSensors are the eyes of the self-driving car. Multiple sensors can increase the accuracy of input data and protect the vehicle from misidentification or malfunction. Self-driving simulations provide multiple signals simultaneously, such as camera, LiDAR, radar and more. A given object in the virtual environment will thus be detected by different sensors, and these signals will validate each other to increase accuracy. RightHook and Cognata are two simulators providing multiple self-driving sensors.\n\nThe limitations of self-driving simulations should not be overlooked. Currently, there are two main weaknesses: lack of emergency situation scenarios, and potential consequences of differences between real and simulated data.\n\nEmergency situations are still hard to simulate as each real world accident is unique. Although vehicles can learn general driving operations through simulation, it is impossible to predict every single emergency situation. For example the May 7th, 2016 fatal accident involving a Tesla on autopilot occured because the system failed to distinguish a white truck against a bright sky. Although this is not a rare scenario in the real world, the simulator had not covered it.\n\nThis differences between real and simulated data is another issue that could negatively affect system performance, as the full consequences of these differences remain unclear. Engineers are hard pressed to determine what type of data leads to an accident due to unexplainable features of the algorithms. For example, real world pedestrians with disparate clothing and posture profiles cannot all be reflected in the simulator, and this might reduce a self-driving vehicle\u2019s ability to identify pedestrians.\n\nIn order to overcome these drawbacks, it is important to simulate more scenarios to make abnormal data traceable. SynCity is a Dutch company developing an advanced simulator that aims to present a wider range of scenarios, including other vehicle misbehaviour and various emergency situations, in order to optimize algorithms.\n\nIn the words of Toyota Research Institute Chief Executive Gill Pratt, \u201cSimulation is a tremendous thing.\u201d The correct understanding and prudent application of self-driving simulations are essential to safely accelerating R&D in the autonomous vehicle industry.\n\nReference:"
    },
    {
        "url": "https://medium.com/syncedreview/ai-powered-hockey-analytics-a-game-changer-8534b2e263aa?source=user_profile---------41----------------",
        "title": "AI-Powered Hockey Analytics: A Game Changer \u2013 SyncedReview \u2013",
        "text": "Analytics are all the rage in professional sports. The concept can be traced back to American statistician Bill James, who introduced his \u201cSabermetrics\u201d method for in-game baseball analysis in the 1970s. When the NBA\u2019s Golden State Warriors decided to favour three-pointers over two-point shots in 2016, the winning strategy sent a shockwave through professional basketball. This was a \u201cdata-driven decision\u201d based on higher score probability, explains Alex Martynov.\n\nMartynov is the 24-year old founder of ICEBERG, a Canadian startup using AI algorithms in sports analytics with a focus on ice hockey. Three years ago, Alex shared the idea of an AI sports analytics company with his investor father, who helped him kickstart the idea with $25,000. Not much, but it was enough for Alex to gather programmer friends in Toronto and Moscow and put together a working prototype.\n\nICEBERG installs a set of three FLIR thermal cameras around the rink before the start of each game. The video has lower resolution than an iPhone recording, but provides the constant full-ice view the company\u2019s algorithms require, as broadcast feeds typically leave 50 percent or more of the ice surface and players out of frame.\n\nArtificial neural networks are trained to recognize all moving entities on the ice surface: 12 players grouped by jersey color, on-ice officials, and a small black puck that can reach speeds of 160 kph (100 mph). Computer vision algorithms previously trained on a dataset containing 10,000 variations of numbers from all angles can identify each player by jersey number.\n\nBy tracking player and puck coordinates 10 times per second, a sixty-minute game will generate one million data points. The algorithm matches individual player coordinates with those of the puck to record their passes, body checks, giveaways and takeaways, shots, and goals. Typically, about 7\u20139 percent of all shots result in goals, and variance here predicts higher or lower goal probability.\n\nICEBERG\u2019s AI tracks a total of 500 different metrics which correspond to player and team behavior, and the company sometimes finds statistical nuances that are counterintuitive to hardcore fans or watchful coaches.\n\nIn a match between favorite Canada and underdog Switzerland, Iceberg\u2019s AI found that Switzerland skated 1.7 more kilometers and were 5\u201310 centimeters closer to the puck in micro-episodes. The Swiss also had longer puck possession and generated 2.48 more expected goals (xG). Switzerland\u2019s superior metrics should deliver a win seven times out of ten. But Canada won the game 3\u20130. Why?\n\nMartynov explains that \u201cabout 40 percent of all game outcomes is luck, but the other 60 percent can be predicted, which is what we are trying to do \u2014 predicting what isn\u2019t random. Our clients can play five games and lose five times in a row, but data will show that they could\u2019ve won every time. The coach will call our analyst, and we tell them, \u2018calm down, it\u2019s just the variation, you will get back to the mean, if you continue playing like this you will win five games in a row\u2019.\u201d\n\nICEBERG uses NVIDIA\u2019s GPU and marketing expertise and Microsoft Azure\u2019s cloud storage. The company also participates in NVIDIA\u2019s Inception Program.\n\nPortal subscription fee ranges from US$400 \u2014 $800 per game. If a team plays 60 games in a season that\u2019s approximately US$30,000. Clients receive a report the morning after each game and can access detailed game numbers from the portal. ICEBERG also has on-call analysts to answer clients\u2019 questions.\n\nFinding clients can be a long process of convincing the coach, the manager, and the owner. ICEBERG\u2019s deal with Austria\u2019s Red Bull Salzburg required four months of negotiation. \u201cThere are coaches who are confused, asking \u2018why do I need this?\u2019\u201d says Martynov. \u201cWe are not trying to replace the coach or the manager, but give teams an edge. It makes hockey more intellectual.\u201d\n\nThere are also cases like Swedish teams V\u00e4xj\u00f6 Lakers and F\u00e4rjestad BK, who signed contracts in five minutes. The competitive edge of data analytics is too good to be ignored.\n\n\u201cCurrently, We have a market share of 5\u20137% of global professional hockey teams. But it\u2019s not moving as fast as I would like,\u201d says Martynov, \u201cWe want to go into the soccer market after this. If you get two percent of the soccer market, that\u2019s approximately the same as the entire hockey market. We started in the niche market, but hockey is also a very complicated sport where players skate fast, collide often, change every minute, not to mention the puck is very small. Technically, it\u2019s easy to downgrade from hockey into other sports.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-gtc-2018-peeks-inside-the-gpu-powered-world-3366891680fe?source=user_profile---------42----------------",
        "title": "NVIDIA GTC 2018 Peeks Inside the GPU-Powered World \u2013 SyncedReview \u2013",
        "text": "In 1993, on his 30th birthday, Jensen Huang gave himself the best present ever by founding NVIDIA, the graphic-processor company where he still serves as CEO. Huang and his NVIDIA team pioneered the graphics processing chip (GPU) in 1999, revolutionizing the visual performance of device displays. But even Huang never dreamt that his GPU would one day become a driving force in the arena of artificial intelligence (AI).\n\nIn 2011 at Stanford University, Andrew Ng, one of the top minds in AI, discovered that a dozen NVIDIA GPUs could perform as well as 2,000 CPUs in training deep learning models. A GPU contains thousands of cores and is capable of processing thousands of threads simultaneously. This parallel architecture makes GPU extremely powerful in large-scale but straightforward data computation.\n\nOther top academic institutes and laboratories quickly followed Ng and deployed GPUs for deep learning research with the belief that GPU could eliminate bottlenecks in computing capabilities that had vexed AI researchers for years and spark technological breakthroughs.\n\nNVIDIA quickly recognized this trend and pivoted its strategy to become an AI computing company. Over the last six years, the Santa Clara chipmaker has been pushing the limits of GPU architecture, releasing a cutting edge GPU every one or two years to empower computing-hungry applications and products. Over the last five years, the number of developers with expertise in GPU has grown tenfold; CUDA downloads five times; and total GPU flops of the top 50 systems 15 times.\n\nSince 2009, NVIDIA has been hosting the annual GPU Technology Conference (GTC), which showcases company releases and provides an exhibition venue for GPU-based innovations. GTC attendance has soared from 2000 attendees in 2012 to the 8,500 developers, buyers and innovators who went to Santa Clara, California for GTC 2018 last month.\n\nThe role and deployment of GPUs is changing dramatically, and they are creating significant new market opportunities. Synced visited GTC 2018 to explore a world based on GPUs.\n\nTech giants usually have a capital investment arm \u2014 such as Google Ventures or Microsoft Ventures \u2014 to fund AI startups. NVIDIA GPU Ventures invests in and nurtures next-generation companies built on GPU. A number of NVIDIA portfolio companies have become high-profile AI startups: H2O.ai, Element.ai, and Drive.ai.\n\nAt GTC 2018, a large black semi truck parked outside the San Jose McEnery Convention Center became a centre of attention. The prototype was from TuSimple, a leading Chinese autonomous-driving truck company founded by entrepreneurs Mo Chen and Dr Xiaodi Hou from the California Institute of Technology.\n\nThe TuSimple semi operates on an accelerator fusion of NVIDIA GPUs \u2014 including GTX 1080Ti, NVIDIA Drive PX and Jetson TX \u2014 to process huge amounts amount of data.\n\nDrive PX is NVIDIA\u2019s first in-vehicle supercomputer, introduced in 2015. As computing needs for self-driving vehicles have ramped up, NVIDIA has developing powerful vehicle-specific processors to help the company stay ahead of the race. NVIDIA recently successively unveiled the advanced Drive PX Pegasus and what it billed as the world\u2019s most powerful in-vehicle System-on-Chip (SoC), Xavier.\n\nLast June, TuSimple completed a 200-mile high-automated test drive from San Diego to Yuma, Arizona. The company\u2019s fast tech development attracted interest from NVIDIA GPU Ventures, which joined a group of investors led by Chinese social media company Sina putting more than US$20 million into TuSimple last August.\n\nNVIDIA is not just writing checks. In June 2016, the company introduced a virtual incubator, NVIDIA Inception Program, to nurture AI startups. In 18 months over 2000 companies have applied to the program, and only 70 have so far been accepted. NVIDIA also hosts the Inception Competition at GTC, where its portfolio companies compete for NVIDIA Inception Awards and US$1.5 million in prize money.\n\nIsraeli cybersecurity startup Deep Instinct was one of the winners last year. The company uses a GPU-based neural network and CUDA to achieve 99 percent detection rates, compared with about 80 percent detection from conventional cybersecurity software. Last June, NVIDIA pumped US$10 million into Deep Instinct. \u201cNVIDIA introduced us to their strategic accounts which are now our customers, which has been very helpful,\u201d said a Deep Instinct representative at GTC 2018.\n\nOn this year\u2019s final pitch day, robotic arm company Kinema Systems won the NVIDIA Inception Award and took home US$375,000. The company\u2019s flagship product is the AI-powered industrial robotic vacuum grabber Kinema Pick, which runs on GTX 1060 and NVIDIA embedded AI computing device Jetson TX2.\n\nKinema Founder and CEO Sachin Chitta says NVIDIA provides portfolio companies with \u201cspecial treats,\u201d such as a discount on NVIDIA hardware, a training course conducted by NVIDIA experts, and not least a chance to appear at GTC: \u201cThis conference provides a great opportunity for exposure. We have met many customers and investors who showed a huge interest in our products.\u201d\n\nTech giants believe AI can reimagine conventional diagnostic methodologies in medical health, increasing accuracy and reducing costs. IBM has been using slides to train deep neural networks to detect tumours since 2016. Google has successfully produced a tumour probability prediction heat map algorithm whose localisation score reached 89 percent, significantly outperforming pathologists\u2019 average of 73 percent.\n\nThe medical health field was not widely addressed in the last few GTCs, but NVIDIA is now putting more effort into this area.\n\nAt GTC 2018 NVIDIA unveiled Project Clara \u2014 a medical imaging supercomputer deployed on its cloud platform. Clara is designed to transform standard medical images such as X-rays, ultrasound scans, CTs, MRIs, PETs, and mammograms into high-resolution cinematic renderings. Because deploying GPUs in every clinic and hospital would not be cost-effective, Clara provides its users with high-performance cloud-based services.\n\nNVIDIA added medical health talks and panels at GTC 2018, and invited renowned professionals to represent their research achievements based on GPUs.\n\nThomas Fuchs is the Founder and CEO of New York startup Paige.ai, founded this January to fight cancer with AI. The company has access to a dataset of 25 million pathology images and financial support from Breyer Capital, which led a US$25 million Series A Funding Round.\n\nIn an interview with Synced, Fuchs said he believed the time was right to build Paige.ai because the requirements were all in place: qualified devices, extensive collection of medical images, and full-fledged deep learning algorithms.\n\nMost importantly, GPU advancement drives deep learning development with an unprecedented scale of medical image data. Paige.ai has now built a high performance compute cluster with hundreds of NVIDIA GPUs.\n\nGPUs will not replace the central processing unit (CPU), which is still extremely powerful in performing the necessary arithmetic, logical, control and input/output (I/O) operations for example for personal computers. However, GPU are better than CPU in particular tasks: they are unquestionably the best chip for image processing in gaming, and their parallel architecture happens to be very well-suited for deep learning.\n\nA few innovative startups have discovered that GPU acceleration can also deliver better performance than CPU in databases, particularly in repetitive operations on large amounts of data.\n\nOnline Transaction Processing Databases (OLTP) primarily handle day-to-day transactions for example for banks. Oracle is the dominant vendor here, accounting for nearly 50 percent of the market share. Online Analytical Processing (OLAP) databases meanwhile are designed to handle complex analysis of large volumes of data, and consequently many AI applications are now running on OLAP databases. The estimated market size for OLAP databases is US$22.8 billion in 2020, a lucrative emerging market for startups to target.\n\nGPU database companies first appeared in 2016. Silicon Valley pioneers Kinetica and MapD have raised US$50 and US$37 million respectively; and Israel\u2019s SQream US$31.5 million. Last year, Chinese database leader Zilliz raised an undisclosed amount (reportedly RMB\uffe5100 million or US$16 million).\n\nSays Zilliz Founder and CEO Chao Xie, \u201cUsing a GPU to run a database can be traced back to 15 years ago in academia. But it didn\u2019t work as the top software has long been constrained by the underlying hardware. Now, NVIDIA and other chipmakers are building an infrastructure for GPUs, helping developers to reduce the threshold of developing GPU-based applications.\u201d\n\nZilliz\u2019s GTC 2018 booth showcased the company\u2019s cutting-edge GPU database, which it says can increase the performance of data processing up to 100 times over CPUs; reduce hardware cost by 10 times; and lower data centre operation and energy costs by up to 95 percent.\n\nThe SQream booth was a few meters from Zilliz\u2019s. Founded in 2010, SQream spent their six years conducting research and building databases, and launched its commercial product in 2016.\n\n\u201cIf you have tens of hundreds of terabytes [data], and you try to do some database operation on what we called Massive Parallel Processing (MPP) databases, sometimes the query goes up to 30 minutes to an hour. When you put data into the SQream instead of MPP, the query goes down to five minutes,\u201d says SQream Senior Solutions Architect Arnon Shimoni.\n\nAs AI applications thrive with complex neural networks and large datasets, researchers and developers can of course invest in GPU clusters \u2014 but another option is to purchase on-demand services on the cloud for GPU-intensive tasks.\n\nThis premise is growing the market for GPU-as-a-Service (GaaS) solutions, which is set to exceed US$5 billion by 2024, according to a new research report by Global Market Insights.\n\nMajor cloud service vendors AWS, Google Cloud, and Microsoft Azure have been hosting NVIDIA GPU-equipped virtual machines for their cloud machine learning services for some time. Google began incorporation of NVIDIA GPU in its cloud computing centres back in November 2016, just months after AWS announced a new Elastic Compute Cloud (EC2) instance type, dubbed P2, which leverages NVIDIA GPUs.\n\n\u201cGaaS will be used for augmented reality, but will also able to handle massively parallel complex app problems like encryption (or decryption), weather forecasting, business intelligence graphical displays, big data comparisons,\u201d writes Jack Gold, founder and principal analyst at analyst firm J. Gold Associates.\n\nAlthough a few cloud services giants will dominate the market for GaaS, small and medium enterprises still have a chance to take a piece of the pie.\n\nCirrascale Cloud Services is a San Diego-based company that enables researchers and data scientists to attach GPU acceleration to a wide range of tasks over the network. Although its service is similar to AWS or Google Cloud, the company appeals to users who train models for weeks or even months at a time by offering a 35 percent lower price point and 35 percent faster speed compared to AWS.\n\nSays Cirrascale \u200eExecutive Account Manager Andrew Kruszewski, \u201cWe realized that there was a market, people [researchers] would come and test on the system, and they wouldn\u2019t want to get off. They also didn\u2019t want to own the equipment because NVIDIA changes their GPU so often.\u201d\n\nLaunched just three years ago, Cirrascale\u2019s cloud services have been growing so quickly that the company had to cut other divisions. Last year, its design and manufacturing business was sold to BOXX technologies.\n\nOver the last two years, NVIDIA\u2019s stock price has skyrocketed thanks to the rapidly increasing role AI is playing in the company\u2019s revenue growth. Full-year revenue of 2017 was US$9.71 billion, up 41 percent from a year earlier, and its discrete GPU market share increased to 72.8% during the third quarter of 2017.\n\nHowever, rivals Google and Intel are catching up, and developing their own AI chips to challenge NVIDIA. This February, Google announced that its Tensor Processing Unit (TPU) \u2014 a custom chip that powers neural network computations \u2014 will be available in beta for researchers and developers on the Google Cloud Platform.\n\nLast year, Google boasted that its TPUs were 15 to 30 times faster than contemporary GPUs and CPUs in inferencing, and delivered a 30\u201380 times improvement in TOPS/Watt measure. In machine learning training, TPU are more powerful in performance (180 vs. 120 TFLOPS) and two times larger in memory capacity (64 GB vs. 32 GB of memory) than NVIDIA\u2019s top GPU Tesla V100.\n\nJoe Pelissier, Distinguished Engineer at Cisco Systems, says that a serious challenger may even replace NVIDIA in the next three to five years.\n\n\u201cThe type of mathematics for machine learning you basically need to be able to do is multiplication. Everything else is at least two orders of magnitude less significant. So you can imagine Silicon Valley, there are a lot of folks saying \u2018hey, if i take out a lot of functionality that GPU has, and only leave the stuff it needs for machine learning, I can either make it cheaper, or I can put more cores in it, or a combination of both\u2019,\u201d says Pelissier.\n\nMany experts believe that while NVIDIA GPU were not initially created for AI, they are now embedded in it and cannot be easily replaced.\n\nBrett Newman, VP marketing and customer engagement at compute hardware company Microway, says NVIDIA has done a good job building the ecosystem. \u201cThey are making software tools better applied for deep learning training. And they are making developer friendly things like Digits [Deep Learning GPU Training System]. I think that stuff is setting them up for success that will persist into the long-term.\u201d\n\nHuang quipped at GTC 2018 that \u201cNVIDIA is still a small company with only 10,000 employees.\u201d But he is too humble. It\u2019s no small achievement to have built NVIDIA from a birthday present into a US$150 billion chip giant in 25 years. And now, the company\u2019s GPU have become the muscle powering AI research and innovation. It\u2019s been an incredible journey for NVIDIA, one that will continue to empower AI long into the future."
    },
    {
        "url": "https://medium.com/syncedreview/lawyee-has-digitized-40-million-chinese-court-records-a53f02fa5061?source=user_profile---------43----------------",
        "title": "Lawyee Has Digitized 40 Million Chinese Court Records",
        "text": "Lawyee is a Peking University spinoff founded in 2003 that was one of the first companies specializing in building searchable legal databases and digitizing IT systems for Chinese courts and law firms.\n\nWhen Lawyee started the Internet\u2019s legal landscape was barren and computer technologies underdeveloped. Things fast-tracked however in 2014 when China\u2019s Supreme People\u2019s Court made publishing court decisions on an official online archivemandatory within seven days of entry into force, excluding politically-sensitive, private or youth crime cases.\n\n\u201cLawyee became the contractor for building this database,\u201d says company General Vice Manager Chen Hao, \u201cand we accumulated several million documents by the end of 2014.\u201d This number skyrocketed to 40 million in mid-2018, and page views exceeded 14 billion. Lawyee also runs a case law database which Chinese law schools can access for an annual subscription fee of few thousand US dollars.\n\nResponding to recent advances in AI, Chen explains that \u201cAI evolved out of decades of statistics research. Legal researchers have also used statistics for a long time, and there are many machine learning algorithms on SPSS Statistics. Since 2006 deep learning has upgraded machine perception.\u201d\n\n\u201cHowever there aren\u2019t many mature commercial applications at least in areas of law. For both common and continental legal systems, technology so far has only tackled vertical problems, but it\u2019s not enough to reinvent the legal industry. Now there are startups building applications on top of IBM Watson that may prove successful.\u201d\n\nBuilding on strong database resources, AI can generate legal documents and perform compliance review of contracts, quality control of documents, and analysis of legal risks, business guidelines, and so on.\n\nOne of Lawyee\u2019s current AI projects is identifying the core issues in a legal document. Lawyee\u2019s AI trained on more than 30 million data samples can currently identify core arguments in about 70 out of 100 documents, and be exactly on-target with 60.\n\nChen says the hard part is human-labelling larger quantities of data. \u201cWhen data samples rise from 100 to 10k, people can no longer handle the repetitive work, not to mention when data size goes up to 300k plus.\u201d\n\nAnother challenging task is finding benchmarks for algorithm performance, which is crucial for measuring accuracy. \u201cThere are no benchmark datasets such as ImageNet for image or SQuAD for NLP in the legal space. Labelling them is very expensive, thus impossible for normal companies. For accuracy, we proclaim that certain software can go up to 70\u201380 percent. Customers are willing to pay more if other options on the market don\u2019t work as well,\u201d says Chen.\n\nThe Chinese government directive mandating legal document digitalization has engendered a number of Beijng legaltech startups, most notably AI-powered legal consulting companies Lvpin Technology and Itslaw, along with natural language processing solution providers Fa\u2019gou\u2019gou and most recently deepcurious.ai."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-ministry-of-education-sinovation-and-peking-u-join-hands-to-train-ai-talents-f2d21ce3387e?source=user_profile---------44----------------",
        "title": "Chinese Ministry of Education, Sinovation, and Peking U Join Hands to Train AI Talents",
        "text": "In Beijing today, China\u2019s Ministry of Education, Sinovation AI Lab, and Peking University jointly announced the Global AI Talent Training Program for Chinese Universities. The program will include tutorships from Turing Award winner John E. Hopcroft, Deep Learning pioneer Geoffrey Hinton, and Sinovation CEO Kai-Fu Lee.\n\nThe program has its origins in a February meeting between Kai-Fu Lee, officials from China\u2019s Ministry of Education, and computer scientists from more than 50 Chinese universities.\n\nThere are two components, the DeeCamp Student Boot Camp and a Teacher Training Program. In the explorational phase, the program will select 100 teachers and 300 students from top CS-heavy Chinese high schools to test teaching methods. The bigger plan is to educate at least 500 teachers and 5,000 students over five years.\n\nIn addition, the Ministry of Education plans to set up the China-US University AI Talents Cooperation Training Program to provide Chinese students with AI scholarships for US colleges and universities.\n\nIn July 2017, the State Council issued the New Generation Artificial Intelligence Development Plan and listed \u201caccelerating the education of top-notch AI talents\u201d as a primary task, emphasizing the importance of \u201cimproving the artificial intelligence education system, strengthening talent reserve and echelon building, and occupying the AI highland in China.\u201d\n\nToday\u2019s announcement provides a framework with the international experience and expertise required to take on that task."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-mar-w-4-apr-w-1-caec29bd494?source=user_profile---------45----------------",
        "title": "AI Biweekly: 10 Bits from Mar W 4 \u2014 Apr W 1 \u2013 SyncedReview \u2013",
        "text": "Now social and behavioral scientists can use the TuringBox platform to study artificial intelligence algorithms. AI contributors can upload existing and novel algorithms for review, gaining a reputation in their community. AI examiners meanwhile develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.\n\nAmper Music is the world\u2019s first AI-powered music composer, enabling users to create and customize music. The company raised an additional US$4 million in seed round funding, which will help double the number of US Amper Music employees and help the company expand internationally.\n\nMarch 22th \u2014 Google Assistant Lets You Send and Receive Money on Phones\n\nGoogle Phone users can now send and receive money with Google Pay for free via Google Assistant. This feature is currently only available on new phones due to security concerns. It will be offered on Google Home smart speakers in the coming months.\n\nMarch 27th \u2014 Toshiba Plans to Transform Itself With AI and IoT\n\nToshiba introduces a high-level digital transformation strategy for its business, with a focus on both AI and IoT solution development. Toshiba believes the strategy will maximize customer value through its various ecosystem partners.\n\nWaymo partners with Jaguar to build a 20,000-electric-car fleet with fully autonomous driving capabilities in the next two years. The cars will have sufficient battery life to drive all day on one charge.\n\nMarch 27th \u2014 Nvidia Teams up with ARM to Develop Deep Learning for IoT Devices\n\nAt GTC 2018, NVIDIA and ARM announce a partnership to integrate open source NVIDIA Deep Learning Accelerator architecture into ARM\u2019s Project Trillium platform. This collaboration aims to simplify the product integration process for IoT chip companies.\n\nHitachi\u2019s new AI Technology service analyzes product, inventory, demand and sales information on warehouse management systems (WMS) and learns effective measures based on evaluation results. Hitachi says the service can boost product picking efficiency by 16 percent.\n\nMitsubishi Hitachi Power Systems (MHPS) announces new investments in innovative technologies with digital tools to deliver solutions to the power industry. MHPS plans to use AI technology, battery storage, and geothermal renewable power to achieve better energy efficiency.\n\nApple announces that its health record products are now being used by 40 health systems and 300 hospitals. It has traditionally been time-consuming for patients to obtain their health records, and Apple\u2019s health records products streamline the process. Apple is planning to make the products available to all iOS users.\n\nMicrosoft France pledges US$30 million over three years to further France\u2019s artificial intelligence development. The AI Impact group aims for AI with a positive impact on environment, transport, and health in France. The AI Skills program meanwhile will train 400,000 people over three years and aims to create 3,000 new jobs in the French digital ecosystem."
    },
    {
        "url": "https://medium.com/syncedreview/caffe2-merges-with-pytorch-a89c70ad9eb7?source=user_profile---------46----------------",
        "title": "Caffe2 Merges With PyTorch \u2013 SyncedReview \u2013",
        "text": "Facebook operates two flagship open source machine learning frameworks \u2014 Caffe2 and PyTorch. Their incompatibility, however, made it difficult to transform a PyTorch-defined model into Caffe2 or vice versa. Facebook is doing something about that.\n\nLast Friday the Caffe2 Github page introductory \u201creadme\u201d document was suddenly replaced with a bold link: \u201cSource code now lives in the PyTorch repository.\u201d What this meant was that Caffe2 users could now directly check Caffe2 code in PyTorch.\n\nFacebook AI Researcher and Caffe creator Yangqing Jia says Facebook decided to merge Caffe2 into PyTorch \u201cbecause this will incur minimal overhead for the Python user community.\u201d\n\nNothing changes for a PyTorch user, says Facebook AI Researcher Soumith Chintala. \u201cPyTorch is installed, shipped and used exactly how it is done today. Your code will not break. This is development and backend work. If you are not a core-developer, this issue is not even that relevant to you.\u201d\n\nAlthough most frameworks are similar to certain degrees, each has its unique characteristics. Sharing repository and development infrastructure between different machine learning frameworks can compensate for each one\u2019s shortcomings.\n\nSince its release in October 2016, PyTorch has become a preferred machine learning framework for many AI researchers due to its research flexibility. Over half of Facebook AI projects run on PyTorch. Meanwhile, Caffe 2, launched in April 2017, is more developer-friendly than PyTorch for AI model deployment on IOs, Android and Raspberry Pi devices.\n\nLast September, Facebook and Microsoft announced their Open Neural Network Exchange (ONNX), an open source project that helps researchers to convert models between frameworks. The merging of Caffe2 and PyTorch is a logical next step in this strategy.\n\nThe merging also ups the stakes in Facebook\u2019s challenge to the dominant machine learning framework, TensorFlow.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinese-video-surveillance-giant-hikvision-to-opensource-its-ai-technology-7e68412a14ec?source=user_profile---------47----------------",
        "title": "Chinese Video Surveillance Giant Hikvision to Opensource its AI Technology",
        "text": "At 2018 AI Cloud Ecological International Summit in Hangzhou on Mar. 28, global video surveillance manufacturing leader Hikvision announced it would open access to its AI technology, notably AI Cloud.\n\nHikvision AI Cloud is a distributed structure incorporating cloud computing and edge computing. Launched last year, it can extend an AI algorithm from the cloud centre to an edge network of on-premises video recorders and servers, and further to edge devices, such as security cameras.\n\nHikvision has deployed AI Cloud in more than 30 Chinese provinces, providing AI-empowered solutions for emergency management, maintenance, urban operation, traffic management, business intelligence, etc. Hikvision\u2019s facial recognition system at an intersection in Suqian, Jiangsu, decreased red-light-running violations by over 90 percent.\n\nHikvision CEO Hu Yangzhong says the company will open an AI development platform for application developers. At the same time, AI Cloud will also merge algorithms from other AI companies.\n\n\u201cThe industry should jointly promote the development and application of AI in the security camera industry,\u201d says Hu.\n\nHikvision will also launch an open training system providing transfer learning and augmented learning capabilities; AI services on EZVIZ, its video-service application designed for consumer markets; and data labelling and sharing services.\n\nFounded in 2001, Hikvision is dedicated to improving video surveillance and video analysis technology, and providing surveillance products and solutions. The company accounted for 21.4 percent of the global market in CCTV and video surveillance in 2016, and now leads the security surveillance market with an estimated value of US$63 billion.\n\nHikvision\u2019s surveillance system has integrated AI chips with frontend cameras, and developed data analysis systems on the backend. As a result, it is an essential supplier for China\u2019s Skynet, a real-time surveillance program for public security. Last year BBC reporter John Sudworth agreed to be tracked by the system, which required just seven minutes to locate and \u201capprehend\u201d him.\n\nNot to be outdone by Hikvision, China\u2019s leading tech companies are also building open-sourced AI platforms and sharing data access, tools, and backend codes with developers. Baidu last year opened its smart assistant platform DuerOS and autonomous driving platform Apollo. By January 2018, DuerOS had activated more than 50 million smart devices, with over 10 million active devices per month.\n\nIFlytek, a leading Chinese AI company known for its voice technology, plotted a bold course last year: \u201cProject 1024\u201d will package CN\uffe51.024 billion into a developer fund, build 1024 professional teams, and support 1024 AI projects."
    },
    {
        "url": "https://medium.com/syncedreview/the-new-age-of-discovery-space-exploration-and-machine-learning-64883f7dc7f9?source=user_profile---------48----------------",
        "title": "The New Age of Discovery: Space Exploration and Machine Learning",
        "text": "The Age of Discovery began in the 15th century, when Europeans built their first oceangoing vessels and set out to explore the world. Whether motivated by political, economic or cultural factors, human exploration has traditionally been driven by technological progress.\n\nRocket booster technology developed during World War II enabled the first generation of spaceflight in the mid 20th-century, when the Soviet Union and the United States launched artificial satellites and interplanetary probes. As humans step up to deep space exploration, artificial intelligence technologies are expected to play a huge role.\n\nThe \u201clearning\u201d part of machine learning refers to an algorithm\u2019s ability to find patterns in data to self-improve the machine\u2019s outcomes, ie to use existing data to predict unknowns. Machine learning already has applications in banking, healthcare, aviation, and so on, and the technology is expected to power future space exploration as it can handle huge data volumes, find patterns in planet image datasets, and predict spaceship condition.\n\nThe role of machine learning in space exploration can be roughly divided into data transmission, visual data analytics, navigation, and rocket landing.\n\nSpacecraft and satellites operating in deep space can generate huge amounts of data due to the complexity of their research missions. Because of the different rotations and orbits of their host planets, these massive data packets must be transmitted to earth during specific windows of opportunity. The lag meanwhile will depend on Earth\u2019s light year distance from the spacecraft\u2019s host planet and may be months or even years. Moreover if a data packet transmission is unsuccessful, the data may be permanently lost if it was overwritten with new data in the onboard memory.\n\nMachine learning enables a \u201csmart\u201d method to manage the distant planet to Earth data transmission problem. The outer space machine learning application MEXAR2 (\u2018Mars Express AI Tool) was introduced in 2005 at Italy\u2019s Institute for Cognitive Science and Technology (ISTC-CNR). The onboard learning algorithm can leverage historical data to remove superfluous data and pinpoint the download schedule to optimize data packet transmission. This outer data transmission technique is already being used by NASA and others in their space research programs.\n\nA usual early step in deep space exploration is planet condition and environment analysis. Satellites and space telescopes have already collected a large amount data for example for target planet Mars. Images are the major data source, while the major challenge is how to identify and read the right information from the images. Machine learning has become an effective technique for solving this problem.\n\nThe NASA Frontier Development Lab and top-tier technologies companies such as IBM and Microsoft are collaborating on machine learning as a solution for solar storm damage detection, targeting a target planet\u2019s \u2018space weather\u2019 through magnetosphere and atmosphere measurement. The technique can also be used for resource discovery and to identify suitable planet landing sites.\n\nAnother field where machine learning can improve current technology is in relative spacecraft and satellite motion control. Each control action selected for spaceships or satellites requires considering and processing geometric and kinematical location information in an extremely short timeframe. As outer space missions become increasingly frequent and complex and spacecraft get further from Earth, there will be growing demand for fast and self-adjusting machine-learning based navigation capabilities. The field could include orbit adjustment, autonomous navigation, and space station docking.\n\nThe NASA Jet Propulsion Laboratory (JPL) is already involved in the above research field, and machine learning has emerged as a key technique for measurement and adjustment of a spacecraft\u2019s motion with different orbital parameters. This allows the spacecraft to self-adjust for example orbit and velocity, and can support ground navigation systems to control a spacecraft\u2019s flight path, engine power and orbital position. A spacecraft\u2019s onboard machine learning algorithm also has the potential to perform autonomous navigation in deep space.\n\nRecent research in landing spacecraft has focused on developing algorithms that increase the level of autonomy for air and space systems. Some of the major issues for spaceship or rocket landings include vacuum stage, software errors, guidance and sensor problems etc. Machine learning and computer vision are the core optimization and evaluation techniques for successful landings.\n\nThe SpaceX Falcon 9\u2019s successful landing at Cape Canaveral Air Force Station in 2015 demonstrated machine learning and computer vision\u2019s power to transform space exploration. SpaceX used a convex optimization algorithm to determine the best way to land the rocket, with real-time computer vision data aiding route prediction. These advanced machine learning applications enabled the first reusable rocket in space exploration history \u2014 a feat scientists regard as essential in developing deep space exploration.\n\nDespite the challenges, machine learning will, or must, play a vital role in the coming age of space exploration.\n\nWith the help of advanced machine learning based terrain classifiers and path planning algorithms, NASA built a Mars Rover which can navigate long distances on the planet\u2019s complex surface. Mars may be humans\u2019 current target but the red planet will not be our final destination. There are reports that NASA will deploy robotic machine learning based probes to Jupiter\u2019s moon Europa to search for life, and NASA engineer Hiro Ono says autonomous spacecraft are in the design phase.\n\nSoon, spacecraft may operate using only artificial intelligence and machine learning algorithms. As in the past, it is technological innovations that will enable humans to go \u201cwhere no man has gone before.\u201d For the immediate future, those innovations will continue to emerge from machine learning."
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-ceo-says-fgpa-is-not-the-right-answer-for-accelerating-ai-83c810969edd?source=user_profile---------49----------------",
        "title": "NVIDIA CEO Says \u201cFGPA is Not the Right Answer\u201d for Accelerating AI",
        "text": "Accelerating resource-hungry AI applications demands chip performance beyond what mere CPU or GPU can deliver, prompting researchers to turn to sophisticated Application-specific Integrated Circuits (ASIC) and Field Programmable Gate Arrays (FPGA). Chip giant NVIDIA Founder and CEO Jensen Huang created a bit of a stir at yesterday\u2019s GPU Technology Conference in Santa Clara, USA, when he appeared to dis one of these chips\u2019 appropriateness for autonomous vehicle system development: \u201cFPGA is not the right answer,\u201d he said.\n\n\u201cFPGA is really for prototyping. If you want the [self-driving] car to be perfect, I would build myself an ASIC because self-driving cars deserve it,\u201d says Huang.\n\nFPGAs are logic chips best known for their programmability, which gives engineers the flexibility to configure an FPGA for example as a micro-control unit today, and use the same FPGA as an audio codec tomorrow. ASICs meanwhile are custom chips with little or limited programmability. Because FPGAs are more versatile, chip makers can streamline their operations by developing FPGAs rather than ASICs. However FPGAs are both more expensive and less powerful than ASICs.\n\n\u201cWhen you want to build something for cars, you should have a very large concentrated group of expert engineers design the chip one time and sell it to everyone, instead of a hundred random groups of different levels of capability and expertise build their own chips,\u201d says Huang.\n\nNVIDIA has never been impressed with FPGA. Chief Scientist Bill Dally once said \u201cif you want to solve a problem and you are willing to devote a lot of engineering time, just develop the ASIC directly. I don\u2019t think the FPGA is competitive.\u201d\n\nNVIDIA has been developing ASIC for years and has traditionally kept their tech under wraps. At last year\u2019s GTC however the company decided to share the architecture of their Deep Learning Accelerator (DLA) \u2014 an ASIC for deep learning inferencing \u2014 on the open-source codebase Github.\n\nNVIDIA recently announced an agreement with British chip IP company Arm to integrate DLA architecture into Arm\u2019s new Project Trillium platform, which hastens the development of AI inferencing accelerators. As 90 percent of AI-enabled devices shipped today are based on architecture developed by Arm, NVIDIA\u2019s DLA is expected to be deployed on billions of mobile, consumer electronics, and the Internet of Things (IoT) devices.\n\nHowever even as NVIDIA snubs FPGA, rivals like Intel are ramping up efforts to develop and deploy them. In 2015 Intel acquired top US manufacturer of programmable logic devices Altera in an all-cash transaction estimated at US$16.7 billion. Intel has since developed a CPU+FPGA hybrid chip for deep learning inference on the cloud.\n\nIntel also introduced its Movidius Myriad X Vision Processing Unit (VPU), a system-on-chip (SoC) used for vision devices such as smart cameras, augmented reality headsets and drones. The Myriad X is shipped with a dedicated Neural Compute Engine (NCE) for running deep neural networks at high speed and low power in real time at the edge. With NCE, the Myriad X can reach one trillion operations per second in deep learning inferencing.\n\nMeanwhile, the world\u2019s leading supplier of programmable logic devices Xilinx is competing with Intel\u2019s Altera in the FPGA market. While Intel dominates the server chip market, Xilinx has the technology lead, helping the company win orders from large cloud customers.\n\nChinese startup DeepPhi last year garnered US$40 million in funding \u2014 led by Xilinx \u2014 to develop its Deep-Learning Processing Units (DPU), which include both FPGA chips and ASIC chips."
    },
    {
        "url": "https://medium.com/syncedreview/france-pumps-1-5-billion-into-ai-in-bid-to-catch-up-2470a7fc132b?source=user_profile---------50----------------",
        "title": "France Pumps \u20ac1.5 Billion into AI in Bid to Catch Up",
        "text": "France is pledging \u20ac1.5 billion to hasten the development of its fledgling AI ecosystem. French President Emmanuel Macron made the commitment this morning at the Artificial Intelligence Summit held at the College de France Research Center. The event included industry discussions featuring top-notch researchers and high-level ministers such as Secretary of State for Digital Mounir Mahjoubi. France also announced partnerships with DeepMind, Samsung, and Fujitsu as part of a grand strategy to build Paris into a global AI hub.\n\nA Summit highlight was the release of the 152-page report \u201cAI for Humanity,\u201d written by President Macron\u2019s star technology advisor, Fields Medal-winning mathematician C\u00e9dric Villani.\n\nVillani accepted government appointment just six months ago, and has quickly pulled together \u201cMission Villani,\u201d composed of machine learning researchers and members of Europe\u2019s Digital Advisory Council. The team interviewed 350 industry leaders to help form \u201cune strat\u00e9gie national port\u00e9e par le plus hautes autorit\u00e9s et la d\u00e9cliner en feuille de route concr\u00e8te\u201d \u2014 \u201ca national strategy carried by the highest authorities and transformed into a concrete roadmap.\u201d\n\nThe strategy promises to comply with European Union\u2019s data privacy policies, namely the General Data Protection Regulation (GDPR), which will take effect this May.\n\nBack in 2017, the French government released the 200-page document France Intelligence Artificelle, detailing over 50 policy proposals and placing the number of French AI startups at over 270.\n\nIn the global AI competition, France trails neighbours Germany and UK, and all lag far behind leaders USA and China. President Macron incorporated the nation\u2019s innovation challenge his election campaign, calling on France to become a pro-Europe \u201cstartup nation.\u201d France has clustered talents from EU\u2019s robotics industries, the Human Brain Project, and FET projects, and accelerated industry development with initiatives like La French Tech.\n\nGoogle has already helped over 230,000 French students improve their digital skills and is building four \u201cLes Ateliers Num\u00e9riques\u201d Google Hubs to provide free digital training to the French public. The company is adding 1,000 employees to its sprawling Paris office.\n\nGoogle\u2019s AI research subsidiary, London-based DeepMind said it will open a Paris lab with 15 researchers led by vernacular AI scientist Remis Munos.\n\nFacebook says it will put \u20ac10 million into its French research center over the next five years and double the number of AI research scientists to 100 by 2022. This is Facebook\u2019s biggest investment in France since the Station F Startup Campus in Paris.\n\nKorean consumer electronics giant Samsung will open its third-biggest AI R&D center in Paris. Led by former Apple Siri Chief Luc Julia, the center will house 100 researchers. Corporate President and CSO Young Sohn said today that Samsung sees France\u2019s strong competitive edge in mathematics and physics as a fertile environment for developing AI talents.\n\nJapan\u2019s Fujitsu is intensifying the current expansion of its Paris AI center, pledging US$61 million over the next five years, while Microsoft is spending US$30 million to open an AI school in France.\n\nPresident Macron sent out 32 tweets from the event under the hashtag #AIforhumanity and #ChooseFrance."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-to-connect-10-billion-devices-in-5-years-d2b2d6a678be?source=user_profile---------51----------------",
        "title": "Alibaba to Connect 10 Billion Devices in 5 Years \u2013 SyncedReview \u2013",
        "text": "In a world of increasingly connected humans, the new frontier is devices and the Internet of Things (IoT). Alibaba Cloud wants to secure its position in this rapidly growing market by connecting ten billion devices over the next five years.\n\nPresident of Alibaba Cloud Computing and Corporate Senior Vice-President Hu Xiaoming announced the ambitious plan today at the Computing Conference Summit in Shenzhen, where he compared AI to the human brain, cloud computing to the heart, and the IoT to the sensorium.\n\n\u201cThe Internet\u2019s first phase was digitizing human activities such as shopping, social, and entertainment. This formed the booming market of four billion Internet users,\u201d explains Hu. \u201cThe second half is digitizing the physical world of cars, forests, rivers, factories\u2026 Even trash cans are being connected to the internet! And this will be a profound technological change, a new productivity revolution.\u201d\n\nMarket research firm IDC predicts that by 2020 there will be 50 billion networked terminal devices in the world, storing about 50 percent of all data. Alibaba has been vying for the IoT market since all the way back in 2014 when it formed its Smart Living Group. In 2017 the company fast-tracked IoT development with a dedicated IoT Group, partnering with 200 companies to develop the industry standard ICA Alliance.\n\nIn 2017 August development of the first IoT-dedicated town, Hong\u2019Shan Township, began in partnership with the Wuxi Gaoxin District Government. The town has sensors tracking fire hazards and water pipe leakage, a system that auto adjusts and saves energy on street lamps, and advertising billboards that can flash warnings in case of emergency.\n\nAlibaba Cloud has also signed contracts with Suzhou City on an \u201ceconomic brain\u201d project, Chongqing city on public transportation projects, Suzhou and Shanghai for real-estate management.\n\nAnother component of Alibaba\u2019s IoT strategy are Internet cars powered by its AliOS operating system. There are 500,000 SAIC Motor, Dongfeng Peugeot-Citro\u00ebn, and Ford automobiles on the road connected with AliOS.\n\nAlibaba Cloud IoT says it will focus on providing an open, convenient IoT platform augmented with strong AI capabilities, while at the same streamlining the collaborative computing of cloud, edge, and end devices."
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-unveils-ai-computing-beasts-worlds-largest-gpu-two-petaflop-supercomputer-65dca0cbaf59?source=user_profile---------52----------------",
        "title": "NVIDIA Unveils AI Computing Beasts: World\u2019s Largest GPU & Two Petaflop Supercomputer",
        "text": "Five years ago a University of Toronto team led by Dr. Geoffrey Hinton used two GPUs to train the image recognition model AlexNet in a record time of six days \u2014 and GPUs have been powering AI research ever since. However, as researchers take on more challenging tasks, they need more compute power. NVIDIA Founder and CEO Jensen Huang believes he has the answer: \u201cThe world needs a gigantic GPU.\u201d\n\n \n\n At the GPU Technology Conference in Santa Clara, USA today, Huang unveiled the world\u2019s largest GPU \u2014 a binary beast packed with 16 Tesla V100 with doubled memory 32 GB, 81920 CUDA Cores, 2,000 TFLOPS Tensor Cores, and a bandwidth of 300 GB/Seconds between each GPUs.\n\n \n\n It\u2019s like looking under the hood of a muscle car. And it\u2019s an AI researcher\u2019s dream.\n\n \n\n Incorporating 16 GPUs in a single machine raised huge technical challenges in GPU interconnectivity. NVIDIA developed a new GPU interconnect fabric, NVSwitch, an upgrade on NVIDIA NVLink that delivers bandwidth five times higher than the best PCIe switch, enabling systems with higher GPU hyperconnectivity.\n\n \n\n NVIDIA announced that its gigantic GPU is now integrated into the DGX-2, the company\u2019s latest supercomputer for offices and data centers. DGX-2 is the world\u2019s first system to deliver performance of two PFLOPS, has 512GB HBM2 of memory, energy consumption of 10,000 watts, and 1.5TB system memory. The DGX-2 can train AlexNet in just 18 minutes, 500 times faster than the Hinton team in 2012.\n\n \n\n The DGX-2 is aimed at general academic institutions or established enterprises who demand substantial computing in AI research. It will go on sale in quarter three for US$399,000. Huang joked in his keynote speech: \u201cThe more GPUs you buy, the more money you save!\u201d\n\n \n\n The release of new GPU and DGX-2 is expected to consolidate NVIDIA\u2019s data center business, which doubled to US$2 billion in annual revenue in 2017 to become the company\u2019s second largest revenue source. Last December, NVIDIA controversially prohibited the deployment of its consumer-side GPU GeForce series in data centers. This was believed to be a measure to defend the company\u2019s own data center business. \n\n \n\n Also announced today was Clara, a datacenter medical imaging supercomputer for researchers to train models on reconstructing 3D images, detecting brain tumors, and cinematic rendering.\n\n \n\n NVIDIA enhanced its cloud platform NVIDIA GPU Cloud with the release of TensorRT 4.0, the company\u2019s latest high-performance deep learning inference optimizer. TensorRT 4.0 can accelerate AI applications, such as image recognition, speech synthesis, and natural language processing, and reduce data center power consumption by 70%. It incorporates with today\u2019s most widely used AI open source framework, Google TensorFlow 1.7.\n\n \n\n The NVIDIA GPU Cloud also added Kubernetes, a portable, extensible open-source platform for managing containerized workloads and services. Launched by Google in 2014, Kubernetes can help the NVIDIA GPU Cloud manage computing resources, particularly data centers on the cloud, in a cluster orchestration. This enables portability across infrastructure providers.\n\n \n\n Amazon Web Services, Google Cloud Platform, AliCloud, and Oracle Cloud users can access the NVIDIA GPU Cloud. \n\n \n\n NVIDIA is sending a message to the AI community: Its \u201cgigantic GPU\u201d will save researchers time training AI models so they can put more time into AI innovation. While Intel and Google have been catching up in the AI computing market in recent years, NVIDIA\u2019s new product announcements are expected to ramp up its market share in the critical data center business and dramatically expand its influence on the cloud."
    },
    {
        "url": "https://medium.com/syncedreview/the-yolov3-object-detection-network-is-fast-fcceae0ab650?source=user_profile---------53----------------",
        "title": "The YOLOv3 Object Detection Network Is Fast! \u2013 SyncedReview \u2013",
        "text": "YOLO creators Joseph Redmon and Ali Farhadi from the University of Washington on March 25 released YOLOv3, an upgraded version of their fast object detection network, now available on Github.\n\nAt 320 x 320, YOLOv3 runs in 22 ms at 28.2 mAP, as accurate but three times faster than SSD. It also runs almost four times faster than RetinaNet, achieving 57.9 AP50 in 51 ms on a Pascal Titan X.\n\nThe first generation of YOLO was published on arXiv in June 2015. The model framed objects separated by bounding boxes and associated class probabilities to treat them as a regression problem. A base YOLO model could detect images in real-time at 45 frames per second, while Fast YOLO was capable of processing 155 frames per second, while still outperforming other real-time detectors.\n\nIn 2016 Redmon and Farhadi developed YOLO9000, which could detect up to 9,000 object categories using the improved YOLOv2 model. At 67 frames per second, the detector scored 76.8 mAP on the visual object classes challenge VOOC 2007, beating methods such as Faster RCNN. The model was also trained to detect unlabelled objects.\n\nThe new YOLOv3 follows on YOLO9000\u2019s methodology and predicts bounding boxes using dimension clusters as anchor boxes. It then guesses an objectness score for each bounding box using logistic regression. The model next predicts boxes at three different scales, extracting features from these scales using a similar concept to feature pyramid networks. Redmon uses a hybrid approach to perform feature extraction, building on former YOLOv2, Darknet-19 and residual networks. The new network, Darketnet-53, is significantly larger and has 53 convolutional layers.\n\nWhen the duo ran YOLOv3 on Microsoft\u2019s COCO Dataset it performed on par with RetinaNet and SSD variants, indicating the model\u2019s strength at fitting boxes to objects. However when the IOU threshold raises the model struggles to align boxes perfectly with objects. Redmon and Farhadi say the model does not work well on average AP between 0.5 and 0.95 IOU metric, but performs very well on a threshold metric of 0.5 IOU. It also performs better with small objects than with large objects.\n\nOn a side note, it\u2019s worth mentioning that Redmon and Farhadi\u2019s paper is not only a step forward in object detection, it\u2019s also peppered with humour. Andrej Karpathy retweeted that the paper \u201creads like good stand up comedy.\u201d\n\nAli Farhadi is the Associate Professor of Computer Science and Engineering at the University of Washington. He also leads Project Plato \u2014 which uses computer vision to extracting visual knowledge \u2014 at the Allen Institute of Artificial Intelligence. His student Joseph Redmon is the YOLO paper\u2019s first author. Redman\u2019s personal website is called Survival Strategies for the Robot Rebellion."
    },
    {
        "url": "https://medium.com/syncedreview/crown-prince-sheikh-hamdan-launches-new-round-of-ai-programmes-in-dubai-967c41e36dae?source=user_profile---------54----------------",
        "title": "Crown Prince Sheikh Hamdan Launches New Round of AI Programmes in Dubai",
        "text": "Crown Prince and Chairman of the Board of Trustees of the Dubai Future Foundation Sheikh Hamdan officially opened the 4th edition of the Dubai Future Accelerator (DFA) on March 24th at Dubai\u2019s Emirates Towers. The program aims to boost tech development across the United Arab Emirates.\n\nIn 2016, Hamdan set a goal of achieving 25% autonomous transportation in the UAE by 2030, positioning the country of nine million as a regional reader in AI. This year\u2019s theme \u2014 Take Part in Creating the Future \u2014 matches 12 government entities with private sector partners. The teams will collaborate for nine weeks to solve public challenges using frontier technologies.\n\nThe DFA\u2019s AI deployment is both varied and wide. The Dubai Police will use statistical AI to support their decision-making processes, with a goal of cutting the crime rate by 25 percent by 2021. The Dubai Municipality will use AI and blockchain solutions to improve public infrastructure in areas of pest control, food safety, and health. Dubai\u2019s Department of Economic Development will use AI in optimizing control and inspection produces.\n\nEtisalat Telecommunications will adopt AR and AI to streamline operations and customer support, with the target of 90 percent service automation by 2021. Competitor Du telecommunications meanwhile believes AI and machine learning can upgrade its corporate support services by 30 percent.\n\nOther government departments such as the Dubai Electricity and Water Authority, Smart Dubai, the General Directorate of Residency and Foreigners Affairs, and the Knowledge and Human Development authorities will target challenges in urban environment, education, IoT operations, solar energy, and various explorational smart systems.\n\nThe UAE will host the Middle East\u2019s biggest AI fair this year. \u201cWorld AI Show\u201d will run April 11\u201312 in Dubai before moving to Singapore, Mumbai, and Paris. The AI market in the United Arab Emirates is expected to reach $50 billion by 2025.\n\nLast year, 27-year-old Emirati Omar bin Sultan Al-Olama was named the world\u2019s first-ever Minister of Artificial Intelligence. Al-Olama\u2019s previous contributions to his nation\u2019s top-level strategies include UAE Centennial 2071 and the UAE Strategy for the Fourth Industrial Revolution. In a CNBC interview Al Olama said \u201cIn ten years we will be the capital of AI in service and government. I also think we will be a hub for AI in the region.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/tracking-the-transforming-ai-chip-market-bac117359459?source=user_profile---------55----------------",
        "title": "Tracking the Transforming AI Chip Market \u2013 SyncedReview \u2013",
        "text": "Embedded AI can transform a tabletop speaker into a personal assistant; give a robot brains and dexterity; and turn a smartphone into a smart camera, music player, or game console. Traditional processors, however, lack the computational power to support many of these intelligent features. Chipmakers, startups, and capital are taking this opportunity to the market.\n\nAccording to a Gartner report, the chip market\u2019s total revenue hit US$400 billion in 2017, and the figure is expected to exceed US$459 billion in 2018. Traditional chip makers are putting an increasing focus on AI chip development, venture capital is pumping significant investments into the market, and AI chip startups are emerging.\n\nCPU (Central Processing Units) are a chip designed for general computing purpose, emphasizing calculation and logic control functions. They are strong in processing single complex computing sequential tasks, but poor in large-scale data computation.[2]\n\nGPU (Graphics Processing Units) were originally designed for image processing but have been successfully adopted for AI. A GPU contains thousands of cores and is capable of processing thousands of threads simultaneously. This parallel computing design makes GPU extremely powerful in large-scale data computation.[3]\n\nFPGA (Field Programmable Gate Arrays) are programmable logic chips. This type of processor is powerful in processing small-scale but intensive data access. In addition, FPGA chips allows users to program the circuit path through its tiny logic block, to handle any kind of digital function.[2] [4]\n\nASIC (Application-Specific Integrated Circuit) are highly customized chips tailored to provide superior performance in specific applications. However, a customized ASIC is not alterable once put into production.[5]\n\nOthers chip types such as Neuromorphic Processing Units (NPU) \u2014 which have architecture mimicking that of the human brain \u2014 have the potential to become mainstream in the future but are still at early stages of development.\n\nAI Chips, also known as AI accelerators, are processors for AI-related computing tasks. Machine learning technology places great demands on computing power for training algorithms and running applications, which traditional computing hardware cannot provide. As a result the demand for specialized AI chips is growing rapidly. [6] AI Chips can be divided into three major application areas: training, inference on the cloud, and inference on edge devices.\n\nTraining is a process wherein algorithms analyze data, learn from it and finally obtain the intelligence to respond to real-world events. Trillions of data samples are analyzed by the algorithm during this training process. Chip makers must not only ramp up processor performance, but also provide an entire ecosystem \u2014 including hardware, framework and other supportive tools \u2014 to enable developers to shorten their AI technology development processes [6]. Given these challenges, it\u2019s major companies like NVIDIA and Google who are thriving in this space.\n\nNVIDIA is the leader in training. When developers discovered GPU\u2019s parallel computing architecture could accelerate the deep learning training process this brought a significant advantage to GPU giant NVIDIA. By seizing the opportunity, NVIDIA transformed itself into an AI computing company and developed a new GPU architecture, Volta, which emphasizes deep learning acceleration. NVIDIA \u2018s GPU have been widely adopted for training machine learning algorithms, and the company now holds a virtual monopoly in the hardware training market.\n\nGoogle is another big player in this market. Based on the achievements of AlphaGo and the millions of users on its cloud service, Google has strong potential in the training market. The company has developed its own TPU (Tensor Processing Units) to compete with NVIDIA. TPUs are a type of ASIC designed exclusively for deep learning and Google\u2019s TensorFlow framework. Google says its TPU can provide 180 teraflops of floating-point performance, which is six times better than NVIDIA\u2019s latest data center GPU Tesla V100. [7] [8]\n\nA developed machine learning model for AI application areas such as image recognition or machine translation usually comes with high complexity, and the required inference is too compute-intensive to be deployed on edge devices. Therefore, inference on cloud becomes necessary for the deployment of many AI applications. And when an app is being used by thousands of people simultaneously, the cloud server also requires robust capability to meet inference demands. In such cases, FPGAs are the top choice for cloud companies. [9] This type of processor is good at low-latency streaming and computing-intensive tasks. In addition, FPGAs provide a flexibility which allows cloud companies to modify the chips. Traditional chip makers, cloud service providers, and startups are all developing FPGA solutions.\n\nIntel is one of the major players developing heterogeneous computing technology. By acquiring chip maker Altera, Intel boosted its FPGA technology expertise and developed a CPU+FPGA hybrid chip for deep learning inference on cloud. By utilizing the advantages of both processor types, this hybrid chip provides computing power, high memory bandwidth, and low latency. This technology has been adopted by Microsoft to accelerate its Azure Cloud Service.\n\nChinese tech giant Tencent is an example of cloud service providers developing FPGA solutions to support inference on cloud. Tencent developed China\u2019s first \u201cFPGA Cloud Computing\u201d service for its cloud service Cloud Virtual Machine. Compared to a CPU-based cloud server, the FPGA integrated CVM provides better computing power to support HPC application and deep learning development. [6] Accessing FPGA on cloud also eliminates the need to purchase hardware, reducing the cost of developing AI application. Tencent also supports third-party AI application development for commercial use.\n\nDeePhi Tech is a startup focused on inference on cloud. The company garnered US$40 million in funding to develop its DPU (Deep-Learning Processing Units, an FPGA based ASIC) platform. With the DNNDK (Deep Neural Network Development Kit), DeePhi Tech aims to provide a one-stop service for development and deployment of deep learning technologies. DeePhi co-founder Dr. Song Hang is a respected AI researcher who proposed a methodology called \u201cDeep Compression\u201d to reduce model scale, workload and power consumption in order to improve deep learning efficiency. [10] This methodology has been adopted by chip giants such as Intel and NVIDIA.\n\nInternet connections may not always be stable, and the cloud cannot accommodate all computing loads for AI innovations. Therefore future edge devices will require more independence in their inference features. Smartphones, drones, robots, VR and AR immersive experience devices, self-driving cars and so on all require specific AI hardware support. Moreover, breakthroughs in recent years have reduced chip volume, enabling embedding on almost any device, making inference on edge more viable.[11] To meet the demand for different devices, numerous startups are producing their own ASIC. Large chip makers are also adding AI supportive features to their processors.\n\nLeading Chinese phone and processor producer Huawei is boosting the performance of their SoC by integrating AI chips. In collaboration with chip startup Cambricon, Huawei adopted a NPU (Neural Processing Unit, a type of ASIC from Cambricon) to advance its SoC Kirin 970 for its flagship smartphone Mate 10. [12] This integration enhances the the phone\u2019s camera\u2019s image processing features.\n\nChinese startup WestWell Lab\u2019s DeepSouth neural processors are ASIC which simulate human brain neurons. The company created a DeepSouth-based brain simulator that can be used to accelerate medical devices supporting research in Parkinson\u2019s, Alzheimer\u2019s, and neural impairment.\n\nHorizon Robotics is another startup concentrating on embedded artificial intelligence. The company has developed two types of ASIC to support different AI applications. Sunrise series processors are for face recognition and video analytics solutions in smart cameras. Journey series processors are for self-driving cars, and provide real-time detection and recognition processing capacity in eight categories.[13]\n\nAI is far from maturity, and as the AI innovation ecosystem continues to develop the chip market will fluctuate. With the possibility of new frameworks emerging for algorithm development, current leaders in the training hardware market may face new competition. The inference on the cloud market is also still growing, and competition between cloud service providers will intensify as more AI applications are developed. The inference on edge market meanwhile is an arena with both big companies and startups.\n\nGiven the ever-increasing demands of AI applications, we can expect to see more collaborations between chipmakers and developers. Artificial intelligence has already had a significant impact on the chip market, a trend that will continue into the foreseeable future.\n\n[1] Chip market to top $400 billion in 2017, says Gartner: http://www.eenewseurope.com/news/chip-market-top-400-billion-2017-says-gartner-0\n\n[2] \u8be6\u7ec6\u5206\u6790\u4eba\u5de5\u667a\u80fd\u82af\u7247 CPU/GPU/FPGA\u6709\u4f55\u5dee\u5f02?: http://www.sohu.com/a/131606094_470053\n\n[3] What\u2019s the Difference Between a CPU and a GPU?: https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/\n\n[4] Difference Between FPGA and CPLD: http://www.differencebetween.net/technology/difference-between-fpga-and-cpld/\n\n[5] ASIC and SoC: https://www.eetimes.com/author.asp?doc_id=1285201\n\n[6] \u4e00\u6587\u770b\u61c2\u4eba\u5de5\u667a\u80fd\u82af\u7247\u7684\u4ea7\u4e1a\u751f\u6001\u53ca\u7ade\u4e89\u683c\u5c40: https://www.leiphone.com/news/201709/uuJFzAxdoBY7bzEL.html\n\n[7] CPUs, GPUs, and Now AI Chips: http://www.electronicdesign.com/industrial/cpus-gpus-and-now-ai-chips\n\n[8] Quantifying the performance of the TPU, our first machine learning chip: https://cloudplatform.googleblog.com/2017/04/quantifying-the-performance-of-the-TPU-our-first-machine-learning-chip.html\n\n[9] \u6df1\u5ea6\u5b66\u4e60\u7684\u4e09\u79cd\u786c\u4ef6\u65b9\u6848\uff1aASIC\uff0cFPGA\uff0cGPU\uff1b\u4f60\u66f4\u770b\u597d\uff1f: http://www.sohu.com/a/123176776_463982\n\n[10] \u65af\u5766\u798f\u535a\u58eb\u97e9\u677e\u6bd5\u4e1a\u8bba\u6587\uff1a\u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u6548\u65b9\u6cd5\u4e0e\u786c\u4ef6: https://zhuanlan.zhihu.com/p/30211134\n\n[11] A brief guide to mobile AI chips: https://www.theverge.com/2017/10/19/16502538/mobile-ai-chips-apple-google-huawei-qualcomm\n\n[12] Huawei unveils Kirin 970 chipset with AI: http://www.zdnet.com/article/huawei-unveils-kirin-970-chipset-with-ai/\n\n[13] \u60f3\u6210\u4e3aAI\u9886\u57df\u7684\u82f1\u7279\u5c14\uff0c\u5730\u5e73\u7ebf\u53d1\u5e03\u4e24\u6b3e\u7ec8\u7aef\u89c6\u89c9\u82af\u7247: https://www.jiqizhixin.com/articles/2017-12-21-4\n\n[14] NVIDIA launched Volta GPU computing architecture to bring speed in AI inference and training, as well as for accelerating HPC and graphics workloads.: https://nvidianews.nvidia.com/news/nvidia-launches-revolutionary-volta-gpu-platform-fueling-next-era-of-ai-and-high-performance-computing\n\n[15] Google introduced its TPU (Tensor Processing Units) that accelerates the TensorFlow framework in machine learning.: https://cloud.google.com/tpu/\n\n[16] IBM and U.S. AFRL announced the collaboration on a brain-inspired supercomputing system.: https://www-03.ibm.com/press/us/en/pressrelease/52657.wss\n\n[17] Microsoft is working on AI chips across its different devices, top exec says: https://www.cnbc.com/2017/11/01/microsoft-working-on-ai-chips-across-different-devices-top-exec-says.html\n\n[18] Huawei launched Kirin 970 \u2014 the new glagship SoC with AI capabilities: https://www.androidauthority.com/huawei-announces-kirin-970-797788/\n\n[19] Intel is buying Movidius, a startup that makes vision chips for drones and virtual reality: https://www.recode.net/2016/9/6/12810246/intel-buying-movidius\n\n[20] Report: Amazon working on its own AI chips for Echo devices: https://mashable.com/2018/02/12/amazon-echo-ai-chip/#VFOj98mEfqqy\n\n[21] CB Insight: www.cbinsights.com\n\n[22] Crunchbase: www.crunchbase.com\n\n[23] Hupogu: http://www.hupogu.com"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-mar-w-2-mar-w-3-7c5e00204f5d?source=user_profile---------56----------------",
        "title": "AI Biweekly: 10 Bits from Mar W 2 \u2014 Mar W 3 \u2013 SyncedReview \u2013",
        "text": "March 6th \u2014 Google Builds AI Drone Image Analysis System for the Pentagon\n\nGoogle is collaborating with the United States Department of Defense to create an AI system for identifying drone footage. The system will leverage machine learning and computer vision technologies to collect and categorize drone footage more efficiently.\n\nMarch 7th \u2014 Microsoft Announces an AI Platform for Developers in Windows 10\n\nMicrosoft is adding a new AI platform to Windows 10 which includes or enables various AI-based applications such as Photos app for intelligent video creation, Hello app for face recognition log in, and voice assistant Cortana for question answering.\n\nMarch 7th \u2014 Microsoft and Intel Enable Deep Learning Inference Vision Processing Units (VPU)\n\nMicrosoft announces Intel\u2019s Movidius Myriad X VPU hardware will be included in the Windows ML server. Windows ML can pre-train machine learning data models and process developers\u2019 own deep learning tasks within the OS interface. With the added power of Intel\u2019s VPU, Windows ML will be able to run specific deep neural network AI tasks at higher speeds while using less power.\n\nMarch 9th \u2014 Telenor Enhances AI and IoT Laboratory Research\n\nTelenor has opened a new IoT laboratory research unit in Trondheim, Norway. The company will invest over \u20ac5.2 million in the new lab, which will mainly collaborate with its existing AI lab on Narrowband IoT technology research and developer portal enhancement.\n\nMarch 9th \u2014 Waymo and Google Launch an Autonomous Truck Pilot Project in Atlanta\n\nWaymo and Google announce a pilot project for self-driving truck testing in Atlanta, USA. During this project the team will not only test the capabilities of their autonomous truck but also the automated logistics in allocating loads, connecting shippers, factories, and distribution centres.\n\nMarch 13th \u2014 Google Makes Music with Machine Learning\n\nUnder its Magenta project, Google introduces its NSynth (Neural Synthesizer), which uses machine learning algorithms and deep neural networks to create new sounds for electronic music. The prototype can create over 100,000 sounds based on only 16 source inputs.\n\nMarch 14th \u2014 Huawei Introduces Blockchain Stress Test Project\n\nHuawei introduces its Caliper Project, a method for testing blockchain stress performance which can help developers and engineers evaluate their blockchain applications in a controlled environment. Performance results may include success rate of the transaction, speed of transaction, and hardware resource performance.\n\nMarch 14th \u2014 Alexa Now Able to Play AI-Generated Songs\n\nAlexa\u2019s latest skill is the ability to play AI-generated songs. The new DeepMusic feature uses a deep recurrent neural network to collect music samples, and leverages an algorithm to create melodies and songs without human input.\n\nMarch 15th \u2014 Microsoft\u2019s AI Translation System Performs at Human Expert Level\n\nMicrosoft builds a Chinese to English translation system with natural language processing and machine learning techniques that can translate Mandarin to English at a level matching human translators. Microsoft based its results on blind comparisons by bilingual humans between its system\u2019s outputs and those of human translators.\n\nMarch 15th \u2014 Google Ventures Invests in AI Chip Start-Up Sambanova\n\nGoogle Ventures\u2019 parent company Alphabet pours US$56 million in Series A funding into startup SambaNova. SambaNova builds computer chipsets and applications for artificial intelligence and data analytics. SambaNova CEO Rodrigo Liang says the company is also interested in public cloud services."
    },
    {
        "url": "https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7?source=user_profile---------57----------------",
        "title": "Facebook AI Proposes Group Normalization Alternative to Batch Normalization",
        "text": "As Facebook struggles with fallout from the Cambridge Analytica scandal, its research arm today delivered a welcome bit of good news in deep learning. Research Engineer Dr. Yuxin Wu and Research Scientist Dr. Kaiming He proposed a new Group Normalization (GN) technique they say can accelerate deep neural network training with small batch sizes.\n\nAlthough deep learning thrives with complex neural networks and large datasets, training a model requires much time and power. This has prompted AI researchers to rethink the normalization techniques they use to reduce training costs.\n\nFacebook AI Research had already taken a few steps forward. Last June, it proposed an accurate, large minibatch SGD technique that can train ResNet50 with a minibatch size of 8192 on 256 GPUs in only one hour, while matching small minibatch accuracy.\n\nThe mainstream normalization technique for almost all convolutional neural networks today is Batch Normalization (BN), which has been widely adopted in the development of deep learning. Proposed by Google in 2015, BN can not only accelerate a model\u2019s converging speed, but also alleviate problems such as Gradient Dispersion in the deep neural network, making it easier to train models.\n\nDr. Wu and Dr. He however argue in their paper Group Normalization that normalizing with batch size has limitations, as BN cannot ensure the model accuracy rate when the batch size becomes smaller. As a result, researchers today are normalizing with large batches, which is very memory intensive, and are avoiding using limited memory to explore higher-capacity models.\n\nDr. Wu and Dr. He believe their new GN technique is a simple but effective alternative to BN. Specifically, GN divides channels \u2014 also referred to as feature maps that look like 3D chunks of data \u2014 into groups and normalizes the features within each group. GN only exploits the layer dimensions, and its computation is independent of batch sizes.\n\nThe idea of GN was inspired by many classical image features like SIFT and HOG, which involve group-wise normalization. The paper states, \u201cFor example, a HOG vector is the outcome of several spatial cells where each cell is represented by a normalized orientation histogram.\u201d\n\nThe paper reports that GN had a 10.6% lower error rate than its BN counterpart for ResNet-50 in ImageNet with a batch size of 2 samples; and matched BN performance while outperforming other normalization techniques with a regular batch size. It is worth noting that Dr. He is the main contributor to the development of ResNet (Deep Residual Network).\n\nGN also outperformed BN on other neural networks, such as Mask R-CNN for COCO object detection and segmentation, and 3D convolutional networks for Kinetics video classification.\n\nGN is not the first attempt to replace BN. Layer Normalization (LN), proposed in 2016 by a University of Toronto team led by Dr. Geoffrey Hinton; and Instance Normalization (IN), proposed by Russian and UK researchers, are also alternatives for normalizing batch dimensions. While LN and IN are effective for training sequential models such as RNN/LSTM or generative models such as GANs, GN appears to present a better result in visual recognition."
    },
    {
        "url": "https://medium.com/syncedreview/mapbox-is-mapping-the-future-9d4cf4753bea?source=user_profile---------58----------------",
        "title": "Mapbox Is Mapping the Future \u2013 SyncedReview \u2013",
        "text": "When Google Maps debuted in 2005 the technology amazed users, who could locate an address and even visit the spot using archived \u201cStreet View\u201d captures. The next generation of digital maps for autonomous driving and augmented reality (AR) will require much more \u2014 for example that the states and positions of rendered real world objects be both precise and up-to-the-minute.\n\nThese new challenges and opportunities have prompted mapping companies to reimagine digital mapping technology.\n\nMapbox, a San Francisco-based digital mapping startup, released its Mapbox AR at the Mobile World Conference 2018 in Barcelona. The AR platform comprises a suite of tools, a low-level AR core kit, and a framework. Users can access interactive maps with animated dynamic directions on mobile devices.\n\nFounded in 2011, Mapbox offers clients development tools for mapping their apps and website, including a featured Photoshop-like product, Mapbox Studio, that enables developers to access Mapbox\u2019s visualization layers and make maps with their own data. Mapbox business model focuses on visualizing for example high-density weather radar information; delivering business analytics; and logistics.\n\nWhile Google is unquestionably a dominant player in the digital mapping industry, Mapbox is carving out a niche. Their Road Network collects anonymous data information from 300 million users of apps such as Airbnb, Instacart, Snap, and others with embedded Mapbox SDK. This data can extrapolate detailed patterns, for example, of real-time traffic and environmental status.\n\nDave Cole, a Mapbox founding member and VP of Business Operations and Strategy told Synced that data is the key to his company\u2019s success. \u201cWe have a network now that\u2019s collecting over 220 million miles of anonymous data every single day that goes back into the map and it gets better.\u201d\n\nMapping for autonomous driving is a high-growth sector estimated to open up a US$20 billion market by 2050, according to Goldman Sachs. While breakthroughs in self-driving technologies have thus far come from sensors such as LiDars and Cameras, the important role of maps cannot be overlooked. In the future, maps may even become more important than sensors.\n\n\u201cFor example, when driving on a multi-lane road, the onboard sensor may be unable to detect the road after a turn because of road-side obstructions. Once you have the lane-level positioning and real-time road updates enabled by HD maps, you can slow down and switch to a safe lane in advance to prevent an accident,\u201d says Xudong Cao, Founder and CEO of China\u2019s self-driving startup Momenta.ai.\n\nMapping companies are vying for the vanguard in autonomous driving integration. Dutch mapping firm TomTom is a leader in HD map production, and has attracted investments and partnerships deals from Ford, GM, Fiat Chrysler.\n\nMapbox made its initial move into autonomous driving in 2016 with Mapbox Drive \u2014 now Mapbox Automotive \u2014 a package of developer tools that enabled automakers to customize HD maps and turn-by-turn navigation. Developers can access API offered by Mapbox Automotive and build out specific embedded dash navigation, as well as 3D rendered visualization, to give drivers enhanced environmental context.\n\nMapbox Automotive has also built a Global Lane Network that aggregates sensor data collected by cameras, LiDars and GPS from other self-driving partners and distributes the information.\n\n\u201cThink about a lot of autonomous companies that have invested in computer vision or LiDar. They are generating all this data on the vehicles, but they need a network to actually distribute it back so as one vehicle collects, they can refine the map and make it available for general use for the whole fleet of vehicles,\u201d says Cole.\n\nTo maintain a map that accurately keeps up with the world as it changes, Mapbox researchers are leveraging AI technologies for processing data and turning it into, for example, predictive traffic profiles based on density of people; and using Receptive Field Networks (RFNet) or You-Only-Look-Once (YOLO) algorithms for segmentation and object recognition.\n\nLast year Mapbox purchased Mapdata, an AI mapping startup that uses deep neural networks to improve computer vision and AR. The Belarus-based company is now a satellite R&D team devoted to merging front-facing cameras and navigation.\n\nMapbox has a team of 350 and has thus far accumulated investments of US$227 million. As of February 2018, the number of registered developers on the Mapbox platform topped one million, a milestone for a technology platform.\n\nWhile 60 percent of its business is with US developers, Mapbox is broadening its global operations, particularly in China. This month the former Head of Uber Business Development in Asia-Pacific (APAC) Andy Lee joined Mapbox to lead its APAC expansion.\n\nLee told Synced that because Mapbox is already partnered with Chinese companies such as Alibaba\u2019s trip service Feizhu, it is in a position to help Western companies enter China, and its Chinese partners go global.\n\n\u201cWhat we have done is helpful for many companies who are trying to figure out how to enter China. Building maps in China is a challenge of being able to deliver experiences on a very large scale, not just like hundreds of thousands but sometimes hundreds of millions of users on a daily basis. There are very interesting engineering problems,\u201d says Lee.\n\nWhen Cole spoke to us he shared a story about his youthful hobby of sketching building and drawing maps. Coincidentally, the young Lee loved to carry paper maps his mother gave him and study streets \u201cin a very nerdy way.\u201d\n\n\u201cThe experience has totally changed now because I have a smartphone with a map and travel or dining or lifestyle apps. My early childhood interest in maps is my career now, which is helping developers continue that journey, and that gets me really excited,\u201d says Lee."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-begin-testing-self-driving-cars-on-beijing-roads-c3dd9dd242c0?source=user_profile---------59----------------",
        "title": "Baidu to Begin Testing Self-Driving Cars on Beijing Roads",
        "text": "Uber\u2019s fatal self-driving car accident continues to send repercussions through the autonomous vehicle industry, and has fomented public doubt concerning the technology\u2019s safety. In response, Uber halted all its road tests for self-driving cars. And so it came as somewhat unexpected news when on March 22nd the Beijing Municipal Government gave tech unicorn Baidu the green light to test driverless cars on the city\u2019s public roads.\n\nBaidu is the first company to be granted special license plates for autonomous vehicle testing on public roads in the Chinese capital.\n\nIn December 2017 Beijing issued China\u2019s first self-driving vehicle policies in the official statements \u201cGuiding Opinions on Accelerating the Road Test of Autonomous Vehicles (Test Version)\u201d and \u201cAutonomous Driving Vehicle Road Test Management Regulations (Test Version), which detailed basic thresholds for testing autonomous vehicles on public roads, mandating for example that test vehicles must not drive on public roads, carry US$800k in insurance, be assessed and approved by the municipality\u2019s management agency at closed test sites, exclude passengers that are not testing engineers, and that no more than five vehicles can participate in a single test.\n\nSubsequent regulations such as \u201cStandards and Methods for Assessing Autonomous Vehicles\u2019 Road Test Capability (Test Version),\u201d \u201cRoad Test Requirements for Autonomous Vehicles at Closed Test Sites,\u201d and \u201cRoad Prerequisites for Autonomous Vehicles Road Tests\u201d introduced more detailed clauses.\n\nThe safety regulations in these documents state that autonomous vehicles tested on public routes must complete 5,000km training at designated closed testing sites, be able to comply with traffic regulations, and have a sound response plan prepared in case of emergency. Test engineers must have over three years driving experience, complete 50 hours of operational training, and be ready to take control of the vehicle at any time. Unless specified, testing vehicles are required to avoid peak traffic times and bad weather. Tests can only be conducted on routes outside Beijing\u2019s 5th ring road.\n\nBaidu is China\u2019s leading tech company in the self-driving space. In February it released the self-driving dataset ApolloSpace, which is about 10 times larger than any other existing open-source dataset. Baidu\u2019s platform Apollo grants developers access to a complete set of service solutions and open-source codes and enables software engineers to convert a Lincoln MKZ into a self-driving vehicle in about 48 hours. Apollo has also joined the UC Berkeley DeepDrive (BDD) Industry Consortium, led by Professor Darrell.\n\nAlthough Beijing is being relatively prudent in its approach to self-driving vehicles on public roads, the Baidu permits are a huge step forward for autonomous research. Shanghai, Chongqing, and Shenzhen are also making relevant policies in this space. Interested readers can follow up here: Global Survey of Autonomous Vehicle Regulations."
    },
    {
        "url": "https://medium.com/syncedreview/my-other-lawyer-is-a-robot-lawgeex-automates-contract-review-eef4e2247114?source=user_profile---------60----------------",
        "title": "\u201cMy Other Lawyer is a Robot\u201d \u2014 LawGeex Automates Contract Review",
        "text": "Artificial Intelligence is already helping doctors examine MRI scans, travel agents book flight tickets, and accountants keep books. But the legal industry has been somewhat stubborn when it comes to AI. \u201cThe assumption is that as long as it\u2019s read by humans it\u2019s fine, but there\u2019s no ground for this in any other industry in the world,\u201d says Shmuli Goldberg, VP of Marketing at LawGeex.\n\nFounded in 2014 by international business lawyer Noory Bechor and machine learning guru Ilan Admon, LawGeex is a Tel Aviv-based legal tech startup focused on contract review. It closed US$7 million in Series A funding last March, principally from Indeed.com owner Recruit Holdings, Lool Ventures, and LionBird. The company has a team of close to 50 in its Tel Aviv R&D units and New York sales office.\n\nThis February, LawGeex created a stir in the legal industry when its algorithms outperformed human lawyers at contract review in a study conducted by Stanford, Duke University, and the University of South California.\n\nResearchers asked 20 experienced lawyers and the AI to examine five previously unseen contracts composed of 153 legal clauses copied from standard Non-Disclosure Agreements (NDA). The AI used just 26 seconds to complete its analysis with an accuracy rate of 94 percent. Human lawyers meanwhile required nearly 92 minutes to achieve an average accuracy rate of 85 percent.\n\nA Fortune 1000 company can deal with up to 40,000 contracts annually. Many companies are dissatisfied with the existing organizational contract review process, which is an essential but laborious task. Confidentiality agreements, in particular, take a week or even longer to review.\n\nGoldberg says the LawGeex AI can cut the contract review cycle from a week or a month down to one hour, and reduce the time spent by a lawyer reviewing the contract from hours to minutes \u2014 adding that law firms which bill by the hour are not as interested in automated efficiency. LawGeex can also review NDAs, purchase orders, service agreements, software license, sales and other types of contracts with a consistency, says Goldberg, that tired office-trapped lawyers on a Friday evening cannot match.\n\nThe ideal LawGeex client is an in-house legal departments of medium to large-sized enterprises with over a thousand contracts per year. The client will first making clear their precedents and definition of terms. The AI, pre-trained to understand what they mean, will then identify pertinent elements in the contract and make sure these align with the client\u2019s specifications.\n\nThe default AI application for document review is natural language processing (NLP). However, LawGeex discovered that NLP performed poorly in contract review due to legalese. Although natural language can have multiple interpretations, legalese is specific and rigid.\n\nTo cope with this problem LawGeex developed two new AI algorithms. A Legalese Language Processing (LLP) algorithm first trains the neural network with as many contracts as possible. The AI learns to understand terms such as \u201cnon-compete\u201d and \u201cdisclosure,\u201d and identify terminology-relevant clauses in the contract. A Legalese Language Understanding (LLU) algorithm works on top of the LLP, translating the legalese into legal concepts to help the AI understand unfamiliar clauses.\n\nThe hardest part of building the process, says Goldberg, was finding half a million contracts as training datasets, as there are no standard learning or training sets in the legal world \u2014 such as ImageNet for computer vision or MNIST for handwritten digits. LawGeex enlisted the help of experienced lawyers from top US law firms and spent three years hunting for shared and disclosed contracts.\n\nGoldberg says what separates LawGeex from automated contract service providers such as Legalsifter and Kiva Systems is that \u201cthey are all based on the same premise, if I have thousands of contracts and want to find one thing, they are the best for that. Whereas our tool looks at one contract and answers the question \u2018can I sign this?\u2019 When someone comes to the lawyer, hands them the NDA and asks if he can sign this, our tool opens the NDA up, looks the potential hundreds of issues, highlights what\u2019s relevant and irrelevant and passes it onto the lawyer.\u201d\n\n\u201cThis isn\u2019t a new technology,\u201d says Goldberg, \u201cand we are not saying that AI is taking over the legal world, we are saying the opposite: AI has taken over the legal world.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/2017-turing-award-goes-to-computer-chip-pioneers-2c126b97c8e5?source=user_profile---------61----------------",
        "title": "2017 Turing Award Goes to Computer Chip Pioneers \u2013 SyncedReview \u2013",
        "text": "The Association for Computing Machinery (ACM) today announced John L. Hennessy, former Stanford University President and Chairman of the Board of Alphabet; and David A. Patterson, retired Professor at University of California, Berkeley, as winners of the 2017 Turing Award for their groundbreaking approach to computer architecture design and evaluation.\n\nThe Turing Award is the Nobel Prize of computer science, named after British mathematician Alan M. Turing, who laid the mathematical foundations for and defined the limits of modern computing. The award is sponsored by Google and carries a prize of US$1 million.\n\nHennessy and Patterson proposed a systematic, quantified approach to building faster and more energy-efficient Reduced Instruction Set Computer (RISC) microprocessors, which has been widely adopted by academia and industries. Today, RISC processors account for 99 percent of the more than 16 billion microprocessors produced each year and are used in smartphones, tablets and other embedded devices.\n\nHennessy and Patterson are also the co-authors of Computer Architecture: A Quantitative Approach, an influential 1990 textbook that has contributed to improved microprocessor design across the computer architecture community.\n\nIn the textbook the pair encourage architects to tailor their system designs to accommodate different memory and computing demands. The textbook also prompted considerations of energy consumption, heat dissipation, and off-chip communications, widening computer architecture\u2019s traditional focus from computational power alone.\n\n\u201cHennessy and Patterson\u2019s contributions to energy-efficient RISC-based processors have helped make possible the mobile and IoT revolutions. At the same time, their seminal textbook has advanced the pace of innovation across the industry over the past 25 years by influencing generations of engineers and computer designers,\u201d said ACM President Vicki L. Hanson.\n\nHennessy and Patterson have received numerous honours, including the ACM-IEEE CS Eckert-Mauchly Award and the IEEE John von Neumann Medal. They are fellows of the ACM and IEEE and members of the National Academy of Engineering and the National Academy of Sciences."
    },
    {
        "url": "https://medium.com/syncedreview/huawei-introduces-ai-development-board-hikey-970-763ac996b29a?source=user_profile---------62----------------",
        "title": "Huawei Introduces AI Development Board HiKey 970 \u2013 SyncedReview \u2013",
        "text": "At the Linaro Connect Hong Kong 2018 Developer Conference today, Chinese tech giant Huawei unveiled its latest development board, HiKey 970, which is dedicated to AI-powered devices and applications.\n\nHuawei\u2019s HiKey series are single board Linux computers that developers can use to write and test applications. They run on Huawei\u2019s smartphone SoC (system on a chip), and their development is supported by Linaro, an engineering organization that works on free and open-source software.\n\nHiKey 970 is based on Huawei\u2019s Kirin 970, the world\u2019s first AI processor for smartphones, with four ARM Cortex-A73 and four ARM Cortex-A53 cores, 6GB of LPDDR4 memory, the latest generation 12-core Mali G72MP12 graphic processors, and a Neural Processing Unit dedicated to AI acceleration. The Kirin 970 is up to 25 times faster and 50 times more energy efficient than traditional processors.\n\nHuawei has been a leader in the global telecommunications equipment market since 2012, and released the groundbreaking Kirin 970 SoC last September. The company\u2019s Kirin 970-equipped flagship smartphone Mate 10 is the first-ever mobile phone to ship with AI hardware.\n\nHiKey 970 supports a variety of interfaces including an AI stack, the Huawei HiAI computing architecture, and popular neural network frameworks. Developers can leverage HiKey 970 for easier and more efficient AI development in robots, smart cities, deep learning algorithms, etc.\n\nTo help free AI application developers from concerns regarding costs, distribution/promotion, and IP issues, Huawei improved HiKey 970\u2019s design and introduced features such as a multi-application model, support for machine learning frameworks, comprehensive documentation, rich and efficient APIs, a rapid-start source code, and so on.\n\nHiKey 970 will compete with the UK\u2019s Raspberry Pi boards in this market, and will go on sale in mid-April."
    },
    {
        "url": "https://medium.com/syncedreview/where-is-autonomous-driving-headed-after-the-fatal-uber-accident-in-arizona-130c893a35b6?source=user_profile---------63----------------",
        "title": "Where is Autonomous Driving Headed After the Fatal Uber Accident in Arizona?",
        "text": "An Uber self-driving SUV struck and killed a female pedestrian Sunday evening in Tempe, Arizona. The first known autonomous vehicle-related pedestrian death on a public road stunned the AI community and raised public concerns on autonomous driving safety.\n\nTempe police reported that the accident took place around 10 p.m. local time. The Uber vehicle was in autonomous mode, with a human safety driver at the wheel. The weather in Tempe Sunday night was clear and dry. The US National Transportation Safety Board (NTSB) said that it was joining the accident investigation.\n\nUber immediately suspended all its self-driving testing in North American cities. The company had launched its first self-driving road tests in Arizona only a few months ago.\n\nArizona is a hotbed of self-driving technologies and there are over 600 self-driving cars on the state\u2019s public roads. The state legislature has for years been cultivating an AV-friendly testing environment that rivals California\u2019s. Two weeks ago Arizona gave the green light for public road testing without human drivers in the vehicle.\n\nThere has been no official word yet on whether Arizona regulators will reconsider their relatively accommodating autonomous vehicle testing policies in the aftermath of Sunday\u2019s fatal accident.\n\nUber CEO Dara Khosrowshahi said Sunday\u2019s accident was \u201cincredibly sad news\u2026 We\u2019re thinking of the victim\u2019s family as we work with local law enforcement to understand what happened.\u201d\n\nIt\u2019s been a yearlong series of scandals for Uber, including a sexual harassment class action suit, the former CEO\u2019s departure, and an exodus of core executives. Last month the ride-hailing giant paid Waymo US$245 million in stock to settle a self-driving technology IP infringement lawsuit.\n\nGoogle Cloud Chief Scientist of AI/ML Fei-Fei Li tweeted \u201cA fatal self-driving car accident\u2026 This is what AI is: it\u2019s deeply impactful to human lives; and it takes all of us to work on it to make it safe, fair and benevolent.\u201d\n\nDirector of Microsoft Research Labs Eric Horvitz called for increased diligence in self-driving safety in the wake of the accident, and stressed that \u201cself-driving cars will be held to a higher standard. Automation will almost certainly bring down numbers of deaths on the roads.\u201d\n\nSelf-driving vehicles have logged millions of miles on public roads with strong safety records. \u201cThe probability of having an accident is 50 percent lower if you have Autopilot on,\u201d says Tesla CEO Elon Musk. When things do go wrong, however, they make headlines. In 2016, the driver of a Tesla in semi-autonomous driving mode was killed after colliding with a truck on a Florida highway. Although the driver had reportedly ignored at least seven safety warnings, the accident severely hit Tesla\u2019s credibility in the semi-autonomous driving market.\n\nLast year in Las Vegas a self-driving bus was involved in a crash with a delivery truck, only two hours after it made its debut. No injuries were reported at the scene. While technically the bus was not responsible for the accident \u2014 and the delivery truck driver was cited by police \u2014 passengers on the smart bus complained that it was not intelligent enough to move out of harm\u2019s way as the truck slowly approached.\n\nInsiders are saying the Arizona accident is the most serious setback yet for self-driving vehicles, coming at a critical time when the technology was transitioning from research and development to operation and deployment.\n\nEarlier this year, Waymo bought thousands of autonomous-capable Chrysler Pacifica Hybrids. The Alphabet-owned company has tested its self-driving cars in 25 cities across the US. American car maker Ford meanwhile is testing the capability of its own self-driving technology with delivery tasks in some US states.\n\nMany questions remained unanswered: How will the regrettable accident affect autonomous driving R&D and Startups? Will there be a public backlash against autonomous vehicles? Will lawmakers tighten regulations on self-driving cars on public roads?\n\nSynced is covering the story and will continue to update readers with the latest news."
    },
    {
        "url": "https://medium.com/syncedreview/googles-musicvae-is-a-machine-learning-mozart-eb0e44c790d5?source=user_profile---------64----------------",
        "title": "Google\u2019s MusicVAE Is a Machine Learning Mozart \u2013 SyncedReview \u2013",
        "text": "Google has announced the release of MusicVAE, a machine learning model that makes composing musical scores as easy as mixing paint on a palette. A breakthrough from Google Brain\u2019s Magenta Project, MusicVAE generates and morphs melodies to output multi-instrumental passages optimized for expression, realism and smoothness which sound convincingly like human-composed music.\n\n \n\nWhile breakthroughs in AI technologies have thus far tended to emerge from research into industry solutions, Magenta is exploring AI\u2019s potential in the creative spaces that differentiate humans from machines. Launched in 2016, Magenta uses deep learning and reinforcement learning algorithms to explore art and music and has introduced a number of research tools, including NSynth, a music synthesizer; and SketchRNN, an online neural network-based interactive doodling experiment.\n\n \n\nTeaching a machine to create a standardized method for blending different musical elements is not easy. Google researchers turned to Variational Auto-Encoders (VAE), a widely-used generative model that has yielded state-of-the-art machine learning results in image generation and reinforcement learning since 2013. \n\n \n\nVAEs work in an encoder-decoder structure where the encoder represents the variation in a high-dimensional dataset with a lower-dimensional code, and the decoder morphs the variation in a neural network to create an output. The model can be refined and tuned by comparing the input and output. \n\n \n\nGoogle researchers had already applied the technique to SketchRNN, and have now brought the same infrastructure to MusicVAE. Because musical elements are typically more complicated than sketches, Google researchers developed a novel hierarchical decoder for MusicVAE that is capable of generating long-term structure from individual latent codes.\n\n \n\nGoogle last Thursday released a Tensorflow implementation of MusicVAE and a JavaScript library with pre-trained MusicVAE models to help coders, composers and researchers build tools. \n\n \n\nSeveral Google engineers have already handcrafted applications based on MusicVAE. Melody Mixer is an interface created by Google\u2019s Creative Lab that allows users to generate interpolations between short melody loops. Latent Loops, from Google\u2019s Pie Shop, can generate a palette of melodic loops by sketching on a matrix. \n\n \n\nDemos and music samples generated by MusicVAE are already popping up on social media. \u201cThis MusicVAE thing is absurdly cool. The interpolated (and randomly generated) melodies/songs sound *real*, like they were composed, not generated,\u201d tweeted Alexander Huth, an Assistant Professor in Computer Science and Neuroscience at UT Austin. \n\n \n\nThe Magenta team stresses that MusicVAE and their other smart tools are meant as collaborative tools to \u201callow artists and musicians to extend (not replace!) their processes.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/african-fintech-startups-are-revolutionizing-banking-b726a1b4ccfe?source=user_profile---------65----------------",
        "title": "African Fintech Startups Are Revolutionizing Banking",
        "text": "Africa is a vast continent with diverse economies and a total population of over 1 billion people living in 54 independent countries spread over 30 million square kilometres. By the end of 2017, there were more than 300 fintech startups across the continent. Disrupt Africa\u2019s Finnovating for Africa: Exploring the African Fintech Startup Ecosystem Report 2017 concludes that African fintech startups\u2019 growth since 2015 has been nothing short of tremendous.\n\nThe record shows over US$100 million in fintech funding has been secured across the continent over the last two years, with South Africa receiving 34.2 percent of the total and Nigeria following closely with 34 percent. South Africa has the most fintech startups with 94, followed by Nigeria with 74 and Kenya 56. In this report, we identify the factors and the drivers behind the growth.\n\nFintech has made payments and remittances more convenient across the continent. Most traditional banks are located in cities and commercial areas, making them difficult to access from remote areas. Tanzania, for example, has about 50 million people sparsely distributed across an area nearly four times the size of the United Kingdom. In Nigeria, banks used to be packed with customers queueing to pay their utility and cable TV bills, school fees and so on. It could take hours to make a simple transaction.\n\nIn 2012, a cashless policy was introduced by the Central Bank of Nigeria to curb excess handling of cash and reduce the volume of money in circulation. The policy has facilitated many Nigerian fintech startups\u2019 market penetration and expansion.\n\nCustomer transportation costs, waiting times and the loss risks attached to cash have been eliminated by the smartphone-based fintech services provided by these startups. About 100 of Africa\u2019s fintech startups are focused on streamlining money transfers. According to Tayo Oviosu, Founder & CEO of Nigerian mobile payment platform Paga, \u201cNigerian banks have traditionally not focused on retail. Paga has built the single largest network of financial access points in Nigeria. We are going to leverage that to deliver financial services to the mass market\u201d.\n\nOne of the significant drivers of African fintech startups is high confidence in the market. Early fintech startups demonstrated that the market is strong, and the growth trend has continued, attracting Silicon Valley-based accelerators. Fintech startup funding is presently one of the most attractive investments on the continent. In 2017, over 30% of the US$195 million in VC funding raised by Africa startups went to the fintech sector.\n\nSafaricom\u2019s M-Pesa mobile money service has had a great impact in Kenya, and Nigeria\u2019s Paga, South Africa\u2019s Zoona, Kenya\u2019s BitPesa and others across the continent are garnering increased funding as investors become more confident.\n\nSince the inception of fintech, there have been dramatic changes in the continent\u2019s traditional banking system. Banks and financial institutions are under pressure to match the innovative solutions and services being offered by fintech startups which have reached millions of people who have mobile phones but not bank accounts.\n\nNow, banks and financial institutions are introducing a variety of strategies and tactics to invest in, acquire or collaborate with fintech startups. This is a trend that\u2019s expected to continue.\n\nMore Africans Connected to the Internet\n\nNigeria, South Africa, Egypt, Ethiopia and Kenya are among the most significant mobile markets in Africa. Although 80 million Nigerians \u2014 47 percent of the population \u2014 do not have bank accounts, 142 million Nigerians have mobile network access and 92 million are internet users, according to the Nigerian Communications Commission (NCC).\n\nThe penetration of mobile phones and the internet has enabled fintech to influence how financial services and products are developed and delivered, as more Africans plug into digital financial services in Nigeria and across the continent.\n\nKenya\u2019s M-Pesa is being used by more than half of the country\u2019s adult population, and has recorded transactions worth more than half the country\u2019s GDP since its debut. Similar success stories have been told by the likes of South African startup Zoona, Nigeria\u2019s Paga and others.\n\nA World Bank report notes that Nigeria, like many countries in sub-Sahara Africa, has a growing population that lacks easy access to traditional financial services, and fintech innovators are filling the vacuum by connecting these people.\n\nThis differs from the situation in advanced economies with strong financial institutions, where fintech startups are cast instead as disrupting the traditional banking industry. For example, China\u2019s Wechat is a popular messaging app with a wallet feature that enables users to send and receive money, make payments and so on from within the Wechat app, without connecting to a bank account for many transactions.\n\nAmong the factors responsible for African fintech startup growth are strong competition and loose regulations. Presently across the continent, there are few or no strict regulations compared to advanced economies. Startups can operate relatively tax-free free and with less government interference, leaving them to chart their courses and develop their products with little regulatory interference.\n\nThere is also increasing integration from service providers, many of which are partnering with fintech startups to make transactions more convenient for customers. For example, cable TV companies such as DSTV, HiTV, and TSTV have introduced fintech alternatives to their traditional bank payment models.\n\nAs more banks and financial institutions acquire or partner with fintech startups, the trend is being seen not so much as financial industry competition but as an industry reinvention that has improved financial services companies\u2019 profiles, reach, and products and services; and is beneficial to banks, startups and customers alike.\n\nFintech has thus become one of the most vibrant investment options in the African tech space.\n\nIn recent years, African fintech startups have been outperforming banks in delivering digital financial services. Iyin Aboyeji, Chief Executive of digital payment technology startup Flutterwave, describes fintech as a fundamental element that will drive the digital economy in Africa over the coming years. Investors and companies are expected to get even more involved in African fintech markets because the opportunities are tremendous."
    },
    {
        "url": "https://medium.com/syncedreview/yu-zheng-on-jd-coms-smart-city-ambitions-265253d51746?source=user_profile---------66----------------",
        "title": "Yu Zheng on JD.com\u2019s Smart City Ambitions \u2013 SyncedReview \u2013",
        "text": "Dr. Zheng joined JD Finance last month as Vice President and Chief Data Scientist, Urban Computing Business Unit President, and Urban Computing Lab Director. JD Finance is the fintech arm of JD.com, China\u2019s largest e-commerce platform by revenue.\n\nSynced recently spoke with Dr. Zheng on JD Finance\u2019s entry into Urban Computing.\n\nFintech is composed of two keywords: finance and technology. Many people think of JD Finance as a fintech company, but this is not accurate: we provide empowering technology for the finance industry.\n\nJD Finance\u2019s business model is B2B2C. To illustrate this, let\u2019s say we provide a better risk control model for banks, which in turn give better loan services for customers. In this equation JD Finance is the first B (business), the bank is the second B (business), and the end customer is C (customer).\n\nWe can also replace the middle B (business) with G (government), providing governing institutes the proper technology to serve its people. In other words, urban computing broadens JD Finance\u2019s existing businesses. The Chinese Government\u2019s invitation to JDF to get into these urban computing areas such as transportation and environment will change the public\u2019s view of our company.\n\nThat is confidential for the time being, we have some upcoming announcements. But I can tell you that we are building sub-divisions focusing on environment and transportation control, and the teams will be big.\n\nSmart commerce. People\u2019s impression of urban computing is environment, transportation, and various types of city planning. But in fact commerce also plays a big role in this panorama, for example in business site selection, real estate assessment, and helping banks with risk management.\n\nFor example, if a company seeks a bank loan to build a casino, then the bank will conduct risk assessment on the project itself, whereas in the past, the bank\u2019s approach may have been to just assess the company\u2019s own credit qualifications such as bad debt ratios, creditworthiness, and so on.\n\nCompanies with good qualifications may undertake risky projects by factoring in local development and consumption levels, which are only reflected in forms of data. Criteria such as local spending index, travel method, urban infrastructure including power networks and transportation all contribute to the decision making process.\n\nUrban computing uses diverse spatio-temporal data to do calculations such as analysis, prediction, causal analysis, and anomaly detection for a given scenario.\n\nWe have a lot of data. JD.com has nearly 300 million active users, as shown in our latest financial report. Our huge datasets are composed of product info, user transaction data, and logistic data. Financial figures like wealth management, payment and consumption also contribute to datasets.\n\nSufficient data quantity can accurately reflect a city\u2019s economic well-being. Logistics data for example maps the commercial flow of an area and its business relations with surrounding areas. We have data in abundance which is rare and good.\n\nUrban computing relies on spatio-temporal data, which is neither video, image, nor text. It has its own data management methods and AI algorithms. In other words, you can\u2019t solve the problems at hand by throwing in a CNN or LSTM alone. Spatio-temporal attributes, including time trends, periods, and proximity, spatial distance, and spatial gradation are characteristics that cannot be grasped by commonly used algorithms.\n\nThere are also multiple data sources. For instance, the casino case we mentioned above demands the use of POI, road network data points, plus a lot of data such as environmental and spending, which jointly predict future changes.\n\nMultivariate data fusion is a difficult, and it is also a relatively new discipline and research direction in machine learning. How can data from different fields determine that 1+1 is greater than 2? This is very difficult.\n\nAt the same time, urban computing is not a simple cloud computing problem. Cloud computing platforms can\u2019t support such spatio-temporal data. The data structure query method of spatio-temporal data, as well as the multi-data fusion and indexing mechanism just described, do not yet exist.\n\nThe cloud service providers currently on the market are not suitable for urban computing. Service providers must undergo a special technical build up, in order to manage, analyze, and tap into spatio-temporal big data, and form a dynamic closed loop. It is very difficult, and the threshold is also very high.\n\nLet me give you a concrete example: traffic light control is more difficult to tackle than AlphaGo. AlphaGo faces a 19\u00d719 grid, and the states on each grid are only black, white, or empty.\n\nYet there are tens of thousands of intersections of traffic lights in Beijing, and the status and actions of each intersection have more possibilities. Traffic may be flowing at 40 km/h, 45km/h or 30 km/h; signal light timing may be 30 seconds for the red light or 20 seconds for the green light. All of these are changing continuously. We are also missing data. Roads with no pedestrians, cars, or sensors do not give us any data. The road is also an open system. One man, or even a dog crossing the road will alter the traffic state.\n\nTherefore, urban problems have a large state space, large movement space, and an open system. This is certainly much more difficult to solve than problems on a Go board.\n\nLet me give another example. In urban population flow forecasting, we divide cities into a number of grids, and we want to predict how many people will enter and exit in each grid.\n\nThe flow of people in a grid is related not only to how many people have entered and exited in the previous hour, but also to how many people are moving in and out of neighboring grids. You also want to loop in areas that are far away, taking into consideration for that when big events happen, people will emerge from subway exits. If you only rely on local changes around the grid, you can\u2019t predict bigger things happening, say tragic incidents such as a stampede.\n\nIt\u2019s data and team. Urban computing relies on good databases and data resources. Everyone thinks that governments have a lot of data, but this is not the case. In many cases, they need industry data to support their own decision-making processes and solve problems.\n\nAlso important is the team. People say AI is a talent war, but it\u2019s definitely not a war of numbers. A team of one hundred is not necessarily better than a team of ten. But there are many times when you can\u2019t solve a problem yourself and so rely on others for inspiration. If we are stuck with missing critical data, perhaps an \u201ca-ha!\u201d moment from a teammate will provide alternative measures. This is what the word \u201ctalent\u201d stands for.\n\nA good engineer can work on ten projects at the same time, while a mediocre team might use one hundred people without any results. We realize the importance of talent, and so we have heavily invested in pooling top-notch talents.\n\nPrior to joining JD Finance, Dr. Zheng led the urban computing team at Microsoft Research Asia, publishing profusely in this area; while also holding the positions of Chair Professor of Shanghai Jiaotong University, Guest Professor of Hong Kong University of Science and Technology, and Editor-in-Chief of ACM TIST."
    },
    {
        "url": "https://medium.com/syncedreview/global-survey-of-autonomous-vehicle-regulations-6b8608f205f9?source=user_profile---------67----------------",
        "title": "Global Survey of Autonomous Vehicle Regulations \u2013 SyncedReview \u2013",
        "text": "Most people today realize it is only a matter of time before self-driving vehicles revolutionize our transportation and delivery systems. While breakthroughs in self-driving technologies have thus far come from a cluster of star companies such as Google Waymo, Ford, GM, and Tesla, the important role of lawmakers and local authorities in the research and development process cannot be overlooked.\n\nWithout government permits, testing self-driving cars on public roads is almost universally illegal. The Vienna Convention on Road Traffic, an international treaty that has regulated international road traffic since 1968, stipulates that a human driver must always remain fully in control of and responsible for the behaviour of their vehicle in traffic.\n\nEuropean and North American countries such as the US, Germany, UK, and Netherlands were pioneers of self-driving vehicle licensing, and have introduced regulations for self-driving cars on public roads and issued autonomous testing permits. Asian countries quickly caught up and have been enacting similar legislation over the last three years.\n\nSynced surveyed the international regulations, and here are our picks of major regions and countries that are accommodating the testing and deployment of autonomous driving technologies on their public roads.\n\nWhen the mass-produced Model T Ford was introduced in 1908 the US became the first country where a typical middle-class family could afford a car. Today, Uncle Sam is also a leader in the integration of self-driving technologies.\n\nEach US State is responsible for its own autonomous driving legislation. Last year, 33 states had either passed legislation, issued executive orders, or announced initiatives to accommodate self-driving vehicles on public roads.\n\nCalifornia is undoubtedly the top-ranked state in openness and preparedness for autonomous vehicles. Its autonomous vehicle testing regulations were introduced in September 2014 and required a driver be in the vehicle, ready to assume control. Recently, California took a step forward by allowing fully autonomous vehicles with no driver to operate on its public roads.\n\nFifty self-driving companies are testing their technologies in California, Fortune reports. Google\u2019s Waymo and GM lead in autonomous miles logged: Waymo accumulated 352,545 autonomous public road miles (567,366 km) in the 12 months preceding November 2017, while GM vehicles drove 131,676 miles (211,912 km) in 2017.\n\nArizona meanwhile has also removed obstacles to the deployment of autonomous vehicles, cultivating an AV-friendly testing environment that now rivals California\u2019s. In August 2015, Arizona Governor Doug Ducey signed an executive order directing agencies to \u201cundertake any necessary steps to support the testing and operation of self-driving vehicles on public roads within Arizona.\u201d This March, Ducey updated the executive order and gave the green light for cars without drivers to operate on public roads in Arizona. There are now over 600 self-driving cars on the state\u2019s public roads.\n\nFlorida, Michigan, and Pennsylvania, are also leaders in the accommodation of autonomous vehicles.\n\nChina is nowhere close to a regulation-friendly country for autonomous driving. Although it has prominent autonomous driving companies such as Baidu Apollo, JingChi.ai, and Pony.ai, the country got off to a slow start with legislation and permits.\n\nThings are changing now, especially in first-tier cities. Earlier this month Shanghai issued its first self-driving licenses, allowing two automakers to test their autonomous vehicles on public roads. The tests are limited to a 5.6 km (3.5 mile) stretch of public road in the city\u2019s Jiading District. Shanghai is China\u2019s first Smart Network and Autonomous Driving Pilot City.\n\nThis January, the Beijing Municipal Traffic Commission announced the city\u2019s first autonomous driving test track will be built in suburban Yizhuang. Meanwhile, Hangzhou, the home city of China\u2019s tech giant Alibaba, will open an autonomous driving test track this year, located 1.4 km from Alibaba\u2019s main campus. China\u2019s autonomous vehicle industry hub of Guangzhou recently allowed Pony.ai and JingChi.ai to test vehicles in certain districts.\n\nLast December Chongqing revealed a plan to designate a huge open road test area by 2019 that includes cities, mountains, highways, tunnels and bridges, and is enabling 5G telecommunications across the area. The local government also introduced the Chongqing Autonomous Vehicle Road Test Management Implementation Rules to regulate testing on local public roads.\n\nLast December, the Shenzhen Bus Group\u2019s \u201cSmart Driving Bus System\u201d was introduced on a dedicated 1.2 km route.\n\nAlthough Northern Europe does not get as much attention as the US or China when it comes to autonomous driving, countries such as Netherlands and Sweden are more likely to democratize automated transport systems nationwide than any other regions.\n\nKPMG\u2019s 2018 report Autonomous Vehicles Readiness Index ranks 20 countries\u2019 preparedness for an autonomous vehicle future. The Netherlands took the top spot, outperforming the US (7th) and China (16th). KPMG praised the Netherland\u2019s heavily-used and well-maintained road network. The country has also built almost 30,000 electric vehicle charging points and has high-quality wireless networks for transmitting data to and from autonomous vehicles.\n\nThe Netherlands\u2019 Council of Ministers first approved autonomous vehicle road testing in 2015, and updated its bill last February to allow tests without a driver. The Dutch government is spending \u20ac90 million adapting more than 1,000 of the country\u2019s traffic lights to enable them to communicate with autonomous vehicles.\n\nIn 2016 the Netherlands deployed WEpods in a central Dutch city. The world\u2019s first electric driverless shuttle, WEpods can hold six people, and operate on fixed lanes across the city.\n\nSweden is ranked 3rd in KPMG\u2019s 2018 report. The country that gave birth to IKEA, Spotify, Ericsson and Volvo has ramped up its support for autonomous driving over the last few years.\n\nIn 2015 the Swedish government first explored self-driving vehicle testing, concluding that it was possible to carry out trials at all levels of automation on Swedish roads. The Road Transportation Authority can, as of July 2017, authorize permits and supervise trials in accordance with the law.\n\nLast December Volvo launched its Drive Me project, which provided self-driving cars to a number of people in Gothenburg for use in their everyday lives. The project is aimed at collecting user feedback to hone Volvo\u2019s technology.\n\nLast November, Volvo signed a US$300 million deal with Uber to provide the ride-hailing giant with 24,000 flagship self-driving-ready Volvo XC90 SUVs.\n\nGermany is now a hotbed of autonomous vehicles as the German parliament passed a law last May that allows companies to begin testing self-driving cars on public roadways. Drivers are allowed to remove their hands from the wheel and perform simple tasks such as futzing around on smartphones while the car drives itself. However, drivers are required to remain ready to take control in order to handle possible emergencies.\n\nThe new legislation also requires a black box, a counterpart data recorder for autonomous vehicles designed to record system data and actions for review in the case of accidents.\n\nThe UK is another country that has been progressive with autonomous vehicle policy and regulations. While most European countries adhere to Vienna Convention on Road Traffic, the UK is not a signatory and so is believed to have an advantage in adopting legislation to attract autonomous vehicle manufacturers and tech startups. The UK government is aiming for a wide adoption of autonomous vehicles on its roads by 2021.\n\nIn 2013, the Department for Transport allowed semi-autonomous cars to operate on lightly-used rural and suburban roads. Three years later, in her annual address the Queen herself spoke to the importance of enacting \u201cnew laws to make the UK ready to pioneer driverless cars.\u201d\n\nLast year, the UK government passed a bill to draw up liability and insurance policies related to autonomous vehicles.\n\nSingapore could be the first Asian country to widely adopt autonomous driving \u2014 or even the world\u2019s first. The country has the world\u2019s third highest population density, and the government is under pressure to revamp the transportation system.\n\nKPMG\u2019s 2018 report gives Singapore the maximum score on policy and regulations related to autonomous vehicles. In July 2015, the Singapore Land Transport Authority (LTA) authorized 6 km of test routes, and doubled the distance a year later. In 2017 the LTA expanded its AV test bed to neighboring areas such as the National University of Singapore, Singapore Science Parks 1 and 2, and Dover and Buona Vista, adding 55 km to existing autonomous vehicle trial routes.\n\nLast year, the Government of Singapore passed legislation recognizing motor vehicles don\u2019t require a human driver and regulating the operation of such vehicles on public roads. The rules exempt autonomous vehicles and their operators from existing legislation mandating a human driver must be responsible for the safe use of motor vehicles on the road.\n\nSingapore\u2019s supportive environment attracted Boston-based self-driving software company NuTonomy. The company, which was acquired by Delphi for US$450 million, launched a free trial autonomous taxi service in August 2016 and hopes to run an autonomous taxi service in the city-state by the end of Q2 this year.\n\nSouth Korea is possibly the most aggressive country in terms of government investment in autonomous vehicles. The homeland of Samsung, Hyundai and LG allows autonomous vehicles with issued licences to operate on public roads (two sections of expressways and four sections of regular roads, spanning a combined 320 kilometers), and is building an entire artificial town for autonomous vehicle testing.\n\nLast November, the country\u2019s Ministry of Land, Infrastructure and Transport announced the opening of K-City, which is the largest town model ever built for self-driving car experimentation. K-City cost US$11 billion and presents 35 different driving conditions, including toll gates, pedestrian and train crossings, and even potholes and construction sites.\n\nAt the recent Winter Olympics, South Korea flexed its autonomous driving muscles, with Hyundai Motors deploying a self-driving car fleet while KT Corporation provided a self-driving shuttle service.\n\nNew Zealand is an early and keen adopter of autonomous vehicles, second only to Singapore on specific policy and legislation, according to KPMG\u2019s 2018 report.\n\nThe New Zealand government encourages the testing of semi and fully autonomous vehicles and is facilitating the early adoption of autonomous driving technology. The country has no specific legal requirements for cars to have drivers.\n\nNew Zealand recently approved an autonomous flying taxi trial for Kitty Hawk, the Silicon Valley-based startup run by Google founder Larry Page."
    },
    {
        "url": "https://medium.com/syncedreview/microsoft-ai-achieves-milestone-in-machine-translation-ba0cf8f26408?source=user_profile---------68----------------",
        "title": "Microsoft AI Achieves Milestone in Machine Translation",
        "text": "Microsoft researchers in the US and Asia sent a shockwave through the AIcommunity today with their paper Achieving Human Parity on Automatic Chinese to English News Translation, which introduces a neural machine translation system they say equals the performance of human experts in Chinese-to-English translation.\n\nAlthough artificial intelligence has outperformed humans in tasks such as image accuracy and speech recognition, many experts doubted machines could do so with language translation. \u201cHitting human parity in a machine translation task is a dream that all of us have had,\u201d said Xuedong Huang, a technical fellow in charge of Microsoft\u2019s speech, natural language and machine translation efforts. \u201cWe just didn\u2019t realize we\u2019d be able to hit it so soon.\u201d\n\nMicrosoft\u2019s system was tested on the benchmark news story dataset newstest2017, which was developed by a group of industry and academic partners and released at last fall\u2019s WMT17 research conference. To measure the translation quality accurately, Microsoft researchers hired bilingual human evaluators to compare Microsoft\u2019s results with two independently produced human reference translations, instead of referring to traditional metrics such as BLEU and TER.\n\n\u201cThe same source sentence can be translated in sometimes substantially different but equally correct ways. This makes reference-based evaluation nearly useless in determining the quality of human translations or near-human-quality machine translations,\u201d says the paper.\n\nMicrosoft\u2019s new machine translation system scored 69.0, indistinguishable from human translation which scored 68.6, according to the paper.\n\nHuang told Synced that machine translation is the key to mastering natural language understanding (NLU), which researchers believe will facilitate the development of artificial general intelligence (AGI) \u2014 the long-range, human-intelligence-level target of contemporary AI technology.\n\n\u201cNLU does not have large datasets. However, machine translation does. We use the deep neural network to learn semantic representations, which can be applied to NLU. As we learn the expression of language, we may have a chance to solve NLU and improve Cognitive Services (a set of Microsoft\u2019s machine learning algorithms),\u201d says Huang.\n\nMicrosoft researchers focused on the Chinese (Mandarin) to English language pair as these are the two most used languages in the world, and sampled texts from the news domain because news stories have a wide content variety. Microsoft researchers caution that their results will not necessarily generalize to other language pairs or domains, even though the techniques used were not specific to languages or domains.\n\nHuang attributes the breakthrough to three factors: increased computation capability provided by Nvidia\u2019s GPUs; improved algorithms and a particularly deep neural network; and an optimized dataset, using engineering methods to wipe out low-quality data, or noise.\n\nTo improve the model\u2019s accuracy and fluency researchers used additional training methods, for example a dual learning technique that learns from both source-to-target and target-to-source translation data by taking a sentence translated from Chinese to English and translating it back to Chinese, then comparing the result to the original sentence.\n\nAnother technique employed was deliberation networks, which train the model to repeatedly translate the same text. Similar to how a human might write multiple drafts, the deep neural network gradually improves and refines its output.\n\nThis new system has not yet been applied to Microsoft\u2019s commercial translation products such as Microsoft Translator, PowerPoint Presentation Translator, or Cognitive Services, but Huwang says his team is working on it.\n\nResearchers still face many challenges in machine translation, particularly with real-time translation and speech-to-speech translation. Microsoft\u2019s milestone positions the company among the global leaders in this busy research field."
    },
    {
        "url": "https://medium.com/syncedreview/fintechs-new-big-player-jd-finance-adds-2b-hits-26-30b-valuation-cb5d702d3304?source=user_profile---------69----------------",
        "title": "Fintech\u2019s New Big Player: JD Finance Adds $2B, Hits $26\u201330B Valuation",
        "text": "According to a China Securities Journal report, the China International Capital Corporation (CICC) and China\u2019s largest food processor, manufacturer and trader China National Cereals, Oils and Foodstuffs Corporation (COFCO) are looking to invest US$2 billion (CNY 13 billion) in JD Finance, the fintech spinoff of online retailer JD.com.\n\nThe funding will be applied to acquiring financial licenses through mergers and acquisitions, technology research and development, and further investments. The investing institutes will hold 10 billion JD Finance shares. The transaction is expected to be completed by end of April and will raise JD Finance\u2019s market valuation to a colossal US$26 billion to $30 billion.\n\nEstablished in late 2013, JD Finance has provided fintech services to more than 500,000 corporate clients and 150 million individual consumers. Parent JD.com is Alibaba\u2019s main domestic competitor and the largest Chinese online retailer by revenue, with 335 warehouses covering 2,691 counties and districts in China as of mid-2017.\n\nIn January 2016 the company raised Series A funding of US $1 billion (CNY 6.65 billion) from Sequoia Capital China, China Harvest Investments, and China Taiping Insurance, boosting its valuation to $7 billion and becoming the third-largest fintech company in China, after Alibaba\u2019s Ant Financial and Shanghai-based online finance marketplace LuFax.\n\nJD Finance separated from JD.com in June 2017 and now stands alone as a domestically funded company. This is similar to Alibaba\u2019s splitting AliPay from Alibaba. The partition helps JD Finance dodge JD.com\u2018s overseas investment affiliations and impediments to procuring financial licenses.\n\nLast year JD Finance filed revenues of US$1.6 billion, a 132% increase on 2016. Financial services accounted for 51% of revenues; payment business 15%, and logistics finance and wealth management 15% and 12% respectively.\n\nJD Finance\u2019s Silicon Valley AI Lab Chief Scientist Bo Liefeng is the former principal research scientist at Amazon. He led the AI research team for Amazon Go\u2019s recently opened staff-free store in Seattle. Last year JD Finance launched an international hackathon, attracting talents in the areas of artificial intelligence, big data, and cloud computing."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-discounted-its-top-smart-speaker-to-15-sold-1-million-5fc9243f1af8?source=user_profile---------70----------------",
        "title": "Alibaba Discounted its Top Smart Speaker to $15; Sold 1 Million",
        "text": "I purchased a Tmall Genie X1 \u2014 Alibaba\u2019s flagship smart speaker \u2014 at the discounted price of US$15 during China\u2019s November 11 \u201cSingles Day\u201d shopping festival. I was given order number 560,000-ish, and received the product a month later. The speaker is regularly priced at US$79, about the same as its American counterpart Google Mini.\n\nA Chinese smart speaker industry insider who declined to be named told Synced that it would be impossible to even cover hardware costs for $15, but smaller brands had no choice but to go into a price war with Alibaba, cutting their own Singles Day prices to as low as $10. There were strong smart speaker sales across the board, but the big winner was Genie X1, spurred by an incredible online marketing campaign and supported by superior sales channels.\n\nThe Smart Speaker story begins back in 2011, when Amazon began working on its voice assistant Alexa. Amazon\u2019s Echo smart speaker, powered by Alexa, went on sale in 2014, and although sales were initially sluggish, other tech companies picked up on smart speakers very quickly.\n\nSensing the trend, in April 2015 Alibaba devoted hundreds of engineers to smart speaker software prototypes, teaming up with hardware partners LED manufacturer Lipu Lightingand speaker manufacturer Edifier; and launching its smart voice assistants XiaoFei and MA1/3/5 within the span of three months.\n\nBoth were however poorly received in the market. Team leader Xue Qian says she realized in retrospect that although Alibaba\u2019s first attempts had sound outsourced hardware, the Android-based software system wasn\u2019t the right solution.\n\nIn 2016 Alibaba formed its AI Lab and put Qian in charge. The lab was backed by iDST and ET City Brain and attracted talents such as former Nanyang Technological University Professor Gang Wang; Principal Researcher and Head of the Big Data Mining Group at Microsoft Research Zaiqing Nie, and Project Tango Lead of Google Mingyang Li.\n\nThe lab invested much time and energy in streamlining Smart Speaker hardware and software design, eventually giving birth to AliGenie, the first generation Human-Computer Interaction (HCI) system that runs on the cloud and understands user commands in Mandarin.\n\nAliGenie is the brains behind the Tmall Genie X1, which was launched in July 2017.\n\nTmall Genie X1 employs 5-meter far-field voice recognition technology supported by cutting edge acoustic beamforming technology with six microphones, which enhances user voice and cancels background noise. It uses natural language processing, knowledge graphs, machine learning, and AR to perform functions such as smart home control, voice shopping, mobile phone recharge, takeout orders, and music play. The voice recognition neural network model is trained with massive labeled voice and language data that covers most daily dialogues.\n\nAI Lab\u2019s team monitors and collects all online reviews. \u201cEven bad reviews are good to us because they are user feedback. It\u2019s rare that our competitors get so much feedback, and smaller brands are definitely envious,\u201d explains Qian.\n\nCriticism of Genie X1 includes poor interactive experience and performance with online shopping tasks, limited connectivity as a smart home controller, and performing worse than conventional remote controls.\n\nSmart speakers are being developed as the home\u2019s future portal for connected devices, but ironically competitors are blocking each other\u2019s access in this space with proprietary technology. For example, customers who purchase a Genie X1 will not be able to use Xiaomi\u2019s smart lightbulbs, and will have to buy a designated Midea air conditioner if they want to control it via the Genie X1.\n\nFilling more than one million orders tested Qian\u2019s nerves as her team struggled to meet demand, \u201cwe out-purchased all available amplifiers and chips on the market, with factories running 24/7, and still lagged on delivery date. The capacity to manufacture smart speakers at large quantities is an overlooked problem.\u201d\n\nIn the burgeoning smart speaker market fast iterations are essential to survival. \u201cWe are three years behind Amazon\u2019s Echo,\u201d says Qian, \u201cbut it took us just four months from launching Tmall Genie X1 to reach one million unit sales.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/baidu-apollo-releases-massive-self-driving-dataset-teams-up-with-berkeley-deepdrive-5e785ab4053b?source=user_profile---------71----------------",
        "title": "Baidu Apollo Releases Massive Self-driving Dataset; Teams Up With Berkeley DeepDrive",
        "text": "Baidu this Thursday announced the release of ApolloScape, billed as the world\u2019s largest open-source dataset for autonomous driving technology.\n\nApolloScape was released under Baidu\u2019s autonomous driving platform Apollo, which Baidu hopes will become \u201cthe Android of the auto industry.\u201d Apollo gives developers access to a complete set of service solutions and open-source codes and can enable for example a software engineer to convert a Lincoln MKZ into a self-driving vehicle in about 48 hours. ApolloScape\u2019s open sourced data now provides developers a base for building self-driving vehicles.\n\nThe data volume of ApolloScape is 10 times greater than any other open-source autonomous driving dataset, including Kitti and CityScapes. This data can be utilized for perception, simulation scenes, road networks etc., as well as enabling autonomous driving vehicles to be trained in more complex environments, weather and traffic conditions. ApolloScape also defines 26 different semantic items \u2014 eg. cars, bicycles, pedestrians, buildings, streetlights, etc. \u2014 with pixel-by-pixel semantic segmentation technique.\n\nThe ApolloScape dataset will save researchers and developers a huge amount of time on real-world sensor data collection.\n\nAccording to a Rand Corporation report, accumulating sufficient real road testing data to conclude a 20 percent advantage for autonomous vehicles over human drivers would require a fleet of 100 vehicles driving nonstop for 500 years.\n\nBeyond data, ApolloScape will also facilitate advanced research on cutting-edge simulation technology aiming to create a simulation platform that aligns with real-world experience.\n\nApollo also announced it has joined the Berkeley DeepDrive (BDD) Industry Consortium, a top-tier research alliance investigating state-of-the-art technologies in computer vision and machine learning for automotive applications.\n\nHoused at the University of California, Berkeley and led by Professor Trevor Darrell, Faculty Director of PATH, the BDD consortium has attracted big tech names as partners, including Ford, Nvidia, Qualcomm, and GM. BDD\u2019s main research focus is on deep reinforcement learning, cross-modal transfer learning, and clockwork FCNs for fast video processing.\n\nHaifeng Wang, Baidu Vice president and Head of Baidu Research Institute, told Synced, \u201cThe partnership will incorporate Apollo\u2019s industrial resources and Berkeley\u2019s top academic team to ramp up the innovation of theoretical research, applied technology, and commercial applications.\u201d\n\nApollo Open Platform and BDD will jointly conduct a Workshop on Autonomous Driving at CVPR 2018 (IEEE International Conference on Computer Vision and Pattern Recognition) this June in Salt Lake City where they will organize task competitions based on ApolloScape."
    },
    {
        "url": "https://medium.com/syncedreview/how-ai-can-help-the-oil-industry-b853dda86be6?source=user_profile---------72----------------",
        "title": "How AI Can Help The Oil Industry \u2013 SyncedReview \u2013",
        "text": "Oil, also known as a \u201cblack gold,\u201d has been one of the most valuable and in-demand resources in the world since ancient times. It\u2019s easy to see why countries have historically sought to secure supplies, and why oil has been one of the major factors in military conflicts. Oil provides nearly half the world\u2019s energy, powers the majority of vehicles, and is a base ingredient for many industrial chemicals. Oil is the lifeblood of industrialized countries: most manufacturing, technologies, plastics, and fertilizers would not be possible without it.\n\nThe price of oil has shown sluggish growth since its 2014 collapse. Although the adoption of new technologies such as directional drilling and hydraulic fracturing have increased yields, the industry continues to seek solutions to boost business, and many see AI as the answer.\n\nTo apply AI technology to the oil and gas industry, oil companies and startups generally first establish either a research group or a research center for the purpose. AI for oil and gas is a huge potential market, expected to reach US$2.85 billion by 2022. Presently, North America is the largest market using AI in oil and gas, followed by Europe and Asia Pacific.\n\nOil and gas is a huge industry which includes upstream, midstream and downstream components. There are different ways to apply AI technologies to these different sectors. The common factor is that AI can help oil and gas companies lower costs and make more accurate decisions.\n\nThe oil and gas industry is adopting new technologies in its quest to be more efficient and profitable with low margins, and AI and cognitive computing are a perfect fit. \u201cThe next generation of competitive advantage in the energy marketplace will go to forward-thinking players who invest a lot on digital IoT and artificial intelligence capabilities like Cerebra from Flutura,\u201d says Archie W. Dunham, JAG chairman emeritus and former independent non-executive Chairman of Chesapeake Energy in Oklahoma City and retired ConocoPhillips Chairman.\n\nA decade ago, the most advanced AI could only advise companies in retrospect for example that they should have taken a specific preventative action in order to prevent failure. Nowadays, with the help of advanced sensors and software powered by AI, companies can digest a large amount of data and output real-time responses on the best course of action. Within a few years, the Industrial Internet of Things (IoT) will comprise more than a trillion sensors that generate and share data, and these innovations will dramatically change the way oil and gas companies operate.\n\nThe use cases below illustrate how AI is already helping the oil and gas industry.\n\nThe upstream sector is usually known as the Exploration & Production (E&P) sector, and includes companies that locate and extract crude oil or natural gas. Most drilling and production wells are located in remote areas, and sending workers there increases costs. Onsite operating costs can be reduced by using sensors and the Internet of Things (IoT) powered by AI to handle data collection and system control in real time.\n\nThe midstream sector processes, stores, and transports crude oil, natural gas, and liquefied natural gas. This sector is the important link between remote oil and gas producing areas and population centers where most consumers are located.\n\nThe downstream sector includes oil refineries, petrochemical plants, petroleum product distributors, and natural gas distribution companies. This sector produces countless products including gasoline, diesel, jet fuel, lubricants, plastics, fertilizers, natural gas, and propane.\n\nThe digital revolution in oil and gas industry is taking off. Improved safety and productivity can be achieved by automating routine manual activities, which can also reduce the risk to human workers. AI also has the potential to free scientists and engineers from repetitive and time-consuming tasks, and can assist in decision making. Moreover, AI systems can automate and optimize data-rich processes to mitigate business risks.\n\nAI\u2019s power and potential to increase efficiency and cost-effectiveness is making the technology increasingly attractive and speeding its adoption across all sectors of the oil and gas industry."
    },
    {
        "url": "https://medium.com/syncedreview/samsung-rides-the-ai-wave-3fe95796c4a2?source=user_profile---------73----------------",
        "title": "Samsung Rides the AI Wave \u2013 SyncedReview \u2013",
        "text": "Compared to other tech giants, Samsung is a relative late-bloomer in AI. But recent moves in acquisitions and R&D signal the company is jumping onto the intelligence wave, and doing so with a strong focus on virtual voice assistants.\n\nSamsung introduced its \u201cS Voice\u201d voice interface back in 2012 in response to the rising popularity of Apple\u2019s Siri. The release however received lukewarm reviews, and S Voice stalled in the marketplace. By late 2016 Samsung upped its game by acquiring Viv Labs, an AI assistant startup founded by former Siri creators Dag Kittlaus and Adam Cheyer.\n\nIn 2017 Samsung introduced the intelligent voice assistant Bixby for its smartphones, wireless earphones, TVs, fridges, and so on. The South Korean manufacturer is expected to release Bixby 2.0 later this year alongside a range of smart speakers and the Samsung Galaxy Note 9. The upgraded assistant will feature improvements achieved with help from Viv Labs and Samsung\u2019s other recent acquisitions including Reactor Labs, Expect Labs and Vicarious.\n\nLast week Samsung Research America acquired Egyptian tech company Kngine for an undisclosed sum. The company provides mobile search solutions using deep learning and knowledge-based AI algorithms. Its engine crawls the world wide web, enterprise documents, books, FAQs and even customer service logs to gather information which it can use to help answer queries. It ranks possible responses, presenting users with the most plausible answer for a given question.\n\nSamsung believes Kngine\u2019s functionality will strengthen Bixby and set it apart from rivals Siri, Alexa and Google Assistant. It\u2019s still early to judge Bixby\u2019s full performance, but the product has already garnered positive feedback online.\n\nSamsung\u2019s C-Lab (Creative Lab) is located in the sprawling Samsung Digital City in Suwon, South Korea. C-Lab is currently curating three new AI products that will be showcased at this year\u2019s SXSW conference, which runs March 9\u201318 in Austin, Texas.\n\nFirst up is Aurora, a visualized 3D character assistant that can recognize a user\u2019s gestures and location using a smartphone camera and display visual information on the device. Aurora can also assume various emotional states when interacting with users, for example communicating with the chill style of a best buddy. Aurora could certainly be fun for those who want more character in their virtual assistant.\n\nToonsquare, meanwhile, adds to Samsung\u2019s AI entertainment kit with its ability to convert phrases into cartoon images. The app\u2019s text analyses AI takes what the user wants to communicate, then seasons the phrase with appropriate fun images. The app offers customization options for font, background and speech bubble.\n\nThe third release is an automated ad trading platform for video games called Gadget, which will help marketers put their ads on native game objects such as billboards. Users can choose what types of ad they\u2019ll see. The seamless real-time ads will appeal to those who hate the interruption of pop-ups ads.\n\nC-Lab was founded in 2012 as Samsung\u2019s in-house idea incubator, and has since supported some 100 innovative projects. C-Lab and seven C-lab spinoff startups made a splash at this year\u2019s CES with gadgets such as portable directional speakers and visual aid eyeglasses.\n\nLast year, Samsung established an AI lab in Montreal, Canada in collaboration with AI Yoshua Bengio from University of Montreal. In partnership with China\u2019s Tencent, Samsung is currently developing a line of AI speakers which are expected to be released this year. The company says it intends to build AI into all its home appliances by 2020.\n\nUnlike rivals Google, Amazon, and other tech giants, Samsung has not avidly published AI papers. Instead, the company has so far progressed relatively cautiously in the space. But now it\u2019s up and riding the AI wave just like the rest, with plenty of money to do startup acquisitions; a huge, robust consumer electronics market; and legions of Samsung fans eagerly waiting for the next product release."
    },
    {
        "url": "https://medium.com/syncedreview/paige-ai-combats-cancer-with-ai-and-computational-pathology-5bdd8c6a1421?source=user_profile---------74----------------",
        "title": "Paige.AI Combats Cancer With AI and Computational Pathology",
        "text": "For decades pathologists have rendered their cancer diagnoses by performing a biopsy and examining a patient\u2019s tumour sample under a microscope. Now, an increasing number of top-tier pathologists are adopting artificial intelligence techniques to improve their cancer diagnoses.\n\nPaige.AI is a New York-based startup that fights cancer with AI. Launched last month, the company has an exclusive data license with the Memorial Sloan Kettering Cancer Center (MSK) \u2014 the largest cancer research institute in the US \u2014 which has a dataset of 25 million pathology cancer images (\u201cslides\u201d).\n\nTypically, a pathologist must invest a significant amount of time examining a patient\u2019s numerous tumour slides, each of which could be 10+ gigapixels when digitized at 40X magnification. Even the best pathologists can make a misdiagnosis, and it is not uncommon for professionals to disagree on diagnoses.\n\nThis is why computational pathology for cancer research has gained traction over the last ten years or so. The technology incorporates massive amounts of data, including pathology, radiology, clinical, molecular and lab tests; a computational model based on machine learning algorithms; and a visualized presentation interface that is understandable for both pathologists and patients.\n\n\u201cComputational pathology solutions will help streamline workflows in the future by screening situations that do not require a pathologist review,\u201d said Jeroen van der Laak, Associate Professor at Radboud University Medical Center, in an interview with Philips Healthcare.\n\nDr. Thomas Fuchs is the Director of Computational Pathology at MSK and an early pioneer in the theoretical study of computational pathology, He has many years of experience in the development and application of advanced machine learning and computer vision techniques for tackling large-scale computational pathology challenges.\n\nLast month Dr. Fuchs assumed an additional role as Founder and CEO of Paige.AI. He told Synced he believed the time was right to build Paige.AI because the requirements are all in place: scanners can deliver digital images with quality comparable to what pathologists see under the microscope; cancer centres scan some 40,000 pathology slides each month; and deep learning algorithms are well-suited for large-scale data.\n\nPaige.AI\u2019s technology is built on machine learning algorithms trained at petabyte-scale from tens of thousands of digital slides. Three models are utilized to solve different problems: convolutional neural networks for tasks such as image classification and segmentation, recurrent neural networks for information extraction from pathology reports, and generative adversarial networks to learn the underlying distribution of the unlabeled image data and to embed histology images in lower dimensional feature spaces.\n\nTech giants believe their frontier machine learning algorithms have huge a potential to revamp conventional diagnostic methodologies in the healthcare market, increasing accuracy and reducing costs. IBM has been using slides to train deep neural networks to detect tumours since 2016.\n\nGoogle, meanwhile, has released research on how deep learning can be applied to computational pathology by creating an automated detection algorithm to improve pathologists\u2019 workflow. Google successfully produced a tumor probability prediction heat map algorithm whose localization score (FROC) reached 89 percent, significantly outperforming pathologists\u2019 average score of 73 percent.\n\n\u201cCompanies like Microsoft and IBM are doing pathology, and in general, it is good for the whole field,\u201d says Dr. Fuchs, who also warned that tech companies unfamiliar with the healthcare sector might have a hard time. \u201cYou have to really understand the variety of workflows and the community, and where and how AI can help. Besides, as far as I know, all previous papers published were based on a very tiny data set. Increasing the dataset from a few hundred images to hundreds of thousands of images can make a huge difference.\u201d\n\nIn the short term, Paige.AI will provide pathologists with it\u2019s \u201cAI Module\u201d application suite, equipped with a dedicated physical slide viewer that can integrate with any microscope. The AI module targets prostate, breast and lung cancers and can perform tasks such as cancer detection, quantification of tumour percentages, and survival analysis.\n\nPaige.AI has already rolled out its product institution-wide at MSK, and aims to deliver disease-specific modules to pathologists later this year.\n\nPaige.AI\u2019s forte in algorithms and access to large-scale data attracted interest from Breyer Capital, which led a US$25 million Series A Funding Round for the company. Founder and CEO of Breyer Capital Jim Breyer, a venture capitalist renowned for his smart investments \u2014 most notably Facebook \u2014 wrote in a Medium blog, \u201cPaige.AI is poised to become a powerhouse in computational pathology and an undisputed leader among thousands of healthcare AI competitors.\u201d\n\nPaige.AI certainly does not intend to limit its output to slide viewers \u2014 the company aspires to reshape the entire diagnosis and treatment paradigm. \u201cWith Paige.AI, we can, for example, based on hundreds of thousands of slides, come up with a better grade because you can actually correlate so many patients with the outcomes. Then we compute that correlation, and of course, change how you grade patients and how and which medications are prescribed,\u201d says Dr. Fuchs.\n\nAlthough the road ahead for Paige.AI is bound to be challenging, especially as the company is still at a very early stage in its development, Dr. Fuchs is determined to raise his company to the forefront in AI implementation in healthcare, and their research is likely to spark further technological breakthroughs for computational pathology."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-launches-institute-of-quantum-computing-899454cbe1c5?source=user_profile---------75----------------",
        "title": "Baidu Launches Its Institute of Quantum Computing \u2013 SyncedReview \u2013",
        "text": "Baidu today announced the formation of its Institute of Quantum Computing. The Chinese search engine giant says this will be a first-class institute that will conduct applied research and development on software and information technologies and incorporate with existing businesses.\n\nThe institute will be led by Dr. Runyao Duan, a Professor and the Director of Quantum Computation Laboratory at the University of Technology Sydney and an Australian Research Council Future Fellow. Dr. Duan will report directly to Baidu President Yaqin Zhang.\n\nDr Duan is mainly focused on quantum information theory, quantum state/operation discrimination, quantum zero-error information theory, and measurement-based quantum computation. In 2016 he successfully gave a complete interpretation of the famous Lov\u00e1sz number with quantum information theory, thus solving the open issue that has challenged the field of information theory and graph theory since 1979.\n\nQuantum computing is believed to be the next-generation computing technology. Its promise of greatly improved speed compared to binary-based classical computing and its potential for supercharging artificial intelligence have made quantum computing something of a holy grail for global tech giants. To date, quantum computing has been applied mainly to large-scale data processing and computing problems, as well as network security services based on quantum encryption.\n\nBaidu is under pressure to accelerate its development of quantum computing. Last month, rival Alibaba launched an 11-qubit quantum computer, which is now available to the public on the Quantum Computing Cloud Platform. Alibaba was the second company to provide public cloud computing services with a processing power of 10+ qubits, following IBM\u2019s November release of 20-qubit quantum computers through its cloud service.\n\nGoogle also made a move towards quantum computing this week, introducing a preview of its newest 72-qubit quantum computer Bristlecone. Although Bristlecone is still in a very early stage, Google believes it will one day outperform classical computing on well-defined computer science problems, an achievement referred to as \u201cquantum supremacy.\u201d\n\nThe global race toward quantum computing is heating up. A relative newcomer in the field, Baidu is confident it can catch up with the competition, and founding a dedicated quantum computing institute with top experts is a huge step toward that goal."
    },
    {
        "url": "https://medium.com/syncedreview/fintech-surge-s-p-global-acquires-kensho-for-record-550-million-ant-financial-launches-pre-ipo-256f72b3627f?source=user_profile---------76----------------",
        "title": "Fintech Surge: S&P Global Acquires Kensho for Record $550 Million; Ant Financial Launches Pre-IPO\u2026",
        "text": "We\u2019ve become accustomed to tech giants shelling out millions and millions of dollars to snatch up promising AI startups. But yesterday \u2014 when the largest such acquisition ever was announced \u2014 the buyers were not from Silicon Valley. They were from Wall Street.\n\nS&P Global has signed an agreement to acquire Kensho, a Cambridge, MA-based AI company that uses data analytics and machine learning algorithms to find correlations and arbitrage opportunities in large-scale financial datasets. The company\u2019s holy grail is to make \u201ccomplex financial analysis as easy as a search on Google.\u201d\n\nThe Wall Street traded financial services corporation paid a staggering US$550 million for Kensho, making this the largest AI acquisition ever, topping Google\u2019s purchase of DeepMind, the London startup that produced the epoch-making Go computer AlphaGo.\n\nS&P Global values Kensho\u2019s forte in frontier AI technology and its close relationships with premium financial clients such as Citibank, Bank of America, Goldman Sachs and JP Morgan. Kensho also works with the US Government on national security issues. S&P Global was actually a Kensho client before the acquisition.\n\nThe acquisition further builds S&P Global\u2019s AI profile. Last year, S&P Global led a US$50 million Series B fundraising round for Kensho, and began a collaboration with it to develop financial products. Earlier this year, the company acquired another Cambridge-based AI company, Panjiva, which uses machine learning techniques in global supply chain management.\n\n\u201cKensho\u2019s capabilities are critical for S&P Global to be at the forefront of the technology transformation taking place within the financial markets, and will accelerate multiple commercial and efficiency opportunities,\u201d says S&P Global Chief Financial Officer Ewout Steenbergen. \u201cS&P Global\u2019s strategy is focused on strengthening our technology capabilities across the enterprise.\u201d\n\nAccording to an S&P Global press release, Kensho will continue to operate independently in Cambridge. Daniel Nadler, a Harvard graduate who founded Kensho in 2013, will lead a team of 120 employees, and report to Steenbergen.\n\nOver the last few years Wall Street has discovered how well AI can perform in managing financial portfolios, automated trading, online fraud detection detect, and predicting lending and insurance trends. Assets in quant funds, which employ AI for quantitative analysis, have grown to US$940 billion, a 86 percent increase since 2010, according to Bloomberg.\n\nOne can now hardly find a relevant bank or investment institution that is not using AI. In 2018, three-quarters of banks financial services companies will be either already employing or introducing AI technology in their operations, reports Greenwich Associates, a top market intelligence advisor to the \ufb01nancial services industry.\n\nChina\u2019s financial industry has also been aggressively embracing emerging technologies and AI for years, gestating the world\u2019s most valuable financial giant Ant Financial, an affiliate of China\u2019s Alibaba Group.\n\nAnt Financial officially launched its last round of financing yesterday in advance of an IPO anticipated this year or next. The Financial Times reports that the company is seeking US$5 billion, which would bring its value to US$100 billion.\n\nJust a month ago, Ant Financial\u2019s parent company Alibaba announced it would acquire a 33 percent interest in Ant Financial in exchange for Alibaba IP rights.\n\nTo date a wide range of digital financial service providers have spun out of Ant Financial: Alipay, Ant Fortune, Zhima Credit, MYbank, Yu\u2019e Bao, Ant Credit Pay (\u201cHuabei\u201d), Ant Cash Now (\u201cJiebei\u201d) and more; with the network reaching over 520 million users in China and the world.\n\nAnt Financial bills itself as an AI innovator, implementing machine learning in almost every corner of its ecosystem. The company\u2019s flagship financial intelligence platform Antzero develops reinforcement learning, deep learning and unsupervised learning solutions to revamp banking, insurance, and investment companies. Antzero\u2019s AI-driven car damage assessment system for example can detect vehicle damage and provide an estimated repair cost, while its struc2vec framework-based AI model delivers outstanding performance in malicious account detection.\n\nToday\u2019s announcements illustrate how both established and emerging fintech companies are finding and scaling solutions and growing their businesses by investing in AI. It\u2019s a trend that shows no signs of slowing down."
    },
    {
        "url": "https://medium.com/syncedreview/strengthening-ai-r-d-among-chinas-2018-innovation-goals-dee468e95abb?source=user_profile---------77----------------",
        "title": "Strengthening AI R&D Among China\u2019s 2018 Innovation Goals",
        "text": "In his address to the 13th National People\u2019s Congress (NPC) in Beijing on March 5, Chinese Premier Keqiang Li delivered this year\u2019s official government work report, identifying \u201creinforcing the new generation of artificial intelligence research and development\u201d among the state\u2019s innovation focuses for 2018.\n\n \n\nPremier Li provided a three-part overview of government work, including a review of the past five years, policy direction for this year\u2019s socioeconomic development, and suggestions for upcoming government work in 2018. The report\u2019s keywords include \u201cartificial intelligence\u201d along with \u201cagricultural supply-side structural reform,\u201d \u201cshared economy,\u201d \u201cdigital healthcare,\u201d and \u201cfintech.\u201d\n\nPremier Li noted that in the past five years China\u2019s GDP has increased from USD 8.64 trillion (CNY 54 trillion) to USD 13.2 trillion (CNY 82.7 trillion), with an average annual increase of 7.1%. National GDP now accounts for 15% of the global economy and more than 30% of global economic growth. There have also been major changes in economic structure, with high-tech manufacturing growing at an average annual rate of 11.7%.\n\n \n\nPremier Li said that in 2018 China will expedite upgrading \u201cfrom older to newer generation\u201d technologies, further deploy \u201cinternet+\u201d technologies, promote the widespread application of big data, cloud computing, and IoT.\u201d China will also continue its \u201cMade in China 2025\u201d initiative in order to \u201cstrengthen industrial infrastructure, smart and green manufacturing; and advance the speedy development of manufacturing.\u201d\n\n \n\nThree days before Premier Li\u2019s address, elected CPPCC members who are also AI industry leaders held a press conference.\n\n \n\nBaidu CEO Robin Li pledged, \u201cAI will drive China\u2019s economic development for the next 20\u201350 years.\u201d He proposed encouraging enterprises to opensource their AI platforms to strengthen the application of new technologies, and taking the lead on self-driving car policymaking. He also recommended the government introduce additional AI policy incentives\n\n \n\nSpeaking at the same press conference, Tencent CEO Pony Ma said \u201csince AlphaGo, people have come to realize the power of deep learning. Many deep learning applications are perfectly feasible now. Tencent\u2019s backends are supported by AI technologies like targeted advertising and personalized content recommendation, only users are not really aware of them.\u201d He added that \u201cartificial general intelligence may be far away, but there are many opportunities in vertical applications.\u201d \n\n \n\nLenovo Chairman and CEO Yang Yuanqing introduced a proposal to develop artificial intelligence for industrial applications, comparing AI to water droplets permeating all industries, and pointing out the importance of setting early benchmarks and building platforms.\n\nThe term \u201cAI\u201d first appeared in Chinese government statements in the July 2015 State Council document \u201cGuiding Opinions on Promoting \u2018Internet+\u2019 Initiatives.\u201d In March 2016 \u201cartificial intelligence\u201d appeared in the outline of the \u201c13th Five-Year Plan for Economic and Social Development\u201d.\n\nIn April 2016 Ministry of Industry and Information Technology, National Development and Reform Commission, and Ministry of Finance jointly issued the \u201cRobotic Industry Development Plan (2016\u20132020).\u201d The following month the National Development and Reform Commission, the Ministry of Science and Technology, the Ministry of Industry and Information Technology, and the Ministries and Commissions of the Central Network Information Office jointly released the \u201cInternet+ Artificial Intelligence Three Year Implementation Plan.\u201d \n\n \n\n\u201cArtificial intelligence\u201d also appeared in last year\u2019s official government work report, and last July the State Council formally issued the \u201cNew Generation of Artificial Intelligence Development Plan,\u201d lifting AI to the level of national strategy. \n\n \n\nEarlier this month the Chinese government released its \u201cArtificial Intelligence Standardization White Paper\u201d. The 98-page document was edited by the China Electronics Standardization Institute under the guidance of the National Standardization Management Committee Second Ministry of Industry. \n\n \n\nOn the same day as Premier Li\u2019s address, the University of Nanjing announced the formation of its new Artificial Intelligence Institute. This becomes the third such institute in the country, joining the Chinese Academy of Sciences and Xidian University."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-drops-lawsuit-against-jingchi-as-the-two-team-up-on-self-driving-tech-db589810af24?source=user_profile---------78----------------",
        "title": "Baidu Drops Lawsuit Against JingChi as the Two Team Up on Self-Driving Tech",
        "text": "The Baidu vs JingChi dispute \u2014 China\u2019s first ever lawsuit in the field of autonomous driving technology \u2014 has surprisingly ended up spawning a partnership between the two.\n\nBaidu announced today that self-driving startup JingChi has joined its open-source autonomous driving platform Apollo as a cooperative partner. JingChi\u2019s new CEO Tony Han said in a statement, \u201cJingChi is pleased to join Apollo, and is willing to grow fast with Apollo\u2019s help.\u201d\n\nJingChi\u2019s technology is appealing to Baidu. Founded last April, JingChi was one of 2017\u2019s fastest-growing self-driving startups, successfully completing its first open road autonomous driving testing in only 81 days.\n\nBaidu\u2019s Apollo is China\u2019s state-of-the-art autonomous driving platform. It will provide JingChi with open-source data, codes and tools. Launched last July, Apollo has already attracted over 80 partners, from car makers to self-driving technology providers.\n\nThe Baidu \u2014 JingChi partnership emerges from a unique situation to say the least. Last December, Baidu filed a lawsuit in Beijing IP Court accusing JingChi and its Founder and then-CEO Jin Wang \u2014 who was a Baidu Senior VP before he started the company \u2014 of infringement on its commercial secrets, not returning devices that contained classified information, and poaching technical personnel.\n\nJin Wang initially dismissed the accusation, saying \u201cBaidu\u2019s prosecution is completely unfounded and JingChi lawyers will respond factually and legally.\u201d However, two months later, JingChi released an official statement saying that Wang had left the company due to family issues.\n\nWang\u2019s departure is widely believed to be the event that defused the dispute between Baidu and JingChi, and accelerated the settlement. Chinese media is reporting that although Baidu has dropped its lawsuit against JingChi, it is still suing Wang.\n\n\u201cWang will not be directly or indirectly involved in or interfere with JingChi in any business dealings. JingChi has been fully prepared for the change, and will not be affected,\u201d said Han.\n\nThe landmark settlement between Baidu and JingChi has tangible effects. JingChi is spared from the legal dispute, and can turn its full attention back to business. The company\u2019s rapid growth might however be affected. An investor involved in JingChi fundraising said the recent dramas leave him needing more information and feedback before he can make an assessment on whether JingChi remains suitable for investment.\n\nBaidu meanwhile has consolidated its leading position in China\u2019s self-driving competition and asserted control over its IP. Over the last few years, Baidu has nurtured many AI talents who later left the company, founding startups such as Horizon.ai, a billion-dollar chip company, and Pony.ai, another rising self-driving startup which recently raised US$112 million.\n\nWith tech foes now becoming friends, China\u2019s self-driving development looks like it will emerge the biggest winner."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-feb-w-4-mar-w-1-4386d5a8d935?source=user_profile---------79----------------",
        "title": "AI Biweekly: 10 Bits from Feb W 4 \u2014 Mar W 1 \u2013 SyncedReview \u2013",
        "text": "Capillary Technologies is a cloud-based SaaS company serving over 300 large enterprises and 500 small or medium businesses in over 30 countries. It improves retailers\u2019 engagement with customers through analytics. The company announced it has raised US$20 million in funding to improve its products with AI, and plans to expand to consumer goods and enter the Southeast Asia market.\n\nFacebook is expanding its practice of flagging suicidal users to support more languages and Instagram. Facebook says the number of users who received suicide prevention support content from its compassion team has doubled and algorithms are flagging 20 times more posts related to suicide since the team was formed in 2015.\n\nFebruary 23rd \u2014 Microsoft and Xiaomi Collaborate in Cloud Computing, AI, and Hardware\n\nMicrosoft and Chinese smartphone manufacturer Xiaomi launch a collaboration in cloud computing, AI, and hardware through a new Strategic Framework Memorandum of Understanding (MoU). This is a win-win situation for both tech giants because the partnership will provide shared capabilities and channels.\n\nGoogle announces two new features for Google Assistant: routines and location-based reminders. The Google Home Smart Speaker will now be able to provide location-based reminders at any time via smartphone synchronization. The features also support combining commands to enable execution of multiple tasks with one command.\n\nThe number of languages Google Assistant can speak will jump from 8 to more than 30 this year, with the system eventually supporting more languages than Siri or Alexa. This should greatly benefit multilingual user environments and international customers.\n\nFebruary 25th \u2014 Amazon Designs Its Own AI Chips for Alexa\n\nAccording to a report from The Information, Amazon is designing its own chip for its virtual assistant Alexa, following competitors Google and Apple into the silicon business. In 2015 Amazon acquired chip maker Annapurna Labs to help build capabilities in this area.\n\nCalifornia is taking another major step forward in boosting autonomous driving technology with new permits that will allow manufacturers to test fully driverless vehicles on its roads. The California DMV can now issue three different permits based on specified requirements: testing with a safety driver, driverless testing, and deployment.\n\nIntel is collaborating with leading manufacturer of embedded computing devices AAEON Technologies to provide two possible production designs for developers. One is a mini-PCle module that features an Intel Movidius Myriad 2 VPU; the second is a manufacturing service for companies that require prototype customizations.\n\nIn its 2018 fiscal blueprint, the Canadian government announces a CDN$3.2 billion investment over five years to fund scientific research to stimulate economic growth. This represents the single largest investment in investigator-led fundamental research in Canadian history.\n\nMarch 1st \u2014 China Issues First Licenses to Test Driverless Cars in Shanghai\n\nChina issues licenses to Shanghai-based SAIC and startup automaker NIO, allowing them to test their autonomous driving vehicles on a 5.6 km stretch of public road in Shanghai\u2019s Jiading District. Shanghai plans to open more roads for smart car testing in the future."
    },
    {
        "url": "https://medium.com/syncedreview/ai-in-cyber-threat-detection-5b6fcebfc2e8?source=user_profile---------80----------------",
        "title": "AI in Cyber Threat Detection \u2013 SyncedReview \u2013",
        "text": "Cyber Threats have become a critical issue in today\u2019s world. Worldwide spending on information security climbed to over US$90 billion in 2017, a roughly 15% increase compared to 2016. Research and development of cyber threat detection and response capabilities will continue to grow this year.\n\nCyber attacks can be categorized according to what the attacker is targeting.\n\nPhishing: The attempt to fraudulently obtain sensitive information such as usernames, passwords, or credit card details by posing as a trustworthy entity in an electronic communication.\n\nRansomware: Malicious software that blocks access to the victim\u2019s data and threatens to publish or delete it unless a ransom is paid. The US Computer Emergency Readiness Team (US-CERT) reported an average of 4,000 daily ransomware attacks worldwide in 2016, a fourfold increase over 2015. More than one-quarter of these attacks were in the US.\n\nCross-site scripting: A web application vulnerability enables attackers to inject client-side scripts into web pages viewed by other users.\n\nSQL injection: An injection attack wherein an attacker can execute malicious SQL statements (also commonly referred to as a malicious payload) that control a web application\u2019s database server.\n\nBotnet: A network of private computers infected with malicious software and controlled as a group without the owners\u2019 knowledge, e.g., to send spam messages.\n\nDDoS: A type of DOS attack where multiple compromised systems, which are often infected with a Trojan, are used to target a single system causing a Denial of Service (DoS) attack.\n\nBy harnessing the power and potential of machine learning and deep learning technologies, cybercriminals are innovating faster than everybody could imagine. Although emerging technology-based attacks are still rare, some new types of threats can be identified.\n\nThe 2017 report \u201cGenerating Adversarial Malware Examples for Black-Box Attacks Based on GAN\u201d introduced the threat of machine learning being used for malware creation. The paper proposed a generative adversarial network based algorithm that could easily bypass a detection system. Last year, security company Endgame released research in which they were able to create a new type of malware by modifying Elon Musk\u2019s OpenAI framework. This malware could also easily fool security engines\u2019 defences.\n\nBy using a combination of machine learning, speech recognition and natural language processing (NLP), the quality of phishing emails or other smart attack techniques could become much more humanlike and effective. A paper released at security conference Black Hat USA 2017, \u201cWeaponizing data science for social engineering: Automated E2E spear phishing on Twitter,\u201d introduced a neural network framework that was able to fool 45% of users (in a random test of 90 users) with its targeted phishing tweets.\n\nSecurity Automation, which identifies potential cyber-security incidents by monitoring abnormal data use, is key in defending against cyber threats. AI and machine learning are powerful tools in the field of security automation, and can evolve the monitoring, prioritization and alert processes to the next generation to cut human labour costs and speed up threat management cycle time. Humans remain in the loop only for the purpose of identifying false positives.\n\nWith emerging technologies becoming more and more involved in cyber attacks, simply gathering data or creating digital signatures is no longer sufficient for fast threat detection. Introducing AI solutions allows the system to monitor a wider number of factors and better identify patterns of abnormal activity. By leveraging this data, AI and Machine learning can be trained to track information and deliver predictive analysis.\n\nMIT\u2019s AI2 System is able to work through raw data and leverage unsupervised machine-learning algorithms to detect abnormal information security activities. The system summarizes patterns and provides detailed information to security operators for further decision making. The decision records act as auto feedback to the core machine-learning model to improve its algorithm for future analysis. The AI2 system\u2019s AI-human fusion achieved an impressive cyber attack identification rate of nearly 86%.\n\nSentinel \u2014 Home Security\n\nThe home security company Deep Sentinel leverages deep learning algorithms for property-related safety concerns. The product combines algorithms and computer vision technologies to quickly analyze threat factors in raw video stream data. The company is also researching the use of autonomous drones and IoT device environmental data collection for security solutions. Deep Sentinel\u2019s products aim to leverage pre-trained systems to build a comprehensive home control platform.\n\nCloudflare released its Orbit IoT security solution in 2017. Obert is an IoT security solution that enables IoT device manufacturers to connect their products to Cloudflare\u2019s network automatically, providing users with a machine-learning based API to monitor for suspicious activities."
    },
    {
        "url": "https://medium.com/syncedreview/china-to-facilitate-ai-unicorns-going-public-80964c8016?source=user_profile---------81----------------",
        "title": "China to Facilitate AI Unicorns\u2019 Going Public \u2013 SyncedReview \u2013",
        "text": "The Chinese government is going to make it much easier for artificial intelligence unicorns to go public on its stock exchanges.\n\nChinese media is reporting that several domestic brokers and investment banks were recently informed by the China Securities Regulatory Commission (CSRC) that it will relax regulations on prospectus approval time and profitability minimums for Chinese unicorns in the domains of artificial intelligence, cloud computing, biotech, and high-end manufacturing.\n\nA Chinese AI unicorn C-level executive who asked not to be identified told Synced he believed many AI startups \u2014 particularly those in computer vision such as SenseTime, Face++, CloudWalk, and Yitu \u2014 will benefit from the loosened rules and soon go public.\n\nThe CSRC has not yet publicly commented on the news reports, nor has it released any official statements regarding any changes to its current regulations.\n\nThe new regulations are expected to shorten prospectus approval time from an average of eight months to just two or three months, while also lowering profitability minimums for qualified young Chinese AI unicorns.\n\nUntil now, in order to go public, Chinese companies were required to show a positive net profit for each of the last two to three fiscal years, and have a cumulative net profit for the period of not less than CNY\uffe510 to 30 million.\n\nChina AI unicorns\u2019 variable interest entities (VIE) \u2014 wherein an investor holds a controlling interest that is not based on share ownership \u2014 may no longer be an obstacle to their going public.\n\nMany global tech startups are now going public, a trend that is expected to break out through 2018 and 2019. Sweden-based music streaming service Spotify recently filed direct listings on the New York Exchange, valued at US$20 billion. China\u2019s Netflix-like video streaming platform IQIYI and its high-end electric SUV maker NIO are also working on US stock market listings this year. Uber and Airbnb are expected to go public in 2019.\n\nChina is eager to embrace its next-generation tech innovators and reform its IPO procedures. Earlier this year, the CSRC released a statement saying it is a vital \u201cto absorb the mature and effective systems and methods from international capital markets, and reform the system of issuance and listing.\u201d On February 9, the Shenzhen Stock Exchange issued its Strategic Plan for Development (2018\u20132020), shining a spotlight on unicorns in emerging industries.\n\nChina\u2019s official press agency Xinhua News recently published the article \u201cTo Achieve a BATJ [Baidu, Alibaba, Tencent, JD.com] Dream in China\u2019s Capital Market,\u201d which urged institutions to clear hurdles for local tech companies entering China\u2019s capital market.\n\nLast month, the world\u2019s largest contract electronics manufacturer Foxconn filed an IPO on the Shanghai Exchange to raise US$4.5 billion. It took Foxconn less than a month to go from prospectus filing to pre-disclosure update, a process that usually takes eight months. The company is expected to get its prospectus approval this March."
    },
    {
        "url": "https://medium.com/syncedreview/trashbots-boost-recycling-at-pittsburgh-airport-258004877bcd?source=user_profile---------82----------------",
        "title": "\u201cTrashbots\u201d Boost Recycling at Pittsburgh Airport \u2013 SyncedReview \u2013",
        "text": "There are more than eight million travelers passing through Pittsburgh International Airport each year, leaving behind some 2,000 tons of trash. Pittsburgh is a major American robotics hub, and now its airport has introduced an appropriately automated recycling solution: the \u201cTrashbot.\u201d\n\nTrashbot uses cameras and sensors to scan discarded items. A machine learning algorithm based on Bayesian classifier sorts the trash for either landfill or recycle. If you dump an unfinished can of Coke, excess liquid will be drained off on a Teflon-coated sheet. The robot swallows garbage at the rate of around three seconds per item, with an 81% sorting accuracy rate, and continuously learns to recognize new types of trash. It is backed by a waste auditing dashboard based on Amazon Cloud Services, which presents data on types and quantities of refuse.\n\nCleanRobotics CEO Charles Yhap and VP of Engineering Tanner Cook are social entrepreneurs. Cook regards trash as an important social issue, second only to energy, telling Synced that \u201cdue to contamination and confusion, only 20% of what goes into a recycling bin ends up being recycled.\u201d\n\nThe team took two years to go from a wooden prototype at Pittsburgh\u2019s AlphaLab Gear Startup Accelerator to product. Cook says the hardest part of building the startup was securing money to cover the initial R&D costs, not the 60\u201390 hours a week of work he put into the project.\n\nWhat about cost? \u201cTrashbot is comparable to what\u2019s on the market, we\u2019re looking at US$1,500 to 5,000 per can,\u201d says Cook. For example, Denver international airport\u2019s current trashcan cost is $3000, and Cook says his company could replace those for the same price. Clients also need to pay a small monthly subscription to use the data dashboard via Amazon Web Services.\n\nCleanRobotics targets high-traffic locations such as airports, convention centers, malls, and schools. And although public facilities can be reluctant to adopt new technologies, Cook says many are actually reaching out to CleanRobotics.\n\n2014 data shows a recycling rate of 35% in the United States, with numbers varying drastically across the country. California has the best recycling programs, and San Francisco tops the city list with an 80% success rate. Up north, Portland Oregon follows at 70%. The West Coast\u2019s recycling records and proactive policies make the region an attractive market for AI-powered fast-track recycling solutions.\n\nOne of the biggest challenges facing urban recycling programs is the costs involved, particularly in sorting the trash to extract what\u2019s worth recycling. A study on municipal recycling argues that recycling plastic and aluminum can actually be profitable for cities, while it is less cost-effective to recycle glass and paper.\n\nAlthough it will be difficult to implement recycling procedures if they increase operating costs, many jurisdictions are offering tax credits to encourage recycling. Says Cook, \u201cin Australia, it costs $350 per ton of landfill, versus getting a tax credit for recycling. When you have a facility that\u2019s going through 10,000 tons of trash every year, it makes sense for them to try and divert as much of it into that high-quality recycling space as possible.\u201d Trashbot is already onsite at a Sydney shopping mall.\n\nThe CleanRobotics team is continuing to use artificial intelligence and robotics to explore niche challenges and develop new product prototypes for waste management, and Cook believes the best is yet to come."
    },
    {
        "url": "https://medium.com/syncedreview/datavisor-uses-unsupervised-learning-to-combat-online-fraud-3bd9c925b898?source=user_profile---------83----------------",
        "title": "DataVisor Uses Unsupervised Learning to Combat Online Fraud",
        "text": "From millions of fake accounts spreading across social media to credit hackers stealing from bank accounts, online fraud has become a nightmare for Internet users. As Internet companies redouble their efforts to detect and foil emerging fraud techniques, many are turning to artificial intelligence for solutions.\n\nDataVisor is a Mountain View, California based anti-fraud startup that uses AI to detect fake accounts, prevent money laundering, and protect financial institutions from credit scams. The company offers clients its DataVisor APIs for real-time data connection, and a specialized UI for direct results.\n\nDataVisor Co-founders Yinglian Xie and Fang Yu were both Microsoft Senior Researchers for seven years, using data-driven approaches to solve the online service security challenges Internet companies were wrestling with.\n\nThe market for digital anti-fraud services is growing as transactions increasingly go online. Global non-cash transaction volume grew 11.2% in 2015 to reach 433.1 billion, the highest growth rate of the past decade, according to the World Payments Report 2017. Online fraud is also moving beyond the financial services sector to become a universal problem, also affecting for example gaming and social media.\n\nConventional anti-fraud approaches which are largely grounded in credit investigation and hand-tuned rules can no longer handle the scale of the problem. Fraud detection companies have been adopting AI \u2014 particularly supervised learning \u2014 to automate their fraud detection processes using sophisticated algorithms, vast amount of data, and robust computing capabilities. Trained with historic labeled data that distinguishes real users from fraudulent accounts, supervised learning models can efficiently classify new data and detect suspicious activity.\n\nYet supervised learning struggles when faced with rapidly variational fraud techniques. Says Xie, \u201cNowadays, online fraud can change in 24 hours. The characteristic of this area is that we are faced with a constantly changing fraudster, so it is difficult to get enough historical labeled data, limiting the effectiveness of supervised learning.\u201d\n\nThe unpredictability of online fraud schemes raises another challenge for supervised learning. Users can potentially be defrauded while using any of a platform\u2019s features, or as Xie suggests at any time during a normal \u201cuser life cycle.\u201d This could involve registration, payment, comments, or even app installation. The more features there are in a user life cycle, the more likely that user will get scammed.\n\n\u201cWe have helped IGG, a renowned video game publisher and developer, to combat game installation fraud. I barely noticed this problem before, but then realized it could be a big issue in the game industry,\u201d says Xie.\n\nAn increasing number of fraud detection companies like DataVisor are now employing unsupervised machine learning (UML), a type of machine learning algorithm that clusters unlabeled data by discovering hidden patterns. Madrid-based online financial institute Openbank for example uses UML algorithms to detect fraud and money laundering.\n\nDataVisor\u2019s fraud detection solution has three components: a UML Engine to cluster results, an Automated Rules Engine to replace time-consuming manual rule-making, and a Global Intelligence Network to gather vast amounts of information and domain knowledge.\n\nThe company\u2019s flagship is its UML Engine, which DataVisor bills as the first proven UML solution capable of handling vast volumes of data and discovering patterns from accounts and events in real-time.\n\n\u201cMost papers around unsupervised machine learning are based on very small datasets, and no one has been very successful at applying advanced unsupervised learning algorithms to large-scale data. The challenge is difficult,\u201d says Xie.\n\nAccording to a company white paper, DataVisor\u2019s UML Engine uses techniques such as natural language processing, image metadata analysis and graph analysis to extract features such as profile info, behaviors and activities, comments and metadata etc. from both structured data and unstructured data.\n\nThese features will be then grouped into clustered results with the important feature dimensions and distance functions selected. DataVisor says its UML Engine algorithms provide a more efficient and effective solution than common methods of dimensionality reduction, such as Principal Component Analysis (PCA). The Engine also deploys graph analysis and supervised machine learning algorithms to improve accuracy.\n\nThe last step is to rank the detected accounts, assign them confidence scores, and categorize a collection of malicious accounts with similar features, also known as \u201cattack rings.\u201d\n\nThe DataVisor white paper reports the UML Engine has been in use for over three years \u2014 with a 90\u201399% detection accuracy \u2014 and has that DataVisor has protected over two billion user accounts for large enterprises such as Yelp, Pinterest, Fortune 500 banks, IGG, Toutiao, etc., showing impressive growth for an early-stage startup.\n\nDataVisor is now looking to expand. The company raised US$40 million last month, led by Sequoia Capital China along with existing investors New Enterprise Associates (NEA) and GSR Ventures.\n\n\u201cWe believe a company driven by algorithms and data, such as DataVisor, will be very competitive in the future. Because of some characteristics of the anti-fraud industry, we think the barriers will be high, and there will be a \u2018Matthew effect\u2019 wherein the rich get richer and the poor get poorer,\u201d says Rock Wang, Managing Director of Sequoia China.\n\nThe anti-fraud industry is still in its early stages of development, and while there is a long road ahead for DataVisor, the company believes its detection techniques and services will stay ahead of emerging online fraud schemes. \u201cThe development of technology is endless. I think we can still move forward one step further.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/mit-and-sensetime-become-allies-in-ai-44a21aea7ac2?source=user_profile---------84----------------",
        "title": "MIT and SenseTime Become Allies in AI \u2013 SyncedReview \u2013",
        "text": "The Massachusetts Institute of Technology (MIT) and Chinese AI Unicorn SenseTime today announced the MIT-SenseTime Alliance on Artificial Intelligence, a partnership the duo says \u201caims to open up new avenues of discovery across MIT in areas such as computer vision, human-intelligence-inspired algorithms, medical imaging, and robotics.\u201d The partnership is expected to develop 30\u201350 AI projects each year, and will be part of the MIT Intelligence Quest (MIT IQ) project launched earlier this month.\n\nSenseTime leverages facial recognition technologies powered by deep learning trained computer vision systems, along with video analysis, text recognition and autonomous driving technologies to provide business solutions.\n\nSenseTime has applied AI solutions to 400 companies in multiple industries including automobile, finance, mobile internet, robotics, security, and smartphones. It has scored mega-clients like China Mobile, China UnionPay, Huawei Technologies Co, Xiaomi. and JD.com, and a strategic partnership with Nvidia.\n\nCompany founder Dr. Xiao\u2019ou Tang is a Professor of Information Engineering at the Chinese University of Hong Kong and an MIT alumnus. He conducted his PhD research on applying computer vision to the study and classification of underwater imagery under the supervision of Dr. W. Eric L. Grimson.\n\nDr. Grimson is MIT\u2019s Chancellor for Academic Advancement, and has been their Bernard M. Gordon Professor of Medical Engineering for 24 years. Commenting on the new partnership with SenseTime, he had this to say about his former student: \u201cXiao\u2019ou has become well known throughout China and the world as a leader in the field of AI, especially computer vision and deep learning. I personally am proud of Xiao\u2019ou\u2019s success and the impact he is making on the world, and look forward to a deepened, mutually beneficial relationship between MIT and SenseTime.\u201d\n\nIn late 2016 SenseTime raised US$120 million in funding, led by Beijing-based CDH Investments, Dalian Wanda Group, IDG Capital Partners and Star VC. Some six months later the company closed its Series B funding with US$410 million from CDH and Sailing Capital, which propelled it onto CB Insights\u2019 2017 technology unicorn list. SenseTime has received further strategic investments from Alibaba Group and Qualcomm. The company is sitting on a market valuation of US$3 billion and preparing for an IPO.\n\nIn recent years, MIT has been in an increasingly fierce AI tech competition with other US universities such as Stanford and Carnegie Mellon. In 2017 the latter launched its CMU AI initiative, knitting together more than 100 faculty members and 1,000 students.\n\nMIT clusters more than 200 principal AI investigators in its Computer Science and Artificial Intelligence Laboratory, MIT Media Lab, Department of Brain and Cognitive Sciences, Center for Brains, Minds and Machines, and MIT Institute for Data, Systems, and Society.\n\nMIT IQ, launched on Feb 1, is led by MIT President L. Rafael Reif along with Dr. Anantha Chandrakasan from MIT\u2019s Electrical Engineering and Computer Science department, Dr. Daniele Rus, Dr. Dina Katabi, Dr. James DiCarlo, and Dr. Josh Tenenbaum. The project envisions MIT\u2019s future AI endeavours in two parts: \u201cThe Core,\u201d which will focus on fundamental algorithm research; and \u201cThe Bridge,\u201d which will provide the MIT community with AI education, support, hardware, and resources.\n\nAs AI evolves from research labs into practical applications, this partnership between a top-notch university and an experienced solution provider is bound to unlock new tech breakthroughs and encourage additional strategic collaborations in the global AI race."
    },
    {
        "url": "https://medium.com/syncedreview/pandas-and-ai-lead-the-way-to-the-beijing-2022-winter-olympics-fd58a5b1dceb?source=user_profile---------85----------------",
        "title": "Pandas and AI Lead the Way to the Beijing 2022 Winter Olympics",
        "text": "The Pyeongchang 2018 Winter Olympics\u2019 closing ceremony on Sunday was capped by a spectacular eight-minute presentation from 2022 host Beijing that featured state-of-the-art robotics and AI technology.\n\nBeijing celebrated the transfer of the Olympic flame with a performance by 22 skaters led by a couple of panda \u201ccaptains,\u201d supported by an array of 24 AI-controlled robot-screens patterned after the Great Wall. Skaters glided across projections of traditional Chinese motifs such as knotting, dragons and phoenixes on the ice surface, while images of China\u2019s latest tech achievements such as high-speed trains, aircraft and spacecraft were displayed overhead on the robot-screens.\n\nDirector Yimou Zhang is renowned for his extravagant Beijing 2008 Olympic opening and closing ceremonies, as well as the summer torch handover ceremony in Athens in 2004. Nearly ten years later, Zhang has replaced the hundreds of actors with robot substitutes, and applied AI-powered visual media to the ceremony for the first time in Olympic history.\n\nZhang, who contemplated the theme for an entire year and spent two months preparing and choreographing the show, says two characteristics define this year\u2019s presentation: \u201cOne is the joint performance of AI and actors, and second is using the internet to interact with a Chinese audience.\u201d\n\nChina is determined to put AI on its national name card, and Zhang says that by linking China with AI in the ceremony \u201cwe are sending an invitation to the world, at the same time demonstrating China\u2019s new image in the new era.\u201d\n\nLast month, sharp-eyed viewers identified a couple of interesting books on President Jinping Xi\u2019s bookshelf during his 2018 New Year\u2019s address: Pedro Domingos\u2019 The Master Algorithm and Brett King\u2019s Augmented: Life in the Smart Lane.\n\nIn an official report released during the 19th National Congress of the Communist Party of China, Xi pledged that the Chinese government will \u201cpromote further integration of the internet, big data, and artificial intelligence with the real economy, and foster new growth areas and drivers of growth in medium-high end consumption, innovation-driven development, the green and low-carbon economy, the sharing economy, modern supply chains, and human capital services.\u201d\n\nThe Chinese government also presented its blueprint for further AI integration with the economy, following up with an intensive review on \u201cStandardization of AI Helps Industry Development\u201d and a \u201cThree-Year Action Plan for Promoting the Development of New Generation of AI Industry\u201d.\n\nThe Olympic Games with their global audience provided the perfect venue to showcase this new national endeavour, and China did it beautifully."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-self-driving-startup-ceo-suddenly-steps-down-2cac2b922d2b?source=user_profile---------86----------------",
        "title": "Chinese Self-Driving Startup CEO Suddenly Steps Down",
        "text": "Jin Wang, CEO of China\u2019s self-driving startup JingChi, has stepped down just 11 months after founding the company.\n\nJingChi said in a statement that \u201cJin Wang left the company due to family issues. Despite that, the company remains unchanged.\u201d JingChi CTO Tony Han, the rumored pick to replace Wang as CEO, has not yet responded to Synced\u2019s request for comment.\n\nA CEO exiting a months-old startup is barely seen in the tech world. Wang\u2019s departure is widely rumoured to be related to JingChi\u2019s dispute with Baidu, which sued the startup two months ago for allegedly stealing its self-driving technology.\n\nThe lawsuit echoes the 2017 Waymo vs Uber dispute in which Waymo accused Uber of stealing its LiDar technology. That case was recently resolved, with Uber paying Waymo a settlement of US$245 million in stock options.\n\nBefore Wang started JingChi he was a Baidu senior vice president. He co-founded Baidu\u2019s autonomous driving unit, which spawned the open source autonomous driving development platform Apollo.\n\nLast March, Wang left Baidu and co-founded JingChi in Sunnyvale, California. The company successfully completed its first open road autonomous driving testing in only 81 days, and put a prototype through Silicon Valley rush hour traffic two months later.\n\nThe huge demand for autonomous driving services in China prompted JingChi to move its headquarters to Guangzhou last December. Wang concluded an agreement with Guangzhou Development District to deploy a L4 self-driving ride-hauling service by the end of 2018. The company also promised to deliver 500 to 1000 autonomous driving commercial vehicles through 2018.\n\nJingChi\u2019s fast tech development and ambitious plans attracted interest from Nvidia, who joined a group of investors led by Chinese venture capital firm Qiming Venture in JingChi\u2019s Series Pre A Funding Round, which raised US$52 million.\n\nHowever, JingChi\u2019s rapid rise was interrupted last December when Baidu filed a suit accusing JingChi of infringement on its commercial secrets, not returning devices that contained classified information, and poaching technical personnel. Baidu sought compensation of USD$7.6 million, and a JingChi promise to discontinue use of the company\u2019s proprietary information or technologies relating to autonomous driving.\n\nWang initially dismissed the accusations: \u201cAs an innovative start-up, we are not afraid of competitors, so the challenge will not delay our pace of development\u2026 Baidu\u2019s prosecution is completely unfounded and JingChi lawyers will respond factually and legally.\u201d The lawsuit is ongoing in Beijing Intellectual Property Court.\n\nAt this point it\u2019s still too early to say how Wang\u2019s departure will affect either the lawsuit or the company\u2019s tech development and overall health. Many questions remain unanswered, especially: What will be JingChi\u2019s next move? Synced is covering the story and will continue to update readers with the latest news."
    },
    {
        "url": "https://medium.com/syncedreview/criteria-for-building-a-successful-ai-chatbot-d7d00dd360ad?source=user_profile---------87----------------",
        "title": "Criteria for Building a Successful AI Chatbot \u2013 SyncedReview \u2013",
        "text": "A Chatbot is information service interface done by a computer program, sometimes with the help of artificial intelligence (AI). Chatbot applications range from functional to fun, and have been growing quickly both in sophistication and popularity.\n\nChatbot-based applications have already impacted a variety of industries:\n\nThe Chatbot is becoming a major application across multiple industries and use cases, and many global brands are heavily invested in Chatbot technology.\n\nAlthough Chatbots are now widely used, their core technologies are still in the early stage of application and some Chatbots fail to meet Users\u2019 expectations. Reasons for unfavourable user experience can be categorized:\n\nWhy a Chatbot?\n\n \n\n The Chatbot is not a universal solution for every user interaction interface. To build a successful Chatbot, the first step is to consider \u201cWhy a Chatbot?\u201d This consideration should include conversion friction, usability, ability to get the job done, efficiency, and ecosystem maturity. By exploring these considerations you will able to decide whether to use a Chatbot or Native App for your core business solution. The chart below compares Chatbot, Native App, and Mobile Webpage interfaces.\n\nWhat can users do with your Chatbot?\n\n \n\n You will need to think from the user\u2019s point of view. The Chatbot follows different design principals to provide effective service in various use cases. For example, a banking assistant type would have an advanced banking FAQ database to deliver smart responses, while a shopping assistant type would have an enhanced short memory function, allowing the user to retrieve previous information. \n\n \n\n \n\n How to manage user expectations?\n\n \n\n User expectations of a Chatbot can be understood from the point of view of scale, convenience, engagement, personalization, and natural interaction.\n\nWhat are the intelligence and cost requirements?\n\n \n\n The level of intelligence and cost totally depend on your core business objective. You can evaluate this through industry vertical, level of interaction and complexity factors.\n\nBeyond these three main variables, you should also look at factors such as number of platforms, technical infrastructure and custom integrations. A standard Chatbot will almost always be much cheaper than a customized Chatbot.\n\nNowadays it\u2019s hard to deny the great capabilities that Chatbots are bringing to our daily lives, and the many possibilities they hold for the future. Answering the above four questions will help you identify whether an existing third-party Chatbot aligns with your needs, or set the scope for developing your own. Then you will be ready to take the next step to building a successful Chatbot."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-launches-11-qubit-quantum-computing-cloud-service-ad7f8e02cc8?source=user_profile---------88----------------",
        "title": "Alibaba Launches 11-Qubit Quantum Computing Cloud Service",
        "text": "Quantum computing is a buzzword in the tech world. Its promise of greatly improved speed compared to binary-based classical computing and its potential for supercharging artificial intelligence have made quantum computing something of a holy grail for global tech giants.\n\nAlibaba took a big step towards quantum computing yesterday when its cloud service subsidiary Aliyun (\u201cAlibaba Cloud\u201d) and the Chinese Academy of Sciences jointly launched an 11-qubit quantum computing service, which is available to the public on the Quantum Computing Cloud Platform.\n\nA Qubit, or quantum bit, is a unit of quantum information, the quantum equivalent of a classical binary bit. Qubits can be in superpositions of states, so they are not limited by binary code\u2019s two definite states (0 or 1). The properties of qubits make quantum computing in theory capable of storing more information and processing data exponentially faster.\n\nAlibaba becomes the second company to provide public cloud computing services with processing power of 10+ qubits. Last November IBM released 20-qubit quantum computers through its cloud service.\n\nAliyun encourages users, especially researchers, to run algorithms on Alibaba\u2019s quantum computers to conduct preliminary experiments. This will help detect technical bottlenecks, optimize user experience, and develop next-generation processors.\n\n\u201cThis launch means that Aliyun\u2019s quantum computing researchers can more easily experiment with real-world processors to help them understand the hardware and lead the development of quantum tools while fueling in continuous improvements from the client experience,\u201d says Dr. Yaoyun Shi, Chief Scientist of Quantum Technology at Aliyun.\n\nThe Chinese e-commerce giant accelerated its development of quantum computing in 2015 when Aliyun and the Chinese Academy of Science jointly developed the Alibaba Quantum Laboratory (AQL). The lab has since acquired a number of well-known scientists, such as Dr. Shi from the University of Michigan, and two-time G\u00f6del Prize winner Dr. Mario Szegedy.\n\nAQL has released an ambitious 15-year roadmap. By 2025, it expects to have built quantum computers that will be the world\u2019s fastest by today\u2019s measure. By 2030, AQL hopes to achieve a general quantum computing prototype with 50\u2013100 qubits.\n\nAliyun is also offering a new 32-qubit quantum computer simulation service. By comparing simulated experiment results with real results on quantum computers, users can measure the latter\u2019s performance, verify correctness, etc.\n\nRenowned Chinese physicist Dr. Guangcan Guo also announced yesterday that his team at the Chinese Academy of Sciences has simulated a 64-qubit quantum computer on a conventional supercomputer, outperforming the simulated 56-qubit quantum computers IBM released last year.\n\nThe global quantum computing race is heating up. Not to be outdone by IBM or Alibaba, Google researchers last year released a paper demonstrating how 50-qubit quantum computers can outperform classical computers, also known as Quantum Supremacy. Google also plans to add D-Wave quantum computers to its public cloud.\n\nMeanwhile Microsoft, which has been investing in quantum technology since 1997, released a quantum computing development kit preview last December, which includes tools developers need to get started with quantum computing.\n\nAlibaba sees quantum computing as a revolutionary technology that will spark scientific breakthroughs. The company that was until recently barely noticeable in the field of fundamental research is now eager to get in the game and take the lead."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-startups-hauled-in-half-of-2017-global-ai-funding-49bd97ef3746?source=user_profile---------89----------------",
        "title": "Chinese Startups Hauled In Half of 2017 Global AI Funding",
        "text": "Over US$12.5 billion in funding flowed into AI startups in 2017, with Chinese startups receiving 48 percent of that, according to the new CB Insights report The State of Artificial Intelligence. China overtook the US (38 percent) for the first time, as its AI startups wowed the world with a yearlong series of huge funding rounds.\n\nMuch of the money went to computer vision (CV) technologies, with 41 disclosed deals and US$1.6 billion in disclosed funding. Face++ topped the heap when its Series C round attracted a record-setting US$460 million, while other CV companies like SenseTime (US$410 million) and CloudWalk (US$375 million) also did extremely well. Such companies brought their machine learning technologies to China\u2019s US$15 billion security camera market or the rapidly growing facial recognition payment market.\n\nCV technology also captured attention from non-CV companies such as Chinese speech and voice technology giant iFlytek, which has won multiple object detection and self-driving competitions. In November iFlytek released a medical assistant robot for hospitals to improve diagnosis accuracy.\n\nAlong with the computer vision startups, Chinese computer chip companies also had a great 2017. Cambricon and Horizon \u2014 which develop machine learning processors to power AI applications on smart devices, self-driving vehicles, and smart cities \u2014 raised US$100 million each. Cambricon\u2019s AI computation-boosting neural processing unit is inside Huawei\u2019s new flagship smartphone Mate 10.\n\nChinese startups are also becoming increasingly attractive to US investors. CB Insights reported 20 US-backed equity deals with Chinese startups last year, double the 2016 total. As more investors pump more money into innovative Chinese AI startups, those startups are bound to bring even more innovations in 2018."
    },
    {
        "url": "https://medium.com/syncedreview/ai-adds-colour-to-grandmas-cherished-memories-46d639d47f94?source=user_profile---------90----------------",
        "title": "AI Adds Colour to Grandma\u2019s Cherished Memories \u2013 SyncedReview \u2013",
        "text": "After every Lunar New Year celebration, as the kids prepare to head back to the big city, grandmothers across China take their old tin of family photos out of the cupboard for a nostalgic trip down memory lane. Alas, these beloved old photographs are faded, torn, and monochrome.\n\nTencent Youtu \u2014 literally translated as \u201cimage optimization lab\u201d \u2014 is the image processing, pattern recognition, machine learning, and data mining research arm of Chinese tech giant Tencent Group. Youtu is now offering a free retouching service for old photographs. Upload a scanned copy of your grandparents\u2019 wedding picture on WeChat\u2019s H5 built-in interactive pages, and Tencent\u2019s AI will bathe the scene in natural colours.\n\nThe process begins with the AI detecting whether it is actually dealing with a monochrome photo. The team tells us many apparently black and white photographs are not truly monochromatic due to yellowing, molding, staining, and so forth. Colour contamination can also be also introduced during the scanning process if the scanner\u2019s background is coloured.\n\nThe Youtu team analyzed millions of public stock photos and trained their algorithm to learn in multiple colorimetric spaces. The team separates the image based the HSV (Hue, Saturation, Value) colour model, which helps the algorithm identify moldy and damaged areas of sparse colour distributions precisely while ignoring greyish border areas. The team\u2019s internal testing shows 95% image recognition accuracy.\n\nThe colouring process works as follows: the AI will tag different figures and objects in the photo. A trained neural network will then learn to \u201cunderstand\u201d tagged parts using semantic memories like \u201csky\u201d, \u201cgrass\u201d, \u201cbuilding\u201d, and \u201cface\u201d, and finish the retouching with correct colour matches. AI-coloured images don\u2019t restore the original colours, rather the algorithm is trained to come up with reasonable colour scenarios.\n\nWhat if you have a old faded coloured photograph to begin with? Youtu\u2019s AI can restore a colour photo by comparing it with similar images, then adjusting image intensities to enhance contrast using a technique called \u201chistogram equalization.\u201d\n\nRichard Zhang\u2019s team at University of California at Berkeley first offered the public an automatic photo colouring app in a project titled Interactive Deep Colorization, which is available for download on GitHub. Zhang\u2019s 2016 paper Colourful Image Colorization further explains the network. For the curious, a history of automatic colouring AI can be traced back to Zezhou Cheng et al.\u2019s 2015 work on Deep Colorization.\n\nTencent\u2019s Youtu AI lab making the technology accessible to Chinese netizens is part of a developing \u201cfun AI in 2018\u201d trend, as Tencent\u2019s H5 built-in apps make big bang developments. Last November, Tencent launched a built-in \u201cAI experience center\u201d that includes many fun and handy AI applications. The company also recently open-sourced WeChat\u2019s built-in gaming and search functions to commercial developers."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-ai-can-clone-your-voice-in-seconds-93558a7b984f?source=user_profile---------91----------------",
        "title": "Baidu AI Can Clone Your Voice in Seconds \u2013 SyncedReview \u2013",
        "text": "Baidu\u2019s research arm announced yesterday that its 2017 text-to-speech (TTS) system Deep Voice has learned how to imitate a person\u2019s voice using a mere three seconds of voice sample data.\n\nThe technique, known as voice cloning, could be used to personalize virtual assistants such as Apple\u2019s Siri, Google Assistant, Amazon Alexa; and Baidu\u2019s Mandarin virtual assistant platform DuerOS, which supports 50 million devices in China with human-machine conversational interfaces.\n\nIn healthcare, voice cloning has helped patients who lost their voices by building a duplicate. Voice cloning may even find traction in the entertainment industry and in social media as a tool for satirists.\n\nBaidu researchers implemented two approaches: speaker adaption and speaker encoding. Both deliver good performance with minimal audio input data, and can be integrated into a multi-speaker generative model in the Deep Voice system with speaker embeddings without degrading quality.\n\nSpeaker adaption is a backpropagation-based approach grounded in a multi-speaker generative model or adapted to only low-dimensional speaker embeddings. Speaker encoding meanwhile combines the multi-speaker generative model with a separate model that generates a new speaker embedding from cloned audio. This approach shortens cloning time to just a few seconds and requires a low number of parameters to represent each speaker, making it favorable for low-resource deployment.\n\nBaidu has released multiple three-second cloned audio clips which track the process from original voices to synthesized voices that are strikingly similar.\n\nBaidu is upbeat about the possibilities in the field of voice cloning research. For example, advances in meta-learning, a systematic approach of learning-to-learn, could significantly boost voice cloning quality.\n\nBaidu is not the only institute working on imitating human voices with AI. Google\u2019s DeepMind, which produced the epoch-making Go computer AlphaGo, introduced its TTS project WaveNet in 2016. The system models audio waveforms from real human voices and produces convincingly natural simulations. Adobe also unveiled a prototype software called Project VoCo that can learn to mimic a voice in 20 minutes. Last year, Montreal-based startup Lyrebird pushed voice cloning technology to the next level with a TTS system that required only a 60-second audio sample input to deliver \u201ca digital voice that sounds like you.\u201d\n\nThe recent breakthroughs in synthesizing human voices have also raised concerns. AI could potentially downgrade voice identity in real life or with security systems. For example voice technology could be used maliciously against a public figure by creating false statements in their voice. A BBC reporter\u2019s test with his twin brother also demonstrated the capacity for voice mimicking to fool voiceprint security systems.\n\nBaidu\u2019s Deep Voice has reduced training time and advanced the development of voice cloning, opening possibilities for improvements in virtual assistants, advances in healthcare solutions and applications in many other sectors."
    },
    {
        "url": "https://medium.com/syncedreview/its-all-in-the-eyes-google-ai-calculates-cardiovascular-risk-from-retinal-images-150d1328d56e?source=user_profile---------92----------------",
        "title": "It\u2019s All in the Eyes: Google AI Calculates Cardiovascular Risk From Retinal Images",
        "text": "A retinal fundus image is a photograph of the back of the eye taken through the pupil. For more than 100 years these images have been used for detecting eye disease. Now Google has introduced a surprising new use for retinal images: combined with artificial intelligence, they can also predict a patient\u2019s risk of heart attack or stoke.\n\nResearch arm Google Brain today published a paper in the journal Nature Biomedical Engineering which demonstrates how deep learning models can use retinal images to detect a patient\u2019s age, gender, smoking status and systolic blood pressure; calculate cardiovascular risk factors; and predict the risk of major adverse cardiac events occurring over the next five years.\n\nA problem with today\u2019s mainstream cardiovascular risk calculators such as the Pooled Cohort Equations, Framingham, and Systematic Coronary Risk Evaluation is that they require the input of multiple features such as blood pressure, body mass index, glucose and cholesterol levels, etc. to generate a disease risk result. A study by the American College of Cardiology\u2019s Practice Innovation And Clinical Excellence Program concluded that the data required to calculate 10-year risk was available for less than 30% of patients.\n\nGoogle Brain discovered that a retinal fundus image alone was sufficient to predict many cardiovascular risk factors. The anatomical feature patterns were extracted using a convolutional neural network \u2014 a computational model that excels in analyzing images.\n\nResearchers trained models on retinal images from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients. The trained model identified patients\u2019 ages with 3.26 years, distinguished gender 97 percent of the time, spotted a smoker 71 percent of the time, and calculated blood pressure with a 11.23 mmHg margin of error.\n\nGoogle Brain then took a step forward. Researchers discovered the trained model could predict a patient\u2019s risk of cardiovascular disease over the next five years 70 percent of the time, approaching the accuracy rate of established risk calculators without all the additional data inputs.\n\nDeep learning is often criticized for its lack of transparency and interpretability, and this has hindered the technology\u2019s entry into areas such as medical health and the legal system. But Google Brain believes their methodology is sound. It employs attention techniques to determine which pixels are the most important for predicting a specific cardiovascular risk factor: blood vessels for example are a critical feature for determining blood pressure.\n\nThis is not the first time Google Brain has leveraged the value of retinal images. In November 2016 it presented a study on deep learning for early detection of diabetic retinopathy, which could potentially protect 415 million worldwide diabetics from irreversible blindness.\n\nGoogle Brain\u2019s paper opens up the exciting possibility of applying deep learning to retinal images for improving diagnoses beyond eye disease. Will AI be the key that unlocks even more medical science innovations?"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-february-pt-1-8c322e4c7a16?source=user_profile---------93----------------",
        "title": "AI Biweekly: 10 Bits from February (Pt 2) \u2013 SyncedReview \u2013",
        "text": "February 5th \u2014 NLP Could Help With Health Information\n\nNon-profit organization Atrius Health applies NLP technology to identify clinical data and extract information. The NLP technology is provided by the Linguamatics 12E platform, which can enhance clinical documentation for chronic disorders and improve the medicare ACO quality report. Atrius Health plans to apply the platform in additional areas to support and service behavioral healthcare.\n\nClearBrain helps users aggregate their data and deploys artificial intelligence to help subscription-based businesses target users most likely to sign up. The company is among American seed accelerator Y Combinator\u2019s startups and has raised US$1.2 million in funding from YC and other Venture Capital funds. ClearBrain clients include InVision and The Skimm.\n\nCanada\u2019s Liberal government announces new funding of CDN$4.1 million over five years for the Canadian Centre for Child Protection. The funding will support the development and implementation of AI technology that crawls websites to detect child pornography.\n\nAlphabet\u2019s Waymo believes a former employee brought thousands of confidential documents to Uber. Soon after the lawsuit was filed, Uber fired its head of autonomous driving. The surprise settlement sees Uber paying US$245 million in shares to Waymo, and also affirms a new partnership between the two on autonomous car development, despite their direct competition.\n\nFebruary 12th \u2014 Google Opens Its TPU Chips to the Public\n\nGoogle opens access to its tensor processing units (TPUs) through its cloud-computing services at the rate of US$6.5 per cloud TPU per hour. Google\u2019s TPUs are dedicated to tasks related to machine learning and have allowed Google to be less dependent on chip suppliers like Nvidia. Now they will also generate rental revenue. Ride-sharing company Lyft was one of the early adopters of Google TPUs, which have significantly sped up its development of autonomous vehicles.\n\nFebruary 13th \u2014 Google Uses AI to Reply to Messages Across Major Chat Apps\n\nA Google team is working to bring its smart reply feature to major chat applications such as Hangouts, Allo, WhatsApp, Facebook Messenger, Android Messages, Skype, Twitter DMs, and Slack. This new feature will enable context-aware responses in messages. For example, by accessing your current location, the feature can automatically answer a question like \u201cWhen will you arrive?\u201d\n\nApple is said to be talking with Yangtze Memory Technologies regarding a storage chips purchase. This would be the first Apple chip purchase from a Chinese supplier. The chips will be used in the new iPhone and other products for the Chinese market. Neither Apple nor Yangtze Memory responded to requests for comment.\n\nAlphabet\u2019s autonomous driving company Waymo launches a ride-hailing service in Arizona. Beyond the autonomous driving technology, Waymo is also building a ride-hailing platform. Waymo\u2019s moves in this space are seen as a threat to Uber.\n\nAmazon completes the biggest ever redesign of its Alexa Skills Kit Developer Console. The makeover focuses on improved developer workflows and provides a more user-friendly visual interface, and comes at a time when some third-party companies had stepped in to provide solutions the Amazon toolset had been lacking.\n\nIntel\u2019s new Vaunt smartglasses look just like normal glasses. They can project images or words on a holographic reflector which transmits the content directly to the eyes. Learning from the failed Google Glass, Intel has avoided the tap function and made their glasses less conspicuous to use in public."
    },
    {
        "url": "https://medium.com/syncedreview/synced-celebrates-lunar-new-year-with-30-best-ai-use-case-award-9c24709cb6c1?source=user_profile---------94----------------",
        "title": "Celebrating Chinese New Year 2018: Synced Announces 30 Best AI Use Case Award",
        "text": "Synced celebrates the Chinese New Year 2018 by announcing our 30 Best AI Use Case Award. The Award recognizes companies that have applied AI in various industries or organizations.\n\n \n\nCongratulations to the companies below for their outstanding achievements in the AI industry. Synced will continue to bring AI updates to our readers throughout 2018.\n\nCobot \n\n \n\nCobot\u2019s intelligent grasping system comprises a robot, control system, visual system, and auxiliary system. The company is helping Yuguo Mushroom (\u88d5\u56fd\u83c7\u4e1a) improve efficiency in sorting mushrooms.\n\nNetease AI\n\n \n\nNetEase AI uses facial recognition technology to set up email log in verifications to ensure account safety. \n\nChina\u2019s AI-powered NetEase is Music to Your Ears\n\n\n\nSpeakIn \n\n \n\nSpeakIn provides voiceprint recognition and ID security solution for China\u2019s police departments and judicial institutions. The company uses AI for anti-telecommunication fraud, anti-terrorism, criminal detection, and ID security checks.\n\nMaster Learner\n\n \n\nMaster Learner develops AI applications for education purposes, providing personalized classes for over 80 million primary and middle school students in China.\n\nYixue Education\n\n \n\nYixue Education works with Stanford University\u2019s SRI Lab to research and deploy \u201cintelligent adaptive learning\u201d technologies. The company operates 500 schools in more than 20 Chinese provinces.\n\nKika Tech\n\n \n\nKika leverages speech interaction systems with its AI-powered product Kika Keyboard. The AI engine understands and predicts user intentions to make conversations more expressive.\n\nMeitu\n\n \n\nMeitu AI lab\u2019s facial recognition and augmented reality technology enables Meitu beauty camera users to virtually try on trending makeup looks as well as create their own looks.\n\nAfter Creating 166 Million Super Cute Selfies, Meitu\u2019s AI Sets its Sights on Skin\n\n\u6210\u5c31\u62a4\u80a4\u5927\u4e1a\uff0c\u9664\u4e86\u8d1f\u8d23\u6e05\u7a7a\u8d2d\u7269\u8f66\u7684\u7537\u53cb\uff0c\u59b9\u5b50\u4eec\u8fd8\u9700\u8981\u4e00\u4f4dAI\u76ae\u80a4\u987e\u95ee\n\n \n\nTuputech\n\nWith Tuputech\u2019s real-time image and video recognition API, Inke was able to detect violent and other objectionable or explicit content in images and videos with 99.5% accuracy, reducing manpower by 90% or more.\n\n4Paradigm\n\n \n\n4paradigm AI helps Chinese banks boost performance in verticals like application anti-fraud, transaction anti-fraud, overdue or loss warning, liquidity management, smart collection, and disposal of non-performing assets.\n\n\u7b2c\u56db\u8303\u5f0f\u9648\u96e8\u5f3a\uff1a\u4e07\u5b57\u6df1\u6790\u5de5\u4e1a\u754c\u673a\u5668\u5b66\u4e60\u6700\u65b0\u9ed1\u79d1\u6280\n\n \n\nAnt Financial\n\n \n\nAdopting image recognition technology, Ant Financial\u2019s AI-powered computer vision system helps insurance companies to optimize accident claim procedures and settle cases more quickly.\n\n\u8682\u8681\u91d1\u670d\u53d1\u5e03\u300c\u5b9a\u635f\u5b9d\u300d\uff0c\u63a8\u52a8\u56fe\u50cf\u5b9a\u635f\u6280\u672f\u5728\u8f66\u9669\u9886\u57df\u7684\u5e94\u7528\n\n\u8682\u8681\u91d1\u670d\uff1a\u53ea\u505a Tech\uff0c\u652f\u6301\u91d1\u878d\u673a\u6784\u505a Fin\n\n \n\nCreditX \n\n \n\nCreditX uses deep learning technology and knowledge mapping of financial data to help clients in the finance industry with risk management tasks.\n\n \n\nDelta Entropy\n\n \n\nDelta Entropy employs big data analysis and machine learning algorithms to enable automatic risk evaluation for large-scale machinery and equipment. \n\n \n\nIpampas\n\n \n\nIPampasWealthBrain (iWB) provides AI solutions for financial institutions with intelligent wealth management and global asset allocation.\n\n \n\nJD Finance\n\n \n\nJD Finance uses a multi-dimensional consumer behavior model to analyze JD user data and recommends financial products to meet clients\u2019 requirements.\n\nTachyus\n\nTachyus leverages machine learning in energy production for the oil and gas industry. Petroleum engineers can explore millions of scenarios and identify optimal operational and development plans.\n\nIntuitive Surgical\n\n \n\nIntuitive\u2019s Da Vinci surgical robots enable surgeons to perform micro incision surgeries. The technologies have applications in cardiac, thoracic, urology, gynecologic, colorectal, pediatric and general surgical disciplines.\n\n \n\nIntel\n\n \n\nIntel develops the smart medical imaging technology that helps China\u2019s Aier Eye Hospital improve the accuracy of eye disease diagnoses.\n\n \n\n12sigma\n\n \n\n12sigma Technologies introduces AI and deep learning to medical image diagnosis and medical data analysis to help doctors analyze CT scans more efficiently, especially for lung cancer.\n\nRoss Intelligence \n\n \n\nRoss Intelligence uses NLP technology to automate legal research, and improves efficiency for law firms. The company has partnered with law schools to conduct further research. \n\n\n\n\u4e16\u754c\u9996\u4e2aAI\u6cd5\u5f8b\u52a9\u7406ROSS\u5b8c\u6210A\u8f6e\u878d\u8d44\uff0c\u529b\u62d3\u4e16\u754c500\u5f3a\u6cd5\u5f8b\u670d\u52a1\u5e02\u573a\n\n \n\nPatsnap\n\n \n\nPatsnap\u2019s AI-powered patent search and analysis platform helps business leaders, analysts, researchers, engineers and IP professionals make well-informed decisions.\n\nHikvision\n\n \n\nHikVision\u2019s smart warehouse solutions help China\u2019s Master Logistics process documents and data in warehousing operations.\n\nBloomReach\n\n \n\nBloomReach helps department store Neiman Marcus make their content more discoverable with applications using content management, self-learning personalized site search, SEO optimization and merchandising analytics.\n\nClarifai helps image recognition translate product image tags into 12 different languages to improve SEO and drive traffic to products in its e-commerce store, saving over US$10,000 in agency fees.\n\n\u673a\u5668\u4e4b\u5fc3\u72ec\u5bb6\u5bf9\u8bddClarifai\u521b\u59cb\u4eba\uff1a\u4ece\u56fe\u50cf\u8bc6\u522b\u5230\u65e0\u9650\u53ef\u80fd\n\n \n\nHaomaiyi\n\n \n\nHMY Technology designs virtual fitting rooms. Its AI powered 3D reconstruction technology allows users to \u201cmodel\u201d different outfits.\n\nYi Tunnel uses image & facial recognition technology to produce AI-powered purchasing and vending machines, which allow unattended checkout and make offline shopping more convenient.\n\nAISpeech\n\n \n\nAISpeech specializes in speech interaction technology. Its services are used by well-known home speaker brands in China including Alibaba\u2019s AliGenie. \n\n\u5728 AI \u521b\u4e1a\u7684\u597d\u65f6\u4ee3\uff0c\u521d\u654f\u7ed3\u675f 8 \u5e74\u963f\u91cc\u751f\u6daf\u52a0\u5165\u601d\u5fc5\u9a70\n\n\u601d\u5fc5\u9a70\u53d1\u5e03 DUI \u5f00\u653e\u5e73\u53f0\uff0c\u5168\u94fe\u5b9a\u5236\u5316\u8d4b\u80fd\u66f4\u591a\u7ec8\u7aef\n\n \n\nElevoc\n\n \n\nElevoc Technology provides a new-generation voice signal processing engine for telecommunication, smart home and vehicle devices. It does real-time implementation of practical monaural speech de-noising.\n\niflytek\n\n \n\niFlyTek\u2019s speech recognition solutions are used in cars for hands-free interaction. Users can make voice navigation commands, phone calls, send text messages, access a browser search and so on. \n\nWill iFlytek Voice Input\u2019s 98% Accuracy Kill the Keyboard?\n\n \n\n \n\nKuaishou\n\n \n\nKuaishou is a live video streaming platform broadcasting user generated content. It includes an AI-powered recommendation system."
    },
    {
        "url": "https://medium.com/syncedreview/china-celebrates-lunar-new-year-with-ai-aadd24639281?source=user_profile---------95----------------",
        "title": "China's Lunar New Year\u2019s Gala Wowed Watchers with AI Innovations",
        "text": "The CCTV Spring Festival Gala (\u592e\u89c6\u6625\u665a) is Chinese TV\u2019s Lunar New Year extravaganza, watched by hundreds of millions of people across the country. This year, amid all the acrobatics, musical performances and fireworks, broadcaster CCTV also shone a spotlight on the country\u2019s state-of-the-art AI technologies.\n\nThe showcase included self-driving vehicles, intelligent robots, drones and more \u2014 and won glowing reviews from Chinese netizens on social media.\n\nChina continues to spark technological breakthroughs and establish AI-powered infrastructures for its cities and transportation networks. The government aims to make the AI industry worth US$150 billion by 2030, and is investing $2.1 billion in AI research and development to empower local businesses.\n\nWhile polls suggest many Americans and Europeans fear AI will take their jobs, the Chinese are much more optimistic about the tech, and 65 percent believe AI will create more job opportunities over the next five to ten years.\n\nThe AI innovations at the Spring Festival Gala were choreographed into the show\u2019s performances. Synced has picked out a few highlights.\n\nIn a live stream from the Gala\u2019s parallel session in Zhuhai, over a hundred self-driving cars equipped with Baidu\u2019s autonomous driving platform Apollo crossed the Hong Kong \u2014 Zhuhai \u2014 Macao Bridge. The self-driving fleet included BYD\u2019s new energy vehicles (NEV), King Long\u2019s self-driving micro-location bus, and Zhixingzhe\u2019s self-driving road sweeper and logistics vehicles.\n\nFirst released last July, Apollo is an open-source platform that gives developers access to a complete set of vehicle, hardware, software, and cloud data service solutions; as well as an API and codes for obstacle perception, route planning, vehicle control, and operating system. Aimed at democratizing autonomous driving, Baidu hopes Apollo will become \u201cthe Android of the auto industry.\u201d\n\nAlso live from Hong Kong Harbour were a fleet of 81 self-driving boats. A 7.5-meter-long lead vessel led 80 small boats of 1.6 meters in length under the Hong Kong-Zhuhai-Macao Bridge in the a form of arrow.\n\nThe boats were from Yunzhou Tech, a Zhuhai-based company engaged in R&D on self-driving vessels used in water quality monitoring, hydrological mapping, nuclear radiation monitoring and hydrological research. This was the first public appearance of Yunzhou\u2018s self-driving multi-boat collaborative technology. The company employs a robust and adaptive control system, and for navigation uses high-precision GNSS, inertial navigation, and RTK technology.\n\nA \u201cwow\u201d moment occurred when 300 flying drones \u201cleaped\u201d over the Hong Kong-Zhuhai-Macao Bridge in a shape of a dolphin. This was China\u2019s first light show of large-scale 3D stereoscopic drones.\n\nBeijing-based drone company ZEROTECH and Shenzhen-based innovative high-tech company Highgreat jointly developed the technology behind the drone show. Their engineers adopted GPS-RTK carrier phase differential positioning technology to improve accuracy to the centimeter level, and used technology from Baidu\u2019s Apollo to enable the autonomous flight.\n\nAs 2018 is the Year of the Dog, guests at the Spring Festival Gala\u2019s opening were joined by 24 adorable robot dogs who spent the evening dancing, licking paws, wagging tails, and capturing viewers\u2019 hearts.\n\nThe \u201cJimu\u201d robodogs were produced by Chinese robotics startup UBTECH. Jimu has 19 steering gears, over 900 parts, and LED lights as eyes. Steering gears in head, trunk, and limbs simulate canine joint movement.\n\nSince 2011, the night before the big Lunar New Year show has featured a smaller, run-up show known as Network Chunwan (\u7f51\u7edc\u6625\u665a) which celebrates China\u2019s outstanding digital achievements over the preceding year, again incorporating these with on-stage performances.\n\nThe theme of this year\u2019s show was AI. Moderator Xun Zhu (\u6731\u8fc5) said 2017 was a year when \u201cAI has rapidly been developing. The Internet plus AI has become a new mode for benefitting people\u2019s lives.\u201d\n\nThe evening\u2019s opening performance was a piano battle between seven year-old Chinese prodigy Anke Chen (\u9648\u5b89\u53ef) and robot pianist \u201cTeo Tronico.\u201d The bot won the speed play competition, although his 53 fingers clearly gave him an edge over Chen\u2019s 10. Teo Tronic is a proof of concept android built by Italian music and AI fanatic Matteo Suzzi that has previously faced off against Italian classical pianist Roberto Prosseda and Chinese pianist Lang Lang.\n\nNext up was Baidu\u2019s robot Xiaodu, which used audience word suggestions to compose poems and couplets on the spot. AI-powered Xiaodu, who first appeared on the show in 2015, has outperformed humans at multiple tasks on Chinese reality TV shows.\n\nDobot Magician, a robot arm made by Shenzhen-based robotic startup Dobot, showed off his calligraphy skills, writing the characters for \u201cChinese Dream.\u201d First introduced at CES 2017, Dobot Magician can perform 3D printing, laser engraving, grayscale engraving, drawing, sorting, etc.\n\nThe last AI performer was Wangzai, a popular humanoid robot debuted last year by Chinese search engine giant Sogou. Wangzai transformed still photos of show moderators and performers into dynamic, speaking 3D avatars. Wangzai has a wide knowledge base, and can respond to questions within milliseconds. Last February it defeated a Harvard graduate in the Chinese version of the American game show \u201cWho\u2019s Still Standing?\u201d\n\nThe strong focus on AI in China\u2019s most-watched television event of the year was a pleasant surprise for those working in the field, and is bound to encourage many in the country to explore AI and its rapidly emerging technologies."
    },
    {
        "url": "https://medium.com/syncedreview/augmenting-virtual-assistants-with-personality-and-personalization-f5395707d349?source=user_profile---------96----------------",
        "title": "Augmenting Virtual Assistants with Personality and Personalization",
        "text": "Humans are social animals, we love talking with one other. Typically, human conversational styles are informed by our personalities, expressed in tone of voice, level of knowledge, sense of humor, and so on. These qualities are now being applied to virtual assistants, the AI-powered human-machine conversational interfaces in our smartphones, homes, and cars.\n\nThe personality of a virtual assistant is defined by the words, tone, style, dialect, and behavior it uses. The goal of any virtual assistant is to establish user trust, engagement, and satisfaction.\n\nSpeaking at the recent RE-WORK AI Assistant summit, Dr. Ann Thyme\u0301-Gobbel, Voice UI/UX Design Leader at Sound United, said that content accuracy is most important virtual assistant trait.\n\nAmazon Alexa\u2019s recent attention-grabbing Super Bowl commercial features a host of Hollywood stars filling in for Alexa, who has \u201clost her voice.\u201d The substitute virtual assistants mess around with users, for example a man who requests a grilled cheese sandwich recipe is mocked by celebrity chef Gordon Ramsay: \u201cIt\u2019s name is the recipe, you #&*$%!\u201d\n\nContent accuracy metrics include accurate responses, correct info, dependability, reliability, low error rate, high comprehension, correctness, and minimal repeating. To meet these demands researchers require robust and reliable technologies in knowledge base, speech synthesis, speech recognition, and natural language understanding.\n\nDr. Thyme\u0301-Gobbel says voice quality ranks second, referencing a study comparing text-to-speech (TTS) synthesis with actual human voices. Although both deliver a similar level of trust, real human voices scored much higher in user enjoyment.\n\nGiven the same level of content accuracy and voice quality, users are more likely to engage with a virtual assistant that is not too chatty, admits when it is uncertain about answers, and is friendly yet professional.\n\nDr. Thyme\u0301-Gobbel says an overly crafted virtual assistant does not necessarily deliver a positive user experience, which is a surprising discovery given that major virtual assistant makers employ storytelling advisors to help compose answers. Google has a team tasked with imagining likely questions and creating responses for developers to code. Microsoft\u2019s Cortana team meanwhile has eight full-time writers brainstorming what Cortana will say and how it will say it.\n\nThe recipe is simple. \u201cLight humor or a quirky sense of humor is great. No jokes unless I ask. Don\u2019t be too eager to please, and avoid being fake friendly,\u201d says Dr. Thyme\u0301-Gobbel.\n\nHowever, even a sense or humor or a strong personality might be inappropriate for virtual assistants in domains such as business, healthcare, education, etc. Virtual assistant makers must understand and stay true to different underlying interaction environments. For example, a healthcare virtual assistant should speak and behave as a trusted doctor or nurse would. Its user expectations and goals are very different from a home virtual assistant whose role is to be an advisor or entertainment content provider grounded in interaction and enjoyment.\n\nDr. Thyme\u0301-Gobbel suggests virtual assistant makers should enable further personalization. She believes users will respond favourably for example if a virtual assistant remembers what they previously said and delivers tailored services using personalized voice characteristics such as tone, accent, rate of speech, access preferences, etc.\n\nWhile personalization can boost user engagement, Dr. Thyme\u0301-Gobbel also warns that \u201cusers don\u2019t want machines to automatically personalize their behaviors.\u201d An inappropriate personalization choice might lower user trust and satisfaction, and negatively affect performance. Also, a user communicating for example with an AT&T customer service bot may not require or desire personalized interaction to get a satisfactory user experience.\n\nDr. Thyme\u0301-Gobbel told Synced that studying the personality and personalization of virtual assistants has helped her dig into the technology behind them. \u201cAs a voice designer person, I have to decide on flows and systems, what to include and what not to include. People care most about having good content and being recognized. You know who I am, so you can be more helpful, like a real person.\u201d\n\nHistorically, most innovative products have evolved from uniform to personalized. Ten years after the Model T Ford was introduced in 1908, the first world\u2019s first mass produced car was available in one colour: black. The automotive industry subsequently spawned a multitude of models with different colours, designs and options to target specific market sectors. It\u2019s a safe bet that a similar broadening of options is about to occur with virtual assistants."
    },
    {
        "url": "https://medium.com/syncedreview/chinas-netease-music-uses-ai-to-win-hearts-518880aee6a3?source=user_profile---------97----------------",
        "title": "China\u2019s AI-powered NetEase is Music to Your Ears \u2013 SyncedReview \u2013",
        "text": "China\u2019s music and video streaming unicorn NetEase Music has a database of 10 million songs, 400 million users, and a net worth of over US$1.14 billion.\n\nChinese netizens are all ears for the company\u2019s \u201chearty\u201d AI-powered music recommendations. In an interview with Synced, NetEase Data Scientist Jia Xu and Product Manager Bowen Shen explained the NetEase system, which learns how to predict what songs will resonate with a user\u2019s particular taste in music.\n\nAI recommendation systems are based on the same tech used by news websites, online shopping websites, dating apps, and social media feeds. Consumers generally appreciate a service that learns their preferences and makes accurate predictions, and such systems have become a powerful tool in targeted marketing.\n\nAmazon engineers first put the recommendation systems to work \u2014 if a customer purchased A, they might also want to put B into their basket \u2014 using a common method called collaborative filtering. This works well when supplied with large amounts of user preference data, which NetEase has and continues to accumulate.\n\nNetEase\u2019s collaborative filtering follows two approaches: the first recommends tracks by linking between users who have similar tastes, the second takes a single track as a vector for similarity calculation, and recommends it based on the user\u2019s play history.\n\nIn his blogpost Recommending Music on Spotify with Deep Learning, the creator of Spotify\u2019s music recommendation system and current DeepMind researcher Sander Dieleman explained the shortfalls of collaborative filtering, pointing out that the models are \u201ccontent-agnostic\u201d, have problems dealing with albums with multiple tracks of different styles, and that \u201cnew and unpopular songs cannot be recommended\u201d due to a lack of user data, posing the \u201ccold-start problem.\u201d\n\nTo solve these challenges NetEase turned to deep learning, using input data with features such as song genre, artist, album information, lyrics, beats, user comments, VIP download preferences, and price. The information is projected into a low-dimensional latent space to train deep learning models.\n\nThe system establishes a vector position for each song, encoding all relevant information including user preferences, so the recommendation system can make suggestions even when the song or user have little or no play history. The vectors are \u201cnormalized\u201d to overlook popularity, which ensures that users can also discover and recommend new songs. NetEase also uses a machine learning ranking model to prioritize recommendations on a daily basis.\n\nNetEase is the radio station of the future, a space where even those with the most esoteric tastes can discover new music. Close to half a billion users can\u2019t be wrong."
    },
    {
        "url": "https://medium.com/syncedreview/boston-dynamics-robodog-opens-a-door-owns-the-internet-cded79fae992?source=user_profile---------98----------------",
        "title": "Boston Dynamics Robodog Opens a Door, Owns the Internet",
        "text": "SoftBank-owned robot-maker Boston Dynamics is once again wowing the internet. Its latest creation is a robodog named SpotMini which can deftly open doors with its head-mounted gripper arm. The YouTube video Hey Buddy, Can You Give Me a Hand? featuring SpotMini \u201cpolitely\u201d holding a door open for a buddy garnered more than three million views in just 24 hours.\n\n \n\nSpotMini is an upgraded version of its big brother Spot, which was released in 2016. Boston Dynamics says SpotMini\u2019s special skill is \u201cthe ability to pick up and handle objects using its 5 degree-of-freedom arm and beefed up perception sensors.\u201d The electric robodog weighs 25 kg (30kg with the arm), and is \u201cthe quietest robot we have built.\u201d\n\nWildCat holds the fastest free running quadruped robot in the world record speed of 32km/h, not far off the 37.58 km/h that the world\u2019s fastest man Usain Bolt clocked at the 2009 World Athletic Championships in Berlin. \n\n \n\nBoston Dynamics founder Marc Reibart is a robot fanatic who started building hopping machines in 1979 at Carnegie Mellon University. The finished products\u2019 impressive dexterity trailblazed a new generation of robots with robust legs. Reibert moved to MIT in 1986.\n\n \n\nBoston Dynamics was founded in 1992 as an MIT spinoff. The company worked on a series of military projects for the US Department of Defence before being acquired by Google for $500 million in 2013. Last June Google decided not to pursue the unit\u2019s long-term research and it was sold to Japanese conglomerate SoftBank in a high-profile acquisition. \n\n \n\nNo one will deny Boston Dynamics\u2019 marketing success, the company has continually excited the public imagination. An earlier video meant to demonstrate their robots\u2019 balance shows company staff kicking and pushing poor Spot, which is doggedly determined to remain on its feet. This spawned the mocking phrase \u201cBoston Dynamics robot abuse\u201d and a dedicated website selling \u201cStop Robot Abuse\u201d t-shirts.\n\nNetflix meanwhile has turned robodogs from abused to abuser in its sci-fi dystopia series Black Mirror. Writer Charlie Brooker says he found something \u201ccreepy\u201d about Boston Dynamics products, and this inspired the terrifying new episode Metalhead, in which a woman is relentlessly pursued by a human-slaughtering robodog.\n\nThere\u2019s no need to worry about Boston Dynamics\u2019 robodogs becoming homicidal in real life. They are still rarely seen outside the laboratory. But as they continue to learn real-world skills we may think of ways to put them to good use, believing we can always keep them on a leash."
    },
    {
        "url": "https://medium.com/syncedreview/arm-launches-project-trillium-two-ai-processors-2a58cdb783ca?source=user_profile---------99----------------",
        "title": "Arm Launches Project Trillium, Two AI Processors \u2013 SyncedReview \u2013",
        "text": "Ninety percent of AI-enabled devices shipped today are based on architecture developed by Arm, a leading UK-based chip intellectual property (IP) provider known for its CPU and GPU processors. In order to scale the impact of machine learning the company today announced Project Trillium, an Arm IP suite that includes a machine learning processor, a object-detection processor, and a library of neural network software.\n\n \n\n Project Trillium is the company\u2019s latest ambitious move in artificial intelligence, a ground-up design to improve the performance and efficiency of AI-enabled devices, which are expected increase in number from 300 million today to 3.2 billion by 2028. \n\n \n\n Arm\u2019s efforts in machine learning can be traced back in 2013 when it began exploring the AI marketplace and made a number of strategic acquisitions. In 2017 the company launched its new Machine Learning Group and named Jem Davies as General Manager. In an exclusive interview, Davies told Synced that in his mind there was \u201cno market segment that wasn\u2019t already or about to be impacted by AI.\u201d\n\n \n\n \u201cAI affects everything\u2026mobile phones\u2026cameras\u2026the little smart speaker\u2026 even thermostats. Who thought of a room thermostat as a smart device?\u201d said Davies. \n\n \n\n The machine learning processor introduced today is Arm\u2019s first-generation AI chip targeting the inferencing of mobile devices. The chip delivers no less than 4.6 trillion operations per second (TOPS) of mobile performance per mm2, with a further uplift of 2x-4x in effective throughput in real-world uses of optimization, and an efficiency of over three TOPS per watt (TOPs/W) in thermal and cost-constrained environments. \n\n \n\n Davies says the architecture behind their machine learning processors is completely new, and results from many years of research. The architecture is optimized around 16 integer arithmetic.\n\n \n\n The new architecture will provide a great solution for challenges that CPUs and GPUs struggled with, says Davies. \u201cConvolutional Neural Networks are very common. One of the things is that a traditional architecture, whether CPU, GPU or DSP, is going to involve a lot of intermediate result/data storing and loading from memory. So we have produced a completely new architecture with an intelligent memory system.\u201d\n\n \n\n The object detection processor is an iteration based on Arm\u2019s existing IP family: Spirit, the object detection accelerator that powers the Hive security camera. It was released in 2016 soon after Arm acquired Apical, a company that provides computer vision and imaging processors for over 1.5 billion devices. \n\n \n\n Arm\u2019s second-generation processor can detect virtually unlimited numbers of objects in real time with Full HD at 60fps. Its detailed people model provides rich metadata and enables detection of direction, trajectory, pose and gesture.\n\n \n\n Arm provides an integrated solution comprising machine learning processors and object detection processors. In real-time object recognition tasks, the object detection processor will first isolate areas of interest such as faces. The machine learning processor will then be able to analyze fewer pixels for a faster, fine-grain result.\n\nArm\u2019s neural network software library is a collection of building blocks for imaging, vision and machine learning workloads. Developers can use the software with Arm\u2019s existing implementation tools such as Compute Library to accelerate their algorithms and applications, or CMSIS-NN to maximize performance at the edge. The library supports mainstream frameworks such as TensorFlow and Caffe, and is optimized for Arm Cortex CPU, Mali GPU, and new machine learning processors.\n\n \n\n Arm machine learning processors will be delivered to partners this summer, and object detection processors will be available by the end of this quarter."
    },
    {
        "url": "https://medium.com/syncedreview/googles-tpu-chip-goes-public-in-challenge-to-nvidia-s-gpu-78ced56776b5?source=user_profile---------100----------------",
        "title": "Google\u2019s TPU Chip Goes Public in Challenge to Nvidia\u2019s GPU",
        "text": "Google announced this morning that its Tensor Processing Unit (TPU) \u2014 a custom chip that powers neural network computations for Google services such as Search, Street View, Google Photos and Google Translate \u2014 is now available in beta for researchers and developers on the Google Cloud Platform.\n\nThe TPU is a custom application-specific integrated circuit (ASIC) tailored for machine learning workloads on TensorFlow. Google introduced TPU two years ago, and released the second generation Cloud TPU last year. While the first generation TPU was used in inferencing only, the Cloud TPU is suitable for both inferencing and machine learning training. Built with four custom ASICs, Cloud TPU delivers a robust 64 GB of high-bandwidth memory and 180 TFLOPS of performance.\n\nBefore it opened its TPUs to the public, Google had widely implemented them internally. AlphaGo \u2014 the Google AI masterpiece that beat human champions in the ancient Chinese board game Go \u2014 used 48 TPUs for inferencing.\n\nCloud TPU provides a great solution for shortening the training time of machine learning models. Google Brain team lead Jeff Dean tweeted that a Cloud TPU can train a ResNet-50 model to 75% accuracy in 24 hours.\n\nWhen Cloud TPU was announced, Google offered 1000 free devices for machine learning researchers. Lyft, the second-largest ride-hailing company in the US, has been using Cloud TPUs in its self-driving systems since last year. Says the company\u2019s Head of Software Self-Driving Level 5 Anantha Kancherla, \u201cSince working with Google Cloud TPUs, we\u2019ve been extremely impressed with their speed \u2014 what could normally take days can now take hours.\u201d\n\nAlfred Spector, CTO of New York City-based hedge fund Two Sigma, says, \u201cwe found that moving TensorFlow workloads to TPUs has boosted our productivity by greatly reducing both the complexity of programming new models and the time required to train them.\u201d\n\nGoogle\u2019s Cloud TPU is currently only in beta, offering limited quantities and usage. Developers can rent Cloud TPUs at the rate of US$6.50/Cloud TPU/hour, which seems a reasonable price considering their great compute capability.\n\nGoogle also released several model implementation tools to save developers\u2019 time and effort writing programs for Cloud TPUs, including ResNet-50 and other popular models for image classification, a transformer for machine translation and language modeling, and RetinaNet for object detection.\n\nWhile Google is not directly selling its TPU chips to customers at this stage, their availability represents a challenge to Nvidia, whose GPUs are currently the world\u2019s most-used AI accelerator. Even Google has used large numbers of Nvidia GPUs to provide accelerated cloud computing services. However if researchers switch from GPUs to TPUs as expected, this will reduce Google\u2019s dependency on Nvidia.\n\nLast year, Google boasted that its TPUs were 15 to 30 times faster than contemporary GPUs and CPUs in inferencing, and delivered a 30\u201380 times improvement in TOPS/Watt measure. In machine learning training, the Cloud TPU is more powerful in performance (180 vs. 120 TFLOPS) and four times larger in memory capacity (64 GB vs. 16 GB of memory) than Nvidia\u2019s best GPU Tesla V100.\n\nAlthough it\u2019s too early to crown the Cloud TPU as the AI chip champion, the announcement of its availability has sparked excitement among researchers, and marks the beginning of Google\u2019s ambitious move into the space of AI accelerators."
    },
    {
        "url": "https://medium.com/syncedreview/safety-regulations-for-self-driving-cars-in-america-4dbe5ae44440?source=user_profile---------101----------------",
        "title": "Safety Regulations for Self-Driving Cars in America",
        "text": "California has hosted public road testing for more than 180 self-driving vehicles from 27 companies. Carmakers Audi, BMW, Mercedes, Volvo, GM, Ford, Honda, Toyota, Fiat-Chrysler, and Tesla; technology companies Uber, Lyft, Google\u2019s Waymo, and Apple; and software solution providers Mobileye (Intel), Nvidia, AutoX, and Drive.ai are all using the Golden State\u2019s roadways to perfect their products. It\u2019s getting crowded out there, and in response there are regulations.\n\nSafety is the priority. In a 2017 American Automobile Association (AAA) survey, 78% of respondents said they would not want to ride in a self-driving car due to safety concerns.\n\nLast September the National Highway Traffic Safety Administration (NHTSA), an agency under the US Department of Transportation, introduced a framework called Automated Driving Systems 2.0: A Vision for Safety in an effort to make self-driving cars safe. The US House of Representatives had months earlier passed the Self Drive Act, which regulates autonomous cars from production to testing and distribution under NHTSA supervision.\n\nThe NHTSA also adopted the six levels of autonomy classification scheme proposed by the Society of Automotive Engineers in 2016 for self-driving vehicles:\n\nLevel 1&2 self-driving cars use an Advanced Driver Assistance System, otherwise known as ADAS, to assist the human driver. Levels 3\u20135 rely on an Automated Driving System, otherwise known as ADS.\n\nMost autonomous driving systems on roads today are Level 1&2, while some self-driving test vehicles can reach Level 3. In its November 2017 road test in Arizona, Google\u2019s Waymo made the leap to Level 4 by removing the human driver. (Here we need to keep in mind that a car capable of Level 4 self-driving on Arizona roads may not be able to perform to even a Level 3 standard in India for example, where traffic is more chaotic.)\n\nThe biggest challenges facing self-driving systems are sensing, perception, and control. Studies in SLAM (simultaneous localization and mapping) technology and computer vision are expected to solve the first two challenges in the short term. As for control, the hardware is also not a problem. There is however the problem of unpredictable human drivers. To enable systems to make decisions intelligently, existing data is still far from sufficient to deal with all driving situations.\n\nWhat how much data is required to guarantee safety? The more the better.\n\nStatistically speaking, the average American driver will encounter a deadly accident after one hundred million miles of drivetime. And so even though Waymo for example has logged more than 1.3 million miles of road testing, this is still not nearly enough to give a statistically meaningful estimation of how safe the autonomous car would be.\n\nOther possibilities for improving self-driving vehicle safety include installing RFID tags on roadways or at critical intersections for vehicle-to-infrastructure communication to help the autonomous cars to perceive the environment. Audi has also proposed using LED lights in the front windshield to notify pedestrians when the self-driving car \u201csees\u201d them, so the pedestrian knows it is safe to cross the street.\n\nOf course, an ideal safety solution would be for the many different autonomous driving research teams to simply share their data. Alas, the market is too competitive and the stakes too high to reasonably expect that to happen."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-city-brain-goes-rural-ai-pig-farming-in-sichuan-1cd226ab6405?source=user_profile---------102----------------",
        "title": "Alibaba City Brain Goes Rural: AI Pig Farming in Sichuan",
        "text": "Slogans such as \u201cRise Early to Farm Pigs, Call AI to Help!\u201d and \u201cExcel in Intelligent Pig Farming, Marry a Pretty Wife Early!\u201d are appearing on walls across China\u2019s southern countryside provinces. The campaign is part of the Sichuan pig farming corporation Dekon Group and pig feed supplier Tequ Group\u2019s new partnership with Alibaba Cloud to apply its AI-powered \u201cET Brain\u201d to pig farming. The trio are investing tens of millions of USD in the project, which was announced on February 6, 2018.\n\n \n\nOver the past year, Alibaba has implanted its ET Brain in the aerospace, transportation, environment, and healthcare sectors, fast-tracking China\u2019s social infrastructure revolution. Recent food contamination scandals have made food safety a pressing issue in China, and the agricultural industry a next logical application for ET Brain. \n\n \n\nChina\u2019s pork production accounts for more than half of the world supply, while its per capita pork consumption ranks 3rd. By 2020, Tequ Group sales will exceed 10 million tons, while Dekon will breed up to 10 million pigs annually. This is an opportunity for artificial intelligence to optimize operations, and both companies are actively building their IoT and Enterprise Resource Planning (EPR) systems. \n\n \n\nOn pig farms each pig wears a wireless radio-frequency identification (RFID) tag. These are pricey and difficult to scan, making and farmers must individually log data into mobile applications or fill in paper forms.\n\n \n\nThis is where computer vision and voice recognition AI can help. Real-time video footage is collected through surveillance cameras. Using computer vision, ET Brain will set up profiles for each pig, documenting their breed, age, weight, eating conditions, exercise intensity and frequency, and movement trajectory. The first phase of the launch includes functions such as herd behavior analysis, inventory count, health monitor, and automatic weighing.\n\nOne challenge is telling pigs apart \u2014 Alibaba considered applying facial identification to pigs to no avail. Instead they tattooed identifying numbers on the pigs\u2019 bodies.\n\n \n\nET Brain tracks the entire production line. Based on behavior tracking, gilts are selected for mating. After they give birth to piglets, usually in a litter of ten, ET Brain will use voice recognition to ensure the little ones are not suffocated by their mothers\u2019 weight. This lowers death rate by 3%, increasing annual production rate by three piglets per sow.\n\nAside from breeding, feeding and weighing, other important steps in pig farming are disease control and epidemic monitoring. ET Brain will analyze pigs\u2019 behavior, acoustic characteristics and infrared temperature measurements, to determine the health status of pigs, targeting epidemic early warning signs and specialized vaccinations.\n\n \n\nAlibaba Cloud has dispatched a team of algorithm engineers, product developers, and video analytics team to Sichuan work on the project, while Tequ will add experts on pig farming. \n\n \n\n\u201cOur core solution is to reduce the reliance on farmers and dependence on equipment through automated video analytics,\u201d explains Alibaba Cloud\u2019s Sheng Zhang, who added that the use case is highly replicable. \n\n \n\nAlibaba\u2019s AI solutions have thus far been more widely deployed in urban environments. Its \u201cET City Brain\u201d focuses on improving China\u2019s urban infrastructure using capabilities such as voice, image, text recognition, and natural language processing. Earlier last month, the company announced it will deploy ET City Brain in the Malaysian capital of Kuala Lumpur. \n\n \n\nHowever, it is also worth considering the appropriateness of AI applications for lesser developed areas. Today, 43% of the Chinese population lives in the countryside. Granting this rural economy access to AI is a difficult but important task, especially for underdeveloped industries like pig farming that have limited access to new technology."
    },
    {
        "url": "https://medium.com/syncedreview/volvos-roadmap-to-digitalization-7989dfc92a94?source=user_profile---------103----------------",
        "title": "Volvo\u2019s Roadmap to Digitalization \u2013 SyncedReview \u2013",
        "text": "Each year, the average car owner spends 293 hours driving, 90 hours parking, and dozens of hours on fuel-ups and car washing. Swedish automaker Volvo is freeing up that time with Volvo Concierge Services, \u201cthe first expandable digital ecosystem that connects car owners with convenience services via a smartphone App.\u201d\n\nThe Volvo Concierge Services platform enables users to smoothly coordinate with third-party providers to book services such as on-the-spot fuel-ups or car washing. When users request a Concierge Service, the provider will automatically receive the vehicle\u2019s location, preferred service time, and a single-use digital key for access to the car.\n\nConcierge Services was launched in 2016 as a feature of \u201cVolvo on Call\u201d, a smartphone app that offers remote assistance and other solutions. Volvo boasted it was the first and only automaker to address this drivers\u2019 pain point and penetrate a space with huge potential: American drivers fill up their tanks over 44 million times each day.\n\nThe platform has received positive feedback thus far, says Volvo Cars Senior Business Developer of Cloud and IT Kristoffer Gronowski. Concierge Services\u2019 adoption rate is now 6%, while the retention rate is an extremely high 53%. Gronowski is confident the company can scale the service to reach 30% of US Volvo owners.\n\nConcierge Services is one of many steps Volvo has taken to ramp up its tech game. The company is also betting on self-driving technology, and is committed to producing fully autonomous highway-ready vehicles by 2021.\n\nLast November, Volvo reached a US$300 million deal with Uber to provide the ride-hailing giant with 24,000 flagship XC90 SUVs incorporated with all necessary safety, redundancy and core autonomous driving technologies. This represents Volvo\u2019s largest single vehicle order, and the largest in the autonomous vehicle industry\u2019s history. Uber will develop a self-driving system based on Volvo\u2019s vehicles, and assemble a commercial fleet for the San Francisco Bay Area.\n\nVolvo\u2019s aggressive moves and embracing of innovation seems something of a new path for the traditionally conservative automaker. Volvo Head of Asia Pacific Product & Offer August Wu says the company is working flat out to differentiate its brand from competitors: \u201cWe are designing new ways of thinking and innovating fast.\u201d\n\nThe shift is also evidenced in Volvo\u2019s recent hires. The company has lured multiple executives from outside the automotive sector, such as Chief Digital Officer Atif Rafiq, who built McDonald\u2019s digital unit from scratch.\n\nIn 2016 Volvo established a Silicon Valley-based research center with 80 engineers focused on emerging technologies, digital development and R&D. The center, located across the street from Google\u2019s offices, also coordinates joint efforts with local tech companies and startups \u2014 facilitating the Uber-Volvo partnership for example.\n\nVolvo\u2019s Silicon Valley center worked with Google to co-develop an Android-based in-car infotainment system offering Google applications. Android Auto will be available on new Volvo models in the two years, powering the main touchscreen displays and digital dashboard, and adding new services such as Google Assistant.\n\nLast year Volvo acquired San Francisco-based car valet and concierge startup Luxe \u2014 valued at US$140 million according to Pitchbook \u2014 and incorporated it into the Silicon Valley center. Luxe Co-founder Curtis Lee is now Volvo\u2019s VP of Digital.\n\nLuxe has quickly integrated with Volvo\u2019s digital products, especially Concierge Services, to provide a robust, algorithm-driven logistics and services platform to support Volvo consumer services. The Luxe platform now features mobile applications, real-time location-based monitoring software, a workforce management platform, a dispatch engine, and a algorithm-driven ETA (Estimated Time of Arrival) service.\n\nThe automotive industry will undergo revolutionary changes over the coming years. Volvo\u2019s ambitious moves in this space are informed by a tech roadmap that\u2019s all about innovation, digitalization, and staying ahead of the pack."
    },
    {
        "url": "https://medium.com/syncedreview/google-clouds-jia-li-on-automl-s-first-30-days-430d4e487ee0?source=user_profile---------104----------------",
        "title": "Google Cloud\u2019s Jia Li on AutoML\u2019s First 30 Days \u2013 SyncedReview \u2013",
        "text": "Who is using AutoML Vision, the \u201csimple, secure and flexible ML service that lets you train custom vision models for your own use cases\u201d that Google launched last month? We asked Jia Li, Head of R&D of Cloud AI and Senior Director at Google.\n\nLi says AutoML now has some 10,000 registered users, ranging from startups to big corporations. She divides potential clients into three types, \u201cthe first is AI-savvy companies with loads of data, they need infrastructure such as Tensorflow to train their own models; second is companies with limited expertise and data, they can choose API without training their own models; third is companies with limited expertise, but with ideas and data, that want to build their own models. AutoML can help them customize their models by simply inputting their images without marking training data, designing algorithms, or tuning parameters.\u201d\n\nAutoML users can drag-and-drop to upload images, no AI technical expertise is required. AutoML does this with a combination of three core technologies \u2014 neural architecture search technology, learning2learn, and transfer learning \u2014 which automate the process of selecting the right networks to use, finding hyperparameters for best performance, and applying the model to different use cases directly from Google Cloud.\n\nLi explains: \u201cTransfer learning is easy because it generates models in seconds. Learning2learn has a higher cost with no fixed architecture, and it takes up to a day to generate models. But even this is shorter than traditional ways of training.\u201d\n\nGoogle reports on their blog that \u201cearly results using Cloud AutoML Vision to classify popular public datasets like ImageNet and CIFAR have shown more accurate results with fewer misclassifications than generic ML APIs.\u201d AutoML\u2019s codes run better than those written by engineers; while for labeling objects in an image AutoML achieves 42% accuracy compared to man-made models\u2019 39%.\n\nLi identifies retail and medical imaging as major use cases. In the month since AutoML\u2019s launch Google has consulted with many potential clients in the clothing industry. Clothing with the same color or pattern will have for example different necklines, cuffs etc. Retailers can use AutoML to define their own custom product classifications for such features.\n\nThe debut version of AutoML only supports computer vision models, but features such as speech, translation, video, and NLP are coming soon.\n\nFei-fei Li and Jia Li have said that Google\u2019s mission is to \u201cdemocratize AI\u201d by providing companies who can\u2019t afford their own AI talent a chance to hop on the fast track.\n\nWhile AutoML\u2019s emergence is good news for businesses, one fear is that the tech giants\u2019 cheap and generalized solutions will raise the bar for startups and reduce market penetration opportunities for their solutions.\n\nThe industry has so far responded favourably to AutoML. AI engineers are mostly happy that it eliminates laborious parameter tuning procedures. Businesses meanwhile can now introduce AI into their operations without the high cost of hiring AI engineers and data scientists. Google\u2019s AutoML boasts a growing client list that includes Urban Outfitters, Disney, and the Zoological Society of London and it is likely that Google and Microsoft will continue to expand in this space."
    },
    {
        "url": "https://medium.com/syncedreview/program-lead-jake-lussier-on-udacitys-new-flying-car-nanodegree-7aae00c64c41?source=user_profile---------105----------------",
        "title": "Program Lead Jake Lussier on Udacity\u2019s New Flying Car Nanodegree",
        "text": "Is the flying car merely a sci-fi clich\u00e9, or is it a viable transportation tool for the future? One way to find out is to develop talents to drive the concept to market \u2014 and that\u2019s what Udacity is doing with its new Flying Car Nanodegree Program.\n\nOnline education platform Udacity thrilled flying car enthusiasts with its announcement of the world\u2019s first flying car program. Applications are open until February 7, and the course will start at the end of February.\n\nThere are two terms: the fundamental-level Aerial Robotics and the advanced-level Intelligent Air Systems. Each term costs US$1,200 and runs for three months, with approximately 15 hours of weekly study.\n\nUdacity has specific prerequisites for beginners \u2014 admission requires good programming skills and basic math and physics skills. Graduates of the Udacity Robotics Software Engineer, Self-Driving Car Engineer, or Intro to Self-Driving Cars Nanodegree programs automatically qualify.\n\nUdacity has secured computer science and aerospace gurus as instructors, including Udacity Founder Sebastian Thrun, MIT Professor Nicholas Roy, University of Toronto Professor Angela Schoellig, and Zurich Federal Institute of Technology Professor Raffaello D\u2019Andrea.\n\nSynced sat down with Udacity Flying Car Nanodegree Lead Jake Lussier to delve into the program\u2019s details and the current state of flying car technologies in general.\n\nLussier: I think Udacity\u2018s higher level mission is to provide lifelong learning, and this is just a way of saying that we try to provide a learning experience that will help you keep your professional skills up-to-date so that you can continue to succeed and grow in your career.\n\nA flying car might sound very far out, but it follows the same model that we already see the existing aviation industry and with the booming drone industry. There is a great opportunity for students who have skills and autonomy to get jobs now, and we see that there\u2019s a lot of activity right now in flying cars and over the next few years it\u2019s really going to ramp up. So students who learn this stuff now are going to be really sought after.\n\nLussier: In essence, we\u2019re preparing our students to be very confident software engineers prepared to work in autonomy, in any part of flight and in flying cars specifically.\n\nIn the first term, Aerial Robotics really teaches the fundamentals of what one would need to develop any kind of robot, but we offer this specialization in the sky. There is planning, controls, and estimation. The simulation will be a drone in the sky and students will have the opportunity to port their code to an actual drone in flight.\n\nIn the second term, we go more into the flying cars specifics, and preparing for a world in which there will be a lot more of these vehicles. We will go into fixed wing flight, and we\u2019ll also talk about hybrid designs. Flying car design has not really converged on one design and has elements of quadcopter and elements of fixed-wing vertical-take-off-and-landing (VTOL) aircraft. We first cover fixed wing designs and how this relates to what they learned in the first term. Then we\u2019ll go a bit more into aircraft, how do the control problems change, how is stability different, and then we get into autonomous fleets.\n\nThen it gets into optimizing missions. If you want to deliver things at multiple places, you have to learn how to optimize to do that effectively. And then when you have entire fleets, how do you coordinate them all so that they operate efficiently, and everything works safely with any central authorities that they\u2019re talking to. So that\u2019s kind of the overarching trajectory of the curriculum.\n\nAt every stage the student will not just be learning in the classroom, they\u2019ll also be coding in the classroom, writing Python and getting immediate feedback, and then translating that code into C++ so it goes from code that works well just for understanding the concept, to code that is aircraft ready. Then you can take that C++ code and that\u2019s what you would actually potentially put on a drone.\n\nOverall, they will have an understanding of all these concepts as well as competency implementing these concepts. For the existing industry today, they could bring their skills and autonomy. They\u2019ll be positioned to really be leaders in that space.\n\nLussier: At the highest level, these students are going to be very skilled, with experience in building autonomous systems that are not only intelligent but also extremely reliable and robust. If you\u2019re doing image classification on the web, you will be familiar with training algorithms to just predict very well. If you get it very wrong. it\u2019s not a huge deal. But in flight it\u2019s a huge deal if you get it wrong. So it\u2019s not just a simple algorithm but you have to have a system point of view. All of our graduates will have that skill, which is pretty widely applicable.\n\nI think they can find places even in the financial industry where you have to really understand your predictor at a very high level. They can find a lot of roles with those skills. But then in flight specific, most planes fly autonomously for most of their time in this sky. It is really only take off and landing where a human is heavily involved. There are obviously a lot of fully autonomous planes. There\u2019s already a huge market of companies that could find use for these skills.\n\nThen there are the smaller but rapidly growing markets, where they\u2019ll have an opportunity to get leadership positions and really build out those systems. Drones is already a booming industry that was growing very rapidly, and the flying car industry although a bit smaller right now is going to have a similar kind of growth pattern.\n\nLussier: At a high level it requires a pretty good level of experience in some programming language. You need to be comfortable coding, preferably in Python, C++. You should also have a basic understanding of a bit more advanced mathematics, for example linear algebra probability statistics. A bit about physics since we\u2019ll cover aerodynamics slightly.\n\nIf you have not graduated from other programs but you have those skills, then in our admission process, you can explain your qualifications and we will review every application individually and just make sure that you have the skills necessary.\n\nLussier: There are a number of startups. For example, Kitty Hawk, which was founded by [Udacity Founder] Sebastian Thrun; Volocopter, which is testing autonomous air taxis in Dubai right now. There is Lilium out of Germany, Ehang in China, and Terrafugia is another one. The Aurora Flight Sciences, Joby. There\u2019s more, this is not an exhaustive list.\n\nAlso a lot of the larger companies are either developing these internally themselves or they are developing an acquisition. Aurora Flight Sciences for example was acquired by Boeing. Airbus acquired Bombardier Jet, and Volvo acquired Terrafugia.\n\nThere is a growing space of service providers in that industry. For example Iris Automation raised a Series A and they do collision avoidance using computer vision and AI.\n\nUBER Elevate is Uber\u2019s effort to develop a system of users who want to fly from point A to point B. Any flying car maker can enter, and Uber will provide the passengers.\n\nSo there are the flying car makers, component developers, and user platforms out there.\n\nLussier: One surprising thing is that the basic aerodynamics and the hardware components are largely there. The low level software is mostly there. In some ways, they\u2019re even easier than self driving cars because they don\u2019t have to deal with all the obstacles and people jumping out and randomness. I think this is something that gives us confidence that this actually won\u2019t take that long to get out there because the technology is largely there.\n\nThe remaining challenge is engineers who are competent at the intersection of aerospace in computer science, which is exactly why we\u2019re offering this course right now. We traditionally have a pool of aerospace engineers who are not very comfortable with computation, and are used to a slower iteration on products. And we have computer scientists who are more used to running machine learning on the web and not as comfortable running on real world robotic systems.\n\nOnce you have those engineers who are fluent in both, there are additional challenges in terms of not just getting a single vehicle to fly stably but to have a whole host of vehicles going 120 miles per hour in the sky. This kind of higher level systems thinking and system optimization is still a challenge today.\n\nLussier: Machine learning can be used in most of the process. A lot of flight does not permit black box, therefore we can\u2019t just get some training set, train a model and then hope that the performance is sufficiently good. On lower level control problems, we require that we have a physical model and that the performance of the vehicle abides by that model.\n\nBut then, a lot of these models have parameters so we can measure the efficacy of the vehicle by its desired path. Whenever you have that kind of setup where you have parameters, this always allows for machine learning where you can optimize those parameters over time based on your measure of success.\n\nOnce we move beyond that very low level stuff, when we look at the entire system, this is an optimization problem in the sky. You have a set of resources, and you have some objective that you want to maximize and then you use your data to try to minimize some error function. All levels of this flying car stack, we can use machine learning and AI to try to do a lot better than we might do just using simple programs in simple physical models.\n\nLussier: Safety is the utmost concern and will really impact public perception of this technology. It needs to be implemented in a way that is safe and also has a super strong track record. So what\u2019s important for the industry is to start having use cases where we\u2019re delivering real value to customers. When we show that value, people will appreciate as we do that it\u2019s safe, and they will gain trust in it.\n\nThe other challenges you speak to are definitely when you\u2019re getting things up into the sky. It\u2019s important to do so efficiently and if it\u2019s a really heavy load then it\u2019s going to require a lot of energy and battery, you would have to take that into consideration. That will be more a matter of battery technology progress tackling those problems where the value to the customer can be met by a solution that leverages the available battery technology.\n\nLussier: An analogous situation would be that of the the car industry where there is a very mature car market increasingly using intelligence. If you\u2019re a career-seeking engineer, there is a big market of car companies right now, and the self-driving car is high-growth.\n\nWith flying cars, we have a similar setup where there is an established aerospace market, and there is a smaller but rapidly growing drone and flying car industry.\n\nThe self driving car five years ago was in the space where the technology was starting to be taken more seriously but it was a very niche industry. But it was high growth. The flying car right now is in a similar kind of position in that we\u2019re in the early days of it but it\u2019s going to be high growth. The flying car has this nice advantage over the self driving car at that time, because the self driving car has carved out the path technically and socially for a lot of the progress that the flying car will have to make in terms of the AI, navigation, mapping, and coordination.\n\nSo, in the next one or two years, there\u2019s going to be a huge amount of growth, and we will see a lot more mass adoption over the next five years."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-american-startup-partnership-promises-a-self-driving-electric-suv-by-2020-8a54d516de47?source=user_profile---------106----------------",
        "title": "Chinese-American Startup Partnership Promises a Self-Driving Electric SUV by 2020",
        "text": "Chinese electric car startup BYTON says it will produce a self-driving SUV by 2020. The bold pledge came with the announcement of a partnership with Aurora, a Silicon Valley-based autonomous driving startup founded by Google, Tesla and Uber veterans.\n\nThe deal will incorporate Aurora \u2018s Level 4 autonomous driving solutions into BYTON\u2019s new electric vehicles, which can drive without human intervention in most conditions. Over the next two years, BYTON will deploy 100\u2013200 electric vehicles equipped with Aurora software in San Francisco for road testings and data collection.\n\nAurora will provide BYTON with its self-driving hardware kit, including requirements and integration guidelines for preparing the vehicle platform for self-driving. Aurora will also provide the self-driving software and data services required to operate the system.\n\nBYTON is a subsidiary of Nanjing-based Future Mobility. The company lured former BMW executive Carsten Breitfeld as its CEO, and has assembled a team with rich experience in car design and production.\n\nAt CES 2018 in Las Vegas earlier this year BYTON unveiled their concept car \u2014 a US$45,000 electric SUV featuring a breathtaking 1.25-meter in-car panorama screen and rotating front seats. BYTON cars equipped with a premium Advanced Driver Assistance Systems (ADAS) solution will be available in 2019 in China and 2020 in the US and Europe.\n\nBYTON chose Aurora to keep drivers\u2019 hands off the steering wheel with its expertise in L4/L5 autonomous vehicle solutions. The company was founded in 2016 by Chris Urmson, former head of Alphabet\u2019s autonomous driving affiliate Waymo; Sterling Anderson, who led the launch of Tesla Model X and directed Tesla Autopilot; and Drew Bagnell, who led Uber\u2019s Advanced Technology Center.\n\nAurora created a buzz in the industry earlier this year when it announced collaborations to develop onboard self-driving systems with Volkswagen AG and Hyundai Motor Co. The latter plans to mass produce L4 autonomous vehicles by 2021.\n\n\u201cBYTON is designed for the age of autonomous driving. We are pleased to partner with Aurora, as Aurora is supremely focused on a mission to deliver the benefits of self-driving vehicles safely, quickly and globally,\u201d says Breitfeld.\n\nThe BYTON-Aurora agreement covers pilot deployment in the US market, over the course of which both parties will explore broader application scenarios for Aurora\u2019s self-driving system and BYTON\u2019s production series vehicles.\n\nAlthough BYTON is a newcomer in the field of electric cars, it is catching up with existing automakers, and taking a brave step forward with its autonomous driving SUV project."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-february-pt-1-bf96d9ebb861?source=user_profile---------107----------------",
        "title": "AI Biweekly: 10 Bits from February (Pt 1) \u2013 SyncedReview \u2013",
        "text": "January 21st \u2014 Amazon Go Opens in Seattle\n\nThe staffless Amazon Go opens to the public in downtown Seattle. Just like a normal convenience store, this intelligent store sells pre-cooked food, snacks, drinks , etc. \u2014 but there\u2019s no cashier scanning the goods. Integrating an AI system and computer vision technology, Amazon Go uses weight detecting sensors and hundreds of ceiling-mounted cameras to monitor stock flow. Once customers check in with their ID, they can literally grab what they need and go.\n\nJanuary 22nd \u2014 Paris Is Becoming an AI Tech Center\n\nParis is attracting attention from tech giants. Facebook and Google both plan to increase investment in labs and talent recruiting in the French capital. Facebook announced it will double the number of employees at its Paris AI research center to 100; while Google will create an AI lab focusing on fundamental research such as automatic learning, NLP and computer vision.\n\nJanuary 22nd \u2014 Intel Providing VR Content for Motorsports\n\nIntel is determined to ride the rising wave of AI. In the ten months since the company created its dedicated Artificial Intelligence Products Group lead by Naveen Rao, Intel has secured several development deals in AI-driven immersive experiences for sports, including the NFL and now Motorsports. With a VR headset, viewers will be able to choose their own camera feeds for the upcoming Ferrari Challenge North America series.\n\nJanuary 22nd \u2014 How RBC Uses AI in Decision Making\n\nThe Royal Bank of Canada (RBC) is using artificial intelligence technologies in their capital-markets research to detect the effect of social media sentiment on corporate brands (Chipotle Mexican Grill, etc). By using AI to quantify the measurement of effectiveness, a company can track sentiments resulting from marketing strategies, etc. A company\u2019s earnings, sales predictions and analysis can benefit from the research, as can trading and investestment strategies.\n\nJanuary 24th \u2014 iFLYTEK\u2019s Translator and the Intelligence Behind It\n\nChinese tech company iFLYTEK releases a pocket-size instant translation device called Speak to Me. Based on advances in deep learning inferencing, the portable translator is accelerated by the NVIDIA Tesla P4 and P40 GPUs. iFLYTEK\u2019s performance data shows 97% accuracy and efficiency 15 times higher than CPUs. This technology will not only be used for translation, iFLYTEK is also collaborating with Chinese hospitals and medical institutions to improve patient experience\n\nJanuary 26th- Africa Will Hold an AI Expo 2018\n\nArtificial Intelligence Expo Africa 2018 will be held this September in South Africa. The event aims at promoting AI business opportunities. More than 400 delegates are expected to attend the Expo, which will feature 27 speakers along with 28 AI exhibitors. There will also be an innovation cafe housing 20 AI startups. The Expo comprises six themes and three tracks emphasizing real enterprise AI case studies and applications.\n\nJanuary 29th \u2014 Nvidia Collaborate with BHGE on AI for Oil and Gas\n\nGE\u2019s Baker Hughes and Nvidia are bringing an AI-based end-to-end solution to their business customers. This new solution will leverage advanced GPU and Supercomputer hardware and deep learning technologies to locate and maximize returns on resources. The project will also include maintenance, predication and operation functions for the oil and gas industry.\n\nJanuary 31st \u2014 Sony\u2019s Immersive Gaming Experience at SXSW\n\nSouth by Southwest (SXSW) is one of the largest creative festivals in the US, and Sony is expected to demonstrate its new immersive gaming technology at the upcoming Austin, Texas event. According to a leak, Sony will exhibit AR Air Hockey, which combines the classic arcade table game with augmented reality; and Hero Generator, a VR generator that enables visitors to create their own personalised VR avatars. Sony will also showcase other immersive experiences such VR soccer, etc.\n\nFebruary 1st \u2014 Amazon Bans Google Feature in Alexa\n\nWith the competition between virtual assistants Alexa and Google Assistant heating up, Amazon removes the command \u201cGoogle\u201d from the Alexa skillset. Alexa app developers will no longer be able to have Alexa respond to the verbal command \u201cGoogle\u201d. Amazon says the reason for the prohibition is that Alexa \u201cshould not promote Google Home.\u201d\n\nFebruary 2nd \u2014 Microsoft Launches the Cortana Intelligence Institute in Australia\n\nMicrosoft announces a collaboration with the Royal Melbourne Institute of Technology (RMIT) to establish the Cortana Intelligence Institute. RMIT researchers will collaborate with Microsoft personnel to apply AI to challenges that remain unsolved by neural networks. For specialization and cooperation, the Cortana Institute comprises two separate groups: A 1000-strong Microsoft Research division; and Cortana Research, a younger team focusing on virtual assistant technology."
    },
    {
        "url": "https://medium.com/syncedreview/spotlight-on-eight-african-ai-startups-3f25d650440d?source=user_profile---------108----------------",
        "title": "Spotlight on Eight African AI Startups \u2013 SyncedReview \u2013",
        "text": "According to the World Economic Forum, \u201cno part of the planet is urbanizing faster than sub-Saharan Africa. The continent\u2019s population of roughly 1.1 billion is expected to double by 2050.\u201d Other factors such as improved power supply and high-speed internet, more early-stage investment funding, and increase in smartphone use have created a fertile environment for Africa\u2019s emerging AI industry.\n\nLeading countries include Kenya and Ethiopia in the East, Nigeria and Ghana in West Africa, South Africa in the South, and Egypt in the North.\n\nSynced has identified eight exciting startups bringing AI to Africa\u2019s education, finance, and health revolutions.\n\nThe Egyptian startup Affectiva was launched in 2009 by Rana El Kaliouby, a pioneer in Emotion AI and former MIT research scientist. Affectiva\u2019s \u201cemotion recognition products\u201d draw on the company\u2019s extensive database to detect moods and make decisions based on facial expressions. More than 1,400 healthcare, automotive and gaming brands use its AI emotion technology. In 2016 the startup raised US$14 million in investment to attain US$34 million in venture capital, and in 2017 was cited in Forbes\u2019 10 Hottest Artificial Intelligence Technologies list.\n\nDataProphet is a South Africa-based startup founded in 2013 by Daniel Schwartzkopff, a graduate in Chemical Engineering from the University of Cape Town, and scenario planner Frans Cronje. It applies AI services to finance and insurance, with specialization in predictive analytics and advanced machine learning algorithms. The company has secured investment from YellowWoods Capital and has a sales office in San Francisco.\n\nKudi is a Nigerian fintech startup founded in 2017 that provides access to electronic banking and financial services by leveraging conversational interfaces, NLP, and AI technologies. Users can transfer money, track account details, and pay bills using its chat interfaces on Facebook, Telegram, Slack, etc. Kudi is also piloting business-to-business solutions with banks and telecommunication companies across Nigeria, with plans for expansion across Africa.\n\nThis Nigerian Educational startup matches qualified tutors with students according to area and budget. Tuteria tutors must maintain a user good rating to win more clients and compensation, and falling below a specified minimum level leads to teaching disqualification. Founder Godwin Benson is a Systems Engineering graduate from the University of Lagos, Nigeria.\n\nThe Ugandan fintech startup was launched in 2015 and offers tech solutions to microfinance institutions. Its platform helps digitize business procedures, credit information sharing, and many other services using mobile devices or biometric SaaS (software-as-a-service). Awamo has secured investments from the German Investment and Development Corporation, and plans to expand its networks across East Africa.\n\nAerobotics is a South African startup based in Cape Town, launched in 2014 by Benji Meltzer and James Paterson. The company uses machine learning to analyze maps and extract actionable information for crops such as wheat, citrus, and sugar cane. Its farming consultation services are used in South Africa, Australia, and the UK. Aerobotics participated in Startupbootcamp InsurTech\u2019s accelerator programme in London.\n\nThis South African startup creates AI-powered virtual advisors to drive sales and perform technical consultations. Clevva\u2019s platform enables companies to monitor and manage the virtual advisers they use to advise staff and/or customers. The startup\u2019s innovations attracted attention in the Microsoft BizSpark Program and at the Gartner Symposium ITxpo Africa 2015.\n\nFinChatBot was launched in June 2016 by Far Ventures. The company builts chatbots for client websites. It also monitors industry trends to increase sales conversion rates, predict customer needs and suggest business solutions. FinChatBox is based in Cape Town, South Africa."
    },
    {
        "url": "https://medium.com/syncedreview/woebot-delivers-a-daily-dose-of-mental-wellness-d8e30e2666a4?source=user_profile---------109----------------",
        "title": "Woebot Delivers a Daily Dose of Mental Wellness \u2013 SyncedReview \u2013",
        "text": "Psychological well-being impacts humans\u2019 ability to enjoy their lives, but most people can\u2019t recognize mental health warning signs unless they consult a professional psychologist. A Silicon Valley-based startup has now put the process into an AI-powered chatbot.\n\nWoebot is designed to identify, evaluate and treat mental health conditions such as depression, anxiety, and stress disorders based on Cognitive Behavioral Therapy (CBT), one of the most widely used mental health treatment frameworks. Woebot\u2019s recently released IOS app features an animated robot character that communicates with users using natural language processing (NLP) technology.\n\nWoebot Labs was founded by Stanford School of Medicine Psychologist Alison Darcy with the goal of making mental health evaluation accessible and even entertaining. Woebot is the first AI system to compete for a share of the mental health market, which in 2015 totaled $196 billion in the US alone.\n\n\u201cWe now understand that you need to look after your physical fitness everyday. I think people are reaching the understanding that actually mental health is something that you should also look after and actively seek self-care opportunities,\u201d says Dr. Darcy.\n\nWhile Woebot Labs is an early-stage startup, the company has garnered much attention in the AI community since AI guru and Coursera co-founder Andrew Ng joined Woebot\u2019s board as Chairman, assisting Dr. Darcy with growing the team, and advising on data and machine learning strategies.\n\n\u201cAndrew was telling me that he doesn\u2019t get involved in things unless he thinks they can be potentially the biggest contribution of his life. That\u2019s a huge bonus for us, and a very nice thing that he thinks we can be that impact,\u201d says Dr. Darcy.\n\nWoebot functions like a mental health mentor. It prompts users to check in daily, then asks about their temper, activities for the day, motivations and frustrations and so on. The questions are inspired by CBT, whose methodology is to treat mental illness by changing unhelpful patterns in behavior, emotional self-regulation, and thoughts.\n\nThe chatbot also encourages users to try new activities such as meditation, which is a common CBT therapeutic strategy. Nick Panchyshyn, co-Founder of smart life management and AI Assistant startup LifeMap, gave Synced a positive review of his Woebot experience. \u201cI enjoyed a lot of conversations that were really engaging. Woebot was also offering me different activities in a very nice manner.\u201d\n\nTo enable automated responses that are both accurate and natural, Woebot uses a combination of structured dialog along with natural language understanding (NLU). Woebot Head of Engineering Joe Doyle says the team uses both deep learning and NLU to understand users\u2019 moods and activities. Based on their mood, Woebot can guide users to appropriate content.\n\n\u201cWe utilize the FastText algorithm for some classification as well as applying some internally developed solutions based on popular machine learning frameworks such as TensorFlow. We also rely on many different techniques ranging from the use of simple Regular Expressions all the way to complex, deep neural networks,\u201d says Doyle.\n\nWoebot is now engaged in more than two million conversation per week, and Dr. Darcy has discovered interesting behavior patterns, for example, in how users express distorted thoughts, such as \u201cI\u2019m not smart enough\u201d or \u201cI shouldn\u2019t have said that.\u201d\n\nWoebot includes quizzes designed to educate users about cognitive distortions and distorted thoughts, and help them identify negative words and correct cognitions. \u201cThat is why NLP is going to be so fundamental to transform mental health care,\u201d says Dr. Darcy.\n\nLast June, Dr. Darcy and her colleague Kathleen Kara Fitzpatrick published a randomized trial of Woebot in the Journal of Medical Internet Research. They recruited 70 people who self-reported depression or anxiety, and had them either use Woebot or read a mental health e-book for two weeks. Test subjects using Woebot experienced a higher reduction in depression then the book group.\n\nThis certainly does not mean Woebot is effective enough to replace professional therapists. John Torus, Chair of the American Psychiatric Association\u2019s Smartphone App Evaluation Work Group, said in a Washington Post interview that \u201cthese things can work well on a superficial level with superficial conversations. Are they effective tools, do they change outcomes and do they deliver more efficient care? It\u2019s still early.\u201d\n\nDr. Darcy does not want to exaggerate Woebot\u2019s capabilities. \u201cWoebot is a guide and that is a legitimate role in the field of mental health. Woebot is not going to be able to deliver you therapy but he can certainly ask you the right questions to help people figure out things on their own.\u201d\n\nWoebot still needs improvement: it remains only an IOS application, has relatively poor conversational capabilities and a narrow spectrum of clinical skills. But Dr. Darcy is dreaming big: \u201cI would like Woebot to become a household name, known as a reliable effective helper that you can reach out to at any time in daily life.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/the-aaai-2018-keyword-is-learning-d0aefcccde3a?source=user_profile---------110----------------",
        "title": "The AAAI 2018 Keyword is \u201cLearning\u201d \u2013 SyncedReview \u2013",
        "text": "AI academics from around the world are pouring into New Orleans for the Thirty-Second AAAI Conference on Artificial Intelligence. The five-day event is an all-inclusive technical overview of AI research frontiers. Program chairs are Sheila McIlraith from the University of Toronto and Cornell \u2018s Kilian Weinberger.\n\nSince its inception in 1979 as the \u201cAmerican Association of Artificial Intelligence,\u201d the annual gathering has expanded scope to encourage international participation. In 2012 the conference was first held outside the US, in Toronto, Canada; and last year organizers shifted the schedule to avoid conflicting with the Chinese New Year and better accommodate Chinese academics.\n\nCompared to other prestigious conferences like the IJCAI, ICML, and NIPS, the AAAI takes a broader approach with its topic selection, which includes search, planning, knowledge representation, reasoning, NLP, robotics and perception, multiagent systems, statistical learning, and deep learning.\n\nThe AAAI 2018 selection committee received a record high of over 3,800 paper submissions, accepting 933 for an acceptance rate of just under 25%, down about 5% from last year when 789 papers made it out of 2590. The number of AAAI paper submissions has risen steadily since 2013.\n\nA word-frequency search of the papers reveals this year\u2019s top keywords: (in descending order) learning, multi, neural, deep, networks/network, adversarial, attention, model, data, detection.\n\nMeanwhile, keywords \u201cdata\u201d and \u201cinformation\u201d have decreased in use frequency compared to last year.\n\nAAAI 2017 registered a high of 1,692 participants, and we are bound to see an increase this year.\n\nOrganizers identified Human-Computer Interaction (HCI) as an Emerging Topic this year. There are four talks and 21 technical papers on HCI, focused on trustable and explainable AI, teamwork/team formation, human-aware planning and behaviour prediction, planning and decision support for human-machine teams, human-agent negotiation, human-robot/negotiation, human-robot/agent interaction, human-in-the-loop/learning, human computation, and human and AI communication protocols.\n\nThe AAAI\u2019s first Oxford-style debate on \u201cAdvances in Machine Learning have displaced the need for logic in AI\u201d is expected to ignite discussion in the Twitterverse. Debating \u201cfor\u201d will be former AAAI chair Tom Dietterich and Bart Selman from Cornell, while Gary Marcus from NYU and IBM researcher Francesca Rossi will argue \u201cagainst.\u201d\n\nEarlier this month, Dietterich responded to Marcus\u2019 Critique of Deep Learning with a volley of contentious Tweets. Their first public debate will certainly be fiery.\n\nAAAI 2018 will also present the first AAAI/ACM Conference on AI, Ethics, and Society.\n\nThe AAAI-18 Outstanding Paper Award was won by the University of Alberta\u2019s Memory-Augmented Monte Carlo Tree Search, which proposes a new method called Memory-Augmented Monte Carlo Tree Search (M-MCTS), combining the original MCTS algorithm with a memory framework, to provide a memory-based online value approximation. Performance was evaluated in the game of go, showing better results than vanilla MCTS.\n\nPaper co-author Jincheng Mei says their next application scenario for MCTS \u201cis complex reinforcement learning tasks, for example in strategy games.\u201d Another of the co-authors, Dr. Martin M\u00fcller, told Synced, \u201cwe are also applying MCTs in the game of Hex, with PhD student Chao Gao and Dr. Ryan Hayward.\u201d It\u2019s noteworthy that Dr. M\u00fcller mentored David Siler and Aja Huang, the first and second authors of AlphaGo\u2019s celebrated Nature paper.\n\nThe Outstanding Student Paper award went to Oxford University\u2019s Counterfactual Multi-Agent Policy Gradients, which features a new reinforcement learning method that can efficiently learn decentralized policies in cooperative multi-agent systems. The multi-agent actor-critic method called Counterfactual Multi-Agent (COMA) was evaluated in StarCraft games, which is a challenging reinforcement learning benchmark task.\n\nFebruary 4\n\nRao Kambhampati (Presidential Address) \u2014 Challenges of Human-Aware AI Systems\n\nYejin Choi \u2014 From Naive Physics to Connotation: Learning and Reasoning about the World using Language\n\nFebruary 7\n\nPercy Liang \u2014 How Should We Evaluate Machine Learning for AI?\n\nAI and the Web (AIW)\n\nApplications (APP)\n\nCognitive Modeling (CM)\n\nCognitive Systems (CS)\n\nComputational Sustainability and AI (CSAI)\n\nGame Theory and Economic Paradigms (GTEP)\n\nGame Playing and Interactive Entertainment (GPIE)\n\nHeuristic Search and Optimization (HSO)\n\nHuman-AI Collaboration (HAC)\n\nHuman-Computation and Crowd Sourcing (HCC)\n\nHumans and AI (HAI)\n\nKnowledge Representation and Reasoning (KRR)\n\nMachine Learning Applications (MLA)\n\nMachine Learning Methods (ML)\n\nMultiagent Systems (MAS)\n\nNLP and Knowledge Representation (NLPKR)\n\nNLP and Machine Learning (NLPML)\n\nNLP and Text Mining (NLPTM)\n\nPlanning and Scheduling (PS)\n\nReasoning under Uncertainty (RU)\n\nRobotics (ROB)\n\nSearch and Constraint Satisfaction (SCS)\n\nVision (VIS)"
    },
    {
        "url": "https://medium.com/syncedreview/ces-2018-autonomous-driving-tech-review-464c56fee78a?source=user_profile---------111----------------",
        "title": "CES 2018 Autonomous Driving Tech Review \u2013 SyncedReview \u2013",
        "text": "The term \u201cautonomous driving\u201d arrived in the vernacular at CES 2013, when Audi and Toyota unveiled their self-driving cars. Over the following 12 months major car manufacturers rushed to announce their own autonomous development blueprints or self-driving car prototypes. Google searches for the synonymous term \u201cself-driving\u201d increased sixfold from 2013 to 2017 \u2014 with peak search times corresponding with each CES.\n\nAt CES 2018, 555 companies or organizations participated in the \u201cAutomotive/Vehicle Technology\u201d category, 23% of which were companies using autonomous driving technologies. Featured products can be roughly divided into six categories: Sensors, Perception and Decision, Control, Human-Machine Interaction, V2X, and Miscellaneous.\n\nSensors are any devices which can provide the vehicle system with raw signals. Generally speaking, this includes GPS, IMU, camera, LiDAR, radar, thermal camera, and so forth. At CES 2018, 23% of the autonomous driving related companies were sensor technology companies, half of which were from Israel. Most of showcase releases were in the LiDAR and thermal camera fields.\n\nOne of the most biggest challenges in core autonomous driving application is perception, which refers to the ability to draw relevant conclusions from the raw sensor data using data processing, data calibration and fusion, scenario reconstruction, localization, and obstacle detection. Because chipsets are the key hardware for data collection and processing in perception, chip manufacturers have also been included in this category.\n\nDecision is the process of making choices based on known data. For example, to help the vehicle take action when data indicates there is an obstacle on the road. At 2018 CES, 33% of autonomous driving related companies were in the perception and decision category, with 20 focused on perception and 22 on decision. Hot topics this year were hardware processor chipsets and Lyft\u2019s Aptiv project.\n\nControl refers to the ability to identify and process decisions into real vehicle actions. For example, if there is a decision ordering the vehicle to brake, control will determine how to activate the brake and steering functions to give passengers the best driving experience. At CES 2018 there were only 12 companies in this category, less 10% of all self-driving related companies. Star attractions at CES 2018 included Xpeng\u2019s G3 SUV 360\u00b0 camera system, and Nissan\u2019s IMx SUV.\n\nHMI is the human-machine interface that connects an operator to the controller in a computer system. This comprises a variety of services and functions to support autonomous driving, for example, voice assistants, smart alarms, and in-car interfaces. There were 16 companies working in this area at CES 2018. Among the most interesting showcases was Autolabs\u2019 virtual assistant project and Steering AI\u2019s smart alarm system.\n\nV2X (Vehicle-to-Everything) indicates the communication system between a vehicle and anything else. The category includes V2I (Vehicle-to-Infrastructure), V2V (Vehicle-to-Vehicle), V2P (Vehicle-to-Pedestrian), V2D (Vehicle-to-Device) and V2G (Vehicle-to-Grid). Fifteen V2X companies released blueprints or new products at CES 2018. Especially noteworthy were Qualcomm\u2019s V2X network and RoadEyes\u2019 resSMART camera.\n\nVarious autonomous driving technologies cannot be pigeonholed into the above five categories. Baidu\u2019s self-driving development platform Apollo is one of them. Apollo 2.0 is a major release that not only changes the way autonomous driving technologies are developed but also forms a worldwide ecosystem for this technology.\n\nAmerican companies remained major players in the self-driving field at CES 2018, accounting for just under 30% of the total. Most were from California, suggesting the next autonomous driving wave will most likely emerge from Silicon Valley. Meanwhile, over 40% of sensor companies were Israeli.\n\nCES 2018 data also indicates strong growth for self-driving tech companies in Asia, as more Japanese, South Korea and Chinese companies are entering the market. In China dozens of startups and high-tech companies have shown their capabilities in this area.\n\nBased on trends at CES 2018, it\u2019s easy to see that competition is heating up in the autonomous driving industry, and more investment will be poured into this area. However, such investment may focus on specific breakthrough technology rather than the general self-driving field. LiDAR and Autonomous Driving Processor Chipsets could be two major battlegrounds. Moreover, based on the various partnerships announced at CES, we can envision self-driving competition evolving to the ecosystem level. Dozens of companies aim to have their full set self-driving mass production vehicles on the road by the end of 2018 or early 2019.\n\nAlthough autonomous driving technology has been under development for some time now, 2018 may be the first year that self-driving vehicles actually appear in our everyday lives."
    },
    {
        "url": "https://medium.com/syncedreview/will-new-eu-regulations-starve-data-hungry-deep-learning-models-25403795d26c?source=user_profile---------112----------------",
        "title": "Will New EU Regulations Starve Data-Hungry Deep Learning Models?",
        "text": "The European Union\u2019s new General Data Protection Regulation (GDPR) may outlaw some AI algorithms and data collection practices, warns University of Washington Professor Pedro Domingos, author of the seminal AI introduction The Master Algorithm.\n\nThe GDPR will come into effect on May 25, 2018, replacing the 1995 Data Protection Directive. The regulation aims to \u201charmonize data privacy laws across Europe, to protect and empower all EU citizens data privacy and to reshape the way organizations across the region approach data privacy,\u201d according to the GDPR website, bringing Europe \u201cthe most important change in data privacy regulation in 20 years.\u201d\n\nWhat exactly will change? First, the GDPR applies to all companies processing the personal data of EU citizens regardless of company location. Second, breaching the GDPR will carry stiff penalties, with companies facing fines of up to 4% of their global turnover, or \u20ac20 million. Third, companies will be required to procure customer consent in clear and comprehensible language, allow easy user withdrawal from data collection, and notify local EU Data Processing Authorities of data usage.\n\nEU citizens will be entitled to have their data erased or transferred to another processing agent, and must be notified immediately if their data is misused.\n\nThe new regulation will be detrimental to data mining operations and AI startups, and big tech companies will be scrutinized for using web browsing cookies or social media profiles. Deutsche Bank estimates the GDPR could erase 2% of Google\u2019s revenue.\n\nAlong with its regulations on the use of private data, the GDPR also imposes adverse provisions on scientists and researchers who are deploying machine learning algorithms, especially in deep learning.\n\nSection 4, article 22 of the GDPR requires a \u201cright to explanation\u201d for users on all decisions made by automated or AI algorithm systems. According to the context, a data subject has the right to opt out of a decision made by automated processing or AI that produces legal effects on them, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention.\n\nThe GDPR mandates that all customers or users are entitled to receive \u201cspecific information\u201d regarding the decision. This raises the issue of AI interpretability, especially in deep learning systems. An Oxford Internet Institute research paper predicts the GDPR\u2019s right to explanation will directly impact machine learning.\n\nResearchers and data scientists regard the interpretability of AI as very challenging, especially given that recent advancements in AI are based on deep learning and deep reinforcement learning. The most common fundamental algorithms used in these models learn their objective functions and adjust weights automatically as the learning proceeds via iterative epochs based on huge amounts of labelled data.\n\nResearchers do not know how the learned weights achieve the best result, and it is difficult to determine what kinds of learning the model has achieved. Accordingly, such systems are referred to as \u201cblack-box\u201d mechanisms wherein humans provide the data, the model, and the architecture \u2014 and then the computer outputs the answer.\n\nProf. Domingos\u2019 \u201cmaking deep learning illegal\u201d tweet ignited a heated debate on AI and the GDPR. Some criticized him for misrepresenting the regulation. Eric Topol, an renowned American cardiologist, replied \u201cthat kinda makes human intelligence \u2014 which is unexplainable \u2014 illegal by inference.\u201d\n\nDirector of Product Marketing for Cloudera Data Science Thomas Dinsmore blogged that the GDPR\u2019s adverse impact on deep learning might be exaggerated because a right to explanation has already existed for years and the GDPR is simply expanding the scope and applying such rules to automated processing. \u201cThe need to deliver an explanation affects decision engines but need not influence the choice of methods data scientists use for model training,\u201d wrote Dinsmore.\n\nThe premise and definition of the GDPR\u2019s \u201cright to explanation\u201d is also opaque. Oxford researchers expressed doubts in a January 2017 paper, noting several issues regarding the regulation\u2019s feasibility, such as \u201cit lacks precise language as well as explicit and well-defined rights and safeguards against automated decision-making, and therefore runs the risk of being toothless.\u201d\n\nFor their part, machine learning scholars have over the last two years put increasing focus on the interpretability of AI technologies. IJCAI 2017 held a special workshop on Explainable Artificial Intelligence (XAI); while NIPS 2017 hosted a debate on whether interpretability is necessary for machine learning.\n\nOther efforts in academia hint at the possibility of unveiling the black box\u2019s mysteries. The Defense Advanced Research Projects Agency (DARPA) initiated an XAI program in order to turn the \u201cblack-box\u201d into a \u201cglass-box\u201d so that a \u201chuman-in-the-loop\u201d would know the reasoning behind decisions. Local Interpretable Model-agnostic Explanations (LIME) is a more well-known study aiming to explain black-box algorithms by starting from simpler models in the field of object recognition and text classification.\n\nMeanwhile, many practical AI applications do not demand interpretation. Machine translation for example has been revolutionized by AI technologies, and users are much more concerned with the translation results than how they were generated.\n\nDespite the alarmism from Prof. Domingos and some others in the AI community, the GDPR will not outlaw deep learning. It will however impact big data collection and usage with the goal of protecting users\u2019 privacy, and may even encourage researchers to find a way to open the black box and look inside."
    },
    {
        "url": "https://medium.com/syncedreview/geoffrey-hinton-on-images-words-thoughts-and-neural-patterns-82db0bd04a09?source=user_profile---------113----------------",
        "title": "Geoffrey Hinton on Images, Words, Thoughts, and Neural Patterns",
        "text": "The \u201cGodfather of AI\u201d Professor Geoffrey Hinton told a packed room of professionals and students today that machine learning is reinventing our relationship with our own thoughts. The Professor mused on AI, deep learning, and the nature of intelligence in his 60-minute talk at the University of Toronto.\n\nProfessor Hinton has been contemplating the functioning mechanism of brains for decades. In the talk he asked the question \u201cWhat is a thought?\u201d His response: \u201cIt is a big neural activity pattern. We refer to it by using a symbol string that causes it, but the way we refer to it is quite different from what it is.\u201d\n\nThe Professor comes from an illustrious family of forerunners and scientists. His great-great-grandfather George Boole came up with Boolean logic, which made modern computing possible; his great-grandfather was a mathematician; and his father\u2019s cousin Joan Hinton was a nuclear physicist and one of the few women scientists who worked for the Manhattan Project in Los Alamos.\n\nProfessor Hinton rose to fame at the 2012 ImageNet object recognition challenge, which comprised one million high-resolution training images of 1,000 different classes of objects. For the task, Hinton\u2019s team designed AlexNet, an eight-layer network with five convolutional layers, which performed an impressive 10% better than the University of Tokyo runners-up.\n\nProfessor Hinton\u2019s success applying convolutional neural networks to pixels was soon repeated with words, revolutionizing the field of machine translation, which had been a nagging problem for symbolic AI. The artificial neural network method \u201cconverts the input sentence into a big pattern of neural activities that is language independent. The pattern is a thought vector, which will be converted into a sentence in the target language,\u201d explained Professor Hinton.\n\nIn order to deal with sequential language, text data is fed into an encoder recurrent neural network (RNN) and a decoder RNN. This is the method Google Translate uses to translate some 100 billion words daily. The rise of neural networks has been good news for machine translation, but, as Professor Hinton quipped, \u201cvery bad news for linguists like Chomsky who insist that language is innate and you can\u2019t learn it through data.\u201d\n\nProfessor Hinton also spoke on the human thinking process and analogical reasoning, and philosophized on the nature of our sensory data \u2014 whether all we see, hear and even think could be represented in ways different from how we understand them to be. \u201cSentences can evoke thoughts but they are nothing like thoughts. An array of pixels can evoke a scene, but our representation of a scene is not an array of pixels.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/andrew-ng-launches-175-million-ai-fund-9d4373b04bbd?source=user_profile---------114----------------",
        "title": "Andrew Ng Launches $175 Million AI Fund \u2013 SyncedReview \u2013",
        "text": "As one of the top minds in artificial intelligence, Andrew Ng wears many hats: scientist, entrepreneur, educator, advisor, etc. Now he\u2019s added a new one: investor.\n\nThe former Google and Baidu Chief Scientist and Coursera co-founder today announced the foundation of \u201cAI Fund,\u201d aimed at supporting startups that can transform businesses with AI technologies. AI Fund has thus far raised a staggering US$175 million from VC firms NEA, Sequoia, Greylock Partners, and the SoftBank Group.\n\nNg is AI Fund\u2019s General Partner, while former Fenwick & West partner Eva Wang will serve as COO, bringing considerable operational and legal expertise. The fund\u2019s third partner is Coursera Product Manager and former Sycamore CEO Steven Syverud, who has rich experience in business development.\n\nNg says AI Fund will not operate like traditional venture capital companies that \u201cscreen thousands of applicants in order to find a few to invest in.\u201d Instead it will proactively identify promising AI talents and help them build companies from scratch. If the company evolves a viable AI-based system and business model, AI Fund will provide capital for further scaling. \u201cBuilding AI businesses is a more systematic and repeatable process than most people think,\u201d says Ng.\n\nA similar AI-development methodology can be found at Element AI, a Montreal-based platform founded in 2016 that incubates advanced AI-First solutions. Last year, the company raised US$102 million in Series A funding for hiring talent, developing businesses, and investing in platform-specific AI solutions.\n\nOne of AI Fund\u2019s first investments will be in Landing.ai, Ng\u2019s own recently launched startup which accelerates AI implementation in manufacturing. AI Fund is currently exploring several other AI-powered initiatives which it expects to announce over the next few weeks.\n\nAI Fund\u2019s philosophy will permit portfolio companies to operate in stealth mode, enabling them to conceal their core technologies, upcoming products and business models from competitors\u2019 prying eyes.\n\nLast year Ng gestated two important AI companies \u2014 Landing.ai, and Deeplearning.ai, which offers online courses in machine learning. AI Fund is the latest step toward Ng\u2019s goal of building a supportive ecosystem for AI opportunities, projects and products."
    },
    {
        "url": "https://medium.com/syncedreview/alibabas-cloud-computing-division-alibaba-cloud-announced-today-that-its-ai-powered-et-city-ea482ac7e286?source=user_profile---------115----------------",
        "title": "Kuala Lumpur Gets Alibaba AI\u2019s First Brain Implant \u2013 SyncedReview \u2013",
        "text": "Alibaba\u2019s cloud computing division Alibaba Cloud announced today that its AI-powered \u201cET City Brain\u201d will be deployed in the Malaysian capital Kuala Lumpur in partnership with the Malaysia Digital Economy Corporation (MDEC), the country\u2019s digital economy development agency, and Kuala Lumpur City Hall (DBKL). The system uses real-time video collected by street cameras and aims for traffic optimization based on machine learning and cloud computing. The initial phase of the project will be installed at 281 intersections.\n\nThis is the first known use of AI in Malaysian urban planning and governance, and ET City Brain\u2019s debut outside China. \u201cMalaysian government officials are very open-minded and rational, they are open to new endeavors, and we have great admiration for them,\u201d said Alibaba Cloud President Simon Hu, who also said he believes Malaysia is on the fast track of digitalization compared to other Southeastern countries such as neighbour Singapore.\n\nKuala Lumpur traffic is a serious problem \u2014 suburban commuters can spend up to two hours navigating the morning rush hour. On a visit to Alibaba\u2019s Hangzohu headquarters last May, Malaysian Prime Minister Najib Razak expressed interest in smart city technology, inviting Alibaba CEO Jack Ma to make it viable in Malaysia. Half a year later the project has landed in Kuala Lumpur.\n\n\u201cWith its massive cloud computing and data processing capabilities, City Brain can optimize the flow of vehicles and traffic signals by calculating the time to reach intersections. It will also be able to generate structured summaries of data, such as traffic volume and speed in particular lanes, which can be used to facilitate other tasks including incident detection,\u201d according to an Alibaba press release.\n\n\u201cIn addition, City Brain can connect with various urban management systems including emergency dispatch, ambulance call, traffic command, and traffic light control. By integrating and analyzing real-time data generated from these systems, City Brian can optimize urban traffic flow such as by identifying the quickest route for emergency vehicles to arrive at the scene within the shortest time frame.\u201d\n\nAlibaba Cloud concurrently launched Malaysia Tianchi, a local arm of its (Sky Pool) Big Data Program \u2014 a crowd-intelligence platform designed to find solutions to real-world problems. The global platform now has over 120,000 developers, and is used by 2,700 academic institutions and businesses in 77 countries. It\u2019s hoped that Malaysia Tianchi will cultivate up to 500 homegrown data professionals and spin off 300 startups over the next two years.\n\nOperations will be based on the Alibaba Cloud Data Center in Kuala Lumpur, which opened in October 2017 and provides cloud services to local enterprises. Alibaba has already granted Cloud Training & Certification (ACP) credentials to 100 local operators.\n\nWhile Malaysia marks Alibaba Cloud\u2019s first international real-world application, it won\u2019t be the last. Speaking at the opening ceremony, an optimistic Hu predicted, \u201cIn five years, global operations will contribute 40%-50% of Alibaba\u2019s total revenue.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/trio-of-chinas-big-state-banks-invest-in-4paradigm-ai-72f444ad5e8b?source=user_profile---------116----------------",
        "title": "Trio of China\u2019s Big State Banks Invest in 4Paradigm AI",
        "text": "January 26th, Beijing-based AI firm 4Paradigm has announced strategic investments from the state-owned Industrial and Commercial Bank of China, Bank of China and China Construction Bank; making it the only AI startup to get funding from three major Chinese state-owned banks. 4Paradigm had previously completed an A round with Sequoia Capital China, A+ round with Sinovation Ventures, and B round with Genesis Capital and Zhongwei Capital. \n\n \n\nThe AI industry is changing, as many companies now prioritize securing client orders over winning AI competitions. 4Paradigm has followed this strategy since the 2016 introduction of its large-scale distributed machine learning platform \u201cProphet.\u201d To date, Prophet has helped clients in the finance industry boost performance in verticals like targeted client-booking, personalized recommendation, application anti-fraud, transaction anti-fraud, overdue or loss warning, liquidity management, smart collection, and disposal of non-performing assets.\n\n \n\nNow the company is upgrading Prophet and taking a more comprehensive approach to developing full stack solutions for enterprises.\n\nCEO Wenyuan Dai tells Synced, \u201cWe have finished many industry benchmark cases in the past few years. We have worked with dozens of big banks, in some cases boosting performance by more than 100%. This attracted the attention of state-owned banks, who hope to develop long-term cooperative relationships with us. After this round of investment, we will expedite AI product development and provide AI solution plans in China\u2019s banking, insurance, and securities industry, so that eventually all enterprises and users can enjoy the added value of AI.\u201d \n\n \n\n4Paradigm is now building customized AI brains for its clients, which can potentially spawn hundreds or even thousands of AI applications. Guangdong Development Bank, for example, has already built ten different AI applications based on 4Paradigm\u2019s core systems. The company\u2019s solutions also allow other AI companies to build on its systems. Banks are benefiting from bundled solutions provided by different startups, streamlining the cooperation process. \n\n \n\n\u201cWe started by working on vertical domains, like anti-fraud systems and targeted marketing because initially, no big banks would delegate core businesses to a startup. After three hard years, banks are handing us their trusted \u2018credit cards\u2019. Given this trust, we will work on full upgrades progressively,\u201d explains Dai. \n\n \n\nCEO Dai is a respected transfer learning expert with who served as the principal researcher at Huawei Noah\u2019s Ark Lab. Company co-founder Qiang Yang is the Chair Professor at HKUST in Hong Kong, an ACM fellow, and the founding head of Noah\u2019s Ark Lab which has published over 400 papers in the field. \n\n \n\n4Paradigm\u2019s staff expanded from just a few people to some 300 last year, with the product development team particularly well represented. The company also focused on accumulating data and building multiple algorithm models in 2017; and opened Paradigm University to fast-track the education of its own AI talent using the Prophet platform. Each data scientist will coach several students, providing a robust talent pipeline for the company. \n\n \n\n Read here for more 4Paradigm AI use cases."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-rideshare-unicorn-didi-launches-ai-lab-in-beijing-f71ae7c847ca?source=user_profile---------117----------------",
        "title": "Chinese Rideshare Unicorn DiDi Launches AI Lab in Beijing",
        "text": "China\u2019s ridesharing unicorn DiDi Chuxing unveiled its \u201cDiDi AI Labs\u201d today in Beijing. The new AI lab expands on the DiDi Research Institute and DiDi Labs, and will cluster over 200 AI scientists and engineers to \u201cimprove user experience and transportation efficiency and further enhance [the] intelligent transportation ecosystem,\u201d according to a company press release. Didi AI Labs will be led by company Vice President Professor Jieping Ye.\n\nDiDi manages some 20 million daily rides and serves 400 million active users. The company sits on an impressive valuation of US $56 billion, and in 2016 bought Uber China\u2019s operation for US $7 billion, further consolidating its domestic rideshare monopoly.\n\n \n\nDiDi Labs focuses on big data and smart traffic and conducts AI research on NLP, computer vision for operations research, deep learning, statistics, etc. Research cases include predicting user destination, suggesting pick-up points, intelligent order dispatching, estimated time of arrival (ETA), and routing optimization.\n\nAI Labs head Jieping Ye is a University of Michigan professor and a specialist in machine learning, data mining and analytics, and specializing large-scale sparse model learning. In an April 2017 lecture at Peking University, Prof. Ye said DiDi had accumulated so much data that \u201cif you do the model, sample size can easily go up to billions.\u201d DiDi Rideshare vehicles submit updated GPS data every few seconds, processing 2000 terabytes each day.\n\n \n\nAt the lab launch DiDi also showcased its \u201cDiDi Smart Transportation Brain\u201d, a tech solution for smart city traffic management integrated with local traffic authorities that has already been adopted by more than 20 Chinese cities. Smart Traffic Signals installed at 344 road intersections in the city of Jinan have saved 30,000 hours of travel time and shortened traffic delays by 10\u201320%. The system is now deployed at 1,200 intersections nationwide. \n\n \n\nDiDi Labs is becoming increasingly active and visible in global AI. Last year it published the paper A Taxi Order Dispatch Model based On Combinatorial Optimization at leading international data conference KDD, participated in computer vision conference CVPR, and was a NIPS Platinum Sponsor."
    },
    {
        "url": "https://medium.com/syncedreview/tech-giants-talk-conversational-ai-at-re-work-bc6dcffb0c40?source=user_profile---------118----------------",
        "title": "Tech Giants Talk Conversational AI at RE\u2022WORK \u2013 SyncedReview \u2013",
        "text": "Automating human-like conversations is much more difficult than you might imagine. Even the world\u2019s top conversational AI systems \u2014 Amazon Alexa, Apple\u2019s Siri, and Google Assistant \u2014 remain far from the stage where they can smoothly process unanticipated human requests while also keeping humans engaged with natural responses.\n\nAt the RE\u2022WORK AI Assistant Summit San Francisco yesterday, research scientists and engineers from Amazon, Apple and Google spoke on how they are addressing challenges and evolving their conversational AI systems.\n\nFirst at the podium was Google AI Research Engineer Dr. Pararth Shah, who described a dilemma in developing a conversational AI bot: on one hand, a rule-based bot, which researchers code with each new capability, has low recall in unanticipated interactions and no self-learning capability; on the other hand, a research-focused bot whose model is trained from data encounters the double challenge of costly dataset collection and annotation, and diminished control over the bot\u2019s behavior.\n\nTo achieve both control and flexibility at scale, Dr. Shah proposed using self-play to build conversational AI. Researchers would first create both a user simulator bot and a rule-based bot. Given a specific task scenario, for example a restaurant date chat or booking a movie ticket, the two bots would talk to each other for five minutes.\n\nTheir conversation would be translated into natural (colloquial) dialogue by crowdsourced human labor, then added to an existing neural net-based bot training dataset, aka Supervised Learning. The neural net-based bot would then re-engage with the user simulator and be given reward/punishment based on its performance, aka Reinforcement Learning. Finally the bot could converse with flesh-and-blood users to get real feedback, aka Interactive Reinforcement Learning.\n\nThe initial version of Siri\u2019s NLU was a rule-based system wherein researchers started with vocabulary maps and external knowledge bases for features, rule-based bottom-up tree traversal of the query to compose an intent, and intent rankings enabled by hand-coded weights. The system was deterministic and interpretable, and could easily handle unambiguous requests. However, researchers found it difficult to add new functionalities or improve accuracy.\n\nSiri then proceeded to the next level, from rule-based to machine learned. The new iteration revamped how researchers designed each functionality. For example, Siri researchers reformulated the ranking problem of domain chooser as a classification problem, and adopted Support Vector Machines (SVMs), a sort of supervised learning model.\n\nResearchers deployed two different strategies \u2014 tree structured parses and shallow parses \u2014 for parsing, a process that analyzes a string of natural language or computer language symbols in accordance with grammatical rules. Researchers discovered that the majority of Siri requests required only shallow parses, which was more accurate, faster to train, and more suited to producing annotations compared to tree structured parses. Researchers also began to apply a statistical modeling method Conditional Random Fields (CRFs) to parsing.\n\nTo support their new machine learned models, Apple researchers designed a better UI interface for annotators and developers, a training and prediction system to evaluate impact of new or edited examples, deployments of models to runtime servers, and metrics that indicate performances.\n\nAs deep learning blossomed over the last five years, Siri researchers realized it could deliver better performance than traditional machine learning methods, especially in data-intensive conditions. They switched their previous models from SVMs for domain chooser and CRFs for parsing to Long Short-Term Memory (LSTM) \u2014 a type of Recurrent Neural Network capable of learning long-term dependencies.\n\nSiri researchers choose LSTM for a good reason. In the task of domain choosing for example, LSTM applies one model to all domains, reduces feature spaces to 1,300 (SVM required 500k), captures long-dependency of queries, and achieves better accuracy and generalization.\n\nDr. Kothari finished with an overview of other fields Siri\u2019s NLU researchers are exploring, including semantic labeling, reinforcement learning, questions answering, conversation modeling, etc.\n\nAlexa Prize \u2014 Advancing the State of the Art in Conversational AI\n\nAmazon Alexa is leading the conversational AI race with a 70 percent smart speaker market share. The company is aggressively working to build on this advantage by organizing contests with enticing cash prizes for third-party developers.\n\nAmazon Lab126 AI Scientist Dr. Chandra Khatri introduced the new Amazon Alexa Prize, a competition that challenges participants to create state-of-the-art social chatbots that can converse coherently and engagingly on popular topics for 20 minutes. The first-of-its-kind challenge invites the world\u2019s top universities and institutes to submit solutions.\n\nThe University of Washington team Sounding Board took first place and US$500,000 in prize money at the inaugural Amazon Alexa Challenge with a chatbot that held conversations with an average duration of 10 minutes and 22 seconds, earning a score of 3.17 out of 5 from judges.\n\nAmazon Alexa hopes the challenge can crowdsource human intelligence to solve today\u2019s conversational AI challenges such as conversational Automatic Speech Recognition and NLU, dialog planning and context modeling, ranking and selection response generations, knowledge ingestion and reasoning, etc.\n\nDr. Khatri introduced a couple of techniques adopted by university teams. In dialogue management and context modeling for example, the winning UW team proposed a hybrid dialogue manager wherein a master manages the overall conversation and a collection of miniskills manage different conversation segments. Another top team designed a state graph to track dialog content, conversation state, feedback, user sentiments, and personalization.\n\nOne researcher to benefit from Amazon\u2019s conversational AI development push is Dr. Zhou Yu, an Assistant Professor at the University of California in Davis who created a multi-modal chatbot with strategies designed to keep users engaged in the conversation. The project caught Amazon\u2019s interest and garnered Dr. Yu an annual research sponsorship of US$100,000.\n\nHumans are generally embracing conversational AI \u2014 according to a recent NPR and Edison Research study, 65 percent of American smart speaker users say they could not return to a life without them. But on the flipside, playing music, weather notifications and responding to general questions remain the top three relatively mundane tasks these speakers perform. Expanding capabilities and developing a natural conversational AI is the target that tech giants like Amazon, Apple and Google will aim for over the coming years."
    },
    {
        "url": "https://medium.com/syncedreview/china-aims-to-get-the-jump-on-ai-standardization-f141dcb52de7?source=user_profile---------119----------------",
        "title": "China Aims to Get the Jump on AI Standardization \u2013 SyncedReview \u2013",
        "text": "China has just released its \u201cArtificial Intelligence Standardization White Paper.\u201d The 98-page document was edited by the China Electronics Standardization Institute under the guidance of the National Standardization Management Committee Second Ministry of Industry (\u56fd\u5bb6\u6807\u51c6\u5316\u7ba1\u7406\u59d4\u5458\u4f1a\u5de5\u4e1a\u4e8c\u90e8). Some 30 research institutes, education institutes, and AI companies contributed to the paper, including Tsinghua University, Peking University, Alibaba, Tencent, Baidu, and Toutiao.\n\nConcurrently, China also announced the formation of the National Artificial Intelligence Standardization Group and Expert Advisory Group, which will oversee planning and deployment of the nation\u2019s AI development. Prior to the 19th National Congress of the Party, the Chinese government had laid out its blueprint for further AI integration with the economy, following up with an intensive review on \u201cStandardization of AI Helps Industry Development\u201d and a \u201cThree-Year Action Plan for Promoting the Development of New Generation of AI Industry.\u201d\n\nThe new \u201cAI Standardization White Paper\u201d stresses the importance of establishing standards for the rapid development of AI: \u201cAt present while China\u2019s deployment of AI-related products and services is expanding, there is also the issue of a lack of standards. AI is reaching into many areas, and while some subfields are standardized, their dispersed standards do not form systemic standards.\u201d\n\nThe paper begins: \u201cAI is a prospering new industry. Standardization is at an early stage. China is at the same starting line as all other countries and there is an opportunity now for rapid breakthrough. With fast action plans, China can either seize the commanding heights of innovation standardization, or else miss the opportunity. There is an urgent need to seize opportunities, accelerate research on AI deployment in industry, and systematically review and establish a unified and comprehensive set of standardization.\u201d\n\nThe document comprises six subsections expounding on AI history, key technologies, industry trends, ethical issues; pinpointing international and domestic AI standards, and detailing standard frameworks and their components.\n\nThe paper\u2019s appended Standardization Schedule categorizes 200 technologies pertaining to the field of AI.\n\nInterested readers can download the paper here: http://www.cesi.ac.cn/images/editor/20180118/20180118090346205.pdf"
    },
    {
        "url": "https://medium.com/syncedreview/arm-accelerates-its-ai-efforts-49537e712f69?source=user_profile---------120----------------",
        "title": "Arm Accelerates its AI Efforts \u2013 SyncedReview \u2013",
        "text": "You may not realize it but most of the smart electronics you use every day \u2014 from IoT devices to smartphones to assisted driving systems \u2014 were built on architecture designed by Arm, a leading UK-based intellectual property (IP) provider.\n\nArm\u2019s booth at CES 2018 showcased a wide range of demos and products equipped with Arm processors, including smart speakers Google Home and Amazon Echo smart speakers, smart city solutions in lighting management and smart parking, a smart camera from Hive, and autonomous driving applications Cockpit Controller and Event Data Recorder.\n\nJem Davies, General Manager of the Arm Machine Learning Group, sat down with Synced to outline his company\u2019s ambitious roadmap for machine learning development.\n\nDavies efforts for Arm in machine learning can be traced back to 2013, when Arm tasked him with examining the AI marketplace and making appropriate acquisitions. He soon came to believe there was no market segment that wasn\u2019t already or about to be impacted by the tech. \u201cAI affects everything\u2026mobile phones\u2026cameras\u2026the little smart speaker\u2026 even thermostats. Who thought of a room thermostat as a smart device?\u201d says Davies.\n\nIn 2016 Davies led the acquisition of Apical, a company that provides computer vision and imaging processors for over 1.5 billion devices. The acquisition bootstrapped Arm\u2019s entry into machine learning, enabling the company\u2019s object detection technology on one hand, while developing IP that extends into neural network processing. In March 2017 Arm launched its Mali-C71 image signal processor (ISP), a product series designed specifically for the Advanced Driver Assistance Systems (ADAS) inside vehicles.\n\nLast May the company took a huge step forward in AI and introduced DynamIQ technology. Built on Arm\u2019s big.LITTLE technology which accommodates both powerful and relatively small processors in one chip, DynamIQ improves the flexibility and efficiency of multi-core processing designs, and enables more processors that better perform AI tasks to compute on a single chip.\n\nSays Davies, \u201cBased on DynamIQ, Cortex-A75, Cortex-A55, and Mali-G72 (Arm\u2019s latest iteration of CPU and GPU) are specifically targeting machine learning workloads. So we\u2019ve been analyzing the sort of code that people are writing and working out what best to do to execute those workloads more efficiently.\u201d\n\nWhile most machine learning systems still run on CPUs and GPUs, Davies says Arm is interested in developing special purpose processors for AI acceleration. Based on the company\u2019s history, Arm will likely seek an acquisition as its strategy for entering the AI processor market.\n\nThe company meanwhile is also stepping up its software development and optimization efforts, which is a particular focus of Davies\u2019 new Machine Learning Group. Says Davies, \u201cThe difference between an optimized implementation software and a naive implementation software could effect a 10x improvement.\u201d\n\nArm has also introduced an open-source library to provide optimized routines for accelerating machine learning frameworks such as TensorFlow, MXNet and Caffe. The functions are optimized for Arm Cortex CPU and Mali GPU processors, and target a variety of use cases, including* *image processing, computer vision and machine learning.\n\nFor years Arm has quietly led the way in AI, so what to expect in 2018? Davies did not disclose details, but promised \u201cthere will be more announcements that will be incredibly exciting.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/synced-machine-intelligence-award-2017-d14bb567084a?source=user_profile---------121----------------",
        "title": "Unveiling China\u2019s Mysterious AI Lead: Synced Machine Intelligence Awards 2017",
        "text": "The Synced 2017 Machine Intelligence Awards recognize companies that use AI technologies in smart and commercially viable ways.\n\n \n\nThis year we received more than 500 applications, and with the help of our industry analysts selected the top 10% that really stood out. We are presenting three awards: 30 Most Promising AI Startups, Top 10 Commercial AI Research Labs in China, and 10 Chinese AI Rising Stars to Watch.\n\nThe 30 Most Promising AI Startups category recognizes companies that have demonstrated sound applications of AI technology in various industries. Some are already unicorns, while others are on their way to achieving great things. \n\n \n\nThe latter two awards are exclusive to China. Top 10 Commercial AI Research Labs identifies tech giants that have invested heavily in AI commercial R&D, attracted top research talents, and shown proven results over the past year. 10 Chinese AI Rising Stars to Watch are dark horse newcomers that have shown superior potential. \n\n \n\nCongratulations to these companies and individuals for their outstanding achievements!\n\nVicarious.ai is Teaching Robots to See The World Like Humans"
    },
    {
        "url": "https://medium.com/syncedreview/amazon-go-vs-alibaba-tao-cafe-staffless-shop-showdown-3f3929393d62?source=user_profile---------122----------------",
        "title": "Amazon Go vs Alibaba Tao Cafe: Staffless Shop Showdown",
        "text": "The staffless shop \u2014 where grab-and-go automated payment replaces checkout lines \u2014 is a futuristic retail experience envisioned by e-commerce moguls Amazon and Alibaba. Last July Alibaba showcased its proof-of-concept Tao Cafe in Hangzhou, while in downtown Seattle today Amazon opened its staff-free convenience store Amazon Go.\n\nAmazon and Alibaba\u2019s brick-and-mortar-but-no-staff shops offer very different user experiences. To bring you first-hand feedback, Synced visited them.\n\nBoth shops require customers to use a dedicated mobile app to link to shopping accounts. Customers can then enter the shop by displaying a generated QR code to a scanner. But that\u2019s where the similarities end.\n\nAmazon Go is a convenience store selling snacks, drinks, lunch boxes, salads and sandwiches. Its customers will likely be local office workers (and probably more than a few tech heads).\n\nAmazon Go uses a system called \u201cJust Walk Out,\u201d a first of its kind shopping technology that lets customers select anything from the inventory and leave the store without additional steps. Powered by computer vision, deep learning algorithms and sensor fusion, the system is able to recognize what items which customers have taken in real-time. A backend system then automatically processes the transaction. A Synced reporter tried shutting down their smartphone at the store, but this did not impact the transaction.\n\nOne of the concerns with \u201cJust Walk Out\u201d is the high computational power required for detection and processing. Amazon Go was reportedly incapable of accommodating more than 20 customers at a time during testing. On opening day Synced observed at least 50 in-store customers, although Amazon staff started to limit traffic when the number exceeded 60.\n\nSetting up an unstaffed shop is not cheap \u2014 Amazon Go is equipped with hundreds of cameras and sensor arrays costing millions of dollars. Amazon has however priced the merchandise competitively, a 500ml bottle of water for example costs US$0.49, about the same as in other stores.\n\nTao Caffe meanwhile is a staffless cafeteria and boutique located in Hangzhou, selling drinks and snacks as well as products such as backpacks, notebooks, plush toys, etc. It is only open to users of Alibaba\u2019s e-commerce site Taobao.\n\nTechnically, Tao Cafe is not entirely staffless \u2014 flesh-and-blood waiters take the orders and baristas make the drinks. Like Amazon Go, Tao Cafe enables customers to shop without directly paying, but does this in a very different way.\n\nFor the purchase of drinks, a facial recognition-enabled camera system at the service counter automatically identifies the customer, links to their account, and processes the payment. Customers buying merchandise exit through a processing chamber equipped with multiple sensors, which identifies both the customer and the merchandise they have. The scan works even if the customer has put the goods in pocket or a bag.\n\nSynced observed however that customers required about 10\u201315 seconds to leave Tao Cafe, this occasionally creating a lineup for the exit scanners where only one customer could pass at a time. Amazon Go had no such bottleneck issues.\n\nAmazon also has an advantage over Alibaba at this stage because Amazon Go is open to the public, while Tao Cafe remains something of a randomly recurring pop-up. While it\u2019s still too early to say who will win the battle of the staffless store, it\u2019s clear that neither of these multi-million-dollar experiments will generate any profit in the short term.\n\nBoth companies have invested heavily in brick-and-mortar retail development over the last year: Amazon acquired American supermarket chain Whole Foods for US$13.7 billion, while Alibaba announced an investment of US$2.88 billion for a 36% stake in Gao Xin Retail, China\u2019s largest and fastest-growing hypermarket operator.\n\nWith payroll costs rising and sensor costs falling, more staffless stores are bound to pop up in 2018."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-january-pt-2-1bbea0757?source=user_profile---------123----------------",
        "title": "AI Biweekly: 10 Bits from January (Pt 2) \u2013 SyncedReview \u2013",
        "text": "January 7th \u2014 NVIDIA Announces Delivery Target For Self Driving Processor Xavier\n\nAt CES 2018, NVIDIA CEO Jensen Huang announces the company is planning to deliver the first sample of its Xavier processor during this quarter. NVIDIA DRIVER Xavier contains more than nine billion transistors, making it by far the most complex system on a chip. NVIDIA says that Xavier will power Pegasus, the AI computing platform designed to deliver Level 5 autonomous vehicles.\n\nJanuary 8th \u2014 Facebook Plans to Shut Down Its Personal Assistant \u201cM\u201d on January 19th\n\nFacebook announces it will shut down its virtual chatbot assistant \u201cM.\u201d The AI chatbot in Messenger was only ever available to 2,000 Californians. Facebook says it learned a lot about how users interact with virtual assistants and plans to use that info to further develop its other AI services. Some M users say they never really had a reason to communicate with the bot.\n\n \n\nJanuary 9th \u2014 Amazon Alexa Is Integrating With Windows 10\n\nAmazon Alexa is taking a big step forward by merging itself into Windows PCs. Many OEMs at CES \u2014 such as Acer, ASUS and HP \u2014 announced they would make use of the Alexa App for Windows 10. The app will be limited to specific functions, and cannot call or message. Windows 10 integration does however enable Alexa to jump out of the Amazon ecosystem.\n\n \n\nJanuary 9th \u2014 A US Media Team Plans to Launch Vital Intelligence Data Live\n\nA team of media executives in the US announces VIDL (Vital Intelligence Data Live) the launch this summer. The proprietary technology combines blockchain and AI to deliver accurate news reports from around the world. The platform will be fully automated and promises a foolproof system for delivering news stories that won\u2019t be affected by human bias or warped into fake news.\n\n \n\nJanuary 11th \u2014 Mercedes-Benz Releases In-car Assistant MBUX\n\nMBUX is the new Mercedes-Benz infotainment and multimedia system, a smart platform powered by NVIDIA GPU. It enables users to issue natural language voice commands rather than simple control commands, and can function without an internet connection. MBUX will be ready for all new Mercedes A-Class vehicles this year, and in other Mercedes vehicles in the future.\n\nJanuary 13th \u2014 Google Assistant Gets Much Attention At CES\n\nGoogle did not introduce many new products at CES 2018. Instead, it highlighted a bunch of tech partners \u2014 including Sony, LG, Lenovo, and Huawei \u2014 who promoted Google Assistant in various hardware applications, including TVs, refrigerators and even electric bike wheels. In the virtual assistant race, Google Assistant is closing the gap with Alexa.\n\nJanuary 16th \u2014 GM and Waymo Top New Self-Driving Leaderboard\n\nNavigant Research releases an Autonomous Driving Leaderboard which ranks the industry\u2019s top brands. General Motors and Alphabet\u2019s Waymo are in the top two positions. Meanwhile, former leader Ford has dropped to third, while Tesla sits at the bottom of the ranking with Apple.\n\n \n\nJanuary 16th \u2014 Canada.ai Platform Showcases Canada\u2019s AI Information\n\nNEXT Canada and a group of top Canadian artificial intelligence institutions launch Canada.ai. The web platform showcases Canada\u2019s leadership in the field of artificial intelligence by presenting information such as research results, leading startups and companies in the AI space, and AI-related events happening across the country. Canada is welcoming many international AI researchers, students, institutions and startups.\n\n \n\nJanuary 17th \u2014 AutoML Can Train Machine Learning Models Without Coding\n\nGoogle has announced the alpha launch of AutoML Vision, which helps build custom image recognition models. The new services available under the AutoML brand don\u2019t require users to have machine learning expertise. The user provides images along with their tags, and AutoML Vision automatically creates a custom machine learning model based on them. Google is handling the hard work of training and tuning the model.\n\n \n\nJanuary 19th \u2014 Google Unveils A New Office In Shenzhen, China\n\nRather than Beijing or Shanghai, Google unveils a new office in Shenzhen. According to Google News, the reason for setting up in Shenzhen is to better cooperate with local partners. Google is gradually collaborating more with Chinese companies on products such as Lenovo\u2019s VR set and self-driving cars. While Shenzhen is now in the spotlight, it remains uncertain whether Google can effectively leverage its new Chinese offices to penetrate the country\u2019s AI market."
    },
    {
        "url": "https://medium.com/syncedreview/self-driving-boats-ghosts-on-the-ocean-20dc76546dd4?source=user_profile---------124----------------",
        "title": "Self-Driving Boats: Ghosts on the Ocean \u2013 SyncedReview \u2013",
        "text": "Self-driving boats have existed for a long time: they navigate the oceans, driven by the wind, recording climate data or monitoring water currents. Autonomous underwater vehicles developed by the University of Washington have been used for performing dangerous tasks since 1957. Recent advances in AI may soon broaden the role of these ghost boats.\n\nThe maritime transportation industry has been facing employment shortages. Autonomous vessels may solve this problem and reduce the high crew costs. In comparison to land-based autonomous driving, there are fewer obstacles on the sea where travel routes are more open. Energy consumption and sensor cost are also less restrictive factors.\n\nTo this end, Norwegian chemical company Yara International has announced that it will launch self-driving cargo ships in 2018. The fleet will cost US$25 million, but the company claims it will save up to 90% on crew and energy operating costs.\n\nHuman Labor Cost Reduction: self-driving cargo ships have a significant advantage for long distance travel, especially for saving labor costs. With the help of AI and sensors, a small crew can handle many ships. Sensors also simplify maintenance by locate potential problems and notifying the crew in advance.\n\nEnergy Consumption Reduction: AI can be used to optimized marine routes. Data analytics will find the best travel path taking in consideration current, wind, and sunlight. Travel speed controlled by AI will also save energy during shipping.\n\nSupply Chain Optimization: supply chain optimization will save storage costs and time spent at port. AI in the harbor and management will improve cargo combinations for each shipment, which will shorten travel distances and make full use of cargo space on ships.\n\nCollision Prevention: equipped with an accurate map, ships can avoid reefs and underwater rocks. However, ports with heavy traffic are also dangerous for ships. Improved fleet communication and AI-powered collision detection could provide more safety for passengers and cargo.\n\nEfficient Ship Design: unmanned cargo vessels will change in design, as ships will no longer need to provide space for crews. Moreover, advanced weight control will ensure a more stable and smooth ship. For example, instead of adding water as ballast, the cargo could be re-positioned to achieve the same result.\n\nEco-Friendly Shipping: pollution from the world\u2019s top 16 ships is almost equal to the total pollution from all land vehicles. AI can reduce pollution by reducing fuel consumption. AI will also help boats make better use of currents, wind, and solar energy.\n\nInterference from Interest Groups: like the professional drivers\u2019 backlash against land-based autonomous vehicles, there will likely be resistance from ship crew unions.\n\nBackup Plan to Increase the Robustness: ships must guarantee that at critical moments there will be an alternative plan. All of the components in the vessel should function well during long-distance shipping, but this also means increased operation costs."
    },
    {
        "url": "https://medium.com/syncedreview/life-after-baidu-yuanqing-lins-ai-startup-moves-fast-on-funding-and-b2b-opportunities-c5b183bf588b?source=user_profile---------125----------------",
        "title": "Life After Baidu: Yuanqing Lin\u2019s AI Startup Moves Fast on Funding and B2B Opportunities",
        "text": "Yuanqing Lin\u2019s AI startup Aibee.ai \u2014 or AI2B, AI to business \u2014 has landed US $25 million in angel round funding. The company was founded by the former Director of Baidu\u2019s Institute of Deep Learning (IDL), and provides general AI solutions such as computer vision and image and speech recognition for companies in education, finance, retail, real estate and other sectors.\n\nIt was also announced that Dr. Silvio Savarese, assistant professor of computer science at Stanford University, will join Aibee.ai to lead research and product development.\n\nWhen Lin established Aibee\u2019s new office on the same street as Baidu\u2019s headquarters last November, VCs swarmed to his door, filling the company coffers with US$25 million within a month and quickly solving the money issues that plague most startup CEOs. Junzhang Liang from Kinzon Capital tells Synced that investors feel \u201cobliged\u201d to meet with entrepreneurs who split from BAT (Baidu, Alibaba, Tencent) companies. \u201cA lot of investors tried to reach Yuanqing on the first day, bombarding him with meeting requests. In his case, investing is a very selective process for both sides.\u201d\n\nOver the last three months Lin has consulted with some 40 companies in traditional industries, in order to \u201cland\u201d his technology. He says the main feedback he got was that management feels \u201cpained\u201d when tech contractors disrupt business operations with frequent technical upgrades and patches fixing irrelevant problems.\n\nLin arrived at three conclusions: 1. Know AI has limits. 2. Customers couldn\u2019t care less about what technologies you use, they want to see cost-reduction or measurably improved performance. 3. AI solutions are delivered in iterations and upgrades, there are no instant results.\n\nThe Aibee team of 20 in Beijing and Silicon Valley works with very short iterations, starting each case by first doing a client consultation, and following up with a specialist review by a team of AI engineers before proposing a well-rounded solution. They then collaborate with client-side engineers to deliver the final product or service.\n\nLin knows that multi-billion-dollar companies are reluctant to hand over their system revamp to startups, let alone share their precious data. In other words, Aibee has to work fast and establish feasible industry benchmarks, otherwise, face a survival challenge.\n\n\u201cWe have a portal of opportunity for three years,\u201d says Lin. \u201cThe AI hype will settle by then, and so 2018 is an important watershed. There\u2019s a sense of urgency, we have to move fast but in a down-to-earth manner. You will see our first solution proposal in two months.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/baidu-research-adds-elite-ai-talents-new-labs-a447f0c3492?source=user_profile---------126----------------",
        "title": "Baidu Research Adds Elite AI Talents, New Labs \u2013 SyncedReview \u2013",
        "text": "Baidu announced today that respected Natural Language Processing scientist Dr. Kenneth W. Church will join its research arm as a Distinguished Scientist. This is the latest in a series of eye-catching talent acquisitions for the Chinese \u201cAll in on AI\u201d tech giant.\n\nDr. Church comes to Baidu Research from the IBM Watson Research Center, where he worked on computational linguistics. He was also Chief Scientist of the Human Language Technology Center of Excellence at Johns Hopkins University; and serves as president of Empirical Methods on Natural Language Processing (EMNLP), one of the world\u2019s most influential NLP conferences.\n\nSays Dr. Church, \u201cIn addition to its commitment to fundamental research, Baidu is in a unique position to transfer AI technology from the laboratory into reality and make the world a better place for hundreds of millions of people.\u201d\n\nBaidu Research also announced the acquisition of big data and data mining expert Dr. Jun Huan, a former Professor of Computer Science at the University of Kansas; and Dr. Hui Xiong, a Professor at Rutgers University focused on data and knowledge engineering research.\n\nBaidu Research revealed that it will open two new labs: the Business Intelligence Lab (BIL) will focus on effective and efficient data analysis technology for emerging data intensive applications; while the Robotics and Autonomous Driving Lab (RAL) will conduct research on computer vision. The RAL is expected to advance obstacle perception capabilities at Baidu\u2019s star project Apollo \u2014 an open-source autonomous driving technology platform.\n\nThe new labs complement Baidu Research\u2019s existing Institute of Deep Learning (IDL), Big Data Lab (BDL), and Silicon Valley Artificial Intelligence Lab (SVAIL).\n\nSays Baidu Vice President Haifeng Wang: \u201cThis is the beginning of a new journey for Baidu Research. Our mission will be centered on conducting future-oriented fundamental research, setting the direction for Baidu\u2019s AI development and empowering the company\u2019s long-term strategy.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/barack-obama-is-the-benchmark-for-fake-lip-sync-videos-d85057cb90ac?source=user_profile---------127----------------",
        "title": "Barack Obama is the Benchmark for Fake Lip-Sync Videos",
        "text": "The Collins Dictionary named \u201cfake news\u201d as its Word of the Year for 2017, and AI has just made it a lot more believable. The Montreal Institute for Learning Algorithms (MILA) recently launched ObamaNet \u2014 a photo-realistic lip-sync neural network that can make anyone appear to be saying anything.\n\n \n\nObamaNet is composed of three trainable neural modules: a text-to-speech network based on Char2Wav; a time-delayed LSTM to generate mouth-keypoints synced to the audio; and to generate the video frames conditioned on the key points, a network based on Pix2Pix \u2014 which is emerging as a good general-purpose solution for image-to-image translation problems.\n\nUsing generative networks for images and videos is nothing new, while decades of work have been put into speech synthesis. ObamaNet combines the attributes of both. According to MILA researchers, their system \u201ccan be trained on any set of close shot videos of a person speaking, along with the corresponding transcript. The result is a system that generates speech from an arbitrary text and modifies according to the mouth area of one existing video so that it looks natural and realistic.\u201d \n\n \n\nMILA researchers explain that they chose the former US President because \u201chis videos are commonly used to benchmark lip-sync methods.\u201d There is also more online video data available for public figures such as Obama. For the project, MILA extracted 17 hours of footage from Obama\u2019s 300 weekly presidential addresses. \n\n \n\nThe concept of Obama as a benchmark is rooted in the paper Synthesizing Obama: Learning Lip Sync from Audio, published in July 2017 by Supasorn Suwajanakorn from the University of Washington\u2019s Graphics and Imaging Laboratory (GRAIL). When Suwajanakorn uploaded his results to YouTube, the videos quickly attracted over 750,000 views.\n\nGRAIL\u2019s synthesized Obama video stirred up quite a discussion online. The model worked so well that many YouTube viewers were frightened, one warning \u201cI see the first episode of Black Mirror has begun.\u201d Others welcomed the videos: \u201cIt\u2019s better that the public is aware of such technology than being oblivious. If we were ignorant we would believe things without thinking it was tampered.\u201d\n\nThe Guardian described the project as \u201cthe future of fake news. We\u2019ve long been told not to believe everything we read, but soon we\u2019ll have to question everything we see and hear as well.\u201d\n\n \n\nMILA says their model differs from the GRAIL project because instead of a traditional computer vision model, it uses a neural network topped with a text-to-speech synthesizer. The lab created ObamaNet in conjunction with Lyrebird.ai, whose beta voice synthesis product allows users to generate text-to-speech files in their own \u201cdigital voice\u201d after providing just one minute of sample speech.\n\n \n\nWhile applications are still limited, projects like Synthesizing Obama and ObamaNet provide a glimpse of future possibilities for the tech, including the chilling prospect of just how just realistic fake news may become."
    },
    {
        "url": "https://medium.com/syncedreview/ubiquitous-virtual-assistants-and-ai-homogenization-at-ces-2018-5c7fc67a6e36?source=user_profile---------128----------------",
        "title": "Ubiquitous Virtual Assistants and AI Homogenization at CES 2018",
        "text": "Although AI is opening up a world of possibilities for humanity, the technology\u2019s current application in consumer electronics remains surprisingly limited.\n\n \n\nAt last week\u2019s Consumer Electronics Show in Las Vegas (CES 2018), exhibitors promoted everything from AI-powered vehicles to smart refrigerators and robot puppies. But the tech driving these diverse products was the same: the virtual assistant, a conversational AI that can understand human speech and generate humanlike responses.\n\n \n\nSince Amazon introduced its virtual assistant Alexa and attendant smart speaker Echo three years ago, virtual assistants have been widely integrated into home appliances, smart devices and cars. At CES 2018, South Korean electronics mogul LG announced a partnership with Google to enable customers to communicate with its televisions via Google Assistant, the Alexa rival released in 2016. At the LG booth, a staff member proudly prompted a television to say \u201chola\u201d (\u201chello\u201d in Spanish) via Google Assistant.\n\nOther electronics makers such as TCL, Hisense, Haier and Sony also support Google Assistant in their smart home product lines. When asked about the biggest advantage of AI for home appliances, a CES Hisense representative told Synced \u201cwith virtual assistants, AI brings a better connectivity between devices and electronics.\u201d\n\nVirtual assistants are also bringing their convenient voice interface to cars. Millions of vehicles from top manufacturers like Ford, GM, Volkswagen and Volvo now have Google Assistant built in. \n\n \n\nCES 2018 visitors were greeted by a huge \u201cHey Google\u201d installation at the show\u2019s main entrance, and a city-wide advertising campaign covered Las Vegas with the wake word slogan. The virtual assistant has become the focus of so much contemporary applied AI in part because it is one of the few AI technologies that has proven both successful and marketable over the last few years. Amazon\u2019s Echo sales more than quintupled from 2016 to 2017, while Google Assistant now supports 1,500 devices from 200 brands.\n\nWhile virtual assistants are being deployed across such a wide range of products, most companies are using the same third-party cloud service to power their voice interfaces, so users will converse with the same bot whether they are switching TV channels, navigating a route in their car, or ordering a pizza.\n\n \n\nMany Chinese tech companies \u2014 including Alibaba and Baidu Online \u2014 brought their smart speakers to CES 2018 for the first time. However, Microsoft Product Manager Tucker Kelly told Synced that he could not discern any significant differences between Alibaba\u2019s smart speaker Ali Genie X1 and other Chinese smart speakers. \u201cThey all look the same and have similar features.\u201d\n\n \n\nKun Jing, General Manager of Baidu\u2019s DuerOS platform, told Synced that the virtual assistant marketplace is still new, and he believes more diversified and customised AI products will emerge in 2018. \u201cAndroid as a mobile operating system has pushed innovations to the boundaries, but we don\u2019t have any mainstream operating systems for the era of AI yet,\u201d says Jing. This is an area where Baidu is currently applying its development resources.\n\n \n\nMeanwhile, only a few companies have taken the step of customising their virtual assistants with unique appearances or interlocution styles. Chinese electronics maker Xiaomi, for example, designed an anime avatar Xiao Ai, an adorable redheaded girl with a sweet, natural voice, for its Mi AI Speaker. Considering anime culture is widely popular with Chinese consumers, the character is expected to further drive domestic sales.\n\n \n\niFlytek has also put effort into innovation, showcasing a range of AI-powered voice translators, portable intelligence translators, smart microphones, smart speakers, and AI headphones. An iFlytek representative told Synced its English-Mandarin Voice Translator EASYTRANS 600 was very well-received by CES attendees.\n\nWith virtual assistants getting all the attention these days, other AI technologies are lagging. Facial recognition for example has not been as widely adopted because it remains vulnerable to attacks. Last year, when Apple boldly switched its iPhone X unlocking system from fingerprint-based TouchID to facial recognition-based Face ID, the tech was quickly tricked by siblings and even a cheap mask. Many other promising AI technologies remain relatively underdeveloped because they have limited applications in consumer products.\n\n \n\nAI has wowed humans over the last two years with its capacity for detecting images, translating languages, recognizing voices, and mastering board games such as Go. Going forward, humans will be expecting more from their virtual assistants, and expecting more than virtual assistants from AI."
    },
    {
        "url": "https://medium.com/syncedreview/uc-berkeley-facebook-ai-researchers-introduce-house3d-for-reinforcement-learning-9667efdfb86e?source=user_profile---------129----------------",
        "title": "UC Berkeley & Facebook AI Researchers Introduce House3D for Reinforcement Learning",
        "text": "Reinforcement learning\u2019s prowess in 3D understanding, real-time strategy decision, fast reaction, long-term planning, language and communication have enabled machines to top humans in contests ranging from Atari\u2019s Breakout to the ancient game of Go.\n\n \n\nHowever, current reinforcement learning research is largely focused on over-simplified tasks that are conducted in monotonous virtual environments and are not transferable to the real world. Today\u2019s smart robots are far from functional when it comes to generalized tasks and fall short when dealing with our semantically-rich world.\n\n \n\nTo boost learning research aimed at endowing robots with better generalization capabilities, Yi Wu from UC Berkeley and Yuxin Wu, Georgia Gkioxari, and Yuandong Tian from Facebook AI research recently published the paper Building Generalizable Agents with a Realistic and Rich 3D Environment, which introduces a diverse set of training environments in a virtual property called House3D. \n\n \n\nHouse3D comprises 45,622 human-designed 3D scenes extracted from the SUNCG dataset, which includes housing models ranging from single-room studios to multi-story houses, subdivided into 20 room types such as bedroom, living room, kitchen, bathroom, etc. All scenes are semantically annotated to the level of each object. Agents can make observations of multiple modalities such as RGB images, depth, segmentation masks, top-down 2D view, etc.\n\n \n\nThe benchmark task in this paper is \u201cconcept-driven navigation\u201d known as \u201cRoomNav.\u201d The agent is given a high-level task description such as \u201cGo to the kitchen,\u201d then prompted to explore the House3D environment to reach the target room.\n\nRoomNav is considered a multi-target learning problem to which the paper\u2019s authors propose two baseline models with gated-attention architecture: \u201cA gated-CNN network for continuous actions and a gated-LSTM network form for discrete actions.\u201d The gated-CNN policy network was trained using deep deterministic policy gradient (DDPG), while the gated-LSTM policy was trained using the asynchronous advantage actor-critic algorithm.\n\nThe research results indicate improved generalization capabilities, with the team making the observations that \u201cusing the semantic signal as the input considerably enhances the agent\u2019s generalization ability. Increasing the size of the training environments is important but at the same time introduces fundamental bottlenecks when training agents to accomplish the RoomNav task due to the higher complexity of the underlying task.\u201d\n\n \n\n\u201cWe believe our [House3D] environment will benefit the community and facilitate the efforts towards building better AI agents. We also hope that our initial attempts towards addressing semantic generalization ability in reinforcement learning will serve as an important step towards building real-world robotic systems.\u201d\n\n \n\nYi Wu, who received the NIPS Best Paper Award in 2016 for his paper Value Iteration Network, is advised by esteemed UC Berkeley Professor Stuart Russell. Facebook AI Research team meanwhile have conducted explorational research on the application of reinforcement learning to real-time strategy games like Starcraft. Yuandong Tian has proposed ELF, a platform for reinforcement learning research in gaming. According to his Facebook research blog, \u201cELF allows researchers to test their algorithms in various game environments, including board games, Atari games, and custom-made, real-time strategy games.\u201d\n\nThe paper Building Generalizable Agents with a Realistic and Rich 3D Environment has been submitted to the International Conference on Learning Representations (ICLR) 2018. You can read it here: https://arxiv.org/abs/1801.02209\n\n \n\nThe House3D project has been open-sourced on Github: https://github.com/facebookresearch/House3D"
    },
    {
        "url": "https://medium.com/syncedreview/intro-a-review-of-ai-history-beyond-deep-learning-f166ff8b4b2f?source=user_profile---------130----------------",
        "title": "A Review of AI History Beyond Deep Learning \u2013 SyncedReview \u2013",
        "text": "Related work on Artificial Intelligence can be traced back to the 1940s, when Warren McCulloch and Walter Pitts showed that computing could be done by a network of connected neurons, and Donald Hebb demonstrated Hebbian learning.\n\n \n\nIn 1956, when the term \u201cArtificial Intelligence\u201d (AI) was formally coined, corresponding research started to develop quickly. AI was mostly used for problem-solving at that time. Although there were criticisms during the 1960s and 1970s for the slow development speed and limited progress of the neural network, the emergence of expert systems still attracted the interest and growth of AI studies.\n\n \n\nDuring the 1980s, AI academic studies stepped into a period called \u201cAI Winter\u201d, as most studies of AI failed to deliver their original extravagant promises, resulting in the funding for AI research being moved to other areas. Luckily, connectionists reinvented back-propagation and brought neural networks back onto the stage.\n\nIn the 1990s, more scientific methods such as probabilistic models started to be applied; at the same time, Support Vector Machines (SVM) outperformed and replaced neural networks in many areas. The era of big data came soon after in the 2000s, which helped the development of various learning algorithms and enabled deep learning to flourish in recent years.\n\n \n\nNow, more and more groups are starting to pay attention to AI research, and the number of papers published in elite AI conferences such as AAAI, IJCAI, AUAI, and ECAI has increased dramatically.\n\nThe purpose of this report is to detail the major technological branches of Artificial Intelligence (AI). By identifying these technologies\u2019 paths of development, readers will be able to accurately and comprehensively learn about the past, present, and future of all modern AI research fields. The report will help you sort out the basics and provide you with the necessary background to move forward.\n\nWe\u2019ll review all the mainstream AI research fields, with a focus on:\n\nThe report is organized based on:\n\nWe hope you find the report useful, also, let us know your thoughts in the comment section!"
    }
]