[
    {
        "url": "https://medium.com/@godricglow/a-deeper-understanding-of-nnets-part-3-lstm-and-gru-e557468acb04?source=user_profile---------1----------------",
        "title": "A deeper understanding of NNets (Part 3) \u2014 LSTM and GRU",
        "text": "Vanishing Sensitivity of vanilla RNNS is proven mathematically and comprises two major factors 1. Weight Initialization 2. Back-propagation\n\nWeight Initialization is not a direct solution to avoid vanishing gradients but it helps avoiding any immediate problems. Back-propagation on the other hand is the primary cause of vanishing gradients, this problem becomes more escalated when back propagation and simultaneous forward passes are done to compute error gradients with respects to weights at each time step, read real-time recurrent learning (RTRL) for more info. So it seems a good idea to truncate the back propagation, but knowing when to truncate the back propagation is important because we need to update the weights accordingly allowing the model to progress. Therefore, the solution to vanishing gradients is two parts, knowing how often to truncate the back propagation and how often to update the model.\n\nAfter having solved for vanishing gradients, researchers also wanted to solve for the information morphology problem posed by the vanilla RNNs. In simple words, the information contained in a prior state gets embedded over and over due to non-linearities and is not the best usable state of information in its current state. In essence, the original usable information is lost in the morphed information.\n\nThe originality of information can be preserved and this was proposed by the landmark paper of Hocreiter and Schmidhuber (1997). They asked: \u201chow can we achieve constant error flow through a single unit with a single connection to itself [i.e., a single piece of isolated information]?\u201d\n\nThe answer, quite simply, is to avoid information morphing: changes to the state of an LSTM are explicitly written in, by an explicit addition or subtraction, so that each element of the state stays constant without outside interference: \u201cthe unit\u2019s activation has to remain constant which is ensured by using the identity function\u201d. Hocreiter and Schmidhuber observed that simple addition or subtraction of information at each state may keep the state isolated but at the same time, the addition and subtraction may cancel out or worse, they may complicate the states with only parts of information preserved which gets hard to recover.\n\nHochreiter and Schmidhuber recognized this problem, splitting it into several subproblems, which they termed \u201cinput weight conflict\u201d, \u201coutput weight conflict\u201d, the \u201cabuse problem\u201d, and \u201cinternal state drift\u201d. The LSTM architecture was carefully designed in order to overcome these problems, starting with the idea of selectivity.\n\nAs per the LSTM literature, there are 3 things a LSTM should selectively decide, \u201cwhat to write, what to read and what to forget\u201d. The most fundamental and mathematical way of maintaining selectivity is gates, we call these gates the read, write and forget gates. Our three gates at time step t are denoted , the input gate (for writing), , the output gate (for reading) and , the forget gate (for remembering!).\n\nHere are the mathematical definitions of the gates (notice the similarities):\n\nWith all the gates defined, we now develop a LSTM prototype by defining the required behavior. To write a candidate state s(t), we follow a simple rule of thumb.\n\nBelow is a pictorial view of above equations with arrows pointing to flow of data within the LSTM cell.\n\nIn theory, this prototype should work but turns out it doesn\u2019t. It happens because, even after well thought initializations and write and forget gates, the coordination between these gates in early stage of training gets tricky and very often it becomes large and chaotic at write step. For more details, refer to \u201cinternal state drift\u201d problem, further, an empirical demonstration of this can be found in Greff et al. (2015), which covers 8 variants of LSTMs.\n\nThe solution to above problem is bounding the state to prevent it from becoming chaotic or blowing up. There are 3 variants of LSTM which uses this solution 1. Normalized LSTM, GRU and Pseudo LSTM. We will focus mainly on the GRU for this post but feel free to dive deeper into the other variants."
    },
    {
        "url": "https://becominghuman.ai/a-deeper-understanding-of-nnets-part-2-rnns-b32240998fa9?source=user_profile---------2----------------",
        "title": "A deeper understanding of NNets (Part 2) \u2014 RNNs \u2013",
        "text": "Last week we talked about a very particular type of NNet called Convolutional Neural Network. We can definitely dive deeper into Conv Nets but the essence of the topology was broadly covered in the previous post. We will revisit the Conv Nets after we have covered all the topologies, as discussed in the previous post.\n\nThe architecture for this week is Recurrent Neural Network or RNN. The key difference between a RNN and any Feed Forward Normal/Deep Network is the recurrence or cyclic nature of this architecture. It sounds vague in the first go but lets unroll this architecture to understand it better. We will also be discussing two special cases of RNN namely LSTM and GRU in the next post.\n\nLets take a use-case of RNN, Natural Language Processing (NLP), traditional NLP techniques used statistical methods and rule-based approach to define a language model. Language models computes a probability for a sequence of words: P(w1, w2, \u2026.. wn) which is useful in machine translation and word prediction.\n\n3. To estimate any probability they had to compute n-grams.\n\nComputation of so many n-grams has HUGE RAM requirements, which gets practically impossible after a point. Also, above models relied on hand engineered linguistic features to deliver state-of-the-art performance.\n\nRNNs solve the above problems by using a simple solution called \u201cstatefulness\u201d and \u201crecurrence\u201d. Deep Learning allows RNN to remember or forget things based on few logical values, as we will see later, and perform cyclic operations within the network to achieve better results. Before we start exploring how all this happens, lets first understand a crucial input that goes into RNN, Word Embeddings."
    },
    {
        "url": "https://towardsdatascience.com/a-deeper-understanding-of-nnets-part-1-cnns-263a6e3ac61?source=user_profile---------3----------------",
        "title": "A deeper understanding of NNets (Part 1) \u2014 CNNs \u2013",
        "text": "Deep Learning and AI were the buzz words for 2016; by the end of 2017, they have become more frequent and more confusing. So lets try and understand everything one at a time. We will look into the heart of Deep Learning i.e. Neural Networks (NNets). Most variants of NNets are hard to understand and the underlying architectural components make them all sound (theoretically) and look (graphically) the same.\n\nThanks to Fjodor van Veen from The Asimov Institute, we have a fair representation of the most popular variants of NNet architectures. Please refer to his blog. To improve our understanding of NNets, we will study and implement one architecture every week. Below are the architectures we will be discussing over the next few weeks.\n\nThe architecture for this week is Convolutional Neural Network or CNN. But before starting CNN, we will first have a small deep dive into Perceptrons. NNet is a collection of several units/cells called perceptrons which are binary linear classifiers. Lets take a quick look to understand the same.\n\nInputs and are multiplied with their respective weights w1 and w2 and summed together using function , therefore (bias term, optionally added). Now this function can be any other operation but for perceptrons its generally a summation. This function is then evaluated through an activation which allows the desired classification. Sigmoid function is the most common activation function used for binary classification. For further details on perceptrons, I recommend this article.\n\nNow if we stack multiple inputs and connect them using function with multiple cells stacked in another layer, this forms multiple fully connected perceptrons, the output from these cells(Hidden layer) becomes input to the final cell which again uses function and activation to derive final classification. This, as show below, is the simplest Neural Network.\n\nThe topologies or architectural variants of NNets are diverse because of a unique capability of NNets called \u201cUniversal Approximation function\u201d. This in itself is a huge topic and is best covered by Michael Nielsen here. After reading this we can rely on the fact that NNet can behave as any function no matter how complex. Above mentioned NNets is also referred to as Feed Forward Neural Network or FFNN, since the flow of information is uni-directional and not cyclic. Now that we know the basics of perceptron and FFNN, we can imagine hundreds of inputs connected to several such hidden layer, would form a complex network popular called Deep Neural Network or Deep Feed Forward Network.\n\nCNNs gained their popularity through competitions like ImageNet and more recently they are used for NLP and speech recognition as well. A critical point to remember is that many other variants like RNN, LSTM, GRU etc are based on a similar skeleton as that of CNNs but with some difference in architecture that makes them different. We will later discuss the differences in detail.\n\nCNNs are formed using 3 types of layers namely \u201cConvolution\u201d, \u201cPooling\u201d and \u201cDense or Fully connected\u201d. Our previous NNets were a typical example of \u201cDense\u201d layer NNets as all layers were fully connected. To know more about the need to switch to convolution and pooling layers, please read Andrej Karpathy\u2019s excellent explanation here. Continuing our discussion of layers, lets look at convolution layer.\n\nConvolution Layer: Consider an image of 5X5 pixels with and , this image is recognized as a monochrome image of dimension 5X5. Now imagine a 3X3 matrix with random and this matrix is allowed to do a matrix multiplication with a sub-set of image, this multiplication is recorded in a new matrix as our 3X3 matrix moves a pixel in every iteration. Below, is a visual for this process.\n\nThe 3X3 matrix considered above is called a \u201cfilter\u201d, which has a task to extract features from the image, it does that by using \u201cOptimization Algorithms\u201d to decide specific in a 3X3 matrix. We allow several such filters to extract several features in a convolution layer of a NNet. A single step for the 3X3 matrix is called a \u201cstride\u201d\n\nA detailed view of a 3-channel(RGB) image producing two convolution outputs using two 3-channel filters is provided below. Thanks to Andrej Karpathy!\n\nThese filters are the \u201cconvolutions\u201d and is the extracted feature, a layer consisting all these filters is a Convolutional layer.\n\nPooling Layer: This layer is used to reduce the dimension of input using different functions. In general a \u201cMAX Pooling\u201d layer is frequently used after a convolutional layer. Pooling uses a 2X2 matrix and operates over the image in the same manner as a convolution layer but this time its reducing the image itself. Below are 2 ways to pool an image using a \u201cMax Pooling\u201d or \u201cAvg Pooling\u201d\n\nDense Layer: This layer is a fully connected layer between the activations and the previous layer. This is similar to the simple \u201cNeural Network\u201d we discussed earlier.\n\nNote: Normalization layers are also used in CNN architectures but they will be discussed separately. Also, pooling layers are not preferred since it leads to loss of information. A common practice is to use a larger stride in convolutional layer.\n\nVGGNet, the runner-up in ILSVRC 2014, is a popular CNN and it helped the world to understand the importance of depth in network by using 16 layered network as opposed to 8 layers in AlexNet, ILSVRC 2012 winner. A plug and play model \u201cVGG-16\u201d is available to use in keras, we will be using the same to view a winning CNN architecture.\n\nAfter loading the model in Keras, we can see the \u201cOutput Shape\u201d for each layer to understand the tensor dimensions and \u201cParam #\u201d to see how parameters are calculated to obtain the convoluted features. \u201cParam #\u201d is the total weights updates per convoluted feature for all features.\n\nNow that we are familiar with the CNN architectures and understand it\u2019s layers and how it functions, we can move towards understanding how its used in NLP and video processing. This will be covered in the next week\u2019s post along with an introduction to RNNs and the key differences between CNNs and RNNs. Meanwhile, fee free to read about all the CNN models that won ImageNet competitions since 2012, here, Thansk to Adit Deshpande!\n\nThank you for reading, I hope it helped"
    }
]