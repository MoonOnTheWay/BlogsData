[
    {
        "url": "https://medium.com/@k_dasaprakash/artificial-neural-networks-c518c49a603a?source=user_profile---------1----------------",
        "title": "Artificial Neural Networks \u2013 Dasaprakash K \u2013",
        "text": "Some of the digits are provided below:\n\nThis is a \u201chello world\u201d dataset for computer vision. In this dataset, we have training set of thousands of handwritten images from 0 to 9. Our predictive model should identify the correct digits from the test dataset.\n\nThe dataset for MNIST is available in Kaggle.\n\nThe implementation of the code provided in this blog is under:\n\nIn this blog I will cover the following:\n\nWith Neural Networks, like other Machine Learning algorithms we predict. Deep Neural Networks much predict better. It is improving by the day and in active development. Building Neural Networks architecture, tuning hyperparameters of neural networks and choosing activation functions, etc., are ongoing areas of research.\n\nAny machine learning model have the functions to learn the model parameters from the data and to accurately predict based on the learned parameters.\n\nBased on the architecture of Neural networks we can infer:\n\nThe link to neural networks and logistic regression is each neuron is a logistic unit. If the hidden unit has only one neuron, we are talking about logistic regression scenario. Neural networks are multiple layers of logistic units.\n\nFeed Forward: Input layer sends signals to the hidden layer and activates to output\n\nBack Propagation: Weights between hidden layer and output gets updated based on the propagated error in output layer. Weights between input layer and hidden layer gets updated based on propagated error in hidden layer.\n\nActivation functions are useful in identifying complex arbitrary non-linear mappings between the input and the output variables. Since neural networks identifies these mappings, they are called Universal function approximators which means they can compute any function.\n\nA linear approximation function is not powerful to understand the non-linear mappings of datasets like images, video, audio, etc.,\n\nThe activation functions we will concentrate are:\n\nThe sigmoid activation function is susceptible to vanishing gradient problem and the output is not centered at zero. The derivatives or gradient is too wide which makes optimization difficult and convergence slow.\n\nIn output layer though sigmoid/softmax would be used for classification and linear functions for regression.\n\nThe tanh function on the other hand has a narrow derivative or gradient and optimization becomes easier and convergence faster. In the hidden layers of neural netwroks, it is advised to use tanh than sigmoid function. However, tanh too is susceptible to vanishing gradient problem.\n\nReLU function has a constant gradient and hence no vanishing gradient problem. This is the most simple and efficient form of activation function which works in 95% cases. ReLu is used only in hidden layers.\n\nNeural networks are layers of logistic units. In logistic regression, the linear approximation f=Wx+b is passed to a non-linear function like sigmoid function to identify the probability of y given x.\n\nIn neural networks, we will deal with a hidden layer which is a group of logistic units and the output of the hidden layer would then be passed to another linear or logistic (sigmoid/softmax) function to create the predictions.\n\nWe shall use this diagram to explain back propagation. Let\u2019s say our input is Xk which has 4 features and there are 5 hidden nodes and 3 output classes.\n\nwhere W1 will be a 4 x 5 matrix connecting all the input nodes to hidden layer nodes and b1 will be a vector of size 5.\n\nwhere W2 will be a 5 x 3 matrix connecting all the hidden nodes to output nodes and b2 will be a vector of size 3.\n\nThe weights are initialized randomly and not to zeros to break symmetry avoid all neurons getting same signal.\n\nBack propagation is the process where the weights W1, b1and W2, b2 get updated based on the error. Let\u2019s get into a bit of math and for brevity let\u2019s consider the sigmoid function for output layer.\n\nWe will compute the derivative of J with respect to Wji.\n\nAfter combining all the derivatives above:\n\n\u2202J/\u2202Wji = (yi- ti)hj (Softmax function has the same derivative)\n\nThese are the gradients of error for weights between hidden and output layer. For input and hidden layer, we will do one mode application of this which is back propagation.\n\nAfter combining all the derivatives above:\n\nThe below results are produced after 5000 iterations by implementing the basic neural network without any hyper parameter tuning.\n\nTraining accuracy of ~97% was achieved. I was able to get an accuracy of 95.11% with this implementation without bells and whistles in Kaggle. With hyper parameter tuning we will try and improve this accuracy.\n\nIn my next post, I will cover hyper parameter tuning which involves the following:\n\nWe will understand the mathematics behind this, the implementation and how we can use Deep Learning libraries to achieve these easily."
    },
    {
        "url": "https://medium.com/@k_dasaprakash/logistic-regression-5b371cc0824f?source=user_profile---------2----------------",
        "title": "Logistic Regression \u2013 Dasaprakash K \u2013",
        "text": "While Linear Regression helps us find the correlation of continuous functions, Logistic Regression deals with discrete or classification problems. A discrete variable has a countable set of values.\n\nIn Linear Regression we would predict the $ amount for which the house can sold whereas in Logistic Regression we will predict if the house can be sold or not based on the input variables.\n\nThere are two types of classification:\n\nIn Logistic Regression, we will apply the linear approximation as we did in Linear Regression.\n\nAn then we will apply a Logistic function like sigmoid function to do a binary classification.\n\n\u03c3(x) returns probability of the linear approximation function \u201cf\u201d defined above and the values will be between 0 and 1.\n\nAs we see the graph, we can easily spot the difference between linear approximation function and logistic function. f was linear whereas sigmoid is a non-linear function.\n\nThe sigmoid function returns the probability between 0 and 1.\n\nTo return the probabilities of discrete values, we decide upon the threshold like 0.5 in this case, above which we classify the scores as admitted and below as not admitted.\n\nWe will use a dataset from Coursera ML course by Andrew Ng. The dataset contains the features Exam 1 score and Exam 2 score. Based on the features we have the label of whether the student is admitted to the university or not.\n\nSince we are dealing with sigmoid function, which is non-linear we cannot use the error function which we used for Linear Regression here. We will use another error function called Cross-Entropy or log loss. Squaring the sigmoid predictions will result in a non-convex function with many local minimums. Gradient descent may not find the optimal global minimum in case of many local minimums and the learning will be too slow. To know more about cross entropy, visit the link below:\n\nLet\u2019s analyze the cost function with some inputs:\n\nBased on the above values, we can observe the cross-entropy function\u2019s penalty for making wrong predictions are more than making right predictions. Adding images from Andrew Ng cost function of Logistic regression to graphically represent the above analysis.\n\nWe can represent the above cost function in a single formula as below which will perform both the cases by cancelling out one or another based on the label values 0 or 1.\n\nSo far we have seen the sigmoid function and the cost function for the non-linear sigmoid function and how the cost function improves the learning rate by penalising the wrong predictions.\n\nWe will derive the derivatives of cross-entropy function below and apply it to gradient descent.\n\nThis operation can be vectorized and represented as:\n\nThe derivatives of cost function for W and b can be used for optimizing the cost function J\n\n\u03b7 \u2014 learning rate to update weights. We will see more about choosing learning rates in Neural Networks.\n\nRemember, choosing larger learning rate might overlook the convergence and smaller learning rate will make the model learn slow.\n\nThe training of Logistic Regression is similar to Linear Regression.\n\nThe model evaluation can be done with following steps:\n\nLet\u2019s use another dataset from Coursera ML course by Andrew Ng. The dataset contains test results for some microchips on two different tests. The model should help us determine whether the microchips should be accepted or rejected.\n\nA linear fit for this data set will result in high bias or underfitting.\n\nBy adding extra features of higher order polynomial a better fit can be realised. But we may end up getting into high variance or overfitting. To avoid this, let\u2019s review a concept called regularization which lets us keep all the features, but reduce the magnitude of parameters Wj. Regularization prevents the learning algorithm to overfit the training data or from picking arbitrarily large parameter values.\n\nThe two types of regularizations are l2 (Ridge) and l1(Lasso). The main difference between l2 and l1 is l2 minimizes the impact of irrelevant features on the trained model while l1 regularization removes the irrelevant features and thereby dealing with few features.\n\nWe will improve the model by adding new polynomials of the inputs upto order 6 and applying l2 regularization.\n\nCost function with l2 regularization has an additional product of sum of squares of weights and lambda parameter or regularization parameter. Be sure to use the derivative of the same in Gradient descent. With l1 instead of using sum of squares of weights, absolute value of weights will be used.\n\nNote: Bias term should be excluded from regularization.\n\nBy adding features of higher polynomial and proper regularization rate the classification model with performing with an accuracy of 83%.\n\nI will publish my next blog on Artificial Neural Networks where multi-class classification of MNIST dataset using Logistic Regression and ANN will be compared."
    },
    {
        "url": "https://medium.com/@k_dasaprakash/linear-regression-af99714eea8b?source=user_profile---------3----------------",
        "title": "Linear Regression \u2013 Dasaprakash K \u2013",
        "text": "Simple Linear Regression or line of least squares or line of best fit is a favourite algorithm when I started. It is a statistical method to find relationship between \u201cx\u201d the dependent variable and \u201cy\u201d the independent variable. It takes the form y=Wx+b also given as y = mx+b where \u201cW\u201d or \u201cm\u201d is the weight/slope and \u201cb\u201d is the bias/intercept.\n\nLinear Regression is the method to fit the blue line, where it starts(b) and what is the slope(m or W).\n\nToday, we are going to find how we update weights and find the line of best fit from green line which is y = 0.5x-2 to the blue line y = 1.166x-3.63.\n\nLet\u2019s consider T as the target output and P as predicted output. We need the line of best fit (P) to be close to the points of T. The error/cost function will be average of sum of squares of difference between all P and T which is represented below:\n\nThe partial derivatives or gradients \u2202J/\u2202W and \u2202J/\u2202b are provided below. These are derived by applying power rule and chain rule of differential calculus.\n\nTo update the weights and bias in an iterative manner, we will use Gradient Descent algorithm. We take steps proportional to the gradient and hence the name gradience descent. By simultaneously updating the weights and bias with these derivatives till the convergence is reached we can plot the line of best fit.\n\n\u03b7 \u2014 learning rate to update weights. We will see more about choosing learning rates in Logistic Regression and Neural Networks.\n\nRemember, choosing larger learning rate might overlook the convergence and smaller learning rate will make the model learn slow.\n\nThe implementation is done in a scalar manner for X. We will see how vectorized implementation can be done for Gradient Descent in Multiple Linear Regression.\n\nFor training the model, the following initializations should be done.\n\nTraining is complete when we reach a error rate within acceptable limit or when the cost does not change.\n\nTo evaluate the overall fit of the model R-squares value is used.\n\nMultiple linear regression involves multiple dependent variables unlike Simple Linear Regression and the model, visualization, model validation, correlation of dependent variables to independent variables becomes complex.\n\nStandardizing the inputs can make training faster. We normalize the features by subtracting the mean and dividing by its standard deviation. So the step size is equivalent for all features and the gradient descent reaches the optimum solution faster.\n\nThe cost function for multiple linear regression remains the same as that of SLR.\n\nHence, we will have to derive partial derivatives for W1, W2, \u2026 Wn.\n\nIn order to avoid the above duplications, vector operations can be used in Gradient Descent. Instead of finding P-Y and multiplying by the column we can vectorize the whole operation.\n\nLet\u2019s say X has 5 features and 100 items(100x5 matrix)then W will be of size 5x1. Y will be 100x1. Instead of doing the previous iterative process as we did in Simple Linear Regression, we can do a matrix multiplication as follows:\n\nwhich translates to matrix 5x100 multiplied by 100x1 matrix. The result would be 5x1 matrix which is equal to the size of weights matrix.\n\nTo evaluate the overall fit of the model R-squares value is used. Adjusted R- squared is another evaluation method for Multiple Linear Regression.\n\nNormal equation is the closed form solution for Linear Regression and is represented as:\n\nThe implementation compares the results of linear regression using gradient descent and normal equation. Remember for using normal equation the matrix should be invertible and as the size of the matrix grows bigger normal equation will become less efficient and take lot of computing resources."
    }
]