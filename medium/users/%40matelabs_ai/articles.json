[
    {
        "url": "https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491?source=user_profile---------1----------------",
        "title": "Everything you need to know about Neural Networks \u2013",
        "text": "Forward Propagation \u2014 Forward propagation is a process of feeding input values to the neural network and getting an output which we call predicted value. Sometimes we refer forward propagation as inference. When we feed the input values to the neural network\u2019s first layer, it goes without any operations. Second layer takes values from first layer and applies multiplication, addition and activation operations and passes this value to the next layer. Same process repeats for subsequent layers and finally we get an output value from the last layer.\n\nBack-Propagation \u2014 After forward propagation we get an output value which is the predicted value. To calculate error we compare the predicted value with the actual output value. We use a loss function (mentioned below) to calculate the error value. Then we calculate the derivative of the error value with respect to each and every weight in the neural network. Back-Propagation uses chain rule of Differential Calculus. In chain rule first we calculate the derivatives of error value with respect to the weight values of the last layer. We call these derivatives, gradients and use these gradient values to calculate the gradients of the second last layer. We repeat this process until we get gradients for each and every weight in our neural network. Then we subtract this gradient value from the weight value to reduce the error value. In this way we move closer (descent) to the Local Minima(means minimum loss).\n\nLearning rate \u2014 When we train neural networks we usually use Gradient Descent to optimize the weights. At each iteration we use back-propagation to calculate the derivative of the loss function with respect to each weight and subtract it from that weight. Learning rate determines how quickly or how slowly you want to update your weight(parameter) values. Learning rate should be high enough so that it won\u2019t take ages to converge, and it should be low enough so that it finds the local minima."
    },
    {
        "url": "https://becominghuman.ai/how-to-automate-your-life-ai-meets-ifttt-eb14a22f0256?source=user_profile---------2----------------",
        "title": "How to Automate your Life: AI meets IFTTT \u2013",
        "text": "I am aware of the fact that every Service/Product/Website/App on the internet today which revolves around you, follows certain set of rules to deliver information to you. I am also aware of the fact that these set of rules try so really hard to make it look like \u201chand-picked just for you\u201d, but fail miserably. You end-up spending way too much time on mundane but fairly important tasks.\n\nIf you\u2019re someone who clicks a lot of photos, and is always struggling when it comes to revisiting memories that you clicked or may be a content marketer who is always scouring for content on the web for your audience (I know you must be using a lot of tools to automate it, but not satisfied still). There are a ton of such tasks, which demands your attention and willingly or not you have to spare it for them. The services that are available which claims to free you off such chores, but doesn\u2019t really live up to their promise. Why? because, they\u2019re designed for the COMMON NEEDS, which is sparsely connected to YOUR NEEDS.\n\nStep 2: We will choose \u201cTwitter\u201d here. \u201cNew mention of you\u201d Trigger for our sample applet.\n\nStep 4: We\u2019ll select Attach Intelligence: Text Prediction (Your Model) as our action.\n\nStep 5: Now you need the Model which will do the Predictions for you. In this case we have trained a Sentence or Question Classifier whose model id is 3104 (shown below how you\u2019ll get for yours). And then choose Text (Tweet) which will be taken as the input for the model, and the prediction will be performed on it.\n\nYou\u2019ll have to Login to Mateverse -> Go to My Models -> Click the desired model tab. This is what you\u2019ll see when you\u2019ll click.\n\nThen Click Get API(s). In the Window you\u2019ll find the Model ID right there.\n\nStep 8: Out of a suite of different Triggers, we\u2019ll choose Use Intelligence: Text Model (Your Model) as our Trigger for the sample Execution applet.\n\nStep 9: In the similar fashion the way we built our Action for the Intelligence Applet, we\u2019ll make our Trigger to construct the orchestra.\n\nRemember: When you are filling the Category section, it should be amongst the ones on which you have trained the model and should be written EXACTLY what it is in on Mateverse.\n\nStep 10: Choose the desired action where you need intelligence and you want to automate it.\n\nStep 11: We chose Post a tweet as our action service. You can modify the sample text and make it what you want to."
    },
    {
        "url": "https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046?source=user_profile---------3----------------",
        "title": "Secret Sauce behind the beauty of Deep Learning: Beginners guide to Activation Functions",
        "text": "Activation functions are functions which take an input signal and convert it to an output signal. Activation functions introduce non-linearity to the networks that is why we call them non-linearities. Neural networks are universal function approximators and deep Neural Networks are trained using backpropapagation which requires differentiable activation functions. Backpropapagation uses gradient descent on this function to update the network weights. Understanding activation functions is very important as they play a crucial role in the quality of deep neural networks. In this article I am listing and describing different activation functions.\n\nRange: {0 or 1} Either o or 1\n\nFor the backpropagation process in a neural network, your errors will be squeezed by (at least) a quarter at each layer. Therefore, deeper your network is, more knowledge from the data will be \u201clost\u201d. Some \u201cbig\u201d errors we get from the output layer might not be able to affect the synapses weight of a neuron in a relatively shallow layer much (\u201cshallow\u201d means it\u2019s close to the input layer) \u2014 Source https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions\n\nDerivative of the softplus function is the logistic function.\n\nThe output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true\n\nConclusion: ReLU and it\u2019s variants should be preferred over sigmoid or tanh activation functions. As well as ReLUs are faster to train. If ReLU is causing neurons to be dead, use Leaky ReLUs or it\u2019s other variants. Sigmoid and tanh suffers from vanishing gradient problem and should not be used in the hidden layers. ReLUs are best for hidden layers. Activation functions which are easily differentiable and easy to train should be used."
    },
    {
        "url": "https://towardsdatascience.com/how-to-train-a-machine-learning-model-in-5-minutes-c599fa20e7d5?source=user_profile---------4----------------",
        "title": "How to train a Machine Learning model in 5 minutes \u2013",
        "text": "How to train a Machine Learning model in 5 minutes About Mateverse: We at Mate Labs have built Mateverse to enable each and everyone to build and train machine learning models without writing a single line of code. Training models on Mateverse is just a 5 steps process. There is no need to learn even the coding skills, let alone the concepts of Machine Learning if what you want is JUST an intelligent solution. Take a look at how it really works: 1. Model Naming \u2014 Give Your Model a Name: Let\u2019s start with giving your model a name, describe your model and attach tags to your model. Tags are to make your model searchable.\n\n2. Data Type Selection \u2014 Choose data type(Images/Text/CSV): It\u2019s time to tell us about the type of data you want to train your model. We support Images, Text and *.CSV (categorical data) data types. Select an appropriate data type and click next to proceed to the next step. We have also made some data sets public for everyone to start with. Public Data Sets are to give you an idea of what types of data you need to train your models on Mateverse.\n\n3. Data Upload \u2014 Upload your data or choose from Public Data Sets: Choose from public datasets like Jewellery Data set (Images), Gender Data Set (Images), Question or Sentence Data Set (Text), Numerai Data Set (CSV) or upload your data. Uploading your data is a straight-forward process, just select your files and type the label for selected files. Let\u2019s take an example of image data set. Images can be uploaded directly or can be zipped and uploaded as a compressed file (different compressed files for different labels/categories). All the images belonging to one unique category should be inside that compressed file. P.S.: Advice: Not to compress folders, if you compress folders, it won\u2019t work. Select files and compress them directly.\n\nText \u2014 Similarly you can upload Text (*.txt) files too. Here\u2019s a sample Text file. Each unique sentence should be a different line in the text file. CSV \u2014 If you want to train a model be it Classification or Regression(Forecasting), just upload one csv file which has it all. Once you\u2019ve uploaded the file you\u2019ll have to do two things: a) Skip the columns which you find redundant/insignificant for training the desired model, by pressing the skip button. b) Select one column on which you want to do the prediction after the model gets trained. You can even change the column names for your reference.\n\nP.S.: Try to avoid to unsupported characters in both Text and CSV files. 4. Type category(label) for the files (images/text file) that you have uploaded and click on submit to begin upload. Wait for some time till our web app uploads all the files. You can upload images for as many categories as possible. You need at least 2 labels(categories) for the classification on Image/Text to begin with. We support text files with \u201c*.txt\u201d extension. 5. Start Training: Push the button, to start the training. Now Mateverse\u2019s intelligent backend will start with processing the data that you have uploaded and preparing it for the training. Along with, it will also start selecting the best Machine Learning/Deep Learning algorithm to train the best model with the highest accuracy."
    },
    {
        "url": "https://medium.com/@matelabs_ai/public-data-sets-use-these-to-train-machine-learning-models-on-mateverse-4dda18a27851?source=user_profile---------5----------------",
        "title": "Public Data Sets: Use these to train Machine Learning models on Mateverse",
        "text": "To get you started with Machine Learning.\n\nThe ML platform which enables you to build and train customized models without writing even a single line of code.\n\nThis is the first in the series, and we are planning to make a lot more data sets public in the coming days, be it from the community or something we\u2019ll make."
    },
    {
        "url": "https://medium.com/@matelabs_ai/big-announcement-mateverse-is-in-public-beta-a968e727cfdc?source=user_profile---------6----------------",
        "title": "Big Announcement: Mateverse is in Public Beta \u2013 Mate Labs \u2013",
        "text": "We have been receiving a ton of request from all across the world during the past couple of weeks since the launch. So after testing our platform in the closed environment, we have finally decided to come out. We appreciate all the love and support we have received."
    },
    {
        "url": "https://medium.com/startup-grind/why-do-we-need-the-democratization-of-machine-learning-80104e43c76f?source=user_profile---------7----------------",
        "title": "Why do we need the Democratization of Machine Learning?",
        "text": "We are living in an era of hype. In this article, I am trying to discover the hype around Artificial Intelligence. The First thing I want to clear is that ML/DL are algorithms that are neither conscious nor intelligent or smart machines.\n\nThere\u2019s more to Artificial General Intelligence than just Machine Learning or Deep Learning. I agree that Deep Learning has penetrated industries and it holds the potential to disrupt industries, but it is nowhere near to being conscious or an intelligent machines.\n\n\u2018Singularity\u2019, \u2018AI taking over the world\u2019, and \u2018End of the world\u2019 were some of the most used phrases in media last year. The media being a primary source of information for most people, including investors and financial institutions makes it a vicious circle that is fueling this hype and adding air to the bubble. It can be either good or bad but if it bursts, it is going to affect us all (includes you too).\n\nIf you talk to researchers or experts, their views on the effects are poles apart from that of the media. They echo that AI is over-hyped and AGI is far away. We have a long way to go to achieve true Intelligence. Current AI(Deep learning/Machine Learning) applications can only do what they are trained to do. When the knowledge flows from the source to the publications, it gets distorted.\n\nAI/ML tools are limited to the researchers, industry, colleges and labs. They are not accessible to the masses in a simple easy-to-use form. In the last 2\u20133 years we have seen some pretty interesting use cases of modern deep learning.\n\nApps likes Prisma showed us that we just need to be creative to make it available to the masses. It uses deep learning to extract styles from images(\u201cVincent Van Gogh\u2019s Starry night\u201d, \u201cPicasso\u2019s Self Portrait 1907\u201d, \u201cFrida Cahlo\u201d) and apply extracted styles on to your images. AI as a lawyer, AI as a painter and AI as a doctor and so on\u2026 Just name it and you will find a startup/company working on it.\n\nIf I talk about industry use cases, Baidu runs a food delivery service and it uses AI to predict how long the food will take to get to the customers.\n\nGoogle uses deep learning for machine translation, search and for other products.\n\nFacebook uses it to recognize faces in your images which you upload on Facebook.\n\nGANs (Generative Adversarial Network) are the type of neural networks which learns to imitate and produce original content. Google Research used GANs to invent an \u201cencryption\u201d protocol. Andrew Ng, Chief Scientist at Baidu research says\n\nIn the next 3\u20135 years AI/ML is going to affect almost every industry. Infact it is a new industry and it is worth billions of dollars right now. A report by Bank of America Merrill Lynch forecasts it to be USD 14 Trillion by 2025, and it is a HUGE number.\n\n61% of companies in this survey by Wall Street Journal are planning to use AI within 5 years for business analytics, 45% for Machine Learning and 21% for self-learning robots.\n\nAccording Stack Overflow\u2019s Report, the global economy is seeing an explosion in the demand for Machine Learning expertise this year, as much as 3700 times more than year 2016. This is HUGE!\n\nAnd, we are missing out on one very important user group, the general people. What if users can participate in the creation of the AGI? Like, Creative users coming up with use cases which we had never thought of? What they need are easy-to-use tools. AI with everyone is the future of AI.\n\nPersonal computers revolutionized industries and powered people with tools that amplify one\u2019s creativity and productivity. Internet came out and connected us all together. Handheld devices like Mobile and tablets have given us all of that power in our hands. Now it\u2019s time for AI to go into everyone\u2019s hands, next revolution is getting in shape now and everyone has to participate in shaping it.\n\nAI can find the cure for diseases which we remained incurable for centuries , like cancer and sclerosis. It holds the power for a better future- a future which we have always dreamt of.\n\nAs far as the world is progressing, most of the programming might not even exist in the future. AI will code for us in the future. Data is the one thing on which I can bet on. The second, is how we think and how we use AI for our betterment and betterment of the society. We dream of AI and humans working together as companions, making the world a better place.\n\nIt is our responsibility to shape it well, so that it doesn\u2019t go rogue. Our curiosity will lead us there. One day we will reach there, true AGI.\n\nA revolution is happening right now, the whole world is embracing it. Mate Labs wants to be there as one shoulder for people to bank on and fuel this revolution by enabling everyone with the right tools, libraries, datasets and educational materials. We believe in building a community of enthusiasts, curious and thirsty folks standing for innovation and a bright future.\n\nWe have open sourced the implementation of All Convolutional Networks which is available on Github. We have also published a suite of easy-to-run scripts to install Tensorflow and Docker on your system to get you started. More to come soon."
    },
    {
        "url": "https://medium.com/@matelabs_ai/how-these-researchers-tried-something-unconventional-to-came-out-with-a-smaller-yet-better-image-544327f30e72?source=user_profile---------8----------------",
        "title": "How these researchers tried something unconventional to come out with a smaller yet better Image\u2026",
        "text": "Most modern convolution neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. Now in a recent paper it was noted that max-pooling can simply be replaced by a convolution layer with an increased stride without loss in accuracy on several image recognition benchmarks. Also the next interesting thing mentioned in the paper was removing the Fully Connected layer and put a Global Average pooling instead.\n\nRemoving the Fully Connected layer may not seem that big of a surprise to everybody, people have been doing the \u201cno FC layers\u201d thing for a long time now. Yann LeCun even mentioned it on Facebook a while back \u2014 he has been doing it since the beginning.\n\nIntuitively this makes sense, the Fully connected network are nothing but Convolution layers with the only difference is that the neurons in the Convolution layers are connected only to a local region in the input, and that many of the neurons in a Conv volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical. Therefore, it turns out that it\u2019s possible to convert between FC and CONV layers and sometimes replace FC with Conv layers\n\nAs mentioned, the next thing is removing the spatial pooling operation from the network, now this may raise few eyebrows. Let\u2019s take a closer look at this concept.\n\nThe spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information.\n\nFor example, let\u2019s consider Max Pooling. In case of Max Pooling, we define a spatial window and take the largest element from the feature map within that window. Now remember How Convolution works (Fig. 2). Intuitively the convolution layer with higher strides can serve as subsampling and downsampling layer it can make the input representations smaller and more manageable. Also it can reduce the number of parameters and computations in the network, therefore, controlling things like overfitting.\n\nTo reduce the size of the representation using larger stride in CONV layer once in a while can always be a preferred option in many cases. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). Also it seems likely that future architectures will feature very few to no pooling layers.\n\nConsidering all of the above tips and tweaks, we have published a Keras model implementing the All Convolutional Network on Github."
    },
    {
        "url": "https://medium.com/startup-grind/what-everyone-is-not-telling-you-about-artificial-intelligence-36c8552f3f53?source=user_profile---------9----------------",
        "title": "What Everyone is not Telling You about Artificial Intelligence",
        "text": "\u201cArtificial Intelligence\u201d: this term has become so popular/hyped/*add an adjective of your choice* in this decade, that we\u2019re talking about it more than ever. So much so that anything about AI becomes front page news. Tech media must be having a crush on AI for sure.\n\nPopular voices in the ecosystem, are so polarizing that you\u2019re left scared or excited. Mr. Singularity, Ray Kurzweil says:\n\nWe all must have heard of X-Prize, founded by Peter Diamandis, who has a similar take:\n\nThat\u2019s one heck of a pro side. But wait, even before we get started on what\u2019s missing in the whole picture, let me show you counter arguments too.\n\nMoreover, the poster boy of moonshots, Elon Musk too has some extreme thoughts:\n\nQuite radical, isn\u2019t it? But I cannot completely deny or agree with what these luminaries have to say, mostly because we are still discussing two decades from now.\n\nWell talking about future, there are conferences, tech/non-tech being organized around the globe to celebrate the success of AI that we have witnessed so far, discussing possibilities, new algorithms, and different implementations. People have started echoing their voices on Ethics of AI, and ways to tame the demon that we are creating.\n\nIn between all the hoopla, no one is even remotely considering the role of 3.4 billion population online, let alone 7.2 billion worldwide. Artificial Intelligence, by all meanings, is something so exponential and game changing that it is considered the last invention humankind will need, and humanity as a whole are not even taking part in it!\n\nTo be frank, no one knows what will happen tomorrow let alone 10 years from now. We all are working really hard towards a better future, and to build it in a way we want to. But we must accept it\u2019s just not limited to a pair of hands.\n\nWe wouldn\u2019t be here on Medium if the internet didn\u2019t come out of research labs and the military. There would not have been any internet if computers had not been democratized.\n\nWe\u2019re a democracy, right? Then why not contribute to something which promises so much, and see for ourselves if we\u2018ll be successful in building a Utopia and not turn it into a living nightmare.\n\nAt Mate Labs we have a firm belief that the we will be needing the entire humanity itself to build something that can come an inch closer to us humans. Our platform enables everyone to easily build and train Machine Learning models, without writing a single line of code. If code, complex algorithms, mathematics and tuning an ML model is something that has kept the masses away, then we are here with our shovels, to help build the future, where everyone can participate in unison.\n\nLet\u2019s make sure, that the future is not defined by just a bunch of AI nerds (and we don\u2019t mind being called one) but by the collective integrity of 7.2 billion individuals."
    }
]