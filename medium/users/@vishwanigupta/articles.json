[
    {
        "url": "https://medium.com/@vishwanigupta/more-test-scenarios-for-fake-news-detection-201606693ab3?source=user_profile---------1----------------",
        "title": "More test scenarios for fake news detection. \u2013 Vishwani Gupta \u2013",
        "text": "To garner more confidence in the algorithm, I ran it on a variety of cases.\n\nMost of the news sources are biased. Some try to neutralize it whereas some make money out of it. Usually, such sources write a genuine story but with a infuriating or misleading headline.\n\nOne such example is Political mayhem news. These news sources though report genuine news in the news body (although opinionated), trick us in the news headline. Because of the infuriating headlines, such articles get shared on the social media in abundance. One such example is:\n\nAlthough the title is very misleading, the content is genuine. The source claims that the article is shared 1.1K times on social media. As we all know, when we come across such articles on social media, we do not have time to go through the whole article but rely mostly on the headline. In this scenario, since the body is genuine but the title is not, I decided to label them individually.\n\nThe above story is genuine and from the right wing media, we can also see that Fox news has many articles similar to this story.\n\nWhen we stumble on news articles which are not visible in mainstream media and are also unorthodox, we usually ignore them taking them to be fake or hoax. But sometimes these stories are genuine news. These stories though published by the main stream media are not in focus because Donald Trump makes breaking news every day!\n\nSuch articles should not be assumed to be fake news. This problem can be avoided using this algorithm as it tags them as genuine news.\n\nWhen news players like Vice News (launched in December 2013), publishes some news article, we are not very sure of the genuineness of the article. For example,\n\nBut when we run the algorithm for this article, we find that this is indeed true.\n\nHence slowly this can enable user to build their trust in such news sources.\n\nIn case of a very recent news event, the algorithm tries to gather more proof as time passes. For example: On 8th March 2018, 21:12 PM CET, Trump turns the spotlight on violent video games, as reported by CBS and Reuters. But nothing was reported by most of the other reliable sources.\n\nIn such cases, when tested after some time, we can see that during the interval nearly all other sources reported similar stories.\n\nOne of the most famous fake news article was published by WTOE 5 News. The article claimed that Pope Francis had broken with tradition and unequivocally endorsed Donald Trump for President of the United States. This turns out to be FAKE.\n\nSo when dailybeast published that Pope Francis said that Trump is not a Christian, it is very natural to be skeptical about the article.\n\nAgain, using this algorithm we can avoid prior bias for news articles.\n\nThe technology sector is also not exempted from the problem of fake news. Recently, there was an article which took real facts and concluded something ominous thus creating a hoax. They reported that concerned artificial intelligence researchers hurriedly abandoned an experimental chatbot program after they realized that the bots were inventing their own language.\n\nIn technical news articles, I rely on tech news sources such as:\n\nUsing these sources in my algorithm, I got the following result for the article:\n\nEven with general news sources, the article was tagged as fake:\n\nUsing this algorithm, we can generate a score of similarity between query article and articles from reliable sources. We can protect ourselves from fake news articles published on social media through some novice news sources, yet not miss the genuine articles from the same sources. This is especially beneficial if a new news source comes into the picture.\n\nAlso we can calculate how frequently and in what ratio it publishes genuine news and deceptive news. Keeping this as a reliability score, we can use this as a reliable source. Hence it can break the monopoly of brands like CNN.\n\nFacebook\u2019s measures to fight fake news by prioritizing feeds just from the reliable sources has effected the business of new generation of digital media companies like Vice, Vox.\n\nUsing such an approach, this can be avoided besides keeping the reader informed about the genuineness of the story."
    },
    {
        "url": "https://medium.com/@vishwanigupta/what-i-have-learned-during-last-year-living-alone-in-germany-8c25de18835b?source=user_profile---------2----------------",
        "title": "What I have learned during the last year living alone in Germany.",
        "text": "I come from a humble family where one is taught to work hard since it brings satisfaction and happiness.\n\nBachelors from a private university in India, made me realize that I have to do better. Hence I pursued masters course in Germany.\n\nHere, not only did I find my calling but also learn how to live life:"
    },
    {
        "url": "https://medium.com/@vishwanigupta/combating-fake-news-with-the-help-of-machine-learning-b56537609564?source=user_profile---------3----------------",
        "title": "Combating Fake NEWS with the help of Machine Learning.",
        "text": "In the age of social media, everyone has witnessed the effect of fake news during USA Presidential election 2016, as also discussed in NYTimes. Recently there have been many efforts to alleviate the effect of false stories. Many of these efforts use machine learning approaches to predict the probability of a story being false.\n\nAs a news junkie, I used to get annoyed by fake news articles shared on social media since the latter has become one of my main sources of getting news. To filter such articles from my news feed, I followed the same procedure as most of us would do, that is to fall back on our reliable sources and check if these sources have similar stories on the topic in the matter.\n\nNonetheless, searching for a similar article in the trusted sources is a slow and cumbersome process. To automate it, I tried to use Natural Language Processing(NLP) approaches and came up with a simple algorithm which will do all this hectic work for me. My intention was to give the link to the dubious story and the algorithm tells me whether it can be trusted or not.\n\nI followed an unsupervised approach for detecting fake news. The main reasons I chose to use an unsupervised approach were as follows:\n\nRegardless of Donald Trump\u2019s claims, I trust the following sources as also cited by Forbes,:\n\nThe algorithm find the 20 most related articles from each of these sources and generate a distance measure. Through this distance measure, it then classifies the query article as fake or genuine.\n\nWhen one of my friends shared a fake story about \u2018US banks begin closing customer accounts caught using Bitcoin\u2019, I decided to test my approach.\n\nMy algorithm first scrapes the articles from these reliable news sources related to the Bitcoin and also makes use of the time period in which the article was published while scraping the articles. It then evaluates a similarity score/distance between those articles and the article in question using text mining. Text mining used in the algorithm makes sure that the idea behind the story and facts are understood by the algorithm.\n\nAs expected, the similarity scores were very low and the distances between the reliable source articles and query article were very high.\n\nI used the box plots because it is a useful way to visualize the range and distribution of different articles\u2019 distances from the query article. Details of a box plot can be found here.\n\nAt one point, the algorithm makes use of word embeddings as one of a text mining step to find distance/similarity between different articles. This can make NLP researchers wonder about the relatively new words which do not have a lot of examples to get good word vectors, since Word2vec model which generates word vectors, needs a huge dataset to generate high quality word vectors. The latter is taken care by training the word vectors on an even smaller dataset using Kernel PCA Skip gram model (which was a generated as a part of my masters thesis). More details can be found in the following post:\n\nIntrigued by the fake article results, I also tested the algorithm for the genuine articles like:\n\nI found that most of the sources have reported the articles with same story and hence the distance between the query article and reliable source articles were very small.\n\nThe threshold value was determined after testing a large set of articles (having both fake and genuine articles) using this algorithm. The set aslo included articles from different news sections such as politics, entertainment, sports, technology etc.\n\nBased on the results from these tests, a threshold value of 0.65, was easy to comprehend when using Kernel PCA embeddings. If the value is below 0.65, we can define a probability distribution of trust in the article.\n\nWhereas if the threshold is greater than 0.65, we can again define a probability distribution of mistrust in the article.\n\nI ran the algorithm on articles from various scenarios. For example: the articles, which might have false news article headline but true article body and vice versa.\n\nAlthough the article title is genuine, the story of the article is false as they claim that Chelsea Clinton practices Satanism. Therefore the distances are above the threshold. Here are the results:\n\nAnother example of a genuine news story and the results: As expected all the distances are below threshold.\n\nNow one might question that it is possible that the evaluated article is a very recent news and it might not be published else where. My algorithm takes care of this and will notify the user to test again in few minutes.\n\nAn example scenario is:\n\nHere it can be seen that only three of the reliable sources reported on the same news. When tested after some time to accumulate more proof, one can see that during the interval CNN has also published a story related to the topic:\n\nBased on the thresholding and number of fake news articles it has published, I can assign a reliability score to each source and also to novice news sources. This will make the news sources to be more careful of what they publish.\n\nIn the pursuit of making \u201cmeaningful social interactions\u201d, Facebook has recently announced that the content directly from publishers won\u2019t perform well unless people engage with it. This means that there will be less news related articles on the users\u2019 Facebook wall. It is especially bad according to WSJ as 45% of Americans get their news from Facebook.\n\nThe algorithm in this article, offers a solution to this problem. There can be an initial check before any news post could appear on the Facebook wall. If the probability is below the threshold, it could appear on the wall. Furthermore, the number of reliable sources having the similar article should be weighed in the scoring.\n\nThe use of KPCA Embeddings allows me to generalize the algorithm on any language and region, even on languages for which one doesn't have a huge dataset.\n\nUsing this algorithm, I can protect myself from fake news articles published on social media through some dubious sources, yet not missing the genuine articles from the same sources.\n\nOnce fake news publishers realize that this system can be used on a large scale and will filter out their fake news from people\u2019s access (affecting negatively their business), they will be compelled to be more responsible with the content they share."
    },
    {
        "url": "https://medium.com/@vishwanigupta/kpca-skip-gram-model-improving-word-embedding-a6a0cb7aad49?source=user_profile---------4----------------",
        "title": "Improving word embedding using Kernel PCA Skip gram model",
        "text": "The Natural Language Processing problems are not trivial, making them \u201cAI-hard problem\u201d in the field of computer science. In order to understand a sentence, one should not only understand the words used in it but also the context in which those words are used since a word can have very different meaning when used in different contexts. This makes language understanding ambiguous. \n\nThe solution comes by focusing on the \u201catomic units\u201d of a language, \u201cwords\u201d. To get the precise results, there is a dire need of a numeric vector representation for the words. These word representations help in extracting relations between various human languages, text or speech data.\n\nThere are many representations proposed over time:\n\nThese approaches does not produce good word vectors when we have a relatively smaller data set since they depend on learning from the context in which the words are used and do not consider the morphological information embedded in the words. FastText has tried to use sub word information but don\u2019t produce high quality word vectors with smaller datasets.\n\nWe propose an alternative approach which not only make use of the sub word information but also performs well on smaller datasets. This approach can be applied on both morphological and non-morphological languages. The main challenges we are trying to solve for generating word embeddings are:\n\nBelow, we first discuss nuts and bolts of the algorithm and then compare our model, Kernel PCA (KPCA) skip gram model with word2vec skip gram model trained using same parameters.\n\nThe kernel trick is a mathematical tool which can be applied to any algorithm which consists of a dot product between two vectors. Kernel trick states that to compute the dot products of vectors in the higher dimensional space, a kernel function can be used directly using the lower dimensional vectors. Therefore every dot product can be replaced by a kernel function. A Kernel function, can thus be defined as a function that takes as inputs vectors in the original space and returns the dot product of the vectors in the feature space. The feature space is usually a higher dimensional space where data can be linearly separable.\n\nIn order for a function to be a valid Kernel function, it should fulfill the \u201cMercer\u2019s theorem\u201d. According to the Mercers\u2019 theorem, every positive definite symmetric function can be seen as a kernel function. Here a positive definite symmetric functions correspond to a \u201cpositive definite symmetric Gram matrix\u201d which can be understood as a matrix which has all positive eigenvalues. Therefore a function which fulfill these conditions can be refereed as kernel function.\n\nWe usually, prefer to work with Gaussian Kernel:\n\nThe Gaussian RBF kernel is very popular and makes a good default kernel as the feature space of this kernel has an infinite number of dimensions.\n\nPrincipal component analysis(PCA) is a very important tool in data analysis. It helps in reducing a complex data set to a lower dimensional space thus revealing the hidden, dynamics. It helps in getting the most meaningful basis to re-express any noisy data set.\n\nKernel Principle Component Analysis is the nonlinear form of PCA, which allows to generalize standard PCA to nonlinear dimensionality reduction. In other words, it can be referred as a non-linear dimensionality reduction technique through the use of kernel function. Sch\u00f6lkopf et al. [1997] have shown that the generalization of PCA for non-linear data can be done via a kernel function.\n\nSkip gram is a model proposed by Mikolov et al., which tries to maximize classification of a word based on the words in the same sentence. In other words, the model predicts the context words given their center word.\n\nHere distance of words to the input word, represented by C, also comes in play. This distance is important since the more distant words are less related to the center word. So one can assign less weight to distant context words thus helps to attain better context words.\n\nWe have evaluated the performance of our Kernel PCA skip gram model by comparing it against the performance of the basic skip gram model trained using the same parameters. Since the model follows an unsupervised approach, evaluation of the model can only be done using \u201cintrinsic evaluation tasks\u201d, which assess how well the vectors capture the word meaning and the relationships between those words. This is achieved using evaluating the models on the following tasks:"
    }
]