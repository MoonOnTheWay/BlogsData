[
    {
        "url": "https://medium.com/click-bait/convolutional-neural-networks-81cab026aceb?source=---------0",
        "title": "Convolutional Neural Networks \u2013 click-bait \u2013",
        "text": "Convolutional neural networks are a sub-category of neural networks that have proved to be extremely effective in arduous tasks such as image recognition and classification. The first CNN was built by Yann Le Cunn: LeNet5, but the real interest in CNN was sparked off after the ImageNet success of Alex Krizhevsky who was able to bring down the classification error to a record 15% from 26%.\n\nIn this article I will be walking you through the steps of using a CNN on the MNIST digit classification task.\n\nA CNN is made up of four hidden layers namely:\n\nI will explain the role and the concept behind each layer as we progress through the code.\n\nThe CNN we are going to implement today is the model developed by Alex Krizhevsky and , also named after him -AlexNet.\n\nThis model uses Lasagne with a Theano or Tensorflow backend to train the neural network. So we will be working with these two libraries extensively, also, scikit learn to visualize our prediction information.\n\nThe MNIST dataset can be procured using the urllib module and using pickle to load the files into our model. We define a function load_dataset() :\n\nWe can see the image data from our dataset using the matplotlib module:\n\nThe output we get is the following:\n\nNext we have to create the structure of our neural network i.e. the layers, but before doing that, we have to define some variables which are essential for the functioning of our layers:\n\nThe batch_size refers to the number of training examples in each batch for our batch gradient descent. The output_size is set to 10 which indicates the number of classes we predict in the output layer i.e. 10 digits in mnist. The data_size is the shape or the dimensions of the input data i.e. our data is represented by a three-dimensional block of size 28x28 and as our image set is black and white the third value or RBG channel is set to 1. The input_var and output_var are Tensortype object which will hold the input and output data. Finally, net is the name of our model which is a simple python dictionary.\n\nThis is the structure of our neural net-AlexNet:\n\nThe input layer simply takes the input data and its shape as arguments.\n\nThe first layer is a convolution layer which has a filter i.e. a 2D matrix of fixed size F which shifts throughout the input matrix N with a specific stride S, producing an output of size- (N-F+2P)/S+1.\n\nNext, Pooling layers reduce the spatial size of the output by replacing values in the kernel by a function of those values i.e this layer shrinks the image size. Here we will be using the . A max-pool layer divides the matrix up into pools creates a yet smaller dimensional matrix containing the maximum value from each pool(using the same filter method as discussed above).\n\nThe fully-connected layers are hidden layers where each input neuron is linked to all neurons in the next layer i.e all neurons are connected.\n\nThe dropout layer is positioned just before the output layer. Dropout sets a proportion 1-dropout of activations (neuron outputs) passed on to the next layer to zero. The zeroed-out outputs are chosen randomly.\n\nThis reduces overfitting by checking that the network can provide the right output even if some activations are dropped out.\n\nFinally, our output layer is a softmax layer. In probability theory, the output of the softmax function can be used to represent a categorical distribution\u2014 that is, a probability distribution over n different possible outcomes. As we have 10 classes or 10 handwritten digits we use the softmax layer with ten neurons.\n\nKeep in the mind the filter_size for each convolution layer has to be set carefully, otherwise the shape of the data may become negative due to excessive convoluting(Do the math by hand!).\n\nTo better understand the various layers and layer arguments check out-http://lasagne.readthedocs.io/en/latest/modules/layers.html\n\nNext, to train the neural network we need to set some kind of update rule for our model. Hence, the first step is to define the loss function or the cost function i.e we will use the mean cross-entropy function.\n\nAs you can see we have used L2 regularization technique to reduce the noise in our network. Now, the second step is to define the update rule. Stochastic Gradient Descent is one of the most widely used and effective weight update functions. I have used a variant of the SGD called Adam which is an optimized version of SGD.\n\nFollowed by this, we will define the Theano functions which will train and test the model. We can extract the various inferences such as loss and accuracy and test error,etc. using the theano layer fucntions.\n\nThe theano layer functions enable us to gather inferences from within the layers of a neural network i.e hidden layer representations which are very useful if you are aiming at extracting features from an image set.\n\nNext we simply train the model over our training data using mini-batches of size 100( Remember? batch_size=100) and test our model to display the test error:\n\nI ran the program on my humble CPU (will post the results on GPU later) and got the following output:\n\nThus we got 98.53% accuracy on our test set. The model can be applied to larger datasets and real-world image processing applications."
    },
    {
        "url": "https://medium.com/click-bait/image-operations-gradients-and-edge-detection-c4279049a3ad?source=---------1",
        "title": "Image Operations-Gradients and Edge Detection \u2013 click-bait \u2013",
        "text": "It\u2019s Nishank here, welcome to your second OpenCV wit Python tutorial. We will build upon many concepts of OpenCV from beginner to advanced level and we will end it with a project! You can read the first tutorial Intro to OpenCV here.\n\nWhile processing a video feed, it is important to detect edges in order to recognise certain shapes in an image. Also when we work with depth analysis, it is very important that we understand the video feed, that is a stream of images. This is done by taking gradient in every frame and is the prime focus of this short post.\n\nWe will just be using library for this post for various gradient functions.\n\nNow let\u2019s get the video feed, by normal function.\n\nWe will be using 3 gradients-\n\nThey are different gradient functions which use different mathematical operations to produce the required image. For example Laplacian calculates laplacian derivative whereas sobel is joint gaussian and differenciation operation. Don\u2019t be overwhelmed by the details, just keep in mind that they are just different mathematical functions to analyse an image (you will see soon how). If you want to read more about them, you can read it here or here or here.\n\nThis is a simple filter to our image frame and just produces the laplacian derivative of the same. Here is just the datatype used to create lap. Let\u2019s see the output of the filter on a Donald Trump image.\n\nWhile this is not that helpful for this specific image, it can be used many other applications like object detection. Let\u2019s move on to the the second gradient, that is sobel-x, here x specifies the direction of derivative as x direction. Similarly sobel-y is same as sobel-x but direction is y direction. Let\u2019s observe the output.\n\nAs you can see, both images are different only in derivative direction and looks like some type of engravings. That is the reason why sobel is used for depth-analysis of images.\n\nNow, let\u2019s move on to edge detection part. Edge detection in OpenCV is done using a simple function called Canny Edge Detector, named after it\u2019s creator John.F.Canny. It is a low error rate detector which provides good localization and minimal response. You can learn more about Canny here and here.\n\nHere determines the size of the region we want to consider. We can change the size depending on our applications and the regions we are interested in. Let\u2019s see how well it detect the edges.\n\nObserving the image, we can say that it works pretty well. Now decreasing or increasing the area will give us a more unclear and clearer image respectively. Let\u2019s try with a area.\n\nAs you can see, they are more well defined and clear. This brings us to the end of the post. You can play around with the gradients and the edge-detector all you want and can also apply to various applications. All the code used in this post can be found by clicking the banner below. I will be posting the next post to the series soon. Stay tuned and subscribe to our newsletter for an awesome experience and never missing an update."
    },
    {
        "url": "https://medium.com/click-bait/intro-to-opencv-video-input-color-and-video-analysis-with-python-772ad55e335b?source=---------2",
        "title": "Intro to OpenCV- Video Input, Color And Video Analysis With Python",
        "text": "Intro to OpenCV- Video Input, Color And Video Analysis With Python\n\nIt\u2019s Nishank here, welcome to your first OpenCV wit Python tutorial. Starting here, we will build upon many concepts of OpenCV from beginner to advanced level and we will end it with a project!\n\nNow OpenCV is the one of them most searched tutorials in artificial intelligence but unfortunately, there are very few tutorials about it, and many of tutorials that are there are very complex to understand. So my goal here is to make learning OpenCV, fun and understandable to everyone. We will go very deep but at the same time will maintain the simplicity of the series. So let\u2019s start!\n\nAs an intro to OpenCV, we will start by\n\nSounds fun right? Let\u2019s start by installing the required libraries. Before we start with our libraries, I would like you to install Enthought\u2019s Canopy which is comprehensive Python analysis environment. It has an inbuilt package manager which makes it easier for us to install libraries. After installing Canopy, open it and go to Package Manager tool and search for opencv in the available tools.\n\nIf you don\u2019t want to go through the work of installing Canopy, you can install OpenCV using terminal also (But I would recommend using Canopy as it will be helpful for installing libraries in future also). Just type\n\nNext we will try to get an image and show it\n\nWe imported which is the OpenCV library we will be using in this entire series. Then using function, we read our image (note that the image should be same folder as your code, else give the path). And the we used function to show the image stored in the variable on screen and gave the window a name . In my case, I imported an image of a watch.\n\nNext, we used to wait for any key to be pressed and then used to close all windows. Simple enough?\n\nNow just change the code in function, a little bit.\n\nThe option will import our image and will convert it to grayscale. It would look something like this.\n\nNow that we are done with image, let\u2019s start some video processing. I hope you are familiar with the concept of loop in python. If not see it here.\n\nWe start by declaring and defining the variable we will be using for capturing the video.\n\nWe used here to define our variable for video capture. In this code is used to specify the primary camera, you can depending on how many cameras are you using.\n\nDon\u2019t be overwhelmed. We will take it bit by bit. First, we used a loop (if you don\u2019t about loops in python, refer to the link above). We did so in order to take frames of our video continuously, since a video is made up of multiple frames or images, the while loop is only to take those images, do modifications and show them continuously, so that what we see is a video.\n\nAfter starting the while loop, we took our variable, to take input using the function and stored the input in a variable . The variable is for a different function that we will discuss later, so for now don\u2019t worry about it.\n\nNext we used function, to show our . Remember that all of this is in a while loop, so it we will see it as a video. The next step is similar to what we have done in the image but with a minor difference.\n\nHere, we have change it to to indicate that the window will be closed only if key, i.e will be pressed. We have used to take 8 bit value of q, in a nutshell, \u201c& 0xff\u201d effectively masks the variable so it leaves only the value in the last 8 bits, and ignores all the rest of the bits. And is used to take numerical value of . So, in short, all windows will be closed when you press q. We get something like this, after execution-\n\nNext, we will try to convert it to grayscale. We just need to add a single line to our code, and that would do.\n\nWe just added function and used and as an input. I think this part of code needs no further explanation. After running the code, we would see the following output-\n\nNow that we have our video ready, let\u2019s draw some shapes on it. We will start with a line. To draw a line, write the following code inside the loop, after and before .\n\nUsing function, we can draw a line. The first input is the variable or set of images on which we want to draw a line. The next 2 inputs are the co-ordinates for the line and the third input is the specified color for the line (note that OpenCV uses BGR format). Now the last input is the thickness of the line. You can change these according to your needs. After running the code-\n\nNow let\u2019s write some text on our video.\n\nWe declared what font we want to use by , we will use Hershey Simplex Font. Next, we used function to put our text into the . It would look something like this-\n\nNext we will draw a rectangle, which has the same syntax as the of line but with a few changes.\n\nHere, function takes 2 different inputs from function. The second and third inputs are coordinates of top-left corner and bottom-right corner respectively, rest all parameters are the same. It will yield us the following video output-\n\nI think we are done with the basics of OpenCV. All the code used in this post can be found by clicking the banner below. I will be posting the second post to the series soon. Stay tuned and subscribe to our newsletter for an awesome experience and never missing an update."
    },
    {
        "url": "https://medium.com/click-bait/a-simple-neural-network-with-a-single-neuron-8a4e3b0a4148?source=---------3",
        "title": "A Simple Neural Network With A Single Neuron \u2013 click-bait \u2013",
        "text": "It\u2019s Nishank here, and today we will learn, how to make a simple neural net and predict some numeric output using that. A neural network is a simple mathematical unit, which takes some input and predicts something on the basis of training it received earlier.\n\nA neural network is made up of many neurons which help in computation. A single neuron has something called a weight attached to it, also called synaptic weight. These weights changes in the direction of our prediction when we train our neural network.\n\nSo the focus of this post is, creating a neural network with a single neuron, training it for 10000 runs, predicting the output in every run, obtaining the error by comparing it with our expected output, adjusting weight based on the error and then finally try to predict the actual output.\n\nThe best part about this is, we will not be using any libraries for making our neural net (except obviously). This would help us in understanding the basic structure of a neural network and how it actually works.\n\nWithout making it anymore overwhelming for you, let\u2019s start with the code. We will start by installing the library required, in this case .\n\nThen we will import the libraries required. We will need for exponential (you will discover why, later), (for array obviously), for generating random numbers and for doing the dot product (right the physics one!) all from .\n\nOkay, so we will be using the following function for our program training and testing purposes as data.\n\nYou must be wondering how the heck did he got that output? Don\u2019t you worry, no rocket science here, I just copy-pasted the entire row of the table!\n\nSo we will be using this data set. Now, moving on, let\u2019s declare the input and output variables for our program, which would both be arrays that represent this table.\n\nThe used in is used to transform the array, thereby making it a vector so that it can be multiplied easily and so that we can . tally it easily with the table . Also, is an array of arrays, which will be used in our model. Notice, that I have exactly replicated the table above in our input and output variables.\n\nOur next step in generation of random synaptic weights for our neuron at the starting of training. These generated weights will be adjusted as our program progresses. We will do so by using function.\n\nWe have used function here to keep the generated random weights the same, each time the program is executed, until it is closed.\n\nNow here is a concept worth understanding. Well it\u2019s more of an application than a concept. We need to generate random numbers for a neuron with 3 input connection and 1 output connection, so our function becomes .\n\nAlso we need to generate random numbers in the range -1 to 1 (since our output is either or ). While using the random function for range , the random function is defined as : , so here and and the function becomes something like this, where .\n\nNext, we will create the main part of our neuron, i.e the function we will use to train it. Because a neuron without training is as good as a piece of log.\n\nDon\u2019t be intimidated by this simple function (yeah simple!), let\u2019s break it into small pieces. the train function contains 4 main blocks : the head, output predictor, error calculator and weight adjuster. It\u2019s that simple!\n\nThe first part of function is the head. It accepts 3 arguments, namely (the inputs for training), (the outputs for training) and (the number of time the loop will run to train the neuron, so that it\u2019s weights could be adjusted.).\n\nThere is also a for loop which runs for the number of iterations provided. Here is used to specify the number of time the loop should run. The remaining 3 parts of function are inside this loop.\n\nThis part of is used to predict output for the given using the current . This is done by using function, which can be defined as follows:\n\nHere we will be using sigmoid value of and . For combining these, we have used function to perform the dot product of and and sending them into a function.\n\nA sigmoid function is a common function used in neural networks to predict the output. What it does, is it normalizes the value of the dot product between and (exactly the thing with need!).\n\nIt looks something like this:\n\nThe next part is calculating the error. It is simple step that subtracts the we got in Output Predictor part from the actual expected output stored in the variable.\n\nThe last and the most important part of function. Here are adjusted using the obtained in Error Calculator.\n\nThe sigmoid curve looks something like this -\n\nNotice, that the line gets straighter and constant as we move towards the edges. That means the gradient increases and the surety of getting the right prediction increases. We will also use this concept of sigmoid gradient in our program to get more accurate values.\n\nHere, first we have given our from Output Predictor to function. What it does is, it calculates the Sigmoid Gradient of the output using the formula above so that we can move towards the edges of the sigmoid curve and get more accurate results.\n\nNext, we will multiply our value to sigmoid gradient of to modify our outputs accordingly and finally, we take the dot product of these and the transpose ( ) of to get our adjustment.\n\nThe final step of each run and function is to update the by adding to it. We declare the variable as to access the global variable called (because in python, definition and declaration are together and it would give an error if we update before declaring them, also they would be present in local scope).\n\nThe rest of the program is just passing the values into function and then printing the results.\n\nWe just printed the Random Starting Synaptic Weights, and the called function by passing , and 10000 as our number of runs.\n\nNext we printed the synaptic weights after training and the tested the neural net with a custom input of , ideally we should get as it is in the 2nd row and we just copied the 2nd row in the output (see the top of this post). Let\u2019s see what happens!\n\nObserve that our output is , which is very close to , exactly the output we wanted. Also the synaptic weights have changed and look much more consistent. We trained our model 10000 times and it can predict output so closely, and this is just one neuron, imagine the power of neural nets with 1000s of neurons!\n\nI hope you enjoyed reading this post and it was both informative and interesting and it improved your practical understanding of neural nets. You can get the entire code of this post by clicking the banner below. Don\u2019t forget to subscribe!"
    },
    {
        "url": "https://medium.com/click-bait/web-scrapper-using-beautiful-soup-4-e1847256e82c?source=---------4",
        "title": "Web Scrapper Using Beautiful Soup 4 \u2013 click-bait \u2013",
        "text": "Or, whatever page you want to scrape. Now, because I am using Chrome, right click and inspect will give you the summary of the source of the web page. I want to get the temperature of Chennai, I will inspect the block that displays the temperature, it would look something like this.\n\nWe have imported urllib2 to parse the url which we want to scrape. BeautifulSoup is for scrapping and modifying scrapped data. csv is for storing our scrapped data into a CSV file, which will be discussed later. And, datetime is used to obtain current date and time, at the time we scrape the data.\n\nWeb Scraping is a technique which is used to obtain information from web pages, which saves a lot of time and provides you with abundant data. We will be making a Web Scraper in Python using Beautiful Soup 4 , which is a python library for getting data from HTML pages and saves days of work on the code. We will be parsing AccuWeather for getting the weather for Chennai, India.\n\nNotice that the temperature is displayed in a <span> block and the class is large-temp . Make a note of this as it would be used in our script. Now the next thing I want to display is the other stats like pressure, wind etc. So I will inspect that block, it looks something like this.\n\nHere, entire stats is an unordered list with block <ul> and the class is stats . We should note this also, because it will be used in our script. Now the next thing I want to scrape is the Sunrise/Sunset time. When I inspect that block, it looks something like this.\n\nThe entire block is again an unordered list but this time, the is . After noting this also, we are ready for some action. Let\u2019s write our script!\n\nLet\u2019s start by specifying the url that we have to scrape\n\nNext, we will be get our website content and will store into a variable called and then parse it using Beautiful Soup to store it in a variable called .\n\nWe have used function of to open the and then used to parse the using . Our next step is finding the block that we need in the webpage. We require 3 blocks in total, the temperature, stats, and Sunrise/Sunset time.\n\nHere, we have used function to find the block we need, by specifying the type of block, such as or and also the type of identifier, in this case and also the class name, like for , for and for . Recall that we noted these class names, when were inspecting the web page.\n\nNext step is to remove or strip starting and trailing tags form the data obtained, so that we can get only the information we require. We can do this using function.\n\nWhat this would do is strip() the irrelevant data and will only print the information we require. That is, we will get the current Temperature, Stats and Sunrise Time of Chennai, India.\n\nNow that we have our output, the next step in our script is storing the temperature information into a CSV file, every time we run our code. This will help us keep a track of the temperature. We will also insert the current Date and Time with the temperature reading.\n\nBut before we do that, we have a problem to solve. CSV writer, follows a unicode codec but when we scrape the content from the webpage using BeautifulSoap, it is not in . So, there is a clear need to convert it to before storing in CSV.\n\nAfter we are done encoding, we can now store the scrapped details into a CSV along with the Date and Time. We will use for this purpose.\n\nWe have used function to open a file called in (append mode) mode. Opening a file in append mode ensures that the data that is already present in the file is not overwritten and the data is written after that. We have opened it in a variable called , which is then used to initialize object called . After all the initializations, we write the data into the file using function, in which , , are passed. Also, using library function, , we have stored the current date and time of the script run. The CSV file that is generated i.e. would look something like this."
    },
    {
        "url": "https://medium.com/click-bait/image-data-augmentation-using-keras-a6a61edbc59f?source=---------5",
        "title": "Image Data Augmentation Using Keras \u2013 click-bait \u2013",
        "text": "The focus of this post is Image Data Augmentation. When we work with image classification projects, the input which a user will give can vary in many aspects like angles, zoom and stability while clicking the picture. So we should train our model to accept and make sense of almost all types of inputs.\n\nThis can be done by training the model for all possibilities. But we can\u2019t go around clicking the same training picture in every possible angles and imagine that when the training set is as big as 10000 pictures!\n\nThis can be easily be solved by a technique called Image Data Augmentation, which takes an image, converts it and save it all the possible forms we specify. We will be using Keras for this, which is a deep learning library for Theano and Tensorflow.\n\nLet\u2019s start by installing the packages required for our model.\n\nWe have installed , , , because they are dependencies required for and since keras works on a backend, there is a need to install that as well. You can read more about tensorflow installation here. We will be using for performing Image Augmentation.\n\nWe start our program by importing keras image preprocessing.\n\nHere, is used to specify the parameters like rotation, zoom, width we will be using to generate images, more of which will be covered later. is used to convert the given image to a numpy array which will be used by the , will be used to load the image to modify into our program.\n\nWe have used here to specify the parameters for generating our image, which can be explained as follows:\n\nfill_mode : One of {\u201cconstant\u201d, \u201cnearest\u201d, \u201creflect\u201d or \u201cwrap\u201d}. Points outside the boundaries of the input are filled according to the given mode\n\nAfter specifying the parameters and storing them in variable, we move towards importing our image.\n\nis used to load the required image, you can use any image you like but I would recommend an image with a face like that of a cat, a dog or a human!\n\nNext, we use to convert the image to something numerical, in this case a numpy array, which can be easily fed into our function (don\u2019t worry it is explained later!). We store our converted numpy array to a variable .\n\nThen, we have to reshape the numpy array, adding another parameter of size 1. We do so in order to make it a numpy array of order 4 instead of order 3, to accommodate a parameter called channels axis. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, it should have value 3.\n\nFor instance, I will take this image as my input (Yes, a dog!)\n\nNow that we have our input in form, let\u2019s start producing some output.\n\nWe run a loop for 20 times, and for each iteration we use function. We have given - the numpy array for the input image, - the directory to save output, - the prefix for the names of the images and - the image format as input.\n\nAs we have specified 20 iterations, 20 images of the dog with changes that were specified in datagen will be produced and will be stored in the folder called preview.\n\nThe output images look something like these-\n\nNotice that each image is a bit different from the other due to zoom, rotation, width or height shift etc. This will help the model you will be building to recognise a large number of images, thus making it more efficient.\n\nThat\u2019s all for this post, subscribe for more Machine Learning, Neural Networks and Deep Learning updates."
    },
    {
        "url": "https://medium.com/click-bait/splitting-csv-into-train-and-test-data-1407a063dd74?source=---------6",
        "title": "Splitting CSV Into Train And Test Data \u2013 click-bait \u2013",
        "text": "While working with datasets, a machine learning algorithm works in two stages \u2014 the testing and the training stage. Normally the data split between test-train is 20%-80%.\n\nIn order to successfully implement a ML algo, you need to be clear about how to split the data into testing and training, and this short post talks exactly about that.\n\nWe will start by installing packages needed.\n\nWe will be using to import the dataset we will be working on and for the function, which will be used for splitting the data into the two parts.\n\nNext, we will start our program by importing the packages needed for the process.\n\nAs explained above, for importing dataset and for function.\n\nThe next step would be importing the dataset. We will use Forest Fires Dataset from UC Irvine Machine Learning Repository.\n\nThe CSV file contains the following data, displayed using the function. Now, we will be splitting the following data into labels and features. Labels are the data which we want to predict and features are the data which are used to predict labels.\n\nHere, we have used as the label for predicting temperatures in , data other than is taken as features using the function in .\n\nOur last step would be splitting the data into train and test data, we will do that using function.\n\nIn the function, we passed the variable and that we obtained previously, along with which is used to indicate that the test data should be 20% of the total data and rest 80% should be train data. We used function to print the first five elements of both the data. The function was used to get an idea about the rows and columns of the data we have obtained. Notice that train data has 413 rows whereas test data has 104 rows, which is 20% of the original data, exactly the result we wanted!\n\nI hope this small skill will come handy in every Machine Learning program you would be doing in future which involves working with CSV files. I also hope you enjoyed the post."
    },
    {
        "url": "https://medium.com/click-bait/text-classification-using-machine-learning-cff96602c264?source=---------7",
        "title": "Text Classification Using Machine Learning \u2013 click-bait \u2013",
        "text": "The amount of data being generated has risen exponentially over the past decade. In fact 90% of the data we have today has been generated in the last three years and a majority of that data is text based.\n\nText based data finds the widest scale of application on any platform and this data needs to be analysed and stored in an efficient manner.\n\nIn this post, we will be trying to make a text classifier that will make use of the 20 news groups dataset originally developed by Ken Lang to classify documents into different categories based on their content.\n\nInstall the following packages before starting to write the actual code:\n\nThese are the imports needed to complete the model for our classifier\n\nThe package imports the predefined datasets for scikit learn. The package imports the necessary methods for feature extraction and tf-idf transformation. The last import deals with building the pipeline which is quintessential for our model to work in a scalable manner.\n\nThe dataset can be downloaded from the github link provided at the bottom of the article\n\nThe first part of the code deals with fetching the data and splitting into a training and test set. We will use the to fetch our data.\n\nThe variable are the class labels or the various fields in which our documents will be segregated.\n\nThe predefined method copies the text data into an scikit learn bunch object variable.The extension denotes a scikit learn bunch object which as predefined attributes such as which outputs the categories or labels.\n\nNext, we will be using the method to convert our text data into an feature vector. This is required because the scikit learn algorithms cannot work with text data and need an integer vector variable as an input.\n\nNext, we will be removing the unnecessary and redundant words which can hinder our classifier and give us erroneous results. We will do so by using the method which is included in the sklearn package.\n\nNow we move on to training a classifier model. We are going to use the NaiveBayes algorithm to train our model.\n\nNaive Bayes is one of the most used classifier algorithms in real time application. The variable is defined as a test for the model and the predicted values are stored in . This is a simple implementation of our classifier on two test values.\n\nWe will also a build a pipeline which is a useful method in python used to create a object containing the vectorizer,transformer and classifier methods.\n\nWe can change the arguments of the pipeline method to alter the classifier or transformer for that particular model. This method increases the scalablity of our code by leaps and bounds.\n\nNow our model is completely ready to be experimented on a full size test set so that we can asses our models performance and accuracy.\n\nSo, we test our model on the test dataset.\n\nWe have our finished text classifier ready to be implemented on any kind of dataset. The classifier can be adapted to the size of the dataset by changing the classifier algorithm such as SVM.\n\nHere is the code for the SVM implementation for our model.\n\nVarious optimization algorithms could also be used such as RmsProp and Nesterov momentum to improve the accuracy of the classifier."
    },
    {
        "url": "https://medium.com/click-bait/wine-quality-prediction-using-machine-learning-59c88a826789?source=---------8",
        "title": "Wine Quality Prediction Using Machine Learning \u2013 click-bait \u2013",
        "text": "Having read that, let us start with our short Machine Learning project on wine quality prediction using scikit-learn\u2019s Decision Tree Classifier.\n\nFirst of all, we need to install a bunch of packages that would come handy in the construction and execution of our code. Write the following commands in terminal or command prompt (if you are using Windows) of your laptop.\n\nnumpy will be used for making the mathematical calculations more accurate, pandas will be used to work with file formats like csv, xls etc. and sklearn (scikit-learn) will be used to import our classifier for prediction.\n\nWe are now done with our requirements, let\u2019s start writing some awesome magical code for the predictor we are going to build.\n\nYou maybe now familiar with numpy and pandas (described above), the third import, is used to split our dataset into training and testing data, more of which will be covered later. The next import, is used to preprocess the data before fitting into predictor, or converting it to a range of -1,1, which is easy to understand for the machine learning algorithms. The last import, from is used to import our decision tree classifier, which we will be using for prediction.\n\nThe very next step is importing the data we will be using. For this project, we will be using the Wine Dataset from UC Irvine Machine Learning Repository.\n\nWe use function in pandas to import the data by giving the dataset url of the repository. Notice that \u2018;\u2019 (semi-colon) has been used as the separator to obtain the csv in a more structured format.\n\nNow we have to analyse, the dataset. First we will see what is inside the data set by seeing the first five values of dataset by head() command.\n\nWe see a bunch of columns with some values in them. Now, in every machine learning program, there are two things, features and labels. Features are the part of a dataset which are used to predict the label. And labels on the other hand are mapped to features. After the model has been trained, we give features to it, so that it can predict the labels.\n\nSo, if we analyse this dataset, since we have to predict the wine quality, the attribute will become our label and the rest of the attributes will become the features.\n\nOur next step is to separate the features and labels into two different dataframes.\n\nWe just stored and in , which is the common symbol used to represent the labels in machine learning and dropped and stored the remaining features in , again common symbol for features in ML.\n\nNext, we have to split our dataset into test and train data, we will be using the train data to to train our model for predicting the quality. The next part, that is the test data will be used to verify the predicted values by the model.\n\nWe have used, function that we imported from sklearn to split the data. Notice we have used to make the test data 20% of the original data. The rest 80% is used for training.\n\nNow let\u2019s print and see the first five elements of data we have split using function.\n\nAfter we obtained the data we will be using, the next step is data normalization. It is part of pre-processing in which data is converted to fit in a range of -1 and 1. These are simply, the values which are understood by a machine learning algorithm easily.\n\nYou can observe, that now the values of all the train attributes are in the range of -1 and 1 and that is exactly what we were aiming for.\n\nTime has now come for the most exciting step, training our algorithm so that it can predict the wine quality. We do so by importing a and using to train it.\n\nThe next step is to check how efficiently your algorithm is predicting the label (in this case wine quality). This can be done using the function.\n\nThis score can change over time depending on the size of your dataset and shuffling of data when we divide the data into test and train, but you can always expect a range of \u00b15 around your first result.\n\nNow we are almost at the end of our program, with only two steps left. First of which is the prediction of data. Now that we have trained our classifier with features, we obtain the labels using function.\n\nOur predicted information is stored in but it has far too many columns to compare it with the expected labels we stored in . So we will just take first five entries of both, print them and compare them.\n\nDon\u2019t be intimidated, we did nothing magical there. We just converted from a numpy array to a list, so that we can compare with ease. Then we printed the first five elements of that list using loop. And finally, we just printed the first five values that we were expecting, which were stored in using function. The output looks something like this.\n\nNotice that almost all of the values in the prediction are similar to the expectations. Our predictor got wrong just once, predicting 7 as 6, but that\u2019s it. This gives us the accuracy of 80% for 5 examples. Of course, as the examples increases the accuracy goes down, precisely to 0.621875 or 62.1875%, but overall our predictor performs quite well, in-fact any accuracy % greater than 50% is considered as great.\n\nUnfortunately, our rollercoaster ride of tasting wine has come to an end. But stay tuned to click-bait for more such rides in the world of Machine Learning, Neural Networks and Deep Learning."
    }
]