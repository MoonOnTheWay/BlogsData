[
    {
        "url": "https://medium.com/@ilblackdragon/program-synthesis-papers-at-iclr-2018-3d3fd3b24464?source=user_profile---------1----------------",
        "title": "Program Synthesis Papers at ICLR 2018 \u2013 Illia Polosukhin \u2013",
        "text": "Program Synthesis is a subfield of Computer Science to automatically construct a program that satisfy given specification. Specification can be set of input/output examples (unit tests), natural language, first order logic expression or any other form that is easier to write then the expected program.\n\nProgram Synthesis has been around for awhile as a Programming Languages subfield and usually been approached with symbolic methods. In the last few years though, Machine Learning community started to apply deep learning for these problems.\n\nI strongly believe that Program Synthesis is a great test bed for a lot of methods in deep learning (structured predictions, RL with sparse reward, reasoning, using external knowledge, meta learning, etc) and very happy to see more papers in this field at ICLR 2018. Disclaimer: NEAR has two papers in this list.\n\nParametrized Hierarchical Procedures for Neural Programming. Roy Fox \u00b7 Richard Shin \u00b7 Sanjay Krishnan \u00b7 Ken Goldberg \u00b7 Dawn Song \u00b7 Ion Stoica. In Mon AM Posters\n\nTowards Specification-Directed Program Repair. Richard Shin \u00b7 Illia Polosukhin \u00b7 Dawn Song. In Mon AM Workshops\n\nTree-to-tree Neural Networks for Program Translation. Xinyun Chen \u00b7 Chang Liu \u00b7 Dawn Song. In Mon AM Workshops\n\nNeural-Guided Deductive Search for Real-Time Program Synthesis from Examples. Ashwin Vijayakumar \u00b7 Abhishek Mohta \u00b7 Alex Polozov \u00b7 Dhruv Batra \u00b7 Prateek Jain \u00b7 Sumit Gulwani. In Mon PM Posters\n\nLearning to Represent Programs with Graphs. Miltiadis Allamanis \u00b7 Marc Brockschmidt \u00b7 Mahmoud Khademi. In Tue AM Talks\n\nNeural Sketch Learning for Conditional Program Generation. Vijayaraghavan Murali \u00b7 Letao Qi \u00b7 Swarat Chaudhuri \u00b7 Chris Jermaine. In Tue AM Talks\n\nDynamic Neural Program Embeddings for Program Repair. Ke Wang \u00b7 Rishabh Singh \u00b7 Zhendong Su. In Tue AM Posters\n\nNeural Program Search: Solving Programming Tasks from Description and Examples. Illia Polosukhin \u00b7 Alex Skidanov. In Tue PM Workshops\n\nCombining Symbolic Expressions and Black-box Function Evaluations in Neural Programs. Forough Arabshahi \u00b7 Sameer Singh \u00b7 Anima Anandkumar. In Wed AM Posters\n\nImproving the Universality and Learnability of Neural Programmer-Interpreters with Combinator Abstraction. Da Xiao \u00b7 Jo-Yu Liao \u00b7 Xingyuan Yuan. In Wed AM Posters\n\nLearning to Represent Programs with Graphs. Miltiadis Allamanis \u00b7 Marc Brockschmidt \u00b7 Mahmoud Khademi. In Wed PM Posters\n\nTowards Synthesizing Complex Programs From Input-Output Examples. Xinyun Chen \u00b7 Chang Liu \u00b7 Dawn Song. In Wed PM Posters\n\nLeveraging Grammar and Reinforcement Learning for Neural Program Synthesis. Rudy Bunel \u00b7 Matthew Hausknecht \u00b7 Jacob Devlin \u00b7 Rishabh Singh \u00b7 Pushmeet Kohli. In Thu AM Posters\n\nAs additional bonus, want to mention few neural architecture search, where the task is to find a neural program that best solves some machine learning problem.\n\nHierarchical Representations for Efficient Architecture Search. Hanxiao Liu \u00b7 Karen Simonyan \u00b7 Oriol Vinyals \u00b7 Chrisantha Fernando \u00b7 Koray Kavukcuoglu. In Mon AM Posters\n\nAccelerating Neural Architecture Search using Performance Prediction. Bowen Baker \u00b7 Otkrist Gupta \u00b7 Ramesh Raskar \u00b7 Nikhil Naik. In Thu AM Workshops\n\nFaster Discovery of Neural Architectures by Searching for Paths in a Large Model. Hieu Pham \u00b7 Melody Y. Guan \u00b7 Barret Zoph \u00b7 Quoc V Le \u00b7 Jeff Dean. In Thu AM Workshops\n\nSimple and efficient architecture search for Convolutional Neural Networks. Thomas Elsken \u00b7 Jan Metzen \u00b7 Frank Hutter. In Thu PM Workshops\n\nHope this list is useful to guide your search!\n\nThis is a list of paper that I\u2019ve noticed, which may not be complete. Please let me know if I missed something."
    },
    {
        "url": "https://medium.com/@ilblackdragon/pytorch-dynamic-batching-f4df3dbe09ef?source=user_profile---------2----------------",
        "title": "PyTorch \u2014 Dynamic Batching \u2013 Illia Polosukhin \u2013",
        "text": "If you have been reading my blog, you may have seen that I was a TensorFlow contributor and built a lot of high-level APIs there.\n\nIn Feb 2017 though, I have left Google and co-founded my own company \u2014 NEAR.ai. Where we are teaching machines to write code from natural language.\n\nAs part of this work, we are building Deep Learning models that are reading or writing code in a tree format. After trying to manage this complexity in TensorFlow, I\u2019ve decided to give a try to PyTorch.\n\nPyTorch is a framework built by Facebook AI researchers and has been growing in popularity in Natural Language and Reinforcment Learning research community. It\u2019s main benefit is in dynamic graph building principle \u2014 compared to Tensorflow, where graph is built once and then \u201cexecuted\u201d many times, PyTorch allows to dynamically rebuild graph using simple Python logic, as if you were doing computation with numpy arrays.\n\nThis flexibility attracted people who work with complex input/output data [e.g. language, trees, graphs] or need to run some custom logic in the middle of the computation [e.g. Deep RL].\n\nHere I want to talk about batching things. Even though PyTorch is fast by using GPU accelerators and in general pushing computation on C modules, if you are not batching your computation \u2014 you are still going to pay the toll.\n\nRecursive neural network [TreeLSTM as an example] are especially hard to batch, as each example is a different tree.\n\nThe naive implementation would look like this:\n\nThere is a way to batch this manually: going after each operation that processes inputs differently, figuring out how to batch inputs and then unbatch outputs. Here is an example of this in great article by James Bradbury.\n\nAlternative, is to have a system that would decide to batch things for us depending on exact inputs / outputs we want to compute. Inspired by method described in paper by Moshe et al. \u201cDeep Learning with Dynamic Computation Graphs\u201d [implemented in TensorFlow Fold but seems to be not maintained], very well depicted in this animation:\n\nI have implemented this principles in a simple TorchFold class, with next interface:\n\nNow, if we want to encode tree with a TreeLSTM / Model from previous gist, here\u2019s how we will need to change the code:\n\nHere, at every invocation of encode_tree_folded, the Fold graph is dynamically constructed by adding nodes via fold.add, where op is the name of the function in model to be called. It automatically figures which ops can be groups together and which should follow.\n\nThen at fold.apply time, the operations from passed model are called, passing them batched input tensors [possibly with different batch sizes at different steps] and routing outputs automatically to next steps.\n\nComparing speed between unfolded and folded versions (on a simple model here):\n\nGetting 3\u201310x speed up, due to reducing inefficiency in computations.\n\nThis tool is generally useful for any complex architecture [including RNN] as it removes need to think about batching at least for first experiments.\n\nYou can find implementation and examples here: https://github.com/nearai/pytorch-tools\n\nPS. While writing this article, I have found a recent article on this topic \u2014 https://arxiv.org/pdf/1705.07860.pdf with implementation for DyNet.\n\nPSS. Since upgrading to PyTorch 0.2.0 I saw a slight degradation in performance of TorchFold, so for best speed try running with 0.1.12 until it\u2019s fixed."
    },
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-sequence-to-sequence-3d9d2e238084?source=user_profile---------3----------------",
        "title": "TensorFlow \u2014 Sequence to Sequence \u2013 Illia Polosukhin \u2013",
        "text": "Today I want to show an example of Sequence to Sequence model with all the latest TensorFlow APIs [as of TF 1.3].\n\nSeq2Seq models are very useful when both your input and output have some structure or time component. Most popular applications are all in the language domain, but one can use it to process time series, trees, and many other intrinsically structured data.\n\nTranslation has been domain where this models advanced the most, as it has a large enough dataset to train large and complicated models and provides a clear value from advancing state-of-the-art.\n\nIf you haven\u2019t seen, here are few papers on Neural Language Translation with Seq2Seqs: https://arxiv.org/abs/1409.3215, https://arxiv.org/abs/1609.08144, https://research.googleblog.com/2016/09/a-neural-network-for-machine.html\n\nSeq2Seq model is separated into two components: Encoder and Decoder.\n\nEncoder is structured similar to Text Classification model, it reads token by token input sequence using RNN cell. Internal state of the RNN encodes model\u2019s understanding of the sequence.\n\nAfter input sequence is finished (\u201c<DONE>\u201d token in used to indicate that to the model), Decoder starts processing: producing output tokens one by one. Now there are number of different ways to implement decoder. Two most common: plain RNN decoder and Decoder with Attention.\n\nPlain RNN decoder would just take output of the Encoder step and on each RNN step, taking previous [correct or decided by the model] token and hidden state of RNN to produce next token.\n\nAttention decoder doesn\u2019t just take hidden state of RNN and previous token but also uses hidden state of the decoder RNN to \u201cattend\u201d \u2014 select information from encoder output states. This produces alignment between each output token and some set of input tokens.\n\nAttention mechanics is very important concept in deep learning, so if you are not familiar, you may want to read: http://arxiv.org/abs/1409.0473\n\nAttention is so powerful, that model only with attention can actually outperform Seq2Seq models at Language Translation: https://arxiv.org/pdf/1706.03762.pdf [shameless plug ;) ].\n\nI also want to mention Tensor2Tensor project recently open sourced by Lucasz Kariser: https://github.com/tensorflow/tensor2tensor. It contains large library of battle-tested and tuned models [Seq2Seq and Transformers included] as well as input readers for Machine Translation and other datasets.\n\nAlright, let\u2019s look at the code for this in TensorFlow:\n\nThis is a lot of code, but main ideas here:\n\nYou can find latest version of the code with sample data generator here: https://github.com/ilblackdragon/tf_examples/tree/master/seq2seq"
    },
    {
        "url": "https://machinelearnings.co/tensorflow-text-classification-615198df9231?source=user_profile---------4----------------",
        "title": "TensorFlow \u2014 Text Classification \u2013",
        "text": "On Nov 9, it\u2019s been an official 1 year since TensorFlow released. Looking back there has been a lot of progress done towards making TensorFlow the most used machine learning framework.\n\nAnd as this milestone passed, I realized that still haven\u2019t published long promised blog about text classification. Even though examples has been there in TensorFlow repository, they didn\u2019t have very good description.\n\nText classification is one of the most important parts of machine learning, as most of people\u2019s communication is done via text. We write blog articles, email, tweet, leave notes and comments. All this information is there but is really hard to use compared to a form or data collected from some sensor.\n\nThere been classic NLP techniques dealing with this, by mostly using words as symbols and running linear models. This techniques worked but were very brittle. Recent adoption of embeddings and deep learning opened up a new ways of handling text.\n\nDifference between words as symbols and words as embeddings is similar to described in Part 3 of tutorial \u2014 among other things, allowing to compress similar categories (words) into a smaller space, thus allowing next layers of neural network using this similarity to do job better.\n\nNow, simplest model that everybody should start solving their problem with (or baseline in ML community) is a bag-of-words model. Something that takes words independent of their order and uses it to predict your goal.\n\nFor example, we will take a DBPedia dataset described in this paper. The dataset contains first paragraph of the wikipedia page for ~0.5M entities and the label is on of 15 categories (like People, Company, etc). This is usually called \u201cTopic classification\u201d and can be used in variety of cases, from analyzing comments on your website to sorting incoming emails.\n\nNote, that exactly same techniques would work for sentiment analysis (categorizing if the text is positive or negative sentiment) and even for Question Answering.\n\nFull example can be found in TensorFlow examples: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification.py (note, that code there will be updated with new APIs so it\u2019s better to check out there).\n\nFirst, we need to retrieve and prepare data:\n\nTensorFlow has a handy learn.datasets module that contains few of example datasets, like DBPedia. By accessing it, it will download it and load it in memory. Note, load_dataset has a size argument, that by default for DBPedia loads a small subset. To load full dataset, pass an empty string.\n\nGoing from sentences (strings) to matrices (what TensorFlow or any ML can work with), requires to find all words in the text and remap them into IDs \u2014 a number per each unique word. This is exactly the same as for categorical variables in previous section of this tutorial, but now instead of one value per example, we get a list of values per each word in sentence. For example \u201cmy work is cool!\u201d would map into [23, 500, 5, 1402, 17] (where 17 is \u201c!\u201d).\n\nWe also want to make sure that each sentence is the same length, so we provide MAX_DOCUMENT_LENGTH to identify how long each sentence will be (longer sentences will be truncated, and shorter ones padded).\n\nNow resulting x_train and x_test contain just a matrices that we can pass to our learning algorithm.\n\nWe create a simple TensorFlow model function, that takes features (list of word IDs) and target (one of 15 classes). We use simple bow_encoder which combines creation of embedding matrix, lookup for each ID in the input and then averaging them. Then we just add a fully connected layer on top and the use it to compute loss and classification results tf.argmax(logits, 1). Adding training regime (Adam with 0.01 learning rate) and that\u2019s our function.\n\nNow by simply invoking it with training data we prepared we can see how well bag of words work for this problem:\n\nNote, you can play with training steps and training regime (different learning rate and other parameters optimize_loss has).\n\nBut as we all know the bag of words is not really modeling how languages work \u2014 order of words matter (even though less then you would think in practice) and we want to handle that as well.\n\nThere are few ways how one can do \u2014 add bi-grams, use convolution to learn n-grams over text or Recurrent Neural Network to handle long term dependencies in text. For various problems any of this methods can work better. You can see examples of all of them implemented here (including characters): https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/learn#text-classification\n\nIn this post let\u2019s review the Recurrent Neural Network implementation:\n\nHopefully comments inlined with code give a good description what is done on each step. As you can see the code is not very different from bag of words model, replacing just \u201cencoding\u201d part with rnn function call.\n\nThe same Estimator call with different model function will allow to run this model on the data and see improvements from understanding the sequence in which words appear.\n\nYou now know how to apply some of the basic architectures for text / document classification. Other things to consider is to to load pre-trained embeddings (like GloVe) and doing semi-supervised training, which allows model spend more time training for your problem instead of learning about language from scratch. I\u2019ll try to talk about this in some of the next posts.\n\nAdditionally, I\u2019ll talk more about how to make this models to converge / perform better with some of the tricks implemented in optimize_loss and tf.layers."
    },
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-tutorial-part-4-958c29c717a0?source=user_profile---------5----------------",
        "title": "TensorFlow: Combining Categorical and Continuous Variables",
        "text": "I originally planned to go over some examples in TF.Learn for natural language problems, but somehow work on TF.Learn itself got me busy.\n\nIn previous Part 3 of this tutorial we have reviewed how to add categorical variables to your model.\n\nBut the other day, @philbort filed a bug https://github.com/ilblackdragon/tf_examples/issues/7 and asked how to combine continues and categorical variables in one model.\n\nAfter trying to do it quickly in response, I realized it\u2019s quiet hard. Now, we are going to fix some of that in upcoming changes to TensorFlow, but in a meanwhile I want a way to put together various pieces of TF.Learn to achieve the goal and also talk about some new concepts.\n\nLet\u2019s start with input function \u2014 function you can pass to your fit / predict as alternative to x and y data arrays. Idea here is that you want build piece of the graph that would read and sample your data instead of keeping it always in memory. For example if you have a csv file, you can write an input function like this:\n\nWe use read_batch_examples to setup a reader (TextLineReader) that would read lines from my.csv and batch them into a string tensor of [32]. Then we call decode_csv, which parses each string in tensor into list of columns. We define number and dtypes of this tensors by providing record_defaults. Finally we return features (string to tensor) and target tensor.\n\nAdditional things to know about input functions \u2014 is that depending on flags to read_batch_examples this may return data infinitely (as long as we are asking for it) or for specific number of epochs. And it randomizes order by default, to disable it pass randomize_input=False.\n\nNow as you see features would contain both continues features like Fare and Age as well as string features like Sex and Pclass. There are different ways to handle this, for example using tf.contrib.lookup.HashTable to build a lookup table right in the graph.\n\nHere I want to talk about alternative path \u2014 doing everything with Pandas and then passing already preprocessed data into the model. This is less scalable (e.g. won\u2019t work in distributed environment very well), but works for local training.\n\nCurrently (2016/10/27) there is a limitation if x is DataFrame, what will model receive (due to legacy reasons, it translates it into a matrix).\n\nTo work around it, we will write an input function that would feed preprocessed DataFrame in correct format. Then we will write a model that can use already mapped categorical variables into indices together with continues variables.\n\nHere we write pandas_input_fn that uses learn.dataframe.queues.feeding_functions.enqueue_data to feed DataFrame into the model (e.g. adds nodes in the graph that in parallel are fed with data) in separate thread. This also should work faster then passing x, y into fit because it doesn\u2019t lock training loop to fetch new records.\n\nNow in our model function, we use process features: continues ones are mapped to float and reshaped into [batch_size, 1]. Categorical features are all embedded using different embedding matrices (see Part 2 for more details about embeddings). Then all this features are concatenated into one feature vector and passed into deep 3-layers neural network. The later part is the same as in previous models.\n\nThe final results are better the either just categorical or just continues variables, getting after a bit of training:\n\nAs always, you can find all code on github: https://github.com/ilblackdragon/tf_examples. Feel free to create an issue or file a pull request!\n\nHopefully, this time I\u2019ll be able to write a bit more about text classification. Stay tuned!"
    },
    {
        "url": "https://medium.com/@ilblackdragon/errors-for-humans-7d89e53eeebe?source=user_profile---------6----------------",
        "title": "Errors for Humans \u2013 Illia Polosukhin \u2013",
        "text": "In recent months I\u2019ve been working on TF.Learn \u2014 API that provides easy model building and training capabilities on top TensorFlow. This was extending and merging work I\u2019ve done before on SKFlow.\n\nAs part of this effort, I was facing the question of error checking of user code, inputs, etc. It\u2019s easy to write library that does something based on inputs you are thinking about, it\u2019s a lot harder to predict what inputs user will pass in.\n\nNow, I also use the TF.Learn for work as a Researcher and experience pain of using the library and getting un-helpful messages when you accidentally passed a wrong input or used something incorrectly.\n\nGoing even more into this, Python (and other programming languages) itself has a lot of unhelpful errors as well. To give you few examples:\n\nIn case of your own code, you usually can guess what this means from the invocation, but when calling library or colleague\u2019s code \u2014 it becomes an investigation with debugger and/or logging.\n\nI want to formulate principle of \u201cError For Humans\u201d that would define where things should be checked and how errors should be reported:\n\nThe whole point of the error reporting is to signal about difference in expectations that author of the code had and how they are violated given current input (and in some cases state).\n\nThis means that error should be structured:\n\nFor example, if you are checking if file(s) exist, instead of just reporting \u201cCheckpoint not found\u201d or even \u201cCheckpoint not found in %s\u201d, it can be \u201cExpected checkpoint files in %s, found no checkpoint file: %s\u201d % (dir, os.ListDir(dir)).\n\nSame can go for function calls: \u201cFunction q() expects arguments a, b, c: received a=2, b=3 and not c.\u201d\n\nNot checking errors on inputs in the user facing functions leads to leaking errors from the implementation of your code. Which is a \u201cLeaky Abstraction\u201d that requires user to understand your code to figure out which input was passed / transformed to the place that lead to exception.\n\nExample: \u201cNoneType object has no attribute rsplit\u201d when user calls object.save(path), which will lead user to a which hunt in the code of the save to and underlaying code to only figure out that path is None.\n\nThis principle suggests to add a simple error check right in save function that would report: \u201cSave expected path to be string, received: None\u201d.\n\nState of the object is managed by the object itself (or APIs around it), which means that author is responsible for communicating how object should be used and in case of wrong usage \u2014 the best way is to refer to that documentation.\n\nUsually some objects have a special way to be used (for example start() or stop() method should be called at some point). Usually this results in errors due to state not initialized / changed. Report errors like:\n\n\u201cObject %s is not initialized via start() method before calling score(). Refer to ../../readme.md for detailed usage.\u201d\n\nHopefully, this principles will help writing better more useful libraries.\n\nLet me know if you have more principles to uphold when reporting #ErrorsForHumans."
    },
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-tutorial-part-3-c5fc0662bc08?source=user_profile---------7----------------",
        "title": "TensorFlow Tutorial \u2014 Part 3 \u2013 Illia Polosukhin \u2013",
        "text": "In the previous Part 1 and Part 2 of this tutorial, I introduced a bit of TensorFlow and Scikit Flow and showed how to build various models on Titanic dataset.\n\nIn this part, let\u2019s make a more complex model, something that can handle categorical variables.\n\nUsually in machine learning, handling of categorical variables requires creating a one-hot vector for each category. In deep learning, there is an alternative solution for that \u2014 distributed representations or embeddings.\n\nUsing embeddings, you can represent each category as a vector of floats of the desired size, which can be used as features for the rest of the model. Note, that because of the fully differentiable nature of the TensorFlow components (and other Deep Learning frameworks), this allows to \u201ctrain\u201d the most optimal representation for your task. This has shown been the most powerful tool in the Deep Learning toolkit as it removes need to do manual feature engineering.\n\nThis brings most value when you have a lot of categories or discrete sparse values \u2014 e.g. hundred and thousands. Then you get compression of the input from N categories to fixed size embedding.\n\nWhen you have a small number of categories it still works by using the embedding space to model one-hot vectors per category (e.g. by spreading categories around without any semantic).\n\nLet\u2019s continue with our Titanic dataset and try a simple example of using just Embarked field as categorical variable for prediction:\n\nFirst, we select only \u201cEmbarked\u201d column, for as our features. We then follow regular 20% train/test split.\n\nIt\u2019s always useful to analyze what kinds of values features have:\n\nThis is passed to CategoricalProcessor \u2014 a helper class that maps categorical variables to ids. In this case it will create a vocabulary of S->1, C->2, Q->3 and unknonw/nan -> 0 and remap this column to integers.\n\nThe final model is simple, it leverages another helper function skflow.ops.categorical_variable which creates an embedding matrix of size n_classes by embedding_size and looks up ids from input in it. This is a similar to skflow.ops.one_hot_matrix but instead returning a learnable distributed representations for given categories.\n\nFinally train model and predict on a test dataset and voila, we got a model using distributed representations for categorical variables.\n\nAfter using embeddings, there is a simple model to compare with one-hot vector representation. It will map from class 1, 2, 3 into a vector with one at the position of the class and zero everywhere else. E.g. class C (2) will be mapped into [0, 0, 1, 0] vector.\n\nIn this case, given only one feature with 3 classes for prediction, results end up been exactly the same as using embeddings.\n\nYou can learn how to combine categorical and continues values in Part 4.\n\nAdditionally, as I mentioned above, the most value in using distributed representation coming from categorical features with large number of classes. In the Part 5 you can use this method of representing categorical variables for Natural Language Understanding tasks, like Document classification."
    },
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-tutorial-part-2-9ffe47049c92?source=user_profile---------8----------------",
        "title": "Tensorflow Tutorial \u2014 Part 2 \u2013 Illia Polosukhin \u2013",
        "text": "In the previous Part 1 of this tutorial, I introduced a bit of TensorFlow and Scikit Flow and showed how to build a simple logistic regression model on Titanic dataset.\n\nIn this part let\u2019s go deeper and try multi-layer fully connected neural networks, writing your custom model to plug into the Scikit Flow and top it with trying out convolutional networks.\n\nOf course, there is not much point of yet another linear/logistic regression framework. An idea behind TensorFlow (and many other deep learning frameworks) is to be able to connect differentiable parts of the model together and optimize them given the same cost (or loss) function.\n\nScikit Flow already implements a convenient wrapper around TensorFlow API for creating many layers of fully connected units, so it\u2019s simple to start with deep model by just swapping classifier in our previous model to the TensorFlowDNNClassifier and specify hidden units per layer:\n\nThis will create 3 layers of fully connected units with 10, 20 and 10 hidden units respectively, with default Rectified linear unit activations. We will be able to customize this setup in the next part.\n\nI didn\u2019t play much with hyperparameters, but previous DNN model actually yielded worse accuracy then a logistic regression. We can explore if this is due to overfitting on under-fitting in a separate post.\n\nFor the sake of this example, I though want to show how to switch to the custom model where you can have more control.\n\nThis model is very similar to the previous one, but we changed the activation function from a rectified linear unit to a hyperbolic tangent (rectified linear unit and hyperbolic tangent are most popular activation functions for neural networks).\n\nAs you can see, creating a custom model is as easy as writing a function, that takes X and y inputs (which are Tensors) and returns two tensors: predictions and loss. This is where you can start learning TensorFlow APIs to create parts of sub-graph.\n\nWhat kind of TensorFlow tutorial would this be without an example of digit recognition? :)\n\nThis is just an example how you can try different types of datasets and models, not limiting to only floating number features. Here, we take digits dataset and write a custom model:\n\nWe\u2019ve created conv_model function, that given tensor X and y, runs 2D convolutional layer with the most simple max pooling \u2014 just maximum. The result is passed as features to skflow.models.logistic_regression, which handles classification to required number of classes by attaching softmax over classes and computing cross entropy loss.\n\nIt\u2019s easy now to modify this code to add as many layers as you want (some of the state-of-the-art image recognition models are hundred+ layers of convolutions, max pooling, dropout and etc).\n\nThe Part 3 is expanding the model for Titanic dataset with handling categorical variables.\n\nPS. Thanks to Vlad Frolov for helping with missing articles and pointing mistakes in the draft :)"
    },
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-tutorial-part-1-c559c63c0cb1?source=user_profile---------9----------------",
        "title": "TensorFlow Tutorial\u2014 Part 1 \u2013 Illia Polosukhin \u2013",
        "text": "UPD (April 20, 2016): Scikit Flow has been merged into TensorFlow since version 0.8 and now called TensorFlow Learn or tf.learn.\n\nGoogle released a machine learning framework called TensorFlow and it\u2019s taking the world by storm. 10k+ stars on Github, a lot of publicity and general excitement in between AI researchers.\n\nNow, but how you to use it for something regular problem Data Scientist may have? (and if you are AI researcher \u2014 we will build up to interesting problems over time).\n\nA reasonable question, why as a Data Scientist, who already has a number of tools in your toolbox (R, Scikit Learn, etc), you care about yet another framework?\n\nThe answer is two part:\n\nLet\u2019s start with simple example \u2014 take Titanic dataset from Kaggle.\n\nFirst, make sure you have installed TensorFlow and Scikit Learn with few helpful libs, including Scikit Flow that is simplifying a lot of work with TensorFlow:\n\nYou can get dataset and the code from http://github.com/ilblackdragon/tf_examples\n\nQuick look at the data (use iPython or iPython notebook for ease of interactive exploration):\n\nLet\u2019s test how we can predict Survived class, based on float variables in Scikit Learn:\n\nWe separate dataset into features and target, fill in N/A in the data with zeros and build a logistic regression. Predicting on the training data gives us some measure of accuracy (of cause it doesn\u2019t properly evaluate the model quality and test dataset should be used, but for simplicity we will look at train only for now).\n\nCongratulations, you just built your first TensorFlow model!\n\nTF.Learn is a library that wraps a lot of new APIs by TensorFlow with nice and familiar Scikit Learn API.\n\nTensorFlow is all about a building and executing graph. This is a very powerful concept, but it is also cumbersome to start with.\n\nLooking under the hood of TF.Learn, we just used three parts:\n\nEven as you get more familiar with TensorFlow, pieces of Scikit Flow will be useful (like graph_actions and layers and host of other ops and tools). See future posts for examples of handling categorical variables, text and images.\n\nPart 2 \u2014 Deep Neural Networks, Custom TensorFlow models with Scikit Flow and Digit recognition with Convolutional Networks."
    },
    {
        "url": "https://medium.com/@ilblackdragon/story-about-programming-7b21d56f9d35?source=user_profile---------10----------------",
        "title": "Story about Programming \u2013 Illia Polosukhin \u2013",
        "text": "Let me tell you a story about programming.\n\nThis is a story of mental labor that is something people not design to do. Computers are.\n\nWe stare at lines of code and trying to execute the code in our heads. Some of us are the best in the world in such operations and are proud of that. There are competitions where you can show your ability to solve complex problems by decomposing them into computer operations. We do all this because this is the way it has been since the first computer was designed by mathematician. They are usually pretty good at doing such operations and that is obviously affected how computers and programming languages are designed.\n\nWe stare at text files, that have hundreds and hundreds of lines of a language that somebody made up. Quirks of syntax and style guide that gives grief to beginners doesn\u2019t bother us anymore. But the real problem lies in the complexity of the systems. When you start a new project, everything is great \u2014 you can keep the whole system and all the code in your mind. Problems arise when many people work on a rapidly growing system. You spend hours debugging some simple thing, just because some internal assumptions of the function you used were different than yours.\n\nThe core problem is assumptions. They are everywhere. Making assumptions is what we do everyday to write code. Any line of code you write starts with an assumption. Is this data going to be on disk? How many users will access this service at any moment? etc etc\n\nBeginners usually oversimplify assumptions and then pay for it. They didn\u2019t account for many future questions. In result, they will need to hack across the system or spend very long time refactoring to change an assumption.\n\nMedium experienced people start to over-engineer everything. These are beginners who got burned repeatedly on oversimplification. They realized that assumptions change as projects progress. To prevent rewriting and hacking they start building \u201cspaceships\u201d. Software which has configuration file that allows to change it\u2019s functionality from flying into space to drilling to the earth core. Usually such projects takes a lot longer and they many times fail because of time/money/complexity of design.\n\nExperienced programmers find the balance of making right assumptions when they know what will be the need in future year and leave enough flexibility in the places that may change in close future. This comes with better understanding of business needs and reasons why requirements change. The skills to design such systems also come after building over-engineered systems that didn\u2019t deliver value and were abandoned or dismantled.\n\nBut even an experienced programmers can\u2019t predict future and in a year or two their code needs to be rewritten. Why? Because if business grows or shifts, so do the needs for the software. It may be customer requested features that nobody thought about before, regulations, new algorithms, new frameworks, new data became available, better machine learning, acquisition or partnership with other products, just simply not enough design originally to support growth (of traffic or features).\n\nThis leads to weeks and months of refactoring or rebuilding from scratch. Refactoring takes a toll of having working state every time you change something. Rebuilding may never end because either over-engineering kicks in (oh, our design couldn\u2019t handle this, let\u2019s rebuild everything so we can handle anything next time) or just while development was frozen the product competitors overcame with better offering. Sometimes people get tired of working on dying code and they leave, and now you have new people trying to refactor previous solution while not knowing all the details and assumptions that are encoded in millions of lines of code. This leads to degradation of the product and customer churn.\n\nThis happens all the time and everywhere. Any software company big or small have these problems. The code churns all the time, systems get build, refactored and rewritten. Business get born out of hacks, gets traction, hires number of engineers to rewrite it and sometimes dies before they manage to rebuild the first or second version.\n\nIs there a way out of this?\n\nI argue that yes. We need to lift our understanding of software from lines of code to problems it solves. We need to change our perception of engineering from bottoms up, where you need come up with an algorithm that matches current assumptions and only define assumptions and let computer to find appropriate way to handle them. Move from text files to UX-driven development, where UX is aligned with human thought process.\n\nWhat are assumptions that costed you dearly? Did you ever spent weeks or month reworking something that would\u2019ve been fairly straightforward to change earlier in development process?"
    },
    {
        "url": "https://medium.com/@ilblackdragon/augmented-intelligence-cf9a4b9134b0?source=user_profile---------11----------------",
        "title": "Augmented Intelligence \u2013 Illia Polosukhin \u2013",
        "text": "There is a lot of conversations lately in media and even in AI community about future of AI field and possible concerns with \u201csuper-intelligent\u201d AI that may drive humanity into extinction.\n\nThe people\u2019s fear of AI coming from looking at other humans and seeing how they abuse their intelligence to do evil / self-profiting things, without considering community (or in a larger sense \u2014 world) around them. Projecting humans on AI is been something people done in science fiction for many years, and now when computational power started to catch up, it\u2019s becoming threatening for many intelligent people.\n\nOn the other hand, there is a number of AI researchers, who are lot more optimistic about future. Special committee of people gathered to define some way to reassure public and media that they are not overlooking this issues \u2014 this lead to Future of Life institute and their Open Letter. They recognize that AI research in long term should consider possible issues, but actually main talking point of the letter itself is to define short term issues and research possible solutions.\n\nThis brings us to difference between Narrow AI and General AI.\n\nArtificial Narrow Intelligence (ANI) or sometimes Weak AI, is usually defined as an AI that specializes in one specific area. Examples of such a machine are DeepBlue (won against human in chess in 1997), Watson (won against human in Jeopardy in 2011).\n\nArtificial General Intelligence (AGI) or Strong AI, is defined as AI that is as smart as human in all areas and may be self aware.\n\nCurrent state of AI research is mostly specializes on building ANI for tasks with a goal to sometime find a way to build AGI. Even teams (Cog, DeepMind) that announced that they work on AGI, are still work on fairly limiting subspace of problems. One of the problems with building AGI per se, is that it\u2019s hard to define what exactly should it be doing. Originally people were envisioning robots (e.g Azimov) be a medium for intelligence. But computational power is been progressing a lot faster then the field of robotics. And even though there is a lot of progress in robotics as well, people are not anymore expecting AI to be bound to robotic body.\n\nNow, if we look at what currently happening with society getting more connected then ever and processing power been available to crunch tons of data, we see that ANI is becoming part of everybody's life.\n\nWe extend our intelligence by using devices, like smartphones, smart watches and laptops. We can speak with our friends and family over thousands of miles. Many of us could possible work and perform part of their job not been bound to one place. We are using search and question answering systems to find answers we sometimes don\u2019t even fully realize the correct question (Google figures out that for you).\n\nNow, let\u2019s imagine future, where people have even more wearable devices. There is a number of EEG headsets (Emotiv, NeuroSky) hitting the market this year, that will supply more data to process for this ANI systems to figure out your needs and wants. Augmenting reality glasses will bring unprecedented ability to lay over information on top of real world. Technologies like nano-sensors that are in your body can monitor all your vitals.\n\nAI algorithms so far had issues with number of tasks that seem easy for humans. And even though recent developments in Deep Neural Networks have shown some great progress, there was also a lot of interesting research that connected humans and machine via MRI to improve on image / audio recognition. This is just one of the things that symbiosis of human and machine can easily tackle.\n\nFor example, I would expect that if you measure IQ of person who has augmented reality glasses and machinery that scans human brain and does algorithmic tasks while human brain is focused on concepts \u2014 it will be a lot higher then just human alone.\n\nTo conclude, I think AGI is a possibility in some fairly distant future, but the augmentation of human intelligence with smart technologies to the extent of us becoming what futurist of past where writing about AI is happening now."
    }
]