[
    {
        "url": "https://medium.com/randomai/playing-with-loss-functions-in-deep-learning-26faf29c85f?source=---------0",
        "title": "Playing with Loss Functions in Deep Learning \u2013 randomAI \u2013",
        "text": "In this post, we are going to be developing custom loss functions in deep learning applications such as semantic segmentation. We use Python 2.7 and Keras 2.x for implemention.\n\nLoss Functions are at the heart of any learning-based algorithm. We convert the learning problem into an optimization problem, define a loss function and then optimize the algorithm to minimize the loss function.\n\nConsider a semantic segmentation of C objects. This means that there are C objects in the image that need to be segmented. We are given a set of images and corresponding annotations for training and developing the algorithm. For simplicity, let us assume that there are C=3 objects including an ellipse, rectangle and a circle. We can use a simple code such as below to generate some masks with three objects.\n\nTypical ground truth masks for the objects would look like below:\n\nAlso assume that we develop a deep learning model, which predicts the following outputs:\n\nFirst, we are going to use the standard loss function for semantic segmentation, i.e., the categorical cross entropy as written below:\n\nHere C is the number of objects, y_i is the ground truth and p_i is the prediction probability per pixel. Also, y_i is one if the pixel belongs to class i and zero otherwise. Note that i=0 corresponds to the background. The loss will be calculated for all pixels in each image and for all images in the batch. The average of all these values will be a single scalar value reported as the loss value. In case of categorical cross entropy, the ideal loss will be zero!\n\nTo be able to easily debug and compare results, we develop two loss functions, one using Numpy as:\n\nAnd its equivalent using tensor functions of the Keras backends as:\n\nAs you can see, there is not much difference between the two loss functions except using the backend versus numpy. If we try 8 random annotations and predictions, we obtain loss_numpy=0.256108245968 and loss_tensor=0.256108, from the numpy and the tensor functions, respectively. Practically, the same values!\n\nNow we are going to develop our own custom loss function. This customization may be needed due to issues in the quality of data and annotations. Let us see a concrete example.\n\nIn our case study, let us assume that for some reason there is missing ground truth. For instance, in the below figure there is no ground truth for object 3 (circle) while the deep learning model provides a prediction.\n\nIn another example, the ground truth is missing for the first object (ellipse):\n\nIn these scenarios, if we still use the standard loss functions, we may be penalizing the AI model incorrectly. The reason is that the pixels belong to the missing ground truth will be considered as the background and multiplied by -log(p_i), where p_i is the small prediction probability and as a result -log(p_i) is going to be a large number. Note that this is based on our assumption that there should be a ground truth but for whatever reason annotators missed it.\n\nAgain, if we try 8 annotations and predictions this time with two random missing annotations, the standard loss value= 0.493853! Clearly, this shows a higher loss value compare to when all the ground truths were available.\n\nOne easy solution would be to remove the images with missing ground truth. This means that if even one object out of C object has a missing ground truth we have to remove that image from the training data. However, that means less data for training!\n\nInstead, we may be able to develop a smart loss function that avoids such penalization in case of missing ground truth. In this case, we write the loss function as:\n\nwhere w_i is the smart weight. If w_i=1 it will be the same as the standard loss function. We know that if the ground truth is missing for an object, that means that it is assigned as the background. As such if we set w_0=0 for those pixels that are detected as the object without the ground truth, we will remove any contribution of the background in the loss value. In another words, the custom loss function can be written as below:\n\nTo this end, we consider two conditions. First, we find images with missing ground truth. This is possible using:\n\nNext, we find the predicted classes per pixel for all images using:\n\nThen, we check if the predicted output is in fact equal to the missing object. This is also possible using:\n\nNote, in the actual implementation, we use:\n\nsince we want both conditions to be False so that the logical-OR is False.\n\nIf these two conditions are satisfied we set the the background weight equal to zero. This will guarantee that if an object has missing ground truth (in fact mistakenly labeled as background), then the contribution of background in the loss function is zero. The final custom loss function is here:\n\nIf we calculate the loss for 8 annotations and two random missing objects we will get custom_loss= 0.191179. This shows that we do not penalize the AI model for providing a correct output just because the ground truth does not exist. In practice, this technique will lead to a better overall performance for the objects with missing ground truth.\n\nWe can always use the standard loss function and they work fine for most cases. However, if you encounter special cases and would like a better performance, you can customize the loss function based on your needs."
    },
    {
        "url": "https://medium.com/randomai/a-million-dollar-question-in-era-of-deep-learning-f57352c649df?source=---------1",
        "title": "A Million Dollar Question in era of Deep Learning \u2013 randomAI \u2013",
        "text": "Deep learning algorithms, as a new wave of AI, are powerful tools that have impacted many industries within the last few years. We can now do tasks at scale that used to be very difficult or impossible. Apart from the hype, with the current pace of adopting deep learning in various industries, we are going to see many innovative products and services that will eventually make our life easier.\n\nFor many AI researchers, probably, it is not that big a deal to know the amount of data they need to develop AI algorithms. You just go with whatever data already exists. However, if you are dealing with specific industrial problems and need to collect and label your own data, this is a million dollar question: how much data do we need?\n\nFor many of AI applications such as image classification and object detection, it is probably easier and cheaper to collect data. We can scrap images, text and documents from the internet. Even data labeling and annotation, as one of the main bottlenecks in AI development, is not a huge hassle in these applications thanks to mechanical turks and crowed sourcing.\n\nHowever, that is not the case in industries with niche problems and many regulations such healthcare. Everybody agrees that data collection and annotation is very expensive and time consuming in the the healthcare industry. Many factors have to work out: IRB review, protocols, patient selection, site selection, clinicians, etc.\n\nAs such, it is not surprising that companies are eager to know and plan the time and cost of data collection and annotation for AI related projects. That is why project managers would strangle you to get an answer to this million dollar question: How much data do we need to develop and validate this AI-focused project?\n\nAs you know the classical answer is \u201cthe more the better\u201d, an answer that usually companies do not like! :)\n\nBelieve or not, at this point, there is no better answer than \u201cthe more the better\u201d. There are many evidences that deep learning algorithms are data hungry. You would probably have seen this famous plot by Andrew Ng in his talks.\n\nThe plot roughly depicts that the performance of traditional machine learning techniques saturates after certain point. However, large neural networks (aka deep learning) algorithms are able to absorb as much data as you can feed them and become better at their job.\n\nEven for very large datasets, this article provides interesting results! They experimented with very large datasets and they show that the performance on vision tasks increases logarithmically based on volume of training data size.\n\nAnother interesting study shows that neural networks perform well or generalize well in the vicinity of the training data. Intuitively, a direct conclusion is that the more data you train your algorithm with the higher chance any new unseen data would land in the vicinity of the training data manifold and guaranteed to perform well. So let us set this ground rule that MORE DATA IS ALWAYS BETTER (conditionally)!\n\nBefore anything, let me point to common mistakes that usually people make that misled them in their experiments. The first mistake happens when we expand our training dataset by a small portion and expect a linear performance improvement. For instance, consider a case when you build a model on N=100 samples and then deploy it on a test set of N=10 samples and you get accuracy=80%. Now, you happen to collect another extra 10 samples and rebuild your model with N=110 samples and deploy it on the same test set and you get accuracy=79%! Shocking! Not only you did not make any improvement but also the performance dropped a bit. Now you would become a non-believer and loose trust in data without considering the stochastic nature of the training process and other factors such as the random seed point, number of iterations, data augmentation, etc.\n\nAnother common mistake is when we significantly expand our training data but at the same time change the test set. For instance, suddenly you happen to collect N=1000 samples, and since now you have more data, you increase your training data and also decide to adjust your test set as well. In this scenario, you build a model but instead of deploying your model on the same original test set you deploy your model on a different test set and you would again see a performance that will not help you make a correct conclusion.\n\nMoreover, another major mistake is collecting data without paying too much attention to the quality of data and labeling of your data. Another words, it is important to have a clean dataset in both training and test set. Noisy labels in the training data will make it hard for a model to learn. Noisy labels in the test set will make it difficult to objectively evaluate and track the performance of your model.\n\nNow going back to the original discussion, the good news is that we can start developing a neural network/deep learning model with a few examples and we do not have to wait for an infinite data. In fact, what is really important when developing an AI model is that how good you want your model to perform. While we can develop a baby model with a handful of data samples, we do not expect the baby model to perform impressively on the wild data in the world.\n\nTherefore, the main question we need to be answering is how good our model want to be and on what test set? Another words, what is your test set and what is the acceptance criteria for this test set?\n\nBased on these discussions, it is more important to come up with a sizable and clean test set to evaluate your AI system. This problem has been around for many years and there are plenty of research and work on it. It is called sample size selection and it is mainly regardless of the type of algorithms.\n\nPractically speaking, you want to show that your AI algorithm generalizes well across different categories and variations that typically exist in datasets. For instance, if you are building an AI model to work with CT dataset, you need to generalize across different CT devices, patient anatomies, BMI, imaging quality, and so forth. So, by working with statisticians and experts in the field you can come up with the key generalization factors and sample size.\n\nNow you can evaluate your baby model on the test dataset and get a sense of how it is doing. It will tell you where you are and how far you are from the desired performance. If it meets the performance criteria (less likely) that is great! If not, you roughly know how much more data you need. More, the bias-variance analysis can always help you to reach the optimum point. And of course along the way some tweaking and tuning is required to squeeze as much performance as possible from your data. (I am assuming that your model can be reasonably complex and not like crazy 1000-ensembles solutions that people use on Kaggle competitions!).\n\nIn summary, there is no right answer to the amount of data you need for AI algorithms. Instead, try to first establish the right test set and acceptance criteria. Make sure your data is clean. Then, build an initial AI algorithm using a small dataset and estimate the rough amount of training data based on the its performance and your acceptance criteria."
    },
    {
        "url": "https://medium.com/randomai/model-surgery-copy-weights-from-model-to-model-a31b1dec7a7a?source=---------2",
        "title": "Model Surgery: copy weights from model to model \u2013 randomAI \u2013",
        "text": "Consider you have a trained model named model_1 and you want to copy its weights into another model named model_2. This is typical when we want to initialize weights in a deep learning network with the weights from a pre-trained model. Examples are, you have model_1 trained for a task and you want to use its weights to initialize model_2 for another task with different outputs. In this case, typically you want to copy the weights from model_1 to a certain layer. Another example is when model_1 is the base model and model_2 is a time-distributed version of model_1 with a RNN layer at the top layer. In this cases, we want to read the weights from model_1 layer by layer and set them as weights for model_2. The below code does this task.\n\nNote that it is assumed that model_1 and model_2 have the same weights size in the layers before the certain layer.\n\nNow assume that you trained or fine tuned model_2 and want to copy back the fine-tuned weights into model_1. This may have potential applications. For instance, in case of RNN models, you can save back the weights in the base model. The below code does this task."
    },
    {
        "url": "https://medium.com/randomai/another-look-into-over-fitting-33e15b044a5e?source=---------3",
        "title": "Another look into overfitting \u2013 randomAI \u2013",
        "text": "Overfitting is the nightmare of AI and machine learning practitioners. You work hard on your model to achieve a better performance and improve\n\nsee how overfitting can effect your life as a data scientist is by comparing public and private leaderboards of kaggle competitions. An example of such cases can be seen in the below screeenshot from the private leaderboard of a competition that supposedly winner 3 drops by 105 and even more extreme when rank 10 drops by 3165!!\n\nLet us assume that, in the context of supervised learning, we are given training (X_train,Y_train) and test datasets (X_test,Y_test typically unknown to us). For instances, a dataset of images and corresponding labels in case of image classification. Our task is to develop a learning model on the training dataset and then deploy it on the test dataset and achieve the best performance possible.\n\nThis is the foremost type of overfitting that can happen easily. We convert the learning problem into an optimization problem and define a loss function. We use an optimization algorithm in a loop and minimize the cost function and update the weights iteration by iteration. This is the first loop that we create.\n\nAs expected, the loss value gradually decreases. However, if we blindly continue to train the model, the model overfits to the training data!\n\nTo monitor and avoid overfitting to training data, it is common practice to separate a portion (typically 10%-20%) of data as validation data (X_val,Y_val). By monitoring the performance of the model as it is trained in each iteration on the local validation, we can see that the validation loss will get worse at some point (overfitting point). This is the point where usually we either stop training or reduce the learning rate and continue training until reaching a plateau.\n\nCongratulations! Our first baby model, which thank to validation data is also not an overfitted model, is ready for deployment. We deploy it on the test dataset and get a descent performance probably close to the validation score. Time to rest up! But NO, what to do with the temptation to improve our model? :)\n\nWhat it comes next is more interesting. After building the first baby model we typically, in the hope to get a better performance, perform hyper-parameter optimization. What that means is that we play around with the model architecture, add and remove layers, regularization, change learning rate, and sometime become creative and come up with strange techniques! And unknowingly we close another loop and over-fit to the validation data.\n\nThis is the point where we achieve an impressive performance on the validation data however when we deploy the model on the test dataset we realize that the model performs poorly. It has been argued that by closing the second loop, we leak information from the validation data to the training process and that is why the model performs much better on the validation data than the test dataset.\n\nTo avoid this type of overfitting, it is advised to perform K-fold cross validation. To this end, we randomly partition the training dataset into K folds, train a model on (K-1) folds and validate it on the remaining fold. We repeat this process for all folds creating K models. We may observe that some models perform better than the others. The average score across all models is considered the final performance of the model. In other words, when we report/decide on the performance of the model based on the local validation data, we do not pick the best validation score but take average score. The intuition behind cross-validation is that it is technically harder to overfit to the whole dataset than overfitting to the 10%-20% of the dataset.\n\nCongratulations again! The model you pick using K-fold cross-validation is likely to suffer less from the overfitting problem or does it?\n\nOverfitting to test data is another type of overfitting that mostly happen when your test data is small or only a small portion is public, for instance the public leaderboards of Kaggle competitions. Eventhough we perform K-fold cross-validation, we are going to deploy the final model on the test data anyway. One thing that may be ignored is the creation of another outer loop by continuous testing and extra tunning of your model solely based on the result we get from the small test data.\n\nThe remedy to this type of overfitting is to mainly trust your K-fold cross validation results and test less often. This way, you leak less information from the test data to training and will not contribute to the most shameful fault of a data scientist! On top of this, the classical techniques such as data augmentation and regularization should always help with reducing the overfitting problem at no danger.\n\nIt is very easy to overfit. The art is not to overfit! To summarize:"
    },
    {
        "url": "https://medium.com/randomai/gan-for-medical-imaging-generating-images-and-annotations-8ad7c778809c?source=---------4",
        "title": "GAN for Medical Imaging: Generating Images and Annotations",
        "text": "In this post we are going to show a way of using generative adversarial networks (GANs) to simultaneously generate medical images and corresponding annotations. We use cardiac MR images for the experiment. For model development, we use Keras with theano backend.\n\nAutomatic organ detection and segmentation have a huge role in medical imaging applications. For instance in cardiac analysis, automatic segmentation of the heart chambers is used for cardiac volume and ejection fraction calculation. One main challenge in this field is the lack of data and annotations. Specifically, medical imaging annotations have to be performed by clinical experts, which is costly and time-consuming. In this work, we introduce a method for simultaneous generation of data and annotations using GANs. Considering the scarcity of data and annotations in medical imaging applications, the generated data and annotations using our method can be used for developing data-hungry deep learning algorithms.\n\nWe used the MICCAI 2012 RV segmentation challenge dataset. TrainingSet, including 16 patients with images and expert annotations, was used to develop the algorithm. We convert the annotations to binary masks with the same size as images. The original images/masks dimensions are 216 by 256. For tractable training, we downsampled the images/masks to 32 by 32. A sample image and corresponding annotation of the right ventricle (RV) of the heart is shown below.\n\nWe use a classic GAN network with two blocks:\n\nHere mask refers to a binary mask corresponding to the annotation.\n\nThe block diagram of the network is shown below.\n\nTo train the algorithm we follow these steps:\n\nIt is noted that, initially, the generated images and masks are practically garbage. As training continuous they will become more meaningful. Some sample generated images and masks are depicted below.\n\nThis was mainly a prototype of a proof of concept. Clearly, the idea could be expanded to generating a higher resolution data.\n\nThe code is shared in this jupyter notebook"
    },
    {
        "url": "https://medium.com/randomai/setup-guide-for-the-ai-workstations-fa69e221c449?source=---------5",
        "title": "System setup for the AI Workstations \u2013 randomAI \u2013",
        "text": "This post provides instructions to install Ubunt 16.04 with NVIDIA GPU support for the workstations we built in the previous post.\n\nUbuntu 16.04 can be downloaded from here. The .iso download file can be burnt on a USB flash drive as a bootable disk. If you have another machine with Ubuntu, you can use Startup Disk Creator application to write an .iso file into a USB stick.\n\nOnce ready, boot from the USB stick by going to the system setup and selecting the corresponding USB drive as the boot-up drive. Typically, by pressing F2, you can go to the system BIOS setup.\n\nIf booted, you will see two options: Try Ubuntu without installing, Install Ubuntu. I usually, first choose \u201cTry Ubuntu without installing\u201d, to check the memory, hard drives, delete partitions or format disks. Then, I reboot, and choose install Ubuntu. Ubuntu installation is pretty straight forward. On this page, we select both check boxes.\n\nNow it is time to install NVIDA driver, CUDA toolkit, cuDNN and other software tools. Make sure that you download NVIDIA driver, NVIDIA CUDA toolkit and cuDNN tools before moving to next steps. Nvidia Driver can be downloaded from here. For TITAN X and Ubuntu 16, the following configuration works:\n\nNote, if we choose Ubuntu 16.04 for \u201cOperating System\u201d, it does not find any tools.\n\nBefore moving on, enable SSH so that we can use the secure shell service later.\n\nDo not forget to find and write down the IP address of your machine for SSH connection (ex. 192.168.0.5). You can also use Angry IP Scanner to find IP addresses of machines connected to a network. Also, download PUTTY on another laptop/computer and enter the IP address into putty and make sure that the connection works fine. The SSH connection will save your time if during NVIDIA driver installation something happens and we loose access to the system. Not surprisingly, this is very likely to happen!\n\nThis is the simplest way to install NVIDIA driver. Simply find Additional Drivers in the Ubuntu search bar. It may look like below. On fresh Ubuntu, \u201cUsing X.Org X Server-Nouveau display driver\u201d is selected by default. You can select \u201cUsing NVIDIA binary driver-version xxx.xxx from nvidia-xxx (propriety, tested)\u201d and click \u201cApply Changes\u201d. This will install automatically install the driver!\n\nAfter installing NVIDIA driver, reboot Ubuntu and verify driver in \u201cNVIDIA X Server Settings\u201d.\n\nIf you just go ahead and install NVIDIA driver, you will end up with an error regarding Nouveau kernel driver. So, we are going to first disable Nouveau driver. I followed the below steps and it worked perfectly.\n\n1- remove all nvidia packages ,skip this if your system is fresh installed\n\nIf vim is not installed, try $ apt-get install vim. We can also use nano instead of vim for easier interface!\n\n4- Disable the Kernel nouveau by typing the following commands( may not exist,it is ok):\n\n5- build the new kernel by:\n\nNow, we SSH to the system using Putty and enter login and password to get access.\n\nNVIDIA toolkit can be downloaded from here. We first downloaded CUDA 9.1 however it did not work properly with Theano 1.x so we settled on CUDA 8.0 instead.\n\nThe following configuration worked for us.\n\nSimply choose default settings except say NO to \u201cinstall NVIDIA driver\u201d otherwise it will mess things up!\n\nAt the end, you will get a summary such as below.\n\nDon\u2019t forget to add the following lines to either .profile or .bashrc files of your system as also suggested in the summary. Note, you can use \u201ccuda\u201d instead of \u201ccudax.x\u201d.\n\nWe settled on cuDNN 6.0 that works with CUDA 8.0.\n\nYou can verify CUDA installations by $ which nvcc or $ nvcc \u2014 version\n\nNext, you can move with insrtalling Anaconda, Keras, Theano which are straightforward. For theano 1.x, the backend has changed so you need to configure \u201c.theanorc\u201d accordingly (theano webpage)."
    },
    {
        "url": "https://medium.com/randomai/a-summary-of-deep-reinforcement-learning-rl-bootcamp-lecture-5-7bbcfdff46a2?source=---------6",
        "title": "A summary of Deep Reinforcement Learning (RL) Bootcamp: Lecture 5",
        "text": "This post is a summary of Lecture 5 of Deep RL Bootcamp 2017 at UC Berkely. All of the figures, equations and text are taken from the lecture slides and videos available here.\n\nThis lecture is focused on more advanced policy gradient methods. Previously, vanilla policy gradient was presented in Lecture 4b. In vanilla policy gradient, we estimate the policy gradient and plug it into stochastic gradient descent as an optimization tool. However, there is two limitations with vanilla policy gradient methods:\n\nAs we know in supervised learning we convert the learning problem to a numerical optimization problem. Then use some captured data to minimize the training error, which also leads to minimizing the test error. Similarly, in reinforcement learning, we would like to use all data captured so far to compute the best policy. However, writing an optimization problem is not clear in reinforcement learning. Q-learning in principle uses all transitions (data) seen so far although it does not directly optimize the policy. On the other hand, vanilla policy gradient methods use stochastic gradient descent to optimize the policy but do not use all data available. The goal of this lecture is to write down an optimization problem that allows us to do a small update to policy \u03c0 based on data sampled from \u03c0 (on-policy data).\n\nAs in vanilla policy gradient, we can optimize the following loss function:\n\nHowever, you cannot do many optimization steps on the the above loss function because the advantage estimate is noisy and optimizing the loss function too far may radically worsen the policy.\n\nEquivalently, we can use the following loss function:\n\nIn practice Eq. 2 is not much different than the log-prob (Eq. 1) version for reasonably small policy changes.\n\nA solution to this is to define a trust region update such as\n\nIn this case, a local approximation of the function exists which is locally accurate but it gets inaccurate if we go far away from the starting point. We have a trust region that we trust our local approximation and as long as we are in the trust region we are willing to optimize our local approximation.\n\nThe pseudo-code for constrained trust region policy optimization algorithm is outlined below.\n\nThe constrained problem can be solved using conjugate gradients.\n\nIt is also noted that a penalized KL-divergent can be used instead of constrained optimization as:\n\nThe solution for this corresponds to natural gradient step under linear quadratic approximation.\n\nThere is connections between trust region policy optimization (TRPO) and other methods as listed below. For instance, with no constraint, TRPO will be similar to policy iteration. Also, if we use euclidean penalty instead of KL, TRPO becomes like vanilla policy gradient.\n\nThe limitations of TRPO are listed below:\n\nAs alternative to TRPO, there are other algorithms such as the Kronecker-factored approximate Fisher (KFAC), Combined A2C with KFAC Natural Gradient ACKTR and Proximal Policy Optimization.\n\nIn this lecture we found out that vanilla policy gradient has some limitations in step size and sample efficiency. To this end, trust region policy optimization (TRPO) algorithm was introduced."
    },
    {
        "url": "https://medium.com/randomai/building-workstations-for-ai-experiments-38a5a4087bdc?source=---------7",
        "title": "Building Workstations for AI Experiments \u2013 randomAI \u2013",
        "text": "We decided to expand our AI workstations so that we can run more AI experiments. Initially, we wanted to buy from Lambda Lab. But then, we thought it is more fun to build them in house!\n\nWe used PCPartPicker to pick and check the compatibility of the parts. Then, we ordered the parts mostly from Amazon. All parts are piled up here! We got two cases, two TITAN V GPUs, three 1080 Ti GPUs, two 1500 W power supplies, and plenty of memory and SSD storage.\n\nThe case is: Corsair Carbide Series Air 540 Chassis (with upgraded front fans). Here, we open it up so that we can start installing the power supply.\n\nGrounding the case using an earthing mat can eliminate electric shocks and possible damages!\n\nAX1500i Digital ATX Power Supply is a 1500 W power supply enough to support up to four GPUs.\n\nThe motherboard has four PCIe slot to support up to four GPUs.\n\nThe I/O shield is included in the motherboard box. Don\u2019t forget to install it on the case.\n\nCheck the correct pin out and put the CPU on the motherboard. Use the handles to lock the CPU in its place.\n\nInstalling the CPU cooler was a little tricky. Since it comes with an attached radiator, we had to remove the pre-installed rear fan from the case to be able to install the radiator.\n\nWhen installing the radiator section on the case, make sure the tubes are on the correct side.\n\n128 GB of memory are installed onto 8 slots of the motherboard.\n\nAnd now we are ready to put the motherboard in the case.\n\nAfter that we install the cooler on the CPU.\n\nIt is time to connect the data and power cables.\n\nWe are ready to boot up the system and check the setup to see if everything is normal.\n\nIt took us around 2 hours to build the first system. Learning from the first system, the second system was built very quickly.\n\nBuilding workstations is FUN! You can continue with system setup in the next post."
    },
    {
        "url": "https://medium.com/randomai/ensemble-and-store-models-in-keras-2-x-b881a6d7693f?source=---------8",
        "title": "Ensemble and Store Models in Keras 2.x \u2013 randomAI \u2013",
        "text": "Ensembling multiple models is a powerful technique to boost the performance of machine learning systems. One simple way of ensembling deep learning models in Keras is to load individual models, perform prediction using \u201cmodel.predict\u201d for each model and then average the predictions. However, we would like to build an ensemble model and store it as a single model so that we can later deploy it easier. This post provids a method to do so when you define models, train them, load pre-trained weights into models and then ensemble them.\n\nHere, we assume that we have multiple trained models saved as \u201cmodelx.hdf5\u201d without access to model definitions and model weights and we want to ensemble these models and save it as \u201cmodelEns.hdf5\u201d.\n\nFirst, we load individual models and stacked them into a list:\n\nAs you can see, we make sure that models have different names by changing their name. If you are sure that the models have different names, then you can comment out that line.\n\nWe define a function to ensemble models as\n\nThen, we can do:\n\nNote that model input should be a tensor with the same shape as input data. For example, in case of image inputs with channel first format: (1,96,128).\n\nThe ensemble model is ready to be saved as a new model:\n\nYou can load it and get the model summary or prediction:"
    },
    {
        "url": "https://medium.com/randomai/a-summary-of-deep-reinforcement-learning-rl-bootcamp-lectures-4a-and-4b-68fb9609bc07?source=---------9",
        "title": "A summary of Deep Reinforcement Learning (RL) Bootcamp: Lectures 4a and 4b",
        "text": "This post is a summary of Lectures 4a and 4b of Deep RL Bootcamp 2017 at UC Berkely. All of the figures, equations and text are taken from the lecture slides and videos available here.\n\nIn previous lectures we learned about dynamic programming-based methods such as value iteration, policy iteration and Q-learning. In this lecture, we learn about methods that are based on policy optimization.\n\nIn policy optimization, the agent directly learns the optimal policy to be able to act in the environment as opposed to indirect policy learning in dynamic programming-based methods. As an example, a control policy can be represented by parameterized function such as a neural network mapping states s to actions u as depicted below.\n\nThe goal remains the same, we want to maximize the expected reward under policy \u03c0. Here, policy \u03c0(u|s) is stochastic and provides a probability of action u in state s. Working with a stochastic policy class simplifies the problem as it gives us the opportunity to apply gradient descent for optimization.\n\nThere are several reasons that we use policy optimization.\n\nAs mentioned in Equation 1 above, the goal is to find parameter \u03b8 such that the expected reward is maximized. To find the unknown parameter \u03b8, we can take the derivative of\n\nwhere, \u03c4 is a state-action sequence s_0, u_0,\u2026, s_H, u_H. After some approximations (we skip the math), a new concept called the likelihood ratio gradient is obtained as:\n\nEquation 3 can be further decomposed into two parts based on states and actions. By using the fact that the state part is independent of \u03b8, the likelihood ratio can be even simplified to obtain an unbiased estimation of gradient without access to a dynamics model.\n\nWhile the above estimation is an unbiased estimate, it is noisy and requires impractical number of samples to obtain a good estimate. To go around these issues, a set of tricks including baseline and temporal structure are introduced.\n\nIn the first trick, a baseline b is subtracted from R(.) in Eq. 3 as shown below while the estimator is still unbiased.\n\nIt can be shown that this modification will lead to a lower variance. Different choices exist for the baseline as listed below.\n\nFinally, the vanilla policy gradient is given as:\n\nMoreover, discounting and function approximation can be employed for variance reduction of estimation which lead to actor-critic algorithms such as Async Advantage Actor Critic (A3C) and Generalized Advantage Estimation (GAE).\n\nComparison of A3C method with other methods for different games published in [Mnih et al, ICML 2016] is shown below.\n\nThis lecture, presented by Andrej Karpathy, is mostly focused on practical implementation of the Pong game using policy gradient methods. The presenter has previously published a blog post on this topic however he provided new practical insights in this talk.\n\nThis is a simple RL task to play with. A simple neural network is used to learn the policy. Arrays of 80 by 80 screen pixels are fed to a neural network with one hidden layer (200 nodes) and the probability of moving up is provided at the output. Such network has around 1.3 M parameters.\n\nThe policy network can be simply implemented using a few lines of python code as shown below.\n\nObviously, one inefficient way of solving this problem would be a brute force sampling over 1.3 M numbers, trying to find the best policy, which would take forever. Also, if we happened to have the labels for all actions, we could perform a supervised learning approach, which is not the case here! On the other hand, we can follow the RL paradigm to solve this problem: \u201cTry a bunch of stuff and see what happens. Do more of the stuff that worked in the future.\u201d\n\nThe idea is to collect many rollouts of the game. Some may win (good actions) and some may lose (bad actions). This way, we create a fake dataset (set of actions) with corresponding +1/-1 labels.\n\nNow that we have a dataset with labels, we can treat it like a supervised learning problem and optimize the below loss function.\n\nWhen discounting, we blame each action assuming that its effects have exponentially decaying impact into the future as nicely shown in the below figure.\n\nThe python code was also provided and described at the end.\n\nThis lecture discussed policy optimization methods to directly find the optimal policy as opposed to in-direct dynamic-programming methods."
    },
    {
        "url": "https://medium.com/randomai/a-summary-of-deep-reinforcement-learning-rl-bootcamp-lecture-3-23713061159f",
        "title": "A summary of Deep Reinforcement Learning (RL) Bootcamp: Lecture 3",
        "text": "This post is a summary of Lecture 3 of Deep RL Bootcamp 2017 at UC Berkely. All of the figures, equations and text are taken from the lecture slides and videos available here.\n\nIn Lecture 2, it was mentioned to use function fitting to generalize as apposed to storing equation values over all states and actions. Specifically, in case of (tabular) Q-learning, we can learn a parametric Q function as below.\n\nOne way of function fitting was done by classical machine learning techniques together with feature engineering. Now, the natural question would be \u201cDoes it work with a neural network Q function?\u201d And the answer is yes, and of course with some tricks!\n\nTwo main issues with using a Q function neural network are:\n\nTo stabilizing Q learning, two ideas were introduced in Mnih et al. (2015) for developing agents that can play Atari games. The idea was to make Q-learning look like supervised learning. The tricks are:\n\nThe benefit of using a target network with periodical weight updates can be also seen intuitively in an example Atari game. As seen in the figure below, the state of the game changes from S to S\u2019 and some rewards are achieved. However, the two states look very similar to a neural network. If the current parameters are used for computing the target, the network can end up chasing its own tail because of bootstrapping.\n\nThe block diagram of DQN training as well as the algorithm are shown below.\n\nThe network architecture is depicted below. The actual network consists of three convolutional neural network (CNN) layers. A stack of four frames is given to the network as input. There is one output per action.\n\nThe achieved scores by DQN for different Atari games with and without experience replay and target network are listed below. It seems that experience replay has a major contribution in achieving the scores.\n\nIt is interesting to look at the action values for four consecutive frames in a Pong game. As seen, in the beginning, all actions get almost the same value meaning that it does not matter which way to go. However, for later frames that the pong is approaching, the UP action gets a higher value than the other actions.\n\nIn the last part of the lecture, some new improvements on DQN such as Double DQN, prioritized experience replay, Dueling DQN were briefly described. The corresponding papers are listed in References.\n\nIn Lecture 3, Deep Q-learning was described in detail. The bottlenecks of Q function neural networks were outlined. Two solutions i.e., experience replay and target network were given and shown to be effective."
    },
    {
        "url": "https://medium.com/randomai/a-summary-of-deep-reinforcement-learning-rl-bootcamp-lecture-2-c3a15db5934e",
        "title": "A summary of Deep Reinforcement Learning (RL) Bootcamp: Lecture 2",
        "text": "This post is a summary of Lecture 2 of Deep RL Bootcamp 2017 at UC Berkely. All of the figures, equations and text are taken from the lecture slides and videos available here.\n\nIn Lecture 1, the exact methods for finding the optimal policy for a given MDP were introduced. These methods come with some limitations such as:\n\nTo overcome these limitations, sampling-based approximations and function fitting are introduced as follows.\n\nTo move beyond the first limitation, i.e., requirement of transition function, sampling-based approximations are given as a remedy.\n\nFirst, we look at sampling-based approximations for Q-learning. As mentioned in Lecture 1, in Q-learning we compute Q*(s, a) using Q-value iteration defined as:\n\nAs seen, Q-value iteration requires the knowledge of transition function P(s\u2019|s,a), which is not easy to know in many practical cases. Instead, we can sample from the environment and use that to compute Q-value iteration in a running average. This is called (tabular) Q-learning.\n\nTo choose actions, a method called \u025b-Greedy is used where we choose random actions with probability of \u025b, otherwise we choose the action greedily, i.e., the action that maximizes Q_k(s,a). Also, alpha is the learning rate and typically a small value.\n\nQ-learning properties: Q-learning converges to optimal policy even if you are acting sub-optimally (also called off-policy learning).\n\nAn example RL problem that can be solved using (tabular) Q-learning is Crawler that will be covered in Lab 1.\n\nSampling-based approximation is not straightforward to be used for value iteration since it is not clear how to draw samples due to the max operation in the value iteration equation.\n\nAlso, policy iteration has two steps: policy evaluation and policy improvement. Sampling-based approximation can be formulated for policy evaluation similar to tabular Q-learning. However, for policy improvement, again it is not clear due to the max operation in the equation.\n\nIt turns out that we are going to need a huge amount of storage, depending on the RL problem, for tabular methods. For instance, for discrete environments such as Atari breakout, this could be around 10\u00b3\u2070\u2078! Clearly, this is intractable.\n\nIn realistic situations, we cannot possibly learn about every single state! Instead, we need to generalize by learning about some small number of training states from experience and generalize that experience to new, similar situations. For instance, instead of a table we can approximate the Q-function with a parametrized Q-function such as a linear function of features or a complicated neural network. This will be the topic of deep reinforcement learning and will be discussed in more details in future lectures.\n\nIn this lecture, two solutions including sampling-based approximations and function fitting were introduced to resolve the limitations of value iteration and Q-learning. Later, deep RL will be discussed in more detail."
    },
    {
        "url": "https://medium.com/randomai/a-summary-of-deep-reinforcement-learning-rl-bootcamp-147022ecda13",
        "title": "A summary of Deep Reinforcement Learning (RL) Bootcamp: Lecture 1",
        "text": "This post is a summary of Lecture 1 of Deep RL Bootcamp 2017 at UC Berkely. All of the figures, equations and text are taken from the lecture slides and videos available here.\n\nRL problems are modeled as Markov Decision Processes (MDP). In an MDP, there is an agent that interacts with the environment around it. The agent can observe state (s_t) and reward (r_t), and perform action (a_t). As a result of its action, the environment will change to state s_(t+1) and an immediate reward r_(t+1) is received.\n\nSeveral examples of deep RL success stories include: Atari game 2013, Go player 2016.\n\nThere are multiple ways of solving RL problems. Here we first look at policy iteration and value iteration, which are based on dynamic programming.\n\nTo formulate the problem, an MDP is defined by\n\nAs an example, consider a Gridworld as shown below. The agent can take actions of moving to north, east, west and south. If the agent reaches to the blue diamond, it will receive reward +1. If it falls into the orange square, it will receive reward -1. Reaching anywhere else in the maze has zero reward.\n\nThe goal is to find the optimal policy to maximize the expected sum of the rewards under that policy.\n\nA policy \u03c0 determines what action to take for a given state. It could be a distribution over actions or a deterministic function. As an example, a deterministic policy \u03c0 for the Gridworld is shown below.\n\nThe problem of optimal control or planning is to given an MDP (S, A, P, R, \u03b3, H), find the optimal policy \u03c0*. Two exact methods to solve this problem are value iteration and policy iteration.\n\nIn value iteration, a concept called optimal value function V* is defined as\n\nwhich is the sum of discounted rewards when starting at state s and acting optimally.\n\nFor example, the optimal value function of Gridworld with deterministic transition function, that is actions are always successful, gamma=1 and H=100 are calculates as below:\n\nAnother example, the optimal value function of Gridworld when actions are always successful, gamma=0.9 and H=100 are calculated as:\n\nAnother example, the optimal value function if actions are successful with probability 0.8, with probability 0.1 it may stay in the same place and probability of 0.1 it may go to neighbor state, gamma = 0.9, H = 100, are calculated as:\n\nAs you can see, in case of stochastic transition function, the optimal value function for a state depends on the value function of other states. Another word, it requires a recursive/iterative calculation. That is where value iteration can play a role!\n\nThe value iteration algorithm is shown below:\n\nThe optimal values for Gridworld with H=100, discount=0.9 and noise=0.2 is calculated and shown below. It is noted that after certain number of iterations, the value function stops changing significantly.\n\nValue iteration is guaranteed to converge. At convergence, the optimal value function is found and as a result the optimal policy is found.\n\nWe consider another method called Q-learning to solve RL problems. To this end, optimal Q-value is defined as:\n\nQ-values are similar to V-values except that in addition to state s, action a is also given to the function. Similarly, there is a Bellman equation for Q-values\n\nTo solve Q*, Q-value iteration is defined as\n\nThere are multiple benefits of using Q-learning vs value iteration that will be discussed later. For now, it is worth noting that in Q-learning, we only compute Q-values and it implicitly encodes the optimal policy as apposed to value iteration that we need to keep track of both policy and value function.\n\nAs an example, Q-values for the Gridworld with gamma=0.9 and noise=0.2 after 100 iterations would look like below. There are four Q-values per state since there are four actions to take.\n\nFinally, we look at policy evaluation/policy iteration. In policy evaluation, we fix the policy and compute the value iteration for given policy as\n\nAs seen in the above equation, the max operation is removed since the policy is fixed now and as a result there is only one action to take, which is \u03c0(s).\n\nAnd thus, policy iteration is given as below. We repeat until policy converges. It converges faster than value iteration under some conditions.\n\nNote: Lab 1 includes examples for value iteration and policy iteration.\n\nIn the first lecture, basics of RL and MDP were introduced. The exact methods to solve small MDP problems were described. These methods are: value iteration, Q-learning and policy iteration. The limitations of these methods include: they require to iterate over and have storage for all states and actions. So they are suitable for small, discrete state-action space. Also, to update equations they require access to the dynamics of the environment or the transition function P(s\u2019|s,a)."
    }
]