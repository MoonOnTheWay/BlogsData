[
    {
        "url": "https://medium.com/tinymind/a-practical-guide-to-relu-b83ca804f1f7?source=---------0",
        "title": "A Practical Guide to ReLU \u2013 TinyMind \u2013",
        "text": "ReLU is linear (identity) for all positive values, and zero for all negative values. This means that:\n\nNote: We are discussing model sparsity here. Data sparsity (missing information) is different and usually bad.\n\nWhy is sparsity good? It makes intuitive sense if we think about the biological neural network, which artificial ones try to imitate. While we have billions of neurons in our bodies, not all of them fire all the time for everything we do. Instead, they have different roles and are activated by different signals.\n\nSparsity results in concise models that often have better predictive power and less overfitting/noise. In a sparse network, it\u2019s more likely that neurons are actually processing meaningful aspects of the problem. For example, in a model detecting cats in images, there may be a neuron that can identify ears, which obviously shouldn\u2019t be activated if the image is about a building.\n\nFinally, a sparse network is faster than a dense network, as there are fewer things to compute.\n\nThe downside for being zero for all negative values is a problem called \u201cdying ReLU.\u201d\n\nA ReLU neuron is \u201cdead\u201d if it\u2019s stuck in the negative side and always outputs 0. Because the slope of ReLU in the negative range is also 0, once a neuron gets negative, it\u2019s unlikely for it to recover. Such neurons are not playing any role in discriminating the input and is essentially useless. Over the time you may end up with a large part of your network doing nothing.\n\nYou may be confused as of how this zero-slope section works in the first place. Remember that a single step (in SGD, for example) involves multiple data points. As long as not all of them are negative, we can still get a slope out of ReLU. The dying problem is likely to occur when learning rate is too high or there is a large negative bias.\n\nLower learning rates often mitigates the problem. If not, leaky ReLU and ELU are also good alternatives to try. They have a slight slope in the negative range, thereby preventing the issue."
    },
    {
        "url": "https://medium.com/tinymind/so-what-is-machine-learning-12bee38c245c?source=---------1",
        "title": "So, what is machine learning? \u2013 TinyMind \u2013",
        "text": "So, what is machine learning?\n\nWelcome to machine learning (for humans)! This is a series in which we explore machine learning concepts, techniques, and tools in plain English. You won\u2019t find any maths here \u2014 while formulas are what powers computations, we believe why machine learning works should make intuitive sense.\n\nIn this very first post, we will lay the foundation by answering the most fundamental question of machine learning, namely what it is and why we need it. If you already know the answer and are here for more advanced stuff, you may swiftly move to the next post. :)\n\nA program is a piece of code that takes some input, processes it, and produces some output. For example, a calculator program may take two numbers as its input, add them together, and return the sum as its output.\n\nMost programs like the calculator above are fixed. We predefine some rule (addition), and they simply follow (2+3=5). In contrast, machine learning refers to programs that are dynamic. Instead of upfront rules, we give them a goal and some general guidelines to hit it. Then we let them try to hit the goal, learn how far they missed, make adjustments and try again, hoping that they will get closer over time. This process is called machine learning because it mimics how we learn new skills as humans. For instance, when learning to shoot a basketball, our goal is to let the ball go through the basket, and we have some general directions (throw the ball towards the basket), but the right force to apply at different positions can only be learned through trial-and-error.\n\nLet\u2019s consider a practical example. Say we have a coin and want to predict how many heads we will get if we flip it 10 times. Obviously, our best blind guess is 5 \u2014 the chance of head for a fair coin is 50%. If we write a program, it looks like:\n\nNow let\u2019s actually throw the coin. First 10 throws, 8 heads. Next 10 throws, 8 heads. Next 10 throws, still 8 heads. Oops, it looks like an unfair coin! As humans, we would quickly learn from this and tell ourselves, okay, I will guess a larger number from now on, maybe 8.\n\nHow do we let the program do the same? Well, we can write another program that returns 8, but that obviously doesn\u2019t generalize well. If we get a new coin, we need to throw that coin many times ourselves, and then write another program for it. Also, 80% may not even be the real ratio for this coin \u2014 it\u2019s possible that after 1000 throws we have 700 heads and 300 tails.\n\nIntroducing machine learning. The goal is to make a guess closest to reality. Just like how we learn as humans, we let the program make a guess, tell it the actual outcome, and let it adjust by moving the guess towards the actual outcome. Intuitively, the more flips it has seen, the more confident it should be about its guess, and additional observations should influence it less. Therefore, weighted average is a good way of making the adjustment. The program looks like the following:\n\nCongrats! We\u2019ve just written our very first machine learning program. As you can see, the more flips we do, the closer our predictor gets to the value 8. Now we can give it any coin, and it will learn to make the best prediction for that coin!\n\nFrom the above, we see that machine learning has the ability to learn from past examples, and derive rules to predict the future. Coin flipping itself may not be something worth predicting, but many real problems are similar. For example, if we were to guess whether a stock will go up or down in the next week, without additional information we can only give a 50/50 guess. However, if we give our program historical data for the past year, it can learn to make a much more reasonable prediction, just like what we did above.\n\nMany problems can be solved only with machine learning. One example is facial recognition. There are 7 billion (and growing!) faces on Earth, so it\u2019s clearly impossible to write a non-ML program to recognize all of them. Moreover, \u201chow someone looks\u201d is very hard to convey even to other humans. We have to let computers learn this themselves. As it turns out, neural networks (whoa, fancy term!) are very good at learning such things. We will discuss them in depth in the next article.\n\nSome common real-world problems machine learning is solving right now:\n\nIn subsequent posts, we will be looking at many of the above problems, and understand why machine learning techniques are able to solve them."
    }
]