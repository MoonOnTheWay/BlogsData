[
    {
        "url": "https://medium.com/@rohanthomas.me/convolutional-networks-for-everyone-1d0699de1a9d?source=user_profile---------1----------------",
        "title": "Convolutional Networks for everyone \u2013 Rohan Thomas \u2013",
        "text": "Building a Convolutional Neural Networks (CNN) is not a big challenge , that a Data Scientist or a Machine learning engineer can do.Ones someone understand it\u2019s architecture it is so simple to implement it for solving an Artificial Intelligence (AI)or a Machine Mearning (ML)problem.\n\nThis post is for making the CNN Architecture easy understandable without going much into math.\n\nIn ANN there will be an Input layer where the input will be length of input vector (eg. 28 x 28 = 784 Neurons). Let\u2019s see how convolutional networks differ from ANNs\n\n2. Their architecture is different from feedforward neural networks to make them more efficient by reducing the number of parameters to be learnt.\n\n3. In ANN, if you have a 150x150x3 image, each neuron in the first hidden layer will have 67500 weights to learn.\n\n4. ConvNets have 3D input of neurons and the neurons in a layer are only connected to a small region of the layer before it.\n\nThe neurons in the layers of ConvNet are arranged in 3 dimensions: height, width, depth.\n\nDepth here is not the depth of the entire network. It refers to the third dimension of the layers and hence a third dimension of the activation volumes.\n\nIn essence, a ConvNet is made of layers which have a simple API \u2014 transform a 3-D Input volume to a 3-D Output volume with some differentiable function which may or may not have parameters.\n\nA filter is represented by a vector of weights with which we convolve the input. You can increase the number of filters on the input volume to increase the number of activation maps you get. Each filter gives you an activation map.\n\nEach activation map you get, tries to lean a different aspect of the image such as an edge, a blotch of colour etc.\n\nIf on a 32x32x3 image volume, you implement 12 filters of size 5x5x3, then the first convolutional layer will have dimension 28x28x12 under certain conditions.\n\nseveral filters are used to extract several features in a convolution layer of a NNet. A single step for the 3X3 matrix is called a \u201cstride\u201d.\n\nThe activation function is usually an abstraction representing the rate of action potential firing in the cell.\n\nThere are mainly Linear Activation and Non Linear Activations.Without non linear the neural network would be much powerful. Activation function used to introduce non linearity is needed.\n\nAs in feedforward neural networks, the purpose of an activation layer in Convnet is to introduce nonlinearity.\n\nR(z) = max(0,z) is the equation of RELU Activation. Consider two integers positive and a negative\n\nIt reduces the number of parameters in the network, thus enabling it to learn faster.\n\nSoftmax is logistic activation function which is used for multiclass classification.\n\nSoftmax function is applied in the last layer of the network for taking the maximum probability from the classes and to predict.\n\nPooling is used for downsampling on the width, height of the image but depth remains same. Mainly there are three types of pooling. Min, Max, Average Pooling\n\nThe pooling layer works on each depth slice independently, resizes it using the mathematical operation specified such as MAX or Avg. etc.\n\nFinally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular neural networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.\n\nsoftmax activation is used in the fully connected layer for taking the max prob and to make a prediction.\n\nOverfitting may be seen in the classification accuracy on the training data, If the training accuracy is out performing our test accuracy, it means that our model is learning details and noises of training data and specifically working of training data. source (Rutger Ruizendaal, Towards Data Science, https://goo.gl/87as34)\n\nLet\u2019s try to implement CNN in MNSIT Dataset.\n\nTo download the data you can goto (http://yann.lecun.com/exdb/mnist/)"
    }
]