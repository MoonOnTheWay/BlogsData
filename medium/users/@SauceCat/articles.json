[
    {
        "url": "https://towardsdatascience.com/introducing-pydqc-7f23d04076b3?source=user_profile---------1----------------",
        "title": "Introducing pydqc \u2013",
        "text": "pydqc is an automatic data quality check package written in Python. It acts as an easy-to-use data summary framework, which could, to some extent, relieve us from the pain of writing tedious codes for general data understanding.\n\nTo make it simple, pydqc accepts a data table as input and provides data summary report or data compare report. It also allows you to transform the full process of data summary as well as data compare into jupyter notebooks. (data summary notebook, data compare notebook) The graph below summarizes the workflow of pydqc.\n\nTo implement data_summary or data_compare, we need to first have data schema, which contains names and data types of columns to be checked. It is necessary because different types of data require different strategies to generate meaningful statistical summary. For instance, in terms of categorical columns, you may want to check the value counts, while for numeric columns you prefer plotting out the distribution graphs.\n\nHowever, the automatic infer_schema function is not yet perfect enough to do everything correctly. For example, infer_schema function (at least this version) can\u2019t infer columns with \u2018key\u2019 type, columns that don\u2019t have concrete meaning themselves, but act as \u2018key\u2019 to link with other tables. And sometimes it would infer \u2018str\u2019 type as \u2018numeric\u2019 type when the categorical column represents its categories using numeric value. Therefore, the automatically generated data schema still needs human modification to ensure its correctness. You can easily do the modification by selecting from a drop down list.\n\nData summary, as it is named, is to summary useful statistical information for a data table. As mentioned previously, pydqc classifies data columns into four types. For each of these four types, data_summary function provides varied statistical information.\n\n\u2018key\u2019 and \u2018str\u2019: sample value, rate of nan values, number of unique values, top 10 value count.\n\n\u2018date\u2019: sample value, rate of nan values, number of unique values, minimum numeric value, mean numeric value, median numeric value, maximum numeric value, maximum date value, minimum date value, distribution graph for numeric values. ( numeric value for \u2018date\u2019 column is calculated as the time difference between the date value and today in months.)\n\n\u2018numeric\u2019: sample value, rate of nan values, number of unique values, minimum value, mean value, median value, maximum value, distribution graph (log10 scale automatically when absolute maximum value is larger than 1M).\n\nSometimes we may want to see whether the test set is different from the training set and how they are different from each other. And for data set with different snapshots, it might be useful to check whether data is consistent through different snapshots. This kind of use cases gives rise to the data_compare function that compares statistical characteristics of the same columns between two different tables. The same as data_summary, columns with different types are compared with varied methods.\n\n\u2018key\u2019: compare sample value, rate of nan values, number of unique values, size of intersection set, size of set only in table1, size of set only in table2, venn graph. (upper value for table1, lower one for table2)\n\n\u2018str\u2019: compare sample value, rate of nan values, number of unique values, size of intersection set, size of set only in table1, size of set only in table2, top 10 value counts. (upper value and blue bar for table1, lower one and orange bar for table2.)\n\n\u2018date\u2019: compare sample value, rate of nan values, number of unique values, minimum numeric value, mean numeric value, median numeric value, maximum numeric value, maximum date value, minimum date value, distribution graph for numeric values. (upper value and blue line for table1, lower one and orange line for table2.)\n\n\u2018numeric\u2019: compare sample value, rate of nan values, number of unique values, minimum value, mean value, median value, maximum value,distribution graph. (upper value and blue line for table1, lower one and orange line for table2.)\n\nFor more details about pydqc package, please refer to https://github.com/SauceCat/pydqc."
    },
    {
        "url": "https://towardsdatascience.com/introducing-pdpbox-2aa820afd312?source=user_profile---------2----------------",
        "title": "Introducing PDPbox \u2013",
        "text": "PDPbox is a partial dependence plot toolbox written in Python. The goal is to visualize the impact of certain features towards model prediction for any supervised learning algorithm. (now support all scikit-learn algorithms)\n\nIt is inspired by ICEbox as well as PyCEbox. ICEbox is a R package for Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. It is developed by one of the authors of the paper: Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation, where individual conditional expectation plots were introduced. While PyCEbox is a Python implementation of individual conditional expecation plots.\n\nPDPbox aims to wrap up and enrich some useful functions mentioned in the paper in Python. To install PDPbox, please refer to PDPbox.\n\nWhen using black box machine learning algorithms like random forest and boosting, it is hard to understand the relations between predictors and model outcome. For example, in terms of random forest, all we get is the feature importance. Although we can know which feature is significantly influencing the outcome based on the importance calculation, it really sucks that we don\u2019t know in which direction it is influencing. And in most of the real cases, the effect is non-monotonic. We need some powerful tools to help understanding the complex relations between predictors and model prediction.\n\nFriedman\u2019s partial dependence plot aims to visualize the marginal effect of a given predictor towards the model outcome by plotting out the average model outcome in terms of different values of the predictor.\n\nTo formally define PDP, let S be the chosen predictor, and C be its complete set (containing all other predictors), partial dependence function of f on x_S is given by\n\nIn practice, it is impossible to integrate over all possible values of C, thus the formula above is usually estimated as\n\nwhere N is the total number of observations in the training set. By generating estimations on different values of S, we can get a line showing how model prediction will change through different values of S.\n\nFor example, let\u2019s assume a data set that only contains three data points and three features (A, B, C) as shown below.\n\nIf we want to see how feature A is influencing the prediction Y, what PDP does is to generate a new data set as follow and do prediction as usual. (here we assume that feature A has three unique values: A1, A2, A3)\n\nIn this way, PDP would generate nrows * num_grid_points number of predictions and averaged them for each unique value of Feature A.\n\nFinally, PDP would only plot out the average predictions.\n\nAuthors of ICEbox pointed out that Friedman\u2019s PDP might obfuscate the complexity of the model relationship since it is an average curve. The major idea of ICEbox is to disaggregate the average plot by displaying the estimated functional relationship for each observation. So instead of only plotting out the average predictions, ICEbox displays all individual lines. (three lines in total in this case)\n\nThey believe that at least in this way, with everything displayed in its initial state, any interesting discovers wouldn\u2019t be simply shielded from view because of the averaging inherent in the PDP. Here is an vivid example showed in the paper:\n\nIn this example, if you only look at the PDP in Figure b, you would think that on average, X2 is not meaningfully associated with the predicted Y. While in light of the scatter plot showed in Figure a, the conclusion is plainly wrong. However, if you try to plot out the individual estimated conditional expectation curves, everything is obvious.\n\nThe authors argue that, ICE algorithm gives the user insight into the several variants of conditional relationships estimated by the black box.\n\nPDPbox aims to enrich the ideas mentioned by ICEbox in Python.\n\nPDPbox has a simple structure as follows.\n\nFor single variable plot, the basic plot is exactly Friedman\u2019s PDP, but with standard deviation outline.\n\nWith basic plot, you can choose to plot out the original data points,\n\nand also the individual estimated lines.\n\nYou can also try to cluster the individual lines and only plot out the cluster centers.\n\nWhen it comes to multiclass problem, you can choose to plot out all classes at a time,\n\nor only plot out one class.\n\nIn terms of two variable interaction plot, the complete plot contains two individual single variable plot and a two variable interaction contour plot. For the individual single variable plot, all options are the same as those in single variable plot.\n\nYou can choose to only plot out the two variable interaction contour plot.\n\nWhen it comes to multiclass problem, you can choose to plot out all classes at a time or just plot out one of them.\n\nFor more details of PDPbox, please refer to this GitHub repo."
    },
    {
        "url": "https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d?source=user_profile---------3----------------",
        "title": "Boosting algorithm: XGBoost \u2013",
        "text": "This article continues the previous post Boosting algorithm: GBM. This time we are going to discuss XGBoost! (Finally!)\n\nXGBoost, short for \u201cExtreme Gradient Boosting\u201d, was introduced by Chen in 2014. Since its introduction, XGBoost has become one of the most popular machine learning algorithm. In this post, we will dive deeply into the algorithm itself and try to figure out how XGBoost differs from the traditional boosting algorithms GBM.\n\nAs mentioned in the previous post, GBM divides the optimization problem into two parts by first determining the direction of the step and then optimizing the step length. Different from GBM, XGBoost tries to determine the step directly by solving\n\nfor each x in the data set. By doing second-order Taylor expansion of the loss function around the current estimate f(m-1)(x), we get\n\nwhere g_m(x) is the gradient, same as the one in GBM, and h_m(x) is the Hessian (second order derivative) at the current estimate:\n\nThen the loss function can be rewritten as\n\nLetting G_jm represents the sum of gradient in region j and H_jm equals to the sum of hessian in region j, the equation can be rewritten as\n\nWith the fixed learned structure, for each region, it is straightforward to determine the optimal weight :\n\nPlugging it back to the loss function, we get\n\nAccording to Chen, this is the structure score for a tree. The smaller the score is, the better the structure is. Thus, for each split to make, the proxy gain is defined as\n\nWell, all deductions above didn\u2019t take regularization into consideration. Note that XGBoost provides variety of regularization to improve generalization performance. Taking regularization into consideration, we can rewrite the loss function as\n\nwhere \u03b3 is the penalization term on the number of terminal nodes, \u03b1 and \u03bb are for L1 and L2 regularization respectively. The optimal weight for each region j is calculated as:\n\nThe gain of each split is defined correspondingly:\n\nFor better understanding, we are going to walk through the source code of XGBoost. For simplification, we will only focus on binary classification and the most important code snippets.\n\nupdater_seq is a comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. In default, it is set as \u201cgrow_colmaker,prune\u201d, which means first run updater_colmaker and then run updater_prune.\n\nFor updater_colmaker, each tree is updated by the builder depth by depth.\n\nCalculations for several important statistics are listed as follow.\n\nTo predict a new instance, first get the leaf indexes, then sum up the leaf values.\n\nThe leaf value is calculated as:\n\nGBM has broader application. At each iteration, both GBM and XGBoost need to calculate gradient at current estimate. XGBoost also needs to calculate hessian, requiring the objective function to be twice differentiable (strictly convex). GBM only requires a differentiable loss function, thus it can be used in more applications.\n\nXGBoost is faster. Comparing the weights calculated by GBM and XGBoost, for GBM, the weight is simply the average value of the gradients, while for XGBoost, it is the sum of gradients scaled by the sum of hessians.\n\nFor XGBoost, the weight is also known as the Newton \u201cstep\u201d, which naturally has step length of 1. Thus, line search is not necessary for XGBoost. This might be the reason why XGBoost is always much faster than GBM.\n\nXGBoost provides more regularization options, including L1(\u03b1) and L2(\u03bb) regularization as well as penalization on the number of leaf nodes(\u03b3).\n\nHowever, in terms of GBM in sklearn package, various useful regularization strategies are also provided. In version 0.19, parameter min_impurity_decrease, similar to \u03b3 in XGBoost, is added.\n\nBesides, for each tree in the ensemble, regularization options, including , , and , are implemented. For XGBoost, individual tree is regularized by , , as well as L1 and L2 penalization.\n\nXGBoost introduces more randomization. For GBM in sklearn package, we have parameter (similar to subsample in XGBoost) to implement Stochastic Gradient Boosting, and for column sampling. XGBoost also provides two similar options. The only difference is that XGBoost provides two levels of column sampling, and , thus introducing more randomness into the learning process."
    },
    {
        "url": "https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3?source=user_profile---------4----------------",
        "title": "Boosting algorithm: GBM \u2013",
        "text": "This article continues the previous post Boosting algorithm: AdaBoost. This time we will turn to GBM (Gradient Boosting Machine).\n\nBefore diving into tree boosting algorithms, let\u2019s first review the learning procedure of tree models. Tree models partition the feature space X into a set of T non-overlapping rectangular regions. For each region R1, \u2026, RT, prediction is usually generated by a simple constant model.\n\nThere are various algorithms for learning tree models, like CART, C4.5 and CHAID. For GBM, CART is used and XGBoost also utilizes an algorithm similar to CART. Thus, we will only discuss CART in this post.\n\nCART grows the tree greedily in a top-down fashion using binary splits. For each tree node, every split parallel to the coordinate axes are considered and the one minimizing the objective is chosen. The split procedure is repeated until some stopping criterion is reached.\n\nThe loss function for a tree f with T terminal nodes can be written as\n\nwhere L_j is the aggregated loss at node j. Consider we are going to split at node k, the split procedure is represented as\n\nThe gain of split is defined as\n\nFor each split made, the split gain is calculated for every possible split for every possible feature and the one with the maximal gain is taken.\n\nGiven the learned structure, for a region R_j, the weight is estimated by\n\nFor example, for squared error loss, the estimated weight will simply be the average of all responses in the region.\n\nBoosting algorithm fits the ensemble models of the kind\n\nwhere f\u200b0 is the initial guess, \u03d5m(x) is the base estimator at iteration m and \u03b8\u200bm is the weight for the m_th estimator. The product \u03b8\u200bm*\u03d5m(x) denotes the \u201cstep\u201d at iteration m.\n\nMost boosting algorithms can be viewed as to solve\n\nat each iteration, where f(m-1) denotes the current estimation. Therefore, the ensemble problem is simplified greedily as a forward stage-wise additive model. We don\u2019t optimize the ensemble in a global manner, but instead seek to improve the result based on the current estimate.\n\nFor AdaBoost, it solves this equation for the exponential loss function under the constraint that \u03d5m(x) only outputs -1 or 1. While GBM and XGBoost can be viewed as two general boosting algorithms that solve the equation approximately for any suitable loss function.\n\nGBM, short for \u201cGradient Boosting Machine\u201d, is introduced by Friedman in 2001. It is also known as MART (Multiple Additive Regression Trees) and GBRT (Gradient Boosted Regression Trees).\n\nGBM constructs a forward stage-wise additive model by implementing gradient descent in function space. Similar to gradient descent in parameter space, at the m_th iteration, the direction of the steepest descent is given by the negative gradient of the loss function:\n\nTaking a step along this direction is guaranteed to reduce the loss.\n\nAt each iteration, a regression tree model is fitted to predict the negative gradient. Typically the squared error is used as a surrogate loss.\n\nHowever, the negative gradient only gives the direction of the step. Further effort is necessary to determine the step length \u03c1\u200bm. A popular way is to do line search:\n\nRegarding this, Friedman presented a special enhancement. Considering that at each iteration, the algorithm was actually generating T separate predictions for each region R1, \u2026, RT, he proposed to do T line search steps, one for each region.\n\nFriedman also introduced shrinkage, where the step length at each iteration is multiplied by some factor \u03b7 (0<\u03b7\u22641), which is proved to enhance the model performance in practice. \u03b7 is also referred as the learning rate, as lowering it will slow down the learning process. Combining all of these, the \u201cstep\u201d taken at each iteration m is give by\n\nwhere the step length \u03c1\u200bm can be different for each region.\n\nThen the resulting model can be written as\n\nwhere f0 is typically initialized using a constant:\n\nWith big picture in mind, let\u2019s now go deeper into the base estimator. First, the loss function can be rewritten as follows:\n\nwhere n_jm is number of instances rest in region j. Letting G_jm represents the sum of gradient in region j, the equation can be further rewritten as\n\nFor a fixed learned structure, we can directly calculate the weights that minimize the loss function:\n\nPlugging the weight back to the loss function, the loss function becomes:\n\nThen, for a single node j, the proxy gain (ignore the other nodes and the normalization term) of a split is defined as\n\nwhere G_jmL denotes the left node of the split and G_jmR denotes the right node.\n\nFor better understanding, we are going to walk through the source code of sklearn.ensemble.GradientBoostingClassifier. For simplification, we will only focus on binary classification and the most important code snippets.\n\nThe default criterion for GBM is , a modified kind of specially for boosting algorithms. However, in our case, let\u2019s just stick to the simpliest . In terms of loss function, is default for binary classification problem, while is specially for AdaBoost.\n\nThe fit function is inherited from . Firstly, the learning process is initialized by , where as well as is assigned.\n\nThe initial prediction is generated through and , where is assigned as .\n\nAfter initialization, is started to fit the boosting stages.\n\nFor each stage, a is fitted to predict the residual (negative gradient).\n\nIn details, and is defined as follow.\n\nRemember we have mentioned the term , which is defined as:\n\nIn practice, this trick can be further simplified as:\n\nIt is utilized during the node split procedure of the regression tree.\n\nThe true gain of split is only calcualted for the best split when it is found. Note that the impurity improvement is scaled by the fraction of samples in that node.\n\nThe true absolute node impurity is calculated as:\n\nwhere m is the number of samples at that node.\n\nFor prediction, probabilities are generated by . Initial prediction is given by . At each stage, scores are updated by . Finally, the scores are normalized as probabilities.\n\nWhere is the implementation of logistic sigmoid function.\n\nReally feel so good to walk through the code! \ud83d\ude3b\ud83d\udcaf"
    },
    {
        "url": "https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c?source=user_profile---------5----------------",
        "title": "Boosting algorithm: AdaBoost \u2013",
        "text": "As a data scientist in consumer industry, what I usually feel is, boosting algorithms are quite enough for most of the predictive learning tasks, at least by now. They are powerful, flexible and can be interpreted nicely with some tricks. Thus, I think it is necessary to read through some materials and write something about the boosting algorithms.\n\nMost of the content in this acticle is based on the paper: Tree Boosting With XGBoost: Why Does XGBoost Win \u201cEvery\u201d Machine Learning Competition?. It is a really informative paper. Almost everything regarding boosting algorithms is explained very clearly in the paper. So the paper contains 110 pages :(\n\nFor me, I will basically focus on the three most popular boosting algorithms: AdaBoost, GBM and XGBoost. I have divided the content into two parts. The first article (this one) will focus on AdaBoost algorithm, and the second one will turn to the comparison between GBM and XGBoost.\n\nAdaBoost, short for \u201cAdaptive Boosting\u201d, is the first practical boosting algorithm proposed by Freund and Schapire in 1996. It focuses on classification problems and aims to convert a set of weak classifiers into a strong one. The final equation for classification can be represented as\n\nwhere f_m stands for the m_th weak classifier and theta_m is the corresponding weight. It is exactly the weighted combination of M weak classifiers. The whole procedure of the AdaBoost algorithm can be summarized as follow.\n\nGiven a data set containing n points, where\n\nHere -1 denotes the negative class while 1 represents the positive one.\n\nInitialize the weight for each data point as:\n\n(1) Fit weak classifiers to the data set and select the one with the lowest weighted classification error:\n\n(2) Calculate the weight for the m_th weak classifier:\n\nFor any classifier with accuracy higher than 50%, the weight is positive. The more accurate the classifier, the larger the weight. While for the classifer with less than 50% accuracy, the weight is negative. It means that we combine its prediction by flipping the sign. For example, we can turn a classifier with 40% accuracy into 60% accuracy by flipping the sign of the prediction. Thus even the classifier performs worse than random guessing, it still contributes to the final prediction. We only don\u2019t want any classifier with exact 50% accuracy, which doesn\u2019t add any information and thus contributes nothing to the final prediction.\n\n(3) Update the weight for each data point as:\n\nwhere Z_m is a normalization factor that ensures the sum of all instance weights is equal to 1.\n\nIf a misclassified case is from a positive weighted classifier, the \u201cexp\u201d term in the numerator would be always larger than 1 (y*f is always -1, theta_m is positive). Thus misclassified cases would be updated with larger weights after an iteration. The same logic applies to the negative weighted classifiers. The only difference is that the original correct classifications would become misclassifications after flipping the sign.\n\nAfter M iteration we can get the final prediction by summing up the weighted prediction of each classifier.\n\nThis part is based on paper: Additive logistic regression: a statistical view of boosting. For more detailed information, please refer to the original paper.\n\nIn 2000, Friedman et al. developed a statistical view of the AdaBoost algorithm. They interpreted AdaBoost as stagewise estimation procedures for fitting an additive logistic regression model. They showed that AdaBoost was actually minimizing the exponential loss function\n\nIt is minimized at\n\nSince for AdaBoost, y can only be -1 or 1, the loss function can be rewritten as\n\nContinue to solve for F(x), we get\n\nWe can further derive the normal logistic model from the optimal solution of F(x):\n\nIt is almost identical to the logistic regression model despite of a factor 2.\n\nSuppose we have a current estimate of F(x) and try to seek an improved estimate F(x)+cf(x). For fixed c and x, we can expand L(y, F(x)+cf(x)) to second order about f(x)=0,\n\nwhere E_w(.|x) indicates a weighted conditional expectation and the weight for each data point is calculated as\n\nIf c > 0, minimizing the weighted conditional expectation is equal to maximizing\n\nSince y can only be 1 or -1, the weighted expectation can be rewritten as\n\nThe optimal solution comes as\n\nAfter determining f(x), the weight c can be calculated by directly minimizing L(y, F(x)+cf(x)):\n\nSolving for c, we get\n\nLet epsilon equals to the weighted sum of misclassified cases, then\n\nNote that c can be negative if the weak learner does worse than random guess (50%), in which case it automatically reverses the polarity.\n\nIn terms of instance weights, after the improved addition, the weight for a single instance becomes,\n\nThus the instance weight is updated as\n\nCompared with those used in AdaBoost algorithm,\n\nwe can see they are in identical form. Therefore, it is reasonable to interpret AdaBoost as a forward stagewise additive model with exponential loss function, which iteratively fits a weak classifier to improve the current estimate at each iteration m:"
    },
    {
        "url": "https://towardsdatascience.com/feed-forward-networks-hierarchical-language-model-8ef814a7633e?source=user_profile---------6----------------",
        "title": "Feed Forward Networks: Hierarchical Language Model \u2013",
        "text": "In Professor Hinton\u2019s lecture(Lecture 4d of course Neural Network for Machine Learning), he mentioned a very interesting topic: learning to predict the next word by predicting a path through a tree. This post aims to go through two related papers and distill some inspiring ideas.\n\nIn previous two posts Forward Propagation for Feed Forward Networks and Backward Propagation for Feed Forward Networks, we have gone through both forward and backward propagation process of the simple feed forward networks. Actually, the toy example is based on Bengio\u2019s NPLM (Neural Probabilistic Language Model) in 2003. The most notable difference is that Bengio\u2019s net contains more than 15,000 output units while our simple net only has 250.\n\nMathematical representation for this predictor is as follow:\n\nwhere x is the concatenation of the input word feature vectors, W represents the direct connections between the input layer and the output layer and is optionally zero (no direct connections).\n\nHowever, as the vocabulary size growing large, NPLM becomes very slow to train and test. Computing the probability of the next word requires normalizing over all words in the vocabulary. What\u2019s worse, calculating exact gradient needs to do this computation repeatedly to update the model parameters iteratively. Thus, it is very time consuming. One potential improving method, as mentioned in the lecture, is to learn in the serial architecture using the following network structure.\n\nFor each context, one can first compute the logit score for each candidate next word and then normalize all the logits using softmax function to get the probabilities.\n\nIf there are only limit number of candidate words, one can save even more time. For example, the candidates can be limited in the set of possible words suggested by the trigram model.\n\nThis part is based on Morin and Bengio\u2019s paper Hierarchical Probabilistic Neural Network Language Model.\n\nThis paper proposes a much faster variant of the original HPLM. The basic idea is to construct a hierarchical description of a word by arranging all the words in a binary tree with words as the leaves (each tree leaf is associated with one word). Then the task becomes to learn to take probabilistic decision at each tree node so as to get down to the target word (each tree node represents a cluster of words).\n\nIn formal representation, instead of computing P(Y|X) directly, we can define a clustering partition for Y with a deterministic function c(.) mapping Y to cluster C, then P(Y|X) can be decomposed as:\n\nThe proposed binary tree is actually a binary hierarchical clustering of words, which can be interpreted as a series of binary stochastic decisions associated with nodes. Each word can be represented by a bit vector (b1(v), b2(v),\u2026,bm(v)), where m is the number of decisions needed to take to reach the target word. For example, according to the tree structure below, the word \u201ccat\u201d can be represented as (1, 0). (1 means to take the left node) And we can see that words \u201cdog\u201d and \u201ccat\u201d belong to the same second-level word cluster, while \u201ctree\u201d and \u201cflower\u201d belong to the other.\n\nThen, conditional probability for a next word v can be represented as sequential prediction down a tree path.\n\nIn this way, for each training sample, there will be target and gradients only for the path associated with the target word. This is the major source of time saving during training.\n\nThe other trick to reduce computation is to share parameters across the hierarchy tree. First, it is reasonable to share the word embedding across all nodes. Second, it also makes sense to associate each node with a feature vector similar to that for word embedding because each tree node is actually representing a cluster of similar words. Then we would have feature vectors for both words and tree nodes.\n\nWith all these assumptions, at each tree node, we can consider the model to predict the next tree node based on two kinds of input: the context and the current tree node (more precisely, is the concatenated feature vectors of the context words and the feature vector of the current tree node). This can be represented by a model similar to NPLM but with two kinds of input.\n\nThe predict function can be written as:\n\nWe can see that this formula is quite similar to the following one, which is from the original NPLM. The major difference is that the feature vector of potential next word v is replaced by the feature vector of the current node.\n\nA very important component of this model is to construct the hierarchical binary word tree. This paper proposes to utilize the prior knowledge from the WordNet resource.\n\nThe IS-A taxonomy in WordNet is already a hierarchical tree of words classified by semantic type. However, some tree nodes have multiple parents and some words are associated with multiple leaves (because leaves are actually senses, it is common that one word is related to multiple senses). Some simple modifications are necessary to make the tree useful: manually select one parent for each of the few nodes associated with several parents and assign each of the few words associated with multiple leaves with only one leaf.\n\nThe last step is to turn the modified tree into a binary one. The proposed method is to perform a data-driven binary hierarchical clustering of the children associated with each node using simple K-means algorithm. To compare nodes, each tree node is associated with the subset of words that it covers. Since each word in the vocabulary set can be associated with a TF-IDF vector calculated from the training corpus, each tree node can be reasonably represented by the dimension-wise median of the TF-IDF vectors of the subset of words it covers.\n\nThe implementation and the experiments show that the proposed model significantly speeds up the learning process when the number of output classes grows huge. But it doesn\u2019t generalize as well as the original NPLM. However, given the very large speed-up, it is certainly worth further investigation. One promising point is to fully utilize the word sense ambiguity information provided by WordNet\u2019s taxonomy by allowing one word to be associated with multiple senses (tree leaves).\n\nThis part is based on Mnih and Hinton\u2019s paper A Scalable Hierarchical Distributed Language Model.\n\nThis paper claims the main limitation of the hierarchical model mentioned above is the precedure of tree construction. It proposes an automated method for building tree directly from the traing data without any kind of prior knowledge.\n\nThe simple log-bilinear language model (LBL) is used as the building block for the proposed model. To predict the next word w_n based on the context w_1 to w_n-1, the model first calculates the predicted feature vector (with same dimension as the embedding feature vector) for the next word by just simply linearly combining the feature vectors of the context words:\n\nThen the similarities between all candidate words and the predicted feature vector can be computed using the inner product. The similarities are further exponentiated and normalized to obtain the distribution over the next word.\n\nThe NPLM needs to compute the hidden activities once for each decision since the feature vector of the tree node, which keeps changing down the path, is included in the model input. However, in the LBL model, the hidden layer is replaced with the predicted feature vector. Thus, the LBL model can save a lot of time by calculating the hidden activities once per prediction (one prediction contains a sequence of decisions).\n\nThe hierarchical model here is based on Morin and Bengio\u2019s idea but uses LBL to compute provabilities at each node and allows each word to be associated with multiple leaves.\n\nThe probability of the next word being w is the joint probability of making sequence of binary decisions specified by the word\u2019s code. To make things simple, let\u2019s first assume that each word is associated with exactly one leaf. Then each word is actually corresponding to only one path down the tree, which can be described by a binary string. For example, string \u201c10\u201d means to take left child node at the first node and then go to right child node at the second node. The probability of the next word can be written as a product of probabilities of the binary decisions:\n\nwhere d_i is the i_th digit in the code for word w, and q_i is the feature vector for the i_th node in the path corresponding to that code. In details, the probability of each decision is given by:\n\nWhen extending to words with multiple codes (each word can relate to more than one leaf), the equation becomes:\n\nwhere D(w) is a set of codes associated with word w.\n\nIn terms of tree construction, it is proposed to use a pure learning approach instead of using expert knowledge like the IS-A taxonomy DAG from WordNet. In other words, it prefers to implement hierarchical clustering algoithm directly on the training data. With this idea in mind, we need to consider how to represent each word in the vocabulary set.\n\nBefore really going into the solution, we can first think about what we want from the clustering of words. Well, we would like to make it easilier to predict the next word based on the context. Thus, it is quite natural to consider describing each word in terms of the contexts that can precede it. Certainly we can use the distribution of contexts that can precede the target word. However, as the context size grows, the number of contexts that might potentially procede the target word would grow exponentially. Thus, it is not a good idea. We want condense, low-dimensional real-valued representation.\n\nThe predicted vector could be a reasonable choice. It is generated based on the context and has much smaller dimension compared with the vocabulary size. Thus it is condense and can somehow represent the information provided by the context. However, to generate the predicted feature vector, we need to train the HLBL with a hierarchical tree. The paper proposes a really clever bootstrapping procedure: first generate a random binary tree of words, then train a HLBL based on the random tree, and finally use the learned predicted feature vectors to reconstruct the tree.\n\nIn this way, each context would finally be associated with a learned predicted feature vector. Since there are many possible contexts can precede to a specific next word, each word can be associated with a number of predicted vectors generated by all possible contexts. It is proposed to summarize all these predicted feature vectors for a specific word by computing the expected predicted feature vectors w.r.t. the distribution of all possible contexts. Then the word is finally represented by this expected vector."
    },
    {
        "url": "https://towardsdatascience.com/backward-propagation-for-feed-forward-networks-afdf9d038d21?source=user_profile---------7----------------",
        "title": "Backward Propagation for Feed Forward Networks \u2013",
        "text": "This is the continuation of the previous post Forward Propagation for Feed Forward Networks. After understanding the forward propagation process, we can start to do backward propagation.\n\nTo train the networks, a specific error function is used to measure the model performance. The goal is to minimize the error(cost) by updating the corresponding model parameters. To know which direction and how much to update the parameters, their derivatives w.r.t the error function must be calculated. And this is what backward propagation is used for.\n\nThe choice of a error function is always closely related to the specific use case. Well, for this case, as mentioned in the previous post, the output layer is activated by the softmax function,\n\nFor the softmax function, cross entropy error is a well-known choice. The reason might be that, as I guess, the cross entropy error is specially designed for probability outputs, while other error function, like the square error, is for general cases. Besides, unlike the misclassification error, cross entropy error measures the model performance more comprehensively. Because, for example, for the binary case, assigns 0.6 or 0.7 to the wrong class yields the same model performance when the performance is measured by misclassification error, while the cross entropy error would prefer the first model rather than the second one.\n\nIt is worth to point out that, because of the specialty of the softmax function, each output net unit is connected to all output activate units. Thus, during back propagation, each output net unit receives errors from all output activate units."
    },
    {
        "url": "https://medium.com/@SauceCat/forward-propagation-for-feed-forward-networks-ac8fcb6bdd60?source=user_profile---------8----------------",
        "title": "Forward Propagation for Feed Forward Networks \u2013 SauceCat \u2013",
        "text": "During the study of Professor Geoffrey Hinton\u2019s online course (Neural Networks for Machine Learning), I found that the best way to understand a type of networks is to spend some time going thoroughly the forward and backward propagation. I want to start with the simplest but the most fundamental feed forward networks. The specific network structure used in this post came from the assignment of the course. Actually it is a toy example for the \u2018learning word representations\u2019 task. The goal is to predict the fourth word based on the previous three. The three word embedding vectors, each with 50 elements, are concatenated according to their appearing order and formed a 150-dimensional input vector. The hidden layers contains 200 units and the output layer contains 250 units. All units from different layers (input layer, hidden layer and output layer) are fully connected to each other. (hidden net layer and hidden activate layer belong to the same layer here)\n\nUsually, for a simple feed forward network, only the weights need to be learned. In this case, the word embedding vectors are also included in the learning process. That\u2019s why it is named \u2018learning word representations\u2019. For the toy example, only 250 words are included in the dictionary.\n\nIn practice, the input is actually in format \u2018[word_index_1, word_index_2, word_index_3]\u2019. These three indexes represents the row indexes of the representation matrix. When the mini-batch gradient descent algorithm is implemented, let N be the batch size, the input vector is expanded into a \u2018N-column\u2019 input matrix.\n\nFor all hidden units with a batch of training samples: To understand the whole things clearly, I have visualized the process of the matrix calculation (I used to struggle a lot here).\n\nEach column of the hidden matrix, which represents a hidden vector for a single training sample, can be decomposed as the linear combination of the columns of the weight matrix (the sum of contribution from all input units).\n\nFor the output layer, while other things remain almost the same, each unit of the activate layer is connected with all units of the net output units because of the softmax activate function.\n\nEach column of the output matrix, which represents a output vector for a single training sample, can be viewed as the linear combination of the columns of the weight matrix (the sum of contribution from all hidden units). As we see, the forward propagation for the simple feed forward networks is quite straightforward. Only one thing need to notice is the softmax activate function. It will become quite tricky in the back propagation part, as you will see in the next post about the back propagation of the feed forward networks."
    }
]