[
    {
        "url": "https://medium.com/onfido-tech/us-research-development-f5625c43d4b4?source=---------0",
        "title": "Us: Research & Development \u2013 Onfido Tech \u2013",
        "text": "At Onfido, we\u2019re proud of building new, exciting and world-changing tech. But to keep up with the pace of change, we need the best of the best working with us. Our research team is just that: pragmatic people focused on productionising the most promising ideas as quickly as possible.\n\nRecently, I\u2019ve been getting more and more involved with the research team. Not by participating in the development of algorithms, or actually implementing the hottest ones in our area of research (my team focuses on the facial similarity and \u2018liveness\u2019 solutions) \u2014 but instead, by iterating on the hard work of the research team. We help transform their ideas into machine learning models that can recognise similar faces, head movements and spoofing attempts (a photo of a screen, or photo of a photo, for instance) and deliver them to our customers.\n\nBy \u201chelping\u201d, I mean that we pick up almost productionable algorithms and making them run as microservices. They\u2019re then integrated into our complex and growing pipeline of automatically processed checks.\n\nSome of the tweaks we make can include: collecting metrics to understand the number of web workers that should be running in the Kubernetes pods we provision, defining an autoscaling strategy accordingly, pre-loading models to speed up the first incoming request in each web worker, and adding acceptance tests to give us a greater degree of confidence when deploying these services \u2014 among other improvements here and there.\n\nIn order to make this work, good communication among everyone in the team is paramount. For that reason, we\u2019ve have been adopting some processes and tools to achieve the much sought after \u2018communication nirvana\u2019. We think it\u2019s been working very well so far:\n\nResearch and Engineering in the same team \ud83e\udd17\n\nPreviously, engineering and research worked as two complementary but separate functions. We only overlapped each other a little bit until Onfido decided to include the Research function into cross-functional teams (which we already did for Testing).\n\nThe working dynamics of researchers are totally different from engineers, particularly in regards to time to deliver. Keeping the whole team updated of the progress and decisions that have been made in daily stand-ups really helps. Research joins us every other day so we can chat about updates and discuss trade-offs.\n\nAs an engineering team, we\u2019re committed to finish a Sprint\u2019s work every 2 weeks. Research now contributes to that work via a shared board, commiting to finishing (or at least contributing via exploratory tasks) whatever we\u2019re currently working on productionising. It enables wider visibility and enhanced knowledge sharing.\n\nDue to the growing need for better communication, a separate Slack channel was created to facilitate short, offline discussions that don\u2019t require a full meeting. It\u2019s also where we share achievements (and lots of emojis).\n\nOnfido has an incredibly talented research team, whose algorithms improve the quality of what we and our clients deliver. This is still the beginning of what we expect to be a very fruitful and enjoyable symbiosis between Research and Development at Onfido."
    },
    {
        "url": "https://medium.com/onfido-tech/caspian-a-serverless-self-service-data-pipeline-using-aws-and-elk-stack-6d576f8ce369?source=---------1",
        "title": "Caspian: a Serverless, Self-Service Data Pipeline using AWS and ELK Stack",
        "text": "At Onfido, we are using data to improve our services, solve challenging problems using machine learning, and serve our customers efficiently. We have built a robust, secure and scalable pipeline to handle our data and help us to achieve the above goals.\n\nResearch: Machine learning is at the heart of our products, from extracting document information to identifying fraudulent documents, we are leveraging machine learning to increase the automation and accuracy of our services. The most crucial part of creating an ML model is to support data labelling, and to be able to easily search/query and download the relevant data for training various models.\n\nPrivacy/Security: Privacy is crucial within the company. We are dealing with sensitive user data and we care about it. One of the purposes of having such a unified data-pipeline is to make sure that all the data used across the engineering team is stored in a secure manner. Therefore, data has to be be stored and encrypted at rest and transit.\n\nInternal Metrics: Each team develops new microservices or deploys a new version of a service every day. We need a way to easily expose different metrics/data and visualise them to be able to pinpoint any limitations and errors. This data helps us monitor the product performance of a service or an SDK before and after deploying a new version.\n\nBusiness: We serve our customers on a daily basis and our internal services generate data every day. For monitoring purposes and also as a way to quickly and efficiently track complex metrics and generate different reports for our product managers we need to have a flexible and efficient way to expose and visualise the data.\n\nThe key principles to design and architect a self service data-pipeline are as follows:\n\nThe architecture can be divided into 4 main parts.\n\nFirst, Development and deployment of the whole infrastructure stack and Caspian logic.\n\nSecond, Controller unit which allows the user to configure(using a cli tool) and introduce the new data source and internal logic.\n\nThird, Ingestion unit with which users and external services are able to push the data. This unit will collect, encrypt and store the data in S3.\n\nFourth, Internal logic which includes applying any filter on the data, transforming it if necessary and indexing it into Elasticsearch.\n\nFollowing explains each of the above components in more detail.\n\nTo build the above pipeline there are two things we need to handle\n\nWe follow infrastructure as code at Onfido. We use Terraform as a cloud provisioner to create/destroy/recreate/modify each AWS services in our infrastructure. For each service we define a terraform file where the service and the access control between the services and the users are defined.\n\nWe use two AWS accounts for development and production. This way we make sure the same setup in development will be replicated exactly in production and any changes can be tested in development and then safely deployed to production.\n\nWe define two hooks from the development and master branches in bitbucket to trigger a custom build in Jenkins. After committing/pushing to the project repository, the predefined hook will trigger the custom defined build in Jenkins. Thereafter, the lambda dependencies and the code will be packaged/zipped and then the will be applied. This applies the changes required to reach the predetermined set of actions generated by the , once that is successful it will also deploy the lambda if something has changed compared to the previous state of code.\n\nWe decided to store all the logic related to a new data source in a central location where we can easily manage and configure its relevant characteristics. One record is defined per data source in Dynamodb table. That is the configuration for the new candidate to be registered to Caspian. The record looks like below:\n\nWe have services that could run inside or outside of Kubernates(K8s). We needed a way to collect the data from different services and push them in batches to Caspian. Moreover, for compliance purposes we needed to obscure the raw data that is stored in Caspian. We developed two ways to collect the data.\n\n1) Logstash. We maintain a Logstash service inside Kubernates which we used to collect the data and then store that into a S3 bucket. For encryption purposes we developed and open sourced a plugin for logstash to encrypt the content. We wrote a separate blog post about it which can be found here. Logstash collects the data, stores them in a filesystem and uploads the file(encrypted) to S3 based on the two rules, whichever is met first. One is based on a predefined time interval and the other is based the size of the file. Since the data will be processed inside the pipeline, logstash does not need to be aware of the content. However, each record should contain a field called which will be used inside the Caspian.\n\n2) AWS Kinesis Firehose. We defined this alternative way to collect the data for three reasons, 1) scalability 2) not necessarily all the services have access to Logstash 3) easy adoption: It provides a self service capability that anyone across the company can easily push their data by using a simple two line python/ruby code. For example, for the python version we just defined a wrapper function in our data-pipeline-app to push the data into either dev or prod AWS account.\n\nSimilar to Logstash, we could configure two rules to deliver the buffer data to S3 based on either size or time interval whichever is met first.\n\nAfter the data is collected and stored in S3 using either of the above ingestion options, it is time to process, transform and prepare the data for the ingestion. Therefore, we defined and configured a lambda to be triggered whenever a file is uploaded in S3. We divided the above process into two separate lambda functionality. 1) Big files that contain all the data needed to be broken into multiple different files per data source and stored into another S3 path. This will separate the data into different paths that can be used in future if necessary. Note, that all the files are encrypted and uploaded in S3. The S3 paths will look like this:\n\nWe configure the lambda to be triggered for each file. Inside the lambda, using we get all the metadata related to that src_id from dynamodb. Then any filter or transformation function that is defined in metadata(described in section) will be applied on data. At the end, we have a list of records which will be indexed using api. The API will return a list of records which failed to be indexed along with the reasons of failure. Our error log handler will then upload those failed records in S3 separately. Those files can be reindexed, later on, after resolving the error. At the end, both successful and failed metrics will be pushed to Datadog for monitoring."
    },
    {
        "url": "https://medium.com/onfido-tech/speed-up-webpack-ff53c494b89c?source=---------2",
        "title": "\ud83d\udd25 Speeding up webpack \u2013 Onfido Tech \u2013",
        "text": "It\u2019s only so often you can wait 5 whole seconds for your build to apply your to change\u2026\n\nAt Onfido, we use webpack as our module bundler. As is always the case given the speed of development, our webpack config grew organically, and the speed of the pipeline was an after-thought.\n\nBut there came a tipping point. Eventually, we snapped and decided to get that build time way down.\n\nThere are already some great articles that discuss ways to increase build speed. To name just a few:\n\nWe followed these articles very closely, and used a lot of their suggestions. But, we also made some changes that haven\u2019t been commonly mentioned anywhere else \u2014 so I want to discuss them here.\n\nWe started by measuring our performance. This really helps to find your current bottlenecks, and compare your progress as you make changes.\n\nI created the Speed Measure Plugin for webpack, which we used to analyse the performance of our plugins and loaders. This let us focus our search, and work out where the easiest wins were. I\u2019ve talked about SMP a bit already, so won\u2019t go into it in any more detail in this post.\n\nOverall, in a repo containing around 50,000 lines of JS/CSS code, these were our build times:\n\nEven a cursory glance at SMP\u2019s output shows that takes a long time to run. For us it took 65% of the entire build time!\n\nWe were originally using webpack 3\u2019s built-in which is set at version 0.4.6.\n\nHowever, there\u2019s nothing stopping you from upgrading your version without upgrading webpack, and just manually importing the plugin instead.\n\nVersion 1 of the plugin introduces some performance features \u2014 namely parallelisation, and caching. If you\u2019re only uglifying on a build server like Jenkins, then caching doesn\u2019t really help you.\n\nTurning on the flag, however, can save you a lot of time \u2014 depending on how beefy your machine is, and how many cores it has. For us, running on i3.4xlarge EC2 instances with 16 vCPUs, this was considerable.\n\nMost webpack configs have a rule to handle images, and that rule is normally followed by , or some other similar image loader.\n\nBut the only necessary loader here is the , which actually allows the image to end up in the output directory, and its URI passed to the bundle.\n\nThe optimises these images, minifying and re-encoding them. For local development, this is quite a lot of unnecessary work.\n\nThere are a few ways to cache with webpack \u2014 like using , , or the babel flag. All of these caching methods have an overhead to boot up. The time saved locally during a re-run is huge, but the initial (cold) run will actually be slower.\n\nCaching on production builds that should be running from scratch each time anyway will just be slowing you down.\n\nWe had some legacy code that was written in CoffeeScript. This was never seen as that big a deal before, as the code was rarely touched, and the loader did its job fine.\n\nHowever, SMP revealed that was taking 1,078 ms on average for each module. Compared with \u2019s average of 561 ms, this was an obvious enough opportunity for improvement. Not to mention the benefit of reducing the time, and reducing our dependency count!\n\nSimply transpiling the CoffeeScript files to JS with the CLI, and then manually cleaning them up removed this dependency for us.\n\nUsing splits out part of your bundle into a separate files \u2014 most often by splitting out stylesheets into a separate CSS file.\n\nThis can speed up the end-user\u2019s experience, but adds overhead into the compilation steps. So again, when running locally, this is unnecessary work. Fortunately, this plugin comes with a simple flag!\n\n(note that this is different to which splits out your bundle into a separate file, but does so in an entirely different process \u2014 which does massively help with your build speed).\n\nWhether you\u2019re using PostCSS, SASS, or any other CSS tool, you likely don\u2019t need it running on all of your stylesheets. For us, we had some old legacy stylesheets, and stylesheets coming from third-party dependencies, which were all vanilla CSS.\n\nThese don\u2019t need to run through PostCSS or SASS to compile into CSS \u2014 so separating out these into separate loader configs can give you a slight speed boost."
    },
    {
        "url": "https://medium.com/onfido-tech/towards-faster-training-and-smaller-generalisation-gaps-in-deep-learning-b9767f68ec23?source=---------3",
        "title": "Towards Faster Training and Smaller Generalisation Gaps in Deep Learning",
        "text": "The unquestionable success of deep learning in recent years has brought a lot of attention to the field. Attendance at top ML conferences is at an all-time high, new tutorials and courses are written every month (here is one from February 2018), and more and more code is being shared thanks to the accessibility of deep learning frameworks (like the Detectron from FAIR, or the Tensorflow object detection from Google Research). Part of the reason for this success is that Deep Learning has now been proven to work on some challenging tasks, including image classification and object detection in the wild.\n\nHowever, even when we have a perfectly clean dataset and a suitable architecture, getting the most out of Deep Learning is time-consuming and resource-intensive. It is not unusual to spend days or even months on the optimisation, which sometimes also requires the luxury of having a GPU farm. And once a model is trained, deploying it in a production environment requires a deep understanding of its generalisation properties in order to minimise the risk of making wrong decisions.\n\nThis blog post is the first of a series of two. In this first post, we\u2019ll look at some key concepts in neural network optimisation, and discuss recent advances which have the potential to significantly reduce wall clock time when training neural networks. In the second post, we will review recent work around generalisation in deep learning, with particular focus on understanding current limitations of widely used optimisation strategies and how some of them are addressed in recent literature.\n\nWhat makes optimising deep neural networks so time-consuming and resource-intensive? Obviously, the large number of parameters needed to optimise, and the amount of data required to achieve good generalisation performance are among the main reasons. But there\u2019s another reason that makes training deep neural networks time-consuming: hyper-parameter tuning.\n\nArguably the most important (in terms of impact on the performance) hyper-parameter when optimising a neural network is the learning rate. It is at least the first that comes to mind when one decides to optimise hyper-parameters. Finding a suitable learning rate is a problem in of itself, but coming up with an optimal strategy to update the learning rate during training is even more complex. One popular strategy is to set the highest learning rate that does not make the loss diverge, then reduce it with a functional policy (e.g. exponential decay) which is not guaranteed to be optimal when defined a priori. An alternative solution is to manually tune the learning rate based on the target performance (e.g. validation accuracy), typically reducing the learning rate by 1/2 or 1/10 when the performance measure \u201cplateaus\u201d.\n\nAutomating this tedious process could, in principle, provide better overall performance and reduce wall clock time when training a model. While recent pre-defined learning rate schedules such as cyclical learning rates and stochastic gradient descent (SGD) with warm restarts have been proven to speed up training if performed correctly, they also introduced new hyper-parameters such as when to trigger the restart (or the cycle), what is the range of learning rates that should be used, and so on. One of the most interesting attempts to tackle this issue has been recently proposed by Baydin et al. The authors introduce a very simple yet effective online learning rate adaptation technique, which should significantly reduce the need for the manual tuning of the initial learning rate and its adaptation policy. The main idea is to cast the learning rate adaptation problem as an additional (higher-level) optimisation problem, which they show can be solved via gradient-based optimisation. Given the nested nature of the optimisation, they call this \u201chyper-gradient descent\u201d (HGD). The most basic form of HGD can be derived from regular gradient descent (GD) as follows. Given an objective function f and the parameters \ud835\udf03\u209c\u208b\u2081 regular GD evaluates the gradient \u2207f(\ud835\udf03\u209c\u208b\u2081) and moves along its direction to obtain the parameters \ud835\udf03\u209c at time t.\n\nwhere \ud835\udefc is the learning rate. In addition to this update rule, the authors derive an update rule for the learning rate itself (which therefore becomes a sequence of \ud835\udefc\u209c)\n\nintroducing \ud835\udefd as the hyper-gradient learning rate. Of course, one could iterate the procedure and apply the same technique to \ud835\udefd.This has been referred to as \u201chigher-order hyper-gradients\u201d. Extensive experimental results on popular classification tasks show that with fixed hyper-gradient learning rate, one can afford to set the \ud835\udefc learning rate in a very wide range (from 10\u207b\u00b9 to 10\u207b\u2076) and still get close-to-optimal results (i.e. obtained with dedicated tuning) for some tasks (Figure 1). In fact, if the initial \ud835\udefc learning rate is too low, the updates will tend to increase it first and then to decrease it afterwards in a nice, continuous way. This means that even though the hyper-gradient learning rate \ud835\udefd is a new hyper-parameter, it should require much less tuning, at parity of performance. It is worth noting that this advantage comes with very little extra computational cost.\n\nFigure 1. Behaviour of hyper-gradient variants against their regular counterparts. Columns: (left) logistic regression on MNIST; (middle) multi-layer neural network on MNIST; (right) VGGNet on CIFAR-10. Rows: (top) evolution of the learning rate \ud835\udefc; (middle) training loss; (bottom) validation loss. Main plots show epoch averages and inset plots highlight the behaviour of the algorithms during initial iterations. Image from Baydin et al.\u2019s paper.\n\nBesides the learning rate, a plethora of other hyper-parameters have a significant influence on the model performance, for instance the number of neurons, architecture type, weighing of the regularization term, number of layers, filter size, momentum coefficient and so on. Several methods have been proposed to explore the space of hyper-parameters; among the most used, grid search and random search. Bayesian search is another well-known choice even though its effectiveness compared to random search has been questioned in the past, thus suggesting alternatives may need to be explored. One such alternative has recently been proposed by Malherbe and Vayatis who discuss a search procedure which relies on the assumption that the loss landscape of the search is continuous to some degree. To formalise, let us denote g the function that maps the hyper-parameters of a training to some loss. Here, the loss is taken to be the validation error after training. In such context, the global minimum of g corresponds to the set of hyper-parameters for which we obtain a trained network with the smallest validation error.\n\nAssuming g is k-Lipschitz and given a set of hyper-parameters H, one can approximate the loss for hyper-parameters close to H (with regard to the Euclidean distance) by a piecewise affine function with slope k.\n\nFigure 2. Visual interpretation of the hyper-parameter search algorithm by Malherbe and Vayatis. The x axis represents sets of hyper-parameters H while the y axis represents the test loss the model achieves with such H. The black line represents the true (unknown) loss line of the hyper-parameter search g. The squares on the line represent the points at which we sampled g in the past. Based on them, we can construct the red-line which gives a lower bound on g(H) for any H. We can use this information to discard regions which are not worth exploring (given by the grey area on the right-hand figure).\n\nAfter performing say N evaluations and obtaining N sets of hyper-parameters, we obtain a piecewise linear function g\u0303 which approximates g over the whole space of hyper-parameters (see Figure 2 \u2014 left). Specifically, for any given set of hyper-parameters, g\u0303 gives a lower bound on the loss we can achieve with these hyper-parameters (i.e. for any H, g\u0303(H) \u2264 g(H)). This is convenient because it highlights regions that we know are not worth exploring: these are the regions in which g\u0303 is always greater than the best loss we obtained so far (see the grey regions in Figure 2 \u2014 right). By systematically excluding these regions from sampling, we reduce the number of useless estimations of g. For a more thorough explanation of this algorithm, you can check out this blog post. Note that we can play with the Lipschitz parameter k to encode the expected smoothness of the loss curve. The lower the k, the smoother we expect the loss function to be, leading to the exclusion of bigger regions. This is at the risk of missing potential interesting minimas as is shown in Figure 3.\n\nFigure 2. We show visually the importance of the Lipschitz parameter k. Left: k is low therefore we expect a smooth loss landscape. This cuts out a large part of the hyper-parameter space. However note that we also cut a point close to the minimum. Right: k is higher meaning we do not make a strong assumption of the smoothness of the curve. The cut-out regions are very narrow.\n\nIn practice, we often focus on hyper-parameter tuning because we take for granted the success of SGD-based algorithms. It is undeniable that Adam (and related algorithms) have proven successful on many tasks compared to vanilla SGD (see here for an introduction of different SGD-based algorithms), but we have to bear in mind that hyper-parameter tuning is only half of the story and we need optimisation algorithms that converge faster (in terms of wall clock time), more easily and which yield better generalisation performance.\n\nA recent work by Zhang et al. uses the k-Lipschitz continuity idea developed by Malherbe et al. that we just discussed and tries to apply it directly to the optimisation of the network weights. A key aspect of the original algorithm developed by Malherbe et al. is that it uses all the previous explored sets of parameters (W\u2081, \u2026, W\u2099) to sample W\u2099\u208a\u2081, contrary to standard gradient descent which relies only on W\u2099 and the gradient of the loss function (denoted f as previously) at W\u2099. This makes applying this method to train deep nets hard as it requires that we store n sets of weights where n can be in the order of thousands and one set of weight can be as large as 500Mb (as it is for VGG variants, for instance). Lastly, we can expect that exploring the entire space of millions of (even bounded) weights is going to be challenging (the work by Zhang et al. gives an upper bound to the number of samplings one needs to perform to achieve this. Not surprisingly, it is exponential in the number of network weights).\n\nTo mitigate these issues, Zhang et al. propose an algorithm called BPGradwhich combines the ideas of exploiting k-Lipschitz properties with standard gradient descent. Their method can be thought of as a gradient descent with adaptive learning rate. In this case, the learning rate is chosen such that the new sampled set of weights W\u209c\u208a\u2081 has the following properties:\n\nFigure 4. Visual interpretation of BPGrad algorithm. The x-axis represents the space of weights while the y-axis gives the training loss obtained by the model with a given set of weights. Left: we sampled f at W\u209c and obtained a loss L. D gives the distance between L and the best loss we obtained so far during our search (at W\u2097). Using the k-Lipschitz assumption, we could sample a new point at a distance D/k in the direction of the Lipschitz line (in red). Right: instead, we sample in the direction of the gradient at W\u209c (blue line) at a distance D/k. This gives the new weights W\u209c\u208a\u2081.\n\nZhang et al. report better testing performance compared to Adagrad, Adadelta, RMSProp and Adam on ImageNet (the classification error was reduced by at least 3.2%) and on PASCAL VOC2007 (the classification accuracy improved by at least 2.2%). Qualitatively, the training curves are shown to be more stable compared to other optimization methods such as Adadelta or Adam. Authors hypothesize that the Lipschitz-continuity assumption acts as a regularizer since it tends to ignore regions of sharp minima which have \u2014 by definition \u2014 local high k-Lipschitz coefficients.\n\nGiven that most of the optimisation algorithms rely on first-order information, it is not surprising that one of the most frequently attempted ways of improving optimisation is through the estimation of second-order information; in fact, this would help identify the optimal step to be used in gradient descent (i.e. the inverse of the Hessian matrix). However, we all know that computing the Hessian matrix directly and inverting it is currently not feasible. To overcome this issue, approximation techniques are usually adopted. Explicit approximations, such as the ones in the Levenberg-Marquardt algorithm, are still too expensive for deep learning applications. For this reason, implicit approximations are being explored. For instance, Krishnan et al. have recently proposed a method called Neumann optimizer that implicitly computes the inverse of the Hessian of each batch to produce better descent direction. Interestingly, this approach allows to use larger batches for training (up to 32,000) with no loss in validation error compared to smaller batches (which the authors define to be less than 512 images), and no increase in the total number of steps (with speed gains in the order of 10\u201330%). With smaller batches, their optimiser improves the validation error in the order of 0.8\u20130.9% on classification tasks.\n\nFigure 5. Training and evaluation curves for the Neumann and RMSProp (baseline) when optimising Inception-V3 on IMAGENET for the image classification task. Image from Krishnan et al.\u2019s original paper.\n\nThe main idea is to use limited but very useful second-order information without explicit approximations of the Hessian matrices. On each mini-batch, the descent direction is computed by solving an intermediate optimisation problem which is based on the Neumann series expansion for the matrix inverse and an observation that allows the authors to replace each occurrence of the Hessian with a single gradient evaluation. The authors show that on large-scale experiments (e.g. ImageNet classification task), the new optimiser has better validation performance than Adam and slightly better than RMSprop, as shown in Figure 4.\n\nIn this blog post, we have discussed some issues of deep neural network optimisation and reviewed some of the latest advances in this fundamental topic. Such advances represent valuable contributions to the end-goal of achieving fast, stable and well-performing optimisation algorithms for both weights and hyper-parameters. As we know, fast training (which ensures low training error) does not necessarily correlate with low testing error, producing a non-negligible generalisation gap. Trying to understand the generalisation gap and how it relates to architecture, performance and the training procedure is therefore extremely important. We will cover this related topic in our next blog post.\n\nMalherbe, C. and Vayatis, N., 2017. Global optimization of Lipschitz functions. arXiv preprint arXiv:1703.02628.\n\nPascanu, R., Dauphin, Y.N., Ganguli, S. and Bengio, Y., 2014. On the saddle point problem for non-convex optimization. arXiv preprint arXiv:1405.4604"
    },
    {
        "url": "https://medium.com/onfido-tech/how-we-devops-onfido-2fb8e69a1b83?source=---------4",
        "title": "How we DevOps @ Onfido \u2013 Onfido Tech \u2013",
        "text": "I\u2019m Harvey and I lead our DevOps team. Our team ensures that our engineering teams can deploy and run their services on a platform that\u2019s stable, reliable, secure and scalable.\n\nWe\u2019re tight-knit with our security team so we\u2019re pretty much a DevSecOps team \u2014 and that\u2019s what makes working in this team unique. Richard, Director of Security, and Pawel, Security Engineer, ensure we implement best practices in everything we do \u2014 they\u2019re involved from the genesis of a new implementation to the deployment. They help us think deeply about threat models and our security posture.\n\nHere\u2019s a high-level view of the platform we run for Onfido\u2019s production service:\n\nServices developed by our engineering teams are deployed, using Jenkins, on top of a foundation built on AWS and Kubernetes, configured and provisioned via Terraform and Ansible, and monitored with Datadog, Sentry and ELK.\n\nOur work falls into several larger themes and, at any one time, we\u2019re working on different projects across them.\n\nIt\u2019s vital to know what\u2019s happening with our infrastructure and applications and have rich data to diagnose any issues. As our infrastructure and product grows, we\u2019ve got to have a great understanding of the live system.\n\nWe work hard to design for resiliency and redundancy, but ultimately, software can be fragile. It\u2019s our job to make sure we\u2019ve got the data to put things right rapidly if (and when!) they do go wrong.\n\nWe use a lot of tools to keep us informed:\n\nWe\u2019re always looking for ways to improve the depth of data we have available and to use that data for investigation and self-healing.\n\nOur engineers want to get new features into production as pain-free and as quickly as possible, so deployment tooling is a big part of our job. We\u2019re not there to deploy services for other teams, but to give other teams the tooling that enables them to deploy rapidly and reliably.\n\nSince we rolled out Kubernetes and Jenkins Pipeline, we\u2019ve brought our deploy times for taking a brand new service to production down from several days to a few hours; and we\u2019re hoping to improve that even further! Being able to deploy new services that rapidly is crucial for microservices work, where a service is a primary unit of change.\n\nThere\u2019s a lot of work that goes into making sure that our services are always \u201cproduction ready\u201d. We\u2019re continuing to invest energy in minimising single points of failure within our infrastructure, ensuring it\u2019s deploying across multiple zones, that auto-scaling rules are appropriate, that scale-out and -in are rapid and that this is cost-efficient: using the right blend of reserved, on-demand and spot instances.\n\nAs we\u2019ve grown, we\u2019ve diversified our technology stack (from a humble Ruby on Rails app toward a significant focus on machine learning), entered new markets, grown the size of the engineering team and run millions of checks. These pressures led us to start to slice apart more of our services into independently deployable services.\n\nIt was natural to use Docker to package these services, and eventually, we selected Kubernetes as a container scheduler to ease running containers in production (this was in late 2016). We now run all production services on Kubernetes, but that\u2019s taken a lot of work:\n\nWe wanted a more secure way of protecting our secrets. Using a combination of AWS KMS and a command line tool (k8s-warden), we encrypt our secrets locally before pushing them to S3. We add an entrypoint for warden in our base Dockerfile, so when a service is deployed, it is able to decrypt the secrets using an AWS role for KMS decrypt at runtime. This also means the secret is stored encrypted within Kubernetes, which means that even if someone gained access to our k8s cluster, they wouldn\u2019t be able to expose our secrets.\n\nOriginally, we ran isolated environments within separate VPCs inside the same AWS account. We weren\u2019t really happy about this, and wanted to completely segregate our Production stack away from others (e.g. Staging, Management\u2026)\n\nThis has a number of benefits:\n\nThe accounts are completely configured using Terraform so we can easily replicate stacks for different environments and be sure they are built in the same way. Our Security team are happy as accounts and access are segregated and our Finance team are happy too, we now know exactly how much we spend on development!\n\nRunning terraform on your local machine is all well and good but what if you wanted to automate this process? We shifted all of our terraform projects over to jenkins-pipeline so changes can be peer reviewed in our source control (bitbucket) then deployed once approved \u2014 automatically. This means we have logs of each terraform plan and apply and can track changes. We don\u2019t need to worry about which version of terraform we are running locally anymore!\n\nThat\u2019s not all \u2014 we\u2019ve got some big projects in the works for the rest of 2018, including multi-region deployments, data pipelines, machine learning industrialization, changing how we do continuous delivery, improving our hack day project and more!\n\nWe\u2019re sharing this because we\u2019re looking for people that want to help us solve some of these problems. There\u2019s only so much insight we can fit into a job advert so we hope this has given a bit more and whet your appetite. If you\u2019re keeping an open mind about a new role or just want a chat \u2014 get in touch or apply \u2014 we\u2019d love to hear from you!"
    },
    {
        "url": "https://medium.com/onfido-tech/securing-logstash-with-aws-kms-3278709280ae?source=---------5",
        "title": "Securing Logstash With AWS KMS \u2013 Onfido Tech \u2013",
        "text": "At Onfido, we deal with a significant amount of data on a daily basis.\n\nWe store our data in the cloud, and while it\u2019s obvious that the data at rest must be secure, the same goes for data in transit. In fact, data in transit is a lot more dangerous since it can go in routes you\u2019ve never imagined or thought of before getting to its destination and one of the routes could definitely be a hacker just waiting for you to be frivolous.\n\nWe have built a robust and self-served data pipeline so anyone who wants their data to be stored in a secure way which is easily accessible (if you have the right credentials) can do so. It is also built assuming the person who stores the data is not necessarily a security ninja so all of the security concerns must be taken into account for them.\n\nOur data pipeline architecture roughly looks like this:\n\nTo ensure that data in transit is indeed secure, we needed to encrypt the data in client-side before logstash pushes it into S3. This seems like a reasonable, legitimate request and indeed it is. The only concern we had was that all of the existing logstash plugins for encrypting data (at the time of writing) were based on a static cipher key, which, although still hard to crack if using best practices, is not the same as using a dynamic key management service that can take all of the load in managing and storing the keys from us.\n\nOur production systems are deployed in AWS, so it was only natural for us to choose AWS KMS.\n\nThis led us to to develop and recently open-source the logstash-filter-cipher_kms plugin. This plugin is a logstash filter plugin, written in Ruby, which can cipher and decipher any data being transferred through logstash using AWS KMS and all of its advantages. That way, as long as we have a well defined mechanism to control our keys creation, deletion and rotation, we can easily use a key alias which will always point to the current key in use."
    },
    {
        "url": "https://medium.com/onfido-tech/how-to-create-products-customers-and-their-customers-love-99e884aab69a?source=---------6",
        "title": "How to create products that customers (and their customers) love",
        "text": "Throughout my career as a product manager, I have had the privilege to work on many different products, helping resolve a wide scope of user problems. In this blog post, I am going to explore some of the challenges working with B2B2C products. I will be using the term customer to refer to businesses that are buying a B2B2C product, and end-user to refer to users of such businesses.\n\nWith a B2B or a B2C product, the customers are the end-users. Having direct access to the end-users means one can relatively easily understand who they are, and what their typical pains and gains are. A product team would typically meet their existing or target users to carry out interviews, prototype testing, MVP testing, etc. Valuable signals also come from direct customer complaints and feedback.\n\nWith a B2B2C product, customer discovery is also relatively straightforward. The team can figure out the industry sectors and the use-cases the product wants to serve. They can then test their ideas with existing or target customers, and listen to the customers\u2019 feedback. However, end-user discovery is much harder for three reasons:\n\nThe identity verification product that we provide at Onfido falls under the B2B2C category, so I will be reflecting on our experience, and sharing what the team and I have learnt. As a quick intro, Onfido provides an identity verification solution to help customers effectively and confidently onboard end-users. For example, mobile-only banks are required by law to remotely verify their end-users\u2019 identities to be KYC/AML (\u201cknow-your-customer\u201d, \u201canti-money-laundering\u201d) compliant, and online car-rental companies need to verify their end-users\u2019 identity documents, including driving licences, to prevent vehicle theft. Onfido verifies identity documents, and compares them against the persons\u2019 facial biometrics, to establish their identities. We also provides an SDK (software development kit) to allow customers to seamlessly embed a frictionless document and face image capture/upload experience into their own products, e.g., inside their end-user onboarding flow.\n\nWhen customers evaluate our product, they often consider a number of factors, including:\n\nThe buying decision would likely involve multiple parties including representatives from product, engineering, UX, compliance and operation. All these make the Onfido product, and in particular the SDK, extremely complex to manage and to develop. Our team has to constantly consider the customer needs (e.g., integration, fraud) as well as the end-user needs (e.g., intuitive UI). This leads to two interesting problems:\n\nTo build products loved by end-users, the team must try to understand who they are, and to build empathy with them. I am going to explore some possible means to do this.\n\nWorking directly with customers to understand their end-users\n\nOften, forward-thinking customers like to work with companies with whom they can form long-term partnerships, and here at Onfido we have seen many successful examples of that. A product team should be encouraged to work directly with customers\u2019 product/UX teams throughout the relationship to understand their end-user flows, to dissect what motivates or frustrates their end-users.\n\nThis close interaction does not mean the team should tweak the product to suit a particular customer\u2019s needs. Good product management often involves absorbing a decent amount of diverse signals, and distilling all those to create products that resonate with the market. What this customer partnership unlocks is the ability for the team to expand the product thinking beyond the limited scope of what their product provides. Understanding what the end-users may be thinking about before they enter the Onfido document image capture experience, and what they may be asking after submitting the document images across a number of scenarios is absolutely essential. Empathising with these common pain points is the only way to deliver a meaningful experience to the end-users.\n\nAs different customers adopt a B2B2C product, the end-user makeup is going to evolve, so it is going to be difficult to profile end-users based on attributes like age, technical proficiency, etc. However, this does not mean the product should not have user personas. From working directly with customers and researching customers\u2019 products, the team should be able to discover end-users\u2019 concerns. Behavioural personas based around these specific concerns should be built and validated. An example is around data privacy. As Onfido is fundamentally dealing with sensitive personally identifiable information, end-users have the right to be concerned about their data privacy. Personas reveal different levels of concerns and varying motivations, which can inform product decisions.\n\nProduct knowledge has to come from both qualitative research and quantitative data. In the case of B2B2C products, where end-user access is more restrictive, data is even more crucial. The team needs to build robust tracking into the product to collect anonymised usage data and monitor funnel performance to understand why end-users are dropping off, or getting stuck within the product flow. If customers are concerned about tracking, the team need to explain to them the benefits and also help them analyse the impact of tracking on their own data privacy as well their end-users\u2019 personal data privacy. The team can also consider sharing insights with the customers, as well as collaborating with their product/UX team to improve the product flow.\n\nUsability testing with the right people\n\nThere is no better way to empathise with end-users than to watch them struggle with a product! While throwing the product in front of any users will always reveal flaws in product/UX assumptions, selecting the right audience and setting the right context will allow the team to get the most out of these sessions. Again, this is where B2B2C development presents additional challenges, as end-user access is more restrictive, and end-user audience is less well-defined.\n\nThe team can however simply select a few customers, research their products, and pick candidates who are likely to be end-users of such customer products. During testing sessions we run at Onfido, it is important for us to set the right context by articulating the circumstances under which the candidates may need to verify their identities via our product, e.g., when hiring a car or opening a bank account. It may not be a bad idea to run these tests using the actual customer products too, if they are easily available (and if the candidates agree to it!) In addition, it is always worthwhile testing competitors\u2019 offering. In this case, the team can simply repeat the above exercise, but with products using competitor services.\n\nFinally, remember to refer to the behavioural personas to make sure certain end-user types are not getting ignored.\n\nVision alone won\u2019t deliver products unless the right priority calls are made. Prioritising customer value against end-user value is never straightforward, as there are so many contributing factors. While the effectiveness of a product is largely defined by whether the end-users can successfully use the product (no customers want to buy products that frustrate their end-users), there are equally many important customer-centric requirements, such as those around integration and compliance, that are going to ultimately drive sales. Sometimes customer value will be aligned with end-user value; at other times alignment will not be so clear.\n\nI certainly do not pretend I have the answer to this problem. Instead, I would like to highlight a few points of consideration to help product managers make informed decisions to try to strike the right balance (and yes, I do think it is about getting the right balance at the end of the day).\n\nBeing somewhat removed from end-users, a B2B2C product team often does not hear their frustrations. Someone in the team \u2014 perhaps the product manager, or the designer \u2014 needs to act as the voice of the end-user. The voice needs to be loud and clear, and it should be based on facts, such as those found in usability testing and data analysis, and not opinions.\n\nDoes the team consider end-user experience a product differentiator? If so, make it explicit, as it is not going to get its fair share of investment otherwise. It is worth thinking about the Kano model. Delighter features can sometimes result in high user satisfaction, and over time, delighter features are just going to become expected features.\n\nCustomers\u2019 first impression of a product is critical, and often they only get the product after seeing a demo. In most cases, excellent end-user experience will contribute to a better customer demo experience. Therefore, even though the customers are not going to be the end-users, end-user experience can have a significant influence on their buying decision.\n\nWhile still looking for product-market fit, the team will have to do whatever they can to engage customers in order to gain valuable learning and feedback, to help guide the next stage of product development. This is not to say that customers will not care about end-user experience. However, the reality is the team will be inundated with customer requests and market signals, and there will be an urgency to act on some of them quickly. It requires great discipline and clear goal alignment to figure out and agree on what is tactical, and what is required for long-term product success.\n\nThe single thing that all highly-motivated and highly-functional teams have in common is their sense of pride. Working on a B2B2C product is incredibly exciting because the product\u2019s end-user reach is going to explode as the customer base scales. The impact of the product can be immense, and hopefully many people\u2019s lives will be enhanced as a result.\n\n\u2026 and finally, if this sounds great, we\u2019re hiring!\n\np.s. special thanks to Daniel Spagnolo, Charlotte Sferruzza, Dawid Dylowicz and Susana Videira Lopes for ideas and feedback!"
    },
    {
        "url": "https://medium.com/onfido-tech/the-not-so-magic-tricks-of-testing-in-elixir-2-2-acdd0368572b?source=---------7",
        "title": "The (not so) Magic Tricks of Testing in Elixir (2/2)",
        "text": "In the first part of this post we described various approaches to test Elixir applications, and ended it by pointing out some of the shortcomings of the approaches mentioned, particularly when we need to create mocks. Let\u2019s recap them here:\n\nLately at Onfido we\u2019ve been using the Mox library in some of our Elixir projects, to solve both issues presented above. Mox was created by Jos\u00e9 Valim, about four months ago. It follows the principles described in his famous blog post Mocks and explicit contracts (by the way, a highly recommended read), and allows the creation of concurrent mocks in Elixir. Its principles are:\n\nWe will now see how we would test the module described in the first part using the Mox library. First of all, we add it as a dependency:\n\nThen we have to define our mock. This is usually done in the file, although you can also define them in a block. In this case we\u2019ll do it in :\n\nNow we need to change the test itself to start using the library:\n\nFirst we import so that we can use its functions without using the fully-qualified name. In the test itself, we pipe the mock defined in to the function. This way we\u2019re expecting that will be called on our mock module. When this happens, the lambda that is passed as third argument is the function that will be executed. Moreover, we\u2019re using pattern match on the argument of the lambda function, which means that we\u2019re expecting the function to be called with . If it doesn\u2019t, the match will fail, along with the test. The body of the lambda function just sends a message to the (which returns the PID of the current process), and then we use to check that we have that message on the mailbox of the current process.\n\nLastly, we use to ensure that the expectations that we defined on our mocks are met. You can also use or if you want to control when the verification is made, and don\u2019t want to do it when the test exists.\n\nWe\u2019re essentially following the same strategy as we did in the first part of this post (of sending a message to ), but now, by using , we have some noteworthy benefits:\n\nIn the first part there\u2019s a section where we talk about what happens when the module that we\u2019re testing is creating a new process before producing the side-effect we\u2019re looking to confirm. We changed the mock module to a , so that we can keep track of the list of processes we want to send messages to. Let\u2019s see how we\u2019d achieve the same thing using :\n\nThe test itself remains the same, but we had to change two things:\n\nTo wrap-up this post, I want to emphasize that, using , you can\u2019t create mocks that aren\u2019t based on behaviours, as its first principle states. This means that you won\u2019t be able to create a mock for a third-party library if it doesn\u2019t define behaviours for its modules. This brings me to this phrase, well-known in TDD communities:\n\nThis phrase is a bit counter intuitive, since most of the mocks you want to create are to control interactions with external parties. But what it actually means is that you shouldn\u2019t just rely on mocks for things that are out of your control.\n\nLet\u2019s explore this further with an example: say that you use the library to access Twitter. In order to follow this principle, you\u2019d create a wrapper around the ExTwitter library, and this wrapper will define the contract between your application and the ExTwitter library. It\u2019s in this wrapper module that you\u2019ll also define the behaviour. Then, the mock that you\u2019ll create is for the wrapper, and not for itself. This wrapper will most of the times be a subset of the library.\n\nThis change may cause some friction at first, especially if you\u2019re used to creating ad-hoc mocks, but remember that this will hugely increase the mid- and long-term maintainability of your application. Furthermore, starting to use will surely spark some healthy discussions inside your team about testing in general, and mocks in particular. Take this opportunity to define how and when you\u2019ll use mocks (also taking the Test Pyramid into account), which will ensure that you have a consistent approach to testing throughout your application(s). In turn, this will make your tests easier to follow and also help on the onboarding of new team members.\n\nI hope you\u2019ve enjoyed this post about testing in Elixir. See you next time!\n\nNote: This post is an adaptation of a talk I gave at Lisbon |> Elixir meetup."
    },
    {
        "url": "https://medium.com/onfido-tech/the-first-steps-improving-inclusivity-and-diversity-at-onfido-751bb480282f?source=---------8",
        "title": "The first steps: improving Inclusivity and Diversity at Onfido",
        "text": "Where do you start?\n\nDiverse companies are successful companies. I\u2019ve seen the value of having a diverse workforce whilst previously employed at the BBC and GDS. But diversity is a huge area and there\u2019s so much to do \u2014 so where do you start?\n\nTrying to find the answer to that question resulted in me delaying ever really pushing initiatives at Onfido. I convinced myself there was always something much more urgent and important to do first.\n\nThen, in October 2017, I went to HR Tech World in Amsterdam. One of the talks I went to see was Unruly\u2019s CEO and Cofounder Sarah Wood. She spoke about diversity in the workforce and one the key takeaways was:\n\n\u2018There are thousands of things you could do for diversity\u2026just start by doing one. Just get on with it. Don\u2019t wait.\u2019\n\nVery wise words \u2014 thank you Sarah!\n\nWhat\u2019s the one thing we started with?\n\nData. I started on our diversity data because if you can measure it, you can improve it.\n\nWe didn\u2019t have readily available diversity data on the following two groups:\n\nWe created an internal opt-in diversity questionnaire for the first and worked with GapJumpers to help us analyse the second.\n\nBoth were interesting. They showed that our diversity of nationalities and languages are impressive but gender and BAME representation, particularly in our Technology Group, are a problem.\n\nWhat did we do next?\n\nWe decided to focus on one diversity group to start with. We chose gender as it was one of our biggest problems.\n\nIn a previous blog about People-related KPIs at Onfido, I mentioned that Technology is one of our four Groups. A tier below that are our Functions. In the Tech Group these consist of Product, Design, Engineering, Research, Automation, Infrastructure, Security and Support.\n\nCurrently only 11% of Onfido Tech workers are women. Though that\u2019s low, only 17% of Tech/ICT workers in the UK are women, and most global tech giants have between 15\u201319% women engineers. While Facebook\u2019s intake was an impressive 27% last year, only 1 in 10 women are currently taking A-Level computer studies, despite the looming digital skills gap the UK needs one million more tech workers by 2020 (source: Tech Talent Charter, 2017).\n\nThere is no silver bullet, and we know we need to run multiple initiatives to reach our desired outcome. We\u2019re starting with:\n\nOutcome 1: A standard platform for measuring diversity data of applicants and employees.\n\nMeasured by: 100% of Onfido diversity for Applicants and Employees is able to be tracked by the end of Q1.\n\nMindful that the questionnaires are always opt-in, the \u2018able to be tracked\u2019 rather than \u2018is tracked\u2019 was a deliberate choice.\n\nWorkable isn\u2019t currently considering creating a Diversity Questionnaire for UK applicants. That\u2019s because not many people apart from me have asked for it (please do contact them to ask for it if you think it\u2019s a good idea). So I\u2019m going to create one and use a tool like Zapier or IFTTT, plus Google Forms or TypeForm, to help with this (if anyone has already created something that works for them, I\u2019d love to hear from you!).\n\nOutcome 2: Improve D&I for women in the Technology Group at Onfido\n\nMeasured by: 100% agree that actions taken in Q1 have had a positive impact on life at Onfido for women in the Technology Group.\n\nWe\u2019re going to create a focus group for the current women in our Technology Group. We\u2019ll follow that up with an inclusive Diversity Squad where together we suggest, agree and commit to impactful actions that will work towards making Onfido a more inclusive environment than it already is.\n\nOutcome 3: Improve D&I for Women in Tech Recruitment at Onfido\n\nMeasured by: More women at the top of the funnel\n\nThere aren\u2019t enough. The data showed us that clearly. All candidates will be continue to be assessed objectively and meritocratically.\n\nWe\u2019ve already committed to a number of actions that may directly and indirectly help:\n\nMeasured by: 100% of employees have signed up to the Code of Conduct by the end of Q1.\n\nThis Code of Conduct will encapsulate our inherent culture at Onfido. It will be a commitment from the team member to their colleagues and from their colleagues to the team member to behave in a way which is inclusive and supportive.\n\nWhat might that include? The incredible Giles Turnbull wrote this piece at GDS on \u2018It\u2019s ok to say what\u2019s ok\u2019."
    },
    {
        "url": "https://medium.com/onfido-tech/the-team-behind-tech-support-at-onfido-1d47cdd1a6d8?source=---------9",
        "title": "The team behind Tech Support at Onfido \u2013 Onfido Tech \u2013",
        "text": "We have 3 amazing members in the Tech Support Team at Onfido: Giulia, (Support Lead), Alan and Goncalo (Support Engineers), split across London and Lisbon.\n\nTech Support is a core part of day-to-day client interactions, making it a very unique environment and dynamic role to support almost everyone in the business.\n\nWe work closely with the Account Management and Software Engineering teams to assist with any technical support queries. This could be anything from helping our customers integrate with our API and SDKs to providing support to our internal staff when using our products. We also work very closely with our Product team, providing them with regular feedback about new product improvements.\n\nOur main goal is of course, to make sure that our customers are successful with Onfido and that our products meet their needs. We typically support customers integrating with our API and SDKs when we sign them on. This can be complex yet fun to work with as you are exposed to different client scenarios and use cases. For example, we help our customers to understand the lifecycle of a check and the right API calls to use for their workflow, addressing and solving any technical issue that they may find throughout the process. We also help them integrate with the Onfido API in different coding languages, as well as with our SDKs (JS, Android and iOS). As another example, we debug and if possible implement hotfixes/features in the application code as well.\n\nOverall, we are here to solve customer issues and contribute to their success and user experience when they use our ID verification solution.\n\nWe like to think that Tech Support is at the heart of our product.\n\nWe help customers at every single point of interaction with our product and mediate with our engineers to continue to make new improvements and product recommendations.\n\nHaving varied exposure across software and commercial teams, Tech Support have a huge influence over how we shape our product for future customers, giving the opportunity to contribute towards how we help solve customer ID verification pain points.\n\nWe also work on internal projects to automate common support queries and service processes. For example, we recently built a web app to automate running batch trials for our prospective customers. We have also built a metrics system to identify and monitor product issues and client requirements: this helps us feedback this information to our Dev and Product teams to make sure that we\u2019re focussed on the right client and business needs.\n\nAnd we have recently started working on developing an improved iOS sample app to integrate with our iOS SDK, which can be used for internal testing as well as to reproduce customer issues.\n\nOnfido\u2019s engineering culture is tech agnostic. Amongst other tools, our team uses JIRA, SQL, Rails, curl, Postman and any of our current libraries (written in Ruby, PHP and Python) to support our customers and reproduce any issues our clients may have.\n\nUnlike other tech support roles, we focus on people with a strong technical background and are growing our team as we onboard more customers!\n\nThis role is very much hands-on, so if you enjoy diving into code, enthusiastic about new technologies and enjoy communicating with people, we\u2019d love to hear from you!\n\nIf you\u2019d like to apply or learn more about the role, follow the link here"
    },
    {
        "url": "https://medium.com/onfido-tech/adversarial-attacks-and-defences-for-convolutional-neural-networks-66915ece52e7",
        "title": "Adversarial Attacks and Defences for Convolutional Neural Networks",
        "text": "Recently, it has been shown that excellent results can be achieved in different real-world applications including self driving cars, medical image analysis and human face recognition. These breakthroughs are attributed to advances in Deep Neural Networks (DNN), as well the availability of huge amounts of data and computational power. Characteristic examples of these breakthroughs are self driving cars which are so reliable that they no longer need human drivers inside as a backup; systems that are better than human experts in detecting cancer metastases; and face recognition software that is capable of surpassing human capabilities. But despite these impressive results, the research community has recently shown that DNNs are vulnerable to adversarial attacks.\n\nAn adversarial attack consists of subtly modifying an original image in such a way that the changes are almost undetectable to the human eye. The modified image is called an adversarial image, and when submitted to a classifier is misclassified, while the original one is correctly classified. The real-life applications of such attacks can be very serious \u2013for instance, one could modify a traffic sign to be misinterpreted by an autonomous vehicle, and cause an accident. Another example is the potential risk of inappropriate or illegal content being modified so that it;s undetectable by the content moderation algorithms used in popular websites or by police web crawlers.\n\nAt Onfido, we are developing state-of-the-art machine learning systems in order to automate a plethora of different problems, including identity verification and fraud detection. For that reason, we\u2019re very interested in understanding these attacks and developing our own defences against them. To this end, three members of our research team recently attended the 2017 Conference on Neural Information Processing Systems (NIPS) in Long Beach, which is considered the most prestigious venue in the field of Machine Learning. This year, the most relevant NIPS event on this topic was the Competition on Adversarial attacks and Defences organized by Google Brain. Here, I\u2019ll summarize some of the most common attacks and defences, as well as the winning methods in the competition.\n\nAn adversarial image is an image that has been slightly modified in order to fool the classifier, i.e., in order to be misclassified. The measure of modification is normally the \u2113\u221e norm, which measures the maximum absolute change in a single pixel.\n\nIn white box attacks the attacker has access to the model\u2019s parameters, while in black box attacks, the attacker has no access to these parameters, i.e., it uses a different model or no model at all to generate adversarial images with the hope that these will transfer to the target model.\n\nThe aim of non-targeted attacks is to enforce the model to misclassify the adversarial image, while in the targeted attacks the attacker pretends to get the image classified as a specific target class, which is different from the true class.\n\nMost successful attacks are gradient-based methods. Namely the attackers modify the image in the direction of the gradient of the loss function with respect to the input image. There are two major approaches to perform such attacks: one-shot attacks, in which the attacker takes a single step in the direction of the gradient, and iterative attacks where instead of a single step, several steps are taken. Three of the most common attacks are briefly described next. The first two are examples of one-shot attacks, and the last one is an iterative attack.\n\nThis method computes an adversarial image by adding a pixel-wide perturbation of magnitude in the direction of the gradient. This perturbation is computed with a single step, thus is very efficient in terms of computation time:\n\nSimilarly to the FGSM, in this method a gradient step is computed, but in this case in the direction of the negative gradient with respect to the target class:\n\nThe iterative methods take T gradient steps of magiture \u03b1 = \u03b5 / T instead of a single step t:\n\nBoth one-shot methods (FGSM and T-FGSM) have lower success rates when compared to the iterative methods (I-FGSM) in white box attacks, however when it comes to black box attacks the basic single-shot methods turn out to be more effective. The most likely explanation for this is that the iterative methods tend to overfit to a particular model.\n\nBoosting Adversarial attacks with Momentum (MI-FGSM) was the winning attack in both non-targeted and targeted adversarial attacks competition.\n\nThis method makes use of momentum to improve the performance of the iterative gradient methods, as described in the following algorithm.\n\nThe results show that this method outperforms all other methods in the competition and shows good transferability results, i.e., it performs well in black box attacks as seen in the figure below.\n\nThis method uses the gradients of the previous t steps with a decay of \u00b5 and the gradient of the step t+1 in order to update the the adversarial image in the step t+1. The results show that this method outperforms all other methods in the competition and shows good transferability results, i.e., it performs well in black box attacks as seen in the figure below.\n\nIn order to produce effective attacks against ensemble defence methods, i.e. methods that use a number of different base classification models, a modification to the original algorithm is proposed in which the logits of all the target models are fused before computing the combined cross-entropy loss:\n\nThe most common defence consists of introducing adversarial images to train a more robust network, which are generated using the target model. It has been shown that this approach has some limitations \u2014 in particular, this kind of defence is less effective against black-box attacks than white-box attacks in which the adversarial images are generated using a different model. This is due to gradient masking, i.e., in these kind of defences, a perturbation in the gradients is introduced, making the white box attacks less effective, but the decision boundary remains mostly unchanged after the adversarial training. An alternative approach has been proposed, in which the generation of the adversarial examples is decoupled from the parameters of the model being trained. This is achieved by drawing the adversarial samples from pre-trained models, which are then added to each batch or used to replace part of the non-adversarial images in the batch.\n\nThe High level representation guided denoiser was the winning submission on the defences track. This solution is built on the observation that despite adversarial perturbations being quite small at the pixel-level, they are amplified throughout the network, producing an adversarial attack. In order to target this challenge, several higher-level denoisers are proposed: a feature guided denoiser (FGD), a logits guided denoiser (LGD) and a class label guided denoiser (CGD). All three methods use a denoising network (DUNET) which is similar to a denoising autoencoder and uses a network structure similar to UNET, i.e., has direct connections between corresponding layers in the encoder and decoder. A fixed pre-trained convolutional neural network (CNN) is also used to guide the training of the denoiser. The FGD uses the responses of last feature layer of the CNN (for the original and denoised images) in order to guide the denoiser. The LGD uses the logits activations of the CNN, and finally the CGD uses the classification output.\n\nThe final submission uses an ensemble of four feature guided denoisers (FGD). Each one is trained using adversarial samples from 14 known attacks and one of four pre-trained CNNs (ensV3, ensIncResV2, Resnet152, and RestNet101). The final prediction in done by averaging logits activations for the four networks.\n\nThe official results can be found here."
    },
    {
        "url": "https://medium.com/onfido-tech/a-chip-off-the-monolith-ec71e06a3015",
        "title": "A Chip off The Monolith \u2013 Onfido Tech \u2013",
        "text": "This post is about how we at Onfido recently chipped out a React SPA from a Rails/Ember monolith, with no feature/code-freeze.\n\nMost companies have monoliths that they regret. We all know the pattern: code is added to one repo at crunch-time because it\u2019s easier. Splitting out a separate pipeline would take longer, so we resolve to clean things up later\u2026 More features are added, and the clean-up is pushed back until we have a little more time\u2026 Until one day, we finally look up to see the monolith we\u2019ve created, only to notice it\u2019s now beginning to blot the sun. Deploy times take hours. Teams you\u2019ve never met block your releases with issues in their part of the monolith. The 20 minute coffee-break you take to let your unit-tests run on feature branches no longer cuts it.\n\nAt Onfido, we aim to split our code by 2 dimensions: horizontal splits, splitting a product by the technology involved (e.g., front and back-end); and vertical splits, splitting a product by services provided (e.g., separating out log-in functionality, microservice-style).\n\nWe recently split one of our codebases horizontally, away from the Rails monolith where it used to live, over to its own repo. This new shiny, independent codebase lives as an SPA, communicating with the monolith via an API.\n\nAs will be the case with most everyone, we couldn\u2019t afford a lengthy feature-freeze. We knew that whatever we did, it would have to be alongside active development on the codebase. This initially sounded like a daunting task, but in the end the whole conversion was relatively painless!\n\nOur codebase was originally Ember & Rails, but is now React & Redux \u2014 so not everything may apply directly to your use-cases, but hopefully our advice here should be transferable for any codebase trying to split away to a React application.\n\nHere\u2019s how we did it.\n\nOur first step was creating a hybrid app (chimera, Frankenstein\u2019s monster \u2014 whatever you want to call it, it\u2019s temporary!). A great feature of React is its interoperability, which Facebook explicitly put into the design spec so that they could perform a gradual refactor of their site to use the framework.\n\nLeveraging this, we began to port over components that were common on a lot of pages over to React.\n\nFor us, this meant porting our old Ember components over to React, and injecting them into the Ember app. We used the following code to inject in our React components into the Ember application:\n\nWe\u2019re currently doing a similar thing in a pure Rails codebase, and use the following code to do the same thing in our ERB templates:\n\nThis lets us start chipping away at the views, moving us over to our new framework. But we can\u2019t port our app architecture using this method, only the view layer. Which brings us on to\u2026\n\nOur next step was creating pure React/Redux pages, powered by their own API requests to the backend (rather than needing their data passed in via props). Creating new pages is easy enough to do, but we needed a way to transfer the user between these 2 different \u201capps\u201d. With a bit of nginx config, so that each of the different apps (React/Ember) has their own unique url structure, we added the following config:\n\nSo any reference to the new pages on the old app, will redirect to the new app. And any references to an unknown page on the new app, will redirect to the old app. (Unknown rather than old, so that the old app is still handling 404s).\n\nOr described with a picture, an architecture something like this:\n\nThe benefit of using different url structures to get to the React/Ember apps is that the nginx config doesn\u2019t need to update every time a new page is ported.\n\nAnd with a bit more config to add feature-flags, dynamically passing only certain users over to the newer routes, giving us the following:\n\nIt\u2019s worth noting that maintaining 2 versions of pages is non-trivial. This is certainly the largest drawback that we encountered in the whole process. Maintaining 2 versions of a page means duplicating any feature-requests and bug-fixes for that page \u2014 which for more complex issues, can require solving the same problem twice.\n\nOur goal here is to get to a new repository, that is separate from the monolith. So our next step was to do just that, so \u2014 even before all of the pages were ready \u2014 we created the new repo and copied our new React app over to it.\n\nAt this stage, we continued to allow feature and bug tickets merge into the old monolith repository. We just wanted to get the pipeline set up in the new repository, and test that the built assets are pushing to S3 correctly.\n\nOnce the pipeline was all set up, the new S3 buckets etc. were all correctly configured, and all of the old app\u2019s pages were ported over to the new app \u2014 we were finally in a position to do The Final Big Swap.\n\nAt this point we finally had to implement a \u201ccode freeze\u201d. In our case we didn\u2019t actually freeze the codebase at all, and rather just picked a time of low activity in the codebase, with the intention to cherry-pick any few commits that did go in during the \u201cfreeze\u201d period. The swap was fortunately very quick, so there were only a handful of these commits that we needed to cherry-pick.\n\nThe config changes required to swap over to your new codebase will depend on your site architecture, but for us it just involved rolling out our DNS changes, so people were pointing to our new S3 buckets, rather than our old monolith boxes.\n\nFinally came just keeping an eye on our logs and metrics to ensure that nothing was going awry.\n\nAnd that\u2019s a wrap! After the dust had all settled\u2026 And the logs seemed okay\u2026 We looked up at our monolith, now a chip smaller."
    },
    {
        "url": "https://medium.com/onfido-tech/live-computer-vision-with-opencv-on-mobiles-f4e5ab15ad48",
        "title": "Live computer vision with OpenCV on mobiles \u2013 Onfido Tech \u2013",
        "text": "Solving problems and making a solution immediately ready for our clients is a big part of our day to day work. In this blog post, written by me and Zhiyuan Shi, we share a quick story about how we discovered a problem, researched it, solved it with a prototype algorithm, and finally connected and deployed this algorithm to production devices across multiple platforms.\n\nIn general, the whole process involves four steps:\n\nDiscovering the problem is what drives us to design a better solution. The following figure illustrates a common problem when a user takes a photo of their document. Glare accidentally appears in the camera spoiling both face recognition and verification. Therefore, helping the user detect the glare on the document would improve capture experience and overall pass rate on Identity Verification (IDV).\n\nResearching and solving the problem aims to propose an optimal and feasible solution to the task, while iterating over prototypes closely with the design team to gain usability insights (see https://medium.com/design-onfido/glare-detection-our-journey-to-help-users-take-higher-quality-photos-9bf656e6d304 by our product designers). Python is an extremely powerful language for rapid prototyping and proof of concept. We adopt Python to implement candidate algorithms benefiting from the ease-of-use of various statistical, machine learning and numerical libraries such as NumPy, Scikit-Learn, TensorFlow, SciPy, Matplotlib, etc. After some efforts on preliminary explorations, there are two main paradigms to tackle this challenge. We can either treat it as a binary classification problem where classifiers can be trained with a Deep Neural Network, Support Vector Machine, Random Forest, etc. or solve it with low-level computer vision and image processing techniques. Considering that the first way requires a massive amount of training data and human annotations, we adopt the second idea to build the initial solution.\n\nIn low-level computer vision, detecting glare is actually equivalent to finding high intensity regions. Let\u2019s assume we have some piece of code of basic image process operations to detect glare as follows:\n\nOptimising and finalising the algorithm (in C++) making the deployment become tractable on mobile devices, especially across multiple platforms. Below, we follow similar steps in python version and reimplement the algorithm in C++. Most of the code can be directly transferred by applying slightly different declaration, parameter feeding. To generate the executable, you need to compile the project using CMake with a CMakeList file.\n\nConnecting and deploying the algorithm to devices\n\nAfter the code is made available on a Git repository, it is ready to be consumed by the Android SDK. To make that happen, we chose to use Git Submodules, which allows you to keep a Git repository as a subdirectory of another Git repository. This way, we can include the native code in our project while keeping the commits separate. We chose to create a new folder under where our native code repositories were cloned to.\n\nBoth these submodules are written in C++, and using as the build tool, which is an advantage since it\u2019s one of the two supported build tools for native libraries (along with ndk-build) and also Android Studio\u2019s default one, making the integration as easy as possible.\n\nTo setup and other needed dependencies, you can access Android Studio\u2019s SDK Manager, and under the SDK Tools tab, download three of them: CMake, LLDB (debugger) and NDK (Native Development Kit).\n\nAt this time, you might be wondering why we need to actually compile the native code along with our Android code, instead of just including the pre-compiled library and use it out-of-the-box \ud83e\udd14 . The answer is that Android environment comprises a multitude of devices, with a wide range of CPUs, which in turn support different instruction sets \ud83d\ude35. For each combination of CPU and instruction set, there is one ABI (Application Binary Interface). The ABI controls, precisely, how an application\u2019s machine code should interoperate with the system at runtime. Actually, in your project you must specify one ABI for each CPU architecture that you want your code to run on, which is made in our case under the file on the SDK\u2019s module. So, in order to support different ABIs, the native code must be compiled once for each target that you want to support.\n\nIn this case, we want to support 4 different ABIs:\n\nAfter defining our target ABIs, it\u2019s time to define our , which is basically a for . Once again, we should add a block to our file (outside of the one), with its location.\n\nAn example of the file is presented below. It should define the minimum version to be used, along with some other properties, required to link our submodules with the project.\n\nSo after these steps our project structure became something like the figure below, and it is now time to use JNI (Java Native Interface) to implement the bridging code which allows the Java code to interact with the C++ algorithm.\n\nOur algorithm was made available by the research team and can be used as a function , part of the class, which receives the binary of an image coming from the mobile device\u2019s camera and returns a boolean on whether the image contains glare or not.\n\nSo to make the algorithm available to the Java code, we need to wrap this call inside a JNI function, in this case inside a file called (the library we declared before). Also, since the camera frames come in full-size and not only containing our region of interest (the document part itself), we will need to pass some more parameters like the frame dimensions and both the position and dimensions of the region of interest (our document rectangle, relative to the origin of the frame).\n\nThe picture above represents the JNI header for the function we want to build, so let\u2019s break it down:\n\nSince our algorithm is written in C++, we need to pass C++ parameters to it. Fortunately, Java primitive types are directly mapped to JNI primitives, which in their turn are automatically mapped into their C++ equivalents, leveraging the integration effort.\n\nNote: A header file under the same name pattern must be created for to avoiding C++ compiler-specific name mangling of the native methods. In this case, a was created with the following content:\n\nOur algorithm, developed by the research team, is using OpenCV (https://opencv.org/), an open source computer vision library used to manipulate images, necessary to perform the algorithm steps on the camera frames. Also, it allows us to perform image decoding and cropping, which is useful for our bridge code. It has mobile SDKs but for performance reasons we chose to use the C++ SDK.\n\nThis said, our bridge code implementation works as follows:\n\nSince OpenCV uses objects to represent images, we need to create two of them (one for decoding the data and another for the conversion). This way, the matrix will be used to decode the and the to hold the final result after conversion, which is achieved using the function from OpenCV.\n\nIn order to call the native libraries\u2019 methods from Java, we need to load these libraries, making use of the call and the keyword on the method declaration, stating that the implementation of this method is not present because it will be provided by a library loaded at runtime.\n\nFrom now on, will execute the algorithm and return the result we explained above.\n\nThe end goal is to periodically run the algorithm and present the results to the user in a visual UI, through some UI element. However, the Android camera API doesn\u2019t have a frame callback with dynamic periodicity, but one for every captured frame instead.\n\nTheoretically, we could run it on every frame, but on any simple 30-fps camera it would become impossible to present this information in an understandable and eye-friendly way, given the high sample rate. This way, we had to find a way to control the camera frame rate.\n\nSince our use case can be well modelled by a sequence of observable frames, we chose to use the benefits of reactive programming, and concretely RxAndroid, a set of RxJava bindings for Android. This library implements and extends the observer pattern and offers a set of operators for composing asynchronous and event-based programs. From all its features, we were really interested in the backpressure handling, to solve the camera frame rate problem, and asynchronous computation, because on Android development the main thread should be relieved from processing heavy tasks that can be done on any other worker thread and mainly focus on rendering what we see on the screen. Given that, our choices were as follows:\n\nAfter that, the subject can also be subscribed to, in order to receive the reemitted items.\n\nSince we want to update our UI on every item emitted (alerting the user that glare was detected if such or hiding this alert if not), results must be observed on the , which is the only thread allowed to touch the UI on the Android framework. Finally, we defined some lambdas (thanks Kotlin!) to specify what should happen on every item emitted (which is passing this result to the view), or in case any error occurs along the stream (logging the error cause).\n\nIn the end, we present the user an inflation animation of a view which looks like a speaking balloon, to catch the user\u2019s attention. This animation takes ~300 ms, which is Android guidelines recommended time for an animation medium duration. Also, for the case where glare was detected but solved, the animation will reverse, making the balloon disappear. Anyway, nothing like checking it with your own eyes.\n\nWe hope you now have an idea of how we apply fast prototyping to solve challenges combining computer vision research and mobile development. Also, this post intends to show how useful JNI can be to perform costly tasks like image manipulation on a mobile environment, and also as an invitation for every developer to give it a try.\n\nThat\u2019s it. See you next time!"
    },
    {
        "url": "https://medium.com/onfido-tech/the-not-so-magic-tricks-of-testing-in-elixir-1-2-89bfcf252321",
        "title": "The (not so) Magic Tricks of Testing in Elixir (1/2)",
        "text": "As shown by Andr\u00e9 Albuquerque\u2019s post, Onfido is betting big on Elixir. It has been a great journey, shifting our mindsets from Object-Oriented to Functional Programming. It\u2019s incredible to be able to leverage the decades of engineering work that have been put on the BEAM VM, and it\u2019s even better to top that up with the productivity tools that Elixir gives us.\n\nHowever, one of the main pain points we\u2019ve felt, when making this transition to Elixir, is related with testing. Tests are an integral part of any application. They not only serve as great living documentation, but are also our safety net when we want to refactor our applications. Thus, it is paramount to be able to test Elixir applications properly. In this post I\u2019ll describe some of the obstacles we\u2019ve found in this odyssey, how we\u2019ve come across them and how we currently test Elixir applications at Onfido.\n\nDisclaimer: What I\u2019ll be talking about only applies to ExUnit. You can use other test frameworks (such as espec) and you probably won\u2019t feel the pains that I\u2019ll be describing here. While I\u2019m not directly advocating for ExUnit, I think it\u2019s extremely valuable to stop and think about some design decisions behind it \u2014 especially when you\u2019ve been using testing frameworks that are very different.\n\nThis is a very famous phrase in the testing community. To follow this principle, instead of trying to remove the duplication in the tests (and follow the Don\u2019t Repeat Yourself principle), one should rather have Descriptive And Meaningful Phrases on the tests. The importance of this is twofold:\n\nLet\u2019s see this principle in action with a code sample:\n\nIn here we have two blocks, which share a similar set-up phase. They both call the function. When looking at this sample, most developers will have the urge to remove this duplication, and put an outer context with the set-up block.\n\nWhile this may seem harmless in this simple example, it becomes a huge problem as your application gets bigger and bigger. The goal here is that you should be able to look at a single test and instantly realise what\u2019s going on, without having to jump around just to understand what\u2019s this test doing (if you come from Ruby like me, I\u2019m pretty sure you\u2019ve felt the pain of jumping into a project, look at the specs and have a brain stack overflow just trying to figure out all contexts ,and shared contexts relevant to that test).\n\nThis descriptiveness is actually enforced in ExUnit, since you can\u2019t create nested blocks (Jos\u00e9 Valim explains the rationale behind this here). While this looks like an insignificant design decision, it\u2019s a concrete example of Elixir striving for the long-term maintainability of a project (and thus long-term productivity of its developers).\n\nI believe that most of the difficulties that I\u2019ve felt can be explained by the differences in philosophy regarding testing.\n\nDetroit-school TDD is the classical one, created by Kent Beck and others in the 90s. This type of TDD tries to maximize the regression safety net introduced by tests, and does that by minimizing the use of test doubles. However, this will inevitably lead to redundant coverage (and all the problems that come with it). Also, the design feedback is weaker when practicing this type of TDD.\n\nIn the London-school TDD, we isolate all the dependencies and focus only on the subject under test. For this reason, followers of this school usually think that these are the TRUE unit tests, since you\u2019re only exercising the code that\u2019s under test. This type of TDD usually yields a higher design feedback, but they have to be complemented with integration tests, to ensure everything is glued together as it should.\n\nWhen talking about Functional Programming languages, it\u2019s more usual to see practitioners of the Detroit-school TDD, since we strive for pure functions whenever possible (and thus minimize side effects), and it\u2019s common in a unit test of a certain module to just let functions from other modules run freely.\n\nSince I\u2019m a practitioner of the London-school, I want to create mocks even when the module/function being called doesn\u2019t have any side effects. This was the biggest pain I\u2019ve felt, since I wanted to have mocks but also run my tests concurrently. Throughout the rest of the post I\u2019ll describe the strategies I use to test Elixir applications.\n\nIn her talk in 2013, Sandi Metz comes up with the following matrix that describes how to create tests that aren\u2019t redundant (if you haven\u2019t watched this talk I highly recommend it!).\n\nThe Incoming Query and Command do not require the creation of mocks, but I\u2019ll quickly show an example of each one. Then, the Outgoing Command is where things get a bit trickier, and that\u2019s where we\u2019ll need to create mocks.\n\nLet\u2019s say that we are creating tests for this module:\n\nThis is an address validation module, which just validates that a certain address has less than 32 characters. Also, let\u2019s say that the function emits an event (our side-effect). This module also reads and writes to the application configuration \u2014 this will not be used and it\u2019s just here to serve as an Incoming Command, but it could describe the level of validation we would apply (e.g. a validation would do more than validate the address length).\n\nAs we can see in the matrix depicted above, in this case we just want to assert on the result of the function that\u2019s being tested. This means that we simply run the function and set the assertion according to the provided arguments. This one was really easy!\n\nIn this case we want to test the direct public side effects of running the command.\n\nNow we\u2019re not testing the function but the function. We run the function under test, and then observe the public side effects of running this command, by running the function. This is what we\u2019re interested in this type of tests. Note that in this case, since the message is incoming, you should NOT expect to receive with the right arguments, as this would be leaking the implementation details to the test. This one was fairly easy as well! Moving on to the last one.\n\nWe now want to make sure that the command gets called, with the right arguments. However, before touching the test code, we need to change our production code! We need to inject the dependency ( in this case). We do that by defining a new module attribute:\n\nand then use it on the function that emits events:\n\nNow, we\u2019re able to inject a mock module when running in a test environment (i.e. in we define the as ). Here\u2019s the code for the mock module:\n\nWe\u2019re essentially using messages between processes to ensure that the side-effect is being triggered. This process sends a message to itself (the first argument passed to ), which will allow us to check for it later in the test. Notice the pattern-match on the function argument, which ensures that the outgoing command was called with the right arguments.\n\nThis has the advantage of creating mocks with explicit contracts that are easy to reason about and put in context. The disadvantage of this method is that the test logic is spread out between files, which makes it harder to comprehend.\n\nWith this in place, the code in our test file is really simple:\n\nAs I\u2019ve mentioned earlier, we\u2019re using messages to test that the command gets called. As such, in the test we simply have to check the mailbox of the current process, and assert that we received the expected message.\n\nHowever, this approach stops working when the code under test is spawning a new process, which is fairly common in Elixir. For instance, our could be behind a worker pool (such as poolboy), and this way our approach of sending messages to would no longer work.\n\nThe cleanest solution I\u2019ve found to this issue is to transform the mock module into a . Here\u2019s the code for our new mock module:\n\nNote: This GenServer is not implementing the client facing functions (that wrap the functions) because in this example we\u2019re just using directly on line 9 of the code example below.\n\nNow, the mock module keeps track of the processes that subscribed to its events. Then, upon receiving the call, it broadcasts the message to all of the subscribed listeners. This entails that the test process needs to subscribe to this . Actually, that\u2019s the only change that needs to happen in the test code:\n\nWe just need to define our before running the test. If we were indeed using the library, this is how we would subscribe to that mock module (assuming that was properly configured in ).\n\nIn this post we\u2019ve seen how Elixir has some interesting philosophy regarding testing, and also how we can properly test Elixir applications, particularly when there\u2019s mocks involved.\n\nIn the second part of this post we\u2019ll be discussing some of the shortcomings of the solutions I\u2019ve presented above, and how to overcome them. Namely:\n\nBe on the lookout for the second part!\n\nNote: This post is an adaptation of a talk I gave at Lisbon |> Elixir meetup."
    },
    {
        "url": "https://medium.com/onfido-tech/onfido-hack-day-pt-3-immunis-2f1513ac6713",
        "title": "Onfido Hack Day (pt. 3) \u2014 Immunis \u2013 Onfido Tech \u2013",
        "text": "This is the third in a series of blog posts about the projects developed during the Onfido Hackathon, in November. If you missed the previous parts, definitely check them out here and here.\n\nIn this post we are going to talk about Immunis, an object security service, meant to transparently scan all objects (files, documents, images, \u2026) uploaded to our services, analyse them for malicious content and if detected, block the files from being used in our pipeline. Similar to how the immune system would act upon external threats.\n\nThe traditional way to integrate an object security scanner (e.g. anti-virus) is to perform hard, in-line, scans and either drop, delete or quarantine an object if it is being detected as malicious. That is where we are right now and this approach comes with advantages like increased security as objects detected as malicious are stopped immediately, but it also introduces additional latency and potential failure points which could heavily impact speed and stability, and therefore availability. We wanted to find a way which has close to zero impact on speed but still provides a good level of security.\n\nSome of our objectives for the project included:\n\nWe had to find a way to access the file objects which are being uploaded with minimal change to our existing services. Some ideas we played around with:\n\nAs we were limited in time and also wanted to play around with new toys (golang in this case) we made the decision to prototype the second idea with a http/s reverse-proxy written in go interfacing with clamav as an object security analyser.\n\nThe actual implementation was pretty straightforward and consists of two parts.\n\nPart 1 is a docker container with the following installed: clamav and any dependencies, a small, 80-line, reverse proxy written in GO and go-bindings for communicating from the go service to the local clamav unix socket. It was important for us to be able to avoid writing the extracted object into a file but to be able to stream the bytes straight into the analyzer. This has multiple advantages. We don\u2019t store potentially sensitive information on any volume, not even temporarily, and we save time by skipping the volume and passing on the bytestream directly.\n\nPart 2 is a lambda function in AWS API Gateway which has a twofold use. First for marking files as malicious/infected and second for asking whether a specific file is identified as malicious and therefore should no longer be used in the pipeline. Each file is also identified with a checksum and all these \u2018marks\u2019 are stored in a DynamoDB table.\n\nThe flow of this setup looks like this:\n\nThis implementation comes with some risks. While we perform the asynchronous object security scans the object is already being processed by the first set of microservices in our platform. Those services can check the status of the object from the Immunis API. Depending on the robustness and resiliency of a specific service, make the decision to process it, before a Immunis result is available. Or alternatively, stall the processing of the specific object until the security analysis result is available. On the flipside, it allows us to run the security analysis in parallel while our Onfido services retrieve the file, perform some sanity checks, store it to an object storage and perform other standard tasks.\n\nFurther improvements for this system could include rewriting the GO proxy as an nginx module for even more flexibility in deployment and for decoupling the extraction of the object from the analysis itself. Also we could extend the checks done on each file to be more thorough than just an antivirus scan. For instance, we could also scan them with foremost to verify that no hidden files are appended and hand the objects over to binaryalert for yara rule scanning or tools which verify the structure (or EXIF data?) of jpeg, png images.\n\nThe hackathon was a great excuse to test some of the improvement ideas we had in our backlog and we can\u2019t wait to get this to production."
    },
    {
        "url": "https://medium.com/onfido-tech/the-kind-of-tester-were-looking-for-9d8757ff03db",
        "title": "The kind of tester we\u2019re looking for \u2013 Onfido Tech \u2013",
        "text": "For the past 3 years I\u2019ve been looking for the perfect blend of skills in a tester, and in that whole time I\u2019ve hired less than 3% of the people I\u2019ve interviewed. Those I have hired turned out to be amazing testers, and well worth the wait \u2014 but I can\u2019t help but wonder why there seem to be so few \u2018test unicorns\u2019 out there.\n\nI should probably give some context right about now. I lead test engineering at Onfido, a 150-strong early stage tech company that\u2019s looking for a type of Test Engineer very different from the norm, which explains why we\u2019re having trouble finding one.\n\nOne obstacle is taxonomy. The correct term for the \u2018test unicorn\u2019 we\u2019re looking for is Software Development Engineer in Test (SDET), as opposed to an automation engineer, or your run-of-the-mill QA. Very few companies use this name, however, and few individuals use the term correctly. Unfortunately, the term is often conflated with other types of testing \u2014 which is a damn shame, as it\u2019s supposed to describe a very specific gap in the industry that most companies would benefit from being filled.\n\nAt Onfido, we want to become a leading actor in a paradigm shift that can\u2019t come soon enough. Let me explain why:\n\nAlthough we might employ manual testing help from 3rd parties from time to time, we don\u2019t believe our core technology team should contain any manual testers. A team of manual testers, we feel, would be a bit divided from the rest of the engineering team; they wouldn\u2019t be speaking the same language. What we need are highly technical engineers who can interact on a deeper level with the rest of the team, and be able to have informed critical conversations about the state of development and deployment.\n\nWe\u2019re looking for someone who wants to spend 90% of their time writing code; who focuses on code quality; whose idea of fun is learning something new, and tackling problems that seem simple enough in concept but tricky in execution.\n\nWe believe having one skilled SDET \u2014 working on a feature, writing up automation for it and finding some bugs as they\u2019re poking through \u2014 is definitely preferable to a small army of manual testers that lift every nook and cranny, finding *all of the bugs*, but having to do it all over again tomorrow. Manual testers are great when they\u2019re there, but once they\u2019ve left there\u2019s no lasting legacy that ensures quality into the long-term \u2014 and there\u2019s also the problem of scalability. That\u2019s why we\u2019re looking for builders, not just maintainers.\n\nWhat\u2019s the latest piece of technology you\u2019ve integrated and why? How did you make sure things were better for your company even after you\u2019d left? What\u2019s one piece of functionality/process that you put in place in your workplace that now assures products are shipped with higher quality thresholds? These are the types of questions we like people to be able to answer.\n\nWe think about quality as another function of engineering. It\u2019s not just an issue that\u2019s pushed down to the end of the development process, or an afterthought for less technical departments to deal with \u2014 quality is integral in design, functionality, usability, and our own infrastructure.\n\nAs a SDET at Onfido, you\u2019ll be challenged to improve community standards and tools. We\u2019re working on lowering the entry barrier for writing tests, whilst increasing the types of systems we cover. What\u2019s the easiest way to test front-end systems? What about back-end systems? What about machine learning algorithms? What about microservices?\n\nYou can tell by now that we\u2019re highly ambitious about the type of people we want to hire. We\u2019re looking for full-stack SDETs \u2014 those who know how to write integration tests, but also know how to build a whole testing framework from scratch, set up CI, optimise our scripts, even trace a bug down to its line of production code and FIX it. For too long I believe testing engineers have been tier 2 engineers, but here at Onfido we believe quality is a worthy *engineering* problem that\u2019s core to improving our offering.\n\nAre these people we\u2019re looking for \u2018unicorns\u2019? Maybe not \u2014 we\u2019ve already found some, and they\u2019re the best of the best, la cr\u00e8me de la cr\u00e8me, the Twinkie in a zombie apocalypse!\n\nI\u2019m sure there are many like-minded others out there, too. Maybe there are some automation testers out there bored of just writing integration tests, and want to take it to the next level. Maybe there are software developers who would love to specialise in testing, but don\u2019t feel like the industry is ready for it. Well, we are!\n\nIf you\u2019re our next SDET, please apply for the following positions:"
    },
    {
        "url": "https://medium.com/onfido-tech/the-best-of-iccv-2017-b029b7ad4ab0",
        "title": "The Best of ICCV 2017 \u2013 Onfido Tech \u2013",
        "text": "Computer vision experts from academia and industry gathered in Venice in late October for the 16th International Conference on Computer Vision(ICCV 2017). I visited this year\u2019s ICCV on behalf of Onfido\u2019s research team. In the following I will share some of my experience of the conference as well as a review of the best papers of this conference; namely, Mask R-CNN and Focal Loss.\n\nStarting in 1987, the International Conference on Computer vision (ICCV) has established itself as one of the most important events in the field. The 16th ICCV 2017 was held in Venice, Italy between 22nd and 29th of October.\n\n2143 valid submissions were made this year, which marks an increase of 26% over the previous edition of the conference in 2015. Out of all submitted papers, a total of 621 papers were accepted (overall acceptance rate 28.9%) with 45 oral presentations (2.09%), 56 spotlight papers (2.61%), and 520 posters (24.26%).\n\nAs customary, the conference was also accompanied by a number of co-located events including a record number of 44 workshops (63% more than the previous edition), 9 tutorials, a doctoral consortium, industrial exhibitions, and demos.\n\nOn the first day, I attended a tutorial on Generative Adversarial Networks, organised by Ian Goodfellow. There was many insightful talks by Ian Goodfellow, Sanjeev Arora and Alexi Efros, among others. I particularly enjoyed David Pfau\u2019s talk on the Connections between adversarial training and Reinforcement Learning.\n\nIn the second of the conference, I attended a tutorial named Beyond Supervised Learning for some very interesting talks by Jitendra Malik, Vladlen Koltun, Michal Irani, Alexei Efros and others.\n\nThe best paper award went to a paper by Kaiming He et al. Their paper named Mask R-CNN addresses the problem of instance segmentation.\n\nInstance Segmentation can be thought of as the combination of semantic segmentation and object detection. More specifically, the aim is to label, at pixel level, every instance of an object in an image.\n\nAt a high level, the Mask R-CNN performs instance segmentation by combining the state-of-the-art in object detection and semantic segmentation. The best modern object detection frameworks are based on the two-stage framework of Regions with CNNs (R-CNN) proposed by Girshick et al. R-CNN splits the task into two stages: The first stage produces a number of region proposals using a process known as Selective Search which starts with an over-segmentation of the input image and combines similar regions into object region proposals (see here for more information about selective search). The second stage of R-CNN then classifies each region proposal into an object or background by using a CNN to extract deep features and a set of per-class SVM classifiers. A bounding box regressor is then used to regress a new tight bounding box for the object from the CNN features.\n\nThe R-CNN approach was later enhanced by Girshik in the Fast R-CNN paper which includes multiple enhancements to R-CNN to improve its speed. The R-CNN framework is slow partly because it needs to run a forward pass of a CNN for every single region proposal. This results in thousands of passes per image but many of these would be overlapping, so many of the forward passes are redundant. Instead, the Fast R-CNN framework only does one pass of convolutional feature extraction, and pools the features within each region proposal into a fixed size feature vector using a process known as RoI Pooling. An RoI in this context is a rectangular window into the a convolutional feature map. RoI Pooling works by dividing the h\u00d7w RoI window into an H\u00d7W grid of sub-windows of approximate size h/H \u00d7 w/W and then max-pooling the values in each sub-window into the corresponding output grid cell. The RoI Pool layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets in which there is only one pyramid level.\n\nNote that RoI Pooling includes quantising the real valued RoI location, x, to the nearest location on a coarse grid, [x]. Similar quantisation is performed when dividing the RoI into bins. This quantisation introduces misalignment between the RoI and the extracted features. This is illustrated in the figure below:\n\nAnother insight used in Fast R-CNN was to combine the multiple tasks performed by R-CNN into a single multi-tasking network. The architecture of the Fast R-CNN network is shown in the figure below.\n\nThe network first processes the whole image to produce a convolutional feature map. Then, for each object proposal an RoI Pooling layer extracts a fixed-length feature vector from the feature map. Each feature vector is fed into a sequence of fully connected layers that finally branch into two sibling output layers: one that produces softmax probability estimates over all classes to replace the SVMs in R-CNN and another layer to regress the refined bounding boxes directly from the same features.\n\nThe Fast R-CNN framework still uses Selective Search to find region proposals. This is a rather slow process. In Faster R-CNN, Ren et al. replaced this mechanism with a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.\n\nFollowing the string of innovations in the R-CNN family of algorithms, we can now look at how Mask R-CNN builds on and adapts these innovations to achieve state-of-the-art in instance segmentation. As mentioned previously, instance segmentation is essentially object detection plus semantic segmentation. Mask R-CNN extends the Faster R-CNN object detection framework by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition.\n\nThe mask prediction branch predicts an m\u00d7m mask from each RoI using a Fully Convolutional Network. During training, a multi-task loss on each sampled RoI is defined as L = L_cls + L_box + L_mask. The classification loss L_cls and bounding box loss L_box are identical to those used by Fast R-CNN and Faster R-CNN. The mask branch has a K\u00d7m\u00d7m dimensional output for each RoI, which encodes K binary masks of resolution m\u00d7m, one for each of the K classes. A per-pixel sigmoid is applied to this output, and L_mask is defined as the average binary cross-entropy loss. For an RoI associated with ground-truth class k, L_mask is only defined on the k-th mask. Other mask outputs do not contribute to the loss. This definition of the mask loss decouples mask and class prediction by allowing the network to generate masks for every class without competition among classes, in contrast to the common practice of applying Fully Convolutional Network to semantic segmentation where masks across classes compete.\n\nIn order to achieve good results for instance segmentation, Mask R-CNN also makes another alteration to the Faster R-CNN architecture in order to ensure pixel to pixel alignment of the masks. As previously mentioned, the Fast R-CNN and Faster R-CNN approaches use RoI Pooling to pool the convolutional features within each region proposal into a fixed size feature vector which effectively results in quantisation of the RoI boundaries, leading to poor segmentation results. To address this problem, the authors proposed a simple, quantization-free layer, called RoI Align, that faithfully preserves exact spatial locations, to replace the RoI Pooling layer. Instead of rounding the real valued boundaries of the RoI to nearest location on a coarse grid, as done in RoI Pool, the RoI Align method uses bilinear interpolation to compute the values of the input features at four regularly sampled locations in each RoI bin, and aggregates the result (using max or average). This way, RoI Align avoids any quantisation of the RoI or the Bin locations. This is illustrated in the Figure below:\n\nRoI Align seems like a very simple, even trivial change. However, the authors report significant improvement to their results in ablation studies. The figure below illustrates some results with Mask R-CNN.\n\nFor more results and experiments as well as application of Mask R-CNN to other tasks such as human body keypoint detection, see the original paper.\n\nThe best student paper award of the conference was also awarded to a paper by FAIR. The paper \u201cFocal Loss for Dense Object Detection\u201d by Lin et al. looks at object detection using a single stage method.\n\nThe R-CNN family of object detection methods reviewed above (i.e. R-CNN, Fast R-CNN and Faster R-CNN) are examples of two-stage algorithms where a classifier is applied in the second stage to a sparse set of candidate object locations generated in the first stage. Such two-stage methods have consistently outperformed their single-stage counterparts, such as YOLO and SSD, which are applied over a dense sampling of possible object locations. Such single-stage algorithms have lower accuracy but can potentially be faster and easier to train.\n\nThe authors of the Focal Loss paper argued that the low accuracy of the single-stage detectors is primarily due to a large imbalance in the background and foreground object samples during training. More specifically, they observed that a large number of object candidate locations are in fact background patches which are easily classified, but still receive a considerable loss during training under the standard cross-entropy criterion.\n\nThey argued that when using the cross-entropy loss, the cost resulting from the large number of \u201ceasy\u201d background examples in the training set can dominate the overall cost. This would not be a problem for two-stage detectors since the first stage would already filter out many of the easy background examples by Selective Search (as in R-CNN and Fast R-CNN) or using a Region Proposal Network, as in Faster R-CNN. To address this problem, they proposed adding a term to the loss which changes the loss function such that the easy examples receive a significantly lower loss compared to the harder foreground examples. Concretely, defining the cross-entropy loss as CE(p_t) = - log(pt), where:\n\nand p is the model\u2019s estimated probability for the foreground class, the authors formulate their loss as:\n\n\u03b1_t in the above equation is a weighting factor to balance the foreground and background classes. The main contribution of the paper, is the addition of the term (1 \u2212 pt)^\u03b3 in the loss function. For \u03b3>0 this term changes the loss function such that the loss received by the well-classified samples (p_t > 0.5) is considerably smaller while the loss received by the harder examples (p_t < 0.5) does not change much.\n\nTo evaluate the effectiveness of their proposed loss, the authors designed and trained a simple dense detector, called RetinaNet, shown in the figure below. It is made of a feedforward ResNet followed by a Feature Pyramid Network (FPN) to extract rich, multi-scale features. This is in turn followed by two branches, one for classification and one for bounding box regression.\n\nIn extensive ablation studies, the authors show that RetinaNet trained with the Focal Loss outperforms a similar model trained without it and was fairly robust to the exact values of \u03b1_t and \u03b3. Furthermore, RetinaNet trained using the proposed Focal Loss managed to outperform all single-stage and two-stage detectors on the bounding box detection task of the COCO challenge. See the original paper for details of their experiments.\n\nThis post reviewed the best papers of ICCV 2017. We also briefly reviewed the developments in the recent years that provided the basis for the Mask R-CNN work. In summary, some insights can be drawn from the collection of works reviewed here:\n\nThe success of the papers reviewed in this post confirms once again that designing powerful deep learning algorithms is not just about having lots of data and compute power. It is also important to fully understand the problem at hand and adapt the approach to the requirements of the specific problem."
    },
    {
        "url": "https://medium.com/onfido-tech/onfido-hack-day-pt-2-dbb9695d9f92",
        "title": "Onfido Hack Day (pt. 2) \u2013 Onfido Tech \u2013",
        "text": "This is the second in a series of blog posts about the projects developed during the Onfido Hackathon earlier this month. If you missed part 1, definitely check it out here.\n\nIn this post we are going talk about a project that makes deploying, testing and experimenting with new features, changes or fixes super easy on Kubernetes, as close to the staging environment as possible.\n\nMost of our services at Onfido run on top of Kubernetes, so it made sense to have at least one hack related to it. To get a high level overview of how we run our services and operate Kubernetes, check out a post I made earlier this year.\n\nThe inspiration for the project came from reading a blog post on how GitHub recently revamped their development environment to run on Kubernetes and how each branch can be deployed with one click. So we asked ourselves: \u2018Why don\u2019t we have this? One-click deploy of any branch would be a super cool feature to have at our disposal\u2019. So we decided to build it.\n\nThe hackathon starts, we gather our team and we start brainstorming. We currently have automated builds and deployments for development and master branches, pushing to staging and production respectively. For new features, changes and fixes, developers commit to their own branches, build and test locally and when everything looks good, they open a pull request to merge into development.\n\nOur initial incentive for the project was to make local service testing obsolete by giving any team the power to deploy and test any branch with one click and have an isolated environment created for them with a unique URL for their branch. As we thought about the different components of the project, we started realising that this could be so much more than trying to solve our complicated and unsynchronised local environment. This means that any new features, changes and fixes would be tested and run in an identical environment to staging (very hard to replicate locally), with access to the same environment variables and configurations. This could also improve the quality of new releases and we could finally get rid of the \u2018works on my machine\u2019 syndrome.\n\n20 hours of coding later, we have the hack mostly completed, split into 5 components:\n\nThe full workflow (using the frontend) in action:\n\nJust to note before we go into details, we\u2019ve setup a service (website-demo) that serves our website statically. On another branch (feature/hack), we commited and pushed a change to the title and subtitle of the main page of the website.\n\nTo simplify the workflow for our terminal loving developers, we\u2019ve also created a deploy script that can be run from the git repository path, automatically finds the repository and branch names and calls the deploy API endpoint (we bypass steps 1\u20133 above).\n\nCheck out part 3 of the series here."
    },
    {
        "url": "https://medium.com/onfido-tech/onfido-hack-day-pt-1-f184397aad2d",
        "title": "Onfido Hack Day \u2014 Orwell \u2013 Onfido Tech \u2013",
        "text": "On November 9th, Onfido held a 24-hour long hackathon. Engineers, designers and product managers formed teams and competed for four awards: most useful hack, most viable, most ambitious and most innovative/creative. We knew we were all in for some tough competition, but it sure was fun!\n\nThis is the first in a series of blog posts, which will describe some of the projects that came out of the hackathon \u2014 the tech used, programming language, framework or library.\n\nTo kick things off, we\u2019ll start with a project which aimed at easily identifying Onfido employees. We could leverage this work to unlock our facilities, similar to what we achieve with a fob nowadays.\n\nFor face recognition, we decided to go with an out-of-the-box solution. We went with Amazon Rekognition \u2014 we had wanted to try it since it was first released, and the hack day seemed like the perfect excuse. Team Orwell had one front-end (Gautier), one mobile (S\u00e9rgio) and two back end developers (Andr\u00e9 and Daniel). The aim was to have an Android mobile app identifying Onfido personnel in real-time with face-tracking technology and a web app that served as an identifying photo booth. Both of them would be communicating with the face recognition back end we were building.\n\nFor the web app, we used React. In order to gain time (this was a hackathon after all) we used create-react-app, an npm library allowing us to create the project with different tools already pre-configured (webpack, babel, jest\u2026).\n\nThe beauty of React is the intrinsic independence of each react component. We used the react-webcam component to capture the user\u2019s selfie, something we hadn\u2019t tried before. It was super easy to use and in less than 5 minutes we had the front-end app uploading base64-encoded images. Since React is not really a framework, it doesn\u2019t have everything baked in. For that reason, client-server communication was dealt with using Axios, a \u201cpromise-based HTTP client for the browser and node.js\u201d.\n\nAnother use case for our application was populating the database with a new individual given a photo, first name, last name, date of birth, etc. For that, we leveraged the simple FormData interface.\n\nOn the mobile side, we found in Google\u2019s Play Services Vision all we needed to put in place a real-time face-tracking system. It offers callbacks for when a new face is detected on the camera preview, and it even supports multiple faces simultaneously.\n\nWe had some challenges from the get-go: the back end was only ready to handle a single face match per selfie, while the Android device detected multiple faces. Our heuristic was to use the largest face on the screen for the identification attempt. This was possible because our face-detection mechanism returned the position and size of the detected faces on the mobile screen.\n\nSo at the time a new face was detected (let\u2019s call it T), we should send the next frame we received after T + delay, in which delay = 750 ms. This is due to the fact that the face detection mechanism would detect faces too early, while they were not yet properly centered on the camera or front-facing it, resulting in poor comparisons when sent to Amazon Rekognition. Also, whenever a new face was detected, and while we were waiting for the back end response of whether it was recognized or not, we would present a question mark inside a colored circle to the user, indicating the recognition was in process.\n\nFinally, if the face was recognized, the question mark would turn into a check sign and the circle would become clickable to show the identified user\u2019s details.\n\nThe back end was a simple Sinatra app which used the aws-sdk-rekognition gem to communicate with Amazon Rekognition. We had an endpoint to upload a new individual to our system and another endpoint for subsequent identification within our search space.\n\nTop problems were agreeing on a format that served both the React app and the Android app (base64-encoded image with some specificities for each, no biggie), implementing a custom endpoint to our internal image retrieval service (Imago) so we could get employees\u2019 selfies via its API, and the non-obvious need to delete photos after identification had been carried out. In hindsight, we think this could well be an option in the Amazon API (i.e., if we want to keep the photo we\u2019re searching for a match against in our Rekognition collection or not).\n\nUnfortunately, we didn\u2019t get to the part of actually integrating our face recognition system with a piece of hardware capable of unlocking the front-door of our offices, but we\u2019re confident with a little bit more work that would be achievable. The hackathon was a good way to experiment with some tech we\u2019ve been meaning to trial. We\u2019re already planning for the next one!\n\nIn part 2 of this series, we will take a look at Kubernetes Unicorns. Stay tuned!\n\nPart 2 is now available here."
    },
    {
        "url": "https://medium.com/onfido-tech/machine-learning-at-onfido-230389103d3d",
        "title": "Machine Learning at Onfido \u2013 Onfido Tech \u2013",
        "text": "I often get asked what Onfido does? And how do we use machine learning? Before I get started on answering this I should introduce myself. My name is Jacques Cali and I am the Director of Research at Onfido.\n\nRecently we hosted our first public tech event at the luxurious office space of Salesforce in the Heron tower. The event was a success and will lead to future gatherings; moreover it gave us the chance to introduce the types of problems that we are working on and some of the ways we are going about trying to solve them. But first a quick primer.\n\nOnfido aims to build trust in an online world by remotely verifying user identities; in doing so we hope to reduce client friction, reduce costs for businesses and ultimately reduce fraud or malicious activity. The way we do this is by verifying your legal identity through the means of a government issued identity document such as a passport, driving license or national identity card. Together with a selfie (video or image) that we match against your document we can validate your legal identity.\n\nThe challenge arises when we need to do this at scale and across different geographies. If we think about that last point alone; the UN recognises 195 different countries in the world, each of these has their own passport, driving license and sometimes national identity card. Of each of these there are multiple versions in circulation and some vary depending on the state or region. In total there are an estimated 6,000 different documents being used to cross borders, open bank accounts and prove right to work, drink or drive. It doesn\u2019t take much to imagine that being an expert on each type of document is almost impossible, and doing so at scale i.e. training people to become document experts on over 6,000 documents is certainly impossible; so what do we do?\n\nArtificial intelligence (AI) refers to apparent intelligent behaviour by machines, often viewed as the ability to \u201clearn\u201d some behaviour or the ability to \u201cproblem solve\u201d. In our case we are interested in decision problems; is this the same person as the one in the document? Is this a legitimate document? In order to create a system that is able to make such complex decisions we use a branch of AI called machine learning (ML).\n\nWhen we talk about machine learning our goal is to create a model (system) that is able to make a decision based on some prior knowledge. Therefore as machine learning experts we design or choose models that should work well for the type of task at hand and then feed them with previously observed data until we are confident that we have modelled our problem and now have a system that is capable of generalising to unseen examples in the real world i.e. that it has \u201clearnt\u201d how to make a decision. So how and where do we apply such techniques?\n\nMost of our Research work focuses on two sections; Document Understanding and Face Biometrics. As such we have the following work streams:\n\nBut what\u2019s next and where do we go from here? If you would like to learn more, our next posts will be introducing more about our challenges and talking about a few practical tips. If that\u2019s not enough then we are working on introducing a dedicated page to our research that we will look to launch some time next year. In the meantime if these sound like the types of problems you would like to work on then please do get in touch or take a look at our careers page, we are always on the hunt for talent and we do like working with Research Interns."
    }
]