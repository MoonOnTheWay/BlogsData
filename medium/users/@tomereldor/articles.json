[
    {
        "url": "https://towardsdatascience.com/capsule-neural-networks-are-here-to-finally-recognize-spatial-relationships-693b7c99b12?source=user_profile---------1----------------",
        "title": "Capsule Neural Networks: The Next Neural Networks? Part 1: CNNs and their problems.",
        "text": "Convolutional Neural Networks (CNNs) are extremely flexible machine learning models which were originally inspired by principles from how our brains are theorized to work.\n\nThe primary building blocks of a Neural Network is a \u201cConvolutional\u201d layer (hence the name). What does it do? It takes raw information from a previous layer, makes sense of patterns in it, and send it onward to the next layer to make sense of a larger picture.\n\n \n\n If you are new to neural networks and want to understand it, I recommend:\n\nLet\u2019s start from the beginning. \n\n The Neural Net receives raw input data. Let\u2019s say it\u2019s a doodle of a dog. When you see a dog, you brain automatically detects it\u2019s a dog. But to the computer, the image is really just an array of numbers representing the colors intensity in the colors channels. Let\u2019s say it\u2019s just a Black&White doodle, so we can represent it with one array where each cell represents the brightness of the pixel from black to white.\n\nWhat is our goal? Our goal is for the net to make visual sense of what is in the picture (which is for it just a sequence of numbers). One way to do it is bottom-up: start by looking at small groups of pixels, make sense of them (like small lines and curves: the curve of the dog\u2019s ear, the small circle of its pupil, the small line in its leg), then build those up to bigger groups of these lines and curves (like: ear, nose, snout, eye), make sense of bigger groups of these shapes (like: face, leg, tail), and finally make sense of the entire picture of the dog.\n\n \n\n It does so with many layers transferring information in a sequence from one to another. \n\n If this was new to you, see my summary of Convolutional Networks Structure here: Understanding Convolutional Neural Networks.\n\n \n\n In case you didn\u2019t read it but it\u2019s still new to you, below is an even shorter summary of that summary:"
    },
    {
        "url": "https://towardsdatascience.com/data-reveals-what-makes-a-ted-talk-popular-6bc15540b995?source=user_profile---------2----------------",
        "title": "Data Reveals: What Makes a Ted Talk Popular? \u2013",
        "text": "First, you should be aware: don\u2019t take this (or almost any other conclusion from observational data) as a causal inference. This wasn\u2019t an experiment and not rigorous enough to prove causation. Observations aren\u2019t matched between control and treatment groups or various subgroups of the data, and even after the regression, the variables\u2019 explanatory power isn\u2019t rigorous enough. The available numerical parameters I had in hand are not sufficient for that kind of a conclusion; I couldn\u2019t match the content that really matters to compare apples to apples, and even with controlling with multiple regression \u2014 not all things are equal (ceteris paribus assumption is still not met). However, I was able to get a decent predictor and understand which variables are most strongly associated with higher view counts.\n\nThe dataset includes the name, title, description and URL of each of the 2550 talks, name and occupation of the main speaker, number of speakers, duration of the talk, the TED event and date it was filmed at, date it was published online, number of comments, languages translated and views. It also included a compressed form of arrays of associated tags, ratings, and related talks, but as inside arrays and these need transformations before they can be used. For a full list, see the dataset page on Kaggle.\n\nFirst, let\u2019s see how each variable is distributed with their histograms.\n\nHistograms are a way to represent the distribution of the values this variable takes; or intuitively representing: \u201chow many talks were there of 1 minute? 2 minutes? 3 minutes? etc\u2026\u201d. Formally, a histogram is a \u201cdiagram consisting of rectangles whose area is proportional to the frequency of a variable and whose width is equal to the class interval\u201d.\n\nSee how our variables look like below.\n\nDuration of talks is closer to a normal distribution, but with a wide right-side tail of a few talks at longer duration, around a mean of 14 minutes and median of 12 minutes. Almost all talks range between 1\u201318 minutes (maximum length of a normal Ted talk).\n\nNumber of comments is a Poisson distribution (visually resembling an exponential distribution, but it is not technically since comments are measured at discrete numbers) strongly skewed to the minimum of 0 comments for the unpopular videos.\n\nNumber of views is also a Poisson like distribution skewed leftwards, With a Median number of views: 1124524, and Mean (average) number of views: 1698297\n\nFinally, an overall picture of our variables distributions:\n\nFrom these distributions we see that:\n\nApparently, writer is the most common occupation for a TED speaker! followed by other creative occupations and number of speakers with that occupation:\n\nThese occupations are very different! they are very skewed by the outliers of the most popular ted talks (for example, # [4] Vulnerability researcher refers solely to Bren\u00e9 Brown), and thus not a representative sample. Anyway, there are some surprising findings here:\n\nLet\u2019s look at the overall correlation pairs scatter-plot matrix between each pair of numerical variables;\n\nTo complement this, see a correlation matrix with colors representing the intensity of the correlation from 0 (white) to dark blue (+1) or dark striped red (strong negative correlation, -1), and with asterisks (***) signifying significance by p-values.\n\nMost of the parameters don\u2019t have strong correlations.\n\nHow do these variables correlate with views? Is it a linear relationship, nonlinear, or non? This is important to understand to know how to insert them into the regression if at all.\n\nLet\u2019s start with day of week the talk was published at.\n\nSo, day of week does seem to have some association with average (mean) views! Ted Talks published on weekends seem to have much less views, with Saturday being the lowest, and Friday is the most popular day for ted talks published day.\n\nBelow are scatter-plots with LOESS flexible regression in white and linear regression in pink, to see how different would a linear shape look from a flexible moving average. This shows us that usually, except for in the tales of the distributions of these variables, where there are only a couple of outliers\u2019 data, the linear model described the relationship somewhat well.\n\nLet\u2019s start with duration and number of tags:\n\nSurprisingly, duration had almost no consistent correlation with number of views; except for the fact that most popular talks were closer to 8\u201318 minutes. Number of tags seemed to be optimal between 3\u20138, which is also the more common number of tags; with some outliers of particularly popular talks.\n\nWhat about numbers of Comments and Translation Languages?\n\nUnsurprisingly, number of comments is, obviously, very well correlated with number of views, and so does number of languages \u2014 they all come from having many viewers. Thus, it is not \u201cfair\u201d to predict views based on these factors, and in the real world, we couldn\u2019t use these parameters to predict, since they are not causes for more views, but they are also a result of many views, and a cause in a reinforcing feedback loop: the more comments, the more engaged the community is around the talk and likelier to spread; the more languages, the more viewers can watch; and the more viewers, the more audience there is to comment and translate. The rest had a small linear effect, where that didn\u2019t deviate much, though small.\n\nThe pink lines are linear regressions while the white lines are LOESS. It seems that none of these loose much information by a linear regression versus a LOESS regression, which is arbitrarily flexible and would reveal a clear non-linear shape. while some of them do have nonlinear shapes \u2014 from a closer look, it is only in the tail where data is scarce and it is biased by the few data points there and some outliers (as in the Comments correlation). Therefore, inputting the regressor as a linear fit might be sufficiently explanatory.\n\nI first ran a regression of number of views upon each variable of interest separately, which enabled to see which variables have explanatory power and how strong is it. Later, I included the better explaining ones, adding them one by one into a multiple regression, while checking that their addition actually improves our performance. The results are in the table below, where each column is a different model, for models (1) through (8)."
    }
]