[
    {
        "url": "https://medium.com/syncedreview/cambricon-unveils-its-first-ai-chip-for-cloud-computing-d3f7acdb4076?source=---------0",
        "title": "Cambricon Unveils its First AI Chip for Cloud Computing",
        "text": "Cambricon Unveils its First AI Chip for Cloud Computing\n\nCambricon today unveiled its Cambricon 1M chip for edge computing, and the MLU100, the first in a new chip series for cloud computing.\n\nCambricon 1M is the company\u2019s third generation AI chip for edge devices. With its TSMC 7nm technology, the AI chip provides efficiency of 5 TOPS/Watt for 8-bit computing. The 1M chip is available in 2, 4, and 8 TOPS versions to support a range of AI applications.\n\nLike the company\u2019s 1H and 1A chips, Cambricon 1M supports deep learning models such as CNN, RNN, SOM; and supports SVM, k-NN, k-Means, decision tree and other algorithms. This is also the world\u2019s first AI processor supporting local machine learning training.\n\nCambricon MLU100 is the first generation of Cambricon\u2019s new product series supporting cloud computing. MLU100 adopts Cambricon\u2019s latest MLUv01 architecture and TSMC 16nm technology. This processor has the capability of providing 128 TFLOPS/166.4 TFLOPS in balance mode and high-performance mode respectively.\n\nCambricon and its partners demonstrated several use cases: Lenovo\u2019s new ThinkSystem SR650 server is based on the MLU100, as is Sugon\u2019s new PHANERON series of products. iFlytek has also announced a collaboration with Cambricon."
    },
    {
        "url": "https://medium.com/syncedreview/ubtech-robotics-gets-us-820-million-funding-becomes-the-worlds-most-valuable-ai-startup-c25cd357e87e?source=---------1",
        "title": "UBTECH Robotics Gets US$820 Million Funding; Becomes the World\u2019s Most Valuable AI Startup",
        "text": "UBTECH Robotics Gets US$820 Million Funding; Becomes the World\u2019s Most Valuable AI Startup\n\nChinese AI and humanoid robotic company UBTECH Robotics today announced a staggering US$820 million in Series C funding. With its new estimated value of US$5 billion, Shenzhen City based UBTECH becomes the world\u2019s most valuable AI startup.\n\nInternet giant Tencent led the funding with a US$120 million investment. It is believed UBTECH\u2019s capabilities in robot design and manufacturing will strengthen Tencent\u2019s AI products. Last year, Tencent reportedly pumped US$40 million into UBTECH. Also joining the funding are the Industrial and Commercial Bank of China, Haier, Minsheng Securities, Telstra, CDHFund, and others.\n\nUBTECH Founder and CEO Zhou Jian told Synced that \u201cthis round of financing will be mainly used for strengthening R&D capabilities, facilitating marketing and brand development, and attracting top-tier talents.\u201d\n\nFounded in 2012, UBTECH aims to \u201cbring a robot into every home, and truly integrate intelligent robots into the daily lives of everyone creating a more intelligent way of life.\u201d The company\u2019s Alpha 1S robot holds the Guinness World Record for \u201cmost robots dancing simultaneously.\u201d A video of UBTECH robodog Jimu dancing and licking paws at the 2018 CCTV Spring Festival Gala went viral. Also in the UBTECH family are Cruzr, an intelligent service robot; the voice-activated, video-enabled companion Lynx; and a Star Wars First Order Stormtrooper robot. The company even produces a BuilderBots Kit for children who want to build robots.\n\nWhile the global robotic market is competitive, UBTECH has an advantage with its technological development of humanoid robot servos and motion controlled gait algorithms. UBTECH is a global leader in robotic joint and body structure manufacturing.\n\nZhou says UBTECH\u2019s sales goal for 2018 will exceed CN\u00a52 billion. He also unveiled the company\u2019s roadmap for the next four years:\n\nZhou\u2019s vision for robots goes beyond the crunching of algorithms and whirring of servos. Rather he stresses the relationship of such machines to human beings. \u201cIn UBTECH, we insist on viewing the robot as a member of a future family.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/ml-community-pledges-to-boycott-natures-new-paywalled-journal-6429d9d9a800?source=---------2",
        "title": "ML Community Pledges to Boycott Nature\u2019s New Paywalled Journal",
        "text": "Thousands in the machine learning community say they will boycott Nature\u2019s paywalled Machine Intelligence Journal, which is set for a January 2019 release. So far, 2482 researchers have signed a petition pledging to not submit any work to the journal or review or edit any of its papers.\n\nThe key objection to the new publication is its subscription-based revenue model, wherein the journal will charge a paper submission fee and individual and institutional subscription fees.\n\nThis, of course, touches a nerve in the machine learning research community, which prides itself on an open sharing culture. It has been easy for the public to access ML papers, codes, and in some cases even datasets. Up-to-date papers from major conferences such as NIPS and CVPR can also be easily shared. ArXiv is sometimes criticized for its relatively loose review mechanism, but its huge volume of papers can nevertheless be accessed by a global readership free of cost.\n\nLeading the boycott call are deep learning pioneers Geoffrey Hinton, Yann LeCun and Yoshua Bengio; research leads at Google, Facebook, Amazon and IBM; and academics from MIT, Stanford, CMU, and Oxford. OSU Professor Emeritus Thomas G. Dietterich, who initiated the petition, was Executive Director of the journal Machine Learning from 1992\u20131998, before the publication became the open-access Journal of Machine Learning Research (JMLR) in 2001.\n\nNature announced plans for Machine Intelligence last November, as a new online-only publication that would cover the \u201cbest research across the field of artificial intelligence\u201d with relevant reviews and commentaries. All editors are internal staff working under Chief Editor Liesbeth Venema.\n\nOpen access in academic publications is an ongoing issue. Research access in areas such as biology, neuroscience, psychology, and social sciences can be extremely costly, even for wealthy institutions like Harvard.\n\nThe academic community has been battling publishers for open access for a decade. Sci-Hub, created by Alexandra Elbakyan in 2011, has an archive of over 64.5 million papers available for direct download. Reddit founder Aaron Swartz\u2019s attempts embroiled him in a legal fight with Jstor and MIT that ended in tragedy.\n\nOutside the machine learning community, research papers in other AI subfields such as robotics and hardware chips are difficult for non-specialists to obtain. There are also labs like Boston Dynamics that can live outside the \u201cpublish-or-perish\u201d curse.\n\nThe machine learning community is rebelling against a publishing orthodoxy it regards as an impediment to progress. In a Reddit discussion thread, one researcher posted that he would judge a paper based on its ability to spread and instruct further research, rather than focusing on the publisher\u2019s prestige."
    },
    {
        "url": "https://medium.com/syncedreview/focus-on-ai-at-facebook-f8-6dbe7e04b565?source=---------3",
        "title": "Focus on AI at Facebook F8; PyTorch 1.0 Released (Updated)",
        "text": "Facebook generally uses its F8 Developer Conference to introduce new platform features. This year however there was a distinct emphasis on \u201cAI,\u201d with the term mentioned more than ever in the event\u2019s keynote speech.\n\nFacebook Founder and CEO Mark Zuckerberg unveiled the company\u2019s strategic AI roadmap, pushing R&D in vision technologies, unsupervised learning, natural language processing, reinforcement learning, generative networks, and AI developer tools. Facebook also announced several AI-related updates to elevate the user experience.\n\nRussian bots, fake news and the Cambridge Analytical scandal have hit Facebook hard. The seminal social media platform \u2014 where billions of users share photos and videos of their lives with family and friends, and exchange thoughts and opinions on events of the day \u2014 is increasingly revealing a dark side, as a malignant tool for spreading false information, instigating hatred, and even influencing elections.\n\nFacebook wants to fight back. Zuckerberg told the F8 audience the company is now using AI algorithms to detect \u201cspammers who just want money,\u201d \u201cfake accounts created by bad actors,\u201d and \u201creal people who are sharing fake information.\u201d\n\nFacebook has previously suggested they were deploying cutting-edge AI to recognize fake news and bot-generated accounts. In 2016, AI pioneer and then Facebook Director of AI Research Yann LeCun said AI could be used to identify fake news or violence in live video content on the site. In 2017, Facebook announced that they would use AI to detect terrorism-related posts.\n\nAI researchers are divided on whether machine learning algorithms can effectively detect fake news. Dean Pomerleau is a research scientist and entrepreneur who helped organize a crowdsourcing challenge to develop machine learning solutions to combat fake news. He told The Verge that in his opinion AI couldn\u2019t fix the fake news problem. On the other hand, Aaron Edell, CEO of AI company Machine Box, claims to have produced a machine learning algorithm that can detect fake news with higher than 95% accuracy.\n\nZuckerberg faced questions about Facebook\u2019s data privacy practices at a congressional hearing last month, where he estimated the company would need five to ten years to build an effective AI-powered fake news detection system that leaves humans out of the loop.\n\nThe Instagram Explore page is a powerful feature that recommends posts based on personal interests and tastes. Users can browse content across topics of interest, even from accounts they don\u2019t follow.\n\nFacebook Data Science & Analytic Manager Tamar Shapiro announced at F8 that Facebook will revamp Instagram Explore to better organize its recommended content into different topic channels, and this new Explore page will be powered by AI.\n\n\u201cIn order to deliver cutting-edge experience, we are augmenting AI with content classification and curation signals from our community,\u201d said Shapiro.\n\nShapiro also introduced a new AI-powered \u201cbullying comment filter,\u201d which can hide content that disturbs or upsets users. Last year, Instagram launched an offensive comment filter, which can automatically hide comments it deems \u201cdivisive\u201d or \u201ctoxic.\u201d\n\nM Translation expands on M Suggestion, a pop-up feature launched last year that suggests relevant content and capabilities. Now, when users receive a Marketplace message in a language different from their default, M Translation can translate the message into their default language.\n\nFacebook smart speakers to sell only outside the US?\n\nRivals Google, Amazon, and Microsoft have already jumped on the smart speaker bandwagon, as have major Chinese tech companies. Now Facebook is rumoured to be readying their own line of smart speakers, although Zuckerberg was tight-lipped about the plan at F8.\n\nMultiple media reports suggest Facebook will launch two devices this July in overseas markets. The unusual marketing plan is said to be due to Facebook\u2019s slipping trustworthiness among users in the US. The smart speaker is expected to be equipped with a touchscreen and camera, and will be powered by the text-based chatbot Facebook Messenger bot, M, which will likely get an upgrade to voice assistant.\n\nOn F8\u2019s second day, Facebook announced PyTorch 1.0, the latest version of its open-source AI software framework that guides and supports researchers from research stages to deployment of trained models for various AI applications. \n\n \n\nVP of Facebook Infrastructure Bill Jia said \u201cPyTorch 1.0 takes the modular, production-oriented capabilities from Caffe2 and ONNX and combines them with PyTorch\u2019s existing flexible, research-focused design.\u201d \n\n \n\nFacebook is pushing the combined PyTorch \u2014 Caffe2 framework. Last month, the Caffe2 Github page introductory \u201creadme\u201d document was suddenly replaced with a link: \u201cSource code now lives in the PyTorch repository,\u201d which enabled Caffe2 users to directly check Caffe2 code in PyTorch.\n\n \n\nSince its release in October 2016, PyTorch has become a preferred machine learning framework for AI researchers due to its flexibility. Over half of Facebook AI projects run on PyTorch. PyTorch 1.0 will be available to beta users later this summer.\n\n \n\n Facebook also announced the open-sourcing of many AI tools, including Translate, a PyTorch Language Library for neural machine translation, and ELF OpenGo, an AI bot based on the ELF (extensive, lightweight and flexible) platform for training gameplay AI with reinforcement learning. \n\n \n\n Facebook Chief AI Scientist Yann LeCun tweeted that the ELF OpenGo bot \u201chas attained professional level in two weeks of training and has won 15 games against 4 top professional human players.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/accutarbio-advances-ai-in-drug-discovery-e28c50abdec3?source=---------4",
        "title": "AccutarBio Advances AI in Drug Discovery \u2013 SyncedReview \u2013",
        "text": "In December 2017 Chinese pharma tech startup AccutarBio raised US$15 million from IDG Capital, YITU Tech, and ZhenFund. This was one of the country\u2019s largest pharma tech funding rounds, and signaled AI\u2019s strong promise and potential in early-stage drug discovery.\n\n\u201cDrug discovery is never subjective,\u201d AccutarBio CEO and Co-founder Dr. Jie Fan tells Synced. \u201cA person may have a good knowledge of thousands of types of drugs. But there is no way they know ten thousand or even one hundred thousand drugs. In contrast, machines can do a much better job than humans.\u201d Dr. Fan also believes that hybrid AI can accelerate drug discovery for targeted therapy and provide cancer patients with further alternative treatment plans.\n\nDr. Fan graduated from Fudan University with a BA and received his Master\u2019s in Biostatistics from the University of California Berkeley in 2004. He completed his PhD under Dr. Nikola Pavletich, studying the crystal structure of DNA binding proteins. Fan then worked as a postdoc researcher under 1999 Nobel Prize laureate G\u00fcnter Blobel, focusing on the structural analysis of nuclear pore complexes (NPCs), which are large protein complexes that allow transport of molecules across the nuclear membrane.\n\nAccutarBio continues to widely collaborate with academics for cross-interdisciplinary research, and the company has deployed labs in both Shanghai and New York.\n\nIn the research paper Chemi-net: a Graph Convolutional Network for Accurate Drug Property Prediction, AccutarBio extended the use of traditional statistical learning methods to create a multi-layer DNN architecture called \u201cChemi-Net\u201d, which can predict ADME (absorption, distribution, metabolism, and excretion) properties of molecular compounds. ADME study is crucial and should be done at an early stage in drug discovery as it helps researchers understand the transport of molecules in organisms and can efficiently eliminate weak drug candidates, increasing success rate in drug trials and shortening drug discovery timelines.\n\nChemi-Net was tested on 5 ADME endpoints \u2014 human microsomal clearance, human CYP450 inhibition, aqueous equilibrium solubility, pregnane X receptor induction, and bioavailability \u2014 with 13 industrial grade datasets selected for predictive model development, involving more than 250,000 data points. Both single-task and multi-task Chemi-Net exhibited dramatic predictive accuracy improvements over benchmarks. Researchers expect Chemi-Net\u2019s significantly increased ADME prediction accuracy to greatly accelerate efficiency and success rates in drug discovery.\n\nFollowing on the success of their AI-powered drug discovery model, AccutarBio plans to further develop and promote Chemi-Net with the aim of revolutionizing traditional experiment-based and experience-based drug development.\n\nAI has demonstrated its abilities in healthcare applications in medical and pharmaceutical fields. The tech\u2019s capability for \u2018learning\u2019 meaningful features from large datasets has been widely used in clinical situations involving computer-aided image detection and diagnosis (e.g., assisted image-based early cancer screening), implementation and maintenance of electronic health records, and continuous patient monitoring.\n\nAccutarBio strategic investor YITU Tech says \u201cAsymmetric information in multidisciplinary fields has brought huge barriers to technology development. We hope our knowledge and work in AI will be able to assist AccutarBio and make great advances in biology. We believe artificial intelligence will make significant difference on the current status of biological research and biopharmaceuticals through our in-depth cooperation with AccutarBio. And collaboration with YITU for Medical AI-powered research in clinical applications will bring more profound value.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-350-kph-self-driving-train-gets-ready-to-hit-the-rails-2301e7753811?source=---------5",
        "title": "China\u2019s 350 kph Self-Driving Train Gets Ready to Hit the Rails",
        "text": "On April 29th, the Assets Supervision and Administration Commission (SASAC) of China\u2019s State Council announced that China Railway Signal & Communication Corp (CRSC) had completed lab testing of their high-speed rail self-driving system (C3 + ATO), and is preparing for field testing. The system will be deployed in Fuxing Hao trains, which are the world\u2019s fastest with an operating speed of 350 km per hour.\n\nThe new self-driving high-speed rail system integrates various technologies, including cloud computing, IoT, artificial intelligence, big data, etc. The rail system infrastructure will include facial recognition for check-in, robot porters and other intelligent services.\n\nCRSC is a world-leading rail signal and communication service provider. It owns the world\u2019s biggest rail technology lab, and developed the Chinese Train Control System (CTCS).\n\nCRSC says they will complete assembling the smart train by the end of 2018, finish rail tests and adjustments by early 2019, and then officially open the \u201cBeijing-Zhangjiakou Intelligent Rail Line\u201d in late 2019, enabling passengers to complete the over 200 km journey in just 50 minutes."
    },
    {
        "url": "https://medium.com/syncedreview/iclr-2018-kicks-off-in-vancouver-f3a99bab70e0?source=---------6",
        "title": "ICLR 2018 Kicks Off in Vancouver \u2013 SyncedReview \u2013",
        "text": "AI researchers are gathering in Vancouver, Canada for the sixth annual ICLR (International Conference on Learning Representations). The event starts today and runs to May 3rd at the Vancouver Convention Centre.\n\nCompared to NIPS, ICML, and ACL, the ICLR is a relative newcomer. Founded in 2013 by deep learning mavericks Yoshua Bengio and Yann LeCun, the conference has become an important destination for AI researchers.\n\nProgram chair Yann LeCun tweeted that \u201cattendance at ICLR doubles every year. It\u2019s up to 2000 this year. It merely doubles because attendance is capped. The number of submissions also doubled from last year. [Google Researcher] Tara Sainath is opening the conference.\u201d\n\nThis year 337 out of 935 paper submissions were accepted, with best paper honours going to On the convergence of Adam and Beyond, Spherical CNNs and Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments.\n\nA statistical analysis by Arthur Pajot shows Google leading both the paper submission and acceptance charts, ahead of academic institutions Berkeley, Stanford, CMU, UofT, and ETH; and corporate research teams from Facebook, Microsoft and IBM. Google is dispatching 130 researchers to the conference and pitching in US$40,000 as a platinum sponsor.\n\nThe most prolific individual submittors are Yoshua Bengio, who submitted 18 papers, (9 rejected, 7 posters, 2 workshops), and UC Berkeley\u2019s Pieter Abbeel with 12 submissions (4 rejected, 4 posters, 2 workshops, 2 orals). The top-submitting countries are the United States, United Kingdom, China, Canada, and Germany.\n\nA paper keyword analysis conducted by Pau Rodr\u00edguez suggests \u201cmeta-learning, exploration, model compression, adversarial examples, variational inference are the hottest topics this year. For instance, 85% of the papers with the keyword exploration were accepted, while classification and CNN only show 12% acceptance rate.\u201d\n\nThe ICLR is known for popularizing the open review system designed by Andrew McCallum from UMass Amherst. Papers submitted to the conference are published on http://OpenReview.net, publicly reviewed and archived on the site."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-apr-w-4-apr-w-5-c31ec014c9ba?source=---------7",
        "title": "AI Biweekly: 10 Bits from Apr W 4 \u2014 Apr W 5 \u2013 SyncedReview \u2013",
        "text": "Google introduces starter kits designed to help people learn and experiment with AI solutions. The AIY kits (a pun on DIY/do-it-yourself) and are aimed at students, who can use for example the Voice Kit to develop a voice-controlled speaker, or the Vision Kit for tasks like object detection, facial detection etc. Each kit comes with a Raspberry Pi Zero WH board.\n\nMicrosoft users can now run translations without an Internet connection, thanks to a new app offering modified neural network translations. The offline translation quality is high, although not as good as online translation backed by AI on the cloud. Offline translation is available for Arabic, Chinese-Simplified, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish and Thai. More will languages will be added later.\n\nApril 19th \u2014 Dubai Uses AI to Turn Waste Into Energy\n\nAs part of its \u201cVision of the Future City, Today\u201d initiative, Dubai Municipality launches its AI-empowered Wasteniser project, which sorts solid waste by type using AI tech, and incinerates materials at optimal temperatures to produce good ash that can be used in the production of green concrete. The energy generated goes to the local electrical grid.\n\nWith Amazon\u2019s new Alexa Blueprints, users can create their own Alexa skills without any coding knowledge. Alexa Blueprints comes with 20+ templates, including Fun & Games, At Home, Storyteller, and Learning & Knowledge. Although these user-created skills will not be available on the Alexa Skills Store, they will appear on a \u201cSkills You\u2019ve Made\u201d page on the Blueprint website. Alexa Blueprints gives Amazon an edge over Apple\u2019s HomePod and Google Home, which do not offer such custom skill creation opportunities. The service is currently only available in the US.\n\nGoogle publishes research showing quick detection of cancer cells using a new Augmented Reality Microscope technology. Google believes the AI-backed ARM will \u201caccelerate and democratize the adoption of deep learning tools for pathologists around the world.\u201d The ARM is compatible with most current devices and can be easily retrofitted.\n\nApril 23rd \u2014 NVIDIA and DEEPCORE Team Up to Boost AI Startups in Japan\n\nNVIDIA announces a partnership with SoftBank-owned startup incubator DEEPCORE, which will use NVIDIA\u2019s AI computing platform to expand its support for startups and promote AI research in Japan. NVIDIA will provide technical training and industry advice for DEEPCORE customers and incubator members via its Deep Learning Institute.\n\nAsahi Shuzo is applying Fujitsu AI technology in a trial project to help visualize and optimize their sake brewing process. The Fujitsu Human Centric AI Zinrai system aims to \u201csystematize the experience and know-how of each employee\u201d to widen and improve real-time monitoring of the sake production process. Fujitsu built the predictive AI model based on the sake brewing company\u2019s historical data.\n\nAlibaba announces collaborations with automakers Daimler, Audi, and Volvo to use its voice assistant Tmall Genie in their cars to monitor battery level, mileage, engine status, etc., and allow remote management via voice command of car status including doors, windows and air conditioning etc. \u201cCars are an environment, alongside the home and the office, where individuals spend a significant amount of time, and which through connectivity can become an important part of life,\u201d said Lijuan Chen, head of Alibaba AI Labs, which created the Genie.\n\nWeb service company IFTTT announces US$24 million in funding from Salesforce, IBM, and others. The investment will be used to hire talents, develop enterprise business and IoT services, and address problems in the market. \u201cEvery business is undergoing a dramatic transformation into a digital service,\u201d says IFTTT CEO Linden Tibbets. \u201cWe could not be more excited to partner with our new investors, our passionate users, and every business working hard to become a service.\u201d\n\nTesla\u2019s ex-autopilot head Jim Keller leaves the autonomous driving company to lead Intel\u2019s silicon engineering team, where his research will focus on SoC development and integration. \u201cJim is one of the most respected microarchitecture design visionaries in the industry, and the latest example of top technical talent to join Intel,\u201d said Dr. Murthy Renduchintala, Intel\u2019s chief engineering officer and group president of the Technology, Systems Architecture & Client Group (TSCG)."
    },
    {
        "url": "https://medium.com/syncedreview/amazon-patents-provide-clues-to-new-home-robot-dedbafe0e1b7?source=---------8",
        "title": "Amazon Patents Provide Clues to New Home Robot \u2013 SyncedReview \u2013",
        "text": "The robotics community is abuzz with speculation following reports that Amazon\u2019s R&D arm Lab126 is hiring senior researchers for a home robot-producing project codenamed \u201cVesta.\u201d The company is also said to be recruiting employees who will test the robot in their homes later this year.\n\nNo one knows if Amazon envisions an enhanced roomba or a precocious R2D2 V2.0. Company Founder and CEO Jeff Bezos was recently spotted at a tech conference walking Spotmini, the Boston Dynamics robotic dog known for autonomously opening doors: Might the bot take the form of a pet?\n\nOne thing is certain: Amazon will cram Alexa into the robot. The company\u2019s star virtual assistant has evolved into a know-it-all that can play music and audiobooks, compile to-do lists, set alarms, stream podcasts, and provide real-time weather, traffic, news and other personalized information. Although most people only use their virtual assistants for music streaming, controlling smart lights, timers and weather forecasts, according to an IFTTT survey, Alexa is backed by so much cutting-edge AI tech that she\u2019s become too smart for just a smart speaker.\n\nWith Alexa as the brains, the bot may be more humanoid, both in physical form and in personalization and communication scope and style. Sarah Osentoski, co-founder of the robotics company Mayfield that makes companion robot Kuri, told Wired, \u201cWhen you have something that\u2019s talking to you and that\u2019s driving around your house you start to expect a lot more. You start to expect the intelligence of a 3-year-old or a 5-year-old.\u201d\n\nThat could be why Amazon is scrambling to update Alexa with more powerful capabilities. At the recent World Wide Web Conference in Lyon, France, the Head of the Alexa Brain group Ruhi Sarikaya said Alexa will soon launch three new features: an enhanced memory system that remembers everything users say, a more natural conversational system, and automatic activation of over 40,000 third-party skills.\n\nTo recognize its owner and family members, navigate rooms and hallways and so on, Amazon\u2019s new robot will certainly require robust computer vision skills. In 2016, Amazon acquired Orbeus, a Silicon Valley startup that specializes in image recognition. Amazon hired nearly all of Orbeus\u2019 technical staff, and picked up its facial recognition system and scene recognition system patents.\n\nLeveraging facial recognition, the robot will be able to capture images and record videos automatically. Amazon has patented approaches for creating high quality images with less blur and noise. The bot for example might be put in charge of photographing a birthday party, or compiling family photo and video albums.\n\nThe new robot might also employ multimodal dialogues to better watch, listen, speak, and physically interact with humans. Dr. Zhou Yu, an assistant professor at University of California in Davis, told Synced she is enabling intelligent assistants such as Siri and Alexa to recognize actions such as facial expressions and give appropriate responses. Since 2016, Amazon has supported her research with US$100,000 in annual funding.\n\nThe Amazon home bot will be mobile, and navigate autonomously with cameras. It will need to recognize and track its user or users in the environment. Some of this tech can be borrowed from the Echo Smart Speaker, which uses real time sound data to determine a user\u2019s physical position. Amazon\u2019s robot could push that technique by incorporating dynamic visual data.\n\nLab126 has been granted several patents for localization techniques which can determine where a user is. One patent describes a system that utilizes a sound location technique to estimate an audio-based sound source position, and then pinpoints the user\u2019s position from analysis of optical images or depth maps generated by multiple sensors, such as LiDar and structured light.\n\nAlthough the patent was originally filed for a mysterious Lab126 project \u2014 rumoured to be a since-abandoned in-home augmented reality entertainment system \u2014 Lab126 is likely to apply this technique in the home robot.\n\nWhile Amazon has prioritized voice commands in human-machine interfaces, their home bot will have to go beyond that, and watch for and respond to physical cues from the user, such as hand gestures.\n\nAmazon has already been granted patents for hand signal detection. One is for a room computing system that uses sensors to detect and respond to hand poses. To detect a hand pose, an observed pose is compared to a hand pose dataset. When you wave goodbye to your Amazon robot as you head to work in the morning, will it wave back? It will if you want it to.\n\nIn any case the home bot will understand you are leaving, and will get on with its day, which may involve assigned tasks such as security monitoring. The new Amazon Key service allows in-home delivery and secure home access for guests. An interior security camera connects with Amazon Key, and it would seem natural to apply that task to the robot.\n\nGo outside to run errands\n\nAutonomous vehicles are a heated innovation area attracting interest from tech companies like Amazon, which this year patented an autonomous ground vehicle (AGV) that can leave its home, pick up a package from a depot and bring it back.\n\nWill the Amazon robot have this capability? It is possible. Tye Michael Brady and Ethan Zane Evans, the researchers behind the technique, point out \u201cthe AGVs may be owned by individual users and/or may service a group of users in a given area (e.g. in an apartment building, neighborhood, etc.).\u201d When the robot knows a delivery truck is approaching, it will navigate autonomously to meet the truck at the pickup point.\n\nYour AGV bot could even meet up with other neighbourhood AGVs to return a rake or borrow a cup of sugar.\n\nThe new Amazon home bot will likely be a wheeled, mobile, Alexa-based robot with an array of cameras and possibly other sensors. Will it have arms and suction grippers to do household chores? Possible, but Amazon seems to be aiming for multiple capabilities across a wide range of functions, not a housekeeper like \u201cAndrew\u201d in the film Bicentennial Man. The potential is huge and the possibilities endless.\n\nOnly time will tell whether Bezos and Lab126\u2019s creation will be a rolling Echo or a game-changing, \u201cwow\u201d product."
    },
    {
        "url": "https://medium.com/syncedreview/the-past-present-future-of-biotech-in-boston-150980e4860c?source=---------9",
        "title": "The Past, Present & Future of Biotech in Boston \u2013 SyncedReview \u2013",
        "text": "If you are familiar with the biotech industry, you know Cambridge. The small city at the center of the Great Boston Area hosts over 1,000 biotechnology-related companies. Most of these companies cluster around Kendall Square, the same block as the Massachusetts Institute of Technology (MIT).\n\nOver the past decade the number of biotech jobs in Boston has jumped by 37%. [1] With the rapid rise in the application of Artificial Intelligence, and given the promising future of bioinformatics and computational biology, the local biotech industry finds itself in the midst of a technological revolution.\n\nThe biotech industry in Boston can be traced back to the 1970s, when molecular biology was in its golden age. The idea of \u201cplaying with genes\u201d however made many uncomfortable at the time. Cambridge City Council held a hearing on DNA experiments, and granted permission to Biogen, a new, local company founded by MIT Professor Phillip Sharp. Biogen was the first US firm to get the green light for genetic engineering. [2]\n\nBio-pharmaceutical companies quickly poured into Kendall Square, creating a well-rounded, self-vitalizing biotech ecosystem and building a global centre for biotechnology.\n\nOne key factor in Boston\u2019s rise in biotechnology is the area\u2019s academic resources, which are second to none.\n\nAlong with traditional biomedical schools such as Harvard Medical School and MIT Whitehead Institute for Biomedical Research, there are also a number of interdisciplinary programs combining biology and other informatic engineering subjects, such as the MIT Computational and Systems Biology Initiative (CSBi) and the Department of Biostatistics at Harvard. Other universities such as Tufts also have their own bioinformatics research groups.\n\nBoston\u2019s many universities are a talent pipeline, and most of the area\u2019s biotech company founders are graduates or professors from Harvard, MIT or other top universities. [4]\n\nBesides laboratories in universities, Boston\u2019s biotech industry is also backed by labs in hospitals and large pharmaceutical companies.\n\nBoston has three respected medical schools, two pharmaceutical schools, and three general hospitals. Having top hospitals not only provides more research facilities, but also more disease study opportunities. It is extremely difficult for example to do clinical testing and study on certain rare diseases which appear early in life and can claim the lives of 30% of patients before age five. The Boston Children\u2019s Hospital International Health Services division treats young patients from over 100 countries every year, and these treatment cases can inform the biotech research ecosystem.\n\nLarge medical companies are becoming more dependent on smaller biotech companies in the research field. As rising costs, patent cliffs, and other factors eat into pharmaceutical industry profits, many big medical companies are turning to Boston\u2019s innovative biotech startups to provide high-quality R&D results at lower costs. By the end of 2017, pharma giants Genzyme, Novartis, Pfizer, and Baxter had all established presences in Boston largely for this reason.\n\nCapital is an essential element in building a vibrant industry ecosystem, and as biotech has grown so has the money behind it. General venture capital and business companies are involved, along with investors specifically targeting biotechnologies.\n\nFidelity Biosciences, with US$2 trillion in assets under management, was one of the earliest venture capital companies to focus on life sciences, specifically \u201cBiopharmaceuticals, MedTech, and Healthcare IT/Services in a stage-agnostic fashion.\u201d [5] Other capital and consulting firms in this market include Flagship Ventures, MPM Capital, Locust Walk Partners, Voisin Consulting, and Fuld & Co.\n\nWhat\u2019s more, Biotech is an area where government can support businesses. Last year Massachusetts Governor Charlie Baker announced plans to invest US$500 million over the next five years in the Massachusetts Life Sciences Initiative. Baker\u2019s predecessor, Gov. Deval Patrick, had launched a US$1 billion initial investment in biotech back in 2008. [6]\n\nRespected biotechnology media company FierceBiotech publishes an annual \u201cFierce 15\u201d list of the world\u2019s most promising biotech companies, which it believes can make future breakthroughs. Cambridge area companies have taken about one-third of the spots on the list over the last five years, an achievement no other city has even approached.\n\nWhat are the most popular areas in biotech industry today? Gene editing is undoubtedly one, especially with the discovery of the CRISPR/Cas9 Method in 2011.\n\nCRISPR, for Clustered Regularly Interspaced Short Palindromic Repeats [7], is a family of DNA sequences in bacteria that contains snippets of DNA from viruses that have attacked the bacterium. These sequences play a key role in a bacterial defence system, and form the basis of a genome editing technology known as CRISPR/Cas9, which allows permanent modification of genes within organisms. [8]\n\nWith its huge application value in genome engineering, gene knockdown/activation, disease models, biomedicine and more, CRISPR technology has fostered many innovative Boston startups. Zhang Feng is the MIT Professor who first successfully applied CRISPR/cas9 in eukaryotic cells, and secured a patent for this method in 2017. [9]\n\nZhang co-founded Editas Medicine, one of the leading Boston companies focused on this technology. Another is CRISPR Therapeutics, which was co-founded by Emmanuelle Charpentier, another seminal figure in the field.\n\nIn 2015, Editas Medicine received US$120 million in Round B funding. Investors included Flagship Ventures (15.6%), Third Rock Ventures (15.6%), Polaris Venture Partners (15.6%), and bng0 under Bill Gates (9%). [10] In 2016, Editas Medicine became the first CRISPR gene editing company to make an IPO. Editas plans to start testing CRISPR in treating blindness, which would be the first instance of editing human genomes using CRISPR.\n\nOther promising players in the gene editing field include Intellia and Bluebird Bio \u2014 whose stock price has risen sixfold since it went public.\n\nThe biotech industry in Boston is booming, while facing potential challenges.\n\nLife sciences research involves a heavy time commitment in the design and execution of experiments, and this can send costs skyrocketing.\n\nEditas Medicine for example remains stuck in the preparation phase for the blindness treatment plan it announced in early 2015, although it confirmed investments of up to US$20 million at its 2016 IPO. This situation is normal in the industry. The success of a biotech company thus depends not only on its technology, but also on the confidence of the capital behind it, which will support it through such delays.\n\nSilicon Valley is another challenge for Boston. The two regions are virtually neck-and-neck in the biotech race, but as the industry starts relying more on software, wearable devices, and big data analysis, etc, this could tilt the contest in favour of Silicon Valley, which has a definite advantage in these tech areas."
    },
    {
        "url": "https://medium.com/syncedreview/pigs-cows-cockroaches-on-the-ai-animal-farm-58280d62f34f",
        "title": "Pigs, Cows & Cockroaches on the AI Animal Farm \u2013 SyncedReview \u2013",
        "text": "Farming is becoming a data-centric business powered by artificial intelligence. China\u2019s big tech firms are using neural network-backed computer vision, wearable devices, and predictive analytics algorithms to reimagine pig, chicken, cow, goose, and cockroach farming.\n\nThe SCMP reports that Gooddoctor Pharmaceutical Group is using AI to cultivate up to six billion cockroaches per year in China\u2019s southeast Sichuan province for medical uses. The operation has generated US$684 million in revenue and is backed by AI algorithms which collect and analyze up to 80 indexes of data, catering to the roaches\u2019 humidity, temperature, and food requirements. AI also keenly monitors and stimulates the roaches\u2019 growth and breeding rates.\n\nOver the past year, Sichuan pig farming corporation Dekon Group and pig feed supplier Tequ Group have been working in partnership with Alibaba Cloud. By 2020, Dekon Group will breed up to 10 million pigs per year. The AI-backed computer vision and voice recognition systems can recognize pigs via numbers tattooed on their flanks and monitor vulnerable piglets for squeals of distress.\n\nAlibaba competitor JD.com meanwhile has launched an AI chicken breeding project wherein each chicken wears a fitness tracker around the ankle. JD.com says it will buy the birds back at triple the price once they walk one million steps. JD Finance CEO Shengqiang Wang says the company wants to rebuild the entire farmhouse infrastructure, monitoring food intake, defecation, and other physiological conditions.\n\nPoking fun at the trend on April Fool\u2019s Day, Tencent announced the \u201cgrand opening\u201d of a purported geese production facility in the mountains of Guizhou. In Chinese, \u201cgoose\u201d (\u9e45) is written one hanzi character away from Tencent\u2019s flagship mascot penguin (\u4f01\u9e45). A Tencent spokesperson claimed the company was starting a pilot goose farming project to explore the potential of \u201cAgriculture + AI + Internet Smart Retail.\u201d The company said the Guizhou operation would be located in excavated mountain caves, begin with 5,000 geese and scale up to 200,000. To further play on the hanzi pun, Tencent promised netizens it was considering adding swans (\u5929\u9e45) and of course, penguins (\u4f01\u9e45).\n\nArtificial intelligence is certainly revamping the animal farming industry, with more and more technology companies hopping onboard. Animal farming is no longer a difficult job plagued with sanitation problems. AI may provide more wholesome and sustainable solutions for this inevitable trend of mass production.\n\nIn Japan, Osaka University\u2019s intelligent cow breeding system can detect contagious viral disease in livestock with up to 99% accuracy. The system is being adopted for cowhouses with automatic milking machines and feeding robots, and several Japanese dairy farms are using it along with wearable devices to fine-tune milking and feeding and provide real-time updates.\n\nAt the same time, computer vision or data manipulating software portals are just small nodes in the bigger IoT makeover of food production. While farmers may be initially skeptical of all these new-fangled cameras, fitbits, and smartphone apps, the AI farming wave is not likely to recede, rather it may completely change the farming status quo."
    },
    {
        "url": "https://medium.com/syncedreview/google-boosting-its-ai-research-in-tokyo-608f8ba11c9",
        "title": "Google Boosting its AI Research in Tokyo \u2013 SyncedReview \u2013",
        "text": "Google is looking to expand its AI research activities in the Japanese capital. The company\u2019s deep learning and AI research team Google Brain yesterday posted a Tokyo job listing seeking talented experts to participate in cutting edge research on machine learning.\n\nApplicants will work on real-world problems involving AI, data mining, natural language processing, hardware and software performance analysis, improving compilers for mobile platforms, as well as core search and much more.\n\nMinimum qualifications for the position are:\n\nGoogle Chief of AI division and Head of Google Brain Jeff Dean tweeted, \u201cHappy to see our #GoogleAI efforts expanding with Google Brain now having a research presence in Tokyo.\u201d\n\nGoogle Brain was initiated in 2011 as \u201cGoogle X,\u201d a project focused on building a large-scale deep learning software system. Its founding members \u2014 Dean, Google Researcher Greg Corrado, and Stanford University professor Andrew Ng \u2014 successfully built a neural network powered by 16,000 computer processors, which was trained to recognize cats in YouTube videos. The project ended up doing so well that it was upgraded into \u201cGoogle Brain\u201d with a mission to improve people\u2019s lives by making machines smarter. Google Brain has since attracted top-tier researchers such as Dr. Geoffrey Hinton, who developed back propagation; and Ian Goodfellow, who created generative adversarial networks (GANs).\n\nGoogle Brain is aggressively pursuing AI talents outside the US. In 2016, Google opened a Zurich research unit focused on machine learning, the digital assistant inside its Allo Chat app, autonomous driving efforts, and improvements to Google\u2019s search engine.\n\nGoogle opened its first office in Japan back in 2001. Headquartered in Tokyo\u2019s Roppongi Hills complex, Google Japan has since grown to a team of over 1,300."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-train-100-000-ai-talents-in-three-years-850ab9c1cc01",
        "title": "Baidu to Train 100,000 AI Talents in Three Years \u2013 SyncedReview \u2013",
        "text": "At a tech conference in Beijing yesterday, Baidu Vice President Yaqin Zhang plotted a bold course for the search engine giant. In response to a growing shortage of AI talents, Zhang announced that Baidu will train over 100,000 AI talents with expertise in engineering and product development over the next three years.\n\nTrainees can learn AI, data analysis, and cloud computing skills on the Cloud Intelligence College (\u4e91\u667a\u5b66\u9662), an education platform launched by Baidu last year that offers online and offline courses, and awards specialized certificates to graduates.\n\n\u201cOur mission, on one hand, is to improve the employment competitiveness of trainees, and on the other hand, to ramp up companies\u2019 product R&D capabilities,\u201d says Zhang.\n\nChina is facing a shortage of five million AI talents. While the country is an AI business deployment leader with well-financed startups, its AI educational infrastructure is lagging. The US has six times more AI education institutions. Meanwhile, high demand has sent AI engineer salaries skyrocketing in China.\n\nEarlier this month China\u2019s Ministry of Education issued an action plan aimed at energizing Chinese universities\u2019 capabilities in AI technological innovation, talent cultivation, and global cooperation. The action plan also pledges to educate 500 teachers and 5,000 students over the next five years in a joint effort with Sinovation Venture and Peking University."
    },
    {
        "url": "https://medium.com/syncedreview/chip-guru-jim-keller-leaves-tesla-for-intel-34affc4a6490",
        "title": "Chip Guru Jim Keller Leaves Tesla for Intel \u2013 SyncedReview \u2013",
        "text": "Last December, Tesla ditched its chip partner Nvidia and announced that they would make their own in-vehicle chips. The Head of Tesla\u2019s Autopilot Hardware Team Jim Keller, best known for his CPU design at AMD and Apple, was supposed to help the company make that happen.\n\nToday, however, Tesla announced Keller had left: \u201cToday is Jim Keller\u2019s last day at Tesla, where he has overseen low-voltage hardware, Autopilot software and infotainment. Prior to joining Tesla, Jim\u2019s core passion was microprocessor engineering and he\u2019s now joining a company where he\u2019ll be able to once again focus on this exclusively. We appreciate his contributions to Tesla and wish him the best.\u201d\n\nPrior to joining Tesla in January 2016, Keller served as Corporate Vice President and Chief Architect at AMD. He earned his reputation as a CPU architect master by leading design on the AMD K8 microarchitecture and Apple A4/A5 processors which empower the iPhone.\n\nKeller\u2019s departure is the latest in a series of setbacks for Tesla. The company is struggling with Model 3 production hiccups, and Moody\u2019s recently downgraded Tesla\u2019s credit rating due these delays; and an Apple engineer died last month when his Tesla Model X, with Autopilot, crashed in Mountain View, California. About the only good news for Tesla is that their stock price has not yet been seriously affected.\n\nKeller\u2019s next stop is Intel, where he will be a senior vice president and lead the company\u2019s silicon engineering, which encompasses system-on-chip (SoC) development and integration.\n\n\u201cThe world will be a very different place in the next decade as a result of where computing is headed. I am excited to join the Intel team to build the future of CPUs, GPUs, accelerators and other products for the data-centric computing era,\u201d says Keller."
    },
    {
        "url": "https://medium.com/syncedreview/liulishuos-ai-app-is-teaching-english-to-70-million-people-31d4fb38a799",
        "title": "Liulishuo\u2019s AI App Is Teaching English to 70 Million People",
        "text": "\u201cLiulishuo is the AI English teacher on your phone. You don\u2019t need to know how it works, yet it helps you learn English more efficiently than a human teacher,\u201d says Yi Wang, Founder and CEO of Liulishuo \u2014 a Beijing-based \u201cAI + language\u201d company whose name translates as \u201cspeak fluently.\u201d Liulishuo hosts the world\u2019s largest speech bank of Chinese speakers speaking English.\n\nAfter obtaining a PhD in computer science from Princeton, Wang worked as a product manager at Google in California. He returned to China in the early 2010s and found many of his friends asking similar questions: \u201cHow do I learn English?\u201d; \u201cWhy is it that I pay so much money for lessons and fail to keep up?\u201d; \u201cShould I watch more American TV shows to learn to speak naturally?\u201d\n\nWang wondered whether people might be able to learn English by speaking with their phones. At the time, AI and online education were much less developed. In 2012 Wang launched Liulishuo with Hui Lin, a Google coworker specializing in voice recognition and machine learning.\n\nLaunched six months ago, the company\u2019s flagship app is now teaching English with personalized and adaptive methods based on deep learning to some 70 million registered users in 175 countries. It was selected as an Apple Store \u201cBest App of the Year,\u201d and Apple VP Philip Schiller and his team visited the company\u2019s offices. Liulishuo is the only Chinese Education company to make the CB Insights AI 100 list.\n\nSynced recently spoke with Liulishuo Yi Wang about his company and AI language learning.\n\nLiulishuo has the world\u2019s largest database of Chinese people speaking English. Based on the database we created a Chinese-speaking-English recognition engine with the highest accuracy and an evaluation engine which provides users with ratings and feedback.\n\nInitially, we used speech recognition to evaluate the user\u2019s verbal skills. Now our engine can perform a full range of verbal language assessments, and we have a separate engine that can correct writing.\n\nWe have a special function tackling the IELTS exams, which tests users on their pronunciation, grammar, vocabulary, and fluency. The scoring algorithms have passed the Turing Test. The variation in score between our AI and a human examiner is lower than between two human examiners. We offer users individualized suggestions to improve their English using their scores in the four areas.\n\nWe launched the world\u2019s first AI English teacher platform in July 2016. Ten million users have already completed our ranking exam, with completion time ranging from 5 minutes to 20 minutes depending on proficiency.\n\nAfter completing the exam, the system selects a starting level for each user. Users improve their English through the immersive learning of scenes from images and animations, without any subtitles or translations. They must attempt to understand the scenario, which is followed by suggested practices, which is then followed by more progressive scenarios. This back and forth is highly effective in improving the user\u2019s English language skills.\n\nWe have the data to prove our effectiveness. ETS TOEFL testing has proven our AI teacher can improve learning efficiency by three times. For example, if it used to require 100 hours of learning to reach a certain level in the CEFR standard, we would only need about 36. Regardless of product and service, we\u2019re the only English learning organization that publishes efficiencies of this caliber, and that really excites me.\n\nBefore launching the product, we asked some American English speakers to record audio for us to cold start the engine. We also collected limited data from Chinese speakers audio recordings via crowdsourcing.\n\nBut since the launch of our product, our users have provided us with massive amounts of incoming data in different skill levels and accents. This data was recorded by users reading what\u2019s displayed on their screen, and so it is also labeled. We effectively received all this data for free from users practicing their English.\n\nIn order to train our model, we invited experts to label our data for us, for example for IELTS we asked IELTS examiners to label our data.\n\nWe have two types of content. The free content is English conversations written by professional writers, on top of User-Generated Content (UGC) and Professionally Generated Content (PGC). We also have the most active English learning community in China, and many of our short videos are contributed by these learners.\n\nThe paid content is created by our own team. We hired Philip Lance Knowles, who has previously proposed Recursive Hierarchical Recognition Theory and other breakthroughs in language learning theory based on cognitive science, as our expert consultant and created our content based on Lance\u2019s theory. Of course, our customized learning material is different from writing a textbook, where all students learn in the same sequence.\n\nThis is the general direction, and we are doing some early stage testing.\n\nThese headphones can benefit let\u2019s say seniors traveling in another country. But I think they won\u2019t be able to replace the language learning market. Firstly, the Ministry of Education will not remove English from the curriculum just because we have translation machines. Secondly, learning a new language isn\u2019t just about translation, translation is just application.\n\nLearning a new language is about learning to communicate with others, and there are cultural contexts one must also learn to understand in order to learn real communication. In the process of learning a new language, there\u2019s a sense of accomplishment, in which the user builds confidence and challenges themselves. Thus we see the social function of language learning, as many people learn to make new friends. There are people who make friends and even find their partner on our platform. In this sense, you can\u2019t equal language learning with translation.\n\nWe want the learning process to be customized and highly effective. To achieve these two goals, we believe that data-driven AI is the key. We\u2019ve only taken the first step in exploring AI teachers. They aren\u2019t intelligent enough just yet, and the learning experience has many areas that can still be optimized. We are working hard at solving these problems.\n\nPeople have limited understanding of how our conscience and brain actually work. We are working with many experts in neuroscience and education, such as the Dean of Education Faculty at Stanford University and a Professor of Neurology at Yale University, in hopes of bringing in new research results. Our platform is also useful for their research because we have large amounts of user data that helps them create new learning models. We have set up an education and AI lab in the Bay Area, hoping to attract top experts in AI, education, and cognitive science, in order to help us create the most intelligent, most efficient AI English teacher in the world.\n\nThe explosive growth of the mobile internet since 2012 has turned mobile device usage into the new way of life. I saw this as an immense opportunity, but I realized that if I only developed small apps focusing on weather, calendar, camera etc, it would be a challenge to make them profitable. Therefore we thought about combining the mobile internet with traditional industries. The markets must be large, with good user paying habits, and room for improving efficiency. We researched applications in finance, health, and education, finally deciding to go with education.\n\nWe set forth to create an easy-to-use product. The first week Liulishuo was available, it was recommended by the Apple Store in mainland China, Hong Kong, Taiwan and Japan, and quickly became one of their \u201cBest Apps of the Year.\u201d Apple\u2019s Senior VP of Worldwide Marketing soon toured our company. This shows that our product-centered strategy is being rewarded.\n\nThe second turning point was the transformation of Liulishuo from a tools App into a community App. Building a community increased user stickiness and activity and created a broad learning environment.\n\nThe third point was in 2014, when we made a strategic decision to create an educational research team, to involve education professionals from a content perspective. Before this, we were purely an internet company. We decided to put a heavy focus on the essence of education and personalization of content. If we had not made that shift, we would just be a marginalized tech company.\n\nIn 2014 we worked with a foreign company and tested two learning techniques which used word games to practice speaking. But they weren\u2019t very successful. From a gameplay perspective, they weren\u2019t as fun as normal games; while from a learning perspective, they weren\u2019t very effective.\n\nDuring the second half of 2014 we wanted to create a textbook product. Our first instinct was to license the best textbooks from top publishers such as Pearson, Cambridge, or Oxford. After we got to know them a bit better, we realized that these textbooks were written and designed for traditional, offline classroom scenarios and were not effective for users lounging on their couch learning with a smartphone. Therefore, we started to work on our own educational research team and spent two years developing the AI teacher. This was a strategic change, and looking back it was the right thing to do.\n\nWe are seeing three historical opportunities. Firstly, learning is now digitized. In the past we did our homework on paper, whereas today 100% of user learning, actions, and interactions are digitized. This is a huge leap forward and the only way to make it possible to use AI. If you\u2019re not digitized and have no structured data, it will be impossible to talk of AI, right?\n\nSecondly, I think we\u2019re experiencing a historic leap from teacher-centric to student-centric learning. There were many more students than teachers in the industrial era, but many students are now practicing one-on-one with a language tutor. However, this is a transition phase because the so-called \u201cone-on-one\u201d is still not necessarily centered around the student, as teachers may not understand the needs of the student and create suitable teaching plans.\n\nLiulishuo\u2019s AI teacher is not human, it is a system that relies on user interactions to make decisions. Through deep learning and other AI technologies, it selects only relevant content from a huge library and recommends it based on the student\u2019s level. I think the pace of review and practice frequency based on an individual student\u2019s needs, strengths and weaknesses is the ultimate student-centric learning experience.\n\nThird, from a business perspective, we believe that we must develop a results-oriented business model to replace a process-oriented one. In a language training institution for example, if you learn for 100 hours and yet still see no improvement, the institution won\u2019t be responsible as it has delivered its service by selling you the teacher\u2019s time. Hence, these traditional training institutions are just wholesalers of teachers\u2019 time. We think this situation will eventually change, and educators will get paid according to the results achieved by each student.\n\nOur paid product works exactly this way, it does not charge based on instructional hours, but instead, provides users with a buffet. For just CN\u00a599 a month, users can spend as much time as they like there. A diligent student can learn at a much faster pace, absorbing all that they can. Our paid users on average spend five hours or more learning on our App each week. Who spends five hours a week learning a new skill anymore as an adult? This shows our product is really effective.\n\nI hope that in the next two to three years Liulishuo can assemble a leading team of researchers and product designers with full-stack development capabilities, dedicated to applying AI to education. As for long-term plans, I hope that in the next decade, we can become a global leader in education."
    },
    {
        "url": "https://medium.com/syncedreview/pytorch-releases-major-update-now-officially-supports-windows-2426c9f29d2d",
        "title": "PyTorch Releases Major Update, Now Officially Supports Windows",
        "text": "PyTorch, an open source machine learning library for Python, today announced the release of PyTorch 0.4.0 with Windows support. \n\n \n\n PyTorch can now be installed on Windows OS via Conda or Pip command line. The new version also merges Tensor and Variable, which means torch.autograd.Variable and torch.Tensor are now in the same class; and unifies the return 0-dimensional vector of size, which makes it more similar to NumPy features. Also, a set of more flexible context managers has replaced the volatile flag.\n\n \n\n As a Facebook-backed open source package released in October 2016, PyTorch has been very well-received in the developer community, and has more than 14.4k stars on GitHub (Google-backed TensorFlow has 97.4K stars, and Amazon-backed Apache MXNet has 13.7K stars on GitHub). It can leverage the capability of GPU, speed up computing for AI tasks, provide GPU-friendly NumPy functions, and robustly support Tensor. \n\n \n\n Detailed update content:\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/new-petuum-cmu-paper-identifies-statistical-correlation-among-deep-generative-models-1d9afc5abb87",
        "title": "New Petuum & CMU Paper Identifies Statistical Correlation Among Deep Generative Models",
        "text": "On April 17th, researchers from Carnegie Mellon University and Petuum, a Pittsburgh-based CMU spinoff focused on artificial intelligence platforms jointly published On Unifying Deep Generative Models. The paper introduces a high-level theoretical connection between various deep generative models, particularly Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). It has been accepted as a 2018 ICLR Conference Paper.\n\n \n\n The paper\u2019s researchers suggest that GAN and VAE lack a unified statistical connection due to their distinct generative parameter learning paradigms. Researchers derived a new GAN formula that has many similarities with VAE, which could spark innovations in R&D of GANs and VAEs, and help researchers discover new common rules of machine intelligence that were previously undetected.\n\nAccording to the original post, many advantages can be achieved by this unified statistical view:\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/mckinsey-report-ai-promises-added-value-of-up-to-us-5-8-trillion-80cc0043ebf6",
        "title": "McKinsey Report: AI Promises Added Value of Up to US$5.8 Trillion",
        "text": "McKinsey Report: AI Promises Added Value of Up to US$5.8 Trillion\n\nAlthough artificial intelligence has triumphed over the world\u2019s best human players at Go and Poker, and outperformed humans at imaging analysis and speech recognition, few are aware of the actual dollar value that AI techniques can bring to industries such as travel, retail, transport & logistics.\n\nThe McKinsey Global Institute this month released the report Notes From the AI Frontier Insights From Hundreds of Use Cases. The 36-page discussion paper surveys cutting-edge machine learning algorithms, and discusses how they can be integrated or transformed into practical applications across 19 selected industries.\n\nAI can potentially create US$3.5\u20135.8 trillion in annual value\n\nThe report defines AI as deep learning techniques based on artificial neural networks, such as feed forward neural networks, recurrent neural networks (RNN), and convolutional neural networks (CNN). These algorithms have grown from fledgling research subjects to mature techniques in real world use. Advanced AI techniques such as generative-adversarial-networks (GANs) and reinforcement learning are not within the scope of the report.\n\nIn the 19 industries studied, AI\u2019s potential annual value was between US$3.5 trillion and US$5.8 trillion. Retail is the industry expected to be most impacted by AI at US$0.4\u20130.8 trillion, followed by travel (US$0.3\u20130.5 trillion), and transport & logistics (US$0.4\u20130.5 trillion). Marketing & sales, and supply-chain management & manufacturing are sectors where AI can help companies grow US$1.2\u20132.6 trillion in annual revenue.\n\nAI can increase value up to 128 percent over traditional analytic techniques\n\nThe report says AI is more likely to improve performance over other analytic tools in 69 percent of the use cases McKinsey studied.\n\nThe industries with most potential incremental value benefit from AI compared to analytical techniques are travel (128%), transport & logistics (89%), and retail (87%). Industries at the bottom of the list are insurance (38%), advanced electronics/semiconductors (36%), and aerospace & space (30%).\n\nSixteen percent of the report\u2019s use cases are new applications developed by AI techniques, for example a smart customer service assistant in retail or medical imaging detection in healthcare.\n\nGetting accurate labeled data to train AI models is challenging\n\nLeveraging AI algorithms requires a large amount of clean and labeled data. The textbook Deep Learning, written by Google Researcher Ian Goodfellow and Head of the Montreal Institute for Learning Algorithms (MILA) Yoshua Bengio, suggests that a deep-learning algorithm can achieve acceptable performance by training with 5,000 labeled examples per category. If a model is expected to match or even exceed human level performance, it has to be trained with a dataset of at least 10 million labeled examples.\n\nCollecting large-scale datasets is challenging, particularly in industries such as healthcare where there is not so much available or useable data. Also vexing is data processing, including data cleansing and labeling, which is a difficult and time-consuming engineering problem. While advanced techniques such as reinforcement learning and GANs can effectively simulate data for academic research, they are not mature enough for wider implementation.\n\nMeanwhile, AI still has other limitations that need solutions. Interpretability, also referred to as \u201cblack box\u201d problem, means that even scientists cannot explain how an AI arrives at a decision. Google researchers recently attempted to create a step by step visualization of the process involved in a computer recognizing an object.\n\nThink twice before you embrace AI\n\nMcKinsey analysts suggest that AI is an elusive proposition for many companies as it remains unclear whether injecting huge investments in AI is worth the potential value the tech promises. There is also the concern that any careless technical executions could cause unpredictable, expensive or grave consequences, especially in sensitive fields like healthcare or legal systems.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-in-the-media-and-entertainment-industry-1ad4b2b701b8",
        "title": "AI in the Media and Entertainment Industry \u2013 SyncedReview \u2013",
        "text": "AI in the Media and Entertainment Industry\n\nThe Media and Entertainment industry is a cornerstone of contemporary human culture, delivering films, TV shows, advertisements and more in a multitude of languages across a wide variety of devices. A PwC report predicts total M&E revenue will reach US$2.2 trillion in the next three years. The industry\u2019s growth rate however has lagged, and slipped by 0.2% in 2017, prompting many companies to turn to AI technologies to boost business.\n\nWith the breakthroughs in machine learning, many intelligent products have made the leap from sci-fi movies to the home. Superhero Ironman\u2019s virtual helper JARVIS (Just A Rather Very Intelligent System) for example is echoed in smart assistants such as Alexa and Google Assistant, which may not catch criminals but can perform a range of practical tasks via connected household devices. NVIDIA meanwhile used VR technology to create a Holodeck similar to one in the sci-fi series Star Trek.\n\nAI technologies are also being applied in filming, visual design, post production etc.\n\nCurrent AI applications in the M&E industry are mainly in four categories: Marketing and Advertising, Service Comprehension, Search and Classification, and Experience Innovation.\n\nThe marketing and advertising sector includes visual design, film promotion and advertising. A machine learning algorithm trained with data such as text, stills and video segments can extract language, objects and concepts from its training resources and suggest marketing and advertising solutions to improve efficiency. Such a system can work as an assistant or even a content creator.\n\nAlibaba\u2019s Luban is an AI designer that can create banners thousands of time faster than a human designer. On China\u2019s online shopping extravaganza \u201cSingles Day\u201d in 2016 Luban generated some 8000 different banner designs per second and 170 million banners in total. The record output would of course be impossible for human designers to process in one day. On Singles Day 2017 Luban raised its one-day record to a staggering 400 million banners.\n\nIBM used their AI system Watson to help 20th Century Fox create a trailer for the horror movie \u201cMorgan.\u201d The research group trained the AI system to analyze and classify input \u201cmoments\u201d from visual, audio, and other composition elements in 100 horror movies to learn what kind of \u201cmoments\u201d should appear in a standard horror movie trailer. Watson needed just 24 hours to create a six-minute movie trailer that may have taken a human professionals weeks to produce.\n\nAlbert Intelligence Marketing\u2019s AI marketing platform accelerates the marketing process using predictive analytics, machine learning, NLP and computer vision technology. The Albert platform can perform audience targeting, customer solution making and generate autonomous campaign management strategies. Albert says companies using the platform report a 183% improvement in customer transaction rate and 600% higher conversation efficiency.\n\nAs user experience personalization becomes more important for the industry, companies are using AI to create personalized services for billions of customers. These include for example recommending content that fits users\u2019 personal tastes while they are browsing a video site or shopping online; and optimizing video definition and fluency for users with different internet speeds and bandwidth.\n\nNetflix\u2019s content recommendation got a boost in May 2016 when the company launched Meson, an intelligent workflow management and scheduling application. This AI system automatically manages the various machine learning pipelines that provide video recommendations. According to the Netflix 2016 annual report, there are 93 million global users streaming over 125 million hours of TV shows and movies per day on the platform. Predicting which shows will attract users\u2019 interest is a key component of the Netflix model.\n\nAI technology is also being applied to optimize video fluency and definition. Slow Internet connections and bandwidth limits can be a problem for streaming services in developing nations and for mobile device users. Netflix collaborated with the University of Southern California and the University of Nantes in France to develop a new machine learning methodology called Dynamic Optimizer which can compress video without degrading image quality to ensure a smooth and high quality streaming experience for its customers, whether they are in India or in Japan.\n\nThe Internet hosts countless media works. Video, audio and text can all be transformed into a digital copies which can be stored and spread so easily that it is getting increasingly difficult for people to find exactly what they want online. AI is helping optimize the accuracy of search results. Computer vision technologies meanwhile are also enabling content producers to better manage visual content and accelerate the media production process.\n\nAdvancements in machine learning technology have enabled Google to augment the world\u2019s leading search engine in multiple ways. One is in image searching. Rather than typing in keywords and checking returned images, users can upload a sample picture to Google Image, which uses image recognition technology to identify image features and search for similar pictures. Another advanced application involves selective link-building. Google applies AI to position ads appropriately \u2014 for example so a cat food ad appears in a pet-related website, but a bacon cheeseburger promotion will not appear on a site for vegetarians.\n\nClarifAI is an AI startup focusing on computer vision technology which partnered with Vintage Cloud to deploy AI on a film digitalization platform. By using ClarifAI\u2019s computer vision API, Vintage Cloud successfully accelerated the progress of movie content classification and categorization. It used to require dozens of hours for humans to recognize and manually classify objects in a movie. AI can do a better job in much less time.\n\nIn the past, papers and books were the main medium for words and images. The introduction of film and TV brought us into the dynamic new world of moving pictures. Now, AI is heralding a new age of immersive experience for visual content. This technology includes Virtual Reality (VR) and Augmented Reality (AR). With machine learning algorithms and computer vision technologies, developers can build complex and holographic scenes within a pair of goggles. This opens up a brand new market.\n\nVR gaming is one of the first areas that comes to mind, and this is where companies like HTC Vive, Samsung Gear VR, Oculus Rift, etc. are focusing their efforts. Various type of headsets have been introduced. Combined with motion sensing games, VR gaming innovation has become a hot market that shows no sign of slowing down.\n\nIntel is now getting into the immersive experience industry. With the application of deep learning and computer vision technology, Intel has become a visual content provider emphasizing Virtual Reality content. Supported by AI algorithms, Intel True VR Technology can perform every piece of a scene with pixels in three-dimensions.\n\nUsing the tech, fans can also watch sports in holographic view. Intel demonstrated this in their widely viewed VR game broadcast of the NFL 2018 Super Bowl. Intel partnered with the International Olympic Committee to broadcast the 2018 Winter Olympic Games as 360-degree video content. With a VR headset or even just a smartphone, fans and families could experience the action from the POV of an athlete.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/tomaso-poggio-on-deep-learning-representation-optimization-and-generalization-66bb8c8e524f",
        "title": "Tomaso Poggio on Deep Learning Representation, Optimization, and Generalization",
        "text": "Those outside academia may know Tomaso Poggio through his students, DeepMind Founder Demis Hassabis and Mobileye Founder Amnon Shashua. The former built the celebrated AI Go champion AlphaGo, while the latter has installed copilot systems in more than 15 million vehicles worldwide, and produced the world\u2019s first L2 autonomous driving system in a car.\n\nWhile Poggio the teacher has taught some extraordinary leaders in AI, Poggio the scientist is renowned for his theory of deep learning, presented in papers with self-explanatory names: Theory of Deep Learning I, II and III.\n\nHe is a Professor in the Department of Brain and Cognitive Sciences, an investigator at the McGovern Institute for Brain Research, a member of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Director of the Center for Biological and Computational Learning at MIT and the Center for Brains, Minds, and Machines.\n\nPoggio\u2019s research focuses on three deep learning problems: 1) Representation: Why are deep neural networks better than shallow ones? 2) Optimization: Why is SGD (Stochastic Gradient Descent) good at finding minima and what are good minima? 3) Generalization: Why is it that we don\u2019t have to worry about overfitting despite overparameterization?\n\nPoggio uses mathematics to explain each problem before inductively working out the theory.\n\nPoggio and mathematician Steve Smale co-authored a 2002 paper that summarized classical learning theories on neural networks with one hidden layer. \u201cClassical theory tells us to use one layer networks, while we find that the brain using many layers,\u201d recalls Poggio.\n\nBoth deep and single-layer networks can approximate continuous functions. This was one reason why research in the 80s focused on simpler single-layer networks.\n\nThe problem occurs in the dimensionality of single-layer networks. In order to represent a complicated function, a single-layer network would require more units than the number of atoms in the universe. Mathematically, this is called \u201cthe curse of dimensionality,\u201d wherein the number of parameters goes up exponentially corresponding to function dimensionality.\n\nMathematicians make assumptions about function smoothness in order to escape the curse of dimensionality. Yet deep learning offers a different approach that uses compositional functions. The units that deep networks require to approximate a compositional function share a linear relationship with function dimensionality.\n\nDeep learning works beautifully for datasets that are compositional in nature, such as images and voice samples. Images can be broken down into related snippets of details, while voice samples can be converted into meaningful phonemes. For an image classification task, there\u2019s no need to look at pixels that are further apart, the model simply observes each small bit and combines them. The neural network escapes the curse of dimensionality by using a very small number of parameters.\n\nIf the target is a function made up of functions with a smaller number of variables, then a deep network can approximate it with a number of units that is linear in dimensionality no matter how big the function is.\n\nKnowing that compositional functions work well with deep networks is far from enough. \u201cFor a computer scientist or a mathematician, can we say something more about compositional functions beyond the fact that they\u2019re compositional? Can we characterize them to get a better understanding of neural networks? This is an interesting open question,\u201d says Poggio.\n\nDeep networks have far more parameters than the number of examples in the training set.\n\nThe CIFAR dataset has 60,000 examples, and we use networks with millions of weights to process it. This is a typical case of overparameterization. We can make a hypothesis to simplify the matter: if one replaces nonlinear neurons in the deep network with univariate polynomials, then getting zero training errors on CIFAR means solving 60,000 polynomial equations. We now have infinite sets of solutions according to B\u00e9zout\u2019s theorem, which ends up becoming the dataset\u2019s infinite global minima.\n\nThus overparameterization guarantees lots of global minima that form flat valleys in the loss space. As SGD is known for its preference for flat valleys, there is a high probability that SGD will find the global minima for neural networks.\n\nPoggio\u2019s work showed that a combination of overparameterization and SGD simplifies the optimization of neural networks.\n\nOverparameterization is good news for optimization, but a nightmare when it comes to generalization. Test errors go down but then up again in classic machine learning, which is called \u201coverfitting.\u201d Yet in deep learning, overfitting is not reported, and so the test error rate goes down and stays there.\n\nWhy is this the case? Poggio likens this to a \u201cchemical reaction\u201d that occurs when classification tasks are mixed with a specific type of loss functions called cross entropy.\n\nAlthough we can use 0\u20131 loss to evaluate error rates, we need an alternative approach when it comes to loss function. Take handwritten digit classifiers as an example, the last step for the neural network is to turn a softmax into a \u201chardmax\u201d, in other words, a class. Thus, even if we only have a bad model that is only 30% sure that the \u201c1\u201d we show it is a \u201c1\u201d, as long as 30% is the highest among the given 10 possibilities, the model will classify the image correctly. Of course no one would be satisfied with a 30% success model. The model needs further optimization, which can\u2019t be done using a 0\u20131 loss.\n\nIn case of cross entropy, as long as the model is not 100% certain, one can continue to optimize it by calculating another gradient, and use backpropagation for fine-tuning. On a side note, the favourable property of using cross entropy as the loss function and 0\u20131 loss as error metrics is that, even when cross entropy is overfitted, the 0\u20131 loss will work just fine. A few months ago, University of Chicago researcher Nathan Srebro and his colleagues proved this for a special case of linear networks with separable datasets.\n\n\u201cOn top of [Srebro\u2019s work], we\u2019ve shown that using differential equations from dynamical system theory will make a deep network behave like a linear network near a global minimum. We can use the Srebro result to say the same thing about deep learning, even if a deep neural network classifier has an overfitting cross entropy, the classifier wouldn\u2019t overfit,\u201d says Poggio.\n\nThis property of cross entropy is shared with loss functions such as exponential loss, but not with simpler ones like the least square error. Why is this the case? Poggio says this remains an unsolved question.\n\nPoggio says his opinion on the shape of minima and their corresponding generalization capabilities has changed recently: \u201cPeople said in papers that flatness is good for generalization. I also said something like this a year ago, but I don\u2019t think this is true anymore.\u201d\n\n\u201cI don\u2019t see a direct relation between flatness and generalization. Generalization relies on properties like choosing classification as the task, choosing cross entropy as the loss function but not flatness. There\u2019s a paper of which two out of four authors are Bengios, which proves that even sharp minima can generalize because you can change the weights in different layers to make it sharp without changing the input-output relation of the network.\u201d\n\nPoggio also doesn\u2019t think it\u2019s possible for a flat minimum to exist, at least not for neural networks that are polynomial in nature.\n\nLearning deep learning theory can be enlightening, but engineers working on applications ask the question: How can theoretical research work help me train my model?\n\nThe No Free Lunch Theorem tells us that two learning algorithms are equal when no prior information is offered for distribution. For any algorithms A and B, there are as many distributions in which algorithm A outperforms B, as distributions in which B outperforms A.\n\nPoggio utilizes the theorem to propose that in machine learning, no algorithm can work the best for every problem. \u201cTheory tells you about the average case and the worst case, what you should or should not do to avoid bad things. But it can\u2019t advise you on the best thing to do for any particular case.\u201d\n\nPoggio suggests engineers who employ deep learning models be careful of overfitting, \u201cOne lesson to learn from the past few decades of machine learning is that when you don\u2019t have enough data, then after many trials, the state-of-the-art method is usually overfitting. It\u2019s not because people have peeped at the validation set, it\u2019s just that the community of researchers has tried too many different algorithms.\u201d\n\n\u201cI\u2019m a physicist by origin. When I was in school, the rule of thumb was that if you have a model or a set of equations with n parameters, you need at least 2n data points. If you want to do something statistical, the recommendation was to have 10n data points. Nowadays people use 300,000 parameters for datasets of any size. The arguments we make like \u2018deep learning models tend not to overfit\u2019 is only true for classification tasks with nice datasets, so people should be more careful about that.\u201d\n\nHumans don\u2019t need millions of pieces of labeled data to learn, thanks to prior knowledge carried in our genes. \u201cThere is not a simple answer for how many priors we need for a model. There are only situations where we know the minimum priors needed to make predictions.\u201d\n\nPoggio uses regression as an example, \u201cIf I want to reconstruct a curve from points, I can\u2019t do anything unless I have all the points. Continuity is essential but not enough, the least I need is something like smoothness. In the end, it\u2019s a tradeoff between how strong the priors are and how much data you need.\u201d\n\nMIT has a tradition of knitting together deep learning and neuroscience, so what is Poggio\u2019s view on learning from the human brain?\n\n\u201cI think it\u2019s unlikely, but not impossible, that things like backpropagation can be done biologically, given what we know about neurons and signal processing. What I think is impossible is labeling everything.\u201d\n\nHow our brain gets around labeling is an interesting question. Poggio assumes that our visual system for example is pre-trained to \u201ccolour-fill\u201d an image. It receives the colour information but only gives black, grey, and white signals to the visual cortex. You do not need an oracle to tell you what the real colour is, your brain hides this part of the information, so that \u201ccolours are measured but not given to the [brain] network,\u201d explains Poggio.\n\n\u201cThe hope is that if you train a network to predict colour or the next image, can this network do other things easier? Can it learn to recognize objects with less data?\u201d asks Poggio. \u201cThese are open questions that, once we get the answers, the whole deep learning community would benefit from.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-test-self-driving-food-delivery-with-meituan-eb3916f3ac83",
        "title": "Baidu to Test Self-driving Food Delivery with Meituan",
        "text": "Xiongan City is getting a new food delivery service that requires no interaction with humans.\n\nChinese media is reporting that Baidu will launch a series of self-driving delivery tests in cooperation with Meituan-Dianping, a group-buying service for local food and retail businesses. The first tests will take place in Xiongan, about 100 kilometres southwest of Beijing.\n\nAn insider familiar with the matter said testing was slated to begin on May 1, however Xiongan is still building the required infrastructure, and so the official announcement will be postponed.\n\nAutonomous vehicles can not only cut labor costs in the food-delivery business, but can also for example reduce security issues for batch deliveries to restricted locations such as construction sites.\n\nBaidu is leading the development of China\u2019s autonomous driving technologies. Last June, the search engine giant introduced its autonomous driving platform Apollo, which is billed as \u201cthe Android of the auto industry.\u201d Apollo aims at democratizing autonomous driving with open-source data and code. Last month, Baidu became the first company granted special license plates for autonomous vehicle testing on public roads in Beijing.\n\nAlso on Thursday, Baidu released the latest iteration of its Apollo platform, Apollo 2.5, which now supports autonomous driving on geo-fenced highways. Baidu also announced the new Apollo Automotive Cybersecurity Lab, which aims to advance safety in mobility.\n\nMeituan set up a project team on self-driving delivery R&D in 2016. Last year the team was promoted to a business division with a staff of over 200.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-national-ai-team-gets-busy-46f226d7d054",
        "title": "China\u2019s \u201cNational AI Team\u201d Gets Busy \u2013 SyncedReview \u2013",
        "text": "Words often have different meanings in the business world. A \u201cUnicorn\u201d for example is not a magical horse with a horn projecting from its forehead, but rather a startup that\u2019s valued at more than a billion dollars.\n\nA phrase that\u2019s cropping up a lot in Chinese media these days is \u201cNational AI Team (\u4eba\u5de5\u667a\u80fd\u56fd\u5bb6\u961f).\u201d While this may suggest a squad of scientists heading off to some sort of AI Olympics, it rather refers to a new class of AI companies that are either backed by national institutes or closely integrated into government-funded programs.\n\nWhy are they calling themselves the National AI Team?\n\nCloudWalk, a spinoff from Chongqing Institutes of Green and Intelligent Technology (CIGIT) of the Chinese Academy of Sciences (CAS), is a typical National AI Team member. Founded in 2015, the company specializes in computer vision and machine learning.\n\nIn China, core industries such as public security, banking, political communication, civil aviation, and urban transport are being overhauled through the concerted application of various AI techniques. Security bureaus for example want object tracking systems to help spot criminal activity and hunt fugitives, while banks are using facial recognition algorithms to provide customers with almost-instant access their bank accounts. The government owns or controls most of these industries, and they are rigorous in selecting their technical partners.\n\n\u201cIf a bank\u2019s system goes down for two hours, its governor will have a problem; if it doesn\u2019t work out in four hours, he will write a report; if it doesn\u2019t work out in eight hours, it\u2019s going to be a serious incident, the bank\u2019s rating is bound to drop, and the bank may even be closed,\u201d CloudWalk Founder and CEO Xi Zhou (\u5468\u66e6) told Synced.\n\nWhile a private company\u2019s best way to attract government clients is robust technology or products, being on the National AI Team can also give it an advantage in core industries. Over 100 banks have adopted CloudWalk\u2019s facial recognition technique, making it the largest provider of the tech to China\u2019s banking industry. The company also develops and deploys facial-recognition-enabled surveillance cameras in over 80 percent of domestic airports.\n\n\u201cI was not aware of this advantage until I founded CloudWalk,\u201d says Zhou. \u201cObviously the state-owned banks and public security departments have more faith in our technologies and products because we come from the Chinese Academy of Sciences.\u201d\n\nWhat if a startup does not enjoy a relationship with a state-owned institute? They might also look for investment from a state-owned venture. China\u2019s computer vision startup Face++ announced financing of US$460 million last November led by the China State-Owned Capital Venture Investment Fund, while the Guangzhou Municipal Government injected US$300 million into CloudWalk\u2019s Series B funding round.\n\nThe Chinese Government owns a large-scale database, which is one of the most important components for developing AI applications and products. Li Xu (\u5f90\u7acb) is the CEO of SenseTime, a company that uses deep learning to replicate tasks performed by the human visual system. In an interview with Quartz, Xu boasted that his company has a database of over two billion images. Much of that data comes from various government agencies.\n\nSenseTime raised US$410 million last June, including a B2 round led by Sailing Capital, whose major shareholder is the large state-owned financial holding company Shanghai International Group. SenseTime is now the world\u2019s most valued AI startup. The injection of the state-owned capital is expected to help SenseTime secure more government contracts. At present, 30 percent of SenseTime\u2019s clients are government-related.\n\nWhile National AI Team status may be an \u201cexpress lane\u201d to government deals, the role also comes with responsibilities. Last year China issued an ambitious policy blueprint calling for the nation to become the world\u2019s primary AI innovation center and aiming to build a domestic industry worth some US$150 billion by 2030. The government is counting on its National AI Team to put that plan into practice.\n\nOver the past five years, the world has seen an explosion of activity in deep learning in both academia and industry all around the world. While AI is energizing industries, the lack of industry-wide standards for research and development is creating a number of problems: How to correctly access users\u2019 data without violating privacy? How to qualify a dataset for use in training AI algorithms? How to ensure the safety and precision of an AI application?\n\nCarlos E. Perez, author of the book Artificial Intuition and Founder of Intuition Machine, stressed the need to standardize the best practices of developing deep learning in his medium blog. \u201cIn conventional software development, we have a more mature conceptual framework that has evolved over time. Deep Learning introduces new kinds of requirements, so we need to understand what these are and what standardize the class of tools needed.\u201d\n\nThe Chinese Government is pushing hard for AI standardization. Last year it released its Standardization of AI Helps Industry Development along with a blueprint for further AI integration with the economy. This year it followed up with the Artificial Intelligence Standardization White Paper.\n\nIn March 2017 the National Development and Reform Commission launched the Public Service Platform for Basic Resources (\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u8d44\u6e90\u516c\u5171\u670d\u52a1\u5e73\u53f0) with a heavy focus on AI. This platform will be built over the next three years and aims to deliver:\n\nProminent National AI Team players Baidu, Tencent, iFlytek, and CloudWalk were designated to take the lead on the Platform\u2019s development.\n\nChina\u2019s voice technology giant iFlytek was founded by alumni of the University of Science and Technology of China, a national research university, while Baidu and Tencent have dominated the country\u2019s tech scene for years.\n\nCloudWalk spokesperson Xiaolong Fu told Synced, \u201cAI is closely related with data and privacy, so National AI Team members can help ensure information security. Meanwhile, we also want to ensure that our intellectual property remains in our own hands.\u201d\n\nChina\u2019s quest to develop cutting-edge AI technologies provides a world of opportunities for companies on the National AI Team, particularly with the increasing number of government initiatives and state-run projects in the field. The Team may also spur development and deployment of improved AI resources for academic, industry and public use.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/tencents-smart-speaker-tingting-brings-wechat-to-the-entire-family-e0787eba8391",
        "title": "Tencent\u2019s Smart Speaker Tingting Brings WeChat to the Entire Family",
        "text": "Tencent\u2019s WeChat is like the Chinese equivalent of WhatsApp + Facebook. Each day hundreds of millions of users press the WeChat Hold to Talk button to send voice messages.\n\nNow Tencent wants to free up its users\u2019 hands. The Chinese Internet giant today officially launched its first-ever smart speaker, Tencent Tingting (\u817e\u8baf\u542c\u542c), which can access users\u2019 WeChat accounts and send or receive voice messages. Moreover, Tencent Tingting can automatically convert text messages to voice format and read them out to you.\n\nThe speaker has a Mandarin interface, and is specially designed to accommodate the elderly and children, who either have no phone or find it difficult to use one. The company says it has tailored Tingting\u2019s speech recognition algorithms to better recognize the particularities of speech in these age demographics.\n\nMany Chinese households have three generations living under the same roof. Tingting aims to create a family-friendly communication environment where grandparents won\u2019t have to put on their reading glasses if they get a text and toddlers can easily talk with their parents remotely. Tingting users need not have a WeChat account.\n\n\u201cWhen my grandpa was speaking to Tingting, I didn\u2019t even know what he said, but the speaker understood,\u201d says Tencent Tingting Chief Products Officer Chaoqin Wang. \u201cWhen the speaker replied to him, he said \u2018Oh, not bad. It\u2019s amazing\u2019. I could feel that the old man was very pleased that he was understood by a modern Internet product.\u201d\n\nThe sleek and cylindrical Tingting comes in black or lime. It has an amusing wake-up word, \u201c9420\u201d (Jiu Si Er Ling), which sounds like \u201cI just love you\u201d in Mandarin. A self-adaptive tuning function can adjust volume based on environment.\n\nTencent boasts that Tingting outperforms its competition in acoustic quality. Music Critic Lihua Su appreciated the full sound, which he said didn\u2019t come from a \u201cpoint,\u201d but rather an \u201carea.\u201d \u201cThe unique speaker design makes anywhere in a room a \u2018sweet spot\u2019,\u201d he told Synced.\n\nTingting features an unplugged design with lithium batteries that can power it for up to 16 hours, five hours in WIFI mode, or six hours in Bluetooth mode. This was a bold choice by Tencent, as integrating batteries with smart speakers raises many technical challenges, including how to compress the algorithms to lower power consumption.\n\nThe speaker\u2019s content library is stuffed with more than 20 million original songs, one million audio stories for children, and one hundred million hours of other audio content. \u201cFor a speaker, content is at the core of customer needs,\u201d says Wang.\n\nTencent\u2019s announcement signals the company\u2019s entry into China\u2019s heated smart speaker market, where Baidu\u2019s Raven H, Alibaba\u2019s Tmall Genie and hundreds of competitors are all vying for the vanguard.\n\nTingting is sold on JD.com at \u00a5699(US$100), which is slightly more expensive than competitors like Xiaomi\u2019s Mi AI speaker Mi at CN\u00a5299 (US$45), and Tmall Genie at CN\u00a5499 (US$80). Genie even went on sale at the rock-bottom price of CN\u00a599 (US$15) during China\u2019s November 11 \u201cSingles Day\u201d online shopping spree.\n\nTencent bills Tingting as a powerful smart assistant with compelling features and superior performance, and it looks and sounds the part. Although Tencent hasn\u2019t yet built much of a reputation in the hardware market, insiders suggest the company views smart speakers as a serious long-term business, and there will be more Tencent smart speakers coming up.\n\nSynced is covering the story and will update readers on Tingting\u2019s performance in the market."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-ups-its-chip-game-with-ali-npu-6dbc469deeb7",
        "title": "Alibaba Ups Its Chip Game With Ali-NPU; Acquires Chipmaker C-Sky Microsystems (Updated)",
        "text": "Alibaba\u2019s R&D arm DAMO Academy announced yesterday that it is developing a new neural network chip called Ali-NPU for AI inferencing in the field of image processing, machine learning, etc.\n\nAli-NPU is expected to be some 40 times more cost-effective than conventional chips. Alibaba Researcher Yang Jiao says this chip\u2019s performance will be 10 times better than mainstream CPU and GPU architecture AI chips currently on the market, with only half the manufacturing cost and power consumption. By self-developing AI chips tailored to its own needs, Alibaba will reduce its dependence on other chip companies.\n\nIn 2015, Alibaba cooperated with Hangzhou C-SKY Microsystems to develop the Yun On Chip (YoC). In November 2016, Alibaba and Tencent invested US$23 million in California chip company Barefoot Networks.\n\nDAMO Academy is an R&D institute for fundamental and disruptive technology research established by Alibaba in November 2017. It has a dozen top-notch international scientists working in quantum computing, machine learning, network security, visual computing, natural language processing, chip technology, and embedded technology. Alibaba is planning to inject US$15 billion into this project over the next three years.\n\nAlibaba announced yesterday that it has acquired chipmaker C-Sky Microsystems in a bid to strengthen its chip-making R&D capability. Financial terms of the deal were not disclosed.\n\nFounded in 2011, Hangzhou-based C-Sky Microsystems is an integrated circuit design company dedicated to 32-bit, high-performance and low-power embedded CPU, with chip architecture licensing as its core business. The company\u2019s embedded CK-CPU chip software and hardware platform provides customers with CPU IP core, System-on-Chips (SoC) design and development platforms.\n\nC-Sky Microsystems bills itself as \u201cthe only embedded CPU volume provider in China with its own instruction set architecture.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/mits-josh-tenenbaum-on-intuitive-physics-psychology-in-ai-99690db3480",
        "title": "MIT\u2019s Josh Tenenbaum on Intuitive Physics & Psychology in AI",
        "text": "When Bayesian Cognitive Scientist Josh Tenenbaum recently told a packed University of Toronto lecture hall that \u201cintelligence is not just about pattern recognition,\u201d the significance of the statement was not lost on the audience.\n\n \n\nThe university is the birthplace of the \u201cHinton School of Neural Networks\u201d, where Geoffrey Hinton redesigned the neural network approach to AI by coding a synthesized biological circuitry of neurons into artificial models that excelled in pattern recognition, trailblazing a new path for the development of artificial intelligence.\n\n \n\nFollowing an introduction by Hinton, Tenenbaum quipped about the old days, when Hinton \u201cwas willing to be called a cognitive scientist.\u201d This drew laughter from the audience, considering the latter\u2019s eminence in the field of computer science. As the principal AI investigator at MIT\u2019s Center for Brain, Minds, and Machines (CBMM), Center for Brain and Cognitive Sciences (BCS), and Computer Science and Artificial Intelligence Laboratory (CSAIL), Tenenbaum is widely respected for his interdisciplinary research in cognitive science and AI. \n\n \n\nBreakthroughs in AI and deep learning have prompted neuroscientists such as Dan Yamins from the Stanford NeuroAILab to rethink the structure of the ventral stream \u2014 the object and visual recognition part of the brain. This cross-pollination is hardly a surprise, considering that early publications on neural networks frequently appear in journals like Psychological Review, Cognitive Science, and Nature, as the field moved forward from the 50s\u2019 single-layer perceptron to Kunihiko Fukushima\u2019s neocognitron in the 80s, to Yan LeCun\u2019s widely-used deep convolutional neural networks of today.\n\nHowever, if humans hope to develop artificial general intelligence, data-munching, pattern-seeking deep neural networks may not be the best approach. Might Bayesian networks, causal models, and predictive coding work better? Or a symbol manipulation engine modeled after logic, lambda calculus, and programming languages be the route to pursue? Tenenbaum wants to steer the research wheel to cognitive science and look for the answer there.\n\nHuman common sense involves the understanding of physical objects, intentional agents and their interactions, which Tenenbaum believes can be explained through intuitive theories. This \u201cabstract system of knowledge\u201d is based on physics (eg. forces, masses) and psychology (eg. desires, beliefs, plans).\n\n \n\nSuch intuitions are present even in young infants, bridging perception, language, and action planning capabilities. A 2011 study by Erno Teglas models the physics reasoning capability of 12-month-olds, while Elizabeth Spelke\u2019s pursued a similar research course in her paper psychological inferencing capability in 10-months-old.\n\nHow do we use computation models to reverse-engineer intuitive theories and teach an AI to evolve based on these principals? Tenenbaum suggests tackling the problem by using a new class of programming language called Probabilistic Programs, which is a compound of symbolic language, probabilistic inference, hierarchical inference (learning to learn), and neural networks.\n\nIn collaboration with Tenenbaum\u2019s group at MIT, DeepMind researcher Peter Battaglia is working on \u201ca realistic model of physics that can estimate physical properties and predict probable futures,\u201d to quote from the paper he co-authored with Tenenbaum, Computational Models of Intuitive Physics. Based on Bayesian inferencing, this model makes predictions in simulated 3D scenarios based on real-life statics, dynamics, forces, collisions, and friction. \n\n \n\nFacebook AI proposed PhysNet in 2016, a neural network that predicted whether a stack of blocks would fall. The network excelled in predicting outcome and estimating block falling trajectories, discerning them based on color. Tenenbaum says that for a prediction involving 2\u20134 cubes, PhysNet required over 200k training scenarios.\n\nIn the joint research program Learning Physics from Dynamic Scenes, developed with Stanford\u2019s Noah Goodman, Tenenbaum proposes a hierarchical Bayesian framework, working with probabilistic programs to model intuitive physical theories. The project trains the model on inferring physics scenarios in varying time periods, with different physical laws and properties at play. Inferring properties include mass, charge, friction, elasticity, and resistance.\n\n \n\nIn order to model intuitive psychology, researchers use a model wherein the agent considers its desires and its beliefs about the environment, which enables planning and then actions. Jara-Ettinger and Julian Schulz at Yale call this the \u201cnaive utility calculus.\u201d Co-authored with Tenenbaum, Jara-Ettinger\u2019s 2017 paper in Nature proposes a Bayesian theory of mind (BToM) model that infers an actor\u2019s beliefs, desires, and percepts from how they move in the local environment.\n\nTenenbaum lectures at the nexus of AI, cognitive science, and neuroscience. As he notes in the 2017 paper Building Machines That Learn and Think Like People co-authored with Brenden Lake, Tomer Ullman and Sam Gershman, the most immediate task for AI is to \u201ca) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and \u00a9 harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations.\u201d \n\n \n\nResearch in intuitive physics and psychology are especially promising in the field of robotics. A robot that knows intuitive physics can navigate the environment and perform nuanced actions such as carrying a cup of coffee, grasping a party balloon, and so on. Meanwhile, a robot that knows intuitive psychology by heart could observe, for example, a child pointing at cotton candy while crying as its parent shakes head, \u201cno,\u201d and would be able to correctly infer both humans\u2019 intentions.\n\n \n\n Tennenbaum is aiming for an AI that more completely understands the physical and psychological landscapes it will exist in. Such machines may also allow us to deepen our own understanding of intelligence.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-gears-up-for-self-driving-road-tests-6d603b45682e",
        "title": "Alibaba Gears Up for Self-Driving Road Tests \u2013 SyncedReview \u2013",
        "text": "China\u2019s self-driving industry has just welcomed another major player: Alibaba Group is developing Level 4 autonomous driving technology and is ready to test its self-driving cars on public roads, Chinese media reported today.\n\nThe e-commerce giant has assigned its Chief Scientist and former Nanyang Technological University in Singapore Associate Professor Gang Wang to lead the project. Alibaba is looking to add more than 50 self-driving experts and specialists to the project to maximize its R&D capability, according to an Alibaba insider.\n\nAlibaba is also ready to conduct its first road test with a converted Lincoln MKZ, a model widely used by self-driving companies such as Baidu, NVIDIA, and many startups. The vehicle will test L4 autonomous driving capabilities, a level defined as \u201cfully autonomous\u201d in that the car can be operated without a human driver in most circumstances.\n\nAlibaba\u2019s engagement in self-driving technology began last year when it formed a partnership with Chinese domestic automakers SAIC Motor and Dongfeng Peugeot-Citro\u00ebn, and equipped the cars with its upgraded operating system AliOS.\n\nAlibaba\u2019s ambitious move represents a direct challenge against China\u2019s other internet giants Baidu and Tencent, which have been making substantial progress in the field of self-driving cars for several years. Baidu leads the race with its autonomous driving platform Apollo, which the company bills as \u201cthe Android of the auto industry.\u201d Just three weeks ago, Baidu received approval from the Beijing Municipal Government for testing its self-driving cars on Beijing roads. Tencent, meanwhile, reportedly tested one of its autonomous vehicles on Beijing highways earlier this month.\n\nWhile Uber and Tesla accidents and scandals in the US may have weakened public confidence in self-driving vehicles, Alibaba\u2019s move into the field is another sign that China remains dedicated to encouraging development and implementation of the technology.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-apr-w-2-apr-w-3-a101f46dd5ef",
        "title": "AI Biweekly: 10 Bits from Apr W 2 \u2014 Apr W 3 \u2013 SyncedReview \u2013",
        "text": "IBM announces a four-year program in collaboration with Calgary-based Natural Resources Solutions Center to help oil and gas companies with sustainability and efficiency. IBM\u2019s AI platform Watson will digest raw data, structure it and come up with insights for oil and gas companies.\n\nAmazon adds a musical upgrade to Alexa\u2019s \u201croutine\u201d function, which enables users to combine multiple actions such into one command \u2014 for example to turn on both the coffee maker and kitchen lights in the morning. Alexa can now play music, podcast or radio shows as part of a routine, and users can also control audio output on the device. The move brings Alexa up to par with Google Assistant, which already integrates radio, podcasts and music.\n\nMontreal-based incubator Element AI will open a new office in Toronto where it will do R&D and work with local businesses that want to integrate AI into their operations and the Ontario Provincial Government. Element AI\u2019s July 2017 fundraising round raised a record US$137.5 million. This will be Element AI\u2019s fifth office, and the company is considering opening more in Asia in the near future.\n\nCalifornia\u2019s Public Utilities Commission has introduced a proposal to allow autonomous vehicles without a backup driver to transport passengers on state roads. The California Department of Motor Vehicles had already approved autonomous driving tests without a backup driver starting this month. The new regulations would allow members of the public to ride in such vehicles, and will be voted on next month.\n\nApril 10th \u2014 Microsoft Collaborates with C3 IoT to Accelerate AI in Enterprise\n\nMicrosoft and software company C3 IoT have announced a partnership to accelerate cutting-edge business level AI and IoT application development in areas such as AI predictive maintenance, dynamic inventory optimization, precision healthcare, and CRM. The co-marketing and co-selling deal also includes a co-development strategy on Microsoft\u2019s Azure cloud platform.\n\nQualcomm announces its own IoT and AI-optimized system on a chip (SoC) platform. The new platform is aimed at computer vision IoT applications such as security cameras, wearable cameras, and smart displays.\n\nAirFusion Wind is a cloud-based workflow and AI-based analysis platform that can identify and classify wind turbine asset damage. AirFusion Wind transforms inspection images from drones, ground-based sensors, and other image capture tools into data that can be used to evaluate conditions and reduce costs.\n\nApril 11th \u2014 NVIDIA Collaborates with Canon Medical Systems to Accelerate Deep Learning in Healthcare\n\nNVIDIA announces a collaboration with Canon Medical Systems to develop research infrastructure to support deep learning technology in the healthcare industry. The partnership will focus on deploying deep learning and big data analytics to support early detection and assisted diagnosis. Canon Medical Systems is the largest medical systems supplier in Japan, and will use the NVIDIA DGX system to process the massive data generated by its platform.\n\nGoogle\u2019s natural language processing and synthesis research just got fun. The company has uploaded two playful and creative interactive web experiments based on its word-association systems. These are offshoots of a new search option that allows users to directly ask the system questions instead of searching for specific words, titles, or authors, etc. The results are not perfect, but the system provides a useful and flexible new query function.\n\nApril 13th \u2014 Facebook Uses AI to Predict and Sell User Preferences\n\nThe Intercept creates a stir when it publishes apparently confidential internal Facebook documents that describe how AI can build predictive models of user behaviour that Facebook can sell to brands, a process some argue is unethical. The Intercept article argues that although Facebook says these algorithms are used to improve the user experience, in fact they are largely used to make money from advertisers.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/iclr-2018s-best-papers-variant-adam-spherical-cnns-and-meta-learning-6b48dca83e8b",
        "title": "ICLR 2018\u2019s Best Papers: Variant Adam, Spherical CNNs, and Meta-Learning",
        "text": "Leading machine learning conference International Conference on Learning Representations (ICLR) has named its best research papers of the last year: On the convergence of Adam and Beyond, Spherical CNNs, and Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments.\n\nLaunched in 2013, the ICLR has grown to a world-class conference for machine learning researchers and engineers. ICRL 2018 received 935 papers \u2014 double last year\u2019s total \u2014 and 337 papers were accepted.\n\nIn On the Convergence of Adam and Beyond, Google New York proposes the new variant Adam, a gradient descent optimization algorithm introduced in ICLR 2015.\n\nGradient Descent is one of the most popular algorithm types for optimizing neural networks, but struggles with a convergence issue in non-convex settings that makes optimizations ineffectual. The Google paper suggests a new Adam algorithm which it says fixes the problem and improves the empirical performance.\n\nResearchers at the University of Amsterdam proposed Spherical Convolutional Neural Networks (CNNs) which can analyze spherical images, a technique in wide demand for drones, robots, autonomous cars, molecular regression problems, and global weather and climate modelling. The paper demonstrates that spherical CNNs can be efficiently applied to 3D model recognition and atomization energy regression.\n\nIn Continuous Adaptation via Meta-learning in Nonstationary and Competitive Environments, top academic institutes CMU, UMass Amherst, OpenAI, and UC Berkeley jointly developed a simple gradient-based meta-learning algorithm that can adapt to dynamically changing and adversarial environments.\n\nOver the past few years, reinforcement learning (RL) has successfully enabled machines to outperform humans in tasks ranging from Atari video games to the ancient and complex Chinese board game Go. However, the AI technique is not adaptable to non-stationary environments, for example a multiplayer game with high randomness. Meta-learning, the so-called learning-to-learn method, can compensate for RL\u2019s weakness.\n\nThis paper also introduced a new multi-agent competitive environment, RoboSumo, for more effective training of meta-learning algorithms.\n\nICLR 2018 runs April 30 to May 3 at the Vancouver Convention Centre in Vancouver, Canada.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/how-ai-can-speed-up-drug-discovery-3c7f01654625",
        "title": "How AI Can Speed Up Drug Discovery \u2013 SyncedReview \u2013",
        "text": "The US FDA defines five steps for the development of a new drug: discovery and development, preclinical research, clinical research, FDA review, and FDA post-market safety monitoring.[1] The Tufts Center for the Study of Drug Development estimates the average cost of developing a new drug at US$2.55 billion, with the process potentially taking more than 10 years.[2]\n\nThe first step, drug discovery, typically involves one of four scenarios: finding new insights into a disease, finding possible effects of a drug by testing molecular compounds, repurposing existing drugs, or manipulating genetic materials. At the drug discovery stage, thousands of compounds may be potential candidates for development as a medication.[3] They need to go through a series of tests and only a small number will advance to further research.\n\nTo accelerate drug discovery and reduce the costs of drug development, pharmaceutical companies are introducing AI technologies such as machine learning and deep learning into the processes.\n\nDrug discovery is a data-driven environment with a massive of data such as high-resolution medical images, genomic profiles, metabolites, molecular structures, and biological information.[4] This information is published in papers and journals, however it can be a challenge for researchers to keep up with it. AI can use machine learning and deep learning to correlate, assimilate, and connect existing data more rapidly in order to help discover patterns in the data pools. By reviewing scientific research papers, AI can make connections that provide possible hypotheses for drug discovery.\n\nFinding new compounds for a medicine is difficult because the possible combinations are countless. Such research requires medical data on genes, proteins, metabolites, molecular structures, and biological information.[7] Processing this huge amount of information can be a very time-consuming task. Pharmaceutical companies are discovering that AI techniques such as deep learning algorithms can process the same information much faster.\n\nDrug repurposing, also known as drug repositioning or therapeutic switching, is the application of known drugs and compounds to treat new indications.[11] One advantage of drug repositioning is that most of the repositioned drugs have already passed a series of tests, and so have less risk of unexpected toxicity or side effects. With the help of machine learning algorithms, pharmaceutical companies can repurpose drugs faster and at lower costs than developing new drugs.\n\nManipulating genetic materials for drug discovery is also known as personalized medicine or precision medicine. This kind of drug discovery can be more effective in treatment because it is based on individual health data paired with predictive analytics.[14] In order to efficiently gather, analyze, store, and trace a person\u2019s detailed information, especially when the data is huge and unstructured, pharmaceutical companies use deep learning, machine learning, or computer vision.\n\nWe know that artificial intelligence can be applied in drug discovery to make the process faster. There are also areas where artificial intelligence can help in drug development. Clinical trials, for example, are currently classified into five phases and usually require more than three thousand test subjects to proceed from phase one to phase three.[17] Most pharmaceutical companies use recruitment firms to find clinical trial subjects by examining individual medical records.[18] This task takes time and the efficacy is low. Companies can use machine learning to train a model that includes age, sex, treatment history, and current health status to build an inclusion/exclusion criteria that will speed up this aspect of clinical trials research.\n\nMoreover, AI can help test the side effects or toxicities of candidate drugs. Cyclica, a Canadian startup, uses a suite of computational algorithms to evaluate and predict how drugs might interact with the human body.[19] This kind of testing helps pharmaceutical companies identify a drug candidate\u2019s side effects before clinical trials, so companies can make corrective adjustments in advance.\n\nDrug discovery can benefit from machine learning, deep learning, and computer modeling. However, there is also the potential for introduction of biases from unbalanced data, which might cause errors or discrimination while AI is training the neural networks.[8] A research team from Insilico Medicine discovered that accuracy could become unstable unless the neural network had been trained using diverse datasets. The range, quantity and quality of input data is therefore a key factor for further implementation of AI in drug discovery.\n\n[4] AI Provides New Insights for Accelerated Drug Development: https://blogs.sap.com/2017/11/02/ai-provides-new-insights-for-accelerated-drug-development/\n\n[5] AI in Pharma and Biomedicine \u2014 Analysis of the Top 5 Global Drug Companies: https://www.techemergence.com/ai-in-pharma-and-biomedicine/\n\n[6] What if AI could take your research to the next level?: http://benevolent.ai/blog/benevolentai/what-if-ai-could-take-your-research-to-the-next-level/\n\n[8] Artificial Intelligence: will it change the way drugs are discovered?: https://www.pharmaceutical-journal.com/news-and-analysis/features/artificial-intelligence-will-it-change-the-way-drugs-are-discovered/20204085.article\n\n[12] IBM, Teva to Use A.I. for Drug Repurposing Program: https://www.rdmag.com/article/2016/10/ibm-teva-use-ai-drug-repurposing-program\n\n[13] How Pharmaceutical And Biotech Companies Go About Applying Artificial Intelligence in R&D: https://www.biopharmatrend.com/post/34-biopharmas-hunt-for-artificial-intelligence-who-does-what/?lipi=urn:li:page:d_flagship3_feed%3BhAZ67GlARQyGqnLUgoHuzA\n\n[18] MEET THE COMPANY TRYING TO DEMOCRATIZE CLINICAL TRIALS WITH AI: https://www.wired.com/story/meet-the-company-trying-to-democratize-clinical-trials-with-ai/\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-yitu-battles-for-position-in-huge-facial-recognition-market-8887bcc87234",
        "title": "China\u2019s Yitu Battles for Position in Huge Facial Recognition Market",
        "text": "If you deposit a cheque or withdraw cash from an ATM in China you\u2019ll no longer need a debit card to do so. New facial-recognition ATMs now enable almost-instant access your bank accounts.\n\nThe tech has been deployed on over 12000 ATMs nationwide by Yitu Technology, a Chinese AI startup specialized in computer vision and machine learning. ATMs are just one facet of Yitu, whose big plan is to put smart eyes on all human-machine interfaces.\n\nFounded by two Chinese AI experts in 2012, the company is particularly competent in recognizing human faces and vehicles. Such capability appeals to China\u2019s public security departments. In 2015, Suzhou local police captured a burglar in just ten minutes, using Yitu\u2019s technique to pinpoint the targeted car among hundreds in a surveillance video.\n\nThe case quickly sent a shockwave across the country. Now, over 30 provincial and 150 municipal public security departments have adopted Yitu\u2019s technique for identifying thefts and hunting fugitives.\n\nThe China Security Industry Network (21csp.com) projects China\u2019s security surveillance market value at CNY\u00a5752 billion (US$120 billion) in 2018. That\u2019s a big pie, and everyone wants a piece.\n\nIn an interview with Synced, Yitu Senior Researcher Shuang Wu said the company can accurately enable facial recognition in just one second, with a large database of over one billion facial photographs.\n\n\u201cYitu\u2019s technology has been well-accepted by many China\u2019s crucial ports and organizations as Yitu shows a superior performance even when it comes to racial spectrum, which is usually a difficulty in facial recognition,\u201d says Wu.\n\nYitu\u2019s advance in facial recognition technique quickly won global accolades as the company scored highest under the \u201cidentification accuracy\u201d category in a competition run by the US National Institute of Standards and Technology (NIST) and the Intelligence Advanced Research Projects Activity (IARPA) in November 2017.\n\nIn recent years Yitu has been expanding the use of its vision technologies to applications in smart cities for example. Partnering with Alibaba last year, Yitu energized Hangzhou\u2019s urban intersections by modelling vehicle behaviour and predicting traffic flow, speeding up traffic flow by 11 percent.\n\nWhile Yitu has consolidated its business in the security surveillance over the years, the company has also been plotting a bold course in healthcare, a traditionally conservative field that is now increasingly driven by technology. \u201cWe believe AI\u2019s potential in healthcare is unquestionably promising,\u201d says Wu.\n\nStruggling with air pollution, China has seen a sharp rise in lung cancer cases over the past 15 years. The country recorded nearly 4.3 million new cancer patients in 2015, 730,000 of whom had lung cancer, accounting for nearly 36 percent of the global total.\n\nYitu launched a smart medical imaging platform in early 2017 for lung cancer early detection \u2014 applying deep learning techniques to locate lung nodules, a type of small tissue mass in the lung that appear as round and white shadows on a CT scan. By analyzing size, shape and location, the system can help doctors compare a current imaging scan with historical scans to track changes in condition. It then creates a structured diagnosis report according to the imaging results, which is delivered to the doctor.\n\nZhejiang Provincial People\u2019s Hospital first deployed Yitu\u2019s system, and hospital clients have now grown to over 20. \u201cOur stats tell us the adoption rate of our diagnosis report is about 92 percent,\u201d says Wu.\n\nWu stressed that AI+healthcare is not to be treated as a short-term venture. \u201cPublic security for example has a clear threshold, and what you need to do is to reach a certain percentage of accuracy. Healthcare, however, is a bit more complicated. You have to keep talking with doctors and ask what and how to improve your product. While it is easy to make a demo, building trust in healthcare is a long-term challenge,\u201d says Wu.\n\nYitu\u2019s fast tech development and rapid rise attracted interest from venture capital. In 2017 the company raised CNY\u00a5380 million (US$60 million) in a Series C funding round led by Hillhouse Capital Group. Last month, Chinese media reported that Yitu had raised a new round funding, bringing the company\u2019s value to an estimated CNY\u00a515 billion (US$2.4 billion). The company has not officially confirmed the news yet.\n\nYitu now has a crew of over 500 in China, Singapore and Silicon Valley.\n\nThis yearlong series of huge funding rounds is providing Yitu with the resources it needs to develop new AI-powered products beyond machine vision, such as those based on natural language processing (NLP). Yitu recently created an NLP-based medical reference database that can efficiently convert patients\u2019 medical records into organized and usable data.\n\n\u201cFrom a technical point of view, deep learning reinforces the inner relationship between different academic areas, or you could say the threshold for developing NLP by a computer vision company is lower. On the other hand, NLP is something that any AI company should invest in,\u201d says Wu.\n\nYitu is one of four billion-dollar computer vision unicorns in China, along with CloudWalk, Face++, and SenseTime \u2014 the three-year-old startup that just became the world\u2019s highest valued AI company. They are all vying for the vanguard in marketplaces such as robotics, finance, security surveillance, transportations, mobile device, and AR/VR.\n\nThe fierce competition between the four companies does not allow Yitu time to relax \u2014 the company must continue growing. Wu says this is an exciting time for Yitu: \u201cThere are surprises every day. A lot of things happen beyond the plan, and somehow they achieve good success, bringing great challenges as well as opportunities for everyone.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-prepares-for-ai-talent-shortage-e66d0a3a0de2",
        "title": "China Prepares for AI Talent Shortage \u2013 SyncedReview \u2013",
        "text": "Chinese State media People\u2019s Daily recently reported the country is facing a shortage of five million AI talents.\n\nA separate LinkedIn survey revealed that while there are some 1.9 million AI engineers worldwide, about one million reside in the US while China is home to just 50,000. And as the talent level increases, the gap only widens \u2014 of the 208 AAAI fellowships granted over the past 27 years, only 4 went to Chinese nationals.\n\nAlthough China is an AI business deployment leader with well-financed startups such as the US$4.5 billion AI unicorn SenseTime, the country\u2019s AI educational infrastructure is lagging. The US has six times more AI education institutions.\n\nMeanwhile, high demand has sent AI engineer salaries skyrocketing in China. IDG Capital\u2019s 2017 Internet Unicorn Salary Report shows compensation for top AI talent is 55 percent higher than average ICT industry employee salaries, 90 percent higher at intermediate positions, and 110 percent higher at junior positions.\n\nAI laboratories established through joint ventures between corporations and universities, such as the iFlytek and Aispeech labs at Shanghai Jiaotong University, currently provide top-level AI education opportunities. Such partnerships grant well-rounded tutelage to students and facilitate talent transfer. Dr. Cheng-Lin Liu from Chinese Academy of Sciences says \u201cPhD students can convert their researchers into direct products or services upon graduation.\u201d\n\nThe AI talent shortage also provides an opportunity for online education providers. Andrew Ng\u2019s deeplearing.ai and Udacity are penetrating the China market, providing coaching in fields of machine learning, deep learning, NLP, computer vision, Python and so on. Chinese competitors 51CTO, CSDN, and Netease also provide AI video tutorials. Last year, iFlytek launched its online \u201cAI University,\u201d offering speech recognition and synthesis expertise and entrepreneurship coaching.\n\nSome employers however are wary of the online training trend, believing AI-related research and engineering skills must be built on years of formal learning and research, and cannot be instilled through relatively short online courses.\n\nTo tackle the problem, the Ministry of Education this week announced ambitious goalsfor the coming decade: establish a set of 100 \u201cAI+X\u201d specialization categories by 2020 in the disciplinary fields of math, physics, biology, psychology, sociology, law, and other related professional fields. The action plan will also compile 50 seminal teaching materials, 50 state-level online open courses, and open 50 additional AI teaching and R&D centres over the coming decade.\n\nThe Affiliated Elementary School of Peking University has begun introducing primary students to genetic algorithms and neural networks using easily explainable graphics and games. This is part of a larger initiative to promote STEM and AI courses in elementary and secondary schools. The Tongzhou District Experimental Primary School has added 75 AI-related courses, including a winter bootcamp to teach students patent filing procedures for robotics, to \u201chelp them build up awareness for intellectual property protection,\u201d explains class teacher Zhang Li.\n\nThis month the Ministry of Education, Sinovation AI Lab, and Peking University jointly announced the Global AI Talent Training Program for Chinese Universities, pledging to educate 500 teachers and 5,000 students over the next five years. Participating staff undergo strict screening and must presently be teaching CS, preferably in an affiliated AI institute.\n\nIn the eastern city of Nanjing, computer vision firm Seetatech is giving middle school students weekly 70-minute demo lessons. \u201cFirstly we help students get a theoretical glimpse of AI, machine learning and their practical applications, then we teach them about object and facial detection, instructing them to build their own AI detection algorithms,\u201d says an onsite Seetech employee.\n\nNanjing University opened one of the first AI institutes in China this March, and is currently seeking AI researchers, offering a base annual salary of US$60k, a housing subsidy of close to US$200k, and over US$300k in research funding as the starter package.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-oks-self-driving-tests-on-public-roads-d73216d0c243",
        "title": "China OKs Self-driving Tests on Public Roads \u2013 SyncedReview \u2013",
        "text": "New Chinese regulations will permit self-driving vehicle testing on public roads across the country.\n\nThe Chinese Ministry of Industry and Information Technology, Ministry of Public Security, and Ministry of Transport yesterday jointly issued the Intelligent Connected Vehicle Road Test Management Standards (Trial) (\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u9053\u8def\u6d4b\u8bd5\u7ba1\u7406\u89c4\u8303\u8bd5\u884c) notice, which introduces regulations for testing self-driving cars on public roads nationwide. The policies exclude \u201clow speed\u201d vehicles and motorcycles, and will take effect on May 1.\n\nThe new regulations aim to facilitate the development of self-driving technology through the wide deployment of public road tests. Chinese media is reporting that the country also plans to make its transportation infrastructure more self-driving-friendly, according to an Officer from the Ministry of Transportation.\n\nThis year the Shanghai, Beijing, and Chongqing municipal governments introduced their own self-driving regulations; and China\u2019s first special autonomous driving licence plates were issued to tech giant Baidu and electric vehicle startup NIO.\n\nThe new regulations require the presence in the test vehicle of a safety driver with at least three years driving experience and a clean record over the past 12 months. Testing companies are responsible for training their safety drivers, providing support in emergency situations on the road, and assume full liability for the vehicle\u2019s actions during testing.\n\nTest vehicles must be new and able to switch from self-driving mode to manual operation efficiently, safely and instantly. They require the capability for real-time monitoring, recording, and storing of vehicle status, control mode information (self-driving or manual), and location, direction and speed. Vehicles must also record environment perception and response data, headlight and turn signal status, and provide 360-degree video monitoring.\n\nTesting companies must meet all technical requirements and provide a detailed test plan and insurance of CNY\u00a55 million for each test vehicle. Once an application is approved, the company will receive an Intelligent Connected Vehicle Road Test Notice (\u667a\u80fd\u7f51\u8054\u6c7d\u8f66\u8def\u6d4b\u8bd5\u901a\u77e5\u4e66), and can then apply for a temporary test license plate from the local Traffic Management Department of Public Security (\u516c\u5b89\u673a\u5173\u4ea4\u901a\u7ba1\u7406\u90e8\u95e8).\n\nTest vehicles must adhere to their submitted test plans. If a test vehicle experiences a severe accident or serious violation, the relevant supervision department (\u4e3b\u7ba1\u90e8\u95e8) can revoke the temporary test license plate. The testing company must submit a summary report one month after each test ends and full test reports every six months.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/berkeley-researchers-create-virtual-acrobat-81427228fb50",
        "title": "Berkeley Researchers Create Virtual Acrobat \u2013 SyncedReview \u2013",
        "text": "Simulated robots can now spin-kick like a karate expert or backflip like an acrobat. The Berkeley Artificial Intelligence Research (BAIR) Lab yesterday proposed DeepMimic, a Reinforcement Learning (RL) technique that enables simulated characters to regenerate highly dynamic physical movements learned from data collected from human subjects. BAIR is a top-tier research lab focused on computer vision, machine learning, natural language processing, and robotics.\n\n \n\nRL methods have been shown to be applicable to a diverse suite of robotic tasks, particularly motion control problems. A typical RL includes a policy function that consists of all action selections that machines can do, and a value function that returns a low or high reward each time a machine takes an action. Machines can self-learn skills by leveraging the reward. The epoch-making Go computer AlphaGo produced by DeepMind is grounded on the same technique.\n\n \n\nHowever, virtual characters trained with deep RL can exhibit abnormal behaviours such as jittering, asymmetric gaits, or excessive movement of limbs.\n\n \n\nBAIR\u2019s new paper DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skill introduces a policy function that collects challenging skills such as locomotion, acrobatics, martial arts, and dancing.\n\n \n\nBAIR next initialises a character to a state sampled randomly, a method known as Reference State Initialization (RSI). The character can learn skills from any state of moves, such as the inflection point of a flip, and RSI can allow the character to know which states will result in high rewards even before it has acquired the proficiency to reach those states.\n\n \n\nBy connecting RSI with Early Termination (ET), a standard practice for RL researchers to stop simulations that lead to failure, BAIR researchers ensured that a substantial proportion of the dataset consists of samples close to the reference trajectory. Without ET, the character may flail or fall, but will not learn to flip.\n\n \n\nThe research shows that the character can learn over 24 skills, with movements nearly indistinguishable from the human reference subjects. BAIR also says its technique is simpler and produces better results than the current leading motion imitation method, Generative Adversarial Imitation Learning (GAIL).\n\n \n\nBAIR hopes the new research will facilitate the development of more dynamic motor skills for both simulated characters and robots in the real world.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/china-puts-education-focus-on-ai-plans-50-ai-research-centres-by-2020-5589c35ba701",
        "title": "China Puts Education Focus on AI; Plans 50 AI Research Centres By 2020",
        "text": "China\u2019s Ministry of Education last week issued its AI Innovation Action Plan for Colleges and Universities to the education departments at all levels and institutions of higher education. This action plan aims to advance China\u2019s universities to world frontiers in science and technology; energize their capabilities in AI technological innovation, talent cultivation, and global cooperation; and provide strategic support for the development of next-generation AI in China.\n\nThe plan sets three goals for the next 12 years:\n\nThe plan further proposes establishing a set of one hundred \u201cAI+X\u201d specialization categories by 2020, with specific goals to:\n\nChina sees AI as the state\u2019s innovation focus, which will spark technological breakthroughs and stroke national pride. In July 2017, the State Council issued the New Generation Artificial Intelligence Development Plan and listed \u201caccelerating the education of top-notch AI talents\u201d as a primary task.\n\nLast week, China\u2019s Ministry of Education, Sinovation Ventures, and Peking University jointly launched the country\u2019s first university program \u2014 Global AI Talent Training Program for Chinese Universities \u2014 to educate over 500 teachers and 5,000 students with AI expertise within the next five years.\n\nThe latest action plan can be viewed as a supplement to this program, with the same ultimate goal of putting the State Council\u2019s Development Plan into practice and making China a world leader in the field of AI by 2030.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/onsite-from-re-work-finance-summit-in-london-deep-learning-trading-6521912aa599",
        "title": "Onsite From RE\u2022WORK Finance Summit in London \u2014 Deep Learning & Trading",
        "text": "Financial markets are becoming a new proving ground for deep learning. The AI technique has already achieved remarkable success in image recognition, speech detection and sentiment analysis, and is believed to be well-suited for dealing with financial data.\n\nAt last month\u2019s RE\u2022WORK Deep Learning in Finance Summit in London, leading AI industry practitioners and academics from prestigious universities discussed their research, provided insights on business trends and real-life AI applications, and addressed current challenges facing the AI industry as a whole.\n\nSynced visited the summit to explore how deep learning techniques, such as neural networks and LSTM, can be applied to proprietary trading. This article focuses on Filippo Scopel\u2019s presentation Learning to Trade and Dr. Luigi Troiano\u2019s Supporting Trading in Financial Markets using Deep Learning Tools.\n\nScopel is a machine intelligence engineer at Merantix, a Berlin research lab building AI ventures. Scopel\u2019s research speciality is mathematical modelling and algorithm development with applications in finance and other fields. For the past year, he has been training deep learning models for time series prediction on financial microstructure data.\n\nScopel discussed Merantix\u2019s use of deep neural networks to predict price movements with short-term samples of under five minutes, and how these predictions can be transferred into trading strategies and then trading algorithms.\n\nDeep learning models are trained to make predictions by absorbing raw data such as past prices, conducting data computation, and generating results. Due to the complexity of the hidden layers within such neural networks, it is impossible to make an exact interpretation of how those predictions were generated.\n\nPredictions will be turned into specific trading strategies regarding bid-asks and particular time horizons. A decision to buy, hold, or sell is then generated and executed by the algorithm.\n\nAlthough the Merantix predictions can be more accurate than traditional methodologies, Scopel pointed out the system is far from infallible. Even with the right predictions, Merantix can still record losses due to the characteristics of high-trading frequency. What he means is transaction costs, and full bid-ask spreads can erode the potential profitability, sometimes resulting in losses.\n\nDr. Troiano is an Assistant Professor of AI, Machine Learning and Data Science at the University of Sannio. He began his presentation by identifying areas within finance where AI and particularly deep learning could be applied, including reshaping the analysis space, searching complex patterns in data, and trading robotisation. Data filtering removes noise to enable better quantitative analyses, and complex correlations and co-occurrences in large datasets can be identified to generate improved trading strategies. Automation reduces costs and improves the long-term effects of hedge funds and proprietary trading firms.\n\nIn forecasting the variability of prices, also referred to as volatility, Dr. Troiano said long short-term memory (LSTM) networks \u2014 a type of Recurrent Neural Network (RNN) capable of learning long-term dependencies \u2014 work best when volatility is extremely high, which is a beneficial finding since such periods also provide high-profit opportunities.\n\nDr. Troiano then discussed his research into algorithmic trading using technical indicators such as the moving average convergence divergence (MACD) with deep learning techniques. A deep learning system can construct trading strategies after observing historical prices and indicator values instead of being explicitly programmed to execute those strategies. What this means is that AI has the potential to learn, or more appropriately, invent trading effective strategies.\n\nHowever, Dr. Troiano stressed that his research is still at its preliminary stage, and only certain deep learning techniques have been tried. More research is needed to determine how neural networks can improve on old-fashioned methods.\n\nOverall, it is clear that deep learning can accelerate the performance of tasks such as short-term future price predictions and proprietary trading strategies, but the AI technique has not yet reached maturity for full application in the trading market. The industry needs to be patient before widely employing neural networks to make market predictions.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/ai-photographs-chinese-jaywalkers-shames-them-on-public-screens-ad0a301a46a6",
        "title": "AI Photographs Chinese Jaywalkers; Shames Them on Public Screens",
        "text": "China\u2019s traffic police are using AI to tackle \u201cChinese-style jaywalking\u201d at major urban intersections. Facial recognition cameras take a 15-second video and four snapshots of pedestrians crossing on a red light. Pictures are matched with photo IDs in the police database, and violators can have their headshots along with family name and partially obscured citizen ID and registration address displayed on large roadside screens.\n\nSince deploying the system in April 2017, Shenzhen traffic police have caught over 13,930 jaywalking pedestrians and non-motorized vehicles. Jaywalkers are fined up to CNY\u00a520 (US$3) and are subject to traffic rule refresher courses or community service at road intersections.\n\n\u201cSince the new technology has been adopted, jaywalking cases have been reduced from 200 to 20 each day at the major intersection of Jing\u2019Shi and Shun\u2019Geng roads. Fewer people are crossing roads during red lights,\u201d said Li Yong, a Traffic Police Officer in the eastern city of Ji\u2019Nan.\n\nChinese-style jaywalking is a social nuisance. According to Ji\u2019Nan city statistics, barging pedestrians and non-motorized vehicles account for 16 and 33 percent of traffic accidents per year respectively. As ever-broadening Chinese roads can now have 10 lanes at urban intersections, ignoring traffic signals is extremely dangerous.\n\nTraffic authorities also plan to build a social credit system wherein jaywalkers will start receiving text messages or Weibo notifications. Traffic police will record the number of violations, and a certain threshold will affect the offender\u2019s social scores, which may limit their ability to borrow from banks.\n\nSome may ask whether this AI is saving lives or infringing on personal privacy? Li Yi, a research fellow at the Shanghai Academy of Social Sciences, says the public display of offenders\u2019 photos and partial personal information may prove to be effective in reducing pedestrian accidents and injuries, \u201chowever, we always need to find a balance between law enforcement and privacy protection.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinas-sensetime-scores-us-600-million-in-funding-to-become-the-world-s-most-valued-ai-startup-a505d2fa9c01",
        "title": "China\u2019s SenseTime Scores US$600 Million in Funding to Become the World\u2019s Most Valued AI Startup",
        "text": "China\u2019s computer vision company SenseTime today announced it had raised a staggering US$600 million in Series C funding, setting a world record for an AI company and bringing its value to an estimated US$4.5 billion to make it the world\u2019s most valued AI startup.\n\nChina\u2019s e-commerce giant Alibaba Group led the funding, joined by Temasek Holdings and Suning Corporation. Says Alibaba Group Vice Chairman Cai Chongxin, \u201cSenseTime\u2018s research capabilities in deep learning and computer vision are impressive. Alibaba looks forward to partnering with SenseTime to inspire more innovations and create value for society.\u201d\n\nFounded in 2014 by Dr. Xiao\u2019ou Tang, a Professor of Information Engineering at the Chinese University of Hong Kong (CUHK), SenseTime uses deep learning in the development of computer vision to replicate tasks performed by the human visual system.\n\nSenseTime Co-Founder and CEO Xu Li told Synced the company set up an R&D team of 200 scientists during its first two years. The investment quickly paid off, as the team came up with an advanced deep learning framework and a cutting-edge 1207-layer neural network in 2016.\n\nSenseTime\u2019s superior AI technologies were soon transformed into a number of marketable software solutions: SensePose to synchronize users\u2019 movements with virtual figures in real-time videos; SenseVideo to recognize the positions and attributes of humans, vehicles and other entities from video input; and SenseFace to detect humans faces in a millisecond.\n\nThese technologies can be applied to multiple industries, including automotive, finance, mobile internet, robotics, security, and smartphones, and helped SenseTime score mega-clients such as China Mobile, China UnionPay, Huawei Technologies Co, Xiaomi, JD.com, and an important strategic partnership with chip giant NVIDIA.\n\nIn late 2016 SenseTime raised US$120 million in funding led by Beijing-based CDH Investments, Dalian Wanda Group, IDG Capital Partners and Star VC. Some six months later the company closed its Series B funding with US$410 million from CDH and Sailing Capital, which propelled it onto CB Insights\u2019 2017 technology unicorn list.\n\nSenseTime has about 700 staff, with 120 researchers who hold doctoral degrees. According to a November Reuters report the company is preparing for an IPO.\n\nXu says the latest funding round will not only strengthen the company\u2019s advantages, but bring more business opportunities: \u201cThe Series C financing will help SenseTime apply its core technology to more industries, expand the business landscape by cooperating with partners globally, and connect the upstream and downstream industry.\u201d\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/simulations-pave-the-road-for-self-driving-technologies-78b696227383",
        "title": "Simulations Pave the Road for Self-Driving Technologies",
        "text": "The Encyclopaedia Britannica defines a computer simulation as \u201cthe use of a computer to represent the dynamic responses of one system by the behaviour of another system modeled after it.\u201d Airline pilots train on simulators, while in the automotive industry driving simulations are used to optimize ride experience and improve engine performance. With the emergence of autonomous vehicle technologies, the development and deployment of effective self-driving simulations has become an industry priority.\n\nSelf-driving simulations collect data to improve autonomous vehicles\u2019 algorithm training capability, sensor accuracy and road data quality. It\u2019s been proposed that autonomous vehicles should log 18 billion kilometers (11 billion miles) of road test data to reach an acceptable safety threshold. This would however require a fleet of 100 vehicles running on roadways 24/7 at a constant speed of 40 kph (25 mph) for 5 years. Self-driving simulations are an ideal solution, enabling continuous and unlimited data collection in the digital environment with relatively low operational costs.\n\nSelf-driving simulations have advantages in milage data collection efficiency, road condition dataset diversity, and sensor corresponding data accuracy.\n\nSelf-driving simulations can boost the speed of data collection to reach mileage accumulation targets while reducing fleet operation costs. Waymo and Baidu have made use of self-driving simulation to speed up self-driving development.\n\nSelf-driving simulations can also add more uncertainty to a dataset to increase the responsiveness of the system. They can produce a variety of scenarios to test and improve vehicle performance under different conditions, for instance in severe weather, heavy traffic environments, and various distinct scenarios. Truevison.ai and AirSim are two leading solutions in this field.\n\nSensors are the eyes of the self-driving car. Multiple sensors can increase the accuracy of input data and protect the vehicle from misidentification or malfunction. Self-driving simulations provide multiple signals simultaneously, such as camera, LiDAR, radar and more. A given object in the virtual environment will thus be detected by different sensors, and these signals will validate each other to increase accuracy. RightHook and Cognata are two simulators providing multiple self-driving sensors.\n\nThe limitations of self-driving simulations should not be overlooked. Currently, there are two main weaknesses: lack of emergency situation scenarios, and potential consequences of differences between real and simulated data.\n\nEmergency situations are still hard to simulate as each real world accident is unique. Although vehicles can learn general driving operations through simulation, it is impossible to predict every single emergency situation. For example the May 7th, 2016 fatal accident involving a Tesla on autopilot occured because the system failed to distinguish a white truck against a bright sky. Although this is not a rare scenario in the real world, the simulator had not covered it.\n\nThis differences between real and simulated data is another issue that could negatively affect system performance, as the full consequences of these differences remain unclear. Engineers are hard pressed to determine what type of data leads to an accident due to unexplainable features of the algorithms. For example, real world pedestrians with disparate clothing and posture profiles cannot all be reflected in the simulator, and this might reduce a self-driving vehicle\u2019s ability to identify pedestrians.\n\nIn order to overcome these drawbacks, it is important to simulate more scenarios to make abnormal data traceable. SynCity is a Dutch company developing an advanced simulator that aims to present a wider range of scenarios, including other vehicle misbehaviour and various emergency situations, in order to optimize algorithms.\n\nIn the words of Toyota Research Institute Chief Executive Gill Pratt, \u201cSimulation is a tremendous thing.\u201d The correct understanding and prudent application of self-driving simulations are essential to safely accelerating R&D in the autonomous vehicle industry.\n\nReference:"
    },
    {
        "url": "https://medium.com/syncedreview/ai-powered-hockey-analytics-a-game-changer-8534b2e263aa",
        "title": "AI-Powered Hockey Analytics: A Game Changer \u2013 SyncedReview \u2013",
        "text": "Analytics are all the rage in professional sports. The concept can be traced back to American statistician Bill James, who introduced his \u201cSabermetrics\u201d method for in-game baseball analysis in the 1970s. When the NBA\u2019s Golden State Warriors decided to favour three-pointers over two-point shots in 2016, the winning strategy sent a shockwave through professional basketball. This was a \u201cdata-driven decision\u201d based on higher score probability, explains Alex Martynov.\n\nMartynov is the 24-year old founder of ICEBERG, a Canadian startup using AI algorithms in sports analytics with a focus on ice hockey. Three years ago, Alex shared the idea of an AI sports analytics company with his investor father, who helped him kickstart the idea with $25,000. Not much, but it was enough for Alex to gather programmer friends in Toronto and Moscow and put together a working prototype.\n\nICEBERG installs a set of three FLIR thermal cameras around the rink before the start of each game. The video has lower resolution than an iPhone recording, but provides the constant full-ice view the company\u2019s algorithms require, as broadcast feeds typically leave 50 percent or more of the ice surface and players out of frame.\n\nArtificial neural networks are trained to recognize all moving entities on the ice surface: 12 players grouped by jersey color, on-ice officials, and a small black puck that can reach speeds of 160 kph (100 mph). Computer vision algorithms previously trained on a dataset containing 10,000 variations of numbers from all angles can identify each player by jersey number.\n\nBy tracking player and puck coordinates 10 times per second, a sixty-minute game will generate one million data points. The algorithm matches individual player coordinates with those of the puck to record their passes, body checks, giveaways and takeaways, shots, and goals. Typically, about 7\u20139 percent of all shots result in goals, and variance here predicts higher or lower goal probability.\n\nICEBERG\u2019s AI tracks a total of 500 different metrics which correspond to player and team behavior, and the company sometimes finds statistical nuances that are counterintuitive to hardcore fans or watchful coaches.\n\nIn a match between favorite Canada and underdog Switzerland, Iceberg\u2019s AI found that Switzerland skated 1.7 more kilometers and were 5\u201310 centimeters closer to the puck in micro-episodes. The Swiss also had longer puck possession and generated 2.48 more expected goals (xG). Switzerland\u2019s superior metrics should deliver a win seven times out of ten. But Canada won the game 3\u20130. Why?\n\nMartynov explains that \u201cabout 40 percent of all game outcomes is luck, but the other 60 percent can be predicted, which is what we are trying to do \u2014 predicting what isn\u2019t random. Our clients can play five games and lose five times in a row, but data will show that they could\u2019ve won every time. The coach will call our analyst, and we tell them, \u2018calm down, it\u2019s just the variation, you will get back to the mean, if you continue playing like this you will win five games in a row\u2019.\u201d\n\nICEBERG uses NVIDIA\u2019s GPU and marketing expertise and Microsoft Azure\u2019s cloud storage. The company also participates in NVIDIA\u2019s Inception Program.\n\nPortal subscription fee ranges from US$400 \u2014 $800 per game. If a team plays 60 games in a season that\u2019s approximately US$30,000. Clients receive a report the morning after each game and can access detailed game numbers from the portal. ICEBERG also has on-call analysts to answer clients\u2019 questions.\n\nFinding clients can be a long process of convincing the coach, the manager, and the owner. ICEBERG\u2019s deal with Austria\u2019s Red Bull Salzburg required four months of negotiation. \u201cThere are coaches who are confused, asking \u2018why do I need this?\u2019\u201d says Martynov. \u201cWe are not trying to replace the coach or the manager, but give teams an edge. It makes hockey more intellectual.\u201d\n\nThere are also cases like Swedish teams V\u00e4xj\u00f6 Lakers and F\u00e4rjestad BK, who signed contracts in five minutes. The competitive edge of data analytics is too good to be ignored.\n\n\u201cCurrently, We have a market share of 5\u20137% of global professional hockey teams. But it\u2019s not moving as fast as I would like,\u201d says Martynov, \u201cWe want to go into the soccer market after this. If you get two percent of the soccer market, that\u2019s approximately the same as the entire hockey market. We started in the niche market, but hockey is also a very complicated sport where players skate fast, collide often, change every minute, not to mention the puck is very small. Technically, it\u2019s easy to downgrade from hockey into other sports.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-gtc-2018-peeks-inside-the-gpu-powered-world-3366891680fe",
        "title": "NVIDIA GTC 2018 Peeks Inside the GPU-Powered World \u2013 SyncedReview \u2013",
        "text": "In 1993, on his 30th birthday, Jensen Huang gave himself the best present ever by founding NVIDIA, the graphic-processor company where he still serves as CEO. Huang and his NVIDIA team pioneered the graphics processing chip (GPU) in 1999, revolutionizing the visual performance of device displays. But even Huang never dreamt that his GPU would one day become a driving force in the arena of artificial intelligence (AI).\n\nIn 2011 at Stanford University, Andrew Ng, one of the top minds in AI, discovered that a dozen NVIDIA GPUs could perform as well as 2,000 CPUs in training deep learning models. A GPU contains thousands of cores and is capable of processing thousands of threads simultaneously. This parallel architecture makes GPU extremely powerful in large-scale but straightforward data computation.\n\nOther top academic institutes and laboratories quickly followed Ng and deployed GPUs for deep learning research with the belief that GPU could eliminate bottlenecks in computing capabilities that had vexed AI researchers for years and spark technological breakthroughs.\n\nNVIDIA quickly recognized this trend and pivoted its strategy to become an AI computing company. Over the last six years, the Santa Clara chipmaker has been pushing the limits of GPU architecture, releasing a cutting edge GPU every one or two years to empower computing-hungry applications and products. Over the last five years, the number of developers with expertise in GPU has grown tenfold; CUDA downloads five times; and total GPU flops of the top 50 systems 15 times.\n\nSince 2009, NVIDIA has been hosting the annual GPU Technology Conference (GTC), which showcases company releases and provides an exhibition venue for GPU-based innovations. GTC attendance has soared from 2000 attendees in 2012 to the 8,500 developers, buyers and innovators who went to Santa Clara, California for GTC 2018 last month.\n\nThe role and deployment of GPUs is changing dramatically, and they are creating significant new market opportunities. Synced visited GTC 2018 to explore a world based on GPUs.\n\nTech giants usually have a capital investment arm \u2014 such as Google Ventures or Microsoft Ventures \u2014 to fund AI startups. NVIDIA GPU Ventures invests in and nurtures next-generation companies built on GPU. A number of NVIDIA portfolio companies have become high-profile AI startups: H2O.ai, Element.ai, and Drive.ai.\n\nAt GTC 2018, a large black semi truck parked outside the San Jose McEnery Convention Center became a centre of attention. The prototype was from TuSimple, a leading Chinese autonomous-driving truck company founded by entrepreneurs Mo Chen and Dr Xiaodi Hou from the California Institute of Technology.\n\nThe TuSimple semi operates on an accelerator fusion of NVIDIA GPUs \u2014 including GTX 1080Ti, NVIDIA Drive PX and Jetson TX \u2014 to process huge amounts amount of data.\n\nDrive PX is NVIDIA\u2019s first in-vehicle supercomputer, introduced in 2015. As computing needs for self-driving vehicles have ramped up, NVIDIA has developing powerful vehicle-specific processors to help the company stay ahead of the race. NVIDIA recently successively unveiled the advanced Drive PX Pegasus and what it billed as the world\u2019s most powerful in-vehicle System-on-Chip (SoC), Xavier.\n\nLast June, TuSimple completed a 200-mile high-automated test drive from San Diego to Yuma, Arizona. The company\u2019s fast tech development attracted interest from NVIDIA GPU Ventures, which joined a group of investors led by Chinese social media company Sina putting more than US$20 million into TuSimple last August.\n\nNVIDIA is not just writing checks. In June 2016, the company introduced a virtual incubator, NVIDIA Inception Program, to nurture AI startups. In 18 months over 2000 companies have applied to the program, and only 70 have so far been accepted. NVIDIA also hosts the Inception Competition at GTC, where its portfolio companies compete for NVIDIA Inception Awards and US$1.5 million in prize money.\n\nIsraeli cybersecurity startup Deep Instinct was one of the winners last year. The company uses a GPU-based neural network and CUDA to achieve 99 percent detection rates, compared with about 80 percent detection from conventional cybersecurity software. Last June, NVIDIA pumped US$10 million into Deep Instinct. \u201cNVIDIA introduced us to their strategic accounts which are now our customers, which has been very helpful,\u201d said a Deep Instinct representative at GTC 2018.\n\nOn this year\u2019s final pitch day, robotic arm company Kinema Systems won the NVIDIA Inception Award and took home US$375,000. The company\u2019s flagship product is the AI-powered industrial robotic vacuum grabber Kinema Pick, which runs on GTX 1060 and NVIDIA embedded AI computing device Jetson TX2.\n\nKinema Founder and CEO Sachin Chitta says NVIDIA provides portfolio companies with \u201cspecial treats,\u201d such as a discount on NVIDIA hardware, a training course conducted by NVIDIA experts, and not least a chance to appear at GTC: \u201cThis conference provides a great opportunity for exposure. We have met many customers and investors who showed a huge interest in our products.\u201d\n\nTech giants believe AI can reimagine conventional diagnostic methodologies in medical health, increasing accuracy and reducing costs. IBM has been using slides to train deep neural networks to detect tumours since 2016. Google has successfully produced a tumour probability prediction heat map algorithm whose localisation score reached 89 percent, significantly outperforming pathologists\u2019 average of 73 percent.\n\nThe medical health field was not widely addressed in the last few GTCs, but NVIDIA is now putting more effort into this area.\n\nAt GTC 2018 NVIDIA unveiled Project Clara \u2014 a medical imaging supercomputer deployed on its cloud platform. Clara is designed to transform standard medical images such as X-rays, ultrasound scans, CTs, MRIs, PETs, and mammograms into high-resolution cinematic renderings. Because deploying GPUs in every clinic and hospital would not be cost-effective, Clara provides its users with high-performance cloud-based services.\n\nNVIDIA added medical health talks and panels at GTC 2018, and invited renowned professionals to represent their research achievements based on GPUs.\n\nThomas Fuchs is the Founder and CEO of New York startup Paige.ai, founded this January to fight cancer with AI. The company has access to a dataset of 25 million pathology images and financial support from Breyer Capital, which led a US$25 million Series A Funding Round.\n\nIn an interview with Synced, Fuchs said he believed the time was right to build Paige.ai because the requirements were all in place: qualified devices, extensive collection of medical images, and full-fledged deep learning algorithms.\n\nMost importantly, GPU advancement drives deep learning development with an unprecedented scale of medical image data. Paige.ai has now built a high performance compute cluster with hundreds of NVIDIA GPUs.\n\nGPUs will not replace the central processing unit (CPU), which is still extremely powerful in performing the necessary arithmetic, logical, control and input/output (I/O) operations for example for personal computers. However, GPU are better than CPU in particular tasks: they are unquestionably the best chip for image processing in gaming, and their parallel architecture happens to be very well-suited for deep learning.\n\nA few innovative startups have discovered that GPU acceleration can also deliver better performance than CPU in databases, particularly in repetitive operations on large amounts of data.\n\nOnline Transaction Processing Databases (OLTP) primarily handle day-to-day transactions for example for banks. Oracle is the dominant vendor here, accounting for nearly 50 percent of the market share. Online Analytical Processing (OLAP) databases meanwhile are designed to handle complex analysis of large volumes of data, and consequently many AI applications are now running on OLAP databases. The estimated market size for OLAP databases is US$22.8 billion in 2020, a lucrative emerging market for startups to target.\n\nGPU database companies first appeared in 2016. Silicon Valley pioneers Kinetica and MapD have raised US$50 and US$37 million respectively; and Israel\u2019s SQream US$31.5 million. Last year, Chinese database leader Zilliz raised an undisclosed amount (reportedly RMB\uffe5100 million or US$16 million).\n\nSays Zilliz Founder and CEO Chao Xie, \u201cUsing a GPU to run a database can be traced back to 15 years ago in academia. But it didn\u2019t work as the top software has long been constrained by the underlying hardware. Now, NVIDIA and other chipmakers are building an infrastructure for GPUs, helping developers to reduce the threshold of developing GPU-based applications.\u201d\n\nZilliz\u2019s GTC 2018 booth showcased the company\u2019s cutting-edge GPU database, which it says can increase the performance of data processing up to 100 times over CPUs; reduce hardware cost by 10 times; and lower data centre operation and energy costs by up to 95 percent.\n\nThe SQream booth was a few meters from Zilliz\u2019s. Founded in 2010, SQream spent their six years conducting research and building databases, and launched its commercial product in 2016.\n\n\u201cIf you have tens of hundreds of terabytes [data], and you try to do some database operation on what we called Massive Parallel Processing (MPP) databases, sometimes the query goes up to 30 minutes to an hour. When you put data into the SQream instead of MPP, the query goes down to five minutes,\u201d says SQream Senior Solutions Architect Arnon Shimoni.\n\nAs AI applications thrive with complex neural networks and large datasets, researchers and developers can of course invest in GPU clusters \u2014 but another option is to purchase on-demand services on the cloud for GPU-intensive tasks.\n\nThis premise is growing the market for GPU-as-a-Service (GaaS) solutions, which is set to exceed US$5 billion by 2024, according to a new research report by Global Market Insights.\n\nMajor cloud service vendors AWS, Google Cloud, and Microsoft Azure have been hosting NVIDIA GPU-equipped virtual machines for their cloud machine learning services for some time. Google began incorporation of NVIDIA GPU in its cloud computing centres back in November 2016, just months after AWS announced a new Elastic Compute Cloud (EC2) instance type, dubbed P2, which leverages NVIDIA GPUs.\n\n\u201cGaaS will be used for augmented reality, but will also able to handle massively parallel complex app problems like encryption (or decryption), weather forecasting, business intelligence graphical displays, big data comparisons,\u201d writes Jack Gold, founder and principal analyst at analyst firm J. Gold Associates.\n\nAlthough a few cloud services giants will dominate the market for GaaS, small and medium enterprises still have a chance to take a piece of the pie.\n\nCirrascale Cloud Services is a San Diego-based company that enables researchers and data scientists to attach GPU acceleration to a wide range of tasks over the network. Although its service is similar to AWS or Google Cloud, the company appeals to users who train models for weeks or even months at a time by offering a 35 percent lower price point and 35 percent faster speed compared to AWS.\n\nSays Cirrascale \u200eExecutive Account Manager Andrew Kruszewski, \u201cWe realized that there was a market, people [researchers] would come and test on the system, and they wouldn\u2019t want to get off. They also didn\u2019t want to own the equipment because NVIDIA changes their GPU so often.\u201d\n\nLaunched just three years ago, Cirrascale\u2019s cloud services have been growing so quickly that the company had to cut other divisions. Last year, its design and manufacturing business was sold to BOXX technologies.\n\nOver the last two years, NVIDIA\u2019s stock price has skyrocketed thanks to the rapidly increasing role AI is playing in the company\u2019s revenue growth. Full-year revenue of 2017 was US$9.71 billion, up 41 percent from a year earlier, and its discrete GPU market share increased to 72.8% during the third quarter of 2017.\n\nHowever, rivals Google and Intel are catching up, and developing their own AI chips to challenge NVIDIA. This February, Google announced that its Tensor Processing Unit (TPU) \u2014 a custom chip that powers neural network computations \u2014 will be available in beta for researchers and developers on the Google Cloud Platform.\n\nLast year, Google boasted that its TPUs were 15 to 30 times faster than contemporary GPUs and CPUs in inferencing, and delivered a 30\u201380 times improvement in TOPS/Watt measure. In machine learning training, TPU are more powerful in performance (180 vs. 120 TFLOPS) and two times larger in memory capacity (64 GB vs. 32 GB of memory) than NVIDIA\u2019s top GPU Tesla V100.\n\nJoe Pelissier, Distinguished Engineer at Cisco Systems, says that a serious challenger may even replace NVIDIA in the next three to five years.\n\n\u201cThe type of mathematics for machine learning you basically need to be able to do is multiplication. Everything else is at least two orders of magnitude less significant. So you can imagine Silicon Valley, there are a lot of folks saying \u2018hey, if i take out a lot of functionality that GPU has, and only leave the stuff it needs for machine learning, I can either make it cheaper, or I can put more cores in it, or a combination of both\u2019,\u201d says Pelissier.\n\nMany experts believe that while NVIDIA GPU were not initially created for AI, they are now embedded in it and cannot be easily replaced.\n\nBrett Newman, VP marketing and customer engagement at compute hardware company Microway, says NVIDIA has done a good job building the ecosystem. \u201cThey are making software tools better applied for deep learning training. And they are making developer friendly things like Digits [Deep Learning GPU Training System]. I think that stuff is setting them up for success that will persist into the long-term.\u201d\n\nHuang quipped at GTC 2018 that \u201cNVIDIA is still a small company with only 10,000 employees.\u201d But he is too humble. It\u2019s no small achievement to have built NVIDIA from a birthday present into a US$150 billion chip giant in 25 years. And now, the company\u2019s GPU have become the muscle powering AI research and innovation. It\u2019s been an incredible journey for NVIDIA, one that will continue to empower AI long into the future."
    },
    {
        "url": "https://medium.com/syncedreview/lawyee-has-digitized-40-million-chinese-court-records-a53f02fa5061",
        "title": "Lawyee Has Digitized 40 Million Chinese Court Records",
        "text": "Lawyee is a Peking University spinoff founded in 2003 that was one of the first companies specializing in building searchable legal databases and digitizing IT systems for Chinese courts and law firms.\n\nWhen Lawyee started the Internet\u2019s legal landscape was barren and computer technologies underdeveloped. Things fast-tracked however in 2014 when China\u2019s Supreme People\u2019s Court made publishing court decisions on an official online archivemandatory within seven days of entry into force, excluding politically-sensitive, private or youth crime cases.\n\n\u201cLawyee became the contractor for building this database,\u201d says company General Vice Manager Chen Hao, \u201cand we accumulated several million documents by the end of 2014.\u201d This number skyrocketed to 40 million in mid-2018, and page views exceeded 14 billion. Lawyee also runs a case law database which Chinese law schools can access for an annual subscription fee of few thousand US dollars.\n\nResponding to recent advances in AI, Chen explains that \u201cAI evolved out of decades of statistics research. Legal researchers have also used statistics for a long time, and there are many machine learning algorithms on SPSS Statistics. Since 2006 deep learning has upgraded machine perception.\u201d\n\n\u201cHowever there aren\u2019t many mature commercial applications at least in areas of law. For both common and continental legal systems, technology so far has only tackled vertical problems, but it\u2019s not enough to reinvent the legal industry. Now there are startups building applications on top of IBM Watson that may prove successful.\u201d\n\nBuilding on strong database resources, AI can generate legal documents and perform compliance review of contracts, quality control of documents, and analysis of legal risks, business guidelines, and so on.\n\nOne of Lawyee\u2019s current AI projects is identifying the core issues in a legal document. Lawyee\u2019s AI trained on more than 30 million data samples can currently identify core arguments in about 70 out of 100 documents, and be exactly on-target with 60.\n\nChen says the hard part is human-labelling larger quantities of data. \u201cWhen data samples rise from 100 to 10k, people can no longer handle the repetitive work, not to mention when data size goes up to 300k plus.\u201d\n\nAnother challenging task is finding benchmarks for algorithm performance, which is crucial for measuring accuracy. \u201cThere are no benchmark datasets such as ImageNet for image or SQuAD for NLP in the legal space. Labelling them is very expensive, thus impossible for normal companies. For accuracy, we proclaim that certain software can go up to 70\u201380 percent. Customers are willing to pay more if other options on the market don\u2019t work as well,\u201d says Chen.\n\nThe Chinese government directive mandating legal document digitalization has engendered a number of Beijng legaltech startups, most notably AI-powered legal consulting companies Lvpin Technology and Itslaw, along with natural language processing solution providers Fa\u2019gou\u2019gou and most recently deepcurious.ai."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-ministry-of-education-sinovation-and-peking-u-join-hands-to-train-ai-talents-f2d21ce3387e",
        "title": "Chinese Ministry of Education, Sinovation, and Peking U Join Hands to Train AI Talents",
        "text": "In Beijing today, China\u2019s Ministry of Education, Sinovation AI Lab, and Peking University jointly announced the Global AI Talent Training Program for Chinese Universities. The program will include tutorships from Turing Award winner John E. Hopcroft, Deep Learning pioneer Geoffrey Hinton, and Sinovation CEO Kai-Fu Lee.\n\nThe program has its origins in a February meeting between Kai-Fu Lee, officials from China\u2019s Ministry of Education, and computer scientists from more than 50 Chinese universities.\n\nThere are two components, the DeeCamp Student Boot Camp and a Teacher Training Program. In the explorational phase, the program will select 100 teachers and 300 students from top CS-heavy Chinese high schools to test teaching methods. The bigger plan is to educate at least 500 teachers and 5,000 students over five years.\n\nIn addition, the Ministry of Education plans to set up the China-US University AI Talents Cooperation Training Program to provide Chinese students with AI scholarships for US colleges and universities.\n\nIn July 2017, the State Council issued the New Generation Artificial Intelligence Development Plan and listed \u201caccelerating the education of top-notch AI talents\u201d as a primary task, emphasizing the importance of \u201cimproving the artificial intelligence education system, strengthening talent reserve and echelon building, and occupying the AI highland in China.\u201d\n\nToday\u2019s announcement provides a framework with the international experience and expertise required to take on that task."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-mar-w-4-apr-w-1-caec29bd494",
        "title": "AI Biweekly: 10 Bits from Mar W 4 \u2014 Apr W 1 \u2013 SyncedReview \u2013",
        "text": "Now social and behavioral scientists can use the TuringBox platform to study artificial intelligence algorithms. AI contributors can upload existing and novel algorithms for review, gaining a reputation in their community. AI examiners meanwhile develop and post machine intelligence tasks to evaluate and characterize the behavior of AI algorithms, including novel questions of societal importance.\n\nAmper Music is the world\u2019s first AI-powered music composer, enabling users to create and customize music. The company raised an additional US$4 million in seed round funding, which will help double the number of US Amper Music employees and help the company expand internationally.\n\nMarch 22th \u2014 Google Assistant Lets You Send and Receive Money on Phones\n\nGoogle Phone users can now send and receive money with Google Pay for free via Google Assistant. This feature is currently only available on new phones due to security concerns. It will be offered on Google Home smart speakers in the coming months.\n\nMarch 27th \u2014 Toshiba Plans to Transform Itself With AI and IoT\n\nToshiba introduces a high-level digital transformation strategy for its business, with a focus on both AI and IoT solution development. Toshiba believes the strategy will maximize customer value through its various ecosystem partners.\n\nWaymo partners with Jaguar to build a 20,000-electric-car fleet with fully autonomous driving capabilities in the next two years. The cars will have sufficient battery life to drive all day on one charge.\n\nMarch 27th \u2014 Nvidia Teams up with ARM to Develop Deep Learning for IoT Devices\n\nAt GTC 2018, NVIDIA and ARM announce a partnership to integrate open source NVIDIA Deep Learning Accelerator architecture into ARM\u2019s Project Trillium platform. This collaboration aims to simplify the product integration process for IoT chip companies.\n\nHitachi\u2019s new AI Technology service analyzes product, inventory, demand and sales information on warehouse management systems (WMS) and learns effective measures based on evaluation results. Hitachi says the service can boost product picking efficiency by 16 percent.\n\nMitsubishi Hitachi Power Systems (MHPS) announces new investments in innovative technologies with digital tools to deliver solutions to the power industry. MHPS plans to use AI technology, battery storage, and geothermal renewable power to achieve better energy efficiency.\n\nApple announces that its health record products are now being used by 40 health systems and 300 hospitals. It has traditionally been time-consuming for patients to obtain their health records, and Apple\u2019s health records products streamline the process. Apple is planning to make the products available to all iOS users.\n\nMicrosoft France pledges US$30 million over three years to further France\u2019s artificial intelligence development. The AI Impact group aims for AI with a positive impact on environment, transport, and health in France. The AI Skills program meanwhile will train 400,000 people over three years and aims to create 3,000 new jobs in the French digital ecosystem."
    },
    {
        "url": "https://medium.com/syncedreview/caffe2-merges-with-pytorch-a89c70ad9eb7",
        "title": "Caffe2 Merges With PyTorch \u2013 SyncedReview \u2013",
        "text": "Facebook operates two flagship open source machine learning frameworks \u2014 Caffe2 and PyTorch. Their incompatibility, however, made it difficult to transform a PyTorch-defined model into Caffe2 or vice versa. Facebook is doing something about that.\n\nLast Friday the Caffe2 Github page introductory \u201creadme\u201d document was suddenly replaced with a bold link: \u201cSource code now lives in the PyTorch repository.\u201d What this meant was that Caffe2 users could now directly check Caffe2 code in PyTorch.\n\nFacebook AI Researcher and Caffe creator Yangqing Jia says Facebook decided to merge Caffe2 into PyTorch \u201cbecause this will incur minimal overhead for the Python user community.\u201d\n\nNothing changes for a PyTorch user, says Facebook AI Researcher Soumith Chintala. \u201cPyTorch is installed, shipped and used exactly how it is done today. Your code will not break. This is development and backend work. If you are not a core-developer, this issue is not even that relevant to you.\u201d\n\nAlthough most frameworks are similar to certain degrees, each has its unique characteristics. Sharing repository and development infrastructure between different machine learning frameworks can compensate for each one\u2019s shortcomings.\n\nSince its release in October 2016, PyTorch has become a preferred machine learning framework for many AI researchers due to its research flexibility. Over half of Facebook AI projects run on PyTorch. Meanwhile, Caffe 2, launched in April 2017, is more developer-friendly than PyTorch for AI model deployment on IOs, Android and Raspberry Pi devices.\n\nLast September, Facebook and Microsoft announced their Open Neural Network Exchange (ONNX), an open source project that helps researchers to convert models between frameworks. The merging of Caffe2 and PyTorch is a logical next step in this strategy.\n\nThe merging also ups the stakes in Facebook\u2019s challenge to the dominant machine learning framework, TensorFlow.\n\nThere are $28,000 worth of prizes to be won in Alibaba Cloud\u2019s Tianchi International Advertising Algorithm competition! Learn more here and begin competing today!"
    },
    {
        "url": "https://medium.com/syncedreview/chinese-video-surveillance-giant-hikvision-to-opensource-its-ai-technology-7e68412a14ec",
        "title": "Chinese Video Surveillance Giant Hikvision to Opensource its AI Technology",
        "text": "At 2018 AI Cloud Ecological International Summit in Hangzhou on Mar. 28, global video surveillance manufacturing leader Hikvision announced it would open access to its AI technology, notably AI Cloud.\n\nHikvision AI Cloud is a distributed structure incorporating cloud computing and edge computing. Launched last year, it can extend an AI algorithm from the cloud centre to an edge network of on-premises video recorders and servers, and further to edge devices, such as security cameras.\n\nHikvision has deployed AI Cloud in more than 30 Chinese provinces, providing AI-empowered solutions for emergency management, maintenance, urban operation, traffic management, business intelligence, etc. Hikvision\u2019s facial recognition system at an intersection in Suqian, Jiangsu, decreased red-light-running violations by over 90 percent.\n\nHikvision CEO Hu Yangzhong says the company will open an AI development platform for application developers. At the same time, AI Cloud will also merge algorithms from other AI companies.\n\n\u201cThe industry should jointly promote the development and application of AI in the security camera industry,\u201d says Hu.\n\nHikvision will also launch an open training system providing transfer learning and augmented learning capabilities; AI services on EZVIZ, its video-service application designed for consumer markets; and data labelling and sharing services.\n\nFounded in 2001, Hikvision is dedicated to improving video surveillance and video analysis technology, and providing surveillance products and solutions. The company accounted for 21.4 percent of the global market in CCTV and video surveillance in 2016, and now leads the security surveillance market with an estimated value of US$63 billion.\n\nHikvision\u2019s surveillance system has integrated AI chips with frontend cameras, and developed data analysis systems on the backend. As a result, it is an essential supplier for China\u2019s Skynet, a real-time surveillance program for public security. Last year BBC reporter John Sudworth agreed to be tracked by the system, which required just seven minutes to locate and \u201capprehend\u201d him.\n\nNot to be outdone by Hikvision, China\u2019s leading tech companies are also building open-sourced AI platforms and sharing data access, tools, and backend codes with developers. Baidu last year opened its smart assistant platform DuerOS and autonomous driving platform Apollo. By January 2018, DuerOS had activated more than 50 million smart devices, with over 10 million active devices per month.\n\nIFlytek, a leading Chinese AI company known for its voice technology, plotted a bold course last year: \u201cProject 1024\u201d will package CN\uffe51.024 billion into a developer fund, build 1024 professional teams, and support 1024 AI projects."
    },
    {
        "url": "https://medium.com/syncedreview/the-new-age-of-discovery-space-exploration-and-machine-learning-64883f7dc7f9",
        "title": "The New Age of Discovery: Space Exploration and Machine Learning",
        "text": "The Age of Discovery began in the 15th century, when Europeans built their first oceangoing vessels and set out to explore the world. Whether motivated by political, economic or cultural factors, human exploration has traditionally been driven by technological progress.\n\nRocket booster technology developed during World War II enabled the first generation of spaceflight in the mid 20th-century, when the Soviet Union and the United States launched artificial satellites and interplanetary probes. As humans step up to deep space exploration, artificial intelligence technologies are expected to play a huge role.\n\nThe \u201clearning\u201d part of machine learning refers to an algorithm\u2019s ability to find patterns in data to self-improve the machine\u2019s outcomes, ie to use existing data to predict unknowns. Machine learning already has applications in banking, healthcare, aviation, and so on, and the technology is expected to power future space exploration as it can handle huge data volumes, find patterns in planet image datasets, and predict spaceship condition.\n\nThe role of machine learning in space exploration can be roughly divided into data transmission, visual data analytics, navigation, and rocket landing.\n\nSpacecraft and satellites operating in deep space can generate huge amounts of data due to the complexity of their research missions. Because of the different rotations and orbits of their host planets, these massive data packets must be transmitted to earth during specific windows of opportunity. The lag meanwhile will depend on Earth\u2019s light year distance from the spacecraft\u2019s host planet and may be months or even years. Moreover if a data packet transmission is unsuccessful, the data may be permanently lost if it was overwritten with new data in the onboard memory.\n\nMachine learning enables a \u201csmart\u201d method to manage the distant planet to Earth data transmission problem. The outer space machine learning application MEXAR2 (\u2018Mars Express AI Tool) was introduced in 2005 at Italy\u2019s Institute for Cognitive Science and Technology (ISTC-CNR). The onboard learning algorithm can leverage historical data to remove superfluous data and pinpoint the download schedule to optimize data packet transmission. This outer data transmission technique is already being used by NASA and others in their space research programs.\n\nA usual early step in deep space exploration is planet condition and environment analysis. Satellites and space telescopes have already collected a large amount data for example for target planet Mars. Images are the major data source, while the major challenge is how to identify and read the right information from the images. Machine learning has become an effective technique for solving this problem.\n\nThe NASA Frontier Development Lab and top-tier technologies companies such as IBM and Microsoft are collaborating on machine learning as a solution for solar storm damage detection, targeting a target planet\u2019s \u2018space weather\u2019 through magnetosphere and atmosphere measurement. The technique can also be used for resource discovery and to identify suitable planet landing sites.\n\nAnother field where machine learning can improve current technology is in relative spacecraft and satellite motion control. Each control action selected for spaceships or satellites requires considering and processing geometric and kinematical location information in an extremely short timeframe. As outer space missions become increasingly frequent and complex and spacecraft get further from Earth, there will be growing demand for fast and self-adjusting machine-learning based navigation capabilities. The field could include orbit adjustment, autonomous navigation, and space station docking.\n\nThe NASA Jet Propulsion Laboratory (JPL) is already involved in the above research field, and machine learning has emerged as a key technique for measurement and adjustment of a spacecraft\u2019s motion with different orbital parameters. This allows the spacecraft to self-adjust for example orbit and velocity, and can support ground navigation systems to control a spacecraft\u2019s flight path, engine power and orbital position. A spacecraft\u2019s onboard machine learning algorithm also has the potential to perform autonomous navigation in deep space.\n\nRecent research in landing spacecraft has focused on developing algorithms that increase the level of autonomy for air and space systems. Some of the major issues for spaceship or rocket landings include vacuum stage, software errors, guidance and sensor problems etc. Machine learning and computer vision are the core optimization and evaluation techniques for successful landings.\n\nThe SpaceX Falcon 9\u2019s successful landing at Cape Canaveral Air Force Station in 2015 demonstrated machine learning and computer vision\u2019s power to transform space exploration. SpaceX used a convex optimization algorithm to determine the best way to land the rocket, with real-time computer vision data aiding route prediction. These advanced machine learning applications enabled the first reusable rocket in space exploration history \u2014 a feat scientists regard as essential in developing deep space exploration.\n\nDespite the challenges, machine learning will, or must, play a vital role in the coming age of space exploration.\n\nWith the help of advanced machine learning based terrain classifiers and path planning algorithms, NASA built a Mars Rover which can navigate long distances on the planet\u2019s complex surface. Mars may be humans\u2019 current target but the red planet will not be our final destination. There are reports that NASA will deploy robotic machine learning based probes to Jupiter\u2019s moon Europa to search for life, and NASA engineer Hiro Ono says autonomous spacecraft are in the design phase.\n\nSoon, spacecraft may operate using only artificial intelligence and machine learning algorithms. As in the past, it is technological innovations that will enable humans to go \u201cwhere no man has gone before.\u201d For the immediate future, those innovations will continue to emerge from machine learning."
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-ceo-says-fgpa-is-not-the-right-answer-for-accelerating-ai-83c810969edd",
        "title": "NVIDIA CEO Says \u201cFGPA is Not the Right Answer\u201d for Accelerating AI",
        "text": "Accelerating resource-hungry AI applications demands chip performance beyond what mere CPU or GPU can deliver, prompting researchers to turn to sophisticated Application-specific Integrated Circuits (ASIC) and Field Programmable Gate Arrays (FPGA). Chip giant NVIDIA Founder and CEO Jensen Huang created a bit of a stir at yesterday\u2019s GPU Technology Conference in Santa Clara, USA, when he appeared to dis one of these chips\u2019 appropriateness for autonomous vehicle system development: \u201cFPGA is not the right answer,\u201d he said.\n\n\u201cFPGA is really for prototyping. If you want the [self-driving] car to be perfect, I would build myself an ASIC because self-driving cars deserve it,\u201d says Huang.\n\nFPGAs are logic chips best known for their programmability, which gives engineers the flexibility to configure an FPGA for example as a micro-control unit today, and use the same FPGA as an audio codec tomorrow. ASICs meanwhile are custom chips with little or limited programmability. Because FPGAs are more versatile, chip makers can streamline their operations by developing FPGAs rather than ASICs. However FPGAs are both more expensive and less powerful than ASICs.\n\n\u201cWhen you want to build something for cars, you should have a very large concentrated group of expert engineers design the chip one time and sell it to everyone, instead of a hundred random groups of different levels of capability and expertise build their own chips,\u201d says Huang.\n\nNVIDIA has never been impressed with FPGA. Chief Scientist Bill Dally once said \u201cif you want to solve a problem and you are willing to devote a lot of engineering time, just develop the ASIC directly. I don\u2019t think the FPGA is competitive.\u201d\n\nNVIDIA has been developing ASIC for years and has traditionally kept their tech under wraps. At last year\u2019s GTC however the company decided to share the architecture of their Deep Learning Accelerator (DLA) \u2014 an ASIC for deep learning inferencing \u2014 on the open-source codebase Github.\n\nNVIDIA recently announced an agreement with British chip IP company Arm to integrate DLA architecture into Arm\u2019s new Project Trillium platform, which hastens the development of AI inferencing accelerators. As 90 percent of AI-enabled devices shipped today are based on architecture developed by Arm, NVIDIA\u2019s DLA is expected to be deployed on billions of mobile, consumer electronics, and the Internet of Things (IoT) devices.\n\nHowever even as NVIDIA snubs FPGA, rivals like Intel are ramping up efforts to develop and deploy them. In 2015 Intel acquired top US manufacturer of programmable logic devices Altera in an all-cash transaction estimated at US$16.7 billion. Intel has since developed a CPU+FPGA hybrid chip for deep learning inference on the cloud.\n\nIntel also introduced its Movidius Myriad X Vision Processing Unit (VPU), a system-on-chip (SoC) used for vision devices such as smart cameras, augmented reality headsets and drones. The Myriad X is shipped with a dedicated Neural Compute Engine (NCE) for running deep neural networks at high speed and low power in real time at the edge. With NCE, the Myriad X can reach one trillion operations per second in deep learning inferencing.\n\nMeanwhile, the world\u2019s leading supplier of programmable logic devices Xilinx is competing with Intel\u2019s Altera in the FPGA market. While Intel dominates the server chip market, Xilinx has the technology lead, helping the company win orders from large cloud customers.\n\nChinese startup DeepPhi last year garnered US$40 million in funding \u2014 led by Xilinx \u2014 to develop its Deep-Learning Processing Units (DPU), which include both FPGA chips and ASIC chips."
    },
    {
        "url": "https://medium.com/syncedreview/france-pumps-1-5-billion-into-ai-in-bid-to-catch-up-2470a7fc132b",
        "title": "France Pumps \u20ac1.5 Billion into AI in Bid to Catch Up",
        "text": "France is pledging \u20ac1.5 billion to hasten the development of its fledgling AI ecosystem. French President Emmanuel Macron made the commitment this morning at the Artificial Intelligence Summit held at the College de France Research Center. The event included industry discussions featuring top-notch researchers and high-level ministers such as Secretary of State for Digital Mounir Mahjoubi. France also announced partnerships with DeepMind, Samsung, and Fujitsu as part of a grand strategy to build Paris into a global AI hub.\n\nA Summit highlight was the release of the 152-page report \u201cAI for Humanity,\u201d written by President Macron\u2019s star technology advisor, Fields Medal-winning mathematician C\u00e9dric Villani.\n\nVillani accepted government appointment just six months ago, and has quickly pulled together \u201cMission Villani,\u201d composed of machine learning researchers and members of Europe\u2019s Digital Advisory Council. The team interviewed 350 industry leaders to help form \u201cune strat\u00e9gie national port\u00e9e par le plus hautes autorit\u00e9s et la d\u00e9cliner en feuille de route concr\u00e8te\u201d \u2014 \u201ca national strategy carried by the highest authorities and transformed into a concrete roadmap.\u201d\n\nThe strategy promises to comply with European Union\u2019s data privacy policies, namely the General Data Protection Regulation (GDPR), which will take effect this May.\n\nBack in 2017, the French government released the 200-page document France Intelligence Artificelle, detailing over 50 policy proposals and placing the number of French AI startups at over 270.\n\nIn the global AI competition, France trails neighbours Germany and UK, and all lag far behind leaders USA and China. President Macron incorporated the nation\u2019s innovation challenge his election campaign, calling on France to become a pro-Europe \u201cstartup nation.\u201d France has clustered talents from EU\u2019s robotics industries, the Human Brain Project, and FET projects, and accelerated industry development with initiatives like La French Tech.\n\nGoogle has already helped over 230,000 French students improve their digital skills and is building four \u201cLes Ateliers Num\u00e9riques\u201d Google Hubs to provide free digital training to the French public. The company is adding 1,000 employees to its sprawling Paris office.\n\nGoogle\u2019s AI research subsidiary, London-based DeepMind said it will open a Paris lab with 15 researchers led by vernacular AI scientist Remis Munos.\n\nFacebook says it will put \u20ac10 million into its French research center over the next five years and double the number of AI research scientists to 100 by 2022. This is Facebook\u2019s biggest investment in France since the Station F Startup Campus in Paris.\n\nKorean consumer electronics giant Samsung will open its third-biggest AI R&D center in Paris. Led by former Apple Siri Chief Luc Julia, the center will house 100 researchers. Corporate President and CSO Young Sohn said today that Samsung sees France\u2019s strong competitive edge in mathematics and physics as a fertile environment for developing AI talents.\n\nJapan\u2019s Fujitsu is intensifying the current expansion of its Paris AI center, pledging US$61 million over the next five years, while Microsoft is spending US$30 million to open an AI school in France.\n\nPresident Macron sent out 32 tweets from the event under the hashtag #AIforhumanity and #ChooseFrance."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-to-connect-10-billion-devices-in-5-years-d2b2d6a678be",
        "title": "Alibaba to Connect 10 Billion Devices in 5 Years \u2013 SyncedReview \u2013",
        "text": "In a world of increasingly connected humans, the new frontier is devices and the Internet of Things (IoT). Alibaba Cloud wants to secure its position in this rapidly growing market by connecting ten billion devices over the next five years.\n\nPresident of Alibaba Cloud Computing and Corporate Senior Vice-President Hu Xiaoming announced the ambitious plan today at the Computing Conference Summit in Shenzhen, where he compared AI to the human brain, cloud computing to the heart, and the IoT to the sensorium.\n\n\u201cThe Internet\u2019s first phase was digitizing human activities such as shopping, social, and entertainment. This formed the booming market of four billion Internet users,\u201d explains Hu. \u201cThe second half is digitizing the physical world of cars, forests, rivers, factories\u2026 Even trash cans are being connected to the internet! And this will be a profound technological change, a new productivity revolution.\u201d\n\nMarket research firm IDC predicts that by 2020 there will be 50 billion networked terminal devices in the world, storing about 50 percent of all data. Alibaba has been vying for the IoT market since all the way back in 2014 when it formed its Smart Living Group. In 2017 the company fast-tracked IoT development with a dedicated IoT Group, partnering with 200 companies to develop the industry standard ICA Alliance.\n\nIn 2017 August development of the first IoT-dedicated town, Hong\u2019Shan Township, began in partnership with the Wuxi Gaoxin District Government. The town has sensors tracking fire hazards and water pipe leakage, a system that auto adjusts and saves energy on street lamps, and advertising billboards that can flash warnings in case of emergency.\n\nAlibaba Cloud has also signed contracts with Suzhou City on an \u201ceconomic brain\u201d project, Chongqing city on public transportation projects, Suzhou and Shanghai for real-estate management.\n\nAnother component of Alibaba\u2019s IoT strategy are Internet cars powered by its AliOS operating system. There are 500,000 SAIC Motor, Dongfeng Peugeot-Citro\u00ebn, and Ford automobiles on the road connected with AliOS.\n\nAlibaba Cloud IoT says it will focus on providing an open, convenient IoT platform augmented with strong AI capabilities, while at the same streamlining the collaborative computing of cloud, edge, and end devices."
    },
    {
        "url": "https://medium.com/syncedreview/nvidia-unveils-ai-computing-beasts-worlds-largest-gpu-two-petaflop-supercomputer-65dca0cbaf59",
        "title": "NVIDIA Unveils AI Computing Beasts: World\u2019s Largest GPU & Two Petaflop Supercomputer",
        "text": "Five years ago a University of Toronto team led by Dr. Geoffrey Hinton used two GPUs to train the image recognition model AlexNet in a record time of six days \u2014 and GPUs have been powering AI research ever since. However, as researchers take on more challenging tasks, they need more compute power. NVIDIA Founder and CEO Jensen Huang believes he has the answer: \u201cThe world needs a gigantic GPU.\u201d\n\n \n\n At the GPU Technology Conference in Santa Clara, USA today, Huang unveiled the world\u2019s largest GPU \u2014 a binary beast packed with 16 Tesla V100 with doubled memory 32 GB, 81920 CUDA Cores, 2,000 TFLOPS Tensor Cores, and a bandwidth of 300 GB/Seconds between each GPUs.\n\n \n\n It\u2019s like looking under the hood of a muscle car. And it\u2019s an AI researcher\u2019s dream.\n\n \n\n Incorporating 16 GPUs in a single machine raised huge technical challenges in GPU interconnectivity. NVIDIA developed a new GPU interconnect fabric, NVSwitch, an upgrade on NVIDIA NVLink that delivers bandwidth five times higher than the best PCIe switch, enabling systems with higher GPU hyperconnectivity.\n\n \n\n NVIDIA announced that its gigantic GPU is now integrated into the DGX-2, the company\u2019s latest supercomputer for offices and data centers. DGX-2 is the world\u2019s first system to deliver performance of two PFLOPS, has 512GB HBM2 of memory, energy consumption of 10,000 watts, and 1.5TB system memory. The DGX-2 can train AlexNet in just 18 minutes, 500 times faster than the Hinton team in 2012.\n\n \n\n The DGX-2 is aimed at general academic institutions or established enterprises who demand substantial computing in AI research. It will go on sale in quarter three for US$399,000. Huang joked in his keynote speech: \u201cThe more GPUs you buy, the more money you save!\u201d\n\n \n\n The release of new GPU and DGX-2 is expected to consolidate NVIDIA\u2019s data center business, which doubled to US$2 billion in annual revenue in 2017 to become the company\u2019s second largest revenue source. Last December, NVIDIA controversially prohibited the deployment of its consumer-side GPU GeForce series in data centers. This was believed to be a measure to defend the company\u2019s own data center business. \n\n \n\n Also announced today was Clara, a datacenter medical imaging supercomputer for researchers to train models on reconstructing 3D images, detecting brain tumors, and cinematic rendering.\n\n \n\n NVIDIA enhanced its cloud platform NVIDIA GPU Cloud with the release of TensorRT 4.0, the company\u2019s latest high-performance deep learning inference optimizer. TensorRT 4.0 can accelerate AI applications, such as image recognition, speech synthesis, and natural language processing, and reduce data center power consumption by 70%. It incorporates with today\u2019s most widely used AI open source framework, Google TensorFlow 1.7.\n\n \n\n The NVIDIA GPU Cloud also added Kubernetes, a portable, extensible open-source platform for managing containerized workloads and services. Launched by Google in 2014, Kubernetes can help the NVIDIA GPU Cloud manage computing resources, particularly data centers on the cloud, in a cluster orchestration. This enables portability across infrastructure providers.\n\n \n\n Amazon Web Services, Google Cloud Platform, AliCloud, and Oracle Cloud users can access the NVIDIA GPU Cloud. \n\n \n\n NVIDIA is sending a message to the AI community: Its \u201cgigantic GPU\u201d will save researchers time training AI models so they can put more time into AI innovation. While Intel and Google have been catching up in the AI computing market in recent years, NVIDIA\u2019s new product announcements are expected to ramp up its market share in the critical data center business and dramatically expand its influence on the cloud."
    },
    {
        "url": "https://medium.com/syncedreview/the-yolov3-object-detection-network-is-fast-fcceae0ab650",
        "title": "The YOLOv3 Object Detection Network Is Fast! \u2013 SyncedReview \u2013",
        "text": "YOLO creators Joseph Redmon and Ali Farhadi from the University of Washington on March 25 released YOLOv3, an upgraded version of their fast object detection network, now available on Github.\n\nAt 320 x 320, YOLOv3 runs in 22 ms at 28.2 mAP, as accurate but three times faster than SSD. It also runs almost four times faster than RetinaNet, achieving 57.9 AP50 in 51 ms on a Pascal Titan X.\n\nThe first generation of YOLO was published on arXiv in June 2015. The model framed objects separated by bounding boxes and associated class probabilities to treat them as a regression problem. A base YOLO model could detect images in real-time at 45 frames per second, while Fast YOLO was capable of processing 155 frames per second, while still outperforming other real-time detectors.\n\nIn 2016 Redmon and Farhadi developed YOLO9000, which could detect up to 9,000 object categories using the improved YOLOv2 model. At 67 frames per second, the detector scored 76.8 mAP on the visual object classes challenge VOOC 2007, beating methods such as Faster RCNN. The model was also trained to detect unlabelled objects.\n\nThe new YOLOv3 follows on YOLO9000\u2019s methodology and predicts bounding boxes using dimension clusters as anchor boxes. It then guesses an objectness score for each bounding box using logistic regression. The model next predicts boxes at three different scales, extracting features from these scales using a similar concept to feature pyramid networks. Redmon uses a hybrid approach to perform feature extraction, building on former YOLOv2, Darknet-19 and residual networks. The new network, Darketnet-53, is significantly larger and has 53 convolutional layers.\n\nWhen the duo ran YOLOv3 on Microsoft\u2019s COCO Dataset it performed on par with RetinaNet and SSD variants, indicating the model\u2019s strength at fitting boxes to objects. However when the IOU threshold raises the model struggles to align boxes perfectly with objects. Redmon and Farhadi say the model does not work well on average AP between 0.5 and 0.95 IOU metric, but performs very well on a threshold metric of 0.5 IOU. It also performs better with small objects than with large objects.\n\nOn a side note, it\u2019s worth mentioning that Redmon and Farhadi\u2019s paper is not only a step forward in object detection, it\u2019s also peppered with humour. Andrej Karpathy retweeted that the paper \u201creads like good stand up comedy.\u201d\n\nAli Farhadi is the Associate Professor of Computer Science and Engineering at the University of Washington. He also leads Project Plato \u2014 which uses computer vision to extracting visual knowledge \u2014 at the Allen Institute of Artificial Intelligence. His student Joseph Redmon is the YOLO paper\u2019s first author. Redman\u2019s personal website is called Survival Strategies for the Robot Rebellion."
    },
    {
        "url": "https://medium.com/syncedreview/crown-prince-sheikh-hamdan-launches-new-round-of-ai-programmes-in-dubai-967c41e36dae",
        "title": "Crown Prince Sheikh Hamdan Launches New Round of AI Programmes in Dubai",
        "text": "Crown Prince and Chairman of the Board of Trustees of the Dubai Future Foundation Sheikh Hamdan officially opened the 4th edition of the Dubai Future Accelerator (DFA) on March 24th at Dubai\u2019s Emirates Towers. The program aims to boost tech development across the United Arab Emirates.\n\nIn 2016, Hamdan set a goal of achieving 25% autonomous transportation in the UAE by 2030, positioning the country of nine million as a regional reader in AI. This year\u2019s theme \u2014 Take Part in Creating the Future \u2014 matches 12 government entities with private sector partners. The teams will collaborate for nine weeks to solve public challenges using frontier technologies.\n\nThe DFA\u2019s AI deployment is both varied and wide. The Dubai Police will use statistical AI to support their decision-making processes, with a goal of cutting the crime rate by 25 percent by 2021. The Dubai Municipality will use AI and blockchain solutions to improve public infrastructure in areas of pest control, food safety, and health. Dubai\u2019s Department of Economic Development will use AI in optimizing control and inspection produces.\n\nEtisalat Telecommunications will adopt AR and AI to streamline operations and customer support, with the target of 90 percent service automation by 2021. Competitor Du telecommunications meanwhile believes AI and machine learning can upgrade its corporate support services by 30 percent.\n\nOther government departments such as the Dubai Electricity and Water Authority, Smart Dubai, the General Directorate of Residency and Foreigners Affairs, and the Knowledge and Human Development authorities will target challenges in urban environment, education, IoT operations, solar energy, and various explorational smart systems.\n\nThe UAE will host the Middle East\u2019s biggest AI fair this year. \u201cWorld AI Show\u201d will run April 11\u201312 in Dubai before moving to Singapore, Mumbai, and Paris. The AI market in the United Arab Emirates is expected to reach $50 billion by 2025.\n\nLast year, 27-year-old Emirati Omar bin Sultan Al-Olama was named the world\u2019s first-ever Minister of Artificial Intelligence. Al-Olama\u2019s previous contributions to his nation\u2019s top-level strategies include UAE Centennial 2071 and the UAE Strategy for the Fourth Industrial Revolution. In a CNBC interview Al Olama said \u201cIn ten years we will be the capital of AI in service and government. I also think we will be a hub for AI in the region.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/tracking-the-transforming-ai-chip-market-bac117359459",
        "title": "Tracking the Transforming AI Chip Market \u2013 SyncedReview \u2013",
        "text": "Embedded AI can transform a tabletop speaker into a personal assistant; give a robot brains and dexterity; and turn a smartphone into a smart camera, music player, or game console. Traditional processors, however, lack the computational power to support many of these intelligent features. Chipmakers, startups, and capital are taking this opportunity to the market.\n\nAccording to a Gartner report, the chip market\u2019s total revenue hit US$400 billion in 2017, and the figure is expected to exceed US$459 billion in 2018. Traditional chip makers are putting an increasing focus on AI chip development, venture capital is pumping significant investments into the market, and AI chip startups are emerging.\n\nCPU (Central Processing Units) are a chip designed for general computing purpose, emphasizing calculation and logic control functions. They are strong in processing single complex computing sequential tasks, but poor in large-scale data computation.[2]\n\nGPU (Graphics Processing Units) were originally designed for image processing but have been successfully adopted for AI. A GPU contains thousands of cores and is capable of processing thousands of threads simultaneously. This parallel computing design makes GPU extremely powerful in large-scale data computation.[3]\n\nFPGA (Field Programmable Gate Arrays) are programmable logic chips. This type of processor is powerful in processing small-scale but intensive data access. In addition, FPGA chips allows users to program the circuit path through its tiny logic block, to handle any kind of digital function.[2] [4]\n\nASIC (Application-Specific Integrated Circuit) are highly customized chips tailored to provide superior performance in specific applications. However, a customized ASIC is not alterable once put into production.[5]\n\nOthers chip types such as Neuromorphic Processing Units (NPU) \u2014 which have architecture mimicking that of the human brain \u2014 have the potential to become mainstream in the future but are still at early stages of development.\n\nAI Chips, also known as AI accelerators, are processors for AI-related computing tasks. Machine learning technology places great demands on computing power for training algorithms and running applications, which traditional computing hardware cannot provide. As a result the demand for specialized AI chips is growing rapidly. [6] AI Chips can be divided into three major application areas: training, inference on the cloud, and inference on edge devices.\n\nTraining is a process wherein algorithms analyze data, learn from it and finally obtain the intelligence to respond to real-world events. Trillions of data samples are analyzed by the algorithm during this training process. Chip makers must not only ramp up processor performance, but also provide an entire ecosystem \u2014 including hardware, framework and other supportive tools \u2014 to enable developers to shorten their AI technology development processes [6]. Given these challenges, it\u2019s major companies like NVIDIA and Google who are thriving in this space.\n\nNVIDIA is the leader in training. When developers discovered GPU\u2019s parallel computing architecture could accelerate the deep learning training process this brought a significant advantage to GPU giant NVIDIA. By seizing the opportunity, NVIDIA transformed itself into an AI computing company and developed a new GPU architecture, Volta, which emphasizes deep learning acceleration. NVIDIA \u2018s GPU have been widely adopted for training machine learning algorithms, and the company now holds a virtual monopoly in the hardware training market.\n\nGoogle is another big player in this market. Based on the achievements of AlphaGo and the millions of users on its cloud service, Google has strong potential in the training market. The company has developed its own TPU (Tensor Processing Units) to compete with NVIDIA. TPUs are a type of ASIC designed exclusively for deep learning and Google\u2019s TensorFlow framework. Google says its TPU can provide 180 teraflops of floating-point performance, which is six times better than NVIDIA\u2019s latest data center GPU Tesla V100. [7] [8]\n\nA developed machine learning model for AI application areas such as image recognition or machine translation usually comes with high complexity, and the required inference is too compute-intensive to be deployed on edge devices. Therefore, inference on cloud becomes necessary for the deployment of many AI applications. And when an app is being used by thousands of people simultaneously, the cloud server also requires robust capability to meet inference demands. In such cases, FPGAs are the top choice for cloud companies. [9] This type of processor is good at low-latency streaming and computing-intensive tasks. In addition, FPGAs provide a flexibility which allows cloud companies to modify the chips. Traditional chip makers, cloud service providers, and startups are all developing FPGA solutions.\n\nIntel is one of the major players developing heterogeneous computing technology. By acquiring chip maker Altera, Intel boosted its FPGA technology expertise and developed a CPU+FPGA hybrid chip for deep learning inference on cloud. By utilizing the advantages of both processor types, this hybrid chip provides computing power, high memory bandwidth, and low latency. This technology has been adopted by Microsoft to accelerate its Azure Cloud Service.\n\nChinese tech giant Tencent is an example of cloud service providers developing FPGA solutions to support inference on cloud. Tencent developed China\u2019s first \u201cFPGA Cloud Computing\u201d service for its cloud service Cloud Virtual Machine. Compared to a CPU-based cloud server, the FPGA integrated CVM provides better computing power to support HPC application and deep learning development. [6] Accessing FPGA on cloud also eliminates the need to purchase hardware, reducing the cost of developing AI application. Tencent also supports third-party AI application development for commercial use.\n\nDeePhi Tech is a startup focused on inference on cloud. The company garnered US$40 million in funding to develop its DPU (Deep-Learning Processing Units, an FPGA based ASIC) platform. With the DNNDK (Deep Neural Network Development Kit), DeePhi Tech aims to provide a one-stop service for development and deployment of deep learning technologies. DeePhi co-founder Dr. Song Hang is a respected AI researcher who proposed a methodology called \u201cDeep Compression\u201d to reduce model scale, workload and power consumption in order to improve deep learning efficiency. [10] This methodology has been adopted by chip giants such as Intel and NVIDIA.\n\nInternet connections may not always be stable, and the cloud cannot accommodate all computing loads for AI innovations. Therefore future edge devices will require more independence in their inference features. Smartphones, drones, robots, VR and AR immersive experience devices, self-driving cars and so on all require specific AI hardware support. Moreover, breakthroughs in recent years have reduced chip volume, enabling embedding on almost any device, making inference on edge more viable.[11] To meet the demand for different devices, numerous startups are producing their own ASIC. Large chip makers are also adding AI supportive features to their processors.\n\nLeading Chinese phone and processor producer Huawei is boosting the performance of their SoC by integrating AI chips. In collaboration with chip startup Cambricon, Huawei adopted a NPU (Neural Processing Unit, a type of ASIC from Cambricon) to advance its SoC Kirin 970 for its flagship smartphone Mate 10. [12] This integration enhances the the phone\u2019s camera\u2019s image processing features.\n\nChinese startup WestWell Lab\u2019s DeepSouth neural processors are ASIC which simulate human brain neurons. The company created a DeepSouth-based brain simulator that can be used to accelerate medical devices supporting research in Parkinson\u2019s, Alzheimer\u2019s, and neural impairment.\n\nHorizon Robotics is another startup concentrating on embedded artificial intelligence. The company has developed two types of ASIC to support different AI applications. Sunrise series processors are for face recognition and video analytics solutions in smart cameras. Journey series processors are for self-driving cars, and provide real-time detection and recognition processing capacity in eight categories.[13]\n\nAI is far from maturity, and as the AI innovation ecosystem continues to develop the chip market will fluctuate. With the possibility of new frameworks emerging for algorithm development, current leaders in the training hardware market may face new competition. The inference on the cloud market is also still growing, and competition between cloud service providers will intensify as more AI applications are developed. The inference on edge market meanwhile is an arena with both big companies and startups.\n\nGiven the ever-increasing demands of AI applications, we can expect to see more collaborations between chipmakers and developers. Artificial intelligence has already had a significant impact on the chip market, a trend that will continue into the foreseeable future.\n\n[1] Chip market to top $400 billion in 2017, says Gartner: http://www.eenewseurope.com/news/chip-market-top-400-billion-2017-says-gartner-0\n\n[2] \u8be6\u7ec6\u5206\u6790\u4eba\u5de5\u667a\u80fd\u82af\u7247 CPU/GPU/FPGA\u6709\u4f55\u5dee\u5f02?: http://www.sohu.com/a/131606094_470053\n\n[3] What\u2019s the Difference Between a CPU and a GPU?: https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/\n\n[4] Difference Between FPGA and CPLD: http://www.differencebetween.net/technology/difference-between-fpga-and-cpld/\n\n[5] ASIC and SoC: https://www.eetimes.com/author.asp?doc_id=1285201\n\n[6] \u4e00\u6587\u770b\u61c2\u4eba\u5de5\u667a\u80fd\u82af\u7247\u7684\u4ea7\u4e1a\u751f\u6001\u53ca\u7ade\u4e89\u683c\u5c40: https://www.leiphone.com/news/201709/uuJFzAxdoBY7bzEL.html\n\n[7] CPUs, GPUs, and Now AI Chips: http://www.electronicdesign.com/industrial/cpus-gpus-and-now-ai-chips\n\n[8] Quantifying the performance of the TPU, our first machine learning chip: https://cloudplatform.googleblog.com/2017/04/quantifying-the-performance-of-the-TPU-our-first-machine-learning-chip.html\n\n[9] \u6df1\u5ea6\u5b66\u4e60\u7684\u4e09\u79cd\u786c\u4ef6\u65b9\u6848\uff1aASIC\uff0cFPGA\uff0cGPU\uff1b\u4f60\u66f4\u770b\u597d\uff1f: http://www.sohu.com/a/123176776_463982\n\n[10] \u65af\u5766\u798f\u535a\u58eb\u97e9\u677e\u6bd5\u4e1a\u8bba\u6587\uff1a\u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u7684\u9ad8\u6548\u65b9\u6cd5\u4e0e\u786c\u4ef6: https://zhuanlan.zhihu.com/p/30211134\n\n[11] A brief guide to mobile AI chips: https://www.theverge.com/2017/10/19/16502538/mobile-ai-chips-apple-google-huawei-qualcomm\n\n[12] Huawei unveils Kirin 970 chipset with AI: http://www.zdnet.com/article/huawei-unveils-kirin-970-chipset-with-ai/\n\n[13] \u60f3\u6210\u4e3aAI\u9886\u57df\u7684\u82f1\u7279\u5c14\uff0c\u5730\u5e73\u7ebf\u53d1\u5e03\u4e24\u6b3e\u7ec8\u7aef\u89c6\u89c9\u82af\u7247: https://www.jiqizhixin.com/articles/2017-12-21-4\n\n[14] NVIDIA launched Volta GPU computing architecture to bring speed in AI inference and training, as well as for accelerating HPC and graphics workloads.: https://nvidianews.nvidia.com/news/nvidia-launches-revolutionary-volta-gpu-platform-fueling-next-era-of-ai-and-high-performance-computing\n\n[15] Google introduced its TPU (Tensor Processing Units) that accelerates the TensorFlow framework in machine learning.: https://cloud.google.com/tpu/\n\n[16] IBM and U.S. AFRL announced the collaboration on a brain-inspired supercomputing system.: https://www-03.ibm.com/press/us/en/pressrelease/52657.wss\n\n[17] Microsoft is working on AI chips across its different devices, top exec says: https://www.cnbc.com/2017/11/01/microsoft-working-on-ai-chips-across-different-devices-top-exec-says.html\n\n[18] Huawei launched Kirin 970 \u2014 the new glagship SoC with AI capabilities: https://www.androidauthority.com/huawei-announces-kirin-970-797788/\n\n[19] Intel is buying Movidius, a startup that makes vision chips for drones and virtual reality: https://www.recode.net/2016/9/6/12810246/intel-buying-movidius\n\n[20] Report: Amazon working on its own AI chips for Echo devices: https://mashable.com/2018/02/12/amazon-echo-ai-chip/#VFOj98mEfqqy\n\n[21] CB Insight: www.cbinsights.com\n\n[22] Crunchbase: www.crunchbase.com\n\n[23] Hupogu: http://www.hupogu.com"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-mar-w-2-mar-w-3-7c5e00204f5d",
        "title": "AI Biweekly: 10 Bits from Mar W 2 \u2014 Mar W 3 \u2013 SyncedReview \u2013",
        "text": "March 6th \u2014 Google Builds AI Drone Image Analysis System for the Pentagon\n\nGoogle is collaborating with the United States Department of Defense to create an AI system for identifying drone footage. The system will leverage machine learning and computer vision technologies to collect and categorize drone footage more efficiently.\n\nMarch 7th \u2014 Microsoft Announces an AI Platform for Developers in Windows 10\n\nMicrosoft is adding a new AI platform to Windows 10 which includes or enables various AI-based applications such as Photos app for intelligent video creation, Hello app for face recognition log in, and voice assistant Cortana for question answering.\n\nMarch 7th \u2014 Microsoft and Intel Enable Deep Learning Inference Vision Processing Units (VPU)\n\nMicrosoft announces Intel\u2019s Movidius Myriad X VPU hardware will be included in the Windows ML server. Windows ML can pre-train machine learning data models and process developers\u2019 own deep learning tasks within the OS interface. With the added power of Intel\u2019s VPU, Windows ML will be able to run specific deep neural network AI tasks at higher speeds while using less power.\n\nMarch 9th \u2014 Telenor Enhances AI and IoT Laboratory Research\n\nTelenor has opened a new IoT laboratory research unit in Trondheim, Norway. The company will invest over \u20ac5.2 million in the new lab, which will mainly collaborate with its existing AI lab on Narrowband IoT technology research and developer portal enhancement.\n\nMarch 9th \u2014 Waymo and Google Launch an Autonomous Truck Pilot Project in Atlanta\n\nWaymo and Google announce a pilot project for self-driving truck testing in Atlanta, USA. During this project the team will not only test the capabilities of their autonomous truck but also the automated logistics in allocating loads, connecting shippers, factories, and distribution centres.\n\nMarch 13th \u2014 Google Makes Music with Machine Learning\n\nUnder its Magenta project, Google introduces its NSynth (Neural Synthesizer), which uses machine learning algorithms and deep neural networks to create new sounds for electronic music. The prototype can create over 100,000 sounds based on only 16 source inputs.\n\nMarch 14th \u2014 Huawei Introduces Blockchain Stress Test Project\n\nHuawei introduces its Caliper Project, a method for testing blockchain stress performance which can help developers and engineers evaluate their blockchain applications in a controlled environment. Performance results may include success rate of the transaction, speed of transaction, and hardware resource performance.\n\nMarch 14th \u2014 Alexa Now Able to Play AI-Generated Songs\n\nAlexa\u2019s latest skill is the ability to play AI-generated songs. The new DeepMusic feature uses a deep recurrent neural network to collect music samples, and leverages an algorithm to create melodies and songs without human input.\n\nMarch 15th \u2014 Microsoft\u2019s AI Translation System Performs at Human Expert Level\n\nMicrosoft builds a Chinese to English translation system with natural language processing and machine learning techniques that can translate Mandarin to English at a level matching human translators. Microsoft based its results on blind comparisons by bilingual humans between its system\u2019s outputs and those of human translators.\n\nMarch 15th \u2014 Google Ventures Invests in AI Chip Start-Up Sambanova\n\nGoogle Ventures\u2019 parent company Alphabet pours US$56 million in Series A funding into startup SambaNova. SambaNova builds computer chipsets and applications for artificial intelligence and data analytics. SambaNova CEO Rodrigo Liang says the company is also interested in public cloud services."
    },
    {
        "url": "https://medium.com/syncedreview/facebook-ai-proposes-group-normalization-alternative-to-batch-normalization-fb0699bffae7",
        "title": "Facebook AI Proposes Group Normalization Alternative to Batch Normalization",
        "text": "As Facebook struggles with fallout from the Cambridge Analytica scandal, its research arm today delivered a welcome bit of good news in deep learning. Research Engineer Dr. Yuxin Wu and Research Scientist Dr. Kaiming He proposed a new Group Normalization (GN) technique they say can accelerate deep neural network training with small batch sizes.\n\nAlthough deep learning thrives with complex neural networks and large datasets, training a model requires much time and power. This has prompted AI researchers to rethink the normalization techniques they use to reduce training costs.\n\nFacebook AI Research had already taken a few steps forward. Last June, it proposed an accurate, large minibatch SGD technique that can train ResNet50 with a minibatch size of 8192 on 256 GPUs in only one hour, while matching small minibatch accuracy.\n\nThe mainstream normalization technique for almost all convolutional neural networks today is Batch Normalization (BN), which has been widely adopted in the development of deep learning. Proposed by Google in 2015, BN can not only accelerate a model\u2019s converging speed, but also alleviate problems such as Gradient Dispersion in the deep neural network, making it easier to train models.\n\nDr. Wu and Dr. He however argue in their paper Group Normalization that normalizing with batch size has limitations, as BN cannot ensure the model accuracy rate when the batch size becomes smaller. As a result, researchers today are normalizing with large batches, which is very memory intensive, and are avoiding using limited memory to explore higher-capacity models.\n\nDr. Wu and Dr. He believe their new GN technique is a simple but effective alternative to BN. Specifically, GN divides channels \u2014 also referred to as feature maps that look like 3D chunks of data \u2014 into groups and normalizes the features within each group. GN only exploits the layer dimensions, and its computation is independent of batch sizes.\n\nThe idea of GN was inspired by many classical image features like SIFT and HOG, which involve group-wise normalization. The paper states, \u201cFor example, a HOG vector is the outcome of several spatial cells where each cell is represented by a normalized orientation histogram.\u201d\n\nThe paper reports that GN had a 10.6% lower error rate than its BN counterpart for ResNet-50 in ImageNet with a batch size of 2 samples; and matched BN performance while outperforming other normalization techniques with a regular batch size. It is worth noting that Dr. He is the main contributor to the development of ResNet (Deep Residual Network).\n\nGN also outperformed BN on other neural networks, such as Mask R-CNN for COCO object detection and segmentation, and 3D convolutional networks for Kinetics video classification.\n\nGN is not the first attempt to replace BN. Layer Normalization (LN), proposed in 2016 by a University of Toronto team led by Dr. Geoffrey Hinton; and Instance Normalization (IN), proposed by Russian and UK researchers, are also alternatives for normalizing batch dimensions. While LN and IN are effective for training sequential models such as RNN/LSTM or generative models such as GANs, GN appears to present a better result in visual recognition."
    },
    {
        "url": "https://medium.com/syncedreview/mapbox-is-mapping-the-future-9d4cf4753bea",
        "title": "Mapbox Is Mapping the Future \u2013 SyncedReview \u2013",
        "text": "When Google Maps debuted in 2005 the technology amazed users, who could locate an address and even visit the spot using archived \u201cStreet View\u201d captures. The next generation of digital maps for autonomous driving and augmented reality (AR) will require much more \u2014 for example that the states and positions of rendered real world objects be both precise and up-to-the-minute.\n\nThese new challenges and opportunities have prompted mapping companies to reimagine digital mapping technology.\n\nMapbox, a San Francisco-based digital mapping startup, released its Mapbox AR at the Mobile World Conference 2018 in Barcelona. The AR platform comprises a suite of tools, a low-level AR core kit, and a framework. Users can access interactive maps with animated dynamic directions on mobile devices.\n\nFounded in 2011, Mapbox offers clients development tools for mapping their apps and website, including a featured Photoshop-like product, Mapbox Studio, that enables developers to access Mapbox\u2019s visualization layers and make maps with their own data. Mapbox business model focuses on visualizing for example high-density weather radar information; delivering business analytics; and logistics.\n\nWhile Google is unquestionably a dominant player in the digital mapping industry, Mapbox is carving out a niche. Their Road Network collects anonymous data information from 300 million users of apps such as Airbnb, Instacart, Snap, and others with embedded Mapbox SDK. This data can extrapolate detailed patterns, for example, of real-time traffic and environmental status.\n\nDave Cole, a Mapbox founding member and VP of Business Operations and Strategy told Synced that data is the key to his company\u2019s success. \u201cWe have a network now that\u2019s collecting over 220 million miles of anonymous data every single day that goes back into the map and it gets better.\u201d\n\nMapping for autonomous driving is a high-growth sector estimated to open up a US$20 billion market by 2050, according to Goldman Sachs. While breakthroughs in self-driving technologies have thus far come from sensors such as LiDars and Cameras, the important role of maps cannot be overlooked. In the future, maps may even become more important than sensors.\n\n\u201cFor example, when driving on a multi-lane road, the onboard sensor may be unable to detect the road after a turn because of road-side obstructions. Once you have the lane-level positioning and real-time road updates enabled by HD maps, you can slow down and switch to a safe lane in advance to prevent an accident,\u201d says Xudong Cao, Founder and CEO of China\u2019s self-driving startup Momenta.ai.\n\nMapping companies are vying for the vanguard in autonomous driving integration. Dutch mapping firm TomTom is a leader in HD map production, and has attracted investments and partnerships deals from Ford, GM, Fiat Chrysler.\n\nMapbox made its initial move into autonomous driving in 2016 with Mapbox Drive \u2014 now Mapbox Automotive \u2014 a package of developer tools that enabled automakers to customize HD maps and turn-by-turn navigation. Developers can access API offered by Mapbox Automotive and build out specific embedded dash navigation, as well as 3D rendered visualization, to give drivers enhanced environmental context.\n\nMapbox Automotive has also built a Global Lane Network that aggregates sensor data collected by cameras, LiDars and GPS from other self-driving partners and distributes the information.\n\n\u201cThink about a lot of autonomous companies that have invested in computer vision or LiDar. They are generating all this data on the vehicles, but they need a network to actually distribute it back so as one vehicle collects, they can refine the map and make it available for general use for the whole fleet of vehicles,\u201d says Cole.\n\nTo maintain a map that accurately keeps up with the world as it changes, Mapbox researchers are leveraging AI technologies for processing data and turning it into, for example, predictive traffic profiles based on density of people; and using Receptive Field Networks (RFNet) or You-Only-Look-Once (YOLO) algorithms for segmentation and object recognition.\n\nLast year Mapbox purchased Mapdata, an AI mapping startup that uses deep neural networks to improve computer vision and AR. The Belarus-based company is now a satellite R&D team devoted to merging front-facing cameras and navigation.\n\nMapbox has a team of 350 and has thus far accumulated investments of US$227 million. As of February 2018, the number of registered developers on the Mapbox platform topped one million, a milestone for a technology platform.\n\nWhile 60 percent of its business is with US developers, Mapbox is broadening its global operations, particularly in China. This month the former Head of Uber Business Development in Asia-Pacific (APAC) Andy Lee joined Mapbox to lead its APAC expansion.\n\nLee told Synced that because Mapbox is already partnered with Chinese companies such as Alibaba\u2019s trip service Feizhu, it is in a position to help Western companies enter China, and its Chinese partners go global.\n\n\u201cWhat we have done is helpful for many companies who are trying to figure out how to enter China. Building maps in China is a challenge of being able to deliver experiences on a very large scale, not just like hundreds of thousands but sometimes hundreds of millions of users on a daily basis. There are very interesting engineering problems,\u201d says Lee.\n\nWhen Cole spoke to us he shared a story about his youthful hobby of sketching building and drawing maps. Coincidentally, the young Lee loved to carry paper maps his mother gave him and study streets \u201cin a very nerdy way.\u201d\n\n\u201cThe experience has totally changed now because I have a smartphone with a map and travel or dining or lifestyle apps. My early childhood interest in maps is my career now, which is helping developers continue that journey, and that gets me really excited,\u201d says Lee."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-to-begin-testing-self-driving-cars-on-beijing-roads-c3dd9dd242c0",
        "title": "Baidu to Begin Testing Self-Driving Cars on Beijing Roads",
        "text": "Uber\u2019s fatal self-driving car accident continues to send repercussions through the autonomous vehicle industry, and has fomented public doubt concerning the technology\u2019s safety. In response, Uber halted all its road tests for self-driving cars. And so it came as somewhat unexpected news when on March 22nd the Beijing Municipal Government gave tech unicorn Baidu the green light to test driverless cars on the city\u2019s public roads.\n\nBaidu is the first company to be granted special license plates for autonomous vehicle testing on public roads in the Chinese capital.\n\nIn December 2017 Beijing issued China\u2019s first self-driving vehicle policies in the official statements \u201cGuiding Opinions on Accelerating the Road Test of Autonomous Vehicles (Test Version)\u201d and \u201cAutonomous Driving Vehicle Road Test Management Regulations (Test Version), which detailed basic thresholds for testing autonomous vehicles on public roads, mandating for example that test vehicles must not drive on public roads, carry US$800k in insurance, be assessed and approved by the municipality\u2019s management agency at closed test sites, exclude passengers that are not testing engineers, and that no more than five vehicles can participate in a single test.\n\nSubsequent regulations such as \u201cStandards and Methods for Assessing Autonomous Vehicles\u2019 Road Test Capability (Test Version),\u201d \u201cRoad Test Requirements for Autonomous Vehicles at Closed Test Sites,\u201d and \u201cRoad Prerequisites for Autonomous Vehicles Road Tests\u201d introduced more detailed clauses.\n\nThe safety regulations in these documents state that autonomous vehicles tested on public routes must complete 5,000km training at designated closed testing sites, be able to comply with traffic regulations, and have a sound response plan prepared in case of emergency. Test engineers must have over three years driving experience, complete 50 hours of operational training, and be ready to take control of the vehicle at any time. Unless specified, testing vehicles are required to avoid peak traffic times and bad weather. Tests can only be conducted on routes outside Beijing\u2019s 5th ring road.\n\nBaidu is China\u2019s leading tech company in the self-driving space. In February it released the self-driving dataset ApolloSpace, which is about 10 times larger than any other existing open-source dataset. Baidu\u2019s platform Apollo grants developers access to a complete set of service solutions and open-source codes and enables software engineers to convert a Lincoln MKZ into a self-driving vehicle in about 48 hours. Apollo has also joined the UC Berkeley DeepDrive (BDD) Industry Consortium, led by Professor Darrell.\n\nAlthough Beijing is being relatively prudent in its approach to self-driving vehicles on public roads, the Baidu permits are a huge step forward for autonomous research. Shanghai, Chongqing, and Shenzhen are also making relevant policies in this space. Interested readers can follow up here: Global Survey of Autonomous Vehicle Regulations."
    },
    {
        "url": "https://medium.com/syncedreview/my-other-lawyer-is-a-robot-lawgeex-automates-contract-review-eef4e2247114",
        "title": "\u201cMy Other Lawyer is a Robot\u201d \u2014 LawGeex Automates Contract Review",
        "text": "Artificial Intelligence is already helping doctors examine MRI scans, travel agents book flight tickets, and accountants keep books. But the legal industry has been somewhat stubborn when it comes to AI. \u201cThe assumption is that as long as it\u2019s read by humans it\u2019s fine, but there\u2019s no ground for this in any other industry in the world,\u201d says Shmuli Goldberg, VP of Marketing at LawGeex.\n\nFounded in 2014 by international business lawyer Noory Bechor and machine learning guru Ilan Admon, LawGeex is a Tel Aviv-based legal tech startup focused on contract review. It closed US$7 million in Series A funding last March, principally from Indeed.com owner Recruit Holdings, Lool Ventures, and LionBird. The company has a team of close to 50 in its Tel Aviv R&D units and New York sales office.\n\nThis February, LawGeex created a stir in the legal industry when its algorithms outperformed human lawyers at contract review in a study conducted by Stanford, Duke University, and the University of South California.\n\nResearchers asked 20 experienced lawyers and the AI to examine five previously unseen contracts composed of 153 legal clauses copied from standard Non-Disclosure Agreements (NDA). The AI used just 26 seconds to complete its analysis with an accuracy rate of 94 percent. Human lawyers meanwhile required nearly 92 minutes to achieve an average accuracy rate of 85 percent.\n\nA Fortune 1000 company can deal with up to 40,000 contracts annually. Many companies are dissatisfied with the existing organizational contract review process, which is an essential but laborious task. Confidentiality agreements, in particular, take a week or even longer to review.\n\nGoldberg says the LawGeex AI can cut the contract review cycle from a week or a month down to one hour, and reduce the time spent by a lawyer reviewing the contract from hours to minutes \u2014 adding that law firms which bill by the hour are not as interested in automated efficiency. LawGeex can also review NDAs, purchase orders, service agreements, software license, sales and other types of contracts with a consistency, says Goldberg, that tired office-trapped lawyers on a Friday evening cannot match.\n\nThe ideal LawGeex client is an in-house legal departments of medium to large-sized enterprises with over a thousand contracts per year. The client will first making clear their precedents and definition of terms. The AI, pre-trained to understand what they mean, will then identify pertinent elements in the contract and make sure these align with the client\u2019s specifications.\n\nThe default AI application for document review is natural language processing (NLP). However, LawGeex discovered that NLP performed poorly in contract review due to legalese. Although natural language can have multiple interpretations, legalese is specific and rigid.\n\nTo cope with this problem LawGeex developed two new AI algorithms. A Legalese Language Processing (LLP) algorithm first trains the neural network with as many contracts as possible. The AI learns to understand terms such as \u201cnon-compete\u201d and \u201cdisclosure,\u201d and identify terminology-relevant clauses in the contract. A Legalese Language Understanding (LLU) algorithm works on top of the LLP, translating the legalese into legal concepts to help the AI understand unfamiliar clauses.\n\nThe hardest part of building the process, says Goldberg, was finding half a million contracts as training datasets, as there are no standard learning or training sets in the legal world \u2014 such as ImageNet for computer vision or MNIST for handwritten digits. LawGeex enlisted the help of experienced lawyers from top US law firms and spent three years hunting for shared and disclosed contracts.\n\nGoldberg says what separates LawGeex from automated contract service providers such as Legalsifter and Kiva Systems is that \u201cthey are all based on the same premise, if I have thousands of contracts and want to find one thing, they are the best for that. Whereas our tool looks at one contract and answers the question \u2018can I sign this?\u2019 When someone comes to the lawyer, hands them the NDA and asks if he can sign this, our tool opens the NDA up, looks the potential hundreds of issues, highlights what\u2019s relevant and irrelevant and passes it onto the lawyer.\u201d\n\n\u201cThis isn\u2019t a new technology,\u201d says Goldberg, \u201cand we are not saying that AI is taking over the legal world, we are saying the opposite: AI has taken over the legal world.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/2017-turing-award-goes-to-computer-chip-pioneers-2c126b97c8e5",
        "title": "2017 Turing Award Goes to Computer Chip Pioneers \u2013 SyncedReview \u2013",
        "text": "The Association for Computing Machinery (ACM) today announced John L. Hennessy, former Stanford University President and Chairman of the Board of Alphabet; and David A. Patterson, retired Professor at University of California, Berkeley, as winners of the 2017 Turing Award for their groundbreaking approach to computer architecture design and evaluation.\n\nThe Turing Award is the Nobel Prize of computer science, named after British mathematician Alan M. Turing, who laid the mathematical foundations for and defined the limits of modern computing. The award is sponsored by Google and carries a prize of US$1 million.\n\nHennessy and Patterson proposed a systematic, quantified approach to building faster and more energy-efficient Reduced Instruction Set Computer (RISC) microprocessors, which has been widely adopted by academia and industries. Today, RISC processors account for 99 percent of the more than 16 billion microprocessors produced each year and are used in smartphones, tablets and other embedded devices.\n\nHennessy and Patterson are also the co-authors of Computer Architecture: A Quantitative Approach, an influential 1990 textbook that has contributed to improved microprocessor design across the computer architecture community.\n\nIn the textbook the pair encourage architects to tailor their system designs to accommodate different memory and computing demands. The textbook also prompted considerations of energy consumption, heat dissipation, and off-chip communications, widening computer architecture\u2019s traditional focus from computational power alone.\n\n\u201cHennessy and Patterson\u2019s contributions to energy-efficient RISC-based processors have helped make possible the mobile and IoT revolutions. At the same time, their seminal textbook has advanced the pace of innovation across the industry over the past 25 years by influencing generations of engineers and computer designers,\u201d said ACM President Vicki L. Hanson.\n\nHennessy and Patterson have received numerous honours, including the ACM-IEEE CS Eckert-Mauchly Award and the IEEE John von Neumann Medal. They are fellows of the ACM and IEEE and members of the National Academy of Engineering and the National Academy of Sciences."
    },
    {
        "url": "https://medium.com/syncedreview/huawei-introduces-ai-development-board-hikey-970-763ac996b29a",
        "title": "Huawei Introduces AI Development Board HiKey 970 \u2013 SyncedReview \u2013",
        "text": "At the Linaro Connect Hong Kong 2018 Developer Conference today, Chinese tech giant Huawei unveiled its latest development board, HiKey 970, which is dedicated to AI-powered devices and applications.\n\nHuawei\u2019s HiKey series are single board Linux computers that developers can use to write and test applications. They run on Huawei\u2019s smartphone SoC (system on a chip), and their development is supported by Linaro, an engineering organization that works on free and open-source software.\n\nHiKey 970 is based on Huawei\u2019s Kirin 970, the world\u2019s first AI processor for smartphones, with four ARM Cortex-A73 and four ARM Cortex-A53 cores, 6GB of LPDDR4 memory, the latest generation 12-core Mali G72MP12 graphic processors, and a Neural Processing Unit dedicated to AI acceleration. The Kirin 970 is up to 25 times faster and 50 times more energy efficient than traditional processors.\n\nHuawei has been a leader in the global telecommunications equipment market since 2012, and released the groundbreaking Kirin 970 SoC last September. The company\u2019s Kirin 970-equipped flagship smartphone Mate 10 is the first-ever mobile phone to ship with AI hardware.\n\nHiKey 970 supports a variety of interfaces including an AI stack, the Huawei HiAI computing architecture, and popular neural network frameworks. Developers can leverage HiKey 970 for easier and more efficient AI development in robots, smart cities, deep learning algorithms, etc.\n\nTo help free AI application developers from concerns regarding costs, distribution/promotion, and IP issues, Huawei improved HiKey 970\u2019s design and introduced features such as a multi-application model, support for machine learning frameworks, comprehensive documentation, rich and efficient APIs, a rapid-start source code, and so on.\n\nHiKey 970 will compete with the UK\u2019s Raspberry Pi boards in this market, and will go on sale in mid-April."
    },
    {
        "url": "https://medium.com/syncedreview/where-is-autonomous-driving-headed-after-the-fatal-uber-accident-in-arizona-130c893a35b6",
        "title": "Where is Autonomous Driving Headed After the Fatal Uber Accident in Arizona?",
        "text": "An Uber self-driving SUV struck and killed a female pedestrian Sunday evening in Tempe, Arizona. The first known autonomous vehicle-related pedestrian death on a public road stunned the AI community and raised public concerns on autonomous driving safety.\n\nTempe police reported that the accident took place around 10 p.m. local time. The Uber vehicle was in autonomous mode, with a human safety driver at the wheel. The weather in Tempe Sunday night was clear and dry. The US National Transportation Safety Board (NTSB) said that it was joining the accident investigation.\n\nUber immediately suspended all its self-driving testing in North American cities. The company had launched its first self-driving road tests in Arizona only a few months ago.\n\nArizona is a hotbed of self-driving technologies and there are over 600 self-driving cars on the state\u2019s public roads. The state legislature has for years been cultivating an AV-friendly testing environment that rivals California\u2019s. Two weeks ago Arizona gave the green light for public road testing without human drivers in the vehicle.\n\nThere has been no official word yet on whether Arizona regulators will reconsider their relatively accommodating autonomous vehicle testing policies in the aftermath of Sunday\u2019s fatal accident.\n\nUber CEO Dara Khosrowshahi said Sunday\u2019s accident was \u201cincredibly sad news\u2026 We\u2019re thinking of the victim\u2019s family as we work with local law enforcement to understand what happened.\u201d\n\nIt\u2019s been a yearlong series of scandals for Uber, including a sexual harassment class action suit, the former CEO\u2019s departure, and an exodus of core executives. Last month the ride-hailing giant paid Waymo US$245 million in stock to settle a self-driving technology IP infringement lawsuit.\n\nGoogle Cloud Chief Scientist of AI/ML Fei-Fei Li tweeted \u201cA fatal self-driving car accident\u2026 This is what AI is: it\u2019s deeply impactful to human lives; and it takes all of us to work on it to make it safe, fair and benevolent.\u201d\n\nDirector of Microsoft Research Labs Eric Horvitz called for increased diligence in self-driving safety in the wake of the accident, and stressed that \u201cself-driving cars will be held to a higher standard. Automation will almost certainly bring down numbers of deaths on the roads.\u201d\n\nSelf-driving vehicles have logged millions of miles on public roads with strong safety records. \u201cThe probability of having an accident is 50 percent lower if you have Autopilot on,\u201d says Tesla CEO Elon Musk. When things do go wrong, however, they make headlines. In 2016, the driver of a Tesla in semi-autonomous driving mode was killed after colliding with a truck on a Florida highway. Although the driver had reportedly ignored at least seven safety warnings, the accident severely hit Tesla\u2019s credibility in the semi-autonomous driving market.\n\nLast year in Las Vegas a self-driving bus was involved in a crash with a delivery truck, only two hours after it made its debut. No injuries were reported at the scene. While technically the bus was not responsible for the accident \u2014 and the delivery truck driver was cited by police \u2014 passengers on the smart bus complained that it was not intelligent enough to move out of harm\u2019s way as the truck slowly approached.\n\nInsiders are saying the Arizona accident is the most serious setback yet for self-driving vehicles, coming at a critical time when the technology was transitioning from research and development to operation and deployment.\n\nEarlier this year, Waymo bought thousands of autonomous-capable Chrysler Pacifica Hybrids. The Alphabet-owned company has tested its self-driving cars in 25 cities across the US. American car maker Ford meanwhile is testing the capability of its own self-driving technology with delivery tasks in some US states.\n\nMany questions remained unanswered: How will the regrettable accident affect autonomous driving R&D and Startups? Will there be a public backlash against autonomous vehicles? Will lawmakers tighten regulations on self-driving cars on public roads?\n\nSynced is covering the story and will continue to update readers with the latest news."
    },
    {
        "url": "https://medium.com/syncedreview/googles-musicvae-is-a-machine-learning-mozart-eb0e44c790d5",
        "title": "Google\u2019s MusicVAE Is a Machine Learning Mozart \u2013 SyncedReview \u2013",
        "text": "Google has announced the release of MusicVAE, a machine learning model that makes composing musical scores as easy as mixing paint on a palette. A breakthrough from Google Brain\u2019s Magenta Project, MusicVAE generates and morphs melodies to output multi-instrumental passages optimized for expression, realism and smoothness which sound convincingly like human-composed music.\n\n \n\nWhile breakthroughs in AI technologies have thus far tended to emerge from research into industry solutions, Magenta is exploring AI\u2019s potential in the creative spaces that differentiate humans from machines. Launched in 2016, Magenta uses deep learning and reinforcement learning algorithms to explore art and music and has introduced a number of research tools, including NSynth, a music synthesizer; and SketchRNN, an online neural network-based interactive doodling experiment.\n\n \n\nTeaching a machine to create a standardized method for blending different musical elements is not easy. Google researchers turned to Variational Auto-Encoders (VAE), a widely-used generative model that has yielded state-of-the-art machine learning results in image generation and reinforcement learning since 2013. \n\n \n\nVAEs work in an encoder-decoder structure where the encoder represents the variation in a high-dimensional dataset with a lower-dimensional code, and the decoder morphs the variation in a neural network to create an output. The model can be refined and tuned by comparing the input and output. \n\n \n\nGoogle researchers had already applied the technique to SketchRNN, and have now brought the same infrastructure to MusicVAE. Because musical elements are typically more complicated than sketches, Google researchers developed a novel hierarchical decoder for MusicVAE that is capable of generating long-term structure from individual latent codes.\n\n \n\nGoogle last Thursday released a Tensorflow implementation of MusicVAE and a JavaScript library with pre-trained MusicVAE models to help coders, composers and researchers build tools. \n\n \n\nSeveral Google engineers have already handcrafted applications based on MusicVAE. Melody Mixer is an interface created by Google\u2019s Creative Lab that allows users to generate interpolations between short melody loops. Latent Loops, from Google\u2019s Pie Shop, can generate a palette of melodic loops by sketching on a matrix. \n\n \n\nDemos and music samples generated by MusicVAE are already popping up on social media. \u201cThis MusicVAE thing is absurdly cool. The interpolated (and randomly generated) melodies/songs sound *real*, like they were composed, not generated,\u201d tweeted Alexander Huth, an Assistant Professor in Computer Science and Neuroscience at UT Austin. \n\n \n\nThe Magenta team stresses that MusicVAE and their other smart tools are meant as collaborative tools to \u201callow artists and musicians to extend (not replace!) their processes.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/african-fintech-startups-are-revolutionizing-banking-b726a1b4ccfe",
        "title": "African Fintech Startups Are Revolutionizing Banking",
        "text": "Africa is a vast continent with diverse economies and a total population of over 1 billion people living in 54 independent countries spread over 30 million square kilometres. By the end of 2017, there were more than 300 fintech startups across the continent. Disrupt Africa\u2019s Finnovating for Africa: Exploring the African Fintech Startup Ecosystem Report 2017 concludes that African fintech startups\u2019 growth since 2015 has been nothing short of tremendous.\n\nThe record shows over US$100 million in fintech funding has been secured across the continent over the last two years, with South Africa receiving 34.2 percent of the total and Nigeria following closely with 34 percent. South Africa has the most fintech startups with 94, followed by Nigeria with 74 and Kenya 56. In this report, we identify the factors and the drivers behind the growth.\n\nFintech has made payments and remittances more convenient across the continent. Most traditional banks are located in cities and commercial areas, making them difficult to access from remote areas. Tanzania, for example, has about 50 million people sparsely distributed across an area nearly four times the size of the United Kingdom. In Nigeria, banks used to be packed with customers queueing to pay their utility and cable TV bills, school fees and so on. It could take hours to make a simple transaction.\n\nIn 2012, a cashless policy was introduced by the Central Bank of Nigeria to curb excess handling of cash and reduce the volume of money in circulation. The policy has facilitated many Nigerian fintech startups\u2019 market penetration and expansion.\n\nCustomer transportation costs, waiting times and the loss risks attached to cash have been eliminated by the smartphone-based fintech services provided by these startups. About 100 of Africa\u2019s fintech startups are focused on streamlining money transfers. According to Tayo Oviosu, Founder & CEO of Nigerian mobile payment platform Paga, \u201cNigerian banks have traditionally not focused on retail. Paga has built the single largest network of financial access points in Nigeria. We are going to leverage that to deliver financial services to the mass market\u201d.\n\nOne of the significant drivers of African fintech startups is high confidence in the market. Early fintech startups demonstrated that the market is strong, and the growth trend has continued, attracting Silicon Valley-based accelerators. Fintech startup funding is presently one of the most attractive investments on the continent. In 2017, over 30% of the US$195 million in VC funding raised by Africa startups went to the fintech sector.\n\nSafaricom\u2019s M-Pesa mobile money service has had a great impact in Kenya, and Nigeria\u2019s Paga, South Africa\u2019s Zoona, Kenya\u2019s BitPesa and others across the continent are garnering increased funding as investors become more confident.\n\nSince the inception of fintech, there have been dramatic changes in the continent\u2019s traditional banking system. Banks and financial institutions are under pressure to match the innovative solutions and services being offered by fintech startups which have reached millions of people who have mobile phones but not bank accounts.\n\nNow, banks and financial institutions are introducing a variety of strategies and tactics to invest in, acquire or collaborate with fintech startups. This is a trend that\u2019s expected to continue.\n\nMore Africans Connected to the Internet\n\nNigeria, South Africa, Egypt, Ethiopia and Kenya are among the most significant mobile markets in Africa. Although 80 million Nigerians \u2014 47 percent of the population \u2014 do not have bank accounts, 142 million Nigerians have mobile network access and 92 million are internet users, according to the Nigerian Communications Commission (NCC).\n\nThe penetration of mobile phones and the internet has enabled fintech to influence how financial services and products are developed and delivered, as more Africans plug into digital financial services in Nigeria and across the continent.\n\nKenya\u2019s M-Pesa is being used by more than half of the country\u2019s adult population, and has recorded transactions worth more than half the country\u2019s GDP since its debut. Similar success stories have been told by the likes of South African startup Zoona, Nigeria\u2019s Paga and others.\n\nA World Bank report notes that Nigeria, like many countries in sub-Sahara Africa, has a growing population that lacks easy access to traditional financial services, and fintech innovators are filling the vacuum by connecting these people.\n\nThis differs from the situation in advanced economies with strong financial institutions, where fintech startups are cast instead as disrupting the traditional banking industry. For example, China\u2019s Wechat is a popular messaging app with a wallet feature that enables users to send and receive money, make payments and so on from within the Wechat app, without connecting to a bank account for many transactions.\n\nAmong the factors responsible for African fintech startup growth are strong competition and loose regulations. Presently across the continent, there are few or no strict regulations compared to advanced economies. Startups can operate relatively tax-free free and with less government interference, leaving them to chart their courses and develop their products with little regulatory interference.\n\nThere is also increasing integration from service providers, many of which are partnering with fintech startups to make transactions more convenient for customers. For example, cable TV companies such as DSTV, HiTV, and TSTV have introduced fintech alternatives to their traditional bank payment models.\n\nAs more banks and financial institutions acquire or partner with fintech startups, the trend is being seen not so much as financial industry competition but as an industry reinvention that has improved financial services companies\u2019 profiles, reach, and products and services; and is beneficial to banks, startups and customers alike.\n\nFintech has thus become one of the most vibrant investment options in the African tech space.\n\nIn recent years, African fintech startups have been outperforming banks in delivering digital financial services. Iyin Aboyeji, Chief Executive of digital payment technology startup Flutterwave, describes fintech as a fundamental element that will drive the digital economy in Africa over the coming years. Investors and companies are expected to get even more involved in African fintech markets because the opportunities are tremendous."
    },
    {
        "url": "https://medium.com/syncedreview/yu-zheng-on-jd-coms-smart-city-ambitions-265253d51746",
        "title": "Yu Zheng on JD.com\u2019s Smart City Ambitions \u2013 SyncedReview \u2013",
        "text": "Dr. Zheng joined JD Finance last month as Vice President and Chief Data Scientist, Urban Computing Business Unit President, and Urban Computing Lab Director. JD Finance is the fintech arm of JD.com, China\u2019s largest e-commerce platform by revenue.\n\nSynced recently spoke with Dr. Zheng on JD Finance\u2019s entry into Urban Computing.\n\nFintech is composed of two keywords: finance and technology. Many people think of JD Finance as a fintech company, but this is not accurate: we provide empowering technology for the finance industry.\n\nJD Finance\u2019s business model is B2B2C. To illustrate this, let\u2019s say we provide a better risk control model for banks, which in turn give better loan services for customers. In this equation JD Finance is the first B (business), the bank is the second B (business), and the end customer is C (customer).\n\nWe can also replace the middle B (business) with G (government), providing governing institutes the proper technology to serve its people. In other words, urban computing broadens JD Finance\u2019s existing businesses. The Chinese Government\u2019s invitation to JDF to get into these urban computing areas such as transportation and environment will change the public\u2019s view of our company.\n\nThat is confidential for the time being, we have some upcoming announcements. But I can tell you that we are building sub-divisions focusing on environment and transportation control, and the teams will be big.\n\nSmart commerce. People\u2019s impression of urban computing is environment, transportation, and various types of city planning. But in fact commerce also plays a big role in this panorama, for example in business site selection, real estate assessment, and helping banks with risk management.\n\nFor example, if a company seeks a bank loan to build a casino, then the bank will conduct risk assessment on the project itself, whereas in the past, the bank\u2019s approach may have been to just assess the company\u2019s own credit qualifications such as bad debt ratios, creditworthiness, and so on.\n\nCompanies with good qualifications may undertake risky projects by factoring in local development and consumption levels, which are only reflected in forms of data. Criteria such as local spending index, travel method, urban infrastructure including power networks and transportation all contribute to the decision making process.\n\nUrban computing uses diverse spatio-temporal data to do calculations such as analysis, prediction, causal analysis, and anomaly detection for a given scenario.\n\nWe have a lot of data. JD.com has nearly 300 million active users, as shown in our latest financial report. Our huge datasets are composed of product info, user transaction data, and logistic data. Financial figures like wealth management, payment and consumption also contribute to datasets.\n\nSufficient data quantity can accurately reflect a city\u2019s economic well-being. Logistics data for example maps the commercial flow of an area and its business relations with surrounding areas. We have data in abundance which is rare and good.\n\nUrban computing relies on spatio-temporal data, which is neither video, image, nor text. It has its own data management methods and AI algorithms. In other words, you can\u2019t solve the problems at hand by throwing in a CNN or LSTM alone. Spatio-temporal attributes, including time trends, periods, and proximity, spatial distance, and spatial gradation are characteristics that cannot be grasped by commonly used algorithms.\n\nThere are also multiple data sources. For instance, the casino case we mentioned above demands the use of POI, road network data points, plus a lot of data such as environmental and spending, which jointly predict future changes.\n\nMultivariate data fusion is a difficult, and it is also a relatively new discipline and research direction in machine learning. How can data from different fields determine that 1+1 is greater than 2? This is very difficult.\n\nAt the same time, urban computing is not a simple cloud computing problem. Cloud computing platforms can\u2019t support such spatio-temporal data. The data structure query method of spatio-temporal data, as well as the multi-data fusion and indexing mechanism just described, do not yet exist.\n\nThe cloud service providers currently on the market are not suitable for urban computing. Service providers must undergo a special technical build up, in order to manage, analyze, and tap into spatio-temporal big data, and form a dynamic closed loop. It is very difficult, and the threshold is also very high.\n\nLet me give you a concrete example: traffic light control is more difficult to tackle than AlphaGo. AlphaGo faces a 19\u00d719 grid, and the states on each grid are only black, white, or empty.\n\nYet there are tens of thousands of intersections of traffic lights in Beijing, and the status and actions of each intersection have more possibilities. Traffic may be flowing at 40 km/h, 45km/h or 30 km/h; signal light timing may be 30 seconds for the red light or 20 seconds for the green light. All of these are changing continuously. We are also missing data. Roads with no pedestrians, cars, or sensors do not give us any data. The road is also an open system. One man, or even a dog crossing the road will alter the traffic state.\n\nTherefore, urban problems have a large state space, large movement space, and an open system. This is certainly much more difficult to solve than problems on a Go board.\n\nLet me give another example. In urban population flow forecasting, we divide cities into a number of grids, and we want to predict how many people will enter and exit in each grid.\n\nThe flow of people in a grid is related not only to how many people have entered and exited in the previous hour, but also to how many people are moving in and out of neighboring grids. You also want to loop in areas that are far away, taking into consideration for that when big events happen, people will emerge from subway exits. If you only rely on local changes around the grid, you can\u2019t predict bigger things happening, say tragic incidents such as a stampede.\n\nIt\u2019s data and team. Urban computing relies on good databases and data resources. Everyone thinks that governments have a lot of data, but this is not the case. In many cases, they need industry data to support their own decision-making processes and solve problems.\n\nAlso important is the team. People say AI is a talent war, but it\u2019s definitely not a war of numbers. A team of one hundred is not necessarily better than a team of ten. But there are many times when you can\u2019t solve a problem yourself and so rely on others for inspiration. If we are stuck with missing critical data, perhaps an \u201ca-ha!\u201d moment from a teammate will provide alternative measures. This is what the word \u201ctalent\u201d stands for.\n\nA good engineer can work on ten projects at the same time, while a mediocre team might use one hundred people without any results. We realize the importance of talent, and so we have heavily invested in pooling top-notch talents.\n\nPrior to joining JD Finance, Dr. Zheng led the urban computing team at Microsoft Research Asia, publishing profusely in this area; while also holding the positions of Chair Professor of Shanghai Jiaotong University, Guest Professor of Hong Kong University of Science and Technology, and Editor-in-Chief of ACM TIST."
    },
    {
        "url": "https://medium.com/syncedreview/global-survey-of-autonomous-vehicle-regulations-6b8608f205f9",
        "title": "Global Survey of Autonomous Vehicle Regulations \u2013 SyncedReview \u2013",
        "text": "Most people today realize it is only a matter of time before self-driving vehicles revolutionize our transportation and delivery systems. While breakthroughs in self-driving technologies have thus far come from a cluster of star companies such as Google Waymo, Ford, GM, and Tesla, the important role of lawmakers and local authorities in the research and development process cannot be overlooked.\n\nWithout government permits, testing self-driving cars on public roads is almost universally illegal. The Vienna Convention on Road Traffic, an international treaty that has regulated international road traffic since 1968, stipulates that a human driver must always remain fully in control of and responsible for the behaviour of their vehicle in traffic.\n\nEuropean and North American countries such as the US, Germany, UK, and Netherlands were pioneers of self-driving vehicle licensing, and have introduced regulations for self-driving cars on public roads and issued autonomous testing permits. Asian countries quickly caught up and have been enacting similar legislation over the last three years.\n\nSynced surveyed the international regulations, and here are our picks of major regions and countries that are accommodating the testing and deployment of autonomous driving technologies on their public roads.\n\nWhen the mass-produced Model T Ford was introduced in 1908 the US became the first country where a typical middle-class family could afford a car. Today, Uncle Sam is also a leader in the integration of self-driving technologies.\n\nEach US State is responsible for its own autonomous driving legislation. Last year, 33 states had either passed legislation, issued executive orders, or announced initiatives to accommodate self-driving vehicles on public roads.\n\nCalifornia is undoubtedly the top-ranked state in openness and preparedness for autonomous vehicles. Its autonomous vehicle testing regulations were introduced in September 2014 and required a driver be in the vehicle, ready to assume control. Recently, California took a step forward by allowing fully autonomous vehicles with no driver to operate on its public roads.\n\nFifty self-driving companies are testing their technologies in California, Fortune reports. Google\u2019s Waymo and GM lead in autonomous miles logged: Waymo accumulated 352,545 autonomous public road miles (567,366 km) in the 12 months preceding November 2017, while GM vehicles drove 131,676 miles (211,912 km) in 2017.\n\nArizona meanwhile has also removed obstacles to the deployment of autonomous vehicles, cultivating an AV-friendly testing environment that now rivals California\u2019s. In August 2015, Arizona Governor Doug Ducey signed an executive order directing agencies to \u201cundertake any necessary steps to support the testing and operation of self-driving vehicles on public roads within Arizona.\u201d This March, Ducey updated the executive order and gave the green light for cars without drivers to operate on public roads in Arizona. There are now over 600 self-driving cars on the state\u2019s public roads.\n\nFlorida, Michigan, and Pennsylvania, are also leaders in the accommodation of autonomous vehicles.\n\nChina is nowhere close to a regulation-friendly country for autonomous driving. Although it has prominent autonomous driving companies such as Baidu Apollo, JingChi.ai, and Pony.ai, the country got off to a slow start with legislation and permits.\n\nThings are changing now, especially in first-tier cities. Earlier this month Shanghai issued its first self-driving licenses, allowing two automakers to test their autonomous vehicles on public roads. The tests are limited to a 5.6 km (3.5 mile) stretch of public road in the city\u2019s Jiading District. Shanghai is China\u2019s first Smart Network and Autonomous Driving Pilot City.\n\nThis January, the Beijing Municipal Traffic Commission announced the city\u2019s first autonomous driving test track will be built in suburban Yizhuang. Meanwhile, Hangzhou, the home city of China\u2019s tech giant Alibaba, will open an autonomous driving test track this year, located 1.4 km from Alibaba\u2019s main campus. China\u2019s autonomous vehicle industry hub of Guangzhou recently allowed Pony.ai and JingChi.ai to test vehicles in certain districts.\n\nLast December Chongqing revealed a plan to designate a huge open road test area by 2019 that includes cities, mountains, highways, tunnels and bridges, and is enabling 5G telecommunications across the area. The local government also introduced the Chongqing Autonomous Vehicle Road Test Management Implementation Rules to regulate testing on local public roads.\n\nLast December, the Shenzhen Bus Group\u2019s \u201cSmart Driving Bus System\u201d was introduced on a dedicated 1.2 km route.\n\nAlthough Northern Europe does not get as much attention as the US or China when it comes to autonomous driving, countries such as Netherlands and Sweden are more likely to democratize automated transport systems nationwide than any other regions.\n\nKPMG\u2019s 2018 report Autonomous Vehicles Readiness Index ranks 20 countries\u2019 preparedness for an autonomous vehicle future. The Netherlands took the top spot, outperforming the US (7th) and China (16th). KPMG praised the Netherland\u2019s heavily-used and well-maintained road network. The country has also built almost 30,000 electric vehicle charging points and has high-quality wireless networks for transmitting data to and from autonomous vehicles.\n\nThe Netherlands\u2019 Council of Ministers first approved autonomous vehicle road testing in 2015, and updated its bill last February to allow tests without a driver. The Dutch government is spending \u20ac90 million adapting more than 1,000 of the country\u2019s traffic lights to enable them to communicate with autonomous vehicles.\n\nIn 2016 the Netherlands deployed WEpods in a central Dutch city. The world\u2019s first electric driverless shuttle, WEpods can hold six people, and operate on fixed lanes across the city.\n\nSweden is ranked 3rd in KPMG\u2019s 2018 report. The country that gave birth to IKEA, Spotify, Ericsson and Volvo has ramped up its support for autonomous driving over the last few years.\n\nIn 2015 the Swedish government first explored self-driving vehicle testing, concluding that it was possible to carry out trials at all levels of automation on Swedish roads. The Road Transportation Authority can, as of July 2017, authorize permits and supervise trials in accordance with the law.\n\nLast December Volvo launched its Drive Me project, which provided self-driving cars to a number of people in Gothenburg for use in their everyday lives. The project is aimed at collecting user feedback to hone Volvo\u2019s technology.\n\nLast November, Volvo signed a US$300 million deal with Uber to provide the ride-hailing giant with 24,000 flagship self-driving-ready Volvo XC90 SUVs.\n\nGermany is now a hotbed of autonomous vehicles as the German parliament passed a law last May that allows companies to begin testing self-driving cars on public roadways. Drivers are allowed to remove their hands from the wheel and perform simple tasks such as futzing around on smartphones while the car drives itself. However, drivers are required to remain ready to take control in order to handle possible emergencies.\n\nThe new legislation also requires a black box, a counterpart data recorder for autonomous vehicles designed to record system data and actions for review in the case of accidents.\n\nThe UK is another country that has been progressive with autonomous vehicle policy and regulations. While most European countries adhere to Vienna Convention on Road Traffic, the UK is not a signatory and so is believed to have an advantage in adopting legislation to attract autonomous vehicle manufacturers and tech startups. The UK government is aiming for a wide adoption of autonomous vehicles on its roads by 2021.\n\nIn 2013, the Department for Transport allowed semi-autonomous cars to operate on lightly-used rural and suburban roads. Three years later, in her annual address the Queen herself spoke to the importance of enacting \u201cnew laws to make the UK ready to pioneer driverless cars.\u201d\n\nLast year, the UK government passed a bill to draw up liability and insurance policies related to autonomous vehicles.\n\nSingapore could be the first Asian country to widely adopt autonomous driving \u2014 or even the world\u2019s first. The country has the world\u2019s third highest population density, and the government is under pressure to revamp the transportation system.\n\nKPMG\u2019s 2018 report gives Singapore the maximum score on policy and regulations related to autonomous vehicles. In July 2015, the Singapore Land Transport Authority (LTA) authorized 6 km of test routes, and doubled the distance a year later. In 2017 the LTA expanded its AV test bed to neighboring areas such as the National University of Singapore, Singapore Science Parks 1 and 2, and Dover and Buona Vista, adding 55 km to existing autonomous vehicle trial routes.\n\nLast year, the Government of Singapore passed legislation recognizing motor vehicles don\u2019t require a human driver and regulating the operation of such vehicles on public roads. The rules exempt autonomous vehicles and their operators from existing legislation mandating a human driver must be responsible for the safe use of motor vehicles on the road.\n\nSingapore\u2019s supportive environment attracted Boston-based self-driving software company NuTonomy. The company, which was acquired by Delphi for US$450 million, launched a free trial autonomous taxi service in August 2016 and hopes to run an autonomous taxi service in the city-state by the end of Q2 this year.\n\nSouth Korea is possibly the most aggressive country in terms of government investment in autonomous vehicles. The homeland of Samsung, Hyundai and LG allows autonomous vehicles with issued licences to operate on public roads (two sections of expressways and four sections of regular roads, spanning a combined 320 kilometers), and is building an entire artificial town for autonomous vehicle testing.\n\nLast November, the country\u2019s Ministry of Land, Infrastructure and Transport announced the opening of K-City, which is the largest town model ever built for self-driving car experimentation. K-City cost US$11 billion and presents 35 different driving conditions, including toll gates, pedestrian and train crossings, and even potholes and construction sites.\n\nAt the recent Winter Olympics, South Korea flexed its autonomous driving muscles, with Hyundai Motors deploying a self-driving car fleet while KT Corporation provided a self-driving shuttle service.\n\nNew Zealand is an early and keen adopter of autonomous vehicles, second only to Singapore on specific policy and legislation, according to KPMG\u2019s 2018 report.\n\nThe New Zealand government encourages the testing of semi and fully autonomous vehicles and is facilitating the early adoption of autonomous driving technology. The country has no specific legal requirements for cars to have drivers.\n\nNew Zealand recently approved an autonomous flying taxi trial for Kitty Hawk, the Silicon Valley-based startup run by Google founder Larry Page."
    },
    {
        "url": "https://medium.com/syncedreview/microsoft-ai-achieves-milestone-in-machine-translation-ba0cf8f26408",
        "title": "Microsoft AI Achieves Milestone in Machine Translation",
        "text": "Microsoft researchers in the US and Asia sent a shockwave through the AIcommunity today with their paper Achieving Human Parity on Automatic Chinese to English News Translation, which introduces a neural machine translation system they say equals the performance of human experts in Chinese-to-English translation.\n\nAlthough artificial intelligence has outperformed humans in tasks such as image accuracy and speech recognition, many experts doubted machines could do so with language translation. \u201cHitting human parity in a machine translation task is a dream that all of us have had,\u201d said Xuedong Huang, a technical fellow in charge of Microsoft\u2019s speech, natural language and machine translation efforts. \u201cWe just didn\u2019t realize we\u2019d be able to hit it so soon.\u201d\n\nMicrosoft\u2019s system was tested on the benchmark news story dataset newstest2017, which was developed by a group of industry and academic partners and released at last fall\u2019s WMT17 research conference. To measure the translation quality accurately, Microsoft researchers hired bilingual human evaluators to compare Microsoft\u2019s results with two independently produced human reference translations, instead of referring to traditional metrics such as BLEU and TER.\n\n\u201cThe same source sentence can be translated in sometimes substantially different but equally correct ways. This makes reference-based evaluation nearly useless in determining the quality of human translations or near-human-quality machine translations,\u201d says the paper.\n\nMicrosoft\u2019s new machine translation system scored 69.0, indistinguishable from human translation which scored 68.6, according to the paper.\n\nHuang told Synced that machine translation is the key to mastering natural language understanding (NLU), which researchers believe will facilitate the development of artificial general intelligence (AGI) \u2014 the long-range, human-intelligence-level target of contemporary AI technology.\n\n\u201cNLU does not have large datasets. However, machine translation does. We use the deep neural network to learn semantic representations, which can be applied to NLU. As we learn the expression of language, we may have a chance to solve NLU and improve Cognitive Services (a set of Microsoft\u2019s machine learning algorithms),\u201d says Huang.\n\nMicrosoft researchers focused on the Chinese (Mandarin) to English language pair as these are the two most used languages in the world, and sampled texts from the news domain because news stories have a wide content variety. Microsoft researchers caution that their results will not necessarily generalize to other language pairs or domains, even though the techniques used were not specific to languages or domains.\n\nHuang attributes the breakthrough to three factors: increased computation capability provided by Nvidia\u2019s GPUs; improved algorithms and a particularly deep neural network; and an optimized dataset, using engineering methods to wipe out low-quality data, or noise.\n\nTo improve the model\u2019s accuracy and fluency researchers used additional training methods, for example a dual learning technique that learns from both source-to-target and target-to-source translation data by taking a sentence translated from Chinese to English and translating it back to Chinese, then comparing the result to the original sentence.\n\nAnother technique employed was deliberation networks, which train the model to repeatedly translate the same text. Similar to how a human might write multiple drafts, the deep neural network gradually improves and refines its output.\n\nThis new system has not yet been applied to Microsoft\u2019s commercial translation products such as Microsoft Translator, PowerPoint Presentation Translator, or Cognitive Services, but Huwang says his team is working on it.\n\nResearchers still face many challenges in machine translation, particularly with real-time translation and speech-to-speech translation. Microsoft\u2019s milestone positions the company among the global leaders in this busy research field."
    },
    {
        "url": "https://medium.com/syncedreview/fintechs-new-big-player-jd-finance-adds-2b-hits-26-30b-valuation-cb5d702d3304",
        "title": "Fintech\u2019s New Big Player: JD Finance Adds $2B, Hits $26\u201330B Valuation",
        "text": "According to a China Securities Journal report, the China International Capital Corporation (CICC) and China\u2019s largest food processor, manufacturer and trader China National Cereals, Oils and Foodstuffs Corporation (COFCO) are looking to invest US$2 billion (CNY 13 billion) in JD Finance, the fintech spinoff of online retailer JD.com.\n\nThe funding will be applied to acquiring financial licenses through mergers and acquisitions, technology research and development, and further investments. The investing institutes will hold 10 billion JD Finance shares. The transaction is expected to be completed by end of April and will raise JD Finance\u2019s market valuation to a colossal US$26 billion to $30 billion.\n\nEstablished in late 2013, JD Finance has provided fintech services to more than 500,000 corporate clients and 150 million individual consumers. Parent JD.com is Alibaba\u2019s main domestic competitor and the largest Chinese online retailer by revenue, with 335 warehouses covering 2,691 counties and districts in China as of mid-2017.\n\nIn January 2016 the company raised Series A funding of US $1 billion (CNY 6.65 billion) from Sequoia Capital China, China Harvest Investments, and China Taiping Insurance, boosting its valuation to $7 billion and becoming the third-largest fintech company in China, after Alibaba\u2019s Ant Financial and Shanghai-based online finance marketplace LuFax.\n\nJD Finance separated from JD.com in June 2017 and now stands alone as a domestically funded company. This is similar to Alibaba\u2019s splitting AliPay from Alibaba. The partition helps JD Finance dodge JD.com\u2018s overseas investment affiliations and impediments to procuring financial licenses.\n\nLast year JD Finance filed revenues of US$1.6 billion, a 132% increase on 2016. Financial services accounted for 51% of revenues; payment business 15%, and logistics finance and wealth management 15% and 12% respectively.\n\nJD Finance\u2019s Silicon Valley AI Lab Chief Scientist Bo Liefeng is the former principal research scientist at Amazon. He led the AI research team for Amazon Go\u2019s recently opened staff-free store in Seattle. Last year JD Finance launched an international hackathon, attracting talents in the areas of artificial intelligence, big data, and cloud computing."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-discounted-its-top-smart-speaker-to-15-sold-1-million-5fc9243f1af8",
        "title": "Alibaba Discounted its Top Smart Speaker to $15; Sold 1 Million",
        "text": "I purchased a Tmall Genie X1 \u2014 Alibaba\u2019s flagship smart speaker \u2014 at the discounted price of US$15 during China\u2019s November 11 \u201cSingles Day\u201d shopping festival. I was given order number 560,000-ish, and received the product a month later. The speaker is regularly priced at US$79, about the same as its American counterpart Google Mini.\n\nA Chinese smart speaker industry insider who declined to be named told Synced that it would be impossible to even cover hardware costs for $15, but smaller brands had no choice but to go into a price war with Alibaba, cutting their own Singles Day prices to as low as $10. There were strong smart speaker sales across the board, but the big winner was Genie X1, spurred by an incredible online marketing campaign and supported by superior sales channels.\n\nThe Smart Speaker story begins back in 2011, when Amazon began working on its voice assistant Alexa. Amazon\u2019s Echo smart speaker, powered by Alexa, went on sale in 2014, and although sales were initially sluggish, other tech companies picked up on smart speakers very quickly.\n\nSensing the trend, in April 2015 Alibaba devoted hundreds of engineers to smart speaker software prototypes, teaming up with hardware partners LED manufacturer Lipu Lightingand speaker manufacturer Edifier; and launching its smart voice assistants XiaoFei and MA1/3/5 within the span of three months.\n\nBoth were however poorly received in the market. Team leader Xue Qian says she realized in retrospect that although Alibaba\u2019s first attempts had sound outsourced hardware, the Android-based software system wasn\u2019t the right solution.\n\nIn 2016 Alibaba formed its AI Lab and put Qian in charge. The lab was backed by iDST and ET City Brain and attracted talents such as former Nanyang Technological University Professor Gang Wang; Principal Researcher and Head of the Big Data Mining Group at Microsoft Research Zaiqing Nie, and Project Tango Lead of Google Mingyang Li.\n\nThe lab invested much time and energy in streamlining Smart Speaker hardware and software design, eventually giving birth to AliGenie, the first generation Human-Computer Interaction (HCI) system that runs on the cloud and understands user commands in Mandarin.\n\nAliGenie is the brains behind the Tmall Genie X1, which was launched in July 2017.\n\nTmall Genie X1 employs 5-meter far-field voice recognition technology supported by cutting edge acoustic beamforming technology with six microphones, which enhances user voice and cancels background noise. It uses natural language processing, knowledge graphs, machine learning, and AR to perform functions such as smart home control, voice shopping, mobile phone recharge, takeout orders, and music play. The voice recognition neural network model is trained with massive labeled voice and language data that covers most daily dialogues.\n\nAI Lab\u2019s team monitors and collects all online reviews. \u201cEven bad reviews are good to us because they are user feedback. It\u2019s rare that our competitors get so much feedback, and smaller brands are definitely envious,\u201d explains Qian.\n\nCriticism of Genie X1 includes poor interactive experience and performance with online shopping tasks, limited connectivity as a smart home controller, and performing worse than conventional remote controls.\n\nSmart speakers are being developed as the home\u2019s future portal for connected devices, but ironically competitors are blocking each other\u2019s access in this space with proprietary technology. For example, customers who purchase a Genie X1 will not be able to use Xiaomi\u2019s smart lightbulbs, and will have to buy a designated Midea air conditioner if they want to control it via the Genie X1.\n\nFilling more than one million orders tested Qian\u2019s nerves as her team struggled to meet demand, \u201cwe out-purchased all available amplifiers and chips on the market, with factories running 24/7, and still lagged on delivery date. The capacity to manufacture smart speakers at large quantities is an overlooked problem.\u201d\n\nIn the burgeoning smart speaker market fast iterations are essential to survival. \u201cWe are three years behind Amazon\u2019s Echo,\u201d says Qian, \u201cbut it took us just four months from launching Tmall Genie X1 to reach one million unit sales.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/baidu-apollo-releases-massive-self-driving-dataset-teams-up-with-berkeley-deepdrive-5e785ab4053b",
        "title": "Baidu Apollo Releases Massive Self-driving Dataset; Teams Up With Berkeley DeepDrive",
        "text": "Baidu this Thursday announced the release of ApolloScape, billed as the world\u2019s largest open-source dataset for autonomous driving technology.\n\nApolloScape was released under Baidu\u2019s autonomous driving platform Apollo, which Baidu hopes will become \u201cthe Android of the auto industry.\u201d Apollo gives developers access to a complete set of service solutions and open-source codes and can enable for example a software engineer to convert a Lincoln MKZ into a self-driving vehicle in about 48 hours. ApolloScape\u2019s open sourced data now provides developers a base for building self-driving vehicles.\n\nThe data volume of ApolloScape is 10 times greater than any other open-source autonomous driving dataset, including Kitti and CityScapes. This data can be utilized for perception, simulation scenes, road networks etc., as well as enabling autonomous driving vehicles to be trained in more complex environments, weather and traffic conditions. ApolloScape also defines 26 different semantic items \u2014 eg. cars, bicycles, pedestrians, buildings, streetlights, etc. \u2014 with pixel-by-pixel semantic segmentation technique.\n\nThe ApolloScape dataset will save researchers and developers a huge amount of time on real-world sensor data collection.\n\nAccording to a Rand Corporation report, accumulating sufficient real road testing data to conclude a 20 percent advantage for autonomous vehicles over human drivers would require a fleet of 100 vehicles driving nonstop for 500 years.\n\nBeyond data, ApolloScape will also facilitate advanced research on cutting-edge simulation technology aiming to create a simulation platform that aligns with real-world experience.\n\nApollo also announced it has joined the Berkeley DeepDrive (BDD) Industry Consortium, a top-tier research alliance investigating state-of-the-art technologies in computer vision and machine learning for automotive applications.\n\nHoused at the University of California, Berkeley and led by Professor Trevor Darrell, Faculty Director of PATH, the BDD consortium has attracted big tech names as partners, including Ford, Nvidia, Qualcomm, and GM. BDD\u2019s main research focus is on deep reinforcement learning, cross-modal transfer learning, and clockwork FCNs for fast video processing.\n\nHaifeng Wang, Baidu Vice president and Head of Baidu Research Institute, told Synced, \u201cThe partnership will incorporate Apollo\u2019s industrial resources and Berkeley\u2019s top academic team to ramp up the innovation of theoretical research, applied technology, and commercial applications.\u201d\n\nApollo Open Platform and BDD will jointly conduct a Workshop on Autonomous Driving at CVPR 2018 (IEEE International Conference on Computer Vision and Pattern Recognition) this June in Salt Lake City where they will organize task competitions based on ApolloScape."
    },
    {
        "url": "https://medium.com/syncedreview/how-ai-can-help-the-oil-industry-b853dda86be6",
        "title": "How AI Can Help The Oil Industry \u2013 SyncedReview \u2013",
        "text": "Oil, also known as a \u201cblack gold,\u201d has been one of the most valuable and in-demand resources in the world since ancient times. It\u2019s easy to see why countries have historically sought to secure supplies, and why oil has been one of the major factors in military conflicts. Oil provides nearly half the world\u2019s energy, powers the majority of vehicles, and is a base ingredient for many industrial chemicals. Oil is the lifeblood of industrialized countries: most manufacturing, technologies, plastics, and fertilizers would not be possible without it.\n\nThe price of oil has shown sluggish growth since its 2014 collapse. Although the adoption of new technologies such as directional drilling and hydraulic fracturing have increased yields, the industry continues to seek solutions to boost business, and many see AI as the answer.\n\nTo apply AI technology to the oil and gas industry, oil companies and startups generally first establish either a research group or a research center for the purpose. AI for oil and gas is a huge potential market, expected to reach US$2.85 billion by 2022. Presently, North America is the largest market using AI in oil and gas, followed by Europe and Asia Pacific.\n\nOil and gas is a huge industry which includes upstream, midstream and downstream components. There are different ways to apply AI technologies to these different sectors. The common factor is that AI can help oil and gas companies lower costs and make more accurate decisions.\n\nThe oil and gas industry is adopting new technologies in its quest to be more efficient and profitable with low margins, and AI and cognitive computing are a perfect fit. \u201cThe next generation of competitive advantage in the energy marketplace will go to forward-thinking players who invest a lot on digital IoT and artificial intelligence capabilities like Cerebra from Flutura,\u201d says Archie W. Dunham, JAG chairman emeritus and former independent non-executive Chairman of Chesapeake Energy in Oklahoma City and retired ConocoPhillips Chairman.\n\nA decade ago, the most advanced AI could only advise companies in retrospect for example that they should have taken a specific preventative action in order to prevent failure. Nowadays, with the help of advanced sensors and software powered by AI, companies can digest a large amount of data and output real-time responses on the best course of action. Within a few years, the Industrial Internet of Things (IoT) will comprise more than a trillion sensors that generate and share data, and these innovations will dramatically change the way oil and gas companies operate.\n\nThe use cases below illustrate how AI is already helping the oil and gas industry.\n\nThe upstream sector is usually known as the Exploration & Production (E&P) sector, and includes companies that locate and extract crude oil or natural gas. Most drilling and production wells are located in remote areas, and sending workers there increases costs. Onsite operating costs can be reduced by using sensors and the Internet of Things (IoT) powered by AI to handle data collection and system control in real time.\n\nThe midstream sector processes, stores, and transports crude oil, natural gas, and liquefied natural gas. This sector is the important link between remote oil and gas producing areas and population centers where most consumers are located.\n\nThe downstream sector includes oil refineries, petrochemical plants, petroleum product distributors, and natural gas distribution companies. This sector produces countless products including gasoline, diesel, jet fuel, lubricants, plastics, fertilizers, natural gas, and propane.\n\nThe digital revolution in oil and gas industry is taking off. Improved safety and productivity can be achieved by automating routine manual activities, which can also reduce the risk to human workers. AI also has the potential to free scientists and engineers from repetitive and time-consuming tasks, and can assist in decision making. Moreover, AI systems can automate and optimize data-rich processes to mitigate business risks.\n\nAI\u2019s power and potential to increase efficiency and cost-effectiveness is making the technology increasingly attractive and speeding its adoption across all sectors of the oil and gas industry."
    },
    {
        "url": "https://medium.com/syncedreview/samsung-rides-the-ai-wave-3fe95796c4a2",
        "title": "Samsung Rides the AI Wave \u2013 SyncedReview \u2013",
        "text": "Compared to other tech giants, Samsung is a relative late-bloomer in AI. But recent moves in acquisitions and R&D signal the company is jumping onto the intelligence wave, and doing so with a strong focus on virtual voice assistants.\n\nSamsung introduced its \u201cS Voice\u201d voice interface back in 2012 in response to the rising popularity of Apple\u2019s Siri. The release however received lukewarm reviews, and S Voice stalled in the marketplace. By late 2016 Samsung upped its game by acquiring Viv Labs, an AI assistant startup founded by former Siri creators Dag Kittlaus and Adam Cheyer.\n\nIn 2017 Samsung introduced the intelligent voice assistant Bixby for its smartphones, wireless earphones, TVs, fridges, and so on. The South Korean manufacturer is expected to release Bixby 2.0 later this year alongside a range of smart speakers and the Samsung Galaxy Note 9. The upgraded assistant will feature improvements achieved with help from Viv Labs and Samsung\u2019s other recent acquisitions including Reactor Labs, Expect Labs and Vicarious.\n\nLast week Samsung Research America acquired Egyptian tech company Kngine for an undisclosed sum. The company provides mobile search solutions using deep learning and knowledge-based AI algorithms. Its engine crawls the world wide web, enterprise documents, books, FAQs and even customer service logs to gather information which it can use to help answer queries. It ranks possible responses, presenting users with the most plausible answer for a given question.\n\nSamsung believes Kngine\u2019s functionality will strengthen Bixby and set it apart from rivals Siri, Alexa and Google Assistant. It\u2019s still early to judge Bixby\u2019s full performance, but the product has already garnered positive feedback online.\n\nSamsung\u2019s C-Lab (Creative Lab) is located in the sprawling Samsung Digital City in Suwon, South Korea. C-Lab is currently curating three new AI products that will be showcased at this year\u2019s SXSW conference, which runs March 9\u201318 in Austin, Texas.\n\nFirst up is Aurora, a visualized 3D character assistant that can recognize a user\u2019s gestures and location using a smartphone camera and display visual information on the device. Aurora can also assume various emotional states when interacting with users, for example communicating with the chill style of a best buddy. Aurora could certainly be fun for those who want more character in their virtual assistant.\n\nToonsquare, meanwhile, adds to Samsung\u2019s AI entertainment kit with its ability to convert phrases into cartoon images. The app\u2019s text analyses AI takes what the user wants to communicate, then seasons the phrase with appropriate fun images. The app offers customization options for font, background and speech bubble.\n\nThe third release is an automated ad trading platform for video games called Gadget, which will help marketers put their ads on native game objects such as billboards. Users can choose what types of ad they\u2019ll see. The seamless real-time ads will appeal to those who hate the interruption of pop-ups ads.\n\nC-Lab was founded in 2012 as Samsung\u2019s in-house idea incubator, and has since supported some 100 innovative projects. C-Lab and seven C-lab spinoff startups made a splash at this year\u2019s CES with gadgets such as portable directional speakers and visual aid eyeglasses.\n\nLast year, Samsung established an AI lab in Montreal, Canada in collaboration with AI Yoshua Bengio from University of Montreal. In partnership with China\u2019s Tencent, Samsung is currently developing a line of AI speakers which are expected to be released this year. The company says it intends to build AI into all its home appliances by 2020.\n\nUnlike rivals Google, Amazon, and other tech giants, Samsung has not avidly published AI papers. Instead, the company has so far progressed relatively cautiously in the space. But now it\u2019s up and riding the AI wave just like the rest, with plenty of money to do startup acquisitions; a huge, robust consumer electronics market; and legions of Samsung fans eagerly waiting for the next product release."
    },
    {
        "url": "https://medium.com/syncedreview/paige-ai-combats-cancer-with-ai-and-computational-pathology-5bdd8c6a1421",
        "title": "Paige.AI Combats Cancer With AI and Computational Pathology",
        "text": "For decades pathologists have rendered their cancer diagnoses by performing a biopsy and examining a patient\u2019s tumour sample under a microscope. Now, an increasing number of top-tier pathologists are adopting artificial intelligence techniques to improve their cancer diagnoses.\n\nPaige.AI is a New York-based startup that fights cancer with AI. Launched last month, the company has an exclusive data license with the Memorial Sloan Kettering Cancer Center (MSK) \u2014 the largest cancer research institute in the US \u2014 which has a dataset of 25 million pathology cancer images (\u201cslides\u201d).\n\nTypically, a pathologist must invest a significant amount of time examining a patient\u2019s numerous tumour slides, each of which could be 10+ gigapixels when digitized at 40X magnification. Even the best pathologists can make a misdiagnosis, and it is not uncommon for professionals to disagree on diagnoses.\n\nThis is why computational pathology for cancer research has gained traction over the last ten years or so. The technology incorporates massive amounts of data, including pathology, radiology, clinical, molecular and lab tests; a computational model based on machine learning algorithms; and a visualized presentation interface that is understandable for both pathologists and patients.\n\n\u201cComputational pathology solutions will help streamline workflows in the future by screening situations that do not require a pathologist review,\u201d said Jeroen van der Laak, Associate Professor at Radboud University Medical Center, in an interview with Philips Healthcare.\n\nDr. Thomas Fuchs is the Director of Computational Pathology at MSK and an early pioneer in the theoretical study of computational pathology, He has many years of experience in the development and application of advanced machine learning and computer vision techniques for tackling large-scale computational pathology challenges.\n\nLast month Dr. Fuchs assumed an additional role as Founder and CEO of Paige.AI. He told Synced he believed the time was right to build Paige.AI because the requirements are all in place: scanners can deliver digital images with quality comparable to what pathologists see under the microscope; cancer centres scan some 40,000 pathology slides each month; and deep learning algorithms are well-suited for large-scale data.\n\nPaige.AI\u2019s technology is built on machine learning algorithms trained at petabyte-scale from tens of thousands of digital slides. Three models are utilized to solve different problems: convolutional neural networks for tasks such as image classification and segmentation, recurrent neural networks for information extraction from pathology reports, and generative adversarial networks to learn the underlying distribution of the unlabeled image data and to embed histology images in lower dimensional feature spaces.\n\nTech giants believe their frontier machine learning algorithms have huge a potential to revamp conventional diagnostic methodologies in the healthcare market, increasing accuracy and reducing costs. IBM has been using slides to train deep neural networks to detect tumours since 2016.\n\nGoogle, meanwhile, has released research on how deep learning can be applied to computational pathology by creating an automated detection algorithm to improve pathologists\u2019 workflow. Google successfully produced a tumor probability prediction heat map algorithm whose localization score (FROC) reached 89 percent, significantly outperforming pathologists\u2019 average score of 73 percent.\n\n\u201cCompanies like Microsoft and IBM are doing pathology, and in general, it is good for the whole field,\u201d says Dr. Fuchs, who also warned that tech companies unfamiliar with the healthcare sector might have a hard time. \u201cYou have to really understand the variety of workflows and the community, and where and how AI can help. Besides, as far as I know, all previous papers published were based on a very tiny data set. Increasing the dataset from a few hundred images to hundreds of thousands of images can make a huge difference.\u201d\n\nIn the short term, Paige.AI will provide pathologists with it\u2019s \u201cAI Module\u201d application suite, equipped with a dedicated physical slide viewer that can integrate with any microscope. The AI module targets prostate, breast and lung cancers and can perform tasks such as cancer detection, quantification of tumour percentages, and survival analysis.\n\nPaige.AI has already rolled out its product institution-wide at MSK, and aims to deliver disease-specific modules to pathologists later this year.\n\nPaige.AI\u2019s forte in algorithms and access to large-scale data attracted interest from Breyer Capital, which led a US$25 million Series A Funding Round for the company. Founder and CEO of Breyer Capital Jim Breyer, a venture capitalist renowned for his smart investments \u2014 most notably Facebook \u2014 wrote in a Medium blog, \u201cPaige.AI is poised to become a powerhouse in computational pathology and an undisputed leader among thousands of healthcare AI competitors.\u201d\n\nPaige.AI certainly does not intend to limit its output to slide viewers \u2014 the company aspires to reshape the entire diagnosis and treatment paradigm. \u201cWith Paige.AI, we can, for example, based on hundreds of thousands of slides, come up with a better grade because you can actually correlate so many patients with the outcomes. Then we compute that correlation, and of course, change how you grade patients and how and which medications are prescribed,\u201d says Dr. Fuchs.\n\nAlthough the road ahead for Paige.AI is bound to be challenging, especially as the company is still at a very early stage in its development, Dr. Fuchs is determined to raise his company to the forefront in AI implementation in healthcare, and their research is likely to spark further technological breakthroughs for computational pathology."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-launches-institute-of-quantum-computing-899454cbe1c5",
        "title": "Baidu Launches Its Institute of Quantum Computing \u2013 SyncedReview \u2013",
        "text": "Baidu today announced the formation of its Institute of Quantum Computing. The Chinese search engine giant says this will be a first-class institute that will conduct applied research and development on software and information technologies and incorporate with existing businesses.\n\nThe institute will be led by Dr. Runyao Duan, a Professor and the Director of Quantum Computation Laboratory at the University of Technology Sydney and an Australian Research Council Future Fellow. Dr. Duan will report directly to Baidu President Yaqin Zhang.\n\nDr Duan is mainly focused on quantum information theory, quantum state/operation discrimination, quantum zero-error information theory, and measurement-based quantum computation. In 2016 he successfully gave a complete interpretation of the famous Lov\u00e1sz number with quantum information theory, thus solving the open issue that has challenged the field of information theory and graph theory since 1979.\n\nQuantum computing is believed to be the next-generation computing technology. Its promise of greatly improved speed compared to binary-based classical computing and its potential for supercharging artificial intelligence have made quantum computing something of a holy grail for global tech giants. To date, quantum computing has been applied mainly to large-scale data processing and computing problems, as well as network security services based on quantum encryption.\n\nBaidu is under pressure to accelerate its development of quantum computing. Last month, rival Alibaba launched an 11-qubit quantum computer, which is now available to the public on the Quantum Computing Cloud Platform. Alibaba was the second company to provide public cloud computing services with a processing power of 10+ qubits, following IBM\u2019s November release of 20-qubit quantum computers through its cloud service.\n\nGoogle also made a move towards quantum computing this week, introducing a preview of its newest 72-qubit quantum computer Bristlecone. Although Bristlecone is still in a very early stage, Google believes it will one day outperform classical computing on well-defined computer science problems, an achievement referred to as \u201cquantum supremacy.\u201d\n\nThe global race toward quantum computing is heating up. A relative newcomer in the field, Baidu is confident it can catch up with the competition, and founding a dedicated quantum computing institute with top experts is a huge step toward that goal."
    },
    {
        "url": "https://medium.com/syncedreview/fintech-surge-s-p-global-acquires-kensho-for-record-550-million-ant-financial-launches-pre-ipo-256f72b3627f",
        "title": "Fintech Surge: S&P Global Acquires Kensho for Record $550 Million; Ant Financial Launches Pre-IPO\u2026",
        "text": "We\u2019ve become accustomed to tech giants shelling out millions and millions of dollars to snatch up promising AI startups. But yesterday \u2014 when the largest such acquisition ever was announced \u2014 the buyers were not from Silicon Valley. They were from Wall Street.\n\nS&P Global has signed an agreement to acquire Kensho, a Cambridge, MA-based AI company that uses data analytics and machine learning algorithms to find correlations and arbitrage opportunities in large-scale financial datasets. The company\u2019s holy grail is to make \u201ccomplex financial analysis as easy as a search on Google.\u201d\n\nThe Wall Street traded financial services corporation paid a staggering US$550 million for Kensho, making this the largest AI acquisition ever, topping Google\u2019s purchase of DeepMind, the London startup that produced the epoch-making Go computer AlphaGo.\n\nS&P Global values Kensho\u2019s forte in frontier AI technology and its close relationships with premium financial clients such as Citibank, Bank of America, Goldman Sachs and JP Morgan. Kensho also works with the US Government on national security issues. S&P Global was actually a Kensho client before the acquisition.\n\nThe acquisition further builds S&P Global\u2019s AI profile. Last year, S&P Global led a US$50 million Series B fundraising round for Kensho, and began a collaboration with it to develop financial products. Earlier this year, the company acquired another Cambridge-based AI company, Panjiva, which uses machine learning techniques in global supply chain management.\n\n\u201cKensho\u2019s capabilities are critical for S&P Global to be at the forefront of the technology transformation taking place within the financial markets, and will accelerate multiple commercial and efficiency opportunities,\u201d says S&P Global Chief Financial Officer Ewout Steenbergen. \u201cS&P Global\u2019s strategy is focused on strengthening our technology capabilities across the enterprise.\u201d\n\nAccording to an S&P Global press release, Kensho will continue to operate independently in Cambridge. Daniel Nadler, a Harvard graduate who founded Kensho in 2013, will lead a team of 120 employees, and report to Steenbergen.\n\nOver the last few years Wall Street has discovered how well AI can perform in managing financial portfolios, automated trading, online fraud detection detect, and predicting lending and insurance trends. Assets in quant funds, which employ AI for quantitative analysis, have grown to US$940 billion, a 86 percent increase since 2010, according to Bloomberg.\n\nOne can now hardly find a relevant bank or investment institution that is not using AI. In 2018, three-quarters of banks financial services companies will be either already employing or introducing AI technology in their operations, reports Greenwich Associates, a top market intelligence advisor to the \ufb01nancial services industry.\n\nChina\u2019s financial industry has also been aggressively embracing emerging technologies and AI for years, gestating the world\u2019s most valuable financial giant Ant Financial, an affiliate of China\u2019s Alibaba Group.\n\nAnt Financial officially launched its last round of financing yesterday in advance of an IPO anticipated this year or next. The Financial Times reports that the company is seeking US$5 billion, which would bring its value to US$100 billion.\n\nJust a month ago, Ant Financial\u2019s parent company Alibaba announced it would acquire a 33 percent interest in Ant Financial in exchange for Alibaba IP rights.\n\nTo date a wide range of digital financial service providers have spun out of Ant Financial: Alipay, Ant Fortune, Zhima Credit, MYbank, Yu\u2019e Bao, Ant Credit Pay (\u201cHuabei\u201d), Ant Cash Now (\u201cJiebei\u201d) and more; with the network reaching over 520 million users in China and the world.\n\nAnt Financial bills itself as an AI innovator, implementing machine learning in almost every corner of its ecosystem. The company\u2019s flagship financial intelligence platform Antzero develops reinforcement learning, deep learning and unsupervised learning solutions to revamp banking, insurance, and investment companies. Antzero\u2019s AI-driven car damage assessment system for example can detect vehicle damage and provide an estimated repair cost, while its struc2vec framework-based AI model delivers outstanding performance in malicious account detection.\n\nToday\u2019s announcements illustrate how both established and emerging fintech companies are finding and scaling solutions and growing their businesses by investing in AI. It\u2019s a trend that shows no signs of slowing down."
    },
    {
        "url": "https://medium.com/syncedreview/strengthening-ai-r-d-among-chinas-2018-innovation-goals-dee468e95abb",
        "title": "Strengthening AI R&D Among China\u2019s 2018 Innovation Goals",
        "text": "In his address to the 13th National People\u2019s Congress (NPC) in Beijing on March 5, Chinese Premier Keqiang Li delivered this year\u2019s official government work report, identifying \u201creinforcing the new generation of artificial intelligence research and development\u201d among the state\u2019s innovation focuses for 2018.\n\n \n\nPremier Li provided a three-part overview of government work, including a review of the past five years, policy direction for this year\u2019s socioeconomic development, and suggestions for upcoming government work in 2018. The report\u2019s keywords include \u201cartificial intelligence\u201d along with \u201cagricultural supply-side structural reform,\u201d \u201cshared economy,\u201d \u201cdigital healthcare,\u201d and \u201cfintech.\u201d\n\nPremier Li noted that in the past five years China\u2019s GDP has increased from USD 8.64 trillion (CNY 54 trillion) to USD 13.2 trillion (CNY 82.7 trillion), with an average annual increase of 7.1%. National GDP now accounts for 15% of the global economy and more than 30% of global economic growth. There have also been major changes in economic structure, with high-tech manufacturing growing at an average annual rate of 11.7%.\n\n \n\nPremier Li said that in 2018 China will expedite upgrading \u201cfrom older to newer generation\u201d technologies, further deploy \u201cinternet+\u201d technologies, promote the widespread application of big data, cloud computing, and IoT.\u201d China will also continue its \u201cMade in China 2025\u201d initiative in order to \u201cstrengthen industrial infrastructure, smart and green manufacturing; and advance the speedy development of manufacturing.\u201d\n\n \n\nThree days before Premier Li\u2019s address, elected CPPCC members who are also AI industry leaders held a press conference.\n\n \n\nBaidu CEO Robin Li pledged, \u201cAI will drive China\u2019s economic development for the next 20\u201350 years.\u201d He proposed encouraging enterprises to opensource their AI platforms to strengthen the application of new technologies, and taking the lead on self-driving car policymaking. He also recommended the government introduce additional AI policy incentives\n\n \n\nSpeaking at the same press conference, Tencent CEO Pony Ma said \u201csince AlphaGo, people have come to realize the power of deep learning. Many deep learning applications are perfectly feasible now. Tencent\u2019s backends are supported by AI technologies like targeted advertising and personalized content recommendation, only users are not really aware of them.\u201d He added that \u201cartificial general intelligence may be far away, but there are many opportunities in vertical applications.\u201d \n\n \n\nLenovo Chairman and CEO Yang Yuanqing introduced a proposal to develop artificial intelligence for industrial applications, comparing AI to water droplets permeating all industries, and pointing out the importance of setting early benchmarks and building platforms.\n\nThe term \u201cAI\u201d first appeared in Chinese government statements in the July 2015 State Council document \u201cGuiding Opinions on Promoting \u2018Internet+\u2019 Initiatives.\u201d In March 2016 \u201cartificial intelligence\u201d appeared in the outline of the \u201c13th Five-Year Plan for Economic and Social Development\u201d.\n\nIn April 2016 Ministry of Industry and Information Technology, National Development and Reform Commission, and Ministry of Finance jointly issued the \u201cRobotic Industry Development Plan (2016\u20132020).\u201d The following month the National Development and Reform Commission, the Ministry of Science and Technology, the Ministry of Industry and Information Technology, and the Ministries and Commissions of the Central Network Information Office jointly released the \u201cInternet+ Artificial Intelligence Three Year Implementation Plan.\u201d \n\n \n\n\u201cArtificial intelligence\u201d also appeared in last year\u2019s official government work report, and last July the State Council formally issued the \u201cNew Generation of Artificial Intelligence Development Plan,\u201d lifting AI to the level of national strategy. \n\n \n\nEarlier this month the Chinese government released its \u201cArtificial Intelligence Standardization White Paper\u201d. The 98-page document was edited by the China Electronics Standardization Institute under the guidance of the National Standardization Management Committee Second Ministry of Industry. \n\n \n\nOn the same day as Premier Li\u2019s address, the University of Nanjing announced the formation of its new Artificial Intelligence Institute. This becomes the third such institute in the country, joining the Chinese Academy of Sciences and Xidian University."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-drops-lawsuit-against-jingchi-as-the-two-team-up-on-self-driving-tech-db589810af24",
        "title": "Baidu Drops Lawsuit Against JingChi as the Two Team Up on Self-Driving Tech",
        "text": "The Baidu vs JingChi dispute \u2014 China\u2019s first ever lawsuit in the field of autonomous driving technology \u2014 has surprisingly ended up spawning a partnership between the two.\n\nBaidu announced today that self-driving startup JingChi has joined its open-source autonomous driving platform Apollo as a cooperative partner. JingChi\u2019s new CEO Tony Han said in a statement, \u201cJingChi is pleased to join Apollo, and is willing to grow fast with Apollo\u2019s help.\u201d\n\nJingChi\u2019s technology is appealing to Baidu. Founded last April, JingChi was one of 2017\u2019s fastest-growing self-driving startups, successfully completing its first open road autonomous driving testing in only 81 days.\n\nBaidu\u2019s Apollo is China\u2019s state-of-the-art autonomous driving platform. It will provide JingChi with open-source data, codes and tools. Launched last July, Apollo has already attracted over 80 partners, from car makers to self-driving technology providers.\n\nThe Baidu \u2014 JingChi partnership emerges from a unique situation to say the least. Last December, Baidu filed a lawsuit in Beijing IP Court accusing JingChi and its Founder and then-CEO Jin Wang \u2014 who was a Baidu Senior VP before he started the company \u2014 of infringement on its commercial secrets, not returning devices that contained classified information, and poaching technical personnel.\n\nJin Wang initially dismissed the accusation, saying \u201cBaidu\u2019s prosecution is completely unfounded and JingChi lawyers will respond factually and legally.\u201d However, two months later, JingChi released an official statement saying that Wang had left the company due to family issues.\n\nWang\u2019s departure is widely believed to be the event that defused the dispute between Baidu and JingChi, and accelerated the settlement. Chinese media is reporting that although Baidu has dropped its lawsuit against JingChi, it is still suing Wang.\n\n\u201cWang will not be directly or indirectly involved in or interfere with JingChi in any business dealings. JingChi has been fully prepared for the change, and will not be affected,\u201d said Han.\n\nThe landmark settlement between Baidu and JingChi has tangible effects. JingChi is spared from the legal dispute, and can turn its full attention back to business. The company\u2019s rapid growth might however be affected. An investor involved in JingChi fundraising said the recent dramas leave him needing more information and feedback before he can make an assessment on whether JingChi remains suitable for investment.\n\nBaidu meanwhile has consolidated its leading position in China\u2019s self-driving competition and asserted control over its IP. Over the last few years, Baidu has nurtured many AI talents who later left the company, founding startups such as Horizon.ai, a billion-dollar chip company, and Pony.ai, another rising self-driving startup which recently raised US$112 million.\n\nWith tech foes now becoming friends, China\u2019s self-driving development looks like it will emerge the biggest winner."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-feb-w-4-mar-w-1-4386d5a8d935",
        "title": "AI Biweekly: 10 Bits from Feb W 4 \u2014 Mar W 1 \u2013 SyncedReview \u2013",
        "text": "Capillary Technologies is a cloud-based SaaS company serving over 300 large enterprises and 500 small or medium businesses in over 30 countries. It improves retailers\u2019 engagement with customers through analytics. The company announced it has raised US$20 million in funding to improve its products with AI, and plans to expand to consumer goods and enter the Southeast Asia market.\n\nFacebook is expanding its practice of flagging suicidal users to support more languages and Instagram. Facebook says the number of users who received suicide prevention support content from its compassion team has doubled and algorithms are flagging 20 times more posts related to suicide since the team was formed in 2015.\n\nFebruary 23rd \u2014 Microsoft and Xiaomi Collaborate in Cloud Computing, AI, and Hardware\n\nMicrosoft and Chinese smartphone manufacturer Xiaomi launch a collaboration in cloud computing, AI, and hardware through a new Strategic Framework Memorandum of Understanding (MoU). This is a win-win situation for both tech giants because the partnership will provide shared capabilities and channels.\n\nGoogle announces two new features for Google Assistant: routines and location-based reminders. The Google Home Smart Speaker will now be able to provide location-based reminders at any time via smartphone synchronization. The features also support combining commands to enable execution of multiple tasks with one command.\n\nThe number of languages Google Assistant can speak will jump from 8 to more than 30 this year, with the system eventually supporting more languages than Siri or Alexa. This should greatly benefit multilingual user environments and international customers.\n\nFebruary 25th \u2014 Amazon Designs Its Own AI Chips for Alexa\n\nAccording to a report from The Information, Amazon is designing its own chip for its virtual assistant Alexa, following competitors Google and Apple into the silicon business. In 2015 Amazon acquired chip maker Annapurna Labs to help build capabilities in this area.\n\nCalifornia is taking another major step forward in boosting autonomous driving technology with new permits that will allow manufacturers to test fully driverless vehicles on its roads. The California DMV can now issue three different permits based on specified requirements: testing with a safety driver, driverless testing, and deployment.\n\nIntel is collaborating with leading manufacturer of embedded computing devices AAEON Technologies to provide two possible production designs for developers. One is a mini-PCle module that features an Intel Movidius Myriad 2 VPU; the second is a manufacturing service for companies that require prototype customizations.\n\nIn its 2018 fiscal blueprint, the Canadian government announces a CDN$3.2 billion investment over five years to fund scientific research to stimulate economic growth. This represents the single largest investment in investigator-led fundamental research in Canadian history.\n\nMarch 1st \u2014 China Issues First Licenses to Test Driverless Cars in Shanghai\n\nChina issues licenses to Shanghai-based SAIC and startup automaker NIO, allowing them to test their autonomous driving vehicles on a 5.6 km stretch of public road in Shanghai\u2019s Jiading District. Shanghai plans to open more roads for smart car testing in the future."
    },
    {
        "url": "https://medium.com/syncedreview/ai-in-cyber-threat-detection-5b6fcebfc2e8",
        "title": "AI in Cyber Threat Detection \u2013 SyncedReview \u2013",
        "text": "Cyber Threats have become a critical issue in today\u2019s world. Worldwide spending on information security climbed to over US$90 billion in 2017, a roughly 15% increase compared to 2016. Research and development of cyber threat detection and response capabilities will continue to grow this year.\n\nCyber attacks can be categorized according to what the attacker is targeting.\n\nPhishing: The attempt to fraudulently obtain sensitive information such as usernames, passwords, or credit card details by posing as a trustworthy entity in an electronic communication.\n\nRansomware: Malicious software that blocks access to the victim\u2019s data and threatens to publish or delete it unless a ransom is paid. The US Computer Emergency Readiness Team (US-CERT) reported an average of 4,000 daily ransomware attacks worldwide in 2016, a fourfold increase over 2015. More than one-quarter of these attacks were in the US.\n\nCross-site scripting: A web application vulnerability enables attackers to inject client-side scripts into web pages viewed by other users.\n\nSQL injection: An injection attack wherein an attacker can execute malicious SQL statements (also commonly referred to as a malicious payload) that control a web application\u2019s database server.\n\nBotnet: A network of private computers infected with malicious software and controlled as a group without the owners\u2019 knowledge, e.g., to send spam messages.\n\nDDoS: A type of DOS attack where multiple compromised systems, which are often infected with a Trojan, are used to target a single system causing a Denial of Service (DoS) attack.\n\nBy harnessing the power and potential of machine learning and deep learning technologies, cybercriminals are innovating faster than everybody could imagine. Although emerging technology-based attacks are still rare, some new types of threats can be identified.\n\nThe 2017 report \u201cGenerating Adversarial Malware Examples for Black-Box Attacks Based on GAN\u201d introduced the threat of machine learning being used for malware creation. The paper proposed a generative adversarial network based algorithm that could easily bypass a detection system. Last year, security company Endgame released research in which they were able to create a new type of malware by modifying Elon Musk\u2019s OpenAI framework. This malware could also easily fool security engines\u2019 defences.\n\nBy using a combination of machine learning, speech recognition and natural language processing (NLP), the quality of phishing emails or other smart attack techniques could become much more humanlike and effective. A paper released at security conference Black Hat USA 2017, \u201cWeaponizing data science for social engineering: Automated E2E spear phishing on Twitter,\u201d introduced a neural network framework that was able to fool 45% of users (in a random test of 90 users) with its targeted phishing tweets.\n\nSecurity Automation, which identifies potential cyber-security incidents by monitoring abnormal data use, is key in defending against cyber threats. AI and machine learning are powerful tools in the field of security automation, and can evolve the monitoring, prioritization and alert processes to the next generation to cut human labour costs and speed up threat management cycle time. Humans remain in the loop only for the purpose of identifying false positives.\n\nWith emerging technologies becoming more and more involved in cyber attacks, simply gathering data or creating digital signatures is no longer sufficient for fast threat detection. Introducing AI solutions allows the system to monitor a wider number of factors and better identify patterns of abnormal activity. By leveraging this data, AI and Machine learning can be trained to track information and deliver predictive analysis.\n\nMIT\u2019s AI2 System is able to work through raw data and leverage unsupervised machine-learning algorithms to detect abnormal information security activities. The system summarizes patterns and provides detailed information to security operators for further decision making. The decision records act as auto feedback to the core machine-learning model to improve its algorithm for future analysis. The AI2 system\u2019s AI-human fusion achieved an impressive cyber attack identification rate of nearly 86%.\n\nSentinel \u2014 Home Security\n\nThe home security company Deep Sentinel leverages deep learning algorithms for property-related safety concerns. The product combines algorithms and computer vision technologies to quickly analyze threat factors in raw video stream data. The company is also researching the use of autonomous drones and IoT device environmental data collection for security solutions. Deep Sentinel\u2019s products aim to leverage pre-trained systems to build a comprehensive home control platform.\n\nCloudflare released its Orbit IoT security solution in 2017. Obert is an IoT security solution that enables IoT device manufacturers to connect their products to Cloudflare\u2019s network automatically, providing users with a machine-learning based API to monitor for suspicious activities."
    },
    {
        "url": "https://medium.com/syncedreview/china-to-facilitate-ai-unicorns-going-public-80964c8016",
        "title": "China to Facilitate AI Unicorns\u2019 Going Public \u2013 SyncedReview \u2013",
        "text": "The Chinese government is going to make it much easier for artificial intelligence unicorns to go public on its stock exchanges.\n\nChinese media is reporting that several domestic brokers and investment banks were recently informed by the China Securities Regulatory Commission (CSRC) that it will relax regulations on prospectus approval time and profitability minimums for Chinese unicorns in the domains of artificial intelligence, cloud computing, biotech, and high-end manufacturing.\n\nA Chinese AI unicorn C-level executive who asked not to be identified told Synced he believed many AI startups \u2014 particularly those in computer vision such as SenseTime, Face++, CloudWalk, and Yitu \u2014 will benefit from the loosened rules and soon go public.\n\nThe CSRC has not yet publicly commented on the news reports, nor has it released any official statements regarding any changes to its current regulations.\n\nThe new regulations are expected to shorten prospectus approval time from an average of eight months to just two or three months, while also lowering profitability minimums for qualified young Chinese AI unicorns.\n\nUntil now, in order to go public, Chinese companies were required to show a positive net profit for each of the last two to three fiscal years, and have a cumulative net profit for the period of not less than CNY\uffe510 to 30 million.\n\nChina AI unicorns\u2019 variable interest entities (VIE) \u2014 wherein an investor holds a controlling interest that is not based on share ownership \u2014 may no longer be an obstacle to their going public.\n\nMany global tech startups are now going public, a trend that is expected to break out through 2018 and 2019. Sweden-based music streaming service Spotify recently filed direct listings on the New York Exchange, valued at US$20 billion. China\u2019s Netflix-like video streaming platform IQIYI and its high-end electric SUV maker NIO are also working on US stock market listings this year. Uber and Airbnb are expected to go public in 2019.\n\nChina is eager to embrace its next-generation tech innovators and reform its IPO procedures. Earlier this year, the CSRC released a statement saying it is a vital \u201cto absorb the mature and effective systems and methods from international capital markets, and reform the system of issuance and listing.\u201d On February 9, the Shenzhen Stock Exchange issued its Strategic Plan for Development (2018\u20132020), shining a spotlight on unicorns in emerging industries.\n\nChina\u2019s official press agency Xinhua News recently published the article \u201cTo Achieve a BATJ [Baidu, Alibaba, Tencent, JD.com] Dream in China\u2019s Capital Market,\u201d which urged institutions to clear hurdles for local tech companies entering China\u2019s capital market.\n\nLast month, the world\u2019s largest contract electronics manufacturer Foxconn filed an IPO on the Shanghai Exchange to raise US$4.5 billion. It took Foxconn less than a month to go from prospectus filing to pre-disclosure update, a process that usually takes eight months. The company is expected to get its prospectus approval this March."
    },
    {
        "url": "https://medium.com/syncedreview/trashbots-boost-recycling-at-pittsburgh-airport-258004877bcd",
        "title": "\u201cTrashbots\u201d Boost Recycling at Pittsburgh Airport \u2013 SyncedReview \u2013",
        "text": "There are more than eight million travelers passing through Pittsburgh International Airport each year, leaving behind some 2,000 tons of trash. Pittsburgh is a major American robotics hub, and now its airport has introduced an appropriately automated recycling solution: the \u201cTrashbot.\u201d\n\nTrashbot uses cameras and sensors to scan discarded items. A machine learning algorithm based on Bayesian classifier sorts the trash for either landfill or recycle. If you dump an unfinished can of Coke, excess liquid will be drained off on a Teflon-coated sheet. The robot swallows garbage at the rate of around three seconds per item, with an 81% sorting accuracy rate, and continuously learns to recognize new types of trash. It is backed by a waste auditing dashboard based on Amazon Cloud Services, which presents data on types and quantities of refuse.\n\nCleanRobotics CEO Charles Yhap and VP of Engineering Tanner Cook are social entrepreneurs. Cook regards trash as an important social issue, second only to energy, telling Synced that \u201cdue to contamination and confusion, only 20% of what goes into a recycling bin ends up being recycled.\u201d\n\nThe team took two years to go from a wooden prototype at Pittsburgh\u2019s AlphaLab Gear Startup Accelerator to product. Cook says the hardest part of building the startup was securing money to cover the initial R&D costs, not the 60\u201390 hours a week of work he put into the project.\n\nWhat about cost? \u201cTrashbot is comparable to what\u2019s on the market, we\u2019re looking at US$1,500 to 5,000 per can,\u201d says Cook. For example, Denver international airport\u2019s current trashcan cost is $3000, and Cook says his company could replace those for the same price. Clients also need to pay a small monthly subscription to use the data dashboard via Amazon Web Services.\n\nCleanRobotics targets high-traffic locations such as airports, convention centers, malls, and schools. And although public facilities can be reluctant to adopt new technologies, Cook says many are actually reaching out to CleanRobotics.\n\n2014 data shows a recycling rate of 35% in the United States, with numbers varying drastically across the country. California has the best recycling programs, and San Francisco tops the city list with an 80% success rate. Up north, Portland Oregon follows at 70%. The West Coast\u2019s recycling records and proactive policies make the region an attractive market for AI-powered fast-track recycling solutions.\n\nOne of the biggest challenges facing urban recycling programs is the costs involved, particularly in sorting the trash to extract what\u2019s worth recycling. A study on municipal recycling argues that recycling plastic and aluminum can actually be profitable for cities, while it is less cost-effective to recycle glass and paper.\n\nAlthough it will be difficult to implement recycling procedures if they increase operating costs, many jurisdictions are offering tax credits to encourage recycling. Says Cook, \u201cin Australia, it costs $350 per ton of landfill, versus getting a tax credit for recycling. When you have a facility that\u2019s going through 10,000 tons of trash every year, it makes sense for them to try and divert as much of it into that high-quality recycling space as possible.\u201d Trashbot is already onsite at a Sydney shopping mall.\n\nThe CleanRobotics team is continuing to use artificial intelligence and robotics to explore niche challenges and develop new product prototypes for waste management, and Cook believes the best is yet to come."
    },
    {
        "url": "https://medium.com/syncedreview/datavisor-uses-unsupervised-learning-to-combat-online-fraud-3bd9c925b898",
        "title": "DataVisor Uses Unsupervised Learning to Combat Online Fraud",
        "text": "From millions of fake accounts spreading across social media to credit hackers stealing from bank accounts, online fraud has become a nightmare for Internet users. As Internet companies redouble their efforts to detect and foil emerging fraud techniques, many are turning to artificial intelligence for solutions.\n\nDataVisor is a Mountain View, California based anti-fraud startup that uses AI to detect fake accounts, prevent money laundering, and protect financial institutions from credit scams. The company offers clients its DataVisor APIs for real-time data connection, and a specialized UI for direct results.\n\nDataVisor Co-founders Yinglian Xie and Fang Yu were both Microsoft Senior Researchers for seven years, using data-driven approaches to solve the online service security challenges Internet companies were wrestling with.\n\nThe market for digital anti-fraud services is growing as transactions increasingly go online. Global non-cash transaction volume grew 11.2% in 2015 to reach 433.1 billion, the highest growth rate of the past decade, according to the World Payments Report 2017. Online fraud is also moving beyond the financial services sector to become a universal problem, also affecting for example gaming and social media.\n\nConventional anti-fraud approaches which are largely grounded in credit investigation and hand-tuned rules can no longer handle the scale of the problem. Fraud detection companies have been adopting AI \u2014 particularly supervised learning \u2014 to automate their fraud detection processes using sophisticated algorithms, vast amount of data, and robust computing capabilities. Trained with historic labeled data that distinguishes real users from fraudulent accounts, supervised learning models can efficiently classify new data and detect suspicious activity.\n\nYet supervised learning struggles when faced with rapidly variational fraud techniques. Says Xie, \u201cNowadays, online fraud can change in 24 hours. The characteristic of this area is that we are faced with a constantly changing fraudster, so it is difficult to get enough historical labeled data, limiting the effectiveness of supervised learning.\u201d\n\nThe unpredictability of online fraud schemes raises another challenge for supervised learning. Users can potentially be defrauded while using any of a platform\u2019s features, or as Xie suggests at any time during a normal \u201cuser life cycle.\u201d This could involve registration, payment, comments, or even app installation. The more features there are in a user life cycle, the more likely that user will get scammed.\n\n\u201cWe have helped IGG, a renowned video game publisher and developer, to combat game installation fraud. I barely noticed this problem before, but then realized it could be a big issue in the game industry,\u201d says Xie.\n\nAn increasing number of fraud detection companies like DataVisor are now employing unsupervised machine learning (UML), a type of machine learning algorithm that clusters unlabeled data by discovering hidden patterns. Madrid-based online financial institute Openbank for example uses UML algorithms to detect fraud and money laundering.\n\nDataVisor\u2019s fraud detection solution has three components: a UML Engine to cluster results, an Automated Rules Engine to replace time-consuming manual rule-making, and a Global Intelligence Network to gather vast amounts of information and domain knowledge.\n\nThe company\u2019s flagship is its UML Engine, which DataVisor bills as the first proven UML solution capable of handling vast volumes of data and discovering patterns from accounts and events in real-time.\n\n\u201cMost papers around unsupervised machine learning are based on very small datasets, and no one has been very successful at applying advanced unsupervised learning algorithms to large-scale data. The challenge is difficult,\u201d says Xie.\n\nAccording to a company white paper, DataVisor\u2019s UML Engine uses techniques such as natural language processing, image metadata analysis and graph analysis to extract features such as profile info, behaviors and activities, comments and metadata etc. from both structured data and unstructured data.\n\nThese features will be then grouped into clustered results with the important feature dimensions and distance functions selected. DataVisor says its UML Engine algorithms provide a more efficient and effective solution than common methods of dimensionality reduction, such as Principal Component Analysis (PCA). The Engine also deploys graph analysis and supervised machine learning algorithms to improve accuracy.\n\nThe last step is to rank the detected accounts, assign them confidence scores, and categorize a collection of malicious accounts with similar features, also known as \u201cattack rings.\u201d\n\nThe DataVisor white paper reports the UML Engine has been in use for over three years \u2014 with a 90\u201399% detection accuracy \u2014 and has that DataVisor has protected over two billion user accounts for large enterprises such as Yelp, Pinterest, Fortune 500 banks, IGG, Toutiao, etc., showing impressive growth for an early-stage startup.\n\nDataVisor is now looking to expand. The company raised US$40 million last month, led by Sequoia Capital China along with existing investors New Enterprise Associates (NEA) and GSR Ventures.\n\n\u201cWe believe a company driven by algorithms and data, such as DataVisor, will be very competitive in the future. Because of some characteristics of the anti-fraud industry, we think the barriers will be high, and there will be a \u2018Matthew effect\u2019 wherein the rich get richer and the poor get poorer,\u201d says Rock Wang, Managing Director of Sequoia China.\n\nThe anti-fraud industry is still in its early stages of development, and while there is a long road ahead for DataVisor, the company believes its detection techniques and services will stay ahead of emerging online fraud schemes. \u201cThe development of technology is endless. I think we can still move forward one step further.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/mit-and-sensetime-become-allies-in-ai-44a21aea7ac2",
        "title": "MIT and SenseTime Become Allies in AI \u2013 SyncedReview \u2013",
        "text": "The Massachusetts Institute of Technology (MIT) and Chinese AI Unicorn SenseTime today announced the MIT-SenseTime Alliance on Artificial Intelligence, a partnership the duo says \u201caims to open up new avenues of discovery across MIT in areas such as computer vision, human-intelligence-inspired algorithms, medical imaging, and robotics.\u201d The partnership is expected to develop 30\u201350 AI projects each year, and will be part of the MIT Intelligence Quest (MIT IQ) project launched earlier this month.\n\nSenseTime leverages facial recognition technologies powered by deep learning trained computer vision systems, along with video analysis, text recognition and autonomous driving technologies to provide business solutions.\n\nSenseTime has applied AI solutions to 400 companies in multiple industries including automobile, finance, mobile internet, robotics, security, and smartphones. It has scored mega-clients like China Mobile, China UnionPay, Huawei Technologies Co, Xiaomi. and JD.com, and a strategic partnership with Nvidia.\n\nCompany founder Dr. Xiao\u2019ou Tang is a Professor of Information Engineering at the Chinese University of Hong Kong and an MIT alumnus. He conducted his PhD research on applying computer vision to the study and classification of underwater imagery under the supervision of Dr. W. Eric L. Grimson.\n\nDr. Grimson is MIT\u2019s Chancellor for Academic Advancement, and has been their Bernard M. Gordon Professor of Medical Engineering for 24 years. Commenting on the new partnership with SenseTime, he had this to say about his former student: \u201cXiao\u2019ou has become well known throughout China and the world as a leader in the field of AI, especially computer vision and deep learning. I personally am proud of Xiao\u2019ou\u2019s success and the impact he is making on the world, and look forward to a deepened, mutually beneficial relationship between MIT and SenseTime.\u201d\n\nIn late 2016 SenseTime raised US$120 million in funding, led by Beijing-based CDH Investments, Dalian Wanda Group, IDG Capital Partners and Star VC. Some six months later the company closed its Series B funding with US$410 million from CDH and Sailing Capital, which propelled it onto CB Insights\u2019 2017 technology unicorn list. SenseTime has received further strategic investments from Alibaba Group and Qualcomm. The company is sitting on a market valuation of US$3 billion and preparing for an IPO.\n\nIn recent years, MIT has been in an increasingly fierce AI tech competition with other US universities such as Stanford and Carnegie Mellon. In 2017 the latter launched its CMU AI initiative, knitting together more than 100 faculty members and 1,000 students.\n\nMIT clusters more than 200 principal AI investigators in its Computer Science and Artificial Intelligence Laboratory, MIT Media Lab, Department of Brain and Cognitive Sciences, Center for Brains, Minds and Machines, and MIT Institute for Data, Systems, and Society.\n\nMIT IQ, launched on Feb 1, is led by MIT President L. Rafael Reif along with Dr. Anantha Chandrakasan from MIT\u2019s Electrical Engineering and Computer Science department, Dr. Daniele Rus, Dr. Dina Katabi, Dr. James DiCarlo, and Dr. Josh Tenenbaum. The project envisions MIT\u2019s future AI endeavours in two parts: \u201cThe Core,\u201d which will focus on fundamental algorithm research; and \u201cThe Bridge,\u201d which will provide the MIT community with AI education, support, hardware, and resources.\n\nAs AI evolves from research labs into practical applications, this partnership between a top-notch university and an experienced solution provider is bound to unlock new tech breakthroughs and encourage additional strategic collaborations in the global AI race."
    },
    {
        "url": "https://medium.com/syncedreview/pandas-and-ai-lead-the-way-to-the-beijing-2022-winter-olympics-fd58a5b1dceb",
        "title": "Pandas and AI Lead the Way to the Beijing 2022 Winter Olympics",
        "text": "The Pyeongchang 2018 Winter Olympics\u2019 closing ceremony on Sunday was capped by a spectacular eight-minute presentation from 2022 host Beijing that featured state-of-the-art robotics and AI technology.\n\nBeijing celebrated the transfer of the Olympic flame with a performance by 22 skaters led by a couple of panda \u201ccaptains,\u201d supported by an array of 24 AI-controlled robot-screens patterned after the Great Wall. Skaters glided across projections of traditional Chinese motifs such as knotting, dragons and phoenixes on the ice surface, while images of China\u2019s latest tech achievements such as high-speed trains, aircraft and spacecraft were displayed overhead on the robot-screens.\n\nDirector Yimou Zhang is renowned for his extravagant Beijing 2008 Olympic opening and closing ceremonies, as well as the summer torch handover ceremony in Athens in 2004. Nearly ten years later, Zhang has replaced the hundreds of actors with robot substitutes, and applied AI-powered visual media to the ceremony for the first time in Olympic history.\n\nZhang, who contemplated the theme for an entire year and spent two months preparing and choreographing the show, says two characteristics define this year\u2019s presentation: \u201cOne is the joint performance of AI and actors, and second is using the internet to interact with a Chinese audience.\u201d\n\nChina is determined to put AI on its national name card, and Zhang says that by linking China with AI in the ceremony \u201cwe are sending an invitation to the world, at the same time demonstrating China\u2019s new image in the new era.\u201d\n\nLast month, sharp-eyed viewers identified a couple of interesting books on President Jinping Xi\u2019s bookshelf during his 2018 New Year\u2019s address: Pedro Domingos\u2019 The Master Algorithm and Brett King\u2019s Augmented: Life in the Smart Lane.\n\nIn an official report released during the 19th National Congress of the Communist Party of China, Xi pledged that the Chinese government will \u201cpromote further integration of the internet, big data, and artificial intelligence with the real economy, and foster new growth areas and drivers of growth in medium-high end consumption, innovation-driven development, the green and low-carbon economy, the sharing economy, modern supply chains, and human capital services.\u201d\n\nThe Chinese government also presented its blueprint for further AI integration with the economy, following up with an intensive review on \u201cStandardization of AI Helps Industry Development\u201d and a \u201cThree-Year Action Plan for Promoting the Development of New Generation of AI Industry\u201d.\n\nThe Olympic Games with their global audience provided the perfect venue to showcase this new national endeavour, and China did it beautifully."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-self-driving-startup-ceo-suddenly-steps-down-2cac2b922d2b",
        "title": "Chinese Self-Driving Startup CEO Suddenly Steps Down",
        "text": "Jin Wang, CEO of China\u2019s self-driving startup JingChi, has stepped down just 11 months after founding the company.\n\nJingChi said in a statement that \u201cJin Wang left the company due to family issues. Despite that, the company remains unchanged.\u201d JingChi CTO Tony Han, the rumored pick to replace Wang as CEO, has not yet responded to Synced\u2019s request for comment.\n\nA CEO exiting a months-old startup is barely seen in the tech world. Wang\u2019s departure is widely rumoured to be related to JingChi\u2019s dispute with Baidu, which sued the startup two months ago for allegedly stealing its self-driving technology.\n\nThe lawsuit echoes the 2017 Waymo vs Uber dispute in which Waymo accused Uber of stealing its LiDar technology. That case was recently resolved, with Uber paying Waymo a settlement of US$245 million in stock options.\n\nBefore Wang started JingChi he was a Baidu senior vice president. He co-founded Baidu\u2019s autonomous driving unit, which spawned the open source autonomous driving development platform Apollo.\n\nLast March, Wang left Baidu and co-founded JingChi in Sunnyvale, California. The company successfully completed its first open road autonomous driving testing in only 81 days, and put a prototype through Silicon Valley rush hour traffic two months later.\n\nThe huge demand for autonomous driving services in China prompted JingChi to move its headquarters to Guangzhou last December. Wang concluded an agreement with Guangzhou Development District to deploy a L4 self-driving ride-hauling service by the end of 2018. The company also promised to deliver 500 to 1000 autonomous driving commercial vehicles through 2018.\n\nJingChi\u2019s fast tech development and ambitious plans attracted interest from Nvidia, who joined a group of investors led by Chinese venture capital firm Qiming Venture in JingChi\u2019s Series Pre A Funding Round, which raised US$52 million.\n\nHowever, JingChi\u2019s rapid rise was interrupted last December when Baidu filed a suit accusing JingChi of infringement on its commercial secrets, not returning devices that contained classified information, and poaching technical personnel. Baidu sought compensation of USD$7.6 million, and a JingChi promise to discontinue use of the company\u2019s proprietary information or technologies relating to autonomous driving.\n\nWang initially dismissed the accusations: \u201cAs an innovative start-up, we are not afraid of competitors, so the challenge will not delay our pace of development\u2026 Baidu\u2019s prosecution is completely unfounded and JingChi lawyers will respond factually and legally.\u201d The lawsuit is ongoing in Beijing Intellectual Property Court.\n\nAt this point it\u2019s still too early to say how Wang\u2019s departure will affect either the lawsuit or the company\u2019s tech development and overall health. Many questions remain unanswered, especially: What will be JingChi\u2019s next move? Synced is covering the story and will continue to update readers with the latest news."
    },
    {
        "url": "https://medium.com/syncedreview/criteria-for-building-a-successful-ai-chatbot-d7d00dd360ad",
        "title": "Criteria for Building a Successful AI Chatbot \u2013 SyncedReview \u2013",
        "text": "A Chatbot is information service interface done by a computer program, sometimes with the help of artificial intelligence (AI). Chatbot applications range from functional to fun, and have been growing quickly both in sophistication and popularity.\n\nChatbot-based applications have already impacted a variety of industries:\n\nThe Chatbot is becoming a major application across multiple industries and use cases, and many global brands are heavily invested in Chatbot technology.\n\nAlthough Chatbots are now widely used, their core technologies are still in the early stage of application and some Chatbots fail to meet Users\u2019 expectations. Reasons for unfavourable user experience can be categorized:\n\nWhy a Chatbot?\n\n \n\n The Chatbot is not a universal solution for every user interaction interface. To build a successful Chatbot, the first step is to consider \u201cWhy a Chatbot?\u201d This consideration should include conversion friction, usability, ability to get the job done, efficiency, and ecosystem maturity. By exploring these considerations you will able to decide whether to use a Chatbot or Native App for your core business solution. The chart below compares Chatbot, Native App, and Mobile Webpage interfaces.\n\nWhat can users do with your Chatbot?\n\n \n\n You will need to think from the user\u2019s point of view. The Chatbot follows different design principals to provide effective service in various use cases. For example, a banking assistant type would have an advanced banking FAQ database to deliver smart responses, while a shopping assistant type would have an enhanced short memory function, allowing the user to retrieve previous information. \n\n \n\n \n\n How to manage user expectations?\n\n \n\n User expectations of a Chatbot can be understood from the point of view of scale, convenience, engagement, personalization, and natural interaction.\n\nWhat are the intelligence and cost requirements?\n\n \n\n The level of intelligence and cost totally depend on your core business objective. You can evaluate this through industry vertical, level of interaction and complexity factors.\n\nBeyond these three main variables, you should also look at factors such as number of platforms, technical infrastructure and custom integrations. A standard Chatbot will almost always be much cheaper than a customized Chatbot.\n\nNowadays it\u2019s hard to deny the great capabilities that Chatbots are bringing to our daily lives, and the many possibilities they hold for the future. Answering the above four questions will help you identify whether an existing third-party Chatbot aligns with your needs, or set the scope for developing your own. Then you will be ready to take the next step to building a successful Chatbot."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-launches-11-qubit-quantum-computing-cloud-service-ad7f8e02cc8",
        "title": "Alibaba Launches 11-Qubit Quantum Computing Cloud Service",
        "text": "Quantum computing is a buzzword in the tech world. Its promise of greatly improved speed compared to binary-based classical computing and its potential for supercharging artificial intelligence have made quantum computing something of a holy grail for global tech giants.\n\nAlibaba took a big step towards quantum computing yesterday when its cloud service subsidiary Aliyun (\u201cAlibaba Cloud\u201d) and the Chinese Academy of Sciences jointly launched an 11-qubit quantum computing service, which is available to the public on the Quantum Computing Cloud Platform.\n\nA Qubit, or quantum bit, is a unit of quantum information, the quantum equivalent of a classical binary bit. Qubits can be in superpositions of states, so they are not limited by binary code\u2019s two definite states (0 or 1). The properties of qubits make quantum computing in theory capable of storing more information and processing data exponentially faster.\n\nAlibaba becomes the second company to provide public cloud computing services with processing power of 10+ qubits. Last November IBM released 20-qubit quantum computers through its cloud service.\n\nAliyun encourages users, especially researchers, to run algorithms on Alibaba\u2019s quantum computers to conduct preliminary experiments. This will help detect technical bottlenecks, optimize user experience, and develop next-generation processors.\n\n\u201cThis launch means that Aliyun\u2019s quantum computing researchers can more easily experiment with real-world processors to help them understand the hardware and lead the development of quantum tools while fueling in continuous improvements from the client experience,\u201d says Dr. Yaoyun Shi, Chief Scientist of Quantum Technology at Aliyun.\n\nThe Chinese e-commerce giant accelerated its development of quantum computing in 2015 when Aliyun and the Chinese Academy of Science jointly developed the Alibaba Quantum Laboratory (AQL). The lab has since acquired a number of well-known scientists, such as Dr. Shi from the University of Michigan, and two-time G\u00f6del Prize winner Dr. Mario Szegedy.\n\nAQL has released an ambitious 15-year roadmap. By 2025, it expects to have built quantum computers that will be the world\u2019s fastest by today\u2019s measure. By 2030, AQL hopes to achieve a general quantum computing prototype with 50\u2013100 qubits.\n\nAliyun is also offering a new 32-qubit quantum computer simulation service. By comparing simulated experiment results with real results on quantum computers, users can measure the latter\u2019s performance, verify correctness, etc.\n\nRenowned Chinese physicist Dr. Guangcan Guo also announced yesterday that his team at the Chinese Academy of Sciences has simulated a 64-qubit quantum computer on a conventional supercomputer, outperforming the simulated 56-qubit quantum computers IBM released last year.\n\nThe global quantum computing race is heating up. Not to be outdone by IBM or Alibaba, Google researchers last year released a paper demonstrating how 50-qubit quantum computers can outperform classical computers, also known as Quantum Supremacy. Google also plans to add D-Wave quantum computers to its public cloud.\n\nMeanwhile Microsoft, which has been investing in quantum technology since 1997, released a quantum computing development kit preview last December, which includes tools developers need to get started with quantum computing.\n\nAlibaba sees quantum computing as a revolutionary technology that will spark scientific breakthroughs. The company that was until recently barely noticeable in the field of fundamental research is now eager to get in the game and take the lead."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-startups-hauled-in-half-of-2017-global-ai-funding-49bd97ef3746",
        "title": "Chinese Startups Hauled In Half of 2017 Global AI Funding",
        "text": "Over US$12.5 billion in funding flowed into AI startups in 2017, with Chinese startups receiving 48 percent of that, according to the new CB Insights report The State of Artificial Intelligence. China overtook the US (38 percent) for the first time, as its AI startups wowed the world with a yearlong series of huge funding rounds.\n\nMuch of the money went to computer vision (CV) technologies, with 41 disclosed deals and US$1.6 billion in disclosed funding. Face++ topped the heap when its Series C round attracted a record-setting US$460 million, while other CV companies like SenseTime (US$410 million) and CloudWalk (US$375 million) also did extremely well. Such companies brought their machine learning technologies to China\u2019s US$15 billion security camera market or the rapidly growing facial recognition payment market.\n\nCV technology also captured attention from non-CV companies such as Chinese speech and voice technology giant iFlytek, which has won multiple object detection and self-driving competitions. In November iFlytek released a medical assistant robot for hospitals to improve diagnosis accuracy.\n\nAlong with the computer vision startups, Chinese computer chip companies also had a great 2017. Cambricon and Horizon \u2014 which develop machine learning processors to power AI applications on smart devices, self-driving vehicles, and smart cities \u2014 raised US$100 million each. Cambricon\u2019s AI computation-boosting neural processing unit is inside Huawei\u2019s new flagship smartphone Mate 10.\n\nChinese startups are also becoming increasingly attractive to US investors. CB Insights reported 20 US-backed equity deals with Chinese startups last year, double the 2016 total. As more investors pump more money into innovative Chinese AI startups, those startups are bound to bring even more innovations in 2018."
    },
    {
        "url": "https://medium.com/syncedreview/ai-adds-colour-to-grandmas-cherished-memories-46d639d47f94",
        "title": "AI Adds Colour to Grandma\u2019s Cherished Memories \u2013 SyncedReview \u2013",
        "text": "After every Lunar New Year celebration, as the kids prepare to head back to the big city, grandmothers across China take their old tin of family photos out of the cupboard for a nostalgic trip down memory lane. Alas, these beloved old photographs are faded, torn, and monochrome.\n\nTencent Youtu \u2014 literally translated as \u201cimage optimization lab\u201d \u2014 is the image processing, pattern recognition, machine learning, and data mining research arm of Chinese tech giant Tencent Group. Youtu is now offering a free retouching service for old photographs. Upload a scanned copy of your grandparents\u2019 wedding picture on WeChat\u2019s H5 built-in interactive pages, and Tencent\u2019s AI will bathe the scene in natural colours.\n\nThe process begins with the AI detecting whether it is actually dealing with a monochrome photo. The team tells us many apparently black and white photographs are not truly monochromatic due to yellowing, molding, staining, and so forth. Colour contamination can also be also introduced during the scanning process if the scanner\u2019s background is coloured.\n\nThe Youtu team analyzed millions of public stock photos and trained their algorithm to learn in multiple colorimetric spaces. The team separates the image based the HSV (Hue, Saturation, Value) colour model, which helps the algorithm identify moldy and damaged areas of sparse colour distributions precisely while ignoring greyish border areas. The team\u2019s internal testing shows 95% image recognition accuracy.\n\nThe colouring process works as follows: the AI will tag different figures and objects in the photo. A trained neural network will then learn to \u201cunderstand\u201d tagged parts using semantic memories like \u201csky\u201d, \u201cgrass\u201d, \u201cbuilding\u201d, and \u201cface\u201d, and finish the retouching with correct colour matches. AI-coloured images don\u2019t restore the original colours, rather the algorithm is trained to come up with reasonable colour scenarios.\n\nWhat if you have a old faded coloured photograph to begin with? Youtu\u2019s AI can restore a colour photo by comparing it with similar images, then adjusting image intensities to enhance contrast using a technique called \u201chistogram equalization.\u201d\n\nRichard Zhang\u2019s team at University of California at Berkeley first offered the public an automatic photo colouring app in a project titled Interactive Deep Colorization, which is available for download on GitHub. Zhang\u2019s 2016 paper Colourful Image Colorization further explains the network. For the curious, a history of automatic colouring AI can be traced back to Zezhou Cheng et al.\u2019s 2015 work on Deep Colorization.\n\nTencent\u2019s Youtu AI lab making the technology accessible to Chinese netizens is part of a developing \u201cfun AI in 2018\u201d trend, as Tencent\u2019s H5 built-in apps make big bang developments. Last November, Tencent launched a built-in \u201cAI experience center\u201d that includes many fun and handy AI applications. The company also recently open-sourced WeChat\u2019s built-in gaming and search functions to commercial developers."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-ai-can-clone-your-voice-in-seconds-93558a7b984f",
        "title": "Baidu AI Can Clone Your Voice in Seconds \u2013 SyncedReview \u2013",
        "text": "Baidu\u2019s research arm announced yesterday that its 2017 text-to-speech (TTS) system Deep Voice has learned how to imitate a person\u2019s voice using a mere three seconds of voice sample data.\n\nThe technique, known as voice cloning, could be used to personalize virtual assistants such as Apple\u2019s Siri, Google Assistant, Amazon Alexa; and Baidu\u2019s Mandarin virtual assistant platform DuerOS, which supports 50 million devices in China with human-machine conversational interfaces.\n\nIn healthcare, voice cloning has helped patients who lost their voices by building a duplicate. Voice cloning may even find traction in the entertainment industry and in social media as a tool for satirists.\n\nBaidu researchers implemented two approaches: speaker adaption and speaker encoding. Both deliver good performance with minimal audio input data, and can be integrated into a multi-speaker generative model in the Deep Voice system with speaker embeddings without degrading quality.\n\nSpeaker adaption is a backpropagation-based approach grounded in a multi-speaker generative model or adapted to only low-dimensional speaker embeddings. Speaker encoding meanwhile combines the multi-speaker generative model with a separate model that generates a new speaker embedding from cloned audio. This approach shortens cloning time to just a few seconds and requires a low number of parameters to represent each speaker, making it favorable for low-resource deployment.\n\nBaidu has released multiple three-second cloned audio clips which track the process from original voices to synthesized voices that are strikingly similar.\n\nBaidu is upbeat about the possibilities in the field of voice cloning research. For example, advances in meta-learning, a systematic approach of learning-to-learn, could significantly boost voice cloning quality.\n\nBaidu is not the only institute working on imitating human voices with AI. Google\u2019s DeepMind, which produced the epoch-making Go computer AlphaGo, introduced its TTS project WaveNet in 2016. The system models audio waveforms from real human voices and produces convincingly natural simulations. Adobe also unveiled a prototype software called Project VoCo that can learn to mimic a voice in 20 minutes. Last year, Montreal-based startup Lyrebird pushed voice cloning technology to the next level with a TTS system that required only a 60-second audio sample input to deliver \u201ca digital voice that sounds like you.\u201d\n\nThe recent breakthroughs in synthesizing human voices have also raised concerns. AI could potentially downgrade voice identity in real life or with security systems. For example voice technology could be used maliciously against a public figure by creating false statements in their voice. A BBC reporter\u2019s test with his twin brother also demonstrated the capacity for voice mimicking to fool voiceprint security systems.\n\nBaidu\u2019s Deep Voice has reduced training time and advanced the development of voice cloning, opening possibilities for improvements in virtual assistants, advances in healthcare solutions and applications in many other sectors."
    },
    {
        "url": "https://medium.com/syncedreview/its-all-in-the-eyes-google-ai-calculates-cardiovascular-risk-from-retinal-images-150d1328d56e",
        "title": "It\u2019s All in the Eyes: Google AI Calculates Cardiovascular Risk From Retinal Images",
        "text": "A retinal fundus image is a photograph of the back of the eye taken through the pupil. For more than 100 years these images have been used for detecting eye disease. Now Google has introduced a surprising new use for retinal images: combined with artificial intelligence, they can also predict a patient\u2019s risk of heart attack or stoke.\n\nResearch arm Google Brain today published a paper in the journal Nature Biomedical Engineering which demonstrates how deep learning models can use retinal images to detect a patient\u2019s age, gender, smoking status and systolic blood pressure; calculate cardiovascular risk factors; and predict the risk of major adverse cardiac events occurring over the next five years.\n\nA problem with today\u2019s mainstream cardiovascular risk calculators such as the Pooled Cohort Equations, Framingham, and Systematic Coronary Risk Evaluation is that they require the input of multiple features such as blood pressure, body mass index, glucose and cholesterol levels, etc. to generate a disease risk result. A study by the American College of Cardiology\u2019s Practice Innovation And Clinical Excellence Program concluded that the data required to calculate 10-year risk was available for less than 30% of patients.\n\nGoogle Brain discovered that a retinal fundus image alone was sufficient to predict many cardiovascular risk factors. The anatomical feature patterns were extracted using a convolutional neural network \u2014 a computational model that excels in analyzing images.\n\nResearchers trained models on retinal images from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients. The trained model identified patients\u2019 ages with 3.26 years, distinguished gender 97 percent of the time, spotted a smoker 71 percent of the time, and calculated blood pressure with a 11.23 mmHg margin of error.\n\nGoogle Brain then took a step forward. Researchers discovered the trained model could predict a patient\u2019s risk of cardiovascular disease over the next five years 70 percent of the time, approaching the accuracy rate of established risk calculators without all the additional data inputs.\n\nDeep learning is often criticized for its lack of transparency and interpretability, and this has hindered the technology\u2019s entry into areas such as medical health and the legal system. But Google Brain believes their methodology is sound. It employs attention techniques to determine which pixels are the most important for predicting a specific cardiovascular risk factor: blood vessels for example are a critical feature for determining blood pressure.\n\nThis is not the first time Google Brain has leveraged the value of retinal images. In November 2016 it presented a study on deep learning for early detection of diabetic retinopathy, which could potentially protect 415 million worldwide diabetics from irreversible blindness.\n\nGoogle Brain\u2019s paper opens up the exciting possibility of applying deep learning to retinal images for improving diagnoses beyond eye disease. Will AI be the key that unlocks even more medical science innovations?"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-february-pt-1-8c322e4c7a16",
        "title": "AI Biweekly: 10 Bits from February (Pt 2) \u2013 SyncedReview \u2013",
        "text": "February 5th \u2014 NLP Could Help With Health Information\n\nNon-profit organization Atrius Health applies NLP technology to identify clinical data and extract information. The NLP technology is provided by the Linguamatics 12E platform, which can enhance clinical documentation for chronic disorders and improve the medicare ACO quality report. Atrius Health plans to apply the platform in additional areas to support and service behavioral healthcare.\n\nClearBrain helps users aggregate their data and deploys artificial intelligence to help subscription-based businesses target users most likely to sign up. The company is among American seed accelerator Y Combinator\u2019s startups and has raised US$1.2 million in funding from YC and other Venture Capital funds. ClearBrain clients include InVision and The Skimm.\n\nCanada\u2019s Liberal government announces new funding of CDN$4.1 million over five years for the Canadian Centre for Child Protection. The funding will support the development and implementation of AI technology that crawls websites to detect child pornography.\n\nAlphabet\u2019s Waymo believes a former employee brought thousands of confidential documents to Uber. Soon after the lawsuit was filed, Uber fired its head of autonomous driving. The surprise settlement sees Uber paying US$245 million in shares to Waymo, and also affirms a new partnership between the two on autonomous car development, despite their direct competition.\n\nFebruary 12th \u2014 Google Opens Its TPU Chips to the Public\n\nGoogle opens access to its tensor processing units (TPUs) through its cloud-computing services at the rate of US$6.5 per cloud TPU per hour. Google\u2019s TPUs are dedicated to tasks related to machine learning and have allowed Google to be less dependent on chip suppliers like Nvidia. Now they will also generate rental revenue. Ride-sharing company Lyft was one of the early adopters of Google TPUs, which have significantly sped up its development of autonomous vehicles.\n\nFebruary 13th \u2014 Google Uses AI to Reply to Messages Across Major Chat Apps\n\nA Google team is working to bring its smart reply feature to major chat applications such as Hangouts, Allo, WhatsApp, Facebook Messenger, Android Messages, Skype, Twitter DMs, and Slack. This new feature will enable context-aware responses in messages. For example, by accessing your current location, the feature can automatically answer a question like \u201cWhen will you arrive?\u201d\n\nApple is said to be talking with Yangtze Memory Technologies regarding a storage chips purchase. This would be the first Apple chip purchase from a Chinese supplier. The chips will be used in the new iPhone and other products for the Chinese market. Neither Apple nor Yangtze Memory responded to requests for comment.\n\nAlphabet\u2019s autonomous driving company Waymo launches a ride-hailing service in Arizona. Beyond the autonomous driving technology, Waymo is also building a ride-hailing platform. Waymo\u2019s moves in this space are seen as a threat to Uber.\n\nAmazon completes the biggest ever redesign of its Alexa Skills Kit Developer Console. The makeover focuses on improved developer workflows and provides a more user-friendly visual interface, and comes at a time when some third-party companies had stepped in to provide solutions the Amazon toolset had been lacking.\n\nIntel\u2019s new Vaunt smartglasses look just like normal glasses. They can project images or words on a holographic reflector which transmits the content directly to the eyes. Learning from the failed Google Glass, Intel has avoided the tap function and made their glasses less conspicuous to use in public."
    },
    {
        "url": "https://medium.com/syncedreview/synced-celebrates-lunar-new-year-with-30-best-ai-use-case-award-9c24709cb6c1",
        "title": "Celebrating Chinese New Year 2018: Synced Announces 30 Best AI Use Case Award",
        "text": "Synced celebrates the Chinese New Year 2018 by announcing our 30 Best AI Use Case Award. The Award recognizes companies that have applied AI in various industries or organizations.\n\n \n\nCongratulations to the companies below for their outstanding achievements in the AI industry. Synced will continue to bring AI updates to our readers throughout 2018.\n\nCobot \n\n \n\nCobot\u2019s intelligent grasping system comprises a robot, control system, visual system, and auxiliary system. The company is helping Yuguo Mushroom (\u88d5\u56fd\u83c7\u4e1a) improve efficiency in sorting mushrooms.\n\nNetease AI\n\n \n\nNetEase AI uses facial recognition technology to set up email log in verifications to ensure account safety. \n\nChina\u2019s AI-powered NetEase is Music to Your Ears\n\n\n\nSpeakIn \n\n \n\nSpeakIn provides voiceprint recognition and ID security solution for China\u2019s police departments and judicial institutions. The company uses AI for anti-telecommunication fraud, anti-terrorism, criminal detection, and ID security checks.\n\nMaster Learner\n\n \n\nMaster Learner develops AI applications for education purposes, providing personalized classes for over 80 million primary and middle school students in China.\n\nYixue Education\n\n \n\nYixue Education works with Stanford University\u2019s SRI Lab to research and deploy \u201cintelligent adaptive learning\u201d technologies. The company operates 500 schools in more than 20 Chinese provinces.\n\nKika Tech\n\n \n\nKika leverages speech interaction systems with its AI-powered product Kika Keyboard. The AI engine understands and predicts user intentions to make conversations more expressive.\n\nMeitu\n\n \n\nMeitu AI lab\u2019s facial recognition and augmented reality technology enables Meitu beauty camera users to virtually try on trending makeup looks as well as create their own looks.\n\nAfter Creating 166 Million Super Cute Selfies, Meitu\u2019s AI Sets its Sights on Skin\n\n\u6210\u5c31\u62a4\u80a4\u5927\u4e1a\uff0c\u9664\u4e86\u8d1f\u8d23\u6e05\u7a7a\u8d2d\u7269\u8f66\u7684\u7537\u53cb\uff0c\u59b9\u5b50\u4eec\u8fd8\u9700\u8981\u4e00\u4f4dAI\u76ae\u80a4\u987e\u95ee\n\n \n\nTuputech\n\nWith Tuputech\u2019s real-time image and video recognition API, Inke was able to detect violent and other objectionable or explicit content in images and videos with 99.5% accuracy, reducing manpower by 90% or more.\n\n4Paradigm\n\n \n\n4paradigm AI helps Chinese banks boost performance in verticals like application anti-fraud, transaction anti-fraud, overdue or loss warning, liquidity management, smart collection, and disposal of non-performing assets.\n\n\u7b2c\u56db\u8303\u5f0f\u9648\u96e8\u5f3a\uff1a\u4e07\u5b57\u6df1\u6790\u5de5\u4e1a\u754c\u673a\u5668\u5b66\u4e60\u6700\u65b0\u9ed1\u79d1\u6280\n\n \n\nAnt Financial\n\n \n\nAdopting image recognition technology, Ant Financial\u2019s AI-powered computer vision system helps insurance companies to optimize accident claim procedures and settle cases more quickly.\n\n\u8682\u8681\u91d1\u670d\u53d1\u5e03\u300c\u5b9a\u635f\u5b9d\u300d\uff0c\u63a8\u52a8\u56fe\u50cf\u5b9a\u635f\u6280\u672f\u5728\u8f66\u9669\u9886\u57df\u7684\u5e94\u7528\n\n\u8682\u8681\u91d1\u670d\uff1a\u53ea\u505a Tech\uff0c\u652f\u6301\u91d1\u878d\u673a\u6784\u505a Fin\n\n \n\nCreditX \n\n \n\nCreditX uses deep learning technology and knowledge mapping of financial data to help clients in the finance industry with risk management tasks.\n\n \n\nDelta Entropy\n\n \n\nDelta Entropy employs big data analysis and machine learning algorithms to enable automatic risk evaluation for large-scale machinery and equipment. \n\n \n\nIpampas\n\n \n\nIPampasWealthBrain (iWB) provides AI solutions for financial institutions with intelligent wealth management and global asset allocation.\n\n \n\nJD Finance\n\n \n\nJD Finance uses a multi-dimensional consumer behavior model to analyze JD user data and recommends financial products to meet clients\u2019 requirements.\n\nTachyus\n\nTachyus leverages machine learning in energy production for the oil and gas industry. Petroleum engineers can explore millions of scenarios and identify optimal operational and development plans.\n\nIntuitive Surgical\n\n \n\nIntuitive\u2019s Da Vinci surgical robots enable surgeons to perform micro incision surgeries. The technologies have applications in cardiac, thoracic, urology, gynecologic, colorectal, pediatric and general surgical disciplines.\n\n \n\nIntel\n\n \n\nIntel develops the smart medical imaging technology that helps China\u2019s Aier Eye Hospital improve the accuracy of eye disease diagnoses.\n\n \n\n12sigma\n\n \n\n12sigma Technologies introduces AI and deep learning to medical image diagnosis and medical data analysis to help doctors analyze CT scans more efficiently, especially for lung cancer.\n\nRoss Intelligence \n\n \n\nRoss Intelligence uses NLP technology to automate legal research, and improves efficiency for law firms. The company has partnered with law schools to conduct further research. \n\n\n\n\u4e16\u754c\u9996\u4e2aAI\u6cd5\u5f8b\u52a9\u7406ROSS\u5b8c\u6210A\u8f6e\u878d\u8d44\uff0c\u529b\u62d3\u4e16\u754c500\u5f3a\u6cd5\u5f8b\u670d\u52a1\u5e02\u573a\n\n \n\nPatsnap\n\n \n\nPatsnap\u2019s AI-powered patent search and analysis platform helps business leaders, analysts, researchers, engineers and IP professionals make well-informed decisions.\n\nHikvision\n\n \n\nHikVision\u2019s smart warehouse solutions help China\u2019s Master Logistics process documents and data in warehousing operations.\n\nBloomReach\n\n \n\nBloomReach helps department store Neiman Marcus make their content more discoverable with applications using content management, self-learning personalized site search, SEO optimization and merchandising analytics.\n\nClarifai helps image recognition translate product image tags into 12 different languages to improve SEO and drive traffic to products in its e-commerce store, saving over US$10,000 in agency fees.\n\n\u673a\u5668\u4e4b\u5fc3\u72ec\u5bb6\u5bf9\u8bddClarifai\u521b\u59cb\u4eba\uff1a\u4ece\u56fe\u50cf\u8bc6\u522b\u5230\u65e0\u9650\u53ef\u80fd\n\n \n\nHaomaiyi\n\n \n\nHMY Technology designs virtual fitting rooms. Its AI powered 3D reconstruction technology allows users to \u201cmodel\u201d different outfits.\n\nYi Tunnel uses image & facial recognition technology to produce AI-powered purchasing and vending machines, which allow unattended checkout and make offline shopping more convenient.\n\nAISpeech\n\n \n\nAISpeech specializes in speech interaction technology. Its services are used by well-known home speaker brands in China including Alibaba\u2019s AliGenie. \n\n\u5728 AI \u521b\u4e1a\u7684\u597d\u65f6\u4ee3\uff0c\u521d\u654f\u7ed3\u675f 8 \u5e74\u963f\u91cc\u751f\u6daf\u52a0\u5165\u601d\u5fc5\u9a70\n\n\u601d\u5fc5\u9a70\u53d1\u5e03 DUI \u5f00\u653e\u5e73\u53f0\uff0c\u5168\u94fe\u5b9a\u5236\u5316\u8d4b\u80fd\u66f4\u591a\u7ec8\u7aef\n\n \n\nElevoc\n\n \n\nElevoc Technology provides a new-generation voice signal processing engine for telecommunication, smart home and vehicle devices. It does real-time implementation of practical monaural speech de-noising.\n\niflytek\n\n \n\niFlyTek\u2019s speech recognition solutions are used in cars for hands-free interaction. Users can make voice navigation commands, phone calls, send text messages, access a browser search and so on. \n\nWill iFlytek Voice Input\u2019s 98% Accuracy Kill the Keyboard?\n\n \n\n \n\nKuaishou\n\n \n\nKuaishou is a live video streaming platform broadcasting user generated content. It includes an AI-powered recommendation system."
    },
    {
        "url": "https://medium.com/syncedreview/china-celebrates-lunar-new-year-with-ai-aadd24639281",
        "title": "China's Lunar New Year\u2019s Gala Wowed Watchers with AI Innovations",
        "text": "The CCTV Spring Festival Gala (\u592e\u89c6\u6625\u665a) is Chinese TV\u2019s Lunar New Year extravaganza, watched by hundreds of millions of people across the country. This year, amid all the acrobatics, musical performances and fireworks, broadcaster CCTV also shone a spotlight on the country\u2019s state-of-the-art AI technologies.\n\nThe showcase included self-driving vehicles, intelligent robots, drones and more \u2014 and won glowing reviews from Chinese netizens on social media.\n\nChina continues to spark technological breakthroughs and establish AI-powered infrastructures for its cities and transportation networks. The government aims to make the AI industry worth US$150 billion by 2030, and is investing $2.1 billion in AI research and development to empower local businesses.\n\nWhile polls suggest many Americans and Europeans fear AI will take their jobs, the Chinese are much more optimistic about the tech, and 65 percent believe AI will create more job opportunities over the next five to ten years.\n\nThe AI innovations at the Spring Festival Gala were choreographed into the show\u2019s performances. Synced has picked out a few highlights.\n\nIn a live stream from the Gala\u2019s parallel session in Zhuhai, over a hundred self-driving cars equipped with Baidu\u2019s autonomous driving platform Apollo crossed the Hong Kong \u2014 Zhuhai \u2014 Macao Bridge. The self-driving fleet included BYD\u2019s new energy vehicles (NEV), King Long\u2019s self-driving micro-location bus, and Zhixingzhe\u2019s self-driving road sweeper and logistics vehicles.\n\nFirst released last July, Apollo is an open-source platform that gives developers access to a complete set of vehicle, hardware, software, and cloud data service solutions; as well as an API and codes for obstacle perception, route planning, vehicle control, and operating system. Aimed at democratizing autonomous driving, Baidu hopes Apollo will become \u201cthe Android of the auto industry.\u201d\n\nAlso live from Hong Kong Harbour were a fleet of 81 self-driving boats. A 7.5-meter-long lead vessel led 80 small boats of 1.6 meters in length under the Hong Kong-Zhuhai-Macao Bridge in the a form of arrow.\n\nThe boats were from Yunzhou Tech, a Zhuhai-based company engaged in R&D on self-driving vessels used in water quality monitoring, hydrological mapping, nuclear radiation monitoring and hydrological research. This was the first public appearance of Yunzhou\u2018s self-driving multi-boat collaborative technology. The company employs a robust and adaptive control system, and for navigation uses high-precision GNSS, inertial navigation, and RTK technology.\n\nA \u201cwow\u201d moment occurred when 300 flying drones \u201cleaped\u201d over the Hong Kong-Zhuhai-Macao Bridge in a shape of a dolphin. This was China\u2019s first light show of large-scale 3D stereoscopic drones.\n\nBeijing-based drone company ZEROTECH and Shenzhen-based innovative high-tech company Highgreat jointly developed the technology behind the drone show. Their engineers adopted GPS-RTK carrier phase differential positioning technology to improve accuracy to the centimeter level, and used technology from Baidu\u2019s Apollo to enable the autonomous flight.\n\nAs 2018 is the Year of the Dog, guests at the Spring Festival Gala\u2019s opening were joined by 24 adorable robot dogs who spent the evening dancing, licking paws, wagging tails, and capturing viewers\u2019 hearts.\n\nThe \u201cJimu\u201d robodogs were produced by Chinese robotics startup UBTECH. Jimu has 19 steering gears, over 900 parts, and LED lights as eyes. Steering gears in head, trunk, and limbs simulate canine joint movement.\n\nSince 2011, the night before the big Lunar New Year show has featured a smaller, run-up show known as Network Chunwan (\u7f51\u7edc\u6625\u665a) which celebrates China\u2019s outstanding digital achievements over the preceding year, again incorporating these with on-stage performances.\n\nThe theme of this year\u2019s show was AI. Moderator Xun Zhu (\u6731\u8fc5) said 2017 was a year when \u201cAI has rapidly been developing. The Internet plus AI has become a new mode for benefitting people\u2019s lives.\u201d\n\nThe evening\u2019s opening performance was a piano battle between seven year-old Chinese prodigy Anke Chen (\u9648\u5b89\u53ef) and robot pianist \u201cTeo Tronico.\u201d The bot won the speed play competition, although his 53 fingers clearly gave him an edge over Chen\u2019s 10. Teo Tronic is a proof of concept android built by Italian music and AI fanatic Matteo Suzzi that has previously faced off against Italian classical pianist Roberto Prosseda and Chinese pianist Lang Lang.\n\nNext up was Baidu\u2019s robot Xiaodu, which used audience word suggestions to compose poems and couplets on the spot. AI-powered Xiaodu, who first appeared on the show in 2015, has outperformed humans at multiple tasks on Chinese reality TV shows.\n\nDobot Magician, a robot arm made by Shenzhen-based robotic startup Dobot, showed off his calligraphy skills, writing the characters for \u201cChinese Dream.\u201d First introduced at CES 2017, Dobot Magician can perform 3D printing, laser engraving, grayscale engraving, drawing, sorting, etc.\n\nThe last AI performer was Wangzai, a popular humanoid robot debuted last year by Chinese search engine giant Sogou. Wangzai transformed still photos of show moderators and performers into dynamic, speaking 3D avatars. Wangzai has a wide knowledge base, and can respond to questions within milliseconds. Last February it defeated a Harvard graduate in the Chinese version of the American game show \u201cWho\u2019s Still Standing?\u201d\n\nThe strong focus on AI in China\u2019s most-watched television event of the year was a pleasant surprise for those working in the field, and is bound to encourage many in the country to explore AI and its rapidly emerging technologies."
    },
    {
        "url": "https://medium.com/syncedreview/augmenting-virtual-assistants-with-personality-and-personalization-f5395707d349",
        "title": "Augmenting Virtual Assistants with Personality and Personalization",
        "text": "Humans are social animals, we love talking with one other. Typically, human conversational styles are informed by our personalities, expressed in tone of voice, level of knowledge, sense of humor, and so on. These qualities are now being applied to virtual assistants, the AI-powered human-machine conversational interfaces in our smartphones, homes, and cars.\n\nThe personality of a virtual assistant is defined by the words, tone, style, dialect, and behavior it uses. The goal of any virtual assistant is to establish user trust, engagement, and satisfaction.\n\nSpeaking at the recent RE-WORK AI Assistant summit, Dr. Ann Thyme\u0301-Gobbel, Voice UI/UX Design Leader at Sound United, said that content accuracy is most important virtual assistant trait.\n\nAmazon Alexa\u2019s recent attention-grabbing Super Bowl commercial features a host of Hollywood stars filling in for Alexa, who has \u201clost her voice.\u201d The substitute virtual assistants mess around with users, for example a man who requests a grilled cheese sandwich recipe is mocked by celebrity chef Gordon Ramsay: \u201cIt\u2019s name is the recipe, you #&*$%!\u201d\n\nContent accuracy metrics include accurate responses, correct info, dependability, reliability, low error rate, high comprehension, correctness, and minimal repeating. To meet these demands researchers require robust and reliable technologies in knowledge base, speech synthesis, speech recognition, and natural language understanding.\n\nDr. Thyme\u0301-Gobbel says voice quality ranks second, referencing a study comparing text-to-speech (TTS) synthesis with actual human voices. Although both deliver a similar level of trust, real human voices scored much higher in user enjoyment.\n\nGiven the same level of content accuracy and voice quality, users are more likely to engage with a virtual assistant that is not too chatty, admits when it is uncertain about answers, and is friendly yet professional.\n\nDr. Thyme\u0301-Gobbel says an overly crafted virtual assistant does not necessarily deliver a positive user experience, which is a surprising discovery given that major virtual assistant makers employ storytelling advisors to help compose answers. Google has a team tasked with imagining likely questions and creating responses for developers to code. Microsoft\u2019s Cortana team meanwhile has eight full-time writers brainstorming what Cortana will say and how it will say it.\n\nThe recipe is simple. \u201cLight humor or a quirky sense of humor is great. No jokes unless I ask. Don\u2019t be too eager to please, and avoid being fake friendly,\u201d says Dr. Thyme\u0301-Gobbel.\n\nHowever, even a sense or humor or a strong personality might be inappropriate for virtual assistants in domains such as business, healthcare, education, etc. Virtual assistant makers must understand and stay true to different underlying interaction environments. For example, a healthcare virtual assistant should speak and behave as a trusted doctor or nurse would. Its user expectations and goals are very different from a home virtual assistant whose role is to be an advisor or entertainment content provider grounded in interaction and enjoyment.\n\nDr. Thyme\u0301-Gobbel suggests virtual assistant makers should enable further personalization. She believes users will respond favourably for example if a virtual assistant remembers what they previously said and delivers tailored services using personalized voice characteristics such as tone, accent, rate of speech, access preferences, etc.\n\nWhile personalization can boost user engagement, Dr. Thyme\u0301-Gobbel also warns that \u201cusers don\u2019t want machines to automatically personalize their behaviors.\u201d An inappropriate personalization choice might lower user trust and satisfaction, and negatively affect performance. Also, a user communicating for example with an AT&T customer service bot may not require or desire personalized interaction to get a satisfactory user experience.\n\nDr. Thyme\u0301-Gobbel told Synced that studying the personality and personalization of virtual assistants has helped her dig into the technology behind them. \u201cAs a voice designer person, I have to decide on flows and systems, what to include and what not to include. People care most about having good content and being recognized. You know who I am, so you can be more helpful, like a real person.\u201d\n\nHistorically, most innovative products have evolved from uniform to personalized. Ten years after the Model T Ford was introduced in 1908, the first world\u2019s first mass produced car was available in one colour: black. The automotive industry subsequently spawned a multitude of models with different colours, designs and options to target specific market sectors. It\u2019s a safe bet that a similar broadening of options is about to occur with virtual assistants."
    },
    {
        "url": "https://medium.com/syncedreview/chinas-netease-music-uses-ai-to-win-hearts-518880aee6a3",
        "title": "China\u2019s AI-powered NetEase is Music to Your Ears \u2013 SyncedReview \u2013",
        "text": "China\u2019s music and video streaming unicorn NetEase Music has a database of 10 million songs, 400 million users, and a net worth of over US$1.14 billion.\n\nChinese netizens are all ears for the company\u2019s \u201chearty\u201d AI-powered music recommendations. In an interview with Synced, NetEase Data Scientist Jia Xu and Product Manager Bowen Shen explained the NetEase system, which learns how to predict what songs will resonate with a user\u2019s particular taste in music.\n\nAI recommendation systems are based on the same tech used by news websites, online shopping websites, dating apps, and social media feeds. Consumers generally appreciate a service that learns their preferences and makes accurate predictions, and such systems have become a powerful tool in targeted marketing.\n\nAmazon engineers first put the recommendation systems to work \u2014 if a customer purchased A, they might also want to put B into their basket \u2014 using a common method called collaborative filtering. This works well when supplied with large amounts of user preference data, which NetEase has and continues to accumulate.\n\nNetEase\u2019s collaborative filtering follows two approaches: the first recommends tracks by linking between users who have similar tastes, the second takes a single track as a vector for similarity calculation, and recommends it based on the user\u2019s play history.\n\nIn his blogpost Recommending Music on Spotify with Deep Learning, the creator of Spotify\u2019s music recommendation system and current DeepMind researcher Sander Dieleman explained the shortfalls of collaborative filtering, pointing out that the models are \u201ccontent-agnostic\u201d, have problems dealing with albums with multiple tracks of different styles, and that \u201cnew and unpopular songs cannot be recommended\u201d due to a lack of user data, posing the \u201ccold-start problem.\u201d\n\nTo solve these challenges NetEase turned to deep learning, using input data with features such as song genre, artist, album information, lyrics, beats, user comments, VIP download preferences, and price. The information is projected into a low-dimensional latent space to train deep learning models.\n\nThe system establishes a vector position for each song, encoding all relevant information including user preferences, so the recommendation system can make suggestions even when the song or user have little or no play history. The vectors are \u201cnormalized\u201d to overlook popularity, which ensures that users can also discover and recommend new songs. NetEase also uses a machine learning ranking model to prioritize recommendations on a daily basis.\n\nNetEase is the radio station of the future, a space where even those with the most esoteric tastes can discover new music. Close to half a billion users can\u2019t be wrong."
    },
    {
        "url": "https://medium.com/syncedreview/boston-dynamics-robodog-opens-a-door-owns-the-internet-cded79fae992",
        "title": "Boston Dynamics Robodog Opens a Door, Owns the Internet",
        "text": "SoftBank-owned robot-maker Boston Dynamics is once again wowing the internet. Its latest creation is a robodog named SpotMini which can deftly open doors with its head-mounted gripper arm. The YouTube video Hey Buddy, Can You Give Me a Hand? featuring SpotMini \u201cpolitely\u201d holding a door open for a buddy garnered more than three million views in just 24 hours.\n\n \n\nSpotMini is an upgraded version of its big brother Spot, which was released in 2016. Boston Dynamics says SpotMini\u2019s special skill is \u201cthe ability to pick up and handle objects using its 5 degree-of-freedom arm and beefed up perception sensors.\u201d The electric robodog weighs 25 kg (30kg with the arm), and is \u201cthe quietest robot we have built.\u201d\n\nWildCat holds the fastest free running quadruped robot in the world record speed of 32km/h, not far off the 37.58 km/h that the world\u2019s fastest man Usain Bolt clocked at the 2009 World Athletic Championships in Berlin. \n\n \n\nBoston Dynamics founder Marc Reibart is a robot fanatic who started building hopping machines in 1979 at Carnegie Mellon University. The finished products\u2019 impressive dexterity trailblazed a new generation of robots with robust legs. Reibert moved to MIT in 1986.\n\n \n\nBoston Dynamics was founded in 1992 as an MIT spinoff. The company worked on a series of military projects for the US Department of Defence before being acquired by Google for $500 million in 2013. Last June Google decided not to pursue the unit\u2019s long-term research and it was sold to Japanese conglomerate SoftBank in a high-profile acquisition. \n\n \n\nNo one will deny Boston Dynamics\u2019 marketing success, the company has continually excited the public imagination. An earlier video meant to demonstrate their robots\u2019 balance shows company staff kicking and pushing poor Spot, which is doggedly determined to remain on its feet. This spawned the mocking phrase \u201cBoston Dynamics robot abuse\u201d and a dedicated website selling \u201cStop Robot Abuse\u201d t-shirts.\n\nNetflix meanwhile has turned robodogs from abused to abuser in its sci-fi dystopia series Black Mirror. Writer Charlie Brooker says he found something \u201ccreepy\u201d about Boston Dynamics products, and this inspired the terrifying new episode Metalhead, in which a woman is relentlessly pursued by a human-slaughtering robodog.\n\nThere\u2019s no need to worry about Boston Dynamics\u2019 robodogs becoming homicidal in real life. They are still rarely seen outside the laboratory. But as they continue to learn real-world skills we may think of ways to put them to good use, believing we can always keep them on a leash."
    },
    {
        "url": "https://medium.com/syncedreview/arm-launches-project-trillium-two-ai-processors-2a58cdb783ca",
        "title": "Arm Launches Project Trillium, Two AI Processors \u2013 SyncedReview \u2013",
        "text": "Ninety percent of AI-enabled devices shipped today are based on architecture developed by Arm, a leading UK-based chip intellectual property (IP) provider known for its CPU and GPU processors. In order to scale the impact of machine learning the company today announced Project Trillium, an Arm IP suite that includes a machine learning processor, a object-detection processor, and a library of neural network software.\n\n \n\n Project Trillium is the company\u2019s latest ambitious move in artificial intelligence, a ground-up design to improve the performance and efficiency of AI-enabled devices, which are expected increase in number from 300 million today to 3.2 billion by 2028. \n\n \n\n Arm\u2019s efforts in machine learning can be traced back in 2013 when it began exploring the AI marketplace and made a number of strategic acquisitions. In 2017 the company launched its new Machine Learning Group and named Jem Davies as General Manager. In an exclusive interview, Davies told Synced that in his mind there was \u201cno market segment that wasn\u2019t already or about to be impacted by AI.\u201d\n\n \n\n \u201cAI affects everything\u2026mobile phones\u2026cameras\u2026the little smart speaker\u2026 even thermostats. Who thought of a room thermostat as a smart device?\u201d said Davies. \n\n \n\n The machine learning processor introduced today is Arm\u2019s first-generation AI chip targeting the inferencing of mobile devices. The chip delivers no less than 4.6 trillion operations per second (TOPS) of mobile performance per mm2, with a further uplift of 2x-4x in effective throughput in real-world uses of optimization, and an efficiency of over three TOPS per watt (TOPs/W) in thermal and cost-constrained environments. \n\n \n\n Davies says the architecture behind their machine learning processors is completely new, and results from many years of research. The architecture is optimized around 16 integer arithmetic.\n\n \n\n The new architecture will provide a great solution for challenges that CPUs and GPUs struggled with, says Davies. \u201cConvolutional Neural Networks are very common. One of the things is that a traditional architecture, whether CPU, GPU or DSP, is going to involve a lot of intermediate result/data storing and loading from memory. So we have produced a completely new architecture with an intelligent memory system.\u201d\n\n \n\n The object detection processor is an iteration based on Arm\u2019s existing IP family: Spirit, the object detection accelerator that powers the Hive security camera. It was released in 2016 soon after Arm acquired Apical, a company that provides computer vision and imaging processors for over 1.5 billion devices. \n\n \n\n Arm\u2019s second-generation processor can detect virtually unlimited numbers of objects in real time with Full HD at 60fps. Its detailed people model provides rich metadata and enables detection of direction, trajectory, pose and gesture.\n\n \n\n Arm provides an integrated solution comprising machine learning processors and object detection processors. In real-time object recognition tasks, the object detection processor will first isolate areas of interest such as faces. The machine learning processor will then be able to analyze fewer pixels for a faster, fine-grain result.\n\nArm\u2019s neural network software library is a collection of building blocks for imaging, vision and machine learning workloads. Developers can use the software with Arm\u2019s existing implementation tools such as Compute Library to accelerate their algorithms and applications, or CMSIS-NN to maximize performance at the edge. The library supports mainstream frameworks such as TensorFlow and Caffe, and is optimized for Arm Cortex CPU, Mali GPU, and new machine learning processors.\n\n \n\n Arm machine learning processors will be delivered to partners this summer, and object detection processors will be available by the end of this quarter."
    },
    {
        "url": "https://medium.com/syncedreview/googles-tpu-chip-goes-public-in-challenge-to-nvidia-s-gpu-78ced56776b5",
        "title": "Google\u2019s TPU Chip Goes Public in Challenge to Nvidia\u2019s GPU",
        "text": "Google announced this morning that its Tensor Processing Unit (TPU) \u2014 a custom chip that powers neural network computations for Google services such as Search, Street View, Google Photos and Google Translate \u2014 is now available in beta for researchers and developers on the Google Cloud Platform.\n\nThe TPU is a custom application-specific integrated circuit (ASIC) tailored for machine learning workloads on TensorFlow. Google introduced TPU two years ago, and released the second generation Cloud TPU last year. While the first generation TPU was used in inferencing only, the Cloud TPU is suitable for both inferencing and machine learning training. Built with four custom ASICs, Cloud TPU delivers a robust 64 GB of high-bandwidth memory and 180 TFLOPS of performance.\n\nBefore it opened its TPUs to the public, Google had widely implemented them internally. AlphaGo \u2014 the Google AI masterpiece that beat human champions in the ancient Chinese board game Go \u2014 used 48 TPUs for inferencing.\n\nCloud TPU provides a great solution for shortening the training time of machine learning models. Google Brain team lead Jeff Dean tweeted that a Cloud TPU can train a ResNet-50 model to 75% accuracy in 24 hours.\n\nWhen Cloud TPU was announced, Google offered 1000 free devices for machine learning researchers. Lyft, the second-largest ride-hailing company in the US, has been using Cloud TPUs in its self-driving systems since last year. Says the company\u2019s Head of Software Self-Driving Level 5 Anantha Kancherla, \u201cSince working with Google Cloud TPUs, we\u2019ve been extremely impressed with their speed \u2014 what could normally take days can now take hours.\u201d\n\nAlfred Spector, CTO of New York City-based hedge fund Two Sigma, says, \u201cwe found that moving TensorFlow workloads to TPUs has boosted our productivity by greatly reducing both the complexity of programming new models and the time required to train them.\u201d\n\nGoogle\u2019s Cloud TPU is currently only in beta, offering limited quantities and usage. Developers can rent Cloud TPUs at the rate of US$6.50/Cloud TPU/hour, which seems a reasonable price considering their great compute capability.\n\nGoogle also released several model implementation tools to save developers\u2019 time and effort writing programs for Cloud TPUs, including ResNet-50 and other popular models for image classification, a transformer for machine translation and language modeling, and RetinaNet for object detection.\n\nWhile Google is not directly selling its TPU chips to customers at this stage, their availability represents a challenge to Nvidia, whose GPUs are currently the world\u2019s most-used AI accelerator. Even Google has used large numbers of Nvidia GPUs to provide accelerated cloud computing services. However if researchers switch from GPUs to TPUs as expected, this will reduce Google\u2019s dependency on Nvidia.\n\nLast year, Google boasted that its TPUs were 15 to 30 times faster than contemporary GPUs and CPUs in inferencing, and delivered a 30\u201380 times improvement in TOPS/Watt measure. In machine learning training, the Cloud TPU is more powerful in performance (180 vs. 120 TFLOPS) and four times larger in memory capacity (64 GB vs. 16 GB of memory) than Nvidia\u2019s best GPU Tesla V100.\n\nAlthough it\u2019s too early to crown the Cloud TPU as the AI chip champion, the announcement of its availability has sparked excitement among researchers, and marks the beginning of Google\u2019s ambitious move into the space of AI accelerators."
    },
    {
        "url": "https://medium.com/syncedreview/safety-regulations-for-self-driving-cars-in-america-4dbe5ae44440",
        "title": "Safety Regulations for Self-Driving Cars in America",
        "text": "California has hosted public road testing for more than 180 self-driving vehicles from 27 companies. Carmakers Audi, BMW, Mercedes, Volvo, GM, Ford, Honda, Toyota, Fiat-Chrysler, and Tesla; technology companies Uber, Lyft, Google\u2019s Waymo, and Apple; and software solution providers Mobileye (Intel), Nvidia, AutoX, and Drive.ai are all using the Golden State\u2019s roadways to perfect their products. It\u2019s getting crowded out there, and in response there are regulations.\n\nSafety is the priority. In a 2017 American Automobile Association (AAA) survey, 78% of respondents said they would not want to ride in a self-driving car due to safety concerns.\n\nLast September the National Highway Traffic Safety Administration (NHTSA), an agency under the US Department of Transportation, introduced a framework called Automated Driving Systems 2.0: A Vision for Safety in an effort to make self-driving cars safe. The US House of Representatives had months earlier passed the Self Drive Act, which regulates autonomous cars from production to testing and distribution under NHTSA supervision.\n\nThe NHTSA also adopted the six levels of autonomy classification scheme proposed by the Society of Automotive Engineers in 2016 for self-driving vehicles:\n\nLevel 1&2 self-driving cars use an Advanced Driver Assistance System, otherwise known as ADAS, to assist the human driver. Levels 3\u20135 rely on an Automated Driving System, otherwise known as ADS.\n\nMost autonomous driving systems on roads today are Level 1&2, while some self-driving test vehicles can reach Level 3. In its November 2017 road test in Arizona, Google\u2019s Waymo made the leap to Level 4 by removing the human driver. (Here we need to keep in mind that a car capable of Level 4 self-driving on Arizona roads may not be able to perform to even a Level 3 standard in India for example, where traffic is more chaotic.)\n\nThe biggest challenges facing self-driving systems are sensing, perception, and control. Studies in SLAM (simultaneous localization and mapping) technology and computer vision are expected to solve the first two challenges in the short term. As for control, the hardware is also not a problem. There is however the problem of unpredictable human drivers. To enable systems to make decisions intelligently, existing data is still far from sufficient to deal with all driving situations.\n\nWhat how much data is required to guarantee safety? The more the better.\n\nStatistically speaking, the average American driver will encounter a deadly accident after one hundred million miles of drivetime. And so even though Waymo for example has logged more than 1.3 million miles of road testing, this is still not nearly enough to give a statistically meaningful estimation of how safe the autonomous car would be.\n\nOther possibilities for improving self-driving vehicle safety include installing RFID tags on roadways or at critical intersections for vehicle-to-infrastructure communication to help the autonomous cars to perceive the environment. Audi has also proposed using LED lights in the front windshield to notify pedestrians when the self-driving car \u201csees\u201d them, so the pedestrian knows it is safe to cross the street.\n\nOf course, an ideal safety solution would be for the many different autonomous driving research teams to simply share their data. Alas, the market is too competitive and the stakes too high to reasonably expect that to happen."
    },
    {
        "url": "https://medium.com/syncedreview/alibaba-city-brain-goes-rural-ai-pig-farming-in-sichuan-1cd226ab6405",
        "title": "Alibaba City Brain Goes Rural: AI Pig Farming in Sichuan",
        "text": "Slogans such as \u201cRise Early to Farm Pigs, Call AI to Help!\u201d and \u201cExcel in Intelligent Pig Farming, Marry a Pretty Wife Early!\u201d are appearing on walls across China\u2019s southern countryside provinces. The campaign is part of the Sichuan pig farming corporation Dekon Group and pig feed supplier Tequ Group\u2019s new partnership with Alibaba Cloud to apply its AI-powered \u201cET Brain\u201d to pig farming. The trio are investing tens of millions of USD in the project, which was announced on February 6, 2018.\n\n \n\nOver the past year, Alibaba has implanted its ET Brain in the aerospace, transportation, environment, and healthcare sectors, fast-tracking China\u2019s social infrastructure revolution. Recent food contamination scandals have made food safety a pressing issue in China, and the agricultural industry a next logical application for ET Brain. \n\n \n\nChina\u2019s pork production accounts for more than half of the world supply, while its per capita pork consumption ranks 3rd. By 2020, Tequ Group sales will exceed 10 million tons, while Dekon will breed up to 10 million pigs annually. This is an opportunity for artificial intelligence to optimize operations, and both companies are actively building their IoT and Enterprise Resource Planning (EPR) systems. \n\n \n\nOn pig farms each pig wears a wireless radio-frequency identification (RFID) tag. These are pricey and difficult to scan, making and farmers must individually log data into mobile applications or fill in paper forms.\n\n \n\nThis is where computer vision and voice recognition AI can help. Real-time video footage is collected through surveillance cameras. Using computer vision, ET Brain will set up profiles for each pig, documenting their breed, age, weight, eating conditions, exercise intensity and frequency, and movement trajectory. The first phase of the launch includes functions such as herd behavior analysis, inventory count, health monitor, and automatic weighing.\n\nOne challenge is telling pigs apart \u2014 Alibaba considered applying facial identification to pigs to no avail. Instead they tattooed identifying numbers on the pigs\u2019 bodies.\n\n \n\nET Brain tracks the entire production line. Based on behavior tracking, gilts are selected for mating. After they give birth to piglets, usually in a litter of ten, ET Brain will use voice recognition to ensure the little ones are not suffocated by their mothers\u2019 weight. This lowers death rate by 3%, increasing annual production rate by three piglets per sow.\n\nAside from breeding, feeding and weighing, other important steps in pig farming are disease control and epidemic monitoring. ET Brain will analyze pigs\u2019 behavior, acoustic characteristics and infrared temperature measurements, to determine the health status of pigs, targeting epidemic early warning signs and specialized vaccinations.\n\n \n\nAlibaba Cloud has dispatched a team of algorithm engineers, product developers, and video analytics team to Sichuan work on the project, while Tequ will add experts on pig farming. \n\n \n\n\u201cOur core solution is to reduce the reliance on farmers and dependence on equipment through automated video analytics,\u201d explains Alibaba Cloud\u2019s Sheng Zhang, who added that the use case is highly replicable. \n\n \n\nAlibaba\u2019s AI solutions have thus far been more widely deployed in urban environments. Its \u201cET City Brain\u201d focuses on improving China\u2019s urban infrastructure using capabilities such as voice, image, text recognition, and natural language processing. Earlier last month, the company announced it will deploy ET City Brain in the Malaysian capital of Kuala Lumpur. \n\n \n\nHowever, it is also worth considering the appropriateness of AI applications for lesser developed areas. Today, 43% of the Chinese population lives in the countryside. Granting this rural economy access to AI is a difficult but important task, especially for underdeveloped industries like pig farming that have limited access to new technology."
    },
    {
        "url": "https://medium.com/syncedreview/volvos-roadmap-to-digitalization-7989dfc92a94",
        "title": "Volvo\u2019s Roadmap to Digitalization \u2013 SyncedReview \u2013",
        "text": "Each year, the average car owner spends 293 hours driving, 90 hours parking, and dozens of hours on fuel-ups and car washing. Swedish automaker Volvo is freeing up that time with Volvo Concierge Services, \u201cthe first expandable digital ecosystem that connects car owners with convenience services via a smartphone App.\u201d\n\nThe Volvo Concierge Services platform enables users to smoothly coordinate with third-party providers to book services such as on-the-spot fuel-ups or car washing. When users request a Concierge Service, the provider will automatically receive the vehicle\u2019s location, preferred service time, and a single-use digital key for access to the car.\n\nConcierge Services was launched in 2016 as a feature of \u201cVolvo on Call\u201d, a smartphone app that offers remote assistance and other solutions. Volvo boasted it was the first and only automaker to address this drivers\u2019 pain point and penetrate a space with huge potential: American drivers fill up their tanks over 44 million times each day.\n\nThe platform has received positive feedback thus far, says Volvo Cars Senior Business Developer of Cloud and IT Kristoffer Gronowski. Concierge Services\u2019 adoption rate is now 6%, while the retention rate is an extremely high 53%. Gronowski is confident the company can scale the service to reach 30% of US Volvo owners.\n\nConcierge Services is one of many steps Volvo has taken to ramp up its tech game. The company is also betting on self-driving technology, and is committed to producing fully autonomous highway-ready vehicles by 2021.\n\nLast November, Volvo reached a US$300 million deal with Uber to provide the ride-hailing giant with 24,000 flagship XC90 SUVs incorporated with all necessary safety, redundancy and core autonomous driving technologies. This represents Volvo\u2019s largest single vehicle order, and the largest in the autonomous vehicle industry\u2019s history. Uber will develop a self-driving system based on Volvo\u2019s vehicles, and assemble a commercial fleet for the San Francisco Bay Area.\n\nVolvo\u2019s aggressive moves and embracing of innovation seems something of a new path for the traditionally conservative automaker. Volvo Head of Asia Pacific Product & Offer August Wu says the company is working flat out to differentiate its brand from competitors: \u201cWe are designing new ways of thinking and innovating fast.\u201d\n\nThe shift is also evidenced in Volvo\u2019s recent hires. The company has lured multiple executives from outside the automotive sector, such as Chief Digital Officer Atif Rafiq, who built McDonald\u2019s digital unit from scratch.\n\nIn 2016 Volvo established a Silicon Valley-based research center with 80 engineers focused on emerging technologies, digital development and R&D. The center, located across the street from Google\u2019s offices, also coordinates joint efforts with local tech companies and startups \u2014 facilitating the Uber-Volvo partnership for example.\n\nVolvo\u2019s Silicon Valley center worked with Google to co-develop an Android-based in-car infotainment system offering Google applications. Android Auto will be available on new Volvo models in the two years, powering the main touchscreen displays and digital dashboard, and adding new services such as Google Assistant.\n\nLast year Volvo acquired San Francisco-based car valet and concierge startup Luxe \u2014 valued at US$140 million according to Pitchbook \u2014 and incorporated it into the Silicon Valley center. Luxe Co-founder Curtis Lee is now Volvo\u2019s VP of Digital.\n\nLuxe has quickly integrated with Volvo\u2019s digital products, especially Concierge Services, to provide a robust, algorithm-driven logistics and services platform to support Volvo consumer services. The Luxe platform now features mobile applications, real-time location-based monitoring software, a workforce management platform, a dispatch engine, and a algorithm-driven ETA (Estimated Time of Arrival) service.\n\nThe automotive industry will undergo revolutionary changes over the coming years. Volvo\u2019s ambitious moves in this space are informed by a tech roadmap that\u2019s all about innovation, digitalization, and staying ahead of the pack."
    },
    {
        "url": "https://medium.com/syncedreview/google-clouds-jia-li-on-automl-s-first-30-days-430d4e487ee0",
        "title": "Google Cloud\u2019s Jia Li on AutoML\u2019s First 30 Days \u2013 SyncedReview \u2013",
        "text": "Who is using AutoML Vision, the \u201csimple, secure and flexible ML service that lets you train custom vision models for your own use cases\u201d that Google launched last month? We asked Jia Li, Head of R&D of Cloud AI and Senior Director at Google.\n\nLi says AutoML now has some 10,000 registered users, ranging from startups to big corporations. She divides potential clients into three types, \u201cthe first is AI-savvy companies with loads of data, they need infrastructure such as Tensorflow to train their own models; second is companies with limited expertise and data, they can choose API without training their own models; third is companies with limited expertise, but with ideas and data, that want to build their own models. AutoML can help them customize their models by simply inputting their images without marking training data, designing algorithms, or tuning parameters.\u201d\n\nAutoML users can drag-and-drop to upload images, no AI technical expertise is required. AutoML does this with a combination of three core technologies \u2014 neural architecture search technology, learning2learn, and transfer learning \u2014 which automate the process of selecting the right networks to use, finding hyperparameters for best performance, and applying the model to different use cases directly from Google Cloud.\n\nLi explains: \u201cTransfer learning is easy because it generates models in seconds. Learning2learn has a higher cost with no fixed architecture, and it takes up to a day to generate models. But even this is shorter than traditional ways of training.\u201d\n\nGoogle reports on their blog that \u201cearly results using Cloud AutoML Vision to classify popular public datasets like ImageNet and CIFAR have shown more accurate results with fewer misclassifications than generic ML APIs.\u201d AutoML\u2019s codes run better than those written by engineers; while for labeling objects in an image AutoML achieves 42% accuracy compared to man-made models\u2019 39%.\n\nLi identifies retail and medical imaging as major use cases. In the month since AutoML\u2019s launch Google has consulted with many potential clients in the clothing industry. Clothing with the same color or pattern will have for example different necklines, cuffs etc. Retailers can use AutoML to define their own custom product classifications for such features.\n\nThe debut version of AutoML only supports computer vision models, but features such as speech, translation, video, and NLP are coming soon.\n\nFei-fei Li and Jia Li have said that Google\u2019s mission is to \u201cdemocratize AI\u201d by providing companies who can\u2019t afford their own AI talent a chance to hop on the fast track.\n\nWhile AutoML\u2019s emergence is good news for businesses, one fear is that the tech giants\u2019 cheap and generalized solutions will raise the bar for startups and reduce market penetration opportunities for their solutions.\n\nThe industry has so far responded favourably to AutoML. AI engineers are mostly happy that it eliminates laborious parameter tuning procedures. Businesses meanwhile can now introduce AI into their operations without the high cost of hiring AI engineers and data scientists. Google\u2019s AutoML boasts a growing client list that includes Urban Outfitters, Disney, and the Zoological Society of London and it is likely that Google and Microsoft will continue to expand in this space."
    },
    {
        "url": "https://medium.com/syncedreview/program-lead-jake-lussier-on-udacitys-new-flying-car-nanodegree-7aae00c64c41",
        "title": "Program Lead Jake Lussier on Udacity\u2019s New Flying Car Nanodegree",
        "text": "Is the flying car merely a sci-fi clich\u00e9, or is it a viable transportation tool for the future? One way to find out is to develop talents to drive the concept to market \u2014 and that\u2019s what Udacity is doing with its new Flying Car Nanodegree Program.\n\nOnline education platform Udacity thrilled flying car enthusiasts with its announcement of the world\u2019s first flying car program. Applications are open until February 7, and the course will start at the end of February.\n\nThere are two terms: the fundamental-level Aerial Robotics and the advanced-level Intelligent Air Systems. Each term costs US$1,200 and runs for three months, with approximately 15 hours of weekly study.\n\nUdacity has specific prerequisites for beginners \u2014 admission requires good programming skills and basic math and physics skills. Graduates of the Udacity Robotics Software Engineer, Self-Driving Car Engineer, or Intro to Self-Driving Cars Nanodegree programs automatically qualify.\n\nUdacity has secured computer science and aerospace gurus as instructors, including Udacity Founder Sebastian Thrun, MIT Professor Nicholas Roy, University of Toronto Professor Angela Schoellig, and Zurich Federal Institute of Technology Professor Raffaello D\u2019Andrea.\n\nSynced sat down with Udacity Flying Car Nanodegree Lead Jake Lussier to delve into the program\u2019s details and the current state of flying car technologies in general.\n\nLussier: I think Udacity\u2018s higher level mission is to provide lifelong learning, and this is just a way of saying that we try to provide a learning experience that will help you keep your professional skills up-to-date so that you can continue to succeed and grow in your career.\n\nA flying car might sound very far out, but it follows the same model that we already see the existing aviation industry and with the booming drone industry. There is a great opportunity for students who have skills and autonomy to get jobs now, and we see that there\u2019s a lot of activity right now in flying cars and over the next few years it\u2019s really going to ramp up. So students who learn this stuff now are going to be really sought after.\n\nLussier: In essence, we\u2019re preparing our students to be very confident software engineers prepared to work in autonomy, in any part of flight and in flying cars specifically.\n\nIn the first term, Aerial Robotics really teaches the fundamentals of what one would need to develop any kind of robot, but we offer this specialization in the sky. There is planning, controls, and estimation. The simulation will be a drone in the sky and students will have the opportunity to port their code to an actual drone in flight.\n\nIn the second term, we go more into the flying cars specifics, and preparing for a world in which there will be a lot more of these vehicles. We will go into fixed wing flight, and we\u2019ll also talk about hybrid designs. Flying car design has not really converged on one design and has elements of quadcopter and elements of fixed-wing vertical-take-off-and-landing (VTOL) aircraft. We first cover fixed wing designs and how this relates to what they learned in the first term. Then we\u2019ll go a bit more into aircraft, how do the control problems change, how is stability different, and then we get into autonomous fleets.\n\nThen it gets into optimizing missions. If you want to deliver things at multiple places, you have to learn how to optimize to do that effectively. And then when you have entire fleets, how do you coordinate them all so that they operate efficiently, and everything works safely with any central authorities that they\u2019re talking to. So that\u2019s kind of the overarching trajectory of the curriculum.\n\nAt every stage the student will not just be learning in the classroom, they\u2019ll also be coding in the classroom, writing Python and getting immediate feedback, and then translating that code into C++ so it goes from code that works well just for understanding the concept, to code that is aircraft ready. Then you can take that C++ code and that\u2019s what you would actually potentially put on a drone.\n\nOverall, they will have an understanding of all these concepts as well as competency implementing these concepts. For the existing industry today, they could bring their skills and autonomy. They\u2019ll be positioned to really be leaders in that space.\n\nLussier: At the highest level, these students are going to be very skilled, with experience in building autonomous systems that are not only intelligent but also extremely reliable and robust. If you\u2019re doing image classification on the web, you will be familiar with training algorithms to just predict very well. If you get it very wrong. it\u2019s not a huge deal. But in flight it\u2019s a huge deal if you get it wrong. So it\u2019s not just a simple algorithm but you have to have a system point of view. All of our graduates will have that skill, which is pretty widely applicable.\n\nI think they can find places even in the financial industry where you have to really understand your predictor at a very high level. They can find a lot of roles with those skills. But then in flight specific, most planes fly autonomously for most of their time in this sky. It is really only take off and landing where a human is heavily involved. There are obviously a lot of fully autonomous planes. There\u2019s already a huge market of companies that could find use for these skills.\n\nThen there are the smaller but rapidly growing markets, where they\u2019ll have an opportunity to get leadership positions and really build out those systems. Drones is already a booming industry that was growing very rapidly, and the flying car industry although a bit smaller right now is going to have a similar kind of growth pattern.\n\nLussier: At a high level it requires a pretty good level of experience in some programming language. You need to be comfortable coding, preferably in Python, C++. You should also have a basic understanding of a bit more advanced mathematics, for example linear algebra probability statistics. A bit about physics since we\u2019ll cover aerodynamics slightly.\n\nIf you have not graduated from other programs but you have those skills, then in our admission process, you can explain your qualifications and we will review every application individually and just make sure that you have the skills necessary.\n\nLussier: There are a number of startups. For example, Kitty Hawk, which was founded by [Udacity Founder] Sebastian Thrun; Volocopter, which is testing autonomous air taxis in Dubai right now. There is Lilium out of Germany, Ehang in China, and Terrafugia is another one. The Aurora Flight Sciences, Joby. There\u2019s more, this is not an exhaustive list.\n\nAlso a lot of the larger companies are either developing these internally themselves or they are developing an acquisition. Aurora Flight Sciences for example was acquired by Boeing. Airbus acquired Bombardier Jet, and Volvo acquired Terrafugia.\n\nThere is a growing space of service providers in that industry. For example Iris Automation raised a Series A and they do collision avoidance using computer vision and AI.\n\nUBER Elevate is Uber\u2019s effort to develop a system of users who want to fly from point A to point B. Any flying car maker can enter, and Uber will provide the passengers.\n\nSo there are the flying car makers, component developers, and user platforms out there.\n\nLussier: One surprising thing is that the basic aerodynamics and the hardware components are largely there. The low level software is mostly there. In some ways, they\u2019re even easier than self driving cars because they don\u2019t have to deal with all the obstacles and people jumping out and randomness. I think this is something that gives us confidence that this actually won\u2019t take that long to get out there because the technology is largely there.\n\nThe remaining challenge is engineers who are competent at the intersection of aerospace in computer science, which is exactly why we\u2019re offering this course right now. We traditionally have a pool of aerospace engineers who are not very comfortable with computation, and are used to a slower iteration on products. And we have computer scientists who are more used to running machine learning on the web and not as comfortable running on real world robotic systems.\n\nOnce you have those engineers who are fluent in both, there are additional challenges in terms of not just getting a single vehicle to fly stably but to have a whole host of vehicles going 120 miles per hour in the sky. This kind of higher level systems thinking and system optimization is still a challenge today.\n\nLussier: Machine learning can be used in most of the process. A lot of flight does not permit black box, therefore we can\u2019t just get some training set, train a model and then hope that the performance is sufficiently good. On lower level control problems, we require that we have a physical model and that the performance of the vehicle abides by that model.\n\nBut then, a lot of these models have parameters so we can measure the efficacy of the vehicle by its desired path. Whenever you have that kind of setup where you have parameters, this always allows for machine learning where you can optimize those parameters over time based on your measure of success.\n\nOnce we move beyond that very low level stuff, when we look at the entire system, this is an optimization problem in the sky. You have a set of resources, and you have some objective that you want to maximize and then you use your data to try to minimize some error function. All levels of this flying car stack, we can use machine learning and AI to try to do a lot better than we might do just using simple programs in simple physical models.\n\nLussier: Safety is the utmost concern and will really impact public perception of this technology. It needs to be implemented in a way that is safe and also has a super strong track record. So what\u2019s important for the industry is to start having use cases where we\u2019re delivering real value to customers. When we show that value, people will appreciate as we do that it\u2019s safe, and they will gain trust in it.\n\nThe other challenges you speak to are definitely when you\u2019re getting things up into the sky. It\u2019s important to do so efficiently and if it\u2019s a really heavy load then it\u2019s going to require a lot of energy and battery, you would have to take that into consideration. That will be more a matter of battery technology progress tackling those problems where the value to the customer can be met by a solution that leverages the available battery technology.\n\nLussier: An analogous situation would be that of the the car industry where there is a very mature car market increasingly using intelligence. If you\u2019re a career-seeking engineer, there is a big market of car companies right now, and the self-driving car is high-growth.\n\nWith flying cars, we have a similar setup where there is an established aerospace market, and there is a smaller but rapidly growing drone and flying car industry.\n\nThe self driving car five years ago was in the space where the technology was starting to be taken more seriously but it was a very niche industry. But it was high growth. The flying car right now is in a similar kind of position in that we\u2019re in the early days of it but it\u2019s going to be high growth. The flying car has this nice advantage over the self driving car at that time, because the self driving car has carved out the path technically and socially for a lot of the progress that the flying car will have to make in terms of the AI, navigation, mapping, and coordination.\n\nSo, in the next one or two years, there\u2019s going to be a huge amount of growth, and we will see a lot more mass adoption over the next five years."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-american-startup-partnership-promises-a-self-driving-electric-suv-by-2020-8a54d516de47",
        "title": "Chinese-American Startup Partnership Promises a Self-Driving Electric SUV by 2020",
        "text": "Chinese electric car startup BYTON says it will produce a self-driving SUV by 2020. The bold pledge came with the announcement of a partnership with Aurora, a Silicon Valley-based autonomous driving startup founded by Google, Tesla and Uber veterans.\n\nThe deal will incorporate Aurora \u2018s Level 4 autonomous driving solutions into BYTON\u2019s new electric vehicles, which can drive without human intervention in most conditions. Over the next two years, BYTON will deploy 100\u2013200 electric vehicles equipped with Aurora software in San Francisco for road testings and data collection.\n\nAurora will provide BYTON with its self-driving hardware kit, including requirements and integration guidelines for preparing the vehicle platform for self-driving. Aurora will also provide the self-driving software and data services required to operate the system.\n\nBYTON is a subsidiary of Nanjing-based Future Mobility. The company lured former BMW executive Carsten Breitfeld as its CEO, and has assembled a team with rich experience in car design and production.\n\nAt CES 2018 in Las Vegas earlier this year BYTON unveiled their concept car \u2014 a US$45,000 electric SUV featuring a breathtaking 1.25-meter in-car panorama screen and rotating front seats. BYTON cars equipped with a premium Advanced Driver Assistance Systems (ADAS) solution will be available in 2019 in China and 2020 in the US and Europe.\n\nBYTON chose Aurora to keep drivers\u2019 hands off the steering wheel with its expertise in L4/L5 autonomous vehicle solutions. The company was founded in 2016 by Chris Urmson, former head of Alphabet\u2019s autonomous driving affiliate Waymo; Sterling Anderson, who led the launch of Tesla Model X and directed Tesla Autopilot; and Drew Bagnell, who led Uber\u2019s Advanced Technology Center.\n\nAurora created a buzz in the industry earlier this year when it announced collaborations to develop onboard self-driving systems with Volkswagen AG and Hyundai Motor Co. The latter plans to mass produce L4 autonomous vehicles by 2021.\n\n\u201cBYTON is designed for the age of autonomous driving. We are pleased to partner with Aurora, as Aurora is supremely focused on a mission to deliver the benefits of self-driving vehicles safely, quickly and globally,\u201d says Breitfeld.\n\nThe BYTON-Aurora agreement covers pilot deployment in the US market, over the course of which both parties will explore broader application scenarios for Aurora\u2019s self-driving system and BYTON\u2019s production series vehicles.\n\nAlthough BYTON is a newcomer in the field of electric cars, it is catching up with existing automakers, and taking a brave step forward with its autonomous driving SUV project."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-february-pt-1-bf96d9ebb861",
        "title": "AI Biweekly: 10 Bits from February (Pt 1) \u2013 SyncedReview \u2013",
        "text": "January 21st \u2014 Amazon Go Opens in Seattle\n\nThe staffless Amazon Go opens to the public in downtown Seattle. Just like a normal convenience store, this intelligent store sells pre-cooked food, snacks, drinks , etc. \u2014 but there\u2019s no cashier scanning the goods. Integrating an AI system and computer vision technology, Amazon Go uses weight detecting sensors and hundreds of ceiling-mounted cameras to monitor stock flow. Once customers check in with their ID, they can literally grab what they need and go.\n\nJanuary 22nd \u2014 Paris Is Becoming an AI Tech Center\n\nParis is attracting attention from tech giants. Facebook and Google both plan to increase investment in labs and talent recruiting in the French capital. Facebook announced it will double the number of employees at its Paris AI research center to 100; while Google will create an AI lab focusing on fundamental research such as automatic learning, NLP and computer vision.\n\nJanuary 22nd \u2014 Intel Providing VR Content for Motorsports\n\nIntel is determined to ride the rising wave of AI. In the ten months since the company created its dedicated Artificial Intelligence Products Group lead by Naveen Rao, Intel has secured several development deals in AI-driven immersive experiences for sports, including the NFL and now Motorsports. With a VR headset, viewers will be able to choose their own camera feeds for the upcoming Ferrari Challenge North America series.\n\nJanuary 22nd \u2014 How RBC Uses AI in Decision Making\n\nThe Royal Bank of Canada (RBC) is using artificial intelligence technologies in their capital-markets research to detect the effect of social media sentiment on corporate brands (Chipotle Mexican Grill, etc). By using AI to quantify the measurement of effectiveness, a company can track sentiments resulting from marketing strategies, etc. A company\u2019s earnings, sales predictions and analysis can benefit from the research, as can trading and investestment strategies.\n\nJanuary 24th \u2014 iFLYTEK\u2019s Translator and the Intelligence Behind It\n\nChinese tech company iFLYTEK releases a pocket-size instant translation device called Speak to Me. Based on advances in deep learning inferencing, the portable translator is accelerated by the NVIDIA Tesla P4 and P40 GPUs. iFLYTEK\u2019s performance data shows 97% accuracy and efficiency 15 times higher than CPUs. This technology will not only be used for translation, iFLYTEK is also collaborating with Chinese hospitals and medical institutions to improve patient experience\n\nJanuary 26th- Africa Will Hold an AI Expo 2018\n\nArtificial Intelligence Expo Africa 2018 will be held this September in South Africa. The event aims at promoting AI business opportunities. More than 400 delegates are expected to attend the Expo, which will feature 27 speakers along with 28 AI exhibitors. There will also be an innovation cafe housing 20 AI startups. The Expo comprises six themes and three tracks emphasizing real enterprise AI case studies and applications.\n\nJanuary 29th \u2014 Nvidia Collaborate with BHGE on AI for Oil and Gas\n\nGE\u2019s Baker Hughes and Nvidia are bringing an AI-based end-to-end solution to their business customers. This new solution will leverage advanced GPU and Supercomputer hardware and deep learning technologies to locate and maximize returns on resources. The project will also include maintenance, predication and operation functions for the oil and gas industry.\n\nJanuary 31st \u2014 Sony\u2019s Immersive Gaming Experience at SXSW\n\nSouth by Southwest (SXSW) is one of the largest creative festivals in the US, and Sony is expected to demonstrate its new immersive gaming technology at the upcoming Austin, Texas event. According to a leak, Sony will exhibit AR Air Hockey, which combines the classic arcade table game with augmented reality; and Hero Generator, a VR generator that enables visitors to create their own personalised VR avatars. Sony will also showcase other immersive experiences such VR soccer, etc.\n\nFebruary 1st \u2014 Amazon Bans Google Feature in Alexa\n\nWith the competition between virtual assistants Alexa and Google Assistant heating up, Amazon removes the command \u201cGoogle\u201d from the Alexa skillset. Alexa app developers will no longer be able to have Alexa respond to the verbal command \u201cGoogle\u201d. Amazon says the reason for the prohibition is that Alexa \u201cshould not promote Google Home.\u201d\n\nFebruary 2nd \u2014 Microsoft Launches the Cortana Intelligence Institute in Australia\n\nMicrosoft announces a collaboration with the Royal Melbourne Institute of Technology (RMIT) to establish the Cortana Intelligence Institute. RMIT researchers will collaborate with Microsoft personnel to apply AI to challenges that remain unsolved by neural networks. For specialization and cooperation, the Cortana Institute comprises two separate groups: A 1000-strong Microsoft Research division; and Cortana Research, a younger team focusing on virtual assistant technology."
    },
    {
        "url": "https://medium.com/syncedreview/spotlight-on-eight-african-ai-startups-3f25d650440d",
        "title": "Spotlight on Eight African AI Startups \u2013 SyncedReview \u2013",
        "text": "According to the World Economic Forum, \u201cno part of the planet is urbanizing faster than sub-Saharan Africa. The continent\u2019s population of roughly 1.1 billion is expected to double by 2050.\u201d Other factors such as improved power supply and high-speed internet, more early-stage investment funding, and increase in smartphone use have created a fertile environment for Africa\u2019s emerging AI industry.\n\nLeading countries include Kenya and Ethiopia in the East, Nigeria and Ghana in West Africa, South Africa in the South, and Egypt in the North.\n\nSynced has identified eight exciting startups bringing AI to Africa\u2019s education, finance, and health revolutions.\n\nThe Egyptian startup Affectiva was launched in 2009 by Rana El Kaliouby, a pioneer in Emotion AI and former MIT research scientist. Affectiva\u2019s \u201cemotion recognition products\u201d draw on the company\u2019s extensive database to detect moods and make decisions based on facial expressions. More than 1,400 healthcare, automotive and gaming brands use its AI emotion technology. In 2016 the startup raised US$14 million in investment to attain US$34 million in venture capital, and in 2017 was cited in Forbes\u2019 10 Hottest Artificial Intelligence Technologies list.\n\nDataProphet is a South Africa-based startup founded in 2013 by Daniel Schwartzkopff, a graduate in Chemical Engineering from the University of Cape Town, and scenario planner Frans Cronje. It applies AI services to finance and insurance, with specialization in predictive analytics and advanced machine learning algorithms. The company has secured investment from YellowWoods Capital and has a sales office in San Francisco.\n\nKudi is a Nigerian fintech startup founded in 2017 that provides access to electronic banking and financial services by leveraging conversational interfaces, NLP, and AI technologies. Users can transfer money, track account details, and pay bills using its chat interfaces on Facebook, Telegram, Slack, etc. Kudi is also piloting business-to-business solutions with banks and telecommunication companies across Nigeria, with plans for expansion across Africa.\n\nThis Nigerian Educational startup matches qualified tutors with students according to area and budget. Tuteria tutors must maintain a user good rating to win more clients and compensation, and falling below a specified minimum level leads to teaching disqualification. Founder Godwin Benson is a Systems Engineering graduate from the University of Lagos, Nigeria.\n\nThe Ugandan fintech startup was launched in 2015 and offers tech solutions to microfinance institutions. Its platform helps digitize business procedures, credit information sharing, and many other services using mobile devices or biometric SaaS (software-as-a-service). Awamo has secured investments from the German Investment and Development Corporation, and plans to expand its networks across East Africa.\n\nAerobotics is a South African startup based in Cape Town, launched in 2014 by Benji Meltzer and James Paterson. The company uses machine learning to analyze maps and extract actionable information for crops such as wheat, citrus, and sugar cane. Its farming consultation services are used in South Africa, Australia, and the UK. Aerobotics participated in Startupbootcamp InsurTech\u2019s accelerator programme in London.\n\nThis South African startup creates AI-powered virtual advisors to drive sales and perform technical consultations. Clevva\u2019s platform enables companies to monitor and manage the virtual advisers they use to advise staff and/or customers. The startup\u2019s innovations attracted attention in the Microsoft BizSpark Program and at the Gartner Symposium ITxpo Africa 2015.\n\nFinChatBot was launched in June 2016 by Far Ventures. The company builts chatbots for client websites. It also monitors industry trends to increase sales conversion rates, predict customer needs and suggest business solutions. FinChatBox is based in Cape Town, South Africa."
    },
    {
        "url": "https://medium.com/syncedreview/woebot-delivers-a-daily-dose-of-mental-wellness-d8e30e2666a4",
        "title": "Woebot Delivers a Daily Dose of Mental Wellness \u2013 SyncedReview \u2013",
        "text": "Psychological well-being impacts humans\u2019 ability to enjoy their lives, but most people can\u2019t recognize mental health warning signs unless they consult a professional psychologist. A Silicon Valley-based startup has now put the process into an AI-powered chatbot.\n\nWoebot is designed to identify, evaluate and treat mental health conditions such as depression, anxiety, and stress disorders based on Cognitive Behavioral Therapy (CBT), one of the most widely used mental health treatment frameworks. Woebot\u2019s recently released IOS app features an animated robot character that communicates with users using natural language processing (NLP) technology.\n\nWoebot Labs was founded by Stanford School of Medicine Psychologist Alison Darcy with the goal of making mental health evaluation accessible and even entertaining. Woebot is the first AI system to compete for a share of the mental health market, which in 2015 totaled $196 billion in the US alone.\n\n\u201cWe now understand that you need to look after your physical fitness everyday. I think people are reaching the understanding that actually mental health is something that you should also look after and actively seek self-care opportunities,\u201d says Dr. Darcy.\n\nWhile Woebot Labs is an early-stage startup, the company has garnered much attention in the AI community since AI guru and Coursera co-founder Andrew Ng joined Woebot\u2019s board as Chairman, assisting Dr. Darcy with growing the team, and advising on data and machine learning strategies.\n\n\u201cAndrew was telling me that he doesn\u2019t get involved in things unless he thinks they can be potentially the biggest contribution of his life. That\u2019s a huge bonus for us, and a very nice thing that he thinks we can be that impact,\u201d says Dr. Darcy.\n\nWoebot functions like a mental health mentor. It prompts users to check in daily, then asks about their temper, activities for the day, motivations and frustrations and so on. The questions are inspired by CBT, whose methodology is to treat mental illness by changing unhelpful patterns in behavior, emotional self-regulation, and thoughts.\n\nThe chatbot also encourages users to try new activities such as meditation, which is a common CBT therapeutic strategy. Nick Panchyshyn, co-Founder of smart life management and AI Assistant startup LifeMap, gave Synced a positive review of his Woebot experience. \u201cI enjoyed a lot of conversations that were really engaging. Woebot was also offering me different activities in a very nice manner.\u201d\n\nTo enable automated responses that are both accurate and natural, Woebot uses a combination of structured dialog along with natural language understanding (NLU). Woebot Head of Engineering Joe Doyle says the team uses both deep learning and NLU to understand users\u2019 moods and activities. Based on their mood, Woebot can guide users to appropriate content.\n\n\u201cWe utilize the FastText algorithm for some classification as well as applying some internally developed solutions based on popular machine learning frameworks such as TensorFlow. We also rely on many different techniques ranging from the use of simple Regular Expressions all the way to complex, deep neural networks,\u201d says Doyle.\n\nWoebot is now engaged in more than two million conversation per week, and Dr. Darcy has discovered interesting behavior patterns, for example, in how users express distorted thoughts, such as \u201cI\u2019m not smart enough\u201d or \u201cI shouldn\u2019t have said that.\u201d\n\nWoebot includes quizzes designed to educate users about cognitive distortions and distorted thoughts, and help them identify negative words and correct cognitions. \u201cThat is why NLP is going to be so fundamental to transform mental health care,\u201d says Dr. Darcy.\n\nLast June, Dr. Darcy and her colleague Kathleen Kara Fitzpatrick published a randomized trial of Woebot in the Journal of Medical Internet Research. They recruited 70 people who self-reported depression or anxiety, and had them either use Woebot or read a mental health e-book for two weeks. Test subjects using Woebot experienced a higher reduction in depression then the book group.\n\nThis certainly does not mean Woebot is effective enough to replace professional therapists. John Torus, Chair of the American Psychiatric Association\u2019s Smartphone App Evaluation Work Group, said in a Washington Post interview that \u201cthese things can work well on a superficial level with superficial conversations. Are they effective tools, do they change outcomes and do they deliver more efficient care? It\u2019s still early.\u201d\n\nDr. Darcy does not want to exaggerate Woebot\u2019s capabilities. \u201cWoebot is a guide and that is a legitimate role in the field of mental health. Woebot is not going to be able to deliver you therapy but he can certainly ask you the right questions to help people figure out things on their own.\u201d\n\nWoebot still needs improvement: it remains only an IOS application, has relatively poor conversational capabilities and a narrow spectrum of clinical skills. But Dr. Darcy is dreaming big: \u201cI would like Woebot to become a household name, known as a reliable effective helper that you can reach out to at any time in daily life.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/the-aaai-2018-keyword-is-learning-d0aefcccde3a",
        "title": "The AAAI 2018 Keyword is \u201cLearning\u201d \u2013 SyncedReview \u2013",
        "text": "AI academics from around the world are pouring into New Orleans for the Thirty-Second AAAI Conference on Artificial Intelligence. The five-day event is an all-inclusive technical overview of AI research frontiers. Program chairs are Sheila McIlraith from the University of Toronto and Cornell \u2018s Kilian Weinberger.\n\nSince its inception in 1979 as the \u201cAmerican Association of Artificial Intelligence,\u201d the annual gathering has expanded scope to encourage international participation. In 2012 the conference was first held outside the US, in Toronto, Canada; and last year organizers shifted the schedule to avoid conflicting with the Chinese New Year and better accommodate Chinese academics.\n\nCompared to other prestigious conferences like the IJCAI, ICML, and NIPS, the AAAI takes a broader approach with its topic selection, which includes search, planning, knowledge representation, reasoning, NLP, robotics and perception, multiagent systems, statistical learning, and deep learning.\n\nThe AAAI 2018 selection committee received a record high of over 3,800 paper submissions, accepting 933 for an acceptance rate of just under 25%, down about 5% from last year when 789 papers made it out of 2590. The number of AAAI paper submissions has risen steadily since 2013.\n\nA word-frequency search of the papers reveals this year\u2019s top keywords: (in descending order) learning, multi, neural, deep, networks/network, adversarial, attention, model, data, detection.\n\nMeanwhile, keywords \u201cdata\u201d and \u201cinformation\u201d have decreased in use frequency compared to last year.\n\nAAAI 2017 registered a high of 1,692 participants, and we are bound to see an increase this year.\n\nOrganizers identified Human-Computer Interaction (HCI) as an Emerging Topic this year. There are four talks and 21 technical papers on HCI, focused on trustable and explainable AI, teamwork/team formation, human-aware planning and behaviour prediction, planning and decision support for human-machine teams, human-agent negotiation, human-robot/negotiation, human-robot/agent interaction, human-in-the-loop/learning, human computation, and human and AI communication protocols.\n\nThe AAAI\u2019s first Oxford-style debate on \u201cAdvances in Machine Learning have displaced the need for logic in AI\u201d is expected to ignite discussion in the Twitterverse. Debating \u201cfor\u201d will be former AAAI chair Tom Dietterich and Bart Selman from Cornell, while Gary Marcus from NYU and IBM researcher Francesca Rossi will argue \u201cagainst.\u201d\n\nEarlier this month, Dietterich responded to Marcus\u2019 Critique of Deep Learning with a volley of contentious Tweets. Their first public debate will certainly be fiery.\n\nAAAI 2018 will also present the first AAAI/ACM Conference on AI, Ethics, and Society.\n\nThe AAAI-18 Outstanding Paper Award was won by the University of Alberta\u2019s Memory-Augmented Monte Carlo Tree Search, which proposes a new method called Memory-Augmented Monte Carlo Tree Search (M-MCTS), combining the original MCTS algorithm with a memory framework, to provide a memory-based online value approximation. Performance was evaluated in the game of go, showing better results than vanilla MCTS.\n\nPaper co-author Jincheng Mei says their next application scenario for MCTS \u201cis complex reinforcement learning tasks, for example in strategy games.\u201d Another of the co-authors, Dr. Martin M\u00fcller, told Synced, \u201cwe are also applying MCTs in the game of Hex, with PhD student Chao Gao and Dr. Ryan Hayward.\u201d It\u2019s noteworthy that Dr. M\u00fcller mentored David Siler and Aja Huang, the first and second authors of AlphaGo\u2019s celebrated Nature paper.\n\nThe Outstanding Student Paper award went to Oxford University\u2019s Counterfactual Multi-Agent Policy Gradients, which features a new reinforcement learning method that can efficiently learn decentralized policies in cooperative multi-agent systems. The multi-agent actor-critic method called Counterfactual Multi-Agent (COMA) was evaluated in StarCraft games, which is a challenging reinforcement learning benchmark task.\n\nFebruary 4\n\nRao Kambhampati (Presidential Address) \u2014 Challenges of Human-Aware AI Systems\n\nYejin Choi \u2014 From Naive Physics to Connotation: Learning and Reasoning about the World using Language\n\nFebruary 7\n\nPercy Liang \u2014 How Should We Evaluate Machine Learning for AI?\n\nAI and the Web (AIW)\n\nApplications (APP)\n\nCognitive Modeling (CM)\n\nCognitive Systems (CS)\n\nComputational Sustainability and AI (CSAI)\n\nGame Theory and Economic Paradigms (GTEP)\n\nGame Playing and Interactive Entertainment (GPIE)\n\nHeuristic Search and Optimization (HSO)\n\nHuman-AI Collaboration (HAC)\n\nHuman-Computation and Crowd Sourcing (HCC)\n\nHumans and AI (HAI)\n\nKnowledge Representation and Reasoning (KRR)\n\nMachine Learning Applications (MLA)\n\nMachine Learning Methods (ML)\n\nMultiagent Systems (MAS)\n\nNLP and Knowledge Representation (NLPKR)\n\nNLP and Machine Learning (NLPML)\n\nNLP and Text Mining (NLPTM)\n\nPlanning and Scheduling (PS)\n\nReasoning under Uncertainty (RU)\n\nRobotics (ROB)\n\nSearch and Constraint Satisfaction (SCS)\n\nVision (VIS)"
    },
    {
        "url": "https://medium.com/syncedreview/ces-2018-autonomous-driving-tech-review-464c56fee78a",
        "title": "CES 2018 Autonomous Driving Tech Review \u2013 SyncedReview \u2013",
        "text": "The term \u201cautonomous driving\u201d arrived in the vernacular at CES 2013, when Audi and Toyota unveiled their self-driving cars. Over the following 12 months major car manufacturers rushed to announce their own autonomous development blueprints or self-driving car prototypes. Google searches for the synonymous term \u201cself-driving\u201d increased sixfold from 2013 to 2017 \u2014 with peak search times corresponding with each CES.\n\nAt CES 2018, 555 companies or organizations participated in the \u201cAutomotive/Vehicle Technology\u201d category, 23% of which were companies using autonomous driving technologies. Featured products can be roughly divided into six categories: Sensors, Perception and Decision, Control, Human-Machine Interaction, V2X, and Miscellaneous.\n\nSensors are any devices which can provide the vehicle system with raw signals. Generally speaking, this includes GPS, IMU, camera, LiDAR, radar, thermal camera, and so forth. At CES 2018, 23% of the autonomous driving related companies were sensor technology companies, half of which were from Israel. Most of showcase releases were in the LiDAR and thermal camera fields.\n\nOne of the most biggest challenges in core autonomous driving application is perception, which refers to the ability to draw relevant conclusions from the raw sensor data using data processing, data calibration and fusion, scenario reconstruction, localization, and obstacle detection. Because chipsets are the key hardware for data collection and processing in perception, chip manufacturers have also been included in this category.\n\nDecision is the process of making choices based on known data. For example, to help the vehicle take action when data indicates there is an obstacle on the road. At 2018 CES, 33% of autonomous driving related companies were in the perception and decision category, with 20 focused on perception and 22 on decision. Hot topics this year were hardware processor chipsets and Lyft\u2019s Aptiv project.\n\nControl refers to the ability to identify and process decisions into real vehicle actions. For example, if there is a decision ordering the vehicle to brake, control will determine how to activate the brake and steering functions to give passengers the best driving experience. At CES 2018 there were only 12 companies in this category, less 10% of all self-driving related companies. Star attractions at CES 2018 included Xpeng\u2019s G3 SUV 360\u00b0 camera system, and Nissan\u2019s IMx SUV.\n\nHMI is the human-machine interface that connects an operator to the controller in a computer system. This comprises a variety of services and functions to support autonomous driving, for example, voice assistants, smart alarms, and in-car interfaces. There were 16 companies working in this area at CES 2018. Among the most interesting showcases was Autolabs\u2019 virtual assistant project and Steering AI\u2019s smart alarm system.\n\nV2X (Vehicle-to-Everything) indicates the communication system between a vehicle and anything else. The category includes V2I (Vehicle-to-Infrastructure), V2V (Vehicle-to-Vehicle), V2P (Vehicle-to-Pedestrian), V2D (Vehicle-to-Device) and V2G (Vehicle-to-Grid). Fifteen V2X companies released blueprints or new products at CES 2018. Especially noteworthy were Qualcomm\u2019s V2X network and RoadEyes\u2019 resSMART camera.\n\nVarious autonomous driving technologies cannot be pigeonholed into the above five categories. Baidu\u2019s self-driving development platform Apollo is one of them. Apollo 2.0 is a major release that not only changes the way autonomous driving technologies are developed but also forms a worldwide ecosystem for this technology.\n\nAmerican companies remained major players in the self-driving field at CES 2018, accounting for just under 30% of the total. Most were from California, suggesting the next autonomous driving wave will most likely emerge from Silicon Valley. Meanwhile, over 40% of sensor companies were Israeli.\n\nCES 2018 data also indicates strong growth for self-driving tech companies in Asia, as more Japanese, South Korea and Chinese companies are entering the market. In China dozens of startups and high-tech companies have shown their capabilities in this area.\n\nBased on trends at CES 2018, it\u2019s easy to see that competition is heating up in the autonomous driving industry, and more investment will be poured into this area. However, such investment may focus on specific breakthrough technology rather than the general self-driving field. LiDAR and Autonomous Driving Processor Chipsets could be two major battlegrounds. Moreover, based on the various partnerships announced at CES, we can envision self-driving competition evolving to the ecosystem level. Dozens of companies aim to have their full set self-driving mass production vehicles on the road by the end of 2018 or early 2019.\n\nAlthough autonomous driving technology has been under development for some time now, 2018 may be the first year that self-driving vehicles actually appear in our everyday lives."
    },
    {
        "url": "https://medium.com/syncedreview/will-new-eu-regulations-starve-data-hungry-deep-learning-models-25403795d26c",
        "title": "Will New EU Regulations Starve Data-Hungry Deep Learning Models?",
        "text": "The European Union\u2019s new General Data Protection Regulation (GDPR) may outlaw some AI algorithms and data collection practices, warns University of Washington Professor Pedro Domingos, author of the seminal AI introduction The Master Algorithm.\n\nThe GDPR will come into effect on May 25, 2018, replacing the 1995 Data Protection Directive. The regulation aims to \u201charmonize data privacy laws across Europe, to protect and empower all EU citizens data privacy and to reshape the way organizations across the region approach data privacy,\u201d according to the GDPR website, bringing Europe \u201cthe most important change in data privacy regulation in 20 years.\u201d\n\nWhat exactly will change? First, the GDPR applies to all companies processing the personal data of EU citizens regardless of company location. Second, breaching the GDPR will carry stiff penalties, with companies facing fines of up to 4% of their global turnover, or \u20ac20 million. Third, companies will be required to procure customer consent in clear and comprehensible language, allow easy user withdrawal from data collection, and notify local EU Data Processing Authorities of data usage.\n\nEU citizens will be entitled to have their data erased or transferred to another processing agent, and must be notified immediately if their data is misused.\n\nThe new regulation will be detrimental to data mining operations and AI startups, and big tech companies will be scrutinized for using web browsing cookies or social media profiles. Deutsche Bank estimates the GDPR could erase 2% of Google\u2019s revenue.\n\nAlong with its regulations on the use of private data, the GDPR also imposes adverse provisions on scientists and researchers who are deploying machine learning algorithms, especially in deep learning.\n\nSection 4, article 22 of the GDPR requires a \u201cright to explanation\u201d for users on all decisions made by automated or AI algorithm systems. According to the context, a data subject has the right to opt out of a decision made by automated processing or AI that produces legal effects on them, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention.\n\nThe GDPR mandates that all customers or users are entitled to receive \u201cspecific information\u201d regarding the decision. This raises the issue of AI interpretability, especially in deep learning systems. An Oxford Internet Institute research paper predicts the GDPR\u2019s right to explanation will directly impact machine learning.\n\nResearchers and data scientists regard the interpretability of AI as very challenging, especially given that recent advancements in AI are based on deep learning and deep reinforcement learning. The most common fundamental algorithms used in these models learn their objective functions and adjust weights automatically as the learning proceeds via iterative epochs based on huge amounts of labelled data.\n\nResearchers do not know how the learned weights achieve the best result, and it is difficult to determine what kinds of learning the model has achieved. Accordingly, such systems are referred to as \u201cblack-box\u201d mechanisms wherein humans provide the data, the model, and the architecture \u2014 and then the computer outputs the answer.\n\nProf. Domingos\u2019 \u201cmaking deep learning illegal\u201d tweet ignited a heated debate on AI and the GDPR. Some criticized him for misrepresenting the regulation. Eric Topol, an renowned American cardiologist, replied \u201cthat kinda makes human intelligence \u2014 which is unexplainable \u2014 illegal by inference.\u201d\n\nDirector of Product Marketing for Cloudera Data Science Thomas Dinsmore blogged that the GDPR\u2019s adverse impact on deep learning might be exaggerated because a right to explanation has already existed for years and the GDPR is simply expanding the scope and applying such rules to automated processing. \u201cThe need to deliver an explanation affects decision engines but need not influence the choice of methods data scientists use for model training,\u201d wrote Dinsmore.\n\nThe premise and definition of the GDPR\u2019s \u201cright to explanation\u201d is also opaque. Oxford researchers expressed doubts in a January 2017 paper, noting several issues regarding the regulation\u2019s feasibility, such as \u201cit lacks precise language as well as explicit and well-defined rights and safeguards against automated decision-making, and therefore runs the risk of being toothless.\u201d\n\nFor their part, machine learning scholars have over the last two years put increasing focus on the interpretability of AI technologies. IJCAI 2017 held a special workshop on Explainable Artificial Intelligence (XAI); while NIPS 2017 hosted a debate on whether interpretability is necessary for machine learning.\n\nOther efforts in academia hint at the possibility of unveiling the black box\u2019s mysteries. The Defense Advanced Research Projects Agency (DARPA) initiated an XAI program in order to turn the \u201cblack-box\u201d into a \u201cglass-box\u201d so that a \u201chuman-in-the-loop\u201d would know the reasoning behind decisions. Local Interpretable Model-agnostic Explanations (LIME) is a more well-known study aiming to explain black-box algorithms by starting from simpler models in the field of object recognition and text classification.\n\nMeanwhile, many practical AI applications do not demand interpretation. Machine translation for example has been revolutionized by AI technologies, and users are much more concerned with the translation results than how they were generated.\n\nDespite the alarmism from Prof. Domingos and some others in the AI community, the GDPR will not outlaw deep learning. It will however impact big data collection and usage with the goal of protecting users\u2019 privacy, and may even encourage researchers to find a way to open the black box and look inside."
    },
    {
        "url": "https://medium.com/syncedreview/geoffrey-hinton-on-images-words-thoughts-and-neural-patterns-82db0bd04a09",
        "title": "Geoffrey Hinton on Images, Words, Thoughts, and Neural Patterns",
        "text": "The \u201cGodfather of AI\u201d Professor Geoffrey Hinton told a packed room of professionals and students today that machine learning is reinventing our relationship with our own thoughts. The Professor mused on AI, deep learning, and the nature of intelligence in his 60-minute talk at the University of Toronto.\n\nProfessor Hinton has been contemplating the functioning mechanism of brains for decades. In the talk he asked the question \u201cWhat is a thought?\u201d His response: \u201cIt is a big neural activity pattern. We refer to it by using a symbol string that causes it, but the way we refer to it is quite different from what it is.\u201d\n\nThe Professor comes from an illustrious family of forerunners and scientists. His great-great-grandfather George Boole came up with Boolean logic, which made modern computing possible; his great-grandfather was a mathematician; and his father\u2019s cousin Joan Hinton was a nuclear physicist and one of the few women scientists who worked for the Manhattan Project in Los Alamos.\n\nProfessor Hinton rose to fame at the 2012 ImageNet object recognition challenge, which comprised one million high-resolution training images of 1,000 different classes of objects. For the task, Hinton\u2019s team designed AlexNet, an eight-layer network with five convolutional layers, which performed an impressive 10% better than the University of Tokyo runners-up.\n\nProfessor Hinton\u2019s success applying convolutional neural networks to pixels was soon repeated with words, revolutionizing the field of machine translation, which had been a nagging problem for symbolic AI. The artificial neural network method \u201cconverts the input sentence into a big pattern of neural activities that is language independent. The pattern is a thought vector, which will be converted into a sentence in the target language,\u201d explained Professor Hinton.\n\nIn order to deal with sequential language, text data is fed into an encoder recurrent neural network (RNN) and a decoder RNN. This is the method Google Translate uses to translate some 100 billion words daily. The rise of neural networks has been good news for machine translation, but, as Professor Hinton quipped, \u201cvery bad news for linguists like Chomsky who insist that language is innate and you can\u2019t learn it through data.\u201d\n\nProfessor Hinton also spoke on the human thinking process and analogical reasoning, and philosophized on the nature of our sensory data \u2014 whether all we see, hear and even think could be represented in ways different from how we understand them to be. \u201cSentences can evoke thoughts but they are nothing like thoughts. An array of pixels can evoke a scene, but our representation of a scene is not an array of pixels.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/andrew-ng-launches-175-million-ai-fund-9d4373b04bbd",
        "title": "Andrew Ng Launches $175 Million AI Fund \u2013 SyncedReview \u2013",
        "text": "As one of the top minds in artificial intelligence, Andrew Ng wears many hats: scientist, entrepreneur, educator, advisor, etc. Now he\u2019s added a new one: investor.\n\nThe former Google and Baidu Chief Scientist and Coursera co-founder today announced the foundation of \u201cAI Fund,\u201d aimed at supporting startups that can transform businesses with AI technologies. AI Fund has thus far raised a staggering US$175 million from VC firms NEA, Sequoia, Greylock Partners, and the SoftBank Group.\n\nNg is AI Fund\u2019s General Partner, while former Fenwick & West partner Eva Wang will serve as COO, bringing considerable operational and legal expertise. The fund\u2019s third partner is Coursera Product Manager and former Sycamore CEO Steven Syverud, who has rich experience in business development.\n\nNg says AI Fund will not operate like traditional venture capital companies that \u201cscreen thousands of applicants in order to find a few to invest in.\u201d Instead it will proactively identify promising AI talents and help them build companies from scratch. If the company evolves a viable AI-based system and business model, AI Fund will provide capital for further scaling. \u201cBuilding AI businesses is a more systematic and repeatable process than most people think,\u201d says Ng.\n\nA similar AI-development methodology can be found at Element AI, a Montreal-based platform founded in 2016 that incubates advanced AI-First solutions. Last year, the company raised US$102 million in Series A funding for hiring talent, developing businesses, and investing in platform-specific AI solutions.\n\nOne of AI Fund\u2019s first investments will be in Landing.ai, Ng\u2019s own recently launched startup which accelerates AI implementation in manufacturing. AI Fund is currently exploring several other AI-powered initiatives which it expects to announce over the next few weeks.\n\nAI Fund\u2019s philosophy will permit portfolio companies to operate in stealth mode, enabling them to conceal their core technologies, upcoming products and business models from competitors\u2019 prying eyes.\n\nLast year Ng gestated two important AI companies \u2014 Landing.ai, and Deeplearning.ai, which offers online courses in machine learning. AI Fund is the latest step toward Ng\u2019s goal of building a supportive ecosystem for AI opportunities, projects and products."
    },
    {
        "url": "https://medium.com/syncedreview/alibabas-cloud-computing-division-alibaba-cloud-announced-today-that-its-ai-powered-et-city-ea482ac7e286",
        "title": "Kuala Lumpur Gets Alibaba AI\u2019s First Brain Implant \u2013 SyncedReview \u2013",
        "text": "Alibaba\u2019s cloud computing division Alibaba Cloud announced today that its AI-powered \u201cET City Brain\u201d will be deployed in the Malaysian capital Kuala Lumpur in partnership with the Malaysia Digital Economy Corporation (MDEC), the country\u2019s digital economy development agency, and Kuala Lumpur City Hall (DBKL). The system uses real-time video collected by street cameras and aims for traffic optimization based on machine learning and cloud computing. The initial phase of the project will be installed at 281 intersections.\n\nThis is the first known use of AI in Malaysian urban planning and governance, and ET City Brain\u2019s debut outside China. \u201cMalaysian government officials are very open-minded and rational, they are open to new endeavors, and we have great admiration for them,\u201d said Alibaba Cloud President Simon Hu, who also said he believes Malaysia is on the fast track of digitalization compared to other Southeastern countries such as neighbour Singapore.\n\nKuala Lumpur traffic is a serious problem \u2014 suburban commuters can spend up to two hours navigating the morning rush hour. On a visit to Alibaba\u2019s Hangzohu headquarters last May, Malaysian Prime Minister Najib Razak expressed interest in smart city technology, inviting Alibaba CEO Jack Ma to make it viable in Malaysia. Half a year later the project has landed in Kuala Lumpur.\n\n\u201cWith its massive cloud computing and data processing capabilities, City Brain can optimize the flow of vehicles and traffic signals by calculating the time to reach intersections. It will also be able to generate structured summaries of data, such as traffic volume and speed in particular lanes, which can be used to facilitate other tasks including incident detection,\u201d according to an Alibaba press release.\n\n\u201cIn addition, City Brain can connect with various urban management systems including emergency dispatch, ambulance call, traffic command, and traffic light control. By integrating and analyzing real-time data generated from these systems, City Brian can optimize urban traffic flow such as by identifying the quickest route for emergency vehicles to arrive at the scene within the shortest time frame.\u201d\n\nAlibaba Cloud concurrently launched Malaysia Tianchi, a local arm of its (Sky Pool) Big Data Program \u2014 a crowd-intelligence platform designed to find solutions to real-world problems. The global platform now has over 120,000 developers, and is used by 2,700 academic institutions and businesses in 77 countries. It\u2019s hoped that Malaysia Tianchi will cultivate up to 500 homegrown data professionals and spin off 300 startups over the next two years.\n\nOperations will be based on the Alibaba Cloud Data Center in Kuala Lumpur, which opened in October 2017 and provides cloud services to local enterprises. Alibaba has already granted Cloud Training & Certification (ACP) credentials to 100 local operators.\n\nWhile Malaysia marks Alibaba Cloud\u2019s first international real-world application, it won\u2019t be the last. Speaking at the opening ceremony, an optimistic Hu predicted, \u201cIn five years, global operations will contribute 40%-50% of Alibaba\u2019s total revenue.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/trio-of-chinas-big-state-banks-invest-in-4paradigm-ai-72f444ad5e8b",
        "title": "Trio of China\u2019s Big State Banks Invest in 4Paradigm AI",
        "text": "January 26th, Beijing-based AI firm 4Paradigm has announced strategic investments from the state-owned Industrial and Commercial Bank of China, Bank of China and China Construction Bank; making it the only AI startup to get funding from three major Chinese state-owned banks. 4Paradigm had previously completed an A round with Sequoia Capital China, A+ round with Sinovation Ventures, and B round with Genesis Capital and Zhongwei Capital. \n\n \n\nThe AI industry is changing, as many companies now prioritize securing client orders over winning AI competitions. 4Paradigm has followed this strategy since the 2016 introduction of its large-scale distributed machine learning platform \u201cProphet.\u201d To date, Prophet has helped clients in the finance industry boost performance in verticals like targeted client-booking, personalized recommendation, application anti-fraud, transaction anti-fraud, overdue or loss warning, liquidity management, smart collection, and disposal of non-performing assets.\n\n \n\nNow the company is upgrading Prophet and taking a more comprehensive approach to developing full stack solutions for enterprises.\n\nCEO Wenyuan Dai tells Synced, \u201cWe have finished many industry benchmark cases in the past few years. We have worked with dozens of big banks, in some cases boosting performance by more than 100%. This attracted the attention of state-owned banks, who hope to develop long-term cooperative relationships with us. After this round of investment, we will expedite AI product development and provide AI solution plans in China\u2019s banking, insurance, and securities industry, so that eventually all enterprises and users can enjoy the added value of AI.\u201d \n\n \n\n4Paradigm is now building customized AI brains for its clients, which can potentially spawn hundreds or even thousands of AI applications. Guangdong Development Bank, for example, has already built ten different AI applications based on 4Paradigm\u2019s core systems. The company\u2019s solutions also allow other AI companies to build on its systems. Banks are benefiting from bundled solutions provided by different startups, streamlining the cooperation process. \n\n \n\n\u201cWe started by working on vertical domains, like anti-fraud systems and targeted marketing because initially, no big banks would delegate core businesses to a startup. After three hard years, banks are handing us their trusted \u2018credit cards\u2019. Given this trust, we will work on full upgrades progressively,\u201d explains Dai. \n\n \n\nCEO Dai is a respected transfer learning expert with who served as the principal researcher at Huawei Noah\u2019s Ark Lab. Company co-founder Qiang Yang is the Chair Professor at HKUST in Hong Kong, an ACM fellow, and the founding head of Noah\u2019s Ark Lab which has published over 400 papers in the field. \n\n \n\n4Paradigm\u2019s staff expanded from just a few people to some 300 last year, with the product development team particularly well represented. The company also focused on accumulating data and building multiple algorithm models in 2017; and opened Paradigm University to fast-track the education of its own AI talent using the Prophet platform. Each data scientist will coach several students, providing a robust talent pipeline for the company. \n\n \n\n Read here for more 4Paradigm AI use cases."
    },
    {
        "url": "https://medium.com/syncedreview/chinese-rideshare-unicorn-didi-launches-ai-lab-in-beijing-f71ae7c847ca",
        "title": "Chinese Rideshare Unicorn DiDi Launches AI Lab in Beijing",
        "text": "China\u2019s ridesharing unicorn DiDi Chuxing unveiled its \u201cDiDi AI Labs\u201d today in Beijing. The new AI lab expands on the DiDi Research Institute and DiDi Labs, and will cluster over 200 AI scientists and engineers to \u201cimprove user experience and transportation efficiency and further enhance [the] intelligent transportation ecosystem,\u201d according to a company press release. Didi AI Labs will be led by company Vice President Professor Jieping Ye.\n\nDiDi manages some 20 million daily rides and serves 400 million active users. The company sits on an impressive valuation of US $56 billion, and in 2016 bought Uber China\u2019s operation for US $7 billion, further consolidating its domestic rideshare monopoly.\n\n \n\nDiDi Labs focuses on big data and smart traffic and conducts AI research on NLP, computer vision for operations research, deep learning, statistics, etc. Research cases include predicting user destination, suggesting pick-up points, intelligent order dispatching, estimated time of arrival (ETA), and routing optimization.\n\nAI Labs head Jieping Ye is a University of Michigan professor and a specialist in machine learning, data mining and analytics, and specializing large-scale sparse model learning. In an April 2017 lecture at Peking University, Prof. Ye said DiDi had accumulated so much data that \u201cif you do the model, sample size can easily go up to billions.\u201d DiDi Rideshare vehicles submit updated GPS data every few seconds, processing 2000 terabytes each day.\n\n \n\nAt the lab launch DiDi also showcased its \u201cDiDi Smart Transportation Brain\u201d, a tech solution for smart city traffic management integrated with local traffic authorities that has already been adopted by more than 20 Chinese cities. Smart Traffic Signals installed at 344 road intersections in the city of Jinan have saved 30,000 hours of travel time and shortened traffic delays by 10\u201320%. The system is now deployed at 1,200 intersections nationwide. \n\n \n\nDiDi Labs is becoming increasingly active and visible in global AI. Last year it published the paper A Taxi Order Dispatch Model based On Combinatorial Optimization at leading international data conference KDD, participated in computer vision conference CVPR, and was a NIPS Platinum Sponsor."
    },
    {
        "url": "https://medium.com/syncedreview/tech-giants-talk-conversational-ai-at-re-work-bc6dcffb0c40",
        "title": "Tech Giants Talk Conversational AI at RE\u2022WORK \u2013 SyncedReview \u2013",
        "text": "Automating human-like conversations is much more difficult than you might imagine. Even the world\u2019s top conversational AI systems \u2014 Amazon Alexa, Apple\u2019s Siri, and Google Assistant \u2014 remain far from the stage where they can smoothly process unanticipated human requests while also keeping humans engaged with natural responses.\n\nAt the RE\u2022WORK AI Assistant Summit San Francisco yesterday, research scientists and engineers from Amazon, Apple and Google spoke on how they are addressing challenges and evolving their conversational AI systems.\n\nFirst at the podium was Google AI Research Engineer Dr. Pararth Shah, who described a dilemma in developing a conversational AI bot: on one hand, a rule-based bot, which researchers code with each new capability, has low recall in unanticipated interactions and no self-learning capability; on the other hand, a research-focused bot whose model is trained from data encounters the double challenge of costly dataset collection and annotation, and diminished control over the bot\u2019s behavior.\n\nTo achieve both control and flexibility at scale, Dr. Shah proposed using self-play to build conversational AI. Researchers would first create both a user simulator bot and a rule-based bot. Given a specific task scenario, for example a restaurant date chat or booking a movie ticket, the two bots would talk to each other for five minutes.\n\nTheir conversation would be translated into natural (colloquial) dialogue by crowdsourced human labor, then added to an existing neural net-based bot training dataset, aka Supervised Learning. The neural net-based bot would then re-engage with the user simulator and be given reward/punishment based on its performance, aka Reinforcement Learning. Finally the bot could converse with flesh-and-blood users to get real feedback, aka Interactive Reinforcement Learning.\n\nThe initial version of Siri\u2019s NLU was a rule-based system wherein researchers started with vocabulary maps and external knowledge bases for features, rule-based bottom-up tree traversal of the query to compose an intent, and intent rankings enabled by hand-coded weights. The system was deterministic and interpretable, and could easily handle unambiguous requests. However, researchers found it difficult to add new functionalities or improve accuracy.\n\nSiri then proceeded to the next level, from rule-based to machine learned. The new iteration revamped how researchers designed each functionality. For example, Siri researchers reformulated the ranking problem of domain chooser as a classification problem, and adopted Support Vector Machines (SVMs), a sort of supervised learning model.\n\nResearchers deployed two different strategies \u2014 tree structured parses and shallow parses \u2014 for parsing, a process that analyzes a string of natural language or computer language symbols in accordance with grammatical rules. Researchers discovered that the majority of Siri requests required only shallow parses, which was more accurate, faster to train, and more suited to producing annotations compared to tree structured parses. Researchers also began to apply a statistical modeling method Conditional Random Fields (CRFs) to parsing.\n\nTo support their new machine learned models, Apple researchers designed a better UI interface for annotators and developers, a training and prediction system to evaluate impact of new or edited examples, deployments of models to runtime servers, and metrics that indicate performances.\n\nAs deep learning blossomed over the last five years, Siri researchers realized it could deliver better performance than traditional machine learning methods, especially in data-intensive conditions. They switched their previous models from SVMs for domain chooser and CRFs for parsing to Long Short-Term Memory (LSTM) \u2014 a type of Recurrent Neural Network capable of learning long-term dependencies.\n\nSiri researchers choose LSTM for a good reason. In the task of domain choosing for example, LSTM applies one model to all domains, reduces feature spaces to 1,300 (SVM required 500k), captures long-dependency of queries, and achieves better accuracy and generalization.\n\nDr. Kothari finished with an overview of other fields Siri\u2019s NLU researchers are exploring, including semantic labeling, reinforcement learning, questions answering, conversation modeling, etc.\n\nAlexa Prize \u2014 Advancing the State of the Art in Conversational AI\n\nAmazon Alexa is leading the conversational AI race with a 70 percent smart speaker market share. The company is aggressively working to build on this advantage by organizing contests with enticing cash prizes for third-party developers.\n\nAmazon Lab126 AI Scientist Dr. Chandra Khatri introduced the new Amazon Alexa Prize, a competition that challenges participants to create state-of-the-art social chatbots that can converse coherently and engagingly on popular topics for 20 minutes. The first-of-its-kind challenge invites the world\u2019s top universities and institutes to submit solutions.\n\nThe University of Washington team Sounding Board took first place and US$500,000 in prize money at the inaugural Amazon Alexa Challenge with a chatbot that held conversations with an average duration of 10 minutes and 22 seconds, earning a score of 3.17 out of 5 from judges.\n\nAmazon Alexa hopes the challenge can crowdsource human intelligence to solve today\u2019s conversational AI challenges such as conversational Automatic Speech Recognition and NLU, dialog planning and context modeling, ranking and selection response generations, knowledge ingestion and reasoning, etc.\n\nDr. Khatri introduced a couple of techniques adopted by university teams. In dialogue management and context modeling for example, the winning UW team proposed a hybrid dialogue manager wherein a master manages the overall conversation and a collection of miniskills manage different conversation segments. Another top team designed a state graph to track dialog content, conversation state, feedback, user sentiments, and personalization.\n\nOne researcher to benefit from Amazon\u2019s conversational AI development push is Dr. Zhou Yu, an Assistant Professor at the University of California in Davis who created a multi-modal chatbot with strategies designed to keep users engaged in the conversation. The project caught Amazon\u2019s interest and garnered Dr. Yu an annual research sponsorship of US$100,000.\n\nHumans are generally embracing conversational AI \u2014 according to a recent NPR and Edison Research study, 65 percent of American smart speaker users say they could not return to a life without them. But on the flipside, playing music, weather notifications and responding to general questions remain the top three relatively mundane tasks these speakers perform. Expanding capabilities and developing a natural conversational AI is the target that tech giants like Amazon, Apple and Google will aim for over the coming years."
    },
    {
        "url": "https://medium.com/syncedreview/china-aims-to-get-the-jump-on-ai-standardization-f141dcb52de7",
        "title": "China Aims to Get the Jump on AI Standardization \u2013 SyncedReview \u2013",
        "text": "China has just released its \u201cArtificial Intelligence Standardization White Paper.\u201d The 98-page document was edited by the China Electronics Standardization Institute under the guidance of the National Standardization Management Committee Second Ministry of Industry (\u56fd\u5bb6\u6807\u51c6\u5316\u7ba1\u7406\u59d4\u5458\u4f1a\u5de5\u4e1a\u4e8c\u90e8). Some 30 research institutes, education institutes, and AI companies contributed to the paper, including Tsinghua University, Peking University, Alibaba, Tencent, Baidu, and Toutiao.\n\nConcurrently, China also announced the formation of the National Artificial Intelligence Standardization Group and Expert Advisory Group, which will oversee planning and deployment of the nation\u2019s AI development. Prior to the 19th National Congress of the Party, the Chinese government had laid out its blueprint for further AI integration with the economy, following up with an intensive review on \u201cStandardization of AI Helps Industry Development\u201d and a \u201cThree-Year Action Plan for Promoting the Development of New Generation of AI Industry.\u201d\n\nThe new \u201cAI Standardization White Paper\u201d stresses the importance of establishing standards for the rapid development of AI: \u201cAt present while China\u2019s deployment of AI-related products and services is expanding, there is also the issue of a lack of standards. AI is reaching into many areas, and while some subfields are standardized, their dispersed standards do not form systemic standards.\u201d\n\nThe paper begins: \u201cAI is a prospering new industry. Standardization is at an early stage. China is at the same starting line as all other countries and there is an opportunity now for rapid breakthrough. With fast action plans, China can either seize the commanding heights of innovation standardization, or else miss the opportunity. There is an urgent need to seize opportunities, accelerate research on AI deployment in industry, and systematically review and establish a unified and comprehensive set of standardization.\u201d\n\nThe document comprises six subsections expounding on AI history, key technologies, industry trends, ethical issues; pinpointing international and domestic AI standards, and detailing standard frameworks and their components.\n\nThe paper\u2019s appended Standardization Schedule categorizes 200 technologies pertaining to the field of AI.\n\nInterested readers can download the paper here: http://www.cesi.ac.cn/images/editor/20180118/20180118090346205.pdf"
    },
    {
        "url": "https://medium.com/syncedreview/arm-accelerates-its-ai-efforts-49537e712f69",
        "title": "Arm Accelerates its AI Efforts \u2013 SyncedReview \u2013",
        "text": "You may not realize it but most of the smart electronics you use every day \u2014 from IoT devices to smartphones to assisted driving systems \u2014 were built on architecture designed by Arm, a leading UK-based intellectual property (IP) provider.\n\nArm\u2019s booth at CES 2018 showcased a wide range of demos and products equipped with Arm processors, including smart speakers Google Home and Amazon Echo smart speakers, smart city solutions in lighting management and smart parking, a smart camera from Hive, and autonomous driving applications Cockpit Controller and Event Data Recorder.\n\nJem Davies, General Manager of the Arm Machine Learning Group, sat down with Synced to outline his company\u2019s ambitious roadmap for machine learning development.\n\nDavies efforts for Arm in machine learning can be traced back to 2013, when Arm tasked him with examining the AI marketplace and making appropriate acquisitions. He soon came to believe there was no market segment that wasn\u2019t already or about to be impacted by the tech. \u201cAI affects everything\u2026mobile phones\u2026cameras\u2026the little smart speaker\u2026 even thermostats. Who thought of a room thermostat as a smart device?\u201d says Davies.\n\nIn 2016 Davies led the acquisition of Apical, a company that provides computer vision and imaging processors for over 1.5 billion devices. The acquisition bootstrapped Arm\u2019s entry into machine learning, enabling the company\u2019s object detection technology on one hand, while developing IP that extends into neural network processing. In March 2017 Arm launched its Mali-C71 image signal processor (ISP), a product series designed specifically for the Advanced Driver Assistance Systems (ADAS) inside vehicles.\n\nLast May the company took a huge step forward in AI and introduced DynamIQ technology. Built on Arm\u2019s big.LITTLE technology which accommodates both powerful and relatively small processors in one chip, DynamIQ improves the flexibility and efficiency of multi-core processing designs, and enables more processors that better perform AI tasks to compute on a single chip.\n\nSays Davies, \u201cBased on DynamIQ, Cortex-A75, Cortex-A55, and Mali-G72 (Arm\u2019s latest iteration of CPU and GPU) are specifically targeting machine learning workloads. So we\u2019ve been analyzing the sort of code that people are writing and working out what best to do to execute those workloads more efficiently.\u201d\n\nWhile most machine learning systems still run on CPUs and GPUs, Davies says Arm is interested in developing special purpose processors for AI acceleration. Based on the company\u2019s history, Arm will likely seek an acquisition as its strategy for entering the AI processor market.\n\nThe company meanwhile is also stepping up its software development and optimization efforts, which is a particular focus of Davies\u2019 new Machine Learning Group. Says Davies, \u201cThe difference between an optimized implementation software and a naive implementation software could effect a 10x improvement.\u201d\n\nArm has also introduced an open-source library to provide optimized routines for accelerating machine learning frameworks such as TensorFlow, MXNet and Caffe. The functions are optimized for Arm Cortex CPU and Mali GPU processors, and target a variety of use cases, including* *image processing, computer vision and machine learning.\n\nFor years Arm has quietly led the way in AI, so what to expect in 2018? Davies did not disclose details, but promised \u201cthere will be more announcements that will be incredibly exciting.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/synced-machine-intelligence-award-2017-d14bb567084a",
        "title": "Unveiling China\u2019s Mysterious AI Lead: Synced Machine Intelligence Awards 2017",
        "text": "The Synced 2017 Machine Intelligence Awards recognize companies that use AI technologies in smart and commercially viable ways.\n\n \n\nThis year we received more than 500 applications, and with the help of our industry analysts selected the top 10% that really stood out. We are presenting three awards: 30 Most Promising AI Startups, Top 10 Commercial AI Research Labs in China, and 10 Chinese AI Rising Stars to Watch.\n\nThe 30 Most Promising AI Startups category recognizes companies that have demonstrated sound applications of AI technology in various industries. Some are already unicorns, while others are on their way to achieving great things. \n\n \n\nThe latter two awards are exclusive to China. Top 10 Commercial AI Research Labs identifies tech giants that have invested heavily in AI commercial R&D, attracted top research talents, and shown proven results over the past year. 10 Chinese AI Rising Stars to Watch are dark horse newcomers that have shown superior potential. \n\n \n\nCongratulations to these companies and individuals for their outstanding achievements!\n\nVicarious.ai is Teaching Robots to See The World Like Humans"
    },
    {
        "url": "https://medium.com/syncedreview/amazon-go-vs-alibaba-tao-cafe-staffless-shop-showdown-3f3929393d62",
        "title": "Amazon Go vs Alibaba Tao Cafe: Staffless Shop Showdown",
        "text": "The staffless shop \u2014 where grab-and-go automated payment replaces checkout lines \u2014 is a futuristic retail experience envisioned by e-commerce moguls Amazon and Alibaba. Last July Alibaba showcased its proof-of-concept Tao Cafe in Hangzhou, while in downtown Seattle today Amazon opened its staff-free convenience store Amazon Go.\n\nAmazon and Alibaba\u2019s brick-and-mortar-but-no-staff shops offer very different user experiences. To bring you first-hand feedback, Synced visited them.\n\nBoth shops require customers to use a dedicated mobile app to link to shopping accounts. Customers can then enter the shop by displaying a generated QR code to a scanner. But that\u2019s where the similarities end.\n\nAmazon Go is a convenience store selling snacks, drinks, lunch boxes, salads and sandwiches. Its customers will likely be local office workers (and probably more than a few tech heads).\n\nAmazon Go uses a system called \u201cJust Walk Out,\u201d a first of its kind shopping technology that lets customers select anything from the inventory and leave the store without additional steps. Powered by computer vision, deep learning algorithms and sensor fusion, the system is able to recognize what items which customers have taken in real-time. A backend system then automatically processes the transaction. A Synced reporter tried shutting down their smartphone at the store, but this did not impact the transaction.\n\nOne of the concerns with \u201cJust Walk Out\u201d is the high computational power required for detection and processing. Amazon Go was reportedly incapable of accommodating more than 20 customers at a time during testing. On opening day Synced observed at least 50 in-store customers, although Amazon staff started to limit traffic when the number exceeded 60.\n\nSetting up an unstaffed shop is not cheap \u2014 Amazon Go is equipped with hundreds of cameras and sensor arrays costing millions of dollars. Amazon has however priced the merchandise competitively, a 500ml bottle of water for example costs US$0.49, about the same as in other stores.\n\nTao Caffe meanwhile is a staffless cafeteria and boutique located in Hangzhou, selling drinks and snacks as well as products such as backpacks, notebooks, plush toys, etc. It is only open to users of Alibaba\u2019s e-commerce site Taobao.\n\nTechnically, Tao Cafe is not entirely staffless \u2014 flesh-and-blood waiters take the orders and baristas make the drinks. Like Amazon Go, Tao Cafe enables customers to shop without directly paying, but does this in a very different way.\n\nFor the purchase of drinks, a facial recognition-enabled camera system at the service counter automatically identifies the customer, links to their account, and processes the payment. Customers buying merchandise exit through a processing chamber equipped with multiple sensors, which identifies both the customer and the merchandise they have. The scan works even if the customer has put the goods in pocket or a bag.\n\nSynced observed however that customers required about 10\u201315 seconds to leave Tao Cafe, this occasionally creating a lineup for the exit scanners where only one customer could pass at a time. Amazon Go had no such bottleneck issues.\n\nAmazon also has an advantage over Alibaba at this stage because Amazon Go is open to the public, while Tao Cafe remains something of a randomly recurring pop-up. While it\u2019s still too early to say who will win the battle of the staffless store, it\u2019s clear that neither of these multi-million-dollar experiments will generate any profit in the short term.\n\nBoth companies have invested heavily in brick-and-mortar retail development over the last year: Amazon acquired American supermarket chain Whole Foods for US$13.7 billion, while Alibaba announced an investment of US$2.88 billion for a 36% stake in Gao Xin Retail, China\u2019s largest and fastest-growing hypermarket operator.\n\nWith payroll costs rising and sensor costs falling, more staffless stores are bound to pop up in 2018."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-january-pt-2-1bbea0757",
        "title": "AI Biweekly: 10 Bits from January (Pt 2) \u2013 SyncedReview \u2013",
        "text": "January 7th \u2014 NVIDIA Announces Delivery Target For Self Driving Processor Xavier\n\nAt CES 2018, NVIDIA CEO Jensen Huang announces the company is planning to deliver the first sample of its Xavier processor during this quarter. NVIDIA DRIVER Xavier contains more than nine billion transistors, making it by far the most complex system on a chip. NVIDIA says that Xavier will power Pegasus, the AI computing platform designed to deliver Level 5 autonomous vehicles.\n\nJanuary 8th \u2014 Facebook Plans to Shut Down Its Personal Assistant \u201cM\u201d on January 19th\n\nFacebook announces it will shut down its virtual chatbot assistant \u201cM.\u201d The AI chatbot in Messenger was only ever available to 2,000 Californians. Facebook says it learned a lot about how users interact with virtual assistants and plans to use that info to further develop its other AI services. Some M users say they never really had a reason to communicate with the bot.\n\n \n\nJanuary 9th \u2014 Amazon Alexa Is Integrating With Windows 10\n\nAmazon Alexa is taking a big step forward by merging itself into Windows PCs. Many OEMs at CES \u2014 such as Acer, ASUS and HP \u2014 announced they would make use of the Alexa App for Windows 10. The app will be limited to specific functions, and cannot call or message. Windows 10 integration does however enable Alexa to jump out of the Amazon ecosystem.\n\n \n\nJanuary 9th \u2014 A US Media Team Plans to Launch Vital Intelligence Data Live\n\nA team of media executives in the US announces VIDL (Vital Intelligence Data Live) the launch this summer. The proprietary technology combines blockchain and AI to deliver accurate news reports from around the world. The platform will be fully automated and promises a foolproof system for delivering news stories that won\u2019t be affected by human bias or warped into fake news.\n\n \n\nJanuary 11th \u2014 Mercedes-Benz Releases In-car Assistant MBUX\n\nMBUX is the new Mercedes-Benz infotainment and multimedia system, a smart platform powered by NVIDIA GPU. It enables users to issue natural language voice commands rather than simple control commands, and can function without an internet connection. MBUX will be ready for all new Mercedes A-Class vehicles this year, and in other Mercedes vehicles in the future.\n\nJanuary 13th \u2014 Google Assistant Gets Much Attention At CES\n\nGoogle did not introduce many new products at CES 2018. Instead, it highlighted a bunch of tech partners \u2014 including Sony, LG, Lenovo, and Huawei \u2014 who promoted Google Assistant in various hardware applications, including TVs, refrigerators and even electric bike wheels. In the virtual assistant race, Google Assistant is closing the gap with Alexa.\n\nJanuary 16th \u2014 GM and Waymo Top New Self-Driving Leaderboard\n\nNavigant Research releases an Autonomous Driving Leaderboard which ranks the industry\u2019s top brands. General Motors and Alphabet\u2019s Waymo are in the top two positions. Meanwhile, former leader Ford has dropped to third, while Tesla sits at the bottom of the ranking with Apple.\n\n \n\nJanuary 16th \u2014 Canada.ai Platform Showcases Canada\u2019s AI Information\n\nNEXT Canada and a group of top Canadian artificial intelligence institutions launch Canada.ai. The web platform showcases Canada\u2019s leadership in the field of artificial intelligence by presenting information such as research results, leading startups and companies in the AI space, and AI-related events happening across the country. Canada is welcoming many international AI researchers, students, institutions and startups.\n\n \n\nJanuary 17th \u2014 AutoML Can Train Machine Learning Models Without Coding\n\nGoogle has announced the alpha launch of AutoML Vision, which helps build custom image recognition models. The new services available under the AutoML brand don\u2019t require users to have machine learning expertise. The user provides images along with their tags, and AutoML Vision automatically creates a custom machine learning model based on them. Google is handling the hard work of training and tuning the model.\n\n \n\nJanuary 19th \u2014 Google Unveils A New Office In Shenzhen, China\n\nRather than Beijing or Shanghai, Google unveils a new office in Shenzhen. According to Google News, the reason for setting up in Shenzhen is to better cooperate with local partners. Google is gradually collaborating more with Chinese companies on products such as Lenovo\u2019s VR set and self-driving cars. While Shenzhen is now in the spotlight, it remains uncertain whether Google can effectively leverage its new Chinese offices to penetrate the country\u2019s AI market."
    },
    {
        "url": "https://medium.com/syncedreview/self-driving-boats-ghosts-on-the-ocean-20dc76546dd4",
        "title": "Self-Driving Boats: Ghosts on the Ocean \u2013 SyncedReview \u2013",
        "text": "Self-driving boats have existed for a long time: they navigate the oceans, driven by the wind, recording climate data or monitoring water currents. Autonomous underwater vehicles developed by the University of Washington have been used for performing dangerous tasks since 1957. Recent advances in AI may soon broaden the role of these ghost boats.\n\nThe maritime transportation industry has been facing employment shortages. Autonomous vessels may solve this problem and reduce the high crew costs. In comparison to land-based autonomous driving, there are fewer obstacles on the sea where travel routes are more open. Energy consumption and sensor cost are also less restrictive factors.\n\nTo this end, Norwegian chemical company Yara International has announced that it will launch self-driving cargo ships in 2018. The fleet will cost US$25 million, but the company claims it will save up to 90% on crew and energy operating costs.\n\nHuman Labor Cost Reduction: self-driving cargo ships have a significant advantage for long distance travel, especially for saving labor costs. With the help of AI and sensors, a small crew can handle many ships. Sensors also simplify maintenance by locate potential problems and notifying the crew in advance.\n\nEnergy Consumption Reduction: AI can be used to optimized marine routes. Data analytics will find the best travel path taking in consideration current, wind, and sunlight. Travel speed controlled by AI will also save energy during shipping.\n\nSupply Chain Optimization: supply chain optimization will save storage costs and time spent at port. AI in the harbor and management will improve cargo combinations for each shipment, which will shorten travel distances and make full use of cargo space on ships.\n\nCollision Prevention: equipped with an accurate map, ships can avoid reefs and underwater rocks. However, ports with heavy traffic are also dangerous for ships. Improved fleet communication and AI-powered collision detection could provide more safety for passengers and cargo.\n\nEfficient Ship Design: unmanned cargo vessels will change in design, as ships will no longer need to provide space for crews. Moreover, advanced weight control will ensure a more stable and smooth ship. For example, instead of adding water as ballast, the cargo could be re-positioned to achieve the same result.\n\nEco-Friendly Shipping: pollution from the world\u2019s top 16 ships is almost equal to the total pollution from all land vehicles. AI can reduce pollution by reducing fuel consumption. AI will also help boats make better use of currents, wind, and solar energy.\n\nInterference from Interest Groups: like the professional drivers\u2019 backlash against land-based autonomous vehicles, there will likely be resistance from ship crew unions.\n\nBackup Plan to Increase the Robustness: ships must guarantee that at critical moments there will be an alternative plan. All of the components in the vessel should function well during long-distance shipping, but this also means increased operation costs."
    },
    {
        "url": "https://medium.com/syncedreview/life-after-baidu-yuanqing-lins-ai-startup-moves-fast-on-funding-and-b2b-opportunities-c5b183bf588b",
        "title": "Life After Baidu: Yuanqing Lin\u2019s AI Startup Moves Fast on Funding and B2B Opportunities",
        "text": "Yuanqing Lin\u2019s AI startup Aibee.ai \u2014 or AI2B, AI to business \u2014 has landed US $25 million in angel round funding. The company was founded by the former Director of Baidu\u2019s Institute of Deep Learning (IDL), and provides general AI solutions such as computer vision and image and speech recognition for companies in education, finance, retail, real estate and other sectors.\n\nIt was also announced that Dr. Silvio Savarese, assistant professor of computer science at Stanford University, will join Aibee.ai to lead research and product development.\n\nWhen Lin established Aibee\u2019s new office on the same street as Baidu\u2019s headquarters last November, VCs swarmed to his door, filling the company coffers with US$25 million within a month and quickly solving the money issues that plague most startup CEOs. Junzhang Liang from Kinzon Capital tells Synced that investors feel \u201cobliged\u201d to meet with entrepreneurs who split from BAT (Baidu, Alibaba, Tencent) companies. \u201cA lot of investors tried to reach Yuanqing on the first day, bombarding him with meeting requests. In his case, investing is a very selective process for both sides.\u201d\n\nOver the last three months Lin has consulted with some 40 companies in traditional industries, in order to \u201cland\u201d his technology. He says the main feedback he got was that management feels \u201cpained\u201d when tech contractors disrupt business operations with frequent technical upgrades and patches fixing irrelevant problems.\n\nLin arrived at three conclusions: 1. Know AI has limits. 2. Customers couldn\u2019t care less about what technologies you use, they want to see cost-reduction or measurably improved performance. 3. AI solutions are delivered in iterations and upgrades, there are no instant results.\n\nThe Aibee team of 20 in Beijing and Silicon Valley works with very short iterations, starting each case by first doing a client consultation, and following up with a specialist review by a team of AI engineers before proposing a well-rounded solution. They then collaborate with client-side engineers to deliver the final product or service.\n\nLin knows that multi-billion-dollar companies are reluctant to hand over their system revamp to startups, let alone share their precious data. In other words, Aibee has to work fast and establish feasible industry benchmarks, otherwise, face a survival challenge.\n\n\u201cWe have a portal of opportunity for three years,\u201d says Lin. \u201cThe AI hype will settle by then, and so 2018 is an important watershed. There\u2019s a sense of urgency, we have to move fast but in a down-to-earth manner. You will see our first solution proposal in two months.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/baidu-research-adds-elite-ai-talents-new-labs-a447f0c3492",
        "title": "Baidu Research Adds Elite AI Talents, New Labs \u2013 SyncedReview \u2013",
        "text": "Baidu announced today that respected Natural Language Processing scientist Dr. Kenneth W. Church will join its research arm as a Distinguished Scientist. This is the latest in a series of eye-catching talent acquisitions for the Chinese \u201cAll in on AI\u201d tech giant.\n\nDr. Church comes to Baidu Research from the IBM Watson Research Center, where he worked on computational linguistics. He was also Chief Scientist of the Human Language Technology Center of Excellence at Johns Hopkins University; and serves as president of Empirical Methods on Natural Language Processing (EMNLP), one of the world\u2019s most influential NLP conferences.\n\nSays Dr. Church, \u201cIn addition to its commitment to fundamental research, Baidu is in a unique position to transfer AI technology from the laboratory into reality and make the world a better place for hundreds of millions of people.\u201d\n\nBaidu Research also announced the acquisition of big data and data mining expert Dr. Jun Huan, a former Professor of Computer Science at the University of Kansas; and Dr. Hui Xiong, a Professor at Rutgers University focused on data and knowledge engineering research.\n\nBaidu Research revealed that it will open two new labs: the Business Intelligence Lab (BIL) will focus on effective and efficient data analysis technology for emerging data intensive applications; while the Robotics and Autonomous Driving Lab (RAL) will conduct research on computer vision. The RAL is expected to advance obstacle perception capabilities at Baidu\u2019s star project Apollo \u2014 an open-source autonomous driving technology platform.\n\nThe new labs complement Baidu Research\u2019s existing Institute of Deep Learning (IDL), Big Data Lab (BDL), and Silicon Valley Artificial Intelligence Lab (SVAIL).\n\nSays Baidu Vice President Haifeng Wang: \u201cThis is the beginning of a new journey for Baidu Research. Our mission will be centered on conducting future-oriented fundamental research, setting the direction for Baidu\u2019s AI development and empowering the company\u2019s long-term strategy.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/barack-obama-is-the-benchmark-for-fake-lip-sync-videos-d85057cb90ac",
        "title": "Barack Obama is the Benchmark for Fake Lip-Sync Videos",
        "text": "The Collins Dictionary named \u201cfake news\u201d as its Word of the Year for 2017, and AI has just made it a lot more believable. The Montreal Institute for Learning Algorithms (MILA) recently launched ObamaNet \u2014 a photo-realistic lip-sync neural network that can make anyone appear to be saying anything.\n\n \n\nObamaNet is composed of three trainable neural modules: a text-to-speech network based on Char2Wav; a time-delayed LSTM to generate mouth-keypoints synced to the audio; and to generate the video frames conditioned on the key points, a network based on Pix2Pix \u2014 which is emerging as a good general-purpose solution for image-to-image translation problems.\n\nUsing generative networks for images and videos is nothing new, while decades of work have been put into speech synthesis. ObamaNet combines the attributes of both. According to MILA researchers, their system \u201ccan be trained on any set of close shot videos of a person speaking, along with the corresponding transcript. The result is a system that generates speech from an arbitrary text and modifies according to the mouth area of one existing video so that it looks natural and realistic.\u201d \n\n \n\nMILA researchers explain that they chose the former US President because \u201chis videos are commonly used to benchmark lip-sync methods.\u201d There is also more online video data available for public figures such as Obama. For the project, MILA extracted 17 hours of footage from Obama\u2019s 300 weekly presidential addresses. \n\n \n\nThe concept of Obama as a benchmark is rooted in the paper Synthesizing Obama: Learning Lip Sync from Audio, published in July 2017 by Supasorn Suwajanakorn from the University of Washington\u2019s Graphics and Imaging Laboratory (GRAIL). When Suwajanakorn uploaded his results to YouTube, the videos quickly attracted over 750,000 views.\n\nGRAIL\u2019s synthesized Obama video stirred up quite a discussion online. The model worked so well that many YouTube viewers were frightened, one warning \u201cI see the first episode of Black Mirror has begun.\u201d Others welcomed the videos: \u201cIt\u2019s better that the public is aware of such technology than being oblivious. If we were ignorant we would believe things without thinking it was tampered.\u201d\n\nThe Guardian described the project as \u201cthe future of fake news. We\u2019ve long been told not to believe everything we read, but soon we\u2019ll have to question everything we see and hear as well.\u201d\n\n \n\nMILA says their model differs from the GRAIL project because instead of a traditional computer vision model, it uses a neural network topped with a text-to-speech synthesizer. The lab created ObamaNet in conjunction with Lyrebird.ai, whose beta voice synthesis product allows users to generate text-to-speech files in their own \u201cdigital voice\u201d after providing just one minute of sample speech.\n\n \n\nWhile applications are still limited, projects like Synthesizing Obama and ObamaNet provide a glimpse of future possibilities for the tech, including the chilling prospect of just how just realistic fake news may become."
    },
    {
        "url": "https://medium.com/syncedreview/ubiquitous-virtual-assistants-and-ai-homogenization-at-ces-2018-5c7fc67a6e36",
        "title": "Ubiquitous Virtual Assistants and AI Homogenization at CES 2018",
        "text": "Although AI is opening up a world of possibilities for humanity, the technology\u2019s current application in consumer electronics remains surprisingly limited.\n\n \n\nAt last week\u2019s Consumer Electronics Show in Las Vegas (CES 2018), exhibitors promoted everything from AI-powered vehicles to smart refrigerators and robot puppies. But the tech driving these diverse products was the same: the virtual assistant, a conversational AI that can understand human speech and generate humanlike responses.\n\n \n\nSince Amazon introduced its virtual assistant Alexa and attendant smart speaker Echo three years ago, virtual assistants have been widely integrated into home appliances, smart devices and cars. At CES 2018, South Korean electronics mogul LG announced a partnership with Google to enable customers to communicate with its televisions via Google Assistant, the Alexa rival released in 2016. At the LG booth, a staff member proudly prompted a television to say \u201chola\u201d (\u201chello\u201d in Spanish) via Google Assistant.\n\nOther electronics makers such as TCL, Hisense, Haier and Sony also support Google Assistant in their smart home product lines. When asked about the biggest advantage of AI for home appliances, a CES Hisense representative told Synced \u201cwith virtual assistants, AI brings a better connectivity between devices and electronics.\u201d\n\nVirtual assistants are also bringing their convenient voice interface to cars. Millions of vehicles from top manufacturers like Ford, GM, Volkswagen and Volvo now have Google Assistant built in. \n\n \n\nCES 2018 visitors were greeted by a huge \u201cHey Google\u201d installation at the show\u2019s main entrance, and a city-wide advertising campaign covered Las Vegas with the wake word slogan. The virtual assistant has become the focus of so much contemporary applied AI in part because it is one of the few AI technologies that has proven both successful and marketable over the last few years. Amazon\u2019s Echo sales more than quintupled from 2016 to 2017, while Google Assistant now supports 1,500 devices from 200 brands.\n\nWhile virtual assistants are being deployed across such a wide range of products, most companies are using the same third-party cloud service to power their voice interfaces, so users will converse with the same bot whether they are switching TV channels, navigating a route in their car, or ordering a pizza.\n\n \n\nMany Chinese tech companies \u2014 including Alibaba and Baidu Online \u2014 brought their smart speakers to CES 2018 for the first time. However, Microsoft Product Manager Tucker Kelly told Synced that he could not discern any significant differences between Alibaba\u2019s smart speaker Ali Genie X1 and other Chinese smart speakers. \u201cThey all look the same and have similar features.\u201d\n\n \n\nKun Jing, General Manager of Baidu\u2019s DuerOS platform, told Synced that the virtual assistant marketplace is still new, and he believes more diversified and customised AI products will emerge in 2018. \u201cAndroid as a mobile operating system has pushed innovations to the boundaries, but we don\u2019t have any mainstream operating systems for the era of AI yet,\u201d says Jing. This is an area where Baidu is currently applying its development resources.\n\n \n\nMeanwhile, only a few companies have taken the step of customising their virtual assistants with unique appearances or interlocution styles. Chinese electronics maker Xiaomi, for example, designed an anime avatar Xiao Ai, an adorable redheaded girl with a sweet, natural voice, for its Mi AI Speaker. Considering anime culture is widely popular with Chinese consumers, the character is expected to further drive domestic sales.\n\n \n\niFlytek has also put effort into innovation, showcasing a range of AI-powered voice translators, portable intelligence translators, smart microphones, smart speakers, and AI headphones. An iFlytek representative told Synced its English-Mandarin Voice Translator EASYTRANS 600 was very well-received by CES attendees.\n\nWith virtual assistants getting all the attention these days, other AI technologies are lagging. Facial recognition for example has not been as widely adopted because it remains vulnerable to attacks. Last year, when Apple boldly switched its iPhone X unlocking system from fingerprint-based TouchID to facial recognition-based Face ID, the tech was quickly tricked by siblings and even a cheap mask. Many other promising AI technologies remain relatively underdeveloped because they have limited applications in consumer products.\n\n \n\nAI has wowed humans over the last two years with its capacity for detecting images, translating languages, recognizing voices, and mastering board games such as Go. Going forward, humans will be expecting more from their virtual assistants, and expecting more than virtual assistants from AI."
    },
    {
        "url": "https://medium.com/syncedreview/uc-berkeley-facebook-ai-researchers-introduce-house3d-for-reinforcement-learning-9667efdfb86e",
        "title": "UC Berkeley & Facebook AI Researchers Introduce House3D for Reinforcement Learning",
        "text": "Reinforcement learning\u2019s prowess in 3D understanding, real-time strategy decision, fast reaction, long-term planning, language and communication have enabled machines to top humans in contests ranging from Atari\u2019s Breakout to the ancient game of Go.\n\n \n\nHowever, current reinforcement learning research is largely focused on over-simplified tasks that are conducted in monotonous virtual environments and are not transferable to the real world. Today\u2019s smart robots are far from functional when it comes to generalized tasks and fall short when dealing with our semantically-rich world.\n\n \n\nTo boost learning research aimed at endowing robots with better generalization capabilities, Yi Wu from UC Berkeley and Yuxin Wu, Georgia Gkioxari, and Yuandong Tian from Facebook AI research recently published the paper Building Generalizable Agents with a Realistic and Rich 3D Environment, which introduces a diverse set of training environments in a virtual property called House3D. \n\n \n\nHouse3D comprises 45,622 human-designed 3D scenes extracted from the SUNCG dataset, which includes housing models ranging from single-room studios to multi-story houses, subdivided into 20 room types such as bedroom, living room, kitchen, bathroom, etc. All scenes are semantically annotated to the level of each object. Agents can make observations of multiple modalities such as RGB images, depth, segmentation masks, top-down 2D view, etc.\n\n \n\nThe benchmark task in this paper is \u201cconcept-driven navigation\u201d known as \u201cRoomNav.\u201d The agent is given a high-level task description such as \u201cGo to the kitchen,\u201d then prompted to explore the House3D environment to reach the target room.\n\nRoomNav is considered a multi-target learning problem to which the paper\u2019s authors propose two baseline models with gated-attention architecture: \u201cA gated-CNN network for continuous actions and a gated-LSTM network form for discrete actions.\u201d The gated-CNN policy network was trained using deep deterministic policy gradient (DDPG), while the gated-LSTM policy was trained using the asynchronous advantage actor-critic algorithm.\n\nThe research results indicate improved generalization capabilities, with the team making the observations that \u201cusing the semantic signal as the input considerably enhances the agent\u2019s generalization ability. Increasing the size of the training environments is important but at the same time introduces fundamental bottlenecks when training agents to accomplish the RoomNav task due to the higher complexity of the underlying task.\u201d\n\n \n\n\u201cWe believe our [House3D] environment will benefit the community and facilitate the efforts towards building better AI agents. We also hope that our initial attempts towards addressing semantic generalization ability in reinforcement learning will serve as an important step towards building real-world robotic systems.\u201d\n\n \n\nYi Wu, who received the NIPS Best Paper Award in 2016 for his paper Value Iteration Network, is advised by esteemed UC Berkeley Professor Stuart Russell. Facebook AI Research team meanwhile have conducted explorational research on the application of reinforcement learning to real-time strategy games like Starcraft. Yuandong Tian has proposed ELF, a platform for reinforcement learning research in gaming. According to his Facebook research blog, \u201cELF allows researchers to test their algorithms in various game environments, including board games, Atari games, and custom-made, real-time strategy games.\u201d\n\nThe paper Building Generalizable Agents with a Realistic and Rich 3D Environment has been submitted to the International Conference on Learning Representations (ICLR) 2018. You can read it here: https://arxiv.org/abs/1801.02209\n\n \n\nThe House3D project has been open-sourced on Github: https://github.com/facebookresearch/House3D"
    },
    {
        "url": "https://medium.com/syncedreview/intro-a-review-of-ai-history-beyond-deep-learning-f166ff8b4b2f",
        "title": "A Review of AI History Beyond Deep Learning \u2013 SyncedReview \u2013",
        "text": "Related work on Artificial Intelligence can be traced back to the 1940s, when Warren McCulloch and Walter Pitts showed that computing could be done by a network of connected neurons, and Donald Hebb demonstrated Hebbian learning.\n\n \n\nIn 1956, when the term \u201cArtificial Intelligence\u201d (AI) was formally coined, corresponding research started to develop quickly. AI was mostly used for problem-solving at that time. Although there were criticisms during the 1960s and 1970s for the slow development speed and limited progress of the neural network, the emergence of expert systems still attracted the interest and growth of AI studies.\n\n \n\nDuring the 1980s, AI academic studies stepped into a period called \u201cAI Winter\u201d, as most studies of AI failed to deliver their original extravagant promises, resulting in the funding for AI research being moved to other areas. Luckily, connectionists reinvented back-propagation and brought neural networks back onto the stage.\n\nIn the 1990s, more scientific methods such as probabilistic models started to be applied; at the same time, Support Vector Machines (SVM) outperformed and replaced neural networks in many areas. The era of big data came soon after in the 2000s, which helped the development of various learning algorithms and enabled deep learning to flourish in recent years.\n\n \n\nNow, more and more groups are starting to pay attention to AI research, and the number of papers published in elite AI conferences such as AAAI, IJCAI, AUAI, and ECAI has increased dramatically.\n\nThe purpose of this report is to detail the major technological branches of Artificial Intelligence (AI). By identifying these technologies\u2019 paths of development, readers will be able to accurately and comprehensively learn about the past, present, and future of all modern AI research fields. The report will help you sort out the basics and provide you with the necessary background to move forward.\n\nWe\u2019ll review all the mainstream AI research fields, with a focus on:\n\nThe report is organized based on:\n\nWe hope you find the report useful, also, let us know your thoughts in the comment section!"
    },
    {
        "url": "https://medium.com/syncedreview/uncanny-artificial-humans-567e15b08dfe",
        "title": "Uncanny Artificial Humans \u2013 SyncedReview \u2013",
        "text": "One of the most attention-grabbing displays at CES 2018 is \u201cJoel,\u201d a body inside a vacuum-sealed plastic bag, curled up in the fetal position and peacefully waiting for a consciousness. Joel was built by biotech company PsychSec, which can implant a digitized version of a human\u2019s genes, emotions, and memories into the vessel via a special cortical stack implant. Voila! You\u2019ve swapped your tired old body for a new one and now you\u2019re standing six-foot-two and with eight-pack abs.\n\n \n\nYou\u2019ve probably never heard of \u201cPsychSec,\u201d and neither had we. Because it\u2019s fake. And poor Joel is merely a prop in a advertising campaign promoting the upcoming Netflix series Altered Carbon, based on Richard K. Morgan\u2019s sci-fi novel.\n\nThe fact that it\u2019s a publicity stunt has not, however, stopped the crowds from eyeballing Joel and additional PsychSec mannequins. The android bodies are disturbingly realistic, pulling viewers into the eeriness of the uncanny valley.\n\nReaction to the Netflix marketing scheme drives home the point that looks matter. Many if not most humans are more fascinated by humanoid robots than they are by the actual AI that may or may not reside inside them. \n\n \n\nOne of the world\u2019s best-known humanoid bots is Hanson Robotics\u2019 Sophia, who made her first public appearance in 2016, and walked for the first time at CES this week thanks to an upgraded DRC-HUBO body developed in partnership with Rainbow Robotics and Drones and Autonomous Systems Lab.\n\n \n\nSophia returns millions of Google search results, and became a Saudi Arabian citizen in October 2017. Attractive and witty, the bot is a worldwide media darling \u2014 her appearance on The Tonight Show Starring Jimmy Fallon attracted close to eight million viewers. Many in the general public take Sophia very seriously, one YouTube user warning, \u201cshe must be stopped before she gets out of hand\u2026\u201d\n\nWhile Sophia certainly makes a good story, the assessment from AI researchers has been less enthusiastic, with many holding the opinion that Sophia\u2019s interview responses are scripted. Yann LeCun in particular does not like all the attention Sophia is getting. Earlier this month the Facebook AI Director open fired on Twitter: \u201c[Sophia] is to AI as prestidigitation is to real magic. Perhaps we should call this \u201cCargo Cult AI\u201d or \u201cPotemkin AI\u201d or \u201cWizard-of-Oz AI\u201d. In other words, it\u2019s complete bullsh*t (pardon my French).\u201d \n\n \n\nNaturally a scientist would want to peek under the hood and check out what\u2019s powering such androids. Still, although it\u2019s easy to dismiss the algorithm\u2019s sophistication or lack thereof, it\u2019s hard not to look. \n\n \n\nProfessor Manuela Veloso of Carnegie Mellon University, who has been working on robotics for decades, believes this obsession with humanoid bots is misplaced. \u201cRobots should stay like robots. You don\u2019t make a fridge look like a human. I care about whether they work autonomously, not how they look.\u201d\n\n \n\nTry telling that to all the people jostling for a glimpse of Joel."
    },
    {
        "url": "https://medium.com/syncedreview/after-creating-166-million-super-cute-selfies-meitus-ai-sets-its-sights-on-skin-bbfd70f2d44",
        "title": "After Creating 166 Million Super Cute Selfies, Meitu\u2019s AI Sets its Sights on Skin",
        "text": "Meitu is a Chinese tech and selfie-app company which built its net worth of US$4 billion in large part by making selfies look cute. Last November the company launched \u201cAndy,\u201d an AI bot that repaints selfies with a choice of styles and visual effects.\n\n \n\nYou may find Andy\u2019s dreamy creations too cute for your taste, but numbers don\u2019t lie: at time of writing a total of 166,743,348 users had employed the artbot\u2019s \u201ctech magic\u201d to beautify their selfies and pictures of popular figures like Elon Musk and US President Donald Trump. \n\n \n\nAndy runs on DrawNet, a generative network which breaks down the stylistic differences of each image, categorizing elements like composition and paint strokes. Andy was trained in six months on database of portraits with different age, gender, and race profiles. Now, the more it\u2019s used, the more it improves.\n\n \n\nBreakthroughs in computer-generated imagery and machine learning from the company\u2019s R&D division, Meitu Imaging Laboratory (MTLab), have enabled Meitu to monetize its massive user database using facial recognition, image identification, AR, and 3D visual effect applications. \n\n \n\nMeitu\u2019s newest AI-powered bot is a virtual dermatologist developed in partnership with Meitu\u2019s cosmetics e-commerce platform. From a photo, \u201cMTskin\u201d can diagnose skin conditions based on texture, tone, pores, and dark circles. It then outputs recommendations for skincare products. \n\n \n\nThe lab collaborates with the Shanghai Dermatology Hospital \u2014 an institution that performs 70% to 80% of China\u2019s cosmetics testing. Hospital experts teamed with engineers to label skin types and conditions. \u201cWe want our product to be a recommendation tool that is feasible (in the medical sense) and precise,\u201d Meitu CTO Wei Zhang tells Synced.\n\nThere are some challenges. Zhang says for example in low-light conditions it\u2019s difficult for the app to separate acne from noise. External light factors can also affect the pigmentation assessment. In addition to hardware limitations, algorithms sometimes struggle in identifying acne, spots, or blackheads. To this end, deep learning is used in conjunction with traditional machine learning algorithms.\n\n \n\nAt present most members of the MTLab team are tech-savvy males, while most MTskin users are female. \u201cFortunately, we also have females on the team who can give advice from their point of view,\u201d says Zhang, explaining that a female product manager might for example adjust how a skin condition is displayed in order to avoid upsetting users. \n\n \n\nWithin three months of its release MTskin had performed 23 million tests. \u201cThis is a preliminary version of the product and we have a two to three-year plan for its finalization,\u201d says Zhang."
    },
    {
        "url": "https://medium.com/syncedreview/ces-2018-just-got-aied-8aa873db1e89",
        "title": "CES 2018 Just Got AI\u2019ed \u2013 SyncedReview \u2013",
        "text": "Algorithms are taking center stage at the Consumer Electronics Show (CES) 2018, as exhibitors strive to out \u201csmart\u201d each other with AI-powered gadgets and appliances. The four-day event is the largest of its kind, featuring 4,000 exhibitors across than 2.6 million sq ft (186,000 sqm) of exhibition space at the Las Vegas Convention Center. CES runs to Jan 12 and is expected to draw some 180,000 attendees.\n\n \n\nTwitter hashtags such as #AI, #IoT, and #Automation topped the search list for the 51st annual CES, while tech giants jostled for position, particularly in the highly competitive voice-controlled home assistant market. Google erected a huge Hey Google installation at the CES main entrance, repeating the catchphrase on the city\u2019s monorail and billboards.\n\n \n\nThis hotword advertisement for Google Home directly challenges Amazon Alexa and Apple\u2019s upcoming HomePod. Amazon meanwhile is making its mark with Revolutionizing Consumer Electronics with Voice, a series of nine keynotes running through the week.\n\n \n\nTech giants NVIDIA, Baidu, Samsung and LG held their own gala product unveilings in the days leading up the CES kickoff, with some going better than others.\n\n \n\nThings got awkward when South Korean firm LG introduced its line of brainy home appliances powered by ThinQ AI software, including self-learning refrigerators, and washing machines. The fridge is equipped with 29 inch touch screen that displays what\u2019s inside, and like other LG smart appliances is controlled by CLOi, a voice assistant robot. CLOi however went silent in its stage debut, thrice ignoring marketing chief David VanderWaal\u2019s attempts to communicate with it.\n\nLG also unveiled its AI capable OLED TV with a built-in voice command developed in partnership with Google. LG rival Samsung meanwhile showcased its 85-inch 8K television Q9S, which features a dedicated resolution-enhancing algorithm, enabling it to convert a low-resolution signal into 8K quality content for playback.\n\n \n\nCES will hold 21 IoT keynote sessions, second only to the 24 sessions on AI. The first keynote was presented by Ford Motors under the title Mobility Solutions and the Move Towards Smarter Cities. According to CES\u2019s Official Twitter account, \u201cby next year, every Ford vehicle in the US will be connected, and they are partnering with Qualcomm to make this a reality.\u201d\n\nWhile Aptiv and Lyft\u2019s self-driving BMWs chauffeured CES visitors outside, inside the hall Ford, Hyundai, BMW and Audi showcased the latest improvements to their own self-driving technologies, such as smarter parking assistance and collision avoidance.\n\n \n\nCES\u2019s first-ever AI Marketplace is happening this year in LVCC Hall 2. Featured exhibitors include Baidu, Horizon Robotics, iFLYTEK, Aeolus Robotics, NovuMind and others.\n\n \n\nSynced will provide further updates on CES gadgets and goings on throughout the week."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-at-ces-developing-ai-at-china-speed-71315156479a",
        "title": "Baidu at CES: Developing AI at \u201cChina Speed\u201d \u2013 SyncedReview \u2013",
        "text": "China is undergoing a revolutionary transformation as AI rapidly penetrates industries from transportation to communication \u2014 a growth rate phenomenon that Baidu Group President & COO Qi Lu calls \u201cChina speed.\u201d \n\n \n\nBaidu, which bills itself as China\u2019s most powerful AI company, used its CES (Consumer Electronics Show) Baidu World @ Las Vegas event at the Mandarin Oriental Hotel on Monday Jan 8 to showcase its upgraded autonomous driving platform Apollo 2.0, a self-driving vehicle commercialization project, and three smart devices powered by the conversational AI system DuerOS. \n\n \n\nFirst released last July, Apollo is an open-source platform that gives developers access to a complete set of vehicle, hardware, software, and cloud data service solutions; as well as an API and codes for obstacle perception, route planning, vehicle control, and operating system. Aimed at democratizing autonomous driving, Baidu hopes Apollo will become \u201cthe Android of the auto industry.\u201d\n\n \n\nLu reported that Baidu\u2019s Lincoln MKZ test vehicle powered by Apollo 2.0 and Nvidia Drive PX had successfully completed day and nighttime runs on designated city streets in Beijing and Xiongan in China, and San Jose and Peoria in the US. One of the most important features added to Apollo 2.0 is advanced perception capabilities enabled by cameras and radar. Apollo 2.0 can detect obstacles within 300 ft (91 m) and traffic lights within 500 ft (152 m) with 99% accuracy. \n\n \n\nJosh Whitley is a senior engineer at AutonomouStuff, the Illinois-based autonomous system components startup that equipped test vehicles with both Apollo versions. He told Synced that although improved camera and radar processing have accelerated the development of self-driving vehicles, challenges remain. \u201cApollo 2.0 is not really good at handling high speed, which is something they might be working on for the next iteration.\u201d\n\n \n\nApollo 2.0 also comes up with a security solution integrating OTA, IFDS and a black box, designed to protect self-driving vehicles from hacks.\n\n \n\nOver 165,000 lines of code have been created on Apollo, with an average of 146 \u201cCommits\u201d per week received from developers on Github. Says Apollo Senior Director Jingao Wang, \u201cApollo is now the most vibrant autonomous driving platform in the world.\u201d\n\n \n\nBaidu has big dreams for Apollo. By the end of July 2018, the autonomous driving solution will be incorporated in Chinese manufacturer King Long\u2019s self-driving buses for use in designated areas. Chery Automotive, another Chinese vehicle manufacturer, plans to mass produce Apollo-powered Level 3 autonomous vehicles by 2020. \n\n \n\n\u201cThis is the China speed in AI we talk about,\u201d says Lu. \u201cIf you asked the question whether L3/L4 (autonomous vehicles) would be commercially viable a year ago, nobody would believe it would be this early!\u201d\n\n \n\nAs Apollo\u2019s value increases so does Baidu\u2019s push for partnerships, especially in Silicon Valley. At the Mandarin Hotel event Udacity Founder Sebastian Thrun announced the online education platform will jointly launch a self-driving course with Baidu. It will be the first such Udacity partnership with a Chinese company.\n\nBaidu\u2019s DuerOS, a conversational AI platform for virtual assistants like Amazon\u2019s Alexa or Google Assistant, debuted at last year\u2019s CES. DuerOS supports home appliances like TVs and smart speakers and mobile devices such as phones or watches. Last July, DuerOS released open-source SDKs and APIs enabling developers to build third-party voice conversational services. Since then, over 130 companies have entered into partnerships with Baidu, coming up with over 20 hardware solutions. \n\n \n\n\u201cChina is a huge market for driving the accelerated pace of innovation. By 2020, 27% of Chinese households will have smart home systems, 51% will have smart cars, and 68% smartphones and wearables,\u201d says DuerOS lead Kun Jing. \n\n \n\nAt CES Jing introduced three smart device equipped with DuerOS, including the Little Fish VSI smart speaker, Sengled smart lamp speaker, and PopIn Aladdin, a voice-enabled dome light with projector functionality. \n\n \n\nBaidu is plotting a bold course \u2014 packaging its AI technologies into open-source platforms and releasing them for free. While it\u2019s unknown how or when Baidu will turn a profit from its heavy AI investments, it\u2019s certain that the company is driving domestic AI democratization, turning this \u201cChina speed\u201d idea into a reality. Says Lu, \u201cAI is borderless. Innovation benefits everyone.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-news-roundup-4abf8af4c90b",
        "title": "AI Biweekly News Roundup \u2013 SyncedReview \u2013",
        "text": "Dec 25th \u2014 Myntra Applies AI to Clothing Design\n\nIndian online fashion retailer Myntra is using algorithms to learn customers\u2019 favourite styles and create patterns. The AI-designed merchandise is on sale through two in-house brands. Owned by India\u2019s most prominent e-commerce company Flipkart, Myntra also uses augmented reality on mobile devices so customers can rate their purchases and retailers can provide them with more precise recommendations. \n\n \n\nDec 27th \u2014 San Jose Negotiates With Partners to Deploy Autonomous Vehicle Transit\n\nFollowing a Request for Information (RFI) filed last year, the City of San Jose has entered into negotiations with six companies for its ambitious Level 4 automated car pilot project. The city hopes the democratization of autonomous transit will reduce traffic and greenhouse gas emissions. \n\n \n\nDec 31st \u2014 Can Google Street View Images See the Future?\n\nAI researchers at Stanford University say they have found a way to predict demographics and even voting patterns by using a dataset of 50 million Google Street View images. Their vote predictions have been surprisingly close to the actual results.\n\nJan 2nd \u2014 Ctrip and Baidu Launch AI Pocket Chinese-English Translator \n\nChinese online travel services provider Ctrip launches a real-time, palm-sized AI Pocket translator. The Baidu-powered device is able to process a Chinese-English translation as soon as the user finishes speaking. Ctrip aims to add more than 80 languages in the near future.\n\n \n\nJan 3rd \u2014 Canada To Use AI to Track Suicide Trends on Social Media \n\nOttawa-based artificial intelligence services company Advanced Symbolics announces a pilot partnership with the Canadian government to predict suicide trends by monitoring activity on social media platforms. The three-month project will attempt to identify Canadian communities or regions that are susceptible to suicide spikes.\n\n \n\nJan 4th \u2014 Samsung\u2019s Exynos 9810 Has Machine Learning Features Similar to Apple\u2019s A11 Chips\n\nSamsung has launched a new processor for mobile devices. The Exynos 9 Series 9810 supports features like depth-sensing face detection and deep learning capabilities. The new features allow the processor to recognize people or items in photos for fast image searching or categorization. Both Apple and Samsung are working to put advanced deep learning capabilities into mobile processors to enable more advanced and exciting mobile applications.\n\n \n\nJan 4th \u2014 Microsoft and Adaptive Biotechnologies Partner up for Human Immune System Decoding\n\nMicrosoft and Adaptive Biotechnologies announce a joint agreement to leverage artificial intelligence and machine learning technologies to decode the human immune system. The goal is to create a global standard blood test that can detect a wide range of diseases through a patient\u2019s immune system information.\n\n \n\nJan 4th \u2014 Volkswagen and Hyundai Build Partnerships with Aurora\n\nAurora, a start-up founded in 2016 by former Google autonomous driving experts, announces a partnership with Volkswagen and Hyundai. Aurora is working on an entirely autonomous driving solution. Hyundai will test the Aurora prototype on fuel cell SUVs this year. Volkswagen did not disclose any information about their test prototype vehicle.\n\nJan 5th \u2014 Google Cloud Announces Discounted Preemptible GPU Services\n\nGoogle Cloud launches a price schema for preemptible GPUs. Customers can use the GPUs at about 50% of the regular price, but Google may shut them down when it needs the resources. Popular NVIDIA K80 and NVIDIA P100 GPUs support this price schema. The preemptible GPUs should fit any fault-tolerant machine learning workloads.\n\n \n\nJan 5th \u2014 Google Chrome to Use Machine Learning to Increase User Safety\n\nGoogle begins using machine learning as an expansion of its abuse protection service to reduce harm to Chrome users. The browser will have the ability to examine each installation request for bad signals in ads and web pages and will selectively auto-disable any high-risk redirect or installation request."
    },
    {
        "url": "https://medium.com/syncedreview/yamaha-sets-the-pace-for-self-driving-motorcycles-2f5288d068b5",
        "title": "Yamaha Sets the Pace for Self-Driving Motorcycles \u2013 SyncedReview \u2013",
        "text": "The self-driving car is one of the most high-profile real world implementations of AI. This is in part because it\u2019s easy to see how such vehicles can revolutionize things like commuting, package delivery, long-distance trucking and so on. While it\u2019s a bit harder to imagine the practical applications of a self-driving motorcycle, Yamaha has gone ahead and built a couple anyway.\n\nA creation of Yamaha\u2019s new Innovation Center, MOTOROiD will make its international debut this week at the Consumer Electronics Show (CES) in Las Vegas.\n\nMOTOROiD\u2019s computer vision and face recognition system allows the user to summon the bike with the wave of a hand. Stability is achieved via an AI-trained \u201cActive Mass Center Control System\u201d (AMCES) which shifts the centre of gravity in real time by rotating parts of the machine such as the battery, swingarm and rear wheel around an axis running through the center of the bike. An inertial measurement system uses a gyro sensor to detect axis rotation and an accelerometer to detect velocity, and sends data to the bike\u2019s onboard control unit with a lag of less than 0.0005 seconds. MOTOROiD has a lithium ion battery, 3D-printed tires, and a haptic HMI device that cradles the rider\u2019s waist. Yamaha describes the riding experience as one in which \u201cthe rider resonates harmoniously with the machine.\u201d\n\nAt present MOTOROiD only operates at low speeds, but there are plans to push this performance metric in the future.\n\nYamaha\u2019s other new autonomous bike project is not actually a bike, but a humanoid robot called MOTOBOT which can drive a stock motorcycle without any major modifications. MOTOBOT was developed in collaboration with San Francisco research institute SRI International\u2019s robotics program. Says a Yamaha CES Media Spokesperson, \u201cYamaha was mainly involved in designing the algorithms for high speed motorcycle riding, analysis of vehicle dynamics using simulations, and reliability design of electronics; while SRI was involved in developing the robot based on our requirements, and developing the position sensing system.\u201d\n\nMOTOBOT uses its actuators to manipulate a stock bike\u2019s steering, throttle, brake, clutch and shift controls in much the same way a human would. Unlike its relatively sluggish cousin MOTOROiD, MOTOBOT is a speed freak that has already surpassed its project\u2019s straight line velocity target of 200 kph.\n\nOne of the MOTOBOT development advisors is Italian motorcycle racing champion Valentino Rossi. Yamaha\u2019s goal is to get its bot up to the level where it can better Rossi on a racetrack.\n\nLast October at the Thunderhill Raceway Park in Sacramento Valley, California, MOTOBOT clocked a lap time of 117.5 seconds compared to Rossi\u2019s 85.7 seconds on the two mile (3.2 km) circuit. SRI says refinement of the bot\u2019s high-precision GPS, sensor fusion and machine learning technologies will enable MOTOBOT to improve its decisions and optimize track line and race performance. Yamaha boldly predicts the bot will outperform Rossi within two years. Is that possible?\n\nIt\u2019s worth noting that skeptical AI researchers had estimated no machine would beat a top human in the ancient board game Go for another ten years \u2014 that is, until DeepMind\u2019s AlphaGo dispatched world-best Ke Jie last spring in Beijing. After the series, professional Go players began adopting many of AlphaGo\u2019s novel opening strategies into their own games. Might MOTOBOT one day similarly introduce humans to new and more efficient ways of racing a motorcycle around a track?\n\nOnly time will tell. However, like AlphaGo, MOTOBOT is more a proof of concept project. Yamaha says one of the possible practical applications might be using MOTOBOT as a test driver. \u201cWe hope to visualize the human side of motorbike riding and use the information gathered to identify the relationship between the rider and the bike. That would help us to develop more exciting vehicles in future. We could also adopt the control and maneuvering programs to other vehicles such as marine jets and snowmobiles.\u201d\n\nIt\u2019s clear that even traditional manufacturers such as Yamaha cannot afford to ignore AI\u2019s potential for industry disruption, and so are driven to embrace the tech, using it to reimagine their products and push limits. With BMW and Honda also working on self-driving bikes, Yamaha\u2019s first-ever appearance at the CES is bound to rev up this two-wheeled innovation race."
    },
    {
        "url": "https://medium.com/syncedreview/gary-marcuss-deep-learning-critique-triggers-backlash-62c137a47836",
        "title": "Gary Marcus\u2019s Deep Learning Critique Triggers Backlash",
        "text": "It took a mere 72 hours for deep learning researchers to ignite the first AI Twitter debate of 2018.\n\nOn January 2, NYU Professor and Founder of Uber-owned machine learning startup Geometric Intelligence Gary Marcus published the paper Deep Learning: A Critical Appraisal on ArXiv. The paper shortlisted problems that are keeping current research from actualizing artificial general intelligence.\n\nMarcus\u2019s central argument was that present deep learning systems have failed in inference beyond specific datasets they have seen. He listed ten challenges facing deep learning research, such as data hungriness, lack of transparency, inability to extrapolate, and difficult to engineer.\n\nMarcus said his greatest fear is that AI will get pigeonholed as a \u201clocal minimum, focusing too much on the detailed exploration of a particular class of accessible but limited models,\u201d and forget its mission to march towards artificial general intelligence. He then raised possibilities for a future beyond deep learning: focus more on unsupervised learning; symbol-manipulation (aka GOFAI, the Good Old-Fashioned AI); derive insights from cognitive and developmental psychology; or focus more on acquiring common sense knowledge, scientific reasoning, game playing, etc.\n\nA day later, former AAAI Co-chair and NIPS Chair Thomas G. Dietterich countered Gary Marcus\u2019 article with no less than 10 tweets, calling it a \u201cdisappointing article\u2026 DL learns representations as well as mappings. Deep machine translation reads the source sentence, represents it in memory, then generates the output sentence. It works better than anything GOFAI ever produced.\u201d\n\nDietterich added that \u201cDL is essentially a new style of programming \u2014 differentiable programming \u2014 and the field is trying to work out the reusable constructs in this style. We have some: convolution, pooling, LSTM, GAN, VAE, memory units, routing units, etc.\u201d\n\nLong-time deep learning advocate and Facebook Director of AI Research Yann LeCun backed Dietterich\u2019s counter-arguments: \u201cTom is exactly right.\u201d In a response to MIT Tech Review Editor Jason Pontin and Gary Marcus, LeCun testily suggested that the later might have mixed up \u201cdeep learning\u201d and \u201cunsupervised learning\u201d, and said Marcus\u2019s valuable recommendations totalled \u201cexactly zero.\u201d\n\nMarcus and LeCun have a history, they squared off in a New York University debate last October. Marcus is an advocate for deep learning integrating with human cognitive sciences, whereas LeCun is not thrilled by that possibility. You can watch the NYU debate here: https://www.youtube.com/watch?v=aCCotxqxFsk\n\nSome Reddit users argued that Marcus had ignored technical details and recent advancements such as GANs, zero-shot and few-shot deep learning methods. Redditor Gwren commented, \u201cif anything, I came out more convinced DL is the future, if that is the best the critics can do\u2026\u201d\n\nSo is deep learning hitting a wall as argued in Marcus\u2019 paper? Researchers have raised questions: last year Turing Award winner and University of California Los Angeles Professor Judea Pearl\u2019s paper Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution argued that human-level AI cannot emerge from model-blind learning machines that ignore causal relationships. He brought the discussion to NIPS in December.\n\nAt an AI conference in Montreal last October the deep learning trio of Geoff Hinton, Yann LeCun, and Yoshua Bengio agreed that deep learning research is no longer on the fast track. And this raises the questions: have we hit a wall? Opinions bifurcate, but as we roll into 2018 one thing appears certain: this won\u2019t be the last AI Twitter debate of the year."
    },
    {
        "url": "https://medium.com/syncedreview/woobo-makes-ai-cuddly-6de06d735b2e",
        "title": "Woobo Makes AI Cuddly \u2013 SyncedReview \u2013",
        "text": "AI-powered machines are revolutionizing transportation, taking over factories and managing our daily lives and home environments. Although most adults are now accustomed to smart devices, children aged 4\u20139 have thus far had less cause to communicate with virtual assistants or operate other AI-powered products. A Boston-based company is changing that with its new intelligent plush toy.\n\n \n\nMeet \u201cWoobo,\u201d the cuddly children\u2019s smart companion robot. Like Siri or Alexa, Woobo listens to and learns from its human owner. It can answer questions, engage in conversations, and even tell jokes. Children squeeze Woobo\u2019s stubby limbs to activate its home menu, or press on its ears to activate voice commands. Woobo\u2019s touchscreen meanwhile displays facial expressions and other visual elements such as videos and interactive games.\n\nMIT alumnus Feng Tan is Woobo\u2019s co-founder and CEO, and the main mind behind the smart toy. Tan got the idea for an educational children\u2019s robot in 2015, when he was still a PhD candidate, and built a prototype during his one-month winter break. Despite looking something like a green mop with eyes, the first crude Woobo garnered both praise and the third-place prize of US$2,000 at the 2015 MIT Business Plan Competition. \u201cI remember our product got the most applause and cheers,\u201d says Tan.\n\n \n\nFollowing the competition, a venture capitalist offered Tan seed funding. There\u2019s good reason for such investment \u2014 according to P&S Market Research, the global personal robots market is projected to reach US$34.1 billion by 2022.\n\nDuring the two years of Woobo\u2019s development Tan interviewed over 200 families to gain a better understanding of what a children\u2019s robot should look like and what it should do. He discovered for example that children prefer to manually activate Woobo rather than using the voice activation method associated with typical virtual assistants. Children also tended not to conduct multiple-round conversations, opting instead for single-round question-answer interactions. \n\n \n\nDi Wang, co-founder & CTO of Woobo, built an intelligent system that returns an actual answer rather than listing documents or web search results. A PhD from Carnegie Mellon University, Wang won the TREC 2015 Live Question Answering Track in 2015 and 2016. The responses returned by Wang\u2019s system, dubbed WaterlooClarke, have been favourably rated by users on online Q&A interface Yahoo Answers, and the technology has been integrated with Woobo.\n\n \n\nWang\u2019s technical team continuously monitors children\u2019s\u2019 most asked questions to update the system, which covers child-specific topics as well as open-domain questions such as \u201cWhat should I do if my kitten is pregnant?\u201d\n\n \n\nIn recent months Wang has pushed the technology to the next level by adding a social chatbot with a dialogue personalization feature. Empowered by end-to-ending deep learning and transfer learning, Woobo is able to start a casual conversation for example in the style of Batman, simply by studying Batman scripts. \u201cWoobo can become Batman, Lincoln, or anyone,\u201d says Wang, who is now in the process of designing and developing a unique Woobo character profile.\n\n \n\nIn late 2017, the company sent the first 3,000 Woobos to families who had pre-ordered the product through Woobo\u2019s kickstarter campaign. \n\n \n\nSarah Losie of Ohio says Woobo has quickly integrated into her four-year-old daughter Violet\u2019s daily life. The smart doll\u2019s sharp question-answering ability has engaged the child, who also loves the games and multimedia stories. Losie told Synced that Woobo recently took Violet on a virtual adventure to China, which led to mother and daughter discussing other countries and languages, the Great Wall, and viewing photos of historical interest. \n\n \n\n\u201cWe\u2019ve had a lot of great conversations start because of Woobo. My daughter has a blast playing with him and learning from him, and I can tell that this will be a toy that will grow with her,\u201d says Losie. \n\n \n\nTan says Woobo\u2019s goal is to create a supportive environment in which AI can educate, entertain and provide companionship: \u201cI hope Woobo can make children feel more relaxed at home, just like being with a puppy or a kitten.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/tech-giants-are-gobbling-up-ai-startups-22b10d7b9871",
        "title": "Tech Giants Are Gobbling Up AI Startups \u2013 SyncedReview \u2013",
        "text": "It\u2019s no surprise that tech companies are eager to stay abreast of AI developments. When they are not luring AI talents with huge compensation packages, they are on the lookout for AI startups suitable for acquisition. According to a CBInsight report, over 250 private companies using AI algorithms across different verticals have been acquired since 2012, with 37 acquisitions taking place in Q1'17 alone.\n\nGoogle tops the acquisition chart with 13 known adds since 2013, doubling Microsoft or Amazon\u2019s numbers. The acquisitions have a diverse geographical distribution \u2014 early pickups DNNresearch and Granata Decision Systems were Canadian, while 2014 acquisitions DeepMind Technologies, Vision Factory, and Dark Blue Labs were based in the UK. In 2016 the company purchased Paris-based computer vision startup Moodstocks, and last year acquired Bengaluru startup Halli Labs and Belarus-based AIMatter. Back home in California, Google pocketed predictive modeling and analytics competitions platform Kaggle to further deepen its talent pool.\n\nFollowing closely behind with eight acquisitions, Apple is also aggressively adding international startups to its AI portfolio. In 2014 it acquired Novauris Technologies and Vocal IQ from the UK, followed by Indian data-mining startup Tuplejump and Israeli facial recognition company RealFace. Apple also bought US-based Lattice Data, a company that uses AI inference engines to sort unstructured data.\n\nFacebook meanwhile has made a number of pricey vertical domain acquisitions in computer vision, designed to streamline with the company\u2019s high-profile moves in AR. In 2012 they bought Tel Aviv-based technology company Face.com, which developed a platform for efficient and accurate facial recognition in photos.\n\nFast forward to 2016 and the company\u2019s acquisition of another computer vision company, Zurich Eye, which is now operating as a subsidiary of Oculus VR. Facebook\u2019s purchase of Masquerade Technologies Inc. targets live video for customer interaction along with masquerade face tracking and 3D Effects Rendering SDK, which integrates video filters into iOS and Mac OS X applications. The company also grabbed German computer vision startup Fayteq, which has developed a process for adding or removing objects from videos using computer vision.\n\nQualcomm acquired the University of Amsterdam spinoff Scyfer, which builds generalized AI solutions for industries such as manufacturing, healthcare, and finance. As the company targets mobile applications and industrial IoT, some industry insiders believe Scyfer will function as a software unit to support Qualcomm\u2019s hardware endeavors.\n\nVentureBeat describes Intel\u2019s approach to acquisitions as a \u201cbinge\u201d \u2014 the world chip leader has invested over US$1 billion in AI companies. It picked up deep learning and computer vision processor chip manufacturer Movidius in 2016, releasing the Movidius Myriad X chip on the acquisition\u2019s first anniversary. Intel also added AI software companies Nervana Systems and Itseez last year to rebrand towards more AI-oriented tasks.\n\nAI startups are hot properties, and even traditional manufacturing companies are getting in the game. Last summer American car maker Ford shelled out US$1 billion for Argo AI, a move aimed at developing a virtual driver system. As we move through 2018 we can expect big tech and other deep pocketed players to further accelerate the pace of startup acquisitions."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-dec-pt-2-ccec844e84b8",
        "title": "AI Biweekly: 10 Bits from Dec (Pt 2) \u2013 SyncedReview \u2013",
        "text": "Researchers at the MIT Media Lab partner with McKinsey to create a machine learning program, which can recognize and predict emotional responses from movies. The program studies facial expressions and other details to understand the level of emotional response a scene elicits from viewers. It successfully learns to recognize the emotional content of videos and separates the positive from negative.\n\nAfter the Fukushima Daiichi nuclear accident in 2011, the Japanese government decides to redesign the country\u2019s 300-GW of electricity grids. This huge workload provides an opportunity for AI companies. Stem Inc, an international provider of AI-driven energy storage services is working with Mitsui & Co., Ltd. to build the first energy storage unit in Japan. Stem\u2019s AI will help reduce energy costs by providing better control and efficiency.\n\nNew York City plans to implement a dedicated team of experts that will monitor the fair use of algorithms in municipal agencies. The team is responsible for issuing a report that describes the procedure made by AI decision systems. The act primarily targets biases inherent in training data. The whole process is designed to ensure \u201cfairness, accountability, and transparency\u201d. According to news sources, the report will be made available to the public in 18 months.\n\nEBay acquires Toronto-based e-commerce analytics startup Terapeak and aims to integrate it into its Seller Hub. Based on transaction data collected, Terapeak provides sellers with suggestions of product listing, inventory management, and pricing. With insights on the market, Terapeak can help mitigate risk and aid seller decision-making process. As the result of this acquisition, Terapeak with work with eBay exclusively with deal closing before the end of 2017.\n\nMicrosoft is implementing AI & IoT projects in different part of India. Notable examples include Microsoft FarmBeats focusing on data collection for agriculture, an AI-based sowing APP for farmers, smart street lighting in the city of Jaipur, and AI-powered interactive cane to help the visually-impaired. These projects demonstrate the use of AI in Asia\u2019s most populous country.\n\nNASA has detected a star like our sun using a Google AI trained by data from the Kepler Space Telescope. The network was trained with 15,000 labeled data from the Kepler exoplanet catalog and was able to learn to detect weaker signals. NASA plans to use the neural network to examine more incoming data from the Kepler telescope which consists of 150,000 more stars.\n\nIn order to remain competitive with Microsoft and Google in the cloud computing business, Amazon establishes the ML Solutions Lab, which allows company experts to guide clients in developing products through AWS. Based on numbers from the latest quarter financial report, AWS generated US $4.6 billion in revenue, demonstrating that AWS has become one of Amazon\u2019s most profitable business unit.\n\nMicrosoft will invest US $50 million commitment in the AI for Earth program over the next five years. The company will grant universities, companies, and other organizations free use of Microsoft\u2019s AI technology including Azure cloud and mapping tools. Companies working on climate change, water, agriculture, and biodiversity are eligible to apply. One example is Project Premonition which focuses on detecting early pathogens that can cause diseases.\n\nMIT Laboratory for Information and Decision Systems (LIDS) and Michigan State University introduce a new system that automates machine learning model selection. The new system is called ATM (Auto-Tuned Models) and runs in the cloud. It basically searches over a large number of modeling options with high-throughput and finds the best modeling technique for a given problem. It also tunes hyperparameters. It either outperforms human data scientists or comes close in terms of choosing the right solution, but it is much faster than a human.\n\nIndia\u2019s National Association of Software and Services Companies (NASSCOM) and China\u2019s Dalian Municipal People\u2019s Government announce the joint investment for an AI and IoT collaborative platform. The partnership will facilitate technical exchanges between Indian and Chinese companies. The Sino-Indian Digital Collaboration Plaza (SIDCOP) platform will be able to operate both online and offline environment."
    },
    {
        "url": "https://medium.com/syncedreview/synced-christmas-release-trends-of-ai-technology-development-report-c7eaff5c59ba",
        "title": "Synced Christmas Release: Trends of AI Technology Development Report",
        "text": "AI has been called the most transformative technology of our times. But what exactly is AI?\n\nIn our Christmas special, Trends of AI Technology Development \u2014 A comprehensive review of the past, present and future of modern AI research development, our tech analysts cut through the technical jargon and misleading media coverage to break AI down into easily understandable parts. Whether you are a researcher, tech enthusiast, entrepreneur, investor, or student, the report will help you sort out the basics and provide you with the necessary background to move forward.\n\nWe\u2019ll review all the mainstream AI research fields, with a focus on:\n\nThe report is organized based on:\n\nThe report explores in detail the respective theories, applications, potential bottlenecks, and future prospects of these 23 subcategories. We also identify influential researchers, papers, and application models. The heat map below provides a sneak preview, reflecting the history of R&D efforts in each of the subcategories.\n\nClick here to get the full report and subscribe to our upcoming AI newsletter.\n\nIf you have any feedback, please let us know by in the comment section below. If you would like to support the publication of the report, you are also more than welcome!\n\nWe have witnessed epoch-making breakthroughs in AI technology this year, and received valuable feedback from our readers. Our global editorial and analyst teams will continue to work hard to bring you the most up-to-date and accurate global machine intelligence content in the year to come.\n\nHappy holidays to you and yours from all of us at Synced!"
    },
    {
        "url": "https://medium.com/syncedreview/2017-in-review-10-ai-failures-4a88da1bdf01",
        "title": "2017 in Review: 10 AI Failures \u2013 SyncedReview \u2013",
        "text": "This year artificial intelligence programs AlphaGo and Libratus triumphed over the world\u2019s best human players in Go and Poker respectively. While these milestones showed how far AI has come in recent years, many remain sceptical about the emerging technology\u2019s overall maturity \u2014 especially with regard to a number of AI gaffes over the last 12 months.\n\n \n\n At Synced we are naturally fans of machine intelligence, but we also realize some new techniques struggle to perform their tasks effectively, often blundering in ways that humans would not. Here are our picks of noteworthy AI fails of 2017.\n\nFace ID, the facial recognition technique that unlocks the new iPhone X, was heralded as the most secure AI activation method ever, Apple boasting the chances of it being fooled were one-in-a-million. But then Vietnamese company BKAV cracked it using a US$150 mask constructed of 3D-printed plastic, silicone, makeup and cutouts. Bkav simply scanned a test subject\u2019s face, used a 3D printer to generate a face model, and affixed paper-cut eyes and mouth and a silicone nose. The crack sent shockwaves through the industry, upping the stakes on consumer device privacy and more generally on AI-powered security.\n\nThe popular Amazon Echo is regarded as among the more robust smart speakers. But nothing\u2019s perfect. A German man\u2019s Echo was accidentally activated while he was not at home, and began blaring music after midnight, waking the neighbors. They called the police, who had to break down the front door to turn off the offending speaker. The cops also replaced the door lock, so when the man returned he discovered his key no longer worked.\n\nThis July, it was widely reported that two Facebook chatbots had been shut down after communicating with each other in an unrecognizable language. Rumours of a new secret superintelligent language flooded discussion boards until Facebook explained that the cryptic exchanges had merely resulted from a grammar coding oversight.\n\nA self-driving bus made its debut this November in Las Vegas with fanfare \u2014 resident magicians Penn & Teller among celebrities queued for a ride. However in just two hours the bus was involved in a crash with a delivery truck. While technically the bus was not responsible for the accident \u2014 and the delivery truck driver was cited by police \u2014 passengers on the smart bus complained that it was not intelligent enough to move out of harm\u2019s way as the truck slowly approached.\n\nA CNN staff member received an emoji suggestion of a person wearing a turban via Google Allo. This was triggered in response to an emoji that included a pistol. An embarrassed Google assured the public that it had addressed the problem and issued an apology.\n\nHSBC\u2019s voice recognition ID is an AI-powered security system that allows users to access their account with voice commands. Although the company claims it is as secure as fingerprint ID, a BBC reporter\u2019s twin brother was able to access his account by mimicking his voice. The experiment took seven tries. HSBC\u2019s immediate fix was to establish as account-lockout threshold of three unsuccessful attempts.\n\nBy slightly tweaking a photo of rifles, an MIT research team fooled a Google Cloud Vision API into identifying them as helicopters. The trick, aka adverse samples, causes computers to misclassify images by introducing modifications that are undetectable to the human eye. In the past, adversarial examples only worked if hackers know the underlying mechanics of the target computer system. The MIT team took a step forward by triggering misclassification without access to such system information.\n\nResearchers discovered that by using discreet applications of paint or tape to stop signs, they could trick self-driving cars into misclassifying these signs. A stop sign modified with the words \u201clove\u201d and \u201chate\u201d fooled a self-driving car\u2019s machine learning system into misclassifying it as a \u201cSpeed Limit 45\u201d sign in 100% of test cases.\n\nMachine Learning researcher Janelle Shan trained a neural network to generate new paint colors along with names that would \u201cmatch\u201d each colour. The colours may have been pleasant, but the names were hilarious. Even after few iterations of training with colour-name data, the model still labeled sky blue as \u201cGray Pubic\u201d and a dark green as \u201cStoomy Brown.\u201d\n\nThe Amazon Alexa virtual assistant can make online shopping easier. Maybe too easy? In January, San Diego news channel CW6 reported that a six-year-old girl had purchased a US$170 dollhouse by simply asking Alexa for one. That\u2019s not all. When the on-air TV anchor repeated the girl\u2019s words, saying, \u201cI love the little girl saying, \u2018Alexa order me a dollhouse,\u2019\u201d Alexa devices in some viewers\u2019 homes were again triggered to order dollhouses."
    },
    {
        "url": "https://medium.com/syncedreview/hosting-robot-brains-on-the-cloud-632957251f7a",
        "title": "Hosting Robot Brains on the Cloud \u2013 SyncedReview \u2013",
        "text": "The anticipated 2020 deployment of 5G networks will effect a tenfold speed increase compared to 4G LTE, creating an environment favourable for the mass integration of various robots into an AI-powered cloud. Beijing-based CloudMinds wants to create such a cloud.\n\n \n\nIn 2015, Bill Huang left his research position at the China Mobile Research Institute to found CloudMinds. Huang\u2019s business partner Robert Zhang is a telecommunication specialist and the former Head of Service Strategy and Operations for Samsung Telecommunications America. The company\u2019s Chinese name, Data Technology (\u8fbe\u95fc\u79d1\u6280), was inspired by his favorite sentient robot from Star Trek franchise.\n\nSynced spoke with Zhang at last month\u2019s AI World Forum in Toronto, where he participated in the panel discussion The Rise of the Robots. Panelists opined on the future of smart machines in human society, while Zhang advanced his idea of comprehensive cloud-based robot interconnectivity.\n\n \n\nSecurity is the raison d\u2019etre of CloudMinds\u2019 MCS (Mobile-Intranet Cloud Service), an end-to-end system comprising a XaaS cloud, a VBN network based on blockchain technology, and a mobile device for robot control. The network is independent of public networks and therefore immune to cyber attacks. \n\n \n\nCloudMinds\u2019 XaaS cloud hosts AI applications such as natural language processing, computer vision and speech recognition that can be accessed by robots. The company also produces its own mobile device for robot control, DATA A1, which can access both the public cloud and CloudMinds\u2019 private cloud; and the wearable META helmet for guiding the visually impaired.\n\nFunction pipeline of CloudMind\u2019s Mobile-Intranet Cloud Service\n\n \n\nBig ideas need big money, and thus far CloudMinds has raised some US$100 million in funding from SoftBank tycoon Sun Junyi, Foxconn, Keystone Venture, and Walden International. By mid-2016 the company had 180 employees and a budget of US$2 million per month. This may seem extravagant for a startup, but as CEO Bill Huang explains, \u201ca normal startup would not be able to secure the funding needed to enter this market.\u201d\n\n \n\nInvestors have put both their dollars and their faith into CloudMinds\u2019 high-profile founders and their vision that \u201cby 2025, helpful humanoid robots will be affordable for the average household.\u201d\n\n \n\nZhang says CloudMinds is now searching for suitable markets: \u201cWe are looking at certain verticals, such as household vacuuming robots. We plan to start with China and the US, there are lots of business opportunities here, and then move on to Japan and Europe.\u201d The company currently has offices in Silicon Valley, Tokyo, and Beijing, and is looking to establish a base in Canada. \n\n \n\nRegarding competition in the IoT cloud market, Zhang says \u201cwe fend off competition by doing something none of these companies would be interested in doing. Our system can load Amazon or Google\u2019s public cloud. By maintaining a neutral position we can do benchmarking in search of better technology.\u201d \n\n \n\nThe rapid growth of cloud computing over the last 20 years has made it possible to integrate mobile computing, wireless networks, cloud computing, and now AI. Zhang says it will take still some time to fully evolve the CloudMind business plan: \u201cThe whole robotics market is still changing, but we are pretty clear on the fundamental components of this market and will respond nimbly and flexible.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/a-statistical-tour-of-nips-2017-438201fb6c8a",
        "title": "A Statistical Tour of NIPS 2017 \u2013 SyncedReview \u2013",
        "text": "The Conference on Neural Information Processing Systems (NIPS) was first held in Denver in December 1986. The leading annual gathering of its kind, NIPS has since visited a number of Canadian and Spanish cities and will again head up to Montr\u00e9al in 2018. According to official statistics, NIPS 2017 in Long Beach, California was the most popular yet, attracting over 8,000 registered attendees, up 2,000 from last year.\n\nIndustry is playing an increasing role in academic conferences. Corporate sponsorship and paper submissions from corporate labs are now a norm. The following information is based on sponsor contributions as listed on the NIPS official website.\n\nThe 31st NIPS attracted 84 sponsors (totaling US$1.76 million), an increase of 31.5% from the previous year (64 sponsors totaling US$840,000). In 2017, NIPS added a US$80,000 \u201cDiamond\u201d sponsorship category, while the Platinum sponsorship threshold rose from US$25,000 to US$40,000.\n\n \n\nSoaring fees have not dampened sponsors\u2019 enthusiasm. Five Diamond sponsors contributed US$400,000 to the conference this year. The more affordable Gold and Silver category sponsors also increased dramatically. Meanwhile, entry-level Bronze sponsors fell 70% from last year, the dip likely related to the committee\u2019s more rigorous sponsorship screening process.\n\n \n\nHalf of the corporate sponsors were from the tech industry, followed by around 28% from the financial industry (banking, trading, and financial services). Our on-site research suggested that finance companies largely participated for recruitment purposes and to stay abreast the latest trends in machine intelligence. However, few recruiting companies were able to provide clear and concrete hiring objectives or job descriptions, instead they gave vague responses, for example that candidates should simply be \u201cthird-year PhD students,\u201d or \u201cable to analyze big data.\u201d\n\nThere were nine top-level subjects and 156 subareas at this year\u2019s conference. The latter witnessed a 150% increase from last year, reflecting growing research diversification. The hottest subareas were: 1) algorithms; 2) deep learning; and 3) application.\n\n \n\n We extracted technical keywords from all presentations and poster titles. Buzzwords such as \u201clearning,\u201d \u201cdeep,\u201d and \u201cneural networks\u201d topped the list; with related terms such as \u201cstochastic method,\u201d \u201coptimization,\u201d \u201cBayesian,\u201d \u201creinforcement learning,\u201d \u201cadversarial,\u201d \u201cinference,\u201d and \u201cGaussian process\u201d also frequently showing up. Interestingly, although \u201creinforcement learning\u201d and \u201cdata\u201d were smaller subareas, these terms appeared much more frequently in paper titles.\n\nAccording to statistics compiled by Infinia ML CEO Robbie Allen, the most active paper-submitting academic institutions were Carnegie Mellon University, the Massachusetts Institute of Technology, and Stanford University; while corporate paper submissions were led by Google, Microsoft, and IBM. Interested readers can pursue Allen\u2019s breakdown analysis here.\n\nAccording to sample survey of close to 100 conference attendees, we gathered that 77% percent came to NIPS to learn about AI trends, 15% for networking, and 8% for job opportunities.\n\nSome NIPS 2017 events were streamed live on Facebook (excluding Symposiums and Workshops). Most of the videos had views in the hundreds one week after the conference ended. Among the more popular videos was Yann LeCun\u2019s \u201cGeometric Deep Learning on Graphs and Manifolds,\u201d which received 8,300 views. A video combining speeches by Deep Genomics Founder Brendan Frey and Ali Rahimi\u2019s \u201cMachine Learning has become Alchemy\u201d received 1,900 views. As with all contemporary events, social media popularity is a key factor in content distribution. \n\n \n\n Based on on-site session attendance data extracted from the official NIPS phone App, we can see that aside from the poster sessions, Hall A\u2019s tutorial attracted the most participants for the following keynotes: \u201cDeep Learning: Practice and Trends,\u201d \u201cDeep Probabilistic Modelling with Gaussian Processes,\u201d and \u201cGeometric Deep Learning on Graphs and Manifolds.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/embodying-robots-with-intelligence-a91f7af05531",
        "title": "Embodying Robots With Intelligence \u2013 SyncedReview \u2013",
        "text": "Meshing a couple of cogwheels looks like child\u2019s play \u2014 they just seem to naturally fit together. However, monitoring gears\u2019 angular differences and optimizing their alignment and rotation is an engineering problem that can stump an industrial robot. Engineers have long struggled to teach robots how to perform actions that appear simple by human standards. Former OpenAI research scientist Peter Chen believes the solution is transforming such robots into teachable apprentices.\n\nThis September, Chen and his mentor Pieter Abbeel \u2014 one of the top minds in robotic technology and artificial intelligence \u2014 launched Embodied Intelligence, a company dedicated to developing robots that can easily learn new skills without sophisticated programming. Rocky Duan from OpenAI and Tianhao Zhang from Microsoft are also co-founders.\n\n\u201cMore and more young people are reluctant to take on boring and repetitive tasks in factories or warehouses, while current industrial robots cannot adapt to flexible industrial manufacturing that allows the system to react in case of changes,\u201d says Chen.\n\nChen\u2019s approach involves three stages: teaching robots, reducing the cost of teaching robots, and enabling robots to learn by themselves.\n\nThe first stage was completed two years ago at the Berkeley Artificial Intelligence Research (BAIR) laboratory, the top robotics lab where Chen worked as a PhD student. The lab successfully enabled robots to use their vision sensors to recreate actions they had observed. Deep neural networks proved to be a good computational architecture for this.\n\nHowever, the research stalled in the theoretical stage. The process of training robots requires pre-programming and debugging by humans who have deep learning knowledge. A university lab can ask a few doctoral students to work day and night, but this method is neither time nor cost effective, and certainly not applicable in industry.\n\nThis year, Chen\u2019s team implemented imitation learning \u2014 a method for robots to learn specific movements from human demonstrations.\n\nIn Chen\u2019s Emeryville, California office, a colleague dons a virtual reality headset and performs a series of hand movements which the robot tracks in real time. This data is fed into a deep neural network and the robot replicates the movements until it concludes that it has learned to perform them without human assistance. The entire process takes just 30 minutes.\n\n\u201cEven multiplying this 30-minutes by 100 times is far less than the usual cost of training robots,\u201d says Chen.\n\nThis method can be applied to different movements without changing the codes used in VR, demonstration collection, training, or the neural network. Only the demonstrations are different.\n\nHumans\u2019 movements are not always optimal, and robots can discover new and more efficient ways of performing their tasks through a trial-and-error process enabled by reinforcement learning. Chen believes an integrated solution of imitation learning and reinforcement learning will be available to industries as early as next year.\n\n\u201cRecent breakthroughs in AI have enabled robots to learn locomotion, develop manipulation skills from trial and error, and to learn from VR demonstrations. However, all of these advances have been in simulation or laboratory environments,\u201d says Sunil Dhaliwal, General Partner at Embodied Intelligence\u2019s principal investor Amplify Partners. \u201cThe Embodied Intelligence team that led much of this work will now bring these cutting-edge AI and robotics advances into the real world.\u201d\n\nTeaching robots is not Chen\u2019s ultimate goal. Meta Learning, a machine learning approach that uses self-learning instead of expert data, is what the company plans to pursue over the next five to ten years. Meta learning is widely regarded as a possible pathway to artificial general intelligence (AGI), the long-range, human-intelligence-level target of contemporary AI technology.\n\n\u201cAt this stage, we are only trying to reduce teaching time. Ultimately, we want to make robots as humanlike as possible,\u201d says Chen.\n\nChen has had encouraging research results with meta learning. This year, he and Abbeel published two essays on arXiv. Meta Learning Shared Hierarchies explores an approach for learning hierarchical strategies by using shared primitives to improve sample efficiency without tasks; while Meta-Learning with Temporal Convolutions proposes a class of simple and generic meta-learner architectures, based on temporal convolutions, that is domain-agnostic and has no particular strategy or algorithm encoded into it.\n\nSays Chen, \u201cOur envisioned smart robot will have both meta learning and reinforcement learning. Reinforcement learning performs well on a single mission, while meta learning allows robots to learn more quickly.\u201d\n\nThe race to develop smart robots is highly competitive. This year, Google introduced a self-supervision imitation method that teaches robots simple skills through human demonstration videos. Startups are also in the game \u2014 in Union City, a 20-minute drive from Emeryville, Vicarious.ai is developing smart robots by simulating the human visual cortex in conjunction with generative models.\n\nNo one can predict how or when we might achieve AGI in smart robots, or who will get there first. But Embodied Intelligence is definitely a standout competitor in the race."
    },
    {
        "url": "https://medium.com/syncedreview/2017-in-review-10-leading-ai-hubs-e6f4d8a247ee",
        "title": "2017 in Review: 10 Leading AI Hubs \u2013 SyncedReview \u2013",
        "text": "Artificial intelligence and machine learning are 2017 buzzwords \u2014 but where is it all happening? According to a study by Wuzhen Institute, there are 2,905 artificial intelligence companies in the United States, 709 in China, 366 in Britain, 233 in India, 228 in Canada, 173 in Israel, and 160 in Germany. The world\u2019s top two AI hubs share a symbiotic relationship: based on a LinkedIn survey, about half of China\u2019s biggest AI employers are American companies.\n\nSynced cyber-circumnavigated the globe in search of today\u2019s top regional AI hubs. We based our picks on the following criteria: clustering of academic and research institutions, talent pipelines, venture capital and startup ecosystem, and top-down government policy support.\n\nThe biggest of the big, the San Francisco Bay Area is home to top-tier research universities Stanford, UC Berkeley, and UC San Diego. It has become a virtual AI talent pipeline for the FLAG companies (Facebook, LinkedIn, Amazon/Apple, Google), where the average machine learning scientist earns a whopping US$293,000. Over the past five years this area of just 700 square kilometers has attracted 41% of all global investment in AI. The 31st AAAI Conference on Artificial Intelligence was held in San Francisco last February.\n\nThe New York-Boston area has a focus on precision medicine and fintech. This is where you\u2019ll find a cluster of top-notch US east-coast academic institutions \u2014 MIT, Harvard, Boston University and Cornell \u2014 and well-developed cooperation with industry. The most active academic research group is the MIT Computer Science & Artificial Intelligence Lab (CSAIL). The New York \u2014 Boston area is home to the MIT-IBM Watson AI lab, and the go-to place for corporate AI industry conferences.\n\nEven if you know very little about AI, you have probably heard of London-based DeepMind, which built the bot that conquered the 2,500-year-old game of Go last year. The city which played a leading role in the Industrial Revolution is again at the forefront in AI and is Europe\u2019s most concentrated AI hub. With talent pipelines from Cambridge, Oxford and Imperial College, London has fostered research in cloud computing and AI hardware. Semiconductor company ARM is a spinoff of Cambridge University.\n\nIf you Google \u201cAI hub\u201d, you will find numerous entries recommending Montr\u00e9al. Backed by the Montreal Institute for Learning Algorithms (MILA) and the Institute for Data Valorisation (IVADO), the city knits together four local academic institutions including McGill and the Universit\u00e9 de Montr\u00e9al. Deep learning pioneer Yoshua Bengio calls Montr\u00e9al home, and the bilingual city hosts many American tech companies\u2019 R&D operations. The annual Conference on Neural Information Processing Systems (NIPS) will take place in Montr\u00e9al in 2018.\n\nMany industry insiders will point to Professor Geoffrey Hinton and the Canadian Institute for Advanced Research (CIFAR) as responsible for igniting this current round of AI fervor. Hinton returned to Toronto this year to head up the new Vector Institute for Artificial Intelligence. Toronto is one of the three AI hubs pinpointed by the Canadian government\u2019s Pan-Canadian AI Strategy. With the Trump administration tightening up immigration policy south of the border, many AI researchers and engineers are finding Toronto more accommodating, which has also boosted the city\u2019s position in AI.\n\nBangalore, the technology hub of India, is making a name for itself in the field of machine learning, image and voice recognition. India\u2019s Center for Artificial Intelligence & Robotics (CAIR), operating under the country\u2019s Defence Research and Development Organisation (DRDO), has been focusing on AI since 1986. Google and Apple have made acquisitions of local startups. Indian unicorns Flipkart and InMobi, which actively deploy AI in their businesses, are also headquartered in Bangalore.\n\nWe choose Berlin to represent Germany because the capital city holds 54% of all German AI startups, followed by Munich, Hamburg, and Frankfurt. The German Research Center for Artificial Intelligence (DFKI) and Max Planck Institute are top-notch AI research institutes. This year Amazon is expected to open an AI research center in Tuebingen, creating 100 jobs in the next five years. German auto manufacturers such as Audi AG are hopping aboard this transformational technology faster than their American counterparts.\n\nAs China\u2019s economic and political center, Beijing has 43% of all AI startups in the country. Leading research institutions such as Tsinghua, Beihang, and Peking Universities are cultivating talents. Led by Stanford University professor Fei-fei Li, Google recently opened a Beijing AI center as its latest expansion effort in China. The country\u2019s search giant Baidu is based in Beijing, and actively testing autonomous vehicles on intracity highways. China\u2019s National Engineering Laboratory of Deep Learning Technology opened at Baidu\u2019s company campus this year.\n\nShenzhen is home to approximately 20% of Chinese AI companies. The city has long been a technology hub, with strong expertise in manufacturing and hardware. In recent years Shenzhen has fostered internet giant Tencent and international mobile-provider Huawei. Startups such as Megvii, Yitu, and SenseTime all have offices here. As a local hub, Shenzhen has successfully clustered talents from all across Southern China.\n\nTel Aviv\u2019s Habima Square is home to local AI startups such as Twiggle. In recent years Microsoft, Apple, and Uber acquired Israeli startups Equivio, RealFace, and Otto respectively. Intel was the first to open an R&D center in Israel and recently opened AI centers in nearby Ra\u2019anana and Haifa. Google employs more than 600 engineers in the country, with more than half of them from Tel Aviv University. More than 300 multinationals have R&D facilities in the country."
    },
    {
        "url": "https://medium.com/syncedreview/neural-system-combination-for-machine-translation-cd59a7701f79",
        "title": "Neural System Combination for Machine Translation \u2013 SyncedReview \u2013",
        "text": "In recent years, neural machine translation (NMT) is becoming more and more popular, due to its more fluent translation results. However, statistical machine translation (SMT) is better in translation adequacy. To combine the advantages of these two methods, the authors of this paper first adapts the multi-source NMT model, by employing different encoders to capture the semantics of the source language, then the decoder is used to generate the final output by the multiple context vector representations coming from the encoder. Additionally, the author further designs a smart strategy to simulate the real training data for neural system combination.\n\nThe author uses a two-step solution towards neural system combination:\n\nThe following figure shows their proposed model for neural system combination, where the input can be regarded as the source sentence and the results of MT systems.\n\nTo make it easier for understanding, we discuss the paper in a reverse order. We discuss from the final word classification layer to the combined encoder at the beginning.\n\n \n\nAs the figure shown above, the combination model proposed in this paper is an adaptation from multi-source NMT model, where different encoders are based on the best translation by different NMT and SMT systems. The combination of SMT and NMT can provide both the fluency and the adequacy in translation as described in the paper.\n\nhere alpha_{ji}^k means the weight of i-th encoder node of the k-th system in the j-th decoder step, where:\n\ne_{ji} describes how well s \u0303_{j-1} and h_i match.\n\n \n\nh_i is the concatenation of [h_i_forward, h_i_backward] from a bi-directional GRU, and s \u0303_{j-1} = GRU (s_{j-1}, y_{j-1}) is an intermediate state calculated by a GRU function, given previous hidden state s_{j-1} and previous output y_{j-1}. \n\n \n\nAfter knowing the h_i, the k-th system context vector c_{jk} at decoder step j can be calculated by:\n\nwhich is a weighted sum of the source encoder state h_i, given the weighting factor alpha_{ji}^{k}.\n\n \n\nAfter knowing the system context vector context vector c_{jk}, it is easy to calculate the normalized item beta_{jk} for each of them in order to combine different system context vector:\n\nwhere s \u0303_{j-1} = GRU (s_{j-1}, y_{j-1}) is an intermediate state calculated by a GRU function and c_{jk} is the k-th system context vector.\n\n \n\nAfter knowing the normalized vector beta_{jk} for k-th system context vector, the final system combination vector can be calculated by:\n\nwhere c_j is the weighted sum of context vectors of k systems, as illustrated in the above figure.\n\n \n\nAfter knowing the context vector, the probability of the next word y_j, given previously generated target sequences Y_{<j} and the results sequence Z = (Z^n, z^p, Z^h, \u2026 Z^k) of K systems for the same source sentence can be calculated as:\n\nthe c_j is known by previous derivation, y_{j-1} is the previous output embedding feature, and the state of j-th decoder step s_j is calculated by:\n\nwhich is a GRU function given an immediate state s \u0303_{j-1} = GRU (s_{j-1}, y_{j-1}) and system combined context vector c_j.\n\n \n\n \n\nThe author in this paper designed a smart way for training data simulation: they randomly divided the training bilingual corpara into two parts, they trained three different MT systems of half of them, and the source sentences of the other half into target translations. This way, the pre-translated sentences can have a better quality for later combination usecase.\n\nAs shown in the above figure, the results on NIST 03\u201306 is quite good. The multi-combination system itself already got an average BELU of 40.63, better than the best one (HPMT (38.10)) of the individual MT system. When adding the source sentence for training (multi-source combination), the result further improves to 41.75 BELU score. Finally, when they ensemble different decoding models, the results sees another improvement to 43.44 BELU score.\n\nAs shown in the above figure, the NMT model itself cannot translate the OOV word zuzhiwang into a correct word, but the PMBT and HPMT can (kongbu zuzhiwang \u2192 terrorist group). After the neural system combination training, the multi-combination system can translate the OOV word into a correct word with proper grammar.\n\nThis paper took me quite a while to read, since it\u2019s not a general translation pipeline. Instead, the central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. They use NMT and PBMT systems to pre-translate the inputs into target translations, and then generate the final target hypothesis using these pre-translations. They show that the results can be further improved by jointly training with source language input sentences, so the model then become a multi-source language models. Although they designed a good way to simulate the training data for system combination and got a very good results, I still think it really takes too much pre-processing and hand-crafted designs before the final output. The decoding speed in practical scenarios is also still unknown. Compared to this pre-translated technique, I think it would be a better way to directly use some PBMT features, like language model and phrase tables during decoding beam searching, to help finding a better n-best hypothesis, which would be a smarter way to combine the advantages of different MT systems."
    },
    {
        "url": "https://medium.com/syncedreview/andrew-ng-announces-landing-ai-bringing-ai-to-manufacturing-fd9ef8e1ebd0",
        "title": "Andrew Ng Announces Landing.ai \u2014 Bringing AI to Manufacturing",
        "text": "One of the top minds in artificial intelligence, Andrew Ng believes \u201cAI is the new electricity.\u201d However few industries can successfully adopt AI \u2014 especially the manufacturing industry, which is notoriously slow in implementing innovation. Today Ng announced Landing.ai, which is designed to accelerate AI\u2019s implementation in manufacturing.\n\nNg says \u201coutside the IT industries, almost no companies can have access to talents with AI expertise as well as the strategy of integrating AI\u201d. This is where Landing.ai can make a difference by helping enterprises transform for the age of AI. \n\n \n\nThe company will help with AI deployment strategies, including data acquisition, organizational structure design, and prioritization of AI projects. At the same time, the company will develop a wide range of AI transformation programs, such as introducing new technologies, reshaping organizational structures, and training employees. \n\n \n\nLanding.ai targets all industries but is initially focused on manufacturing. Ng believes AI technologies can expand their foothold in manufacturing, and overcome hurdles such as variable quality and yield, inflexible production line design, inability to manage capacity, and rising production costs.\n\nFor example in quality control, a learning algorithm can detect anomalies more accurately than humans. While visual inspection is a tiring task for humans, a self-taught machine can give consistent results, work 24/7, and require only fractions of a second to inspect a part.\n\nThe transformation of jobs is a pressing topic in the age of AI, as self-learning machines will automate much manufacturing work and reshape the manufacturing job market in the US and globally. As a result, manufacturers will be looking for workers who understand AI technologies such as deep learning models, and know how to use the AI-powered machines. \n\n \n\nNg believes Landing.ai is the best candidate to tackle the challenge of retraining workers. The company will develop retraining solutions for current or displaced workers, and deploy skill-training programs to a variety of partners, including local governments. \n\n \n\nLanding.ai has anchored a partnership with Foxconn. The Taipei-based electronic contract manufacturer has been collaborating with Ng\u2019s team since July, and will jointly develop and deploy AI solutions and training globally.\n\nSince he left his Chief Scientist position at Baidu this March, Ng has devoted himself to expanding AI\u2019s influence outside the IT industry. In August he launched Deeplearning.ai to help talents build their careers in AI. Deeplearning.ai\u2019s deep learning courses on Coursera have been very well received by newcomers to the field. \n\n \n\n Landing.ai signals the next step in the Ng\u2019s ambitious goal of building an AI society. \u201cFor the whole world to experience benefits of AI, it must pervade many industries, not just the IT industry.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/synced-recommends-tencents-2017-internet-whitepaper-1d25e9fffa2b",
        "title": "Editor\u2019s Pick: Tencent\u2019s 2017 Internet Whitepaper \u2013 SyncedReview \u2013",
        "text": "Last month, Chinese internet giant Tencent released its Internet Technology Innovation Whitepaper, which opened with a letter to company partners from CEO Huateng Ma stating, \u201cIn the past year our society underwent a comprehensive digitalization process. [\u2026] Companies, public service departments, education departments, scientific research institutions, non-profit organization, and cultural enterprises from all walks of life are eager to hop on the express train of digital transformation.\u201d\n\nIn Ma\u2019s view digitalization is removing barriers between industries and regions. Public and private sectors can now partner to form a new kind of \u2018digital ecology\u2019. As the cloud, big data, and AI create a new infrastructure that powers new types of social governance, seven trends have emerged:\n\n\u27a4 In the past 40 years, society has witnessed the development of IT, the internet and mobile internet, and AI. Tencent believes we are experiencing unprecedented simultaneous cross-sector technological development.\n\n\u27a4 Government policy encourages AI research, commercialization, and implementation. Since 2012, the Chinese government has released over 50 white papers detailing AI development strategies. In Notification of a New Generation of Artificial Intelligence Development Plan, China made clear its goal to achieve international leadership in AI technology by 2030.\n\n\u27a4 Domestic AI development budded when China\u2019s first robotics company Weilai (Future) Robotics was founded in Shanghai in 1996. Five years later the first iBot robot startup obtained US$20.4 million in funding from IDG capital, marking the first AI investment milestone.\n\n\u27a4 By mid-2017, China had 592 AI companies, which is 23% of the global total, making it the second largest AI industry cluster in the world. Domestic VC invested a total of US$63.5 billion into 767 startups, which is 33% of total global investment.\n\n\u27a4 Domestic AI companies are primarily located in Beijing (42%), Guangdong (20%), Shanghai (14%), Zhejiang (14%), and Jiangsu (5%). Major AI application areas are computer vision (146 companies), intelligent robots (125 companies), and natural language processing (92 companies).\n\n\u27a4 China\u2019s 592 AI companies employ over 40,000 employees, but the country still lacks the talents required for building a robust hardware infrastructure.\n\n\u27a4 According to Wuzhen index: Global Artificial Intelligence Development Report, China currently holds 15,745 AI patents, while the US holds 26,891 patents and Japan 14,604. These three countries hold 74% of all patents in the field of artificial intelligence."
    },
    {
        "url": "https://medium.com/syncedreview/google-opens-china-ai-center-8af187b66970",
        "title": "Google Opens China AI Center \u2013 SyncedReview \u2013",
        "text": "Google is opening an artificial intelligence research lab in Beijing, Chief Scientist of AI/ML at Google Cloud Fei-fei Li announced today at the Google Developer Conference in Shanghai.\n\nGoogle\u2019s China AI Center will gather top researchers and scientists with AI expertise, co-led by Dr. Li and Jia Li, Head of Research and Development at Google Cloud.\n\nDr. Li says Google\u2019s China AI Center will focus on basic AI research, while working with local Chinese academia to develop sustainable cooperation. \u201cGoogle cherishes this opportunity to cooperate with the top Chinese AI talents in China, who are bound to be the top in the world.\u201d\n\nRecent Google job postings suggested the tech giant was filling machine learning-related positions for a new Beijing team, including researchers, technology directors, product managers, and software engineers.\n\nDr. Li says she suggested the idea of opening a Google AI center in China after joining the company this January, and it was approved by Google CEO Sundar Pichai and Google Cloud CEO Diane Greene.\n\nThe opening of Google\u2019s AI center in China signals a first step in the company\u2019s return to China. Access to Google Search has been limited in China since 2010, and Google has been seeking the opportunity to re-build its presence in China.\n\nGoogle\u2019s China prospects brightened this May when DeepMind brought the Go computer AlphaGo Master to Wuzhen and topped World \u21161 player Ke Jie (\u67ef\u6d01). In September Google took a leap forward by launching a China-based TensorFlow development team to boost a local machine learning community.\n\n\u201cAI has no boundaries, and there is no boundary for AI\u2019s well-being,\u201d says Dr. Li."
    },
    {
        "url": "https://medium.com/syncedreview/lecun-vs-rahimi-has-machine-learning-become-alchemy-21cb1557920d",
        "title": "LeCun vs Rahimi: Has Machine Learning Become Alchemy?",
        "text": "The medieval art of alchemy was once believed capable of creating gold and even human immortality. The trial-and-error method was however gradually abandoned after pioneers like Issac Newton introduced the science of physics and chemistry in the 1700s. But now, some machine learning researchers are wondering aloud whether today\u2019s artificial intelligence research has become a new sort of alchemy.\n\nThe debate started with Google\u2019s Ali Rahimi, winner of the the Test-of-Time award at the recent Conference on Neural Information Processing (NIPS). Rahimi put it bluntly in his NIPS presentation: \u201cMachine learning has become alchemy.\u201d\n\nAccording to Rahimi, machine learning research and alchemy both work to a certain degree. Alchemists discovered metallurgy, glass-making, and various medications; while machine learning researchers have managed to make machines that can beat human Go players, identify objects from pictures, and recognize human voices.\n\nHowever, alchemists believed they could cure diseases or transmute basic metals into golds, which was impossible. The Scientific Revolution had to dismantle 2000 years worth of alchemical theories.\n\nRahimi believes contemporary machine learning models\u2019 successes \u2014 which are mostly based on empirical methods \u2014 are plagued with the same issues as alchemy. The inner mechanisms of machine learning models are so complex and opaque that researchers often don\u2019t understand why a machine learning model can output a particular response from a set of data inputs, aka the black box problem. Ranimi believes the lack of theoretical understanding or technical interpretability of machine learning models is cause for concern, especially if AI takes responsibility for critical decision-making.\n\n\u201cWe are building systems that govern healthcare and mediate our civic dialogue. We would influence elections. I would like to live in a society whose systems are built on top of verifiable, rigorous, thorough knowledge, and not on alchemy,\u201d said Rahimi.\n\nThat triggered Facebook Director of AI Research Yann LeCun, who responded to Rahimi\u2019s talk the next day, saying the alchemy analogy was \u201cinsulting\u201d and \u201cwrong\u201d. \u201cCriticizing an entire community (and an incredibly successful one at that) for practicing \u2018alchemy\u2019, simply because our current theoretical tools haven\u2019t caught up with our practice is dangerous.\u201d\n\nLeCun said skeptical attitudes like Rahimi\u2019s were the main reason the machine learning community ignored the effectiveness of artificial neural networks in the 1980s. As the main contributor to the current development of convolutional neural networks, LeCun is concerned the scenario could repeat.\n\n\u201cThe engineering artifacts have almost always preceded the theoretical understanding,\u201d said LeCun. \u201cUnderstanding (theoretical or otherwise) is a good thing. It\u2019s the very purpose of many of us in the NIPS community. But another important goal is inventing new methods, new techniques, and yes, new tricks.\u201d\n\nSix hours after LeCun posted his comment on Facebook, Rahimi replied with a softened tone, saying he appreciated LeCun\u2019s thoughtful reaction: \u201cThe \u2018rigor\u2019 I\u2019m asking for are the pedagogical nuggets: simple experiments, simple theorems.\u201d\n\nLeCun agreed with Rahimi\u2019s views on pedagogy, saying \u201cSimple and general theorems are good\u2026 but it could very well be that we won\u2019t have \u2018simple\u2019 theorems that are more specific to neural networks, for the same reasons we don\u2019t have analytical solutions of Navier-Stokes or the 3-body problem.\u201d\n\nThe Rahimi \u2014 LeCun debate grew into a wide-ranging discussion at NIPS and on the internet. Dr. Yiran Chen, Director of the Duke Center of Evolutionary Lab, attempted to make peace, suggesting LeCun had overreacted, and that the opposing positions were actually not so contradictory.\n\n\u201cWhat Rahimi means is that we now lack a clear theoretical explanation of deep learning models and this part needs to be strengthened. LeCun means that lacking a clear explanation does not affect the ability of deep learning to solve problems. Theoretical development can lag behind,\u201d said Dr. Chen.\n\nOther academicians, like Facebook\u2019s Research Scientist and Manager Yuandong Tian, characterized the exchange as a common contradiction between the First Principle and Empiricism: \u201cSuch debates are always happening in academia.\u201d\n\nSparked by the Rahimi \u2014 LeCun\u2019s debate, discussion regarding the understanding or lack of understanding of machine learning models will likely continue for some time. Do we want more effective machine learning models without clear theoretical explanations, or simpler, transparent models that are less effective in solving specific tasks? As AI gradually penetrates critical fields such as law and healthcare, the debate is bound to reignite."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-dec-pt-1-fb4592b4b33f",
        "title": "AI Biweekly: 10 Bits from Dec (Pt 1) \u2013 SyncedReview \u2013",
        "text": "November 26th \u2014 NVIDIA Strides Into Healthcare Industry by Collaborating with GE Healthcare\n\nAt the Radiological Society of North America (RSNA), NVIDIA announces a collaboration with GE Healthcare and Nuance to enhance medical imaging with AI. Through this collaboration, NVIDIA and GE Healthcare will deepen their 10-year partnership and deploy AI technology in 500,000 globally distributed GE medical devices in order to accelerate healthcare data processing.\n\nIntel introduces the Responsibility-Sensitive Safety model (RSS) as its autonomous vehicle driving strategy. To formalize human judgement under comprehensive road situations, RSS will mathematically express driver errors and process the most appropriate move without human intervention. Intel says RSS might not strictly drive obey the law: it will sometimes break the rules \u2014 depending on the actual situation \u2014 in order to reduce the rate of accidents.\n\nAmazon announces a new translation service as part of its Amazon Web Services (AWS), which is competing with Google and Microsoft for dominance in speech recognition and natural language processing. AWS Translate will provide text translations for supported languages. Amazon is pitching the new translation service as a way for businesses to expand products and services. The technology is based on language pairing models represented in neural networks. Amazon hopes to integrate the translation tool with other services.\n\nNovember 30th \u2014 Google Home Gets a Little Smarter With New Multitasking Feature\n\nGoogle\u2019s Smart Speaker recently added a multitasking feature that makes it possible for the device to accomplish two different tasks at the same time (only two though). To initiate this feature, user have to verbally separate tasks into individual commands, as the device won\u2019t understand two tasks crammed into the same sentence. The new feature comes as the company readies Home Max, a premium addition to the Home line due next month.\n\nDecember 4th \u2014 Shenzhen Bus Group Road Tests Self-Driving Alphabus \n\n \n\nOn December 2nd, four unmanned buses running on the \u201cAlphabus Smart Drive Bus System\u201d performed a series of road tests in Futian, Shenzhen. This road test attracted a great amount of attention, with some media coverage negatively emphasizing the job loss potential. Shenzhen Bus Group (SBG) characterizes Alphabus as a support system for drivers, with limited real-world application at present. To deploy Alphabus in the future on a specific route, government approval will be required.\n\nDecember 5th \u2014 SalesForce Claims AI Is Too Advanced for Business\n\n \n\nAccording to Salesforce\u2019s recent \u201cSmall & Medium Business Trends Report\u201d, many small and medium businesses are still manually inputting data: 56% of businesses are not ready for AI technology; 40% find AI too complex for their needs; and 37% don\u2019t have enough knowledge about how AI can help the businesses. As for the evaluation of new technology, 95% of business owners considered \u201cease of use\u201d an important factor; 94% chose \u201cprice\u201d; and 91% chose \u201ctrustworthy vendors\u201d.\n\n \n\nDecember 6th \u2014 Ikea to Collaborate with Sonos on Smart Home Products\n\n \n\nIkea and Sonos want to make it easier to play music anywhere in the home. Together with Sonos, Ikea hopes to democratize music and sound in the home, creating products designed for listening to music. The collaboration is somewhat surprising, as Sonos is a premium brand and Ikea is a budget home furnishing company. But in order to add IoT capability, this is the a logical third step for Ikea\u2019s Home Smart Division \u2014 which launched wireless charging in some of their furniture in 2015, and smart lighting products in 2016.\n\nDecember 6th \u2014 Amazon Echo and Fire TV Will Lose YouTube Access in 2018\n\nAmazon Echo and Fire TV users will no longer be able to access YouTube from those devices starting January 1, 2018. The move from Google comes amid growing tensions with Amazon, and is being framed as rebuttal for not allowing Google Home and Nest devices for sale on the e-commerce giant\u2019s site. This is not a sudden move. Earlier this fall, Google threatened to remove YouTube amid heated negotiations with Amazon, and this seems to have escalated into a fight between the two tech giants.\n\n \n\nDecember 7th \u2014 NVIDIA Unveils its Latest GPU Product at NIPS\n\nAt NIPS, NVIDIA unveils the Titan V GPU, which can be used on desktop machines and targets machine learning applications. The new GPU will feature NVIDIA Volta architecture, which is widely used by researchers, developers, and data scientists for accelerating machine learning applications. Hosting the GPU on a local machine will be convenient for researchers who want to run workloads anytime without having to use a cloud service.\n\nQualcomm and Baidu announce their strategic cooperation on optimizing DuerOS on mobile devices at the second Qualcomm Snapdragon Technology Summit. This will help bring speech-based AI solutions to the next generation of Qualcomm Snapdragon Mobile Platforms, enabling users to utilize their voice (through the DuerOS voice service) at any time to wake their smartphones and IoT devices with minimal power consumption."
    },
    {
        "url": "https://medium.com/syncedreview/guided-alignment-training-for-topic-aware-neural-machine-translation-1531bbd4d404",
        "title": "Guided Alignment Training for Topic-Aware Neural Machine Translation",
        "text": "NMT (Neural Machine Translation) were shown to achieve the state-of-art in many translation tasks, such as WMT news translation, IWSLT spoken language translation, E-commerce corpora translation etc. But the problem is that the attention mechanism in the NMT might be wrongly guided towards a bad alignment result. To solve this problem, the author wants to use a hard-attention (Viterbi alignment matrix) of the SMT (Statistical Machine Translation) to help learn a better alignment matrix in NMT during the training. Additionally, the author combines the topic specified categorical information to further bootstrap the overall translation accuracy. In summary, this paper makes two contributions:\n\nWe will first review the proposed loss function, which is a L2 distance between each term of Viterbi alignment matrix (hard-alignment matrix) and NMT alignment matrix (soft-alignment matrix). Then, we will review the topic-aware decoder for accuracy bootstrapping. Finally, I will give my own thoughts about the paper.\n\nBecause the attention weights only rely on the previous generated word and decoder state, the model cannot capture any additional information if the previous word is an placeholder or out-of-domain word/character (For E-commerce data it is a very common case), which might lead to a misalignment. In order to solve this problem, the author proposed that the Viterbi alignments of IBM model 4 can be used as a hard attention to bias the learned soft attention.\n\nThe goal is to optimize the decoder cost and alignment cost (divergence between soft alignment and hard alignment generated by statistical alignments). At first, the decoder cost can be written as:\n\ny_n, x_n is the n-th training pair, and N is the number of samples. The above formula is the definition of negative log-likelihood, the conditional probability can be further written as:\n\nThis formula shows: the probability of the target sentence, given the source sentence, is equal to the cumulative product of the probability of current word conditioned on the previous generated words and an context vector. T here is the length of the target sentence, and T\u2019 is the length of source sentence, the context vector c is used to capture the coverage information of source sentence.\n\n \n\nThe probability of current word conditioned on the previous sentence and context vector can be regarded as a function based on the current decoder hidden state s_t, previous generated word y_t and an context vector c. Generally g(.) is a non-linear function. \n\n \n\nIf the context vector c is variable based on different step t, then c_t is used instead. c_t can be regarded as a weighted sum over the source sentence:\n\nwhere T\u2019 is the length of source sentence, h_i is i-th encoder hidden state, a_ti is a weight to capture the relative importance of t-th target word and i-th source word at step t. \n\n \n\n a_ti can be computed as following:\n\nwhere each a_ti is a softmax of e_ti, e_ti = a(s_{t-1}, h_i), a(. , .) is a function to calculate the relative score, s_{t-1} is the decoder hidden state at step t-1, h_i is the i-th source hidden state (it can also be referred to a i-th bi-directional source hidden state). In this paper the author chooses dot product to compute the relative score:\n\nOverall, for T words on the target side, each of them needs to compute a context vector with fixed length T\u2019, the author referred the resulting alignment matrix of shape TxT\u2019 to matrix alpha.\n\n \n\n \n\n The pretrained statistical alignments matrix A is also a matrix with shape TxT\u2019, where A_ti refers to the probability of the t_th word on the target side being aligned to i_th word on the source side. The matrix is normalized along the column in order to make it consistent with the neural alignment matrix.\n\n \n\n After the above steps, the author defines the alignment loss to be:\n\nBasically the Loss can be defined in the form of either cross-entropy or mean squared error. After the combination of alignment loss and decoder cost, the overall formula can be written as:\n\nwhere w1 and w2 are just coefficient to balance these two terms.\n\nThe above table is the result compared to baseline NMT and baseline NMT + alignment loss, we can know that no matter using alignment loss in the form of mean-squared error or cross-entropy, the translation result always get better.\n\nThe above figure show that how alignment loss helps the overall attention to become better, as we can see, e.g. in the upper figure, the French word \u201cdolcevita\u201d was aligned to the English word \u201cfederico\u201d before (which is a wrong alignment), it is now correctly aligned to \u201cdolcevita\u201d.\n\nIn the e-commerce domain, the product category may reflect useful information to help product title translation and product description translation. How to help decoding based on the topic vector? The idea is to represent the topic information in a D-dimensional vector l, where D is the number of topics. The conditional probability during the decoding can be written as:\n\nwhere g(.) is a approximate function which can be modeled by a feed-forward network:\n\nAs shown in the above figure, which is a feed forward network to model the decoder. l is the topic vector of dimension D, f_{t-1} is the previous word embedding, c_t is the current context vector and s_{t-1} is the last decoder hidden state, where r_t can be written as:\n\nwhere W_r\u2019 is the original transformation matrix and Wc is the topic transformation matrix, Ec can be regarded as a learned topic vector and can use the meta information to help decoding.\n\nAs the figure shown above, the learned topic vector shows that it helps decoding from 18.6 BELU score to 19.7, it shows that decoding conditioned on an additional topic vector affects alignment, word selection and decoding search, etc.\n\nOn the one hand, This paper uses IBM model 4 Viterbi alignments for guided alignment, it\u2019s also possible to design kinds of hierarchical attention supervision to improve the guided alignment. Moreover, as mentioned in the conclusion of this paper, not only topic meta information could influence the overall translation performance, the monolingual data could also help. Recently, people are always using monolingual data for back-translation to improve the accuracy, it is also possible to investigate decoding conditioned on abundant monolingual data and its seep-up solutions."
    },
    {
        "url": "https://medium.com/syncedreview/nips-2017-day-1-2-highlights-67ab464086c",
        "title": "NIPS: 2017 Day 1 & 2 Highlights \u2013 SyncedReview \u2013",
        "text": "The Neural Information Processing Systems Conference (NIPS) has come a long way since 100 invitation-only researchers gathered in Denver 31 years ago. NIPS has become the world\u2019s leading conference on machine learning and computational neuroscience, with 8,000 attendees flocking to Long Beach, California this week for NIPS 2017, the most popular yet.\n\n \n\nIn the opening keynote speech Program Chair Samy Bengio thanked his team for making the conference happen, announced the best paper of this year award, and presented the official stats for NIPS 2017.\n\n8,000 Registered Attendees\n\n 7 Invited Speakers\n\n 2 Parallel Tracks (mixed orals and spotlights)\n\n 3 Poster Sessions\n\n \n\n3,240 submissions were accepted for review: a record 30% increase and almost twice the total submissions to this year\u2019s ICML (International Conference on Machine Learning). \n\n \n\n156 Subareas: a 150% increase from last year. As lower-level subject area has increased, the area structure has been re-structured to 156 lower-level areas and 9 top-level subjects.\n\nTo ensure fairness in the paper review process, the following constraints were in effect for Reviewers:\n\nThere were 9,747 reviews with at least 3 reviews per submission. Although the acceptance rate for papers dipped to 21%, the high submission volume saw 679 papers accepted for presentation, a 90% increase over last year.\n\nEach year Program Chairs refresh the NIPS agenda. This year five new official competitions made it to the conference out of 23 proposals: 1) Conversational Intelligence (chatbots); 2) Human-Computer QA : similar to the game show Jeopardy!; 3) Learning to Run: The goal is to train human avatars to learn using pure reinforcement learning; 4) Personalized Medicine; and 5) Adversarial Attacks/Defense.\n\nAfter Opening Remarks, the first invited talk was Powering the Next 100 Years delivered by Google engineer John Platt.\n\n \n\n NIPS Tutorials are intended to present new or mature approaches that broaden one\u2019s research interests. There were three parallel Tutorials on Day 1. Synced paid particular interest to the Deep Learning sessions and we share some takeaways below.\n\nDeep Learning: Practice and Trends \u2014 as the name suggests, this tutorial discussed state-of-art deep neural networks and applications enhanced by the technology. The talk was given by Professor Nando de Freitas from Oxford University, who pointed out that deep learning consists of three main building blocks: I/O Modalities, architectures, and losses.\n\n \n\n Prof. De Freitas said tailoring building blocks to different problems is a key to success. Even with good architecture, wrongly designed objectives can result in failure.\n\n \n\n With regard to trends, Prof. De Freitas introduced a variety of successful applications: Autoregressive Model, Domain Alignment, Meta-Learning, Graph Network and Program Induction. \n\n \n\n In the Deep Probabilistic Modelling with Gaussian Processes session, Professor Neil Lawrence from the University of Sheffield discussed modeling neural networks from a probabilistic perspective, with the assumption that models can be represented by Gaussian processes.\n\n \n\n Geometric Deep Learning on Graphs and Manifolds was the most mathematically challenging session of the day, introducing measures of non-Euclidean space, referring to graph and manifolds in applications, and attempting to bridge this notion to convolutional neural networks. \n\n \n\n Powering the Next 100 Years was an invited talk presented by Google Principal Scientist John Platt, who opined that a worldwide shortage of electrical power will develop by 2100. In Platt\u2019s view the most effective way to answer that will be generating clean energy by simulating the process in the sun\u2019s inner core: fusion. But such research is difficult and even dangerous. How can machine learning contribute? Platt\u2019s team is working closely with physicians to explore the safe zones of experiment parameters.\n\n \n\n Among the most popular visuals in the Poster Session were:\n\nThe star of Day 2 was a session introducing the paper Dynamic Routing Between Capsules. Even though it was only a five-minute spotlight, the hall was already full well before the session began. The papers Safe and Nested Subgame Solving for Imperfect-Information Games and A Linear-Time Kernel Goodness-of-Fit Test also garnered much attention.\n\n \n\n Speaking at the Test-of-Time Award presentation, Ali Rahimi recalled when he and colleague Ben Recht worked together in 2007, when what he calls the \u2018NIPS rigor police\u2019 were reviewing papers. He argued that \u201cmachine learning has become alchemy \u2014 alchemy worked, it helped invented many things.\u201d\n\n\u201cIf you are building photo sharing systems, alchemy is OK,\u201d said Rahami. \u201cBut we are beyond that now. We are building systems that govern healthcare and mediate our civic dialogue, we influence elections. I would like to live in a society where systems are built on top of verifiable, rigorous thorough knowledge and not alchemy. As aggravating as the NIPS rigor police were, I miss them and I wish for them to come back.\u201d\n\nBest Paper Award presentations were held in the Theory and Algorithm sessions. Speakers began by outlining fundamental problems in their fields, then explained how their work differs from previous approaches. From their illustrations, it was easy to conclude that the greatest work comes from well-defined research goals and clear boundaries.\n\n \n\n Google\u2019s Nicholas Frosst, co-author of the paper Dynamic Routing Between Capsules illustrated the basic idea behind capsules, then presented surprisingly good results in his close remarks. A number of details were not covered due to time limits, which lead to a crazy, packed poster session: Frosst continued speaking and answering questions for at least three hours.\n\n \n\n NIPS 2017 continues until December 9 at the Long Beach Convention and Entertainment Center. The conference is completely sold out."
    },
    {
        "url": "https://medium.com/syncedreview/stn-ocr-a-single-neural-network-for-text-detection-and-text-recognition-220debe6ded4",
        "title": "STN-OCR: A single Neural Network for Text Detection and Text Recognition",
        "text": "STN-OCR, a single semi-supervised Deep Neural Network(DNN), consist of a spatial transformer network \u2014 which is used to detected text regions in images, and a text recognition network \u2014 which recognizes the textual content of the identified text regions. STN-OCR is an end-to-end scene text recognition system, but it is not easy to train. This model is mostly able to detect text in differently arranged lines of text in images, while also recognizing the content of these words. The overview of the system is shown in Figure 1.\n\nCompared with most of the current text recognition systems, which extract all the information from the image at once, STN-OCR behaves more like a human. First, STN-OCR localizes text regions, and then recognizes the textual content of each text region. To do this, STN-OCR consists of two stages: text detection and text recognition. The complete stages are shown in Figure 2.\n\nThe detection stage consists of three parts. (1)A function f_loc computed by a localization network. It is used to predict the parameters \u03b8. (2) A sampling grid created by their predicted parameters. Sampling grid is used to define which part of the input features should be mapped to the output feature map. (3) Take the generated sampling grid and produces the spatially transformed output feature map O by a differentiable interpolation method.\n\nLocalization Network\n\nThe Localization network takes the feature map as input, and outputs the parameters \u03b8 of the transformation. In this system, it is used to take the image as input and predict N two-dimensional affine transformation matrices, and N can be a number of characters, words or text lines.\n\nTo be specific, the N transformation matrices A_\u03b8^n are produced by using a feed-forward CNN together with an RNN. g_loc is also a feed-forward network. Each of the N transformation matrices is computed using the hidden state h_n for each time-step of the RNN.\n\nThe authors use ResNet as the CNN network, arguing that the residual connections of the ResNet help with retaining a strong gradient down to the very first convolutional layers, which makes the system faster and better performing compared with other structures like VGGNet. Bidirectional Long-Short Term Memory (BLSTM) unit is used as an RNN network. Furthermore, Batch Normalization is used in all experiments.\n\nGrid Generator\n\nThe gird generator produces N regular grids G_i^n of coordinates u_i^n, v_i^n of the input feature map by using a regularly spaced grid G_0 with coordinates y_h_o, x_w_o and the affine transformation matrices A_n^\u03b8. By doing this, N resulting grids G_i^n with bounding boxes of the text regions can be extracted from the input feature map.\n\nImage Sampling\n\nThe N sampling grids G_i^n are used to sample values of the feature map at their corresponding coordinates u_i^n, v_i^n. However, these points will not always perfectly align with the discrete grid of values in the input feature map. Thus, the authors extract the value at a given coordinate by bi-linearly interpolating the values of the nearest neighbors. The authors get the values of the N output feature maps O^n at the location i, j by the following formulation. It is possible to propagate error gradients to the localization network by using standard backpropagation, for this formulation is (sub-)differentiable.\n\nIn the text recognition stage, these N different regions which produced by the detection stage are processed independently of each other. The authors also use ResNet architecture for the recognition stage, and they argue that using a ResNet in the recognition stage is even more important than in the detection stage, for the reason that the detection stage needs to receive strong gradients from the recognition stage in order to successfully update the weights of the localization network. A probability distribution y\u02c6 over the label space L_epsilon are predicted in this stage, where L\u01eb = L \u222a \u01eb,\n\nwith L = 0\u20139a \u2014 z and \u01eb representing the blank label. Each softmax classifier is used to predict one character of the given word.\n\nThe authors also suggest that using Connectionist Temporal Classification (CTC) to train the network and retrieve the most probable labeling by setting y\u02c6 to be the most probable labeling path \u03c0.\n\nL_epsilon^T is the set of all labels that have the length T and p(\u03c0|xn) being the probability that path \u03c0 \u2208 L_epsilon^T is predicted by the DNN. B is a function that removes all predicted blank labels and all repeated labels.\n\nThe authors have evaluated this network architecture on several scene text detection and recognition datasets, such as the SVHN dataset, Robust reading dataset, and the French Street Name Signs (FSNS) dataset. Table 1. shows recognition accuracy on the SVHN dataset. Robust reading dataset is used to explore the performance in detecting and recognizing single characters of the model. FSNS dataset is the most challenge dataset that is used in experiments. For the reason that it contains various lines of text with varying length embedded in natural scenes, and many images which do not include the full name of the streets. Figure 3. shows that this system is able to detect a range of differently arranged text lines and also recognize the content of these words.\n\nThe authors proposed an end-to-end scene text recognition system \u2014 a single multi-task deep neural network. It contains a detection stage and a recognition stage, and the text detection stage was trained in a semi-supervised way. This system is able to detect a range of arranged text lines and recognize the content of these words. However, it is not fully capable of detecting text in arbitrary locations in images.\n\nFrom my perspective, one of the highlights of this paper is that training the single neural network in a semi-supervised method. Because of the training data issue, unsupervised and semi-supervised deep mode learning will become more and more important in the future. In this model, the authors only use images and the labels for text contained in these images as input, and the overall loss function is only based on word recognition accuracy. The localization of the text is learned by the network itself. This means that this model can learn to decide where to look. This can also be considered as a kind of visual attention mechanism. But beyond that, it could be able to solve multiple problems such as translation, rotation, and skew by using affine transform. And I hope this semi-supervised solution for training a localization network could open up a new direction for future research not only for text, but also in the domain of object detection and recognition."
    },
    {
        "url": "https://medium.com/syncedreview/from-zero-to-master-in-hours-alphazero-accelerates-reinforcement-learning-7f74499d0773",
        "title": "From Zero to Master in Hours: AlphaZero Accelerates Reinforcement Learning",
        "text": "Google\u2019s DeepMind has once again surprised the machine learning community, this time with the introduction of AlphaZero \u2014 a new algorithm that can quickly surpass human board game performance through reinforcement learning self-play.\n\n \n\n It was was just two months that DeepMind published their Nature paper on AlphaGo Zero, which mastered the game of Go in days, starting with no human data other than game rules. AlphaZero also starts tabula rasa, but the new algorithm has taken a leap forward in flexibility \u2014 after mere hours of training, and again with no human game records to reference, it beat leading computer chess program Stockfish 8. AlphaZero required even less time to outsmart the top shogi (Japanese chess) bot Elmo, which like Stockfish had already beat human world champions. To remove any doubt about its power and potential AlphaZero also easily dispatched its Go-playing predecessor AlphaGo Zero.\n\nAlphaGo Zero uses deep convolutional neural networks and was trained solely by reinforcement learning from self-play games. AlphaZero is a more generic version of the program, in line with DeepMind\u2019s oft-stated goal of evolving its Go-bots toward other and perhaps more practical real-world tasks.\n\n \n\n The AlphaGo Zero and AlphaZero algorithms differ in that the former takes a binary win/loss position whereas the latter considers draws or other game outcomes not seen in Go. AlphaZero\u2019s algorithm is tailored to asymmetrical game boards, while its neural network is updated continually without waiting for the previous iteration to finish. \n\n \n\n AlphaZero surpassed Stockfish in four hours (300k steps), Elmo in less than two hours (110k steps), and AlphaGo Lee (which beat Korean Go Master Lee Sedol in 2016) in eight hours (165k steps).\n\n \n\n Game boards are structured problems that AI has long sought to tackle. DeepMind\u2019s foray into board games was both a game-changer and a means to an end, namely artificial general intelligence. The key idea is moving from domain-specific to generalized methodologies. \n\n \n\n Says DeepMind CEO and former chess prodigy Demis Hassabis, \u201cIf similar techniques can be applied to other structured problems, such as protein folding, reducing energy consumption or searching for revolutionary new materials, the resulting breakthroughs have the potential to positively impact society.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/neuromorphic-computing-yiran-chens-brainy-architecture-of-the-future-c52d6ec2171b",
        "title": "Neuromorphic Computing: Yiran Chen\u2019s Brainy Architecture of the Future",
        "text": "Neuromorphic computing is built on a brain-inspired silicon chip. It is widely regarded as a computing platform of the future \u2014 uniquely equipped to keep pace with machine learning\u2019s increasingly demanding algorithms and exponentially growing scale of datasets.\n\nNobody knows neuromorphic computing better than Dr. Yiran Chen, who last week published a definitive paper on the subject in the very-large-scale-integration (VLSI) Journal Integration. Dr. Chen spent an entire year on Neuromorphic Computing\u2019s Yesterday, Today, and Tomorrow, a paper that examines both the history and the future prospects of neuromorphic computing.\n\nThe paper details the relationship between neuromorphic computing and traditional artificial neural networks. It also addresses the dramatic rise of deep learning algorithms with questions such as \u201cWhy did deep learning models attract attention in 2006?\u201d and \u201cHow did deep learning models affect neuromorphic computing?\u201d\n\nAn Associate Professor at Duke University and Director of the Duke Center of Evolutionary Lab, Dr. Chen is mainly engaged in the collaborative design of AI-related hardware (nano-scale VLSI) and software (computing platforms). He is an IEEE Fellow for his contributions to memory technologies, and has published a book and more than 270 journal and conference papers.\n\nIntegration\u2019s editor-in-chief Dr. Sheldon Tan tells Synced this paper will be both a very good reference for neuromorphic computing researchers, and an introduction for students who want to learn more about this important research field. The paper includes over 160 references.\n\n\u201cI am sure Dr. Chen\u2019s paper will have many citations down the road,\u201d says Dr. Tan.\n\nThe topic of neuromorphic computing seems to come up every few years in the scientific community, reflecting the need to constantly improve hardware performance to support advanced algorithm development.\n\nThe scale of artificial neural networks \u2014 especially deep learning models \u2014 is growing annually by an order of magnitude. The more advanced these algorithms and architectures become, the more data they can process. But hardware design struggles to keep up, which raises the question of how to increase a chip\u2019s computing power while keeping it compact. Moore\u2019s law says the computing power of a semiconductor can only double once per 18 months. Other hardware improvements must rely on new architectures and optimization.\n\nNeuromorphic chips, which attempt to model the incredibly large and complex parallel processing power of the human brain, have the potential to overcome this hurdle due to their use of VLSI systems, which contain electronic analog circuits which mimic the nervous system in the human brain. If the individual components can change how they connect with each other in response to stimuli, then the computer chip can actually learn from its experiences just as an actual neural network does.\n\nNeuromorphic computing can be roughly divided into three categories: analog, digital and mixed signal. Analog computing studies the biological properties of neural networks; digital computing focuses on accelerated chip design based on FPGA (field-programmable gate array) and ASIC (application-specific integrated circuit) technologies; and mixed signal combines the high accuracy and reliability of digital computing with the low power consumption of analog computing.\n\nAdvancing neuromorphic computing is challenging: new devices remain unstable for commercial use, there is no design that satisfies both generality and high performance, and the technology is constantly changing at a rapid rate.\n\nDr. Chen\u2019s goal is to create stable models and simulations of emerging architectural designs to speed progress toward workable circuits.\n\nDr. Chen was born in Henan, China, a beautiful inland province famous for Shaolin Temple and the home of Kung Fu. He received his Electronic Engineering Bachelor of Science with honors in 1998 and his Master of Science with honors in 2001 from Tsinghua University, and his PhD in Electrical and Computer Engineering from Purdue University in Indiana in 2005.\n\nAfter graduation, Dr. Chen worked with world-leading software and IP company Synopsys as a senior research and development engineer, where he developed the award-winning statistical static timing analysis EDA tool \u201cPrimeTimeVX\u201d.\n\nIn 2007, Dr. Chen joined top US data storage company Seagate Technology, where he focused on emerging memory systems that rely on magnetism, electron spin, or nanoscale resistance. These magnetism-based memory systems perform better than electrons in storing information, offer better scalability, lower power consumption and faster performance.\n\nDr. Chen\u2019s memory systems research achieved great success, spawning his three most-cited papers and a book co-authored with Dr. Hai Li at Duke University: \u201cNonvolatile Memory Design: Magnetic, Resistive, and Phase Changing\u201d, published in 2011 by CRC Press.\n\nAt Seagate Advanced Technology Group Dr. Chen was a pioneer of \u201cmemristor,\u201d a type of resistive memory similar to a human synapse that permits a neuron (or nerve cell) to pass an electrical or chemical signal to another neuron. This inspired Dr. Chen\u2019s R&D on neuromorphic computing and deep learning accelerator.\n\nIn 2009, the subprime mortgage crisis hit many companies, Seagate among them. Dr. Chen took up a position at the University of Pittsburgh in 2010. It was here that his passion for neuromorphic computing would be ignited.\n\nIn the summer of 2010, Dr. Chen organized a panel in a nanometer-structure workshop that discussed the influence of the memristor on the new architecture. The panel included big names such as Google\u2019s Distinguished Hardware Engineer Norm Jouppi, Dr. Robinson E. Pino from Department of Energy\u2019s Office of Science, and Air Force Research Laboratory Principal Electronics Engineer Qing Wu.. The panel proposed the traditional Von Neumann architecture was not working effectively, and to match growing computational demands a new architecture was needed.\n\nA year later, a chance reunion with one of the nanometer panel attendees brought Dr. Chen\u2019s work into sync with the growing world of deep learning. \u201cI realized that nanometer devices could be useful in deep learning models or as acceleration devices for artificial neural networks,\u201d says Dr. Chen.\n\nDr. Chen came to believe that neuromorphic computing could disrupt the existing designs of semiconductors, nanometer devices, circuits, system architectures, design methods, and the development of algorithms. After his wife joined the University of Pittsburgh in 2012, the couple formed the Evolutionary Intelligence Lab. This year the lab transferred to Duke University and was upgraded to the Duke Center for Evolutionary Intelligence.\n\nTeaching and managing the lab consumes the majority of Dr. Chen\u2019s time. He also has to travel to conferences and meet with research sponsors and collaborators. Over a recent six-month period Chen racked up 112,128 frequent flyer miles \u2014 equivalent to circling the equator 4.5 times.\n\nChen somehow also finds time to write. His WeChat blog is a vehicle for his sense of humour and expertise in areas such as memory systems and deep learning, and garners much attention from the AI community.\n\nComing from a high-school teacher\u2019s family, Dr. Chen is passionate about teaching and education. While at the University of Pittsburgh he founded the \u201cCornell Cup\u201d, a national competition of embedded system design for college students; and created an education YouTube channel.\n\nIn an interview with the IEEE community, Dr. Chen encouraged students to study engineering. \u201cEngineer is a high-paying job with lots of fun and creativity. Nothing is cooler than when you can explain to your children how their video game systems work and watch their eyes fill with admiration,\u201d says Dr. Chen, who loves playing Wii U and Xbox One with his two sons.\n\nDr. Chen has many research interests, from hardware to systems to algorithms, applications and so on. Building a tiny and power-efficient silicon chip with an artificial intelligence comparable to the capability of a human brain is by far his most challenging project, but Dr. Chen believes it has huge potential.\n\nImplementing neuromorphic computing will however involve massive hardware overhauls across the industry. And that takes time and money.\n\n\u201cNeuromorphic computing is ready for the market,\u201d says Dr. Chen, \u201cthe only question is whether the market is ready for neuromorphic computing.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/shanghais-subway-smartens-up-with-ai-1b7a36ca5da7",
        "title": "Shanghai\u2019s Subway Smartens Up With AI \u2013 SyncedReview \u2013",
        "text": "Unless you know precisely how Shanghai\u2019s 364 subway stations align with the metropolis above them, navigating the subterranean tangle of China\u2019s largest city can leave you frustrated and hopelessly lost.\n\nOn December 5th Alibaba, Ant Financial, and Shanghai Shentong Metro Group jointly launched a new voice interaction system for purchasing subway tickets. Riders can now tell a machine their destination \u2014 for example, Zhongshan Park \u2014 and the system will use the AutoNavi (\u9ad8\u5fb7\u5730\u56fe) cloud map to issue a ticket for the nearest station.\n\nAlibaba CEO Jack Ma was among the first to try out the new system, which is expected to be available on all Shanghai\u2019s ticketing machines next year as part of China\u2019s project to build smart cities\u2019 IoT infrastructure.\n\nCurrent voice dialogue systems such as smart speakers and voice assistants require \u201ctrigger words.\u201d The iPhone\u2019s voice assistant, for example, activates when it hears \u201cHey Siri.\u201d Alibaba\u2019s new system uses multi-model interaction. Zhijie Yan, head of the Alibaba voice team, says the goal is to eliminate trigger words altogether. \u201cYou just need to approach the machine and it will interact with you naturally.\u201d\n\nSays Yan, \u201cReal life environments are most likely noisy, and that remains to be the biggest technical challenge.\u201d Voice recognition is difficult in open, noisy environments, which is exactly what Shanghai subway is, especially during rush hours. Alibaba\u2019s new ticketing system uses computer vision to identify speaker\u2019s lip movements and measure the distance between speaker and machine before finalizing its voice input. Visual signals are combined with audio signals captured by a large microphone array, with a supporting software signal processor suppressing noise and interference.\n\nLast summer Yan led a five person team on the subway project, identifying stability and rapid learning ability as further challenges to meet, as public service facilities like the subway need to function smoothly 24/7.\n\nThe Shanghai subway is also introducing Alibaba\u2019s facial recognition and Alipay for digital payment at subway entrances.\n\nThis is just the first step: airports, train stations, event spaces, restaurants and shopping malls will soon be able to use multi-model technology to open new human-machine interaction possibilities in information inquiry, interactive advertising, and direction navigation applications."
    },
    {
        "url": "https://medium.com/syncedreview/what-you-need-to-know-about-nips-2017-f74061186a51",
        "title": "What You Need to Know About NIPS 2017 \u2013 SyncedReview \u2013",
        "text": "The Conference on Neural Information Processing (NIPS) kicks off today in Long Beach, California and runs to December 9.\n\nThe NIPS 2017 speaker list is a who\u2019s who of AI: Yann LeCun from Facebook, Demis Hassabis and David Silver from DeepMind, Gary Marcus from New York University, Juergan Shimidhubor from Switzerland-based Dalle Molle Institute for Artificial Intelligence Research, etc.\n\nNIPS 2017 is entirely sold out and did so with unprecedented speed. Facebook AI researcher Alex LeBurn posted a graph on Twitter illustrating the distribution trend of NIPS conference registrations from 2002 to 2017.\n\nFirst held in 1987, NIPS was initially an interdisciplinary meeting for researchers exploring biological and artificial neural networks. Since the biological and artificial systems research streams diverged in the 1990s, NIPS has focused on machine learning, AI and statistics.\n\nThis is the first time since 2013 that NIPS has been held in the US, and may be the last for awhile given travel issues in that country. From the NIPS website: \u201cOur goal is to open NIPS Conferences and Workshops to researchers from all parts of the world and we regret that US government restrictions may make it difficult or impossible for some to participate this year.\u201d NIPS was held in Barcelona last year and it will move to Montreal in 2018.\n\nA record-high 3,240 papers were submitted to NIPS this year, with 678 selected. The 2017 NIPS Best Paper Award goes to CMU Professor Tuomas Sandholm and his student Noam Brown for their paper demonstrating how an AI-powered poker computer (Libratus) beat top human players.\n\nNIPS 2017 has five competition tracks: Learning to Run, Deep RL; Human-Computer Question Answering; Classifying Clinically Actionable Genetic Mutations; Adversarial Attacks and Defences; and the Conversational Intelligence Challenge.\n\nSynced will be reporting from NIPS throughout the week."
    },
    {
        "url": "https://medium.com/syncedreview/zhou-yu-is-teaching-robots-how-not-to-be-ignored-555253906a66",
        "title": "Zhou Yu is Teaching Robots How Not to Be Ignored \u2013 SyncedReview \u2013",
        "text": "A young man sits in front of a webcam, responding to queries about his personal interests and academic and work background. This may look like a remote job interview, but the questions are actually being posed by a bot. The interview proceeds smoothly until at one point the man mumbles a short answer and averts his gaze. The bot immediately recognizes that he is behaving less attentively, and classifies him as \u201cdisengaged.\u201d\n\nMeet Dr. Zhou Yu\u2019s Multimodal HALEF, a real-time intelligent interactive system whose primary mission is to hold humans\u2019 attention.\n\nA Carnegie Mellon University (CMU) PhD grad, Dr. Yu joined the University of California in Davis (UC Davis) as an assistant professor last summer. The 29-year-old scientist was recently named to Forbes 2018 \u201c30 Under 30\u201d in Science list for her groundbreaking work in multimodal dialogues, an interdisciplinary field that enables robots to watch, listen, speak, and physically interact with humans. It\u2019s hoped that Dr. Yu\u2019s research will enable intelligent assistants such as Siri and Alexa to understand the nuances of non-verbal communication and use them to their advantage.\n\nMost dialogue systems are either text- or speech-based. However, natural human communication is not limited to words or voices \u2014 it is supplemented by gestures and facial expressions. We frown or raise our eyebrows to express disagreement, and wave our hands to say goodbye. Dr. Yu believes robots should be able to recognize such actions and give appropriate responses.\n\nIn the human-machine interactions of the future, a customer service bot, for example, could evaluate customers\u2019 moods or emotional states and recommend appropriate products like a real salesperson. A healthcare consultant bot meanwhile might judge whether patients are potentially suffering from mental illnesses.\n\n\u201cA lot of my work is to target different users, and to design unique interaction plans for each individual automatically through data driven methods,\u201d says Dr. Yu.\n\nBorn in the beautiful canal city Suzhou, Dr. Yu\u2019s keen interest in computer science began when her grandfather bought her a computer at a very young age. She started coding in elementary school and participated in coding competitions since then. In 2007 she was admitted to Chu Kochen Honors College, an elite undergraduate school of Zhejiang University, one of China\u2019s top academic institutions. Dr. Yu pursued a double major of Computer Science and English with a focus on linguistics. She also took classes in computer vision and machine translation.\n\nEquipped with her unique inter-disciplinary background, in 2011 Dr. Yu enrolled at CMU\u2019s Language Technology Institute, where she apprenticed to Dr. Alan W Black and Dr. Alexander I. Rudnicky \u2014 top-tier experts in spoken language dialogue systems.\n\nAt CMU Dr. Yu first worked on general aspects of vision and speech dialogues, before narrowing her focus to a simple challenge: How to keep humans engaged in a conversation. Recalls Dr. Rudnicky: \u201cZhou wanted to work on engagement in non-task oriented dialogues (often called chatbots), which wasn\u2019t really a research area at that time. Now the area is much bigger and others are working there. But she was one of the early people who started publishing in that field.\u201d\n\n\u201cThe reason focus on this problem is that we believe engagement is key to determining whether humans will continue interacting with robots,\u201d says Dr. Yu. Her technique incorporates dialogue systems with different information streams \u2014 such as vision information about face & body, and non-speech voice characteristics \u2014 to evaluate a human\u2019s level of engagement in a conversation, and then use that to initiate changes in strategy. The key is choosing the sort of strategies humans use in their own conversations.\n\nThe technique can also be employed in task-oriented dialogues, which are widely used in booking services, customer services, and tutors. For example in natural human communication, interjections like \u201cExcuse me\u201d or \u201cI beg your pardon\u201d are commonly employed to bring a disengaged person back into the conversation.\n\nOne of Dr. Yu\u2019s experiments involves a direction-giving robot that says \u201cExcuse me\u201d if humans look away or talk to other humans instead of the robot. If the human ignores this the robot will deploy a humanlike \u201csilent treatment\u201d until it regains its subject\u2019s attention.\n\nUser engagement may be abstract, but for research purposes it needs to be quantified. Dr. Yu regularly consults with cognitive scientists with expertise in nonverbal behavior study to make annotations. Subjects also review their testing videos and indicate when they did not pay much attention. These annotations and feedback are used for training the dialogue systems with supervised learning, a method of machine learning dependent on labeled data.\n\nMultimodal dialogue remains an early-stage research area, and only five or six research groups have made significant breakthroughs. \u201cThe field has high stakes, but many challenges remain,\u201d says Dr. Yu.\n\nOne of the biggest challenges is the integration of different modalities. In a speech-based dialogue, researchers only deal with the voice sampling range in Hertz. But a multimodal dialogue also involves visual data, such as videos, which are in a total different sampling range. Integrating disparate information in real time is much more difficult.\n\nThat challenge in integration of modalities leads to another problem \u2014 data collection. Unlike deep learning models in image recognition or machine translation that can be trained on large static datasets, training a multimodal dialogue system requires dynamic interaction with people. Such data is both difficult and expensive to acquire. Therefore Dr. Yu has turned to reinforcement learning, a machine learning method to let models take actions along with maximum rewards. Reinforcement learning can generate simulated data of human\u2019s conversations.\n\nAnother challenge remain in speech recognition is speed. Humans want machines to respond as quickly as possible, but speech recognition is time-consuming. Finding a way to reduce this lag inspired Dr. Yu\u2019s research into incremental speech recognition, a method to decode the voice while the user is still speaking.\n\n\u201cOf course the multimodal dialogue is a really hard problem, so it\u2019s not going to be fully solved in a short time,\u201d says Yu\u2019s mentor Dr. Black.\n\nWhen she\u2019s not busy at the lab, Dr. Yu enjoys walks in the park or camping with friends. On rainy days she reads paperbacks or watches YouTube animal videos. \u201cWatching small animals is the best way to relieve my stress,\u201d she says.\n\nLast December Dr. Yu joined the the Language, Multimodal and Interaction Lab at UC Davis as an assistant professor, migrating from the Steel City to the Golden State. Living and working in the world\u2019s greatest AI hub better positions Dr. Yu for financial support. Since 2016 Amazon has supported her research with US$100,000 in annual funding. She is also backed by Intel and Tencent.\n\n\u201cIts good to see PhD graduates have a strong vision for their area, and have a well-defined way to realize their research. She\u2019s in a strong position to do a lot more great work, and has the maturity to lead her own team,\u201d says Dr. Black about his former student.\n\nThis is just the beginning for Dr. Yu, who believes her research can make a positive impact on society. Some have already benefited, like the young man who gained job interview skills from HALEF. She estimates her work could be widely integrated in intelligent bots within five to 10 years. But for this dedicated young researcher, the process is just as important as the product: \u201cMultimodal dialogue is and will always be my passion and my lifelong research area.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/using-ai-to-secure-ride-sharing-platforms-3cc3c4569e9c",
        "title": "Using AI to Secure Ride-Sharing Platforms \u2013 SyncedReview \u2013",
        "text": "Fallout from the Uber data breach scandal is being felt across the industry. In an effort to beef up cybersecurity, researchers are exploring AI solutions. This was a hot topic for the Future of Automated Transportation Panel at Toronto\u2019s recent AI World Forum.\n\nUber came under scrutiny this November for delaying disclosure of a data breach involving 57 million passengers and drivers. The company\u2019s Chief Security Officer Joe Sullivan, attempted to appease the hackers with a US$100,000 payout and was fired as a result.\n\nIn the AI World Forum discussion, Fengmin Gong stressed the danger of scale involved in such a breach: \u201cWith platforms there\u2019s an amplifying effect, and anything you do will affect many people.\u201d Gong is VP of Research and Info Security at China\u2019s ride-hailing giant Didi Chuxing.\n\nIn 2016, Didi Chuxing bought Uber\u2019s China operation for US$7 billion, consolidating its domestic monopoly. Uber\u2019s recklessness in the recent hacking scandal has served as a warning and a lesson for its Asian counterpart.\n\nGong\u2019s department deals with 25 million rides and processes 4,500 terabytes of data per day from its 440 million users. His role is to protect the data from cyber attacks. Gong has worked with Charlie Miller, the hacker-turned-vehicle-security-architect who proved his prowess by remotely hacking and immobilizing a Jeep Cherokee on a St. Louis highway.\n\nAs vehicles increasingly automate, cybersecurity is emerging as a priority for the auto industry. \u201cIn today\u2019s transportation, from autonomous driving to connected and smart transportation, every layer is introducing more technical complexities,\u201d explained Gong. As a white-hat professional, he foresees myriad system loopholes opening as self-driving cars link to centralized cloud platforms, augment with human-machine interaction applications, and connect to other transportation networks.\n\nDidi Chuxing categorizes the threats into four areas: data security, program security, network security, and most importantly traveler and pedestrian security.\n\nAt an event in China last year, Gong laid out his white-hat philosophy in more detail: \u201cCore business must be unified and practiced in a closed loop. Security defenses are shifting into uninterrupted, large-scale monitoring and are using big data and AI to detect threats and anomalies. In short, this means deploying distributed security detection systems for centralized data analysis.\u201d\n\nBusinesses can use machine learning and cloud data collected through end-devices to analyze and learn from malware patterns. As the model learns it will become more robust in its future preventive measures.\n\nGong proposes an \u201cecosystem approach\u201d to cybersecurity in the age of AI. \u201cIt\u2019s key to build protective layers around key businesses, and streamline transactions, data, user information, and operation procedures to protect them as a whole unit.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/neural-translation-of-musical-style-6c16ba975378",
        "title": "Neural Translation of Musical Style \u2013 SyncedReview \u2013",
        "text": "The use of Neural Networks in music is not a new topic. In the field of audio retrieval, people were trying to use Neural Networks to model a variety of music elements [1] like chords, base frequencies, etc. As the author mentioned, in 1943, people started to solve speech recognition problems using Neural Networks. But at that time, computational capabilities were not enough to yield good results, so it never gained popularity. However, due to GPU computing and the availability of large amounts of data today, we are starting to see good results. Thus, the author wanted to run a musical experiment with Neural Networks like Fig.1 to achieve the goal of making the neural translation of musical style. In this article, the author provides us with a very detailed analysis on why and how he selected his approach and presents us with a good result by using the proposed approach.\n\nFirstly, I would like to let you know the background knowledge about both the music itself and the technology that will be used for implementation.\n\nOne of Google\u2019s more famous projects, Google Magenta [2], commands AI composers that use neural networks to generate melodies and produced some groundbreaking results. This demonstrated a successful musical application of neural networks. Google Magenta\u2019s amazing performance made the author believe that neural networks can also do some interesting predictions about music.\n\nThen the author analyzed two important elements of music: the composition and the performance. The composition focuses on the music itself, which means the musical notes that define a song. As author mentions, you can think of this as sheet music in Fig.2.\n\nBut this is just the first thing the musician needs to do. The performance, which means how these notes are played by the performer, can be the soul of the song. Because different performers can perform the exact same song differently, the musical style is used to describe the individualistic way of playing music.\n\nMusical style is hard to define, because we cannot simply parameterize the style like we can with pitch and notes. If you\u2019ve listened to a lot of classical piano music, it\u2019s very clear that a novice pianist and an experienced pianist produce different range of dynamics, which means a variation on the \u201cloudness\u201d of music. This \u201cloudness\u201d of a note can be performed by hitting the key with a hard stroke for piano. In music notation, these dynamics are indicated using Italian letters. These letters are called emoticons. Different people have different feeling, so they have their own emotional performance, which means the unique set of dynamics here. Theses dynamics can be a very important feature of style. We can see the notations in music in Fig.3.\n\nAlso, people can label a song with a genre such as Classical, Jazz etc. From the genre, we find that there are regulations to specific musical styles, such that people can identify the style by some dynamics. That means people can use genres to categorize the musical styles.\n\nIn reference [3, 4, 5], they are trying to generate the composition parts, but not the performance parts, So in this article, the author uses the MIDI file format to make the machine perform like a human by adding in dynamics.\n\nThe author designed and trained a model to synthesize the dynamics for sheet music, and showed two performances of the same sheet music: one performed by a human and the other generated by the machine. This is a blind test to see whether you can tell which one is performed by human. The author mentioned less than half of the respondents gave the correct answer. I\u2019ve also personally listened to the two performances. Unfortunately, I can easily tell which one is performed by human, because the bot\u2019s performance still contained some weird dynamics, and I am sure that a human won\u2019t perform like that. But it is still impressive.\n\nThe Feedforward Neural Network (FNN) is the most commonly used architecture. Neurons are connected in layers. The first layer is the input layer and the last layer is the output layer. The layers between these twos are called hidden layers. Fig. 6 shows the architecture with only one hidden layer. There is an important assumption in FNN: every input is independent of the rest.\n\nThe main limitation of a simple FNN is the lack of memory. This is due to the assumption of FNN that inputs are independent of each other. But in the case of music, it is almost always written with global structure, because musicians write the sequence of music based on the feeling they want to express, and cannot be considered independently. Recurrent Neural Networks (RNNs) can solve this kind of problems. RNN has states and a feed back loop called a recurrent weight. This structure takes previous state into the calculation for the immediate output. This means RNNs can have a short-term memory mechanism to remember what was computed in the past, and use it to compute the current results. This mechanism is shown in Fig. 7 below.\n\nMusicians have the ability to look ahead when performing, which can help them prepare for the upcoming emoticons. But for a simple RNN, it c\n\nan only read inputs in order. So we should introduce a structure that can access the upcoming time-steps. This is called a Bi-Directional RNN [6]. This architecture combines two RNN layers as shown in Fig.8.\n\nThe first layer is called the forward layer, which processes the input sequence in the original order. The second layer is called the backward layer, which can process the input sequence in the reverse order. This seems to be a good choice for the author\u2019s purpose.\n\nThe basic idea of Bi-directional Recurrent Neural Network is that for each training sequence, there is a RNN no matter if you\u2019re going forward or backward, and these two RNNs are connected to the output layer. This structure provides complete information either in the past or in the future. The pseudocode of forward pass and backward pass are showed below:\n\nFor RNNs, one major problem is that they cannot remember long-term dependencies [7, 8]. To solve this, Long Short-Term Memory Network (LSTM) was introduced[9]. As shown in Fig. 9, it gives a gating mechanism to control over the issues like how much needs to be remembered, and how much needs to be forgotten.\n\nFinally, we are ready to design the architecture. The author uses Bi-directional LSTMs in this article. There are two separate networks, one to realize the Genre, called GenreNet, and one to realize the style, called StyleNet.\n\nGenres have definitive musical styles, so the author uses this characteristic to design a basic model to learn the dynamics of a song. There are two main layers in this model, as shown in Fig. 10: the Bi-Directional LSTM Layer and the Linear Layer.\n\nThe Bi-Directional LSTM layers combine the advantages from LSTMs, which will provide the memory for learning dependencies, and the Bi-Directional architecture, which will allow the model to take the \u201cfuture\u201d into consideration. This makes this architecture\u2019s output feed into another layer as input. The linear layer is used to transfer the output, which ranges from [-1, 1], to a larger range.\n\nThis network is used to learn the more complex genres that GenreNet cannot be trained on. There are subnetworks of GenreNet contained in this StyleNet to learn genre-specific style. There is a layer called an interpretation layer that\u2019s shared by the GenreNet subnetworks. It reduces the number of parameters the network need to learn, and StyleNet is just similar to a translation tool to translate the music input into different styles. It can be seen in Fig. 11 that it\u2019s a multitask learning model.\n\nIn this article, the authors use music files in MIDI format, because this format preserves musical properties. There is a parameter called velocity that\u2019s analogous to volume, but with a range of 0\u2013127. Here, the author use velocity to detect the dynamics.\n\nThe author created a piano dataset of 349 tracks, with the genres limited to Classical and Jazz. The MIDI files the author used is from Yamaha Piano e-Competition. Human performances usually have at least 45 different velocities as shown in Fig.12. The author set 20 as the minimum threshold of different numbers of velocities.\n\nFinally, for the time signature, 4/4 is chosen in this article due to it being the most common. This provides the author with more sample than other time signatures.\n\nAfter getting the piano dataset, the next step is to design the input and output matrix.\n\nFirst, the dataset need to be quantized, which will allow the author to use matrices to represent the notes. But we will lose the exact timing of the notes. Here, the author approximate the time-stamps of all the notes to the nearest 1/16th note, allowing the capture of the notes. Fig. 13 shows the difference between the unquantized and the quantized representation of the notes.\n\nThe input will carry the information on note pitches, start time and end time. There are three states for each note, so the author uses a binary vector to represent these three states: \u201cnote on\u201d is encoded as [1,1], \u201cnote sustained\u201d as [0,1] and \u201cnote off\u201d as [0,0].\n\nThe note pitch needs to be encoded as well. A matrix is created. with the first dimension about the MIDI pitch number, and the second dimension about the 1/16 note (quantized time-step). This can be seen in Fig. 14.\n\nThe output matrix carries the velocities of the input. The columns also represent pitch and the rows represent time-step as shown in Fig. 14. The pitch dimension is only 88 notes, because this time we only need to represent velocity. The data is then divided by the max velocity, 127, and finally the output velocity is decoded back to a MIDI file.\n\nThe author provides a lot of detail on the training process, but I\u2019ll just highlight the key points. TensorFlow was used to build the model with two GenreNet units (Classical and Jazz), with GenreNet having 3 layers. The input should be 176 nodes wide and one layer deep. Mini-batches of size 4 were chosen, and the learning rate was set as 0.001. Adam optimizer was used to perform stochastic optimization. Here, the author used 95% Classical Songs and Jazz songs each for training, and 5% for validation. Also, the dropout rate was set as 0.8 due to the experimentation shown in Fig. 15. This dropout rate will make the model learn the underlying patterns. The model was trained for 160 epochs, and the final and validation loss were 0.0007 and 0.0011 respectively.\n\nFig.16 the author shows the training error and validation error.\n\nThe author also presents a lot of epoch snapshots from the training session, so we can see the difference between Classical and Jazz outputs. Fig. 17 is one of the snapshots.\n\nThe key to testing the results is to see whether the StyleNet can generate human-like performances, and the author use a Turing test [11] to test these results. The author created a survey called \u201cIdentify The Human\u201d, with 9 questions in two parts, and the participants will listen to the music in 10 second clips as shown in Fig.18.\n\nThe participants need to identify which performance is human. Fig.19 shows that on average, 53% of participants could highlight the human performance.\n\nTo make this survey more complete, the author added a new option called \u201cCannot Determine\u201d to make sure the participants\u2019 decisions were not from guessing. As shown in Fig.20, this time, the author found 46% of participants could identify the human. This means that the StyleNet model can pass the Turing Test, and it can generate performances indistinguishable from human\u2019s.\n\nI\u2019ve learned to play the violin for 19 years, and I strongly agree with the author\u2019s point that there are two important elements to music: the composition and the performance. And sometimes, the performance is the more difficult element. In my opinion, after hearing the music it generated, StyleNet\u2019s performance is very impressive. I think if the note quantiazation can be made smaller, like in 1/32 note or even 1/64 note, then it can achieve a better result. Also, I think a challeng lies in that you cannot make sure StyleNet learns the correct style from the human\u2019s performance. In the test the author provided, I can select the human\u2019s performance correctly because there are still some tiny details in the note that cannot be learnt by the network, while these details are the ones that I use to judge whether it is from a human or not. One suggestion is to let musicians to listen to the music generated by StyleNet, I think they can provide helpful some professional advice on possible improvements.\n\nI also want to make a comparison between neural style transformation in computer vision [12,13] and the proposed work here. Their basic approach is similar: both use two networks corresponding to style and content. But for neural style transfer in images, the difference between these two networks (one for content reconstruction and the other for style reconstruction) is larger. For the style reconstruction, it will be generated by calculating different subsets of the CNN like conv1_1,[conv1_1, conv2_1],[conv1_1, conv2_1, conv3_1],[conv1_1, conv2_1, conv3_1,conv4_1],[conv1_1, conv2_1, conv3_1, conv4_1, conv5_1] to capture the spatial features with different sizes of sub-sets. In the case of music, I think the main idea of using LSTM in generating music is to capture the features in styles and performances. It can be concluded as follows:\n\nTo summarize, CNN is a typical spatial depth of the neural network, RNN is the depth of time in the neural network. When using CNN, we should focus on spatial mapping, image data particularly fit this scene. But for music, we need the time sequence, and thus we should use RNN.\n\n[1] Colombo, Florian, Alexander Seeholzer, and Wulfram Gerstner. \u201cDeep Artificial Composer: A Creative Neural Network Model for Automated Melody Generation.\u201d International Conference on Evolutionary and Biologically Inspired Music and Art. Springer, Cham, 2017.\n\n[2] https://github.com/tensorflow/magenta\n\n[3] Warren S. McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity.\n\n[4] D. Eck and J. Schmidhuber. A First Look at Music Composition using LSTM Recurrent Neural Networks\n\n[5] Bob L. Sturm, Joao Felipe Santos, Oded Ben-Tal, and Iryna Korshunova. Music transcription modelling and composition using deep learning.\n\n[6] Composing Music With Recurrent Neural Networks.\n\n[7] M. Schuster and K. K Paliwal. Bidirectional recurrent neural networks.\n\n[8] Ilya Sutskever. Training Recurrent Neural Networks.\n\n[9] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem.\n\n[10] Sepp Hochreiter and J Urgen Schmidhuber. LONG SHORT-TERM MEMORY.\n\n[11] M Alan. Turing. Computing machinery and intelligence.\n\n[12] Jing, Yongcheng, et al. \u201cNeural Style Transfer: A Review.\u201d arXiv preprint arXiv:1705.04058 (2017).\n\n[13] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. \u201cA neural algorithm of artistic style.\u201d arXiv preprint arXiv:1508.06576 (2015)."
    },
    {
        "url": "https://medium.com/syncedreview/chinas-smart-speaker-market-heats-up-954df52d919e",
        "title": "China\u2019s Smart Speaker Market Heats Up \u2013 SyncedReview \u2013",
        "text": "Smart Speakers like Google Home and Amazon Echo are enjoying robust sales in the US, but have not penetrated the Chinese market. One reason is simple: They don\u2019t speak the language. The lack of Mandarin interface in these devices has prompted a number of Chinese companies to get in the game with their own smart speakers.\n\nThere are currently about 10 Chinese-language-enabled smart speakers on the market, and most are about the same: cylindrical plastic boxes stuffed with processing chips and an X-microphone array. Also almost indistinguishable are the speakers\u2019 services: connecting with smart home devices, updating news and weather info, and streaming music.\n\nDingdong was China\u2019s first homegrown smart speaker and remains the country\u2019s most popular. The DingDong A1 was launched in August 2015 by tech company Linglong, a joint venture between China\u2019s second largest e-commerce company JD Group and China\u2019s top voice technology company iFlytek.\n\nDingdong A1Similar to Echo, Dingdong A1 can access the JD.com online shopping site. The device is awakened by the playful hot word \u201cDingdong, Dingdong\u201d, which users cannot replace with other words. This year Linglong released the second generation of Dingdong with a price of CNY799 (US$120).\n\nJD.com\u2019s biggest rival Alibaba also debuted its smart speaker this year, the Tianmao Jingling X1, which is Mandarin for \u201cTmall Genie\u201d. The product integrates Alibaba\u2019s intelligent assistant system AliGenie, and links to Alibaba\u2019s online shopping site Tmall for voice-controled online purchasing. Tianmao Jingling X1 differs from Dingdong A1 in that it supports voiceprint payment, which can recognize users\u2019 voices when logging into their account.\n\nIn a bid to boost Tianmao Jingling X1 sales Alibaba dropped the price from CNY499 (US$80) to only CNY99 (US$15) for China\u2019s November 11 \u201cSingles Day\u201d online shopping spree \u2014 an event much like Black Friday in the US. By day\u2019s end over one million Tianmao Jingling X1 had sold, doubling Alibaba\u2019s expectations.\n\nXiaomi\u2019s soon-to-be-released Mi AI Speaker will be the lowest-priced smart speaker on the Chinese market at CNY299 (US$45). Chinese electronics and software company Xiaomi is best known for making affordable smart devices such as smartphones, laptops, wearables, and home appliances, and many industry watchers believe Xiaomi\u2019s forte in hardware and software development will make the speaker popular.\n\nXiaomi is already deeply established in the smart home device market, with over 60 million products sold. The Mi AI speaker will enable voice control for Xiaomi products such as air conditioners and TVs.\n\nXiaomi has created an anime avatar for the speaker. Xiao Ai, which is Mandarin for \u201cLittle Love\u201d, is an adorable redheaded girl with a sweet, natural voice. Anime culture is widely popular with Chinese consumers, and the character is expected to further drive domestic sales.\n\nTop Chinese tech company Baidu also entered the smart speaker marketplace this year. Raven H satisfies the demands of intelligent human-machine interaction and high-quality music playing. This is the first Raven series product released since Baidu\u2019s acquisition of smart home hardware startup Raven Tech in February.\n\nRaven H has a bold design reflecting Raven Tech\u2019s attention to aesthetics. The speaker resembles a stack of multicoloured plastic squares with a retro curly charging cable. It has a detachable LED touch screen controller which can be used as a voice-based remote to connect with Baidu-Raven\u2019s series of home devices.\n\nRaven H is powered by Baidu\u2019s virtual assistant Xiaodu, which was developed in 2015 and is best known for beating some of China\u2019s brightest humans in a Reality TV Show object detection and speech recognition showdown. Priced at CNY1699 (US$258), Raven H is China\u2019s first high-end smart speaker.\n\nWhile most Chinese smart speakers are wire-connected and designed for living rooms or offices, AI unicorn & voice technology company Mobvoi has released a portable smart speaker called TicHome Mini. The small round speaker has a tote band and is the world\u2019s first waterproof smart speaker, making it a good sing-a-long companion in the shower.\n\nTicHome Mini has released both Chinese and English-based versions. The English-based device is integrated with Google Assistant, while the Chinese version is powered by the company\u2019s own Mandarin-based system.\n\nThe smart speaker competition is heating up in China. Although it\u2019s too early to say who will come out on top, a big positive for all the players is the projection of an exponential market surge over the next two or three years. Consultant firm iiMedia Research says that by the end of 2020, Chinese smart speaker sales will surpass one billion yuan (US$150 million)."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-nov-pt-2-febd01985f7d",
        "title": "AI Biweekly: 10 Bits from Nov (Pt 2) \u2013 SyncedReview \u2013",
        "text": "Microsoft releases the new Azure IoT Edge at Microsoft\u2019s Connect 2017 Conference. The Azure IoT Edge aims to provide customers with a full set of AI services ranging from advanced analytics tools to machine learning modules. Azure IoT Edge will support all major programming languages including C, Java, C#, Node.js, and Python.\n\nDevelopers can track metrics including the number of active users, sessions, and user retention on Google\u2019s new Chatbase dashboard. Chatbase applies machine learning techniques to cluster unhandled messages and help developers optimize queries. Chatbase is compatible with most messaging platforms including Facebook, Kik, Viber, Slack, Whatsapp, WeChat, Alexa, Cortana, Allo, Twitter, and so on. Early adopters include Ticketmaster, HBO, Keller Williams, inGenious.AI, and FitWell.\n\nTesla unveils a an electric semi truck prototype with an autonomous driving function. According to CEO Elon Musk, Semi will become more efficient and cost effective than- -diesel trucks. The Level 2 autonomous driving system can control steering, acceleration, and braking on Autopilot. Walmart has ordered 15 trucks to upgrade their supply chain.\n\nApple announces that its premium HomePod smart speaker to be delayed in release. Apple hopes to begin shipping in early 2018 in the US, UK, and Australia. The US$350 speaker is a high-end competitor to Echo and Assistant offerings from Amazon and Google, and the first home device with Siri baked in.\n\nNov 17th \u2014 Amazon Joins Facebook and Microsoft in Support of Open-Source AI Platform\n\nAmazon announces its ONNX-MXNet Python package on Apache MXNet, which allows AI developers to freely switch between frameworks without having to start their model from scratch. Amazon joins Facebook and Microsoft in the Open Neural Network Exchange (ONNX) platform. Yet to join are Apple and Google.\n\nUber announces it will purchase 24,000 Volvo XC90s for their future autonomous fleet. The vehicles will be ready between 2019 and 2021. Volvo President and Chief Executive H\u00e5kan Samuelsson says the company aims to be the supplier of choice for the global autonomous driving ride-sharing service providers. Uber has already tested the XC90 with featured equipment in Pittsburgh. The XC90 has some core autonomous driving capabilities and Uber will add more features.\n\nGoogle reduces the price of its K80 GPUs to US$0.45 per hour and its P100 machines to US$1.46 per hour in the US. GPUs are mainly used for training machine learning models, running physical simulations, and building molecular models.\n\nThe Artificial Intelligence Conference (AIC) in Shenzhen, China attracted representatives from the Shenzhen government, universities and more than 200 companies. Keynote speeches discussed insights and strategies for developing an AI ecosystem in Shenzhen, the hardware capital of China, emphasizing collaboration between technology and industry.\n\nApple has acquired Vrvana, an augmented reality headset startup, for US$30 million. Apple is working on its own AR ecosystem, and the acquisition could indicate the future product and strategic direction of the company. Vrvana released its Totem headset, which was not shipped. The Totem is described as an \u201cextended reality\u201d device, which allows for both AR and VR in one headset. Apple also acquired SMI, another VR and AR headsets solutions company back in June 2017.\n\nNov 22nd \u2014 Uber\u2019s 2016 Data Breach is Said to Affect 57 Global Million Users\n\nUber confirmed this month that in a 2016 data breach hackers gained access to all the passenger and driver data in the company\u2019s database. The data breach has triggered lawsuits in the US, UK and other global Uber markets."
    },
    {
        "url": "https://medium.com/syncedreview/automated-inference-on-criminality-using-face-images-aec51c312cd0",
        "title": "Automated Inference on Criminality using Face Images",
        "text": "In this paper, the authors build four classifiers which are the logistic regression, KNN, SVM, CNN by supervised machine learning to discriminate between criminals and non-criminals. There are 1856 real people\u2019s facial images controlled for face, gender, age and facial expressions. The authors find that there are some discriminating structural features can help predict criminality such as eye inner corner distance and lip curvature. Upon further study, the authors found there is a large difference between criminals and normal people in facial expressions.\n\nThe authors first point out the lack of research on the analysis and quantification of social perception and attributes of faces [1]. While psychologists have known for a long time that humans\u2019 innate traits and social attributes, like trustworthiness and dominance, can be inferred from facial appearance, the authors propose we can use machine learning and computer vision to find that relationship. They point out that for computer vision algorithms, there is no subjective baggage. So that almost guarantees the objectivity of the results. They also mentions that the data for training these classifiers are standard ID images of real people controlled for race, gender, age, and facial expression, which will be more realistic and have higher quality compared with the studies based on sample face images generated by 2D or 3D face models [2, 3, 4].\n\nWe all know that a good data set is very important for experiments. The authors collected 1856 ID photos that satisfy the specific criteria: Chinese, male, range of age is between 18 and 55, no facial hair, no facial scars or another marking. Denote this data set using S, and divide it into two subsets Sn and Sc for non-criminals and criminals. Sn consists of 1126 non-criminals ID photos acquired from the Internet by web spider tool. One thing worth noting is that roughly half of individuals in subset Sn have university degrees. Subset Sc contains 730 criminals\u2019 ID photos, 330 of these are published as wanted suspects, and the others are under a confidentiality agreement. Some sample ID photos in Sc and Sn are displayed in Figure 1.\n\nFor these ID photos, the authors extract the region of the face and upper neck, and the background is removed. All these faces are normalized into 80 * 80 images. They also take extra measures to neutralize any other varied illumination conditions\u2019 possible effects.\n\nThe authors use four different classification methods on data set S to prove or disprove the hypothesis of using face images to distinguish criminals and non-criminals. The classification methods are K-Nearest Neighbor, Logistic Regression, Support Vector Machine, and Convolutional Neural Network. The first three classification methods work on image features, and there are 4 features to evaluate their performances on:\n\nThe criminal subset Sc is defined as positive class and the non-criminal subset Sn is defined as the negative class. The authors run 10-fold cross validation for all possible combinations of the first three feature-driven classifiers, with four types of feature vectors and one data-driven CNN without explicit feature vector. They examine the rate to classify a member of S into Sn or Sc, and average the rates of each case over ten runs in each of these 130 experiments (13 cases * 10 runs).\n\nWe can see the accuracies of all four classifiers for the thirteen cases in Figure 2. CNN classifier achieves 89.51% accuracy. The authors also plot the ROC curves for these four classifiers in Figure 3, and give the corresponding AUC results in Table 1. This can help measure the sensitivities of the data-driven and binary face classifiers for criminality. By far, the authors can say that the predictive power of this proposed approach is established.\n\nBecause of the high social sensitivities and repercussions of this topic, the authors want to excise maximum caution. They design and conduct the following experiments to challenge the validity of the classifiers: they randomly label the faces as negative or positive and redo all the above experiments. The outcomes show that the randomly generated instances cannot be distinguished, and the average classification accuracy is only 48%. They also tested the robustness of the experiments\u2019 results. They take 40 pictures from 10 male Chinese students in different environments, and the classification results are still higher than 83 percent.\n\nThese experiments show that the good accuracies of the four evaluated classifiers are not due to data overfitting.\n\nAfter doing the above experiments, the authors also want to find out what features of a human face is important for classifiers to tell whether this is the face of a criminal. Here, they apply the Feature Generating Machine (FGM) of Tan in [7]. We can see in Figure 4 (a) that the red-marked regions is the most critical parts. The authors find there are three critical areas that are very significant for separating criminals and non-criminals. In Figure 4 (b) it shows the discriminating features, which are the upper lip curvature (denoted by \u03c1), distance between two inner corners of the eye (denoted by d), and the angles between two lines from the tip of the nose to the two corners of the mouth (denoted by \u03b8).\n\nThe authors use Hellinger distance [8], which shows the relationship between two histograms and ranges from 0 to 1, to examine the two histograms. The histograms of the three critical features are shown in Figure 5. The mean and variance of these also are tabulated in Table 2. For angle \u03b8, the average is 19.6% smaller for criminals than non-criminals. Similarly, For the upper lip curvature \u03c1, the average is 23.4% larger for criminals than for non-criminals. And the distance d for criminals is slightly shorter.\n\nThe authors also generate average faces for criminals and non-criminals. It can be seen in Figure 6. But we can find that the average faces of these two datasets are very similar.\n\nTo explain this phenomenon, the authors give an assumption that faces of these two datasets are assumed to populate two distinctive manifolds. They compute the cross-class average manifold distance Dx between these two subsets, and in-class average manifold distances Dc and Dn in Function 2.\n\nThe results show that Dc > Dx > Dn, which means the two manifolds of these two datasets are concentric. Figure 7 shows the relationship of residual variance and Isomap dimensionality. This indicates that the original ultrahigh dimensional data set in a subspace of four to six dimensions can represent itself well.\n\nWe can also see the data clouds of criminals and non-criminals in Figure 8. These Figures and analysis proved that there is no subjectively meaningful typical face of criminals.\n\nIn Figure 9, it shows four subtypes of criminal faces in Sc and three subtypes of non-criminal faces in Sn.\n\nThe authors also asked 50 Chinese students to separate the criminals and non-criminals in Figure 9, and results turned out to match the results the authors expected. Figure 10 shows the relationship of variation within a cluster and number of clusters for the criminal and non-criminal dataset. It clearly illustrates that before K = 4, there are four well separable clusters of criminal faces. While for non-criminals, it doesn\u2019t form as many separable clusters in the geodesic distance. This data analysis helped the authors to draw a conclusion that criminals have greater variations in facial appearance than the general public, although they are a small minority in the total population.\n\nIn this paper, by extensive experiments and vigorous cross-validations, the authors proved that data-driven face classifiers can be used to make reliable inference on criminality. Additionally, the general public has facial appearances variations less than criminals.\n\nBefore I read the whole paper, I already saw a lot of comments for this paper. One of the most famous comments is an article named \u201cPhysiognomy\u2019s New Clothes\u201d [9]. The scientists in that article regarded this paper as scientific racism. In my opinion, from a methodological point of view, this article has merit. After achieving relatively good results, the authors didn\u2019t just stop but instead went on exploring the essentials of this problems. And the authors use mathematical methods to test and support the speculations. I don\u2019t think this can be seen at an ethical level. But in my opinion, using ID photos as a dataset doesn\u2019t make sense. Imagine the conditions that we take ID photos, there are some restrictions due to the place (the police office) we take the photos, people will be more cautious. While the more effective way to detect criminals should focus on their actions, and the demeanor and expressions when they take these actions. I think this will be more meaningful.\n\n[1] A. Todorov and N. N. Oosterhof. Modeling social perception of faces [social sciences]. IEEE Signal Processing Magazine, 28(2):117\u2013122, 2011.\n\n[2] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of cognitive neuroscience, 3(1):71\u201386, 1991.\n\n[3] V. Blanz and T. Vetter. A morphable model for the synthesis of 3d faces. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 187\u2013 194. ACM Press/Addison-Wesley Publishing Co., 1999.\n\n[4] V.BlanzandT.Vetter.Facerecognitionbasedonfittinga3d morphable model. IEEE Transactions on pattern analysis and machine intelligence, 25(9):1063\u20131074, 2003.\n\n[5] R. Gottumukkal and V. K. Asari. An improved face recognition\n\ntechnique based on modular PCA approach. Pattern \n\nRecognition Letters, 25(4):429\u2013436, 2004.\n\n[6] T. Ahonen, A. Hadid, and M. Pietika \u0308inen. Face recognition with local binary patterns. In European conference on computer vision, pages 469\u2013481. Springer, 2004.\n\n[7] M. Tan, L. Wang, and I. W. Tsang. Learning sparse SVM for feature selection on very high dimensional datasets. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 1047\u20131054, 2010.\n\n[8] R. Beran. Minimum Hellinger distance estimates for parametric models. The Annals of Statistics, pages 445\u2013463, 1977.\n\n[9] https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a"
    },
    {
        "url": "https://medium.com/syncedreview/bringing-robotics-from-the-lab-to-the-real-world-d6c94c76cba3",
        "title": "Bringing Robotics from the Lab to the Real World \u2013 SyncedReview \u2013",
        "text": "For today\u2019s large-scale manufacturers deep learning is like a thorny rose. The AI technology has achieved attractive results in machine vision and robotic applications, saving manufacturing time and costs for example by automating assembly or spotting product defects. Many manufacturers however soon realize they lack the talent and resources to properly handle complicated deep learning software unless they hire dedicated AI experts.\n\nAt last week\u2019s Collaborative Robotics and Advanced Vision Conference in San Jose, Cognex Director of Marketing for Vision Software John Petry spoke on the application of deep learning in advancing vision-guided robotics (VGR), stressing the need for robotic technology companies to provide maintainable deep learning-based solutions for their manufacturing customers.\n\nThe AI talent pool is shallow, and tech companies tend to scoop up talents with generous salaries and stock options. Petry believes manufacturers prefer deep learning applications that can be operated by their own engineers. \u201cIf you\u2019re going to have a viable solution, it can\u2019t be something where you need a PhD from Stanford to configure the system,\u201d he says.\n\nState-of-the-art robotic technology, which theoretically does not require complicated manuals and adjustments, is increasingly accommodating of manufacturers\u2019 expectations. Berkeley-based startup Embodied Intelligence, founded two months ago, is prototyping a machine learning system that allows humans to teach robots using virtual reality (VR). After a human performs a 30-minute VR demonstration, the mimicking robot can then learn how to perform that task.\n\nIn addition, Petry pointed out that industrial deep learning solutions are quite different from research lab prototypes, which researchers can develop using massive datasets and cloud-based training.\n\nPetry says that in the real world, deep learning solutions should be custom built for the environment where they will be deployed. For example, robots on a massive production line may not have independent access to cloud servers. Manufacturers are more likely to opt for an application they can run cheaply on a commercial PC.\n\nManufacturers also don\u2019t always have access to rich labeled data, and so robotic companies much make their applications effective with very limited datasets. \u201cIt\u2019s not to say that over time you won\u2019t be adding to it. But at least to get that first system deployed, to convince the customer, you\u2019re going to have to work with tens of images, not thousands,\u201d says Petry.\n\nVision-based deep learning solutions must also be flexible enough to accommodate different camera inputs. VGR equips robots with cameras to enable functionalities like picking, assembling and packaging. Many manufacturers want applications that can catch product defects. To that end, they might use multiple cameras to inspect the same product. A good deep learning solution should be able to deal with different camera positions, angles, lighting and resolutions.\n\nPetry identified four major fields where current deep learning software offers superior adaptability and efficiency over traditional methods: part correctness and orientation, deformation part location, pre-picking, and post-placement.\n\nPetry\u2019s presentation was a highlight of the conference, attracting more attendees than the room\u2019s capacity. While it\u2019s clear that deep learning offers power and potential, challenges remain regarding its commercialization in robotics."
    },
    {
        "url": "https://medium.com/syncedreview/natures-machine-intelligence-journal-to-launch-in-jan-2019-1e27858b97b6",
        "title": "Nature\u2019s Machine Intelligence Journal to Launch in Jan 2019",
        "text": "Nature will publish the first issue of Machine Intelligence in January 2019. The new journal will cover the \u201cbest research from across the field of artificial intelligence,\u201d ranging from machine learning, robotics and human-machine interactions to ethical, legal and societal dimensions of AI.\n\nMachine Intelligence will be an online-only publication, enabling readers to access content weeks before it appears in print. The journal will amalgamate fundamental and applied research papers, reviews, commentaries, and relevant news, adhering to Nature\u2019s editorial style. The editorial department will function independently of other Nature journals, allowing authors to either start a fresh submission or re-submit rejected manuscripts.\n\nUnlike other Nature publications, Machine Intelligence will have no external editors. Nature announced that Liesbeth Venema, who has a PhD in applied physics from Delft University of Technology in Netherlands and 17 years of experience with Nature, will serve as Machine Intelligence\u2019s Chief Editor.\n\nGuide2Research has compiled a list of top journals in the cross-disciplinary field of machine intelligence, ranked according to their JCR impact factor, SJR, and Scopus H-index. Interested readers can check here for more information: http://www.guide2research.com/journals/"
    },
    {
        "url": "https://medium.com/syncedreview/mit-chief-2017-conference-recap-b463bbcaab28",
        "title": "MIT-CHIEF 2017 Conference Recap \u2013 SyncedReview \u2013",
        "text": "Hosted at the MIT campus in Cambridge, the 7th annual MIT-CHIEF Conference addressed key issues in technology, innovation, and entrepreneurship in China and the US. The conference was held November 17\u201319th, and attracted research scientists, industry influencers, venture investors, and entrepreneurs from the Greater Boston Area.\n\nArtificial intelligence was the key topic of this year\u2019s discussions. The AI Panel moderated by Will Knight, senior editor of AI at MIT Technology Review, was first up on the agenda.\n\nDirector of IBM Research Cambridge and Acting Director of the MIT-IBM Watson AI Lab Lisa Amini spoke to the newly-announced US$240 million investment in Watson. MIES Professor David Sontag addressed the potential for AI applications in healthcare while criticizing the lack of collaboration between different AI companies that are working on the same problems. Toutiao VP Weiying Ma followed with his thoughts on AI-driven innovations at Toutiao.com.\n\nThe Autonomous Driving Panel was a highlight of the event, especially the fascinating debate between CTO & Co-Founder of TuSimple Xiaodi Hou and Bryan Reimer, Associate Director of the New England University Transportation Center at MIT.\n\nReimer proposed that conflicts between irrational human behaviors and societal norms might be a challenge in bringing autonomous vehicles to the market. Xiaodi Hou responded that \u201cthe better you disguise yourself as human drivers, the fewer troubles that you\u2019ll have,\u201d relating TuSimple\u2019s attempts to make autonomous trucks faithfully follow formalized societal norms such as speed limits. The company discovered during road tests that autonomous vehicles travelling the posted highway speed limit of 65 mph (105 kph) were being overtaken by speeding drivers with such frequency that changing lanes became unsafe. TuSimple finally had to train their model to behave more like a normal California driver.\n\nOn the topic of societal norms and ethics, panelists agreed that for questions such as the \u201cwould you kill one person to save five?\u201d trolley dilemma, machines can\u2019t know the answer because humans don\u2019t either. \u201cAn autonomous vehicle doesn\u2019t have a purpose,\u201d said Jiahua Zhao, Associate Professor of City and Transportation Planning at MIT, \u201cthe people it serves give it a purpose.\u201d\n\nThe conference\u2019s MIT-CHIEF Business Plan Contest is dedicated to contributing to a global entrepreneurial ecosystem based in the Greater Boston area. Eight out of 200 registered teams proceeded to the finals from seven tracks: Life Science/Healthcare, Artificial Intelligence, Advanced Materials/Energy, Smart Hardware/Robotics/loT, Web/Mobile, Social Impact, and Fin-tech. The winning teams were Cogentics, UrSure, Lind Health, and Pipeguard.\n\nMIT CHIEF 2017 closed with a keynote speech from Harvard Medical School Genetics Professor George Church, who joined the Alibaba DAMO Academy last month. Prof. Church provided his perspectives on the future of gene editing.\n\nMIT-CHIEF is a not-for-profit MIT student organization committed to promoting intellectual exchanges and collaborations between China and the United States."
    },
    {
        "url": "https://medium.com/syncedreview/makers-put-their-specialized-ai-chips-on-the-table-e6e3b08ab2ca",
        "title": "Makers Put Their Specialized AI Chips on the Table \u2013 SyncedReview \u2013",
        "text": "Like a brick to a house, the chip is the essential building block of any computer system. However, the Convolutional Neural Networks (CNN) that have revolutionized AI have thus far been constructed using a hodgepodge of chips originally intended for other purposes. To boost CNN performance, industry leaders and startups alike are now racing to develop specialized AI chips.\n\nGPUs first appeared in CNNs in 2012, when AI researchers half-stumbled onto the discovery that their parallel computing abilities made them suitable for AI tasks. Leading GPU maker Nvidia has pushed the limits of GPUs with its latest iteration, Tensor V100, widely seen as as a game changer. But GPUs are just what the name advertises: graphics processing units, designed for graphic-based vector tasks, not for CNN.\n\nAlthough GPUs remain today\u2019s top choice for AI computation, are they the future? Probably not. Head of Facebook AI Research Yann LeCun recently told Wired that \u201cthere is a lot of headroom for even more specialized chips that are even more efficient.\u201d\n\nMobile AI chips for example are specialized for running AI applications on smartphones, which previously had to connect to cloud servers to perform the advanced tasks demanded by AI applications. Huawei and Apple incorporated such AI chips into their phones this September.\n\nApple\u2019s A11 bionic chip includes a neural engine to enable features like Face ID or Animoji. The company lauds A11 as \u201cthe most powerful and smartest chip ever in a smartphone.\u201d Meanwhile, Huawei\u2019s latest SoC (system on a chip) Kirin 970 also has a neural processing unit, making it faster and more energy efficient than top mobile CPUs.\n\nKirin 970 is backed by tech from Chinese unicorn Cambricon, which makes AI chips for cloud servers, mobile devices, computer vision applications, and autonomous driving. The company recently raised a staggering US$100 million in its Series A funding round.\n\nMany other AI startups are also focusing on AI embedded chips for devices like cameras, home appliances, and mobile tablets \u2014 most of which will become intelligent in the next five to ten years. The stakes are high, last year Intel acquired on-device chip maker Movidius for an estimated US$400 million (price was not disclosed).\n\nSmart chip companies are now tailoring their designs for specific purposes such as deep learning or neural networks. \u201cTake the example of a surveillance camera that is meant to recognize humans\u2019 faces, it only needs a CNN-based AI chip,\u201d says Frank Lin, co-founder of Silicon Valley-based chip startup Gyrfalcon. In September the company released its first dedicated AI chip, the Lightspeeur 2801S Neural Processor, which combines high performance with an impressive energy efficiency rating of 9.3 TOPS/Watt.\n\nThe Lightspeeur incorporates 28,000 parallel computing cores and a new architecture that accelerates AI in the memory, directly eliminating data movement \u2014 a step in other architectures that requires heavy power consumption which can result in overheating.\n\n\u201cOverheating is a problem hindering the development of AI chips. You can build a big system to cool a cloud server, but that is not going to work on the scale of device-embedded AI,\u201d says Lin.\n\nLightspeeur supports CNN, RNN, and LSTM, and is especially suitable for CNN as it draws only 0.3 Watt at 50 MHZ while running 142 frames/second (224*224*3) images.\n\nMeanwhile at MIT, a project dubbed Eyeriss is addressing overheating challenge using a different approach. Their energy-efficient deep CNN chip features a spatial array of 168 processing elements fed by a reconfigurable multicast on-chip network that minimizes data movement by exploiting data reuse. Data gating and compression are used to reduce energy consumption. The chip can run CNNs in AlexNet at 35 fps with 278 MW power consumption, which is 10 times more energy efficient than mobile GPUs.\n\nAlthough widespread implementation of specialized AI chips remains years away, no chip maker wants to fall behind in the race to get there. Says Jianxiong \u201cProfessor X\u201d Xiao, founder of autonomous driving company AutoX, \u201cUsing a general-purpose chip is just a waste of resources. If an ASIC (application-specific integrated circuit) or FPGA (field-programmable gate array) chip is specialized to run convolution, the results will be much better.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/computer-vision-startup-cloudwalk-collects-us-375-million-c1dc8b6d5714",
        "title": "Computer Vision Startup CloudWalk Collects US$375 Million",
        "text": "Chinese startup CloudWalk Technology has received US$75 million in series-B investment led by Shunwei Capital, Oriza Holdings, and Puhua Capital. The windfall comes after the Guangzhou municipal government injected US$300 million into the computer vision company.\n\nEarlier this quarter CloudWalk competitors SenseTime and Face++ garnered funding of the same scale, the latter also backed by a state-owned venture capital fund. The heavy investments in computer vision will energize the IoT revolution that is rapidly building smart city infrastructures all across Asia.\n\nThe company says it will develop a Cross Strategy based on China\u2019s National Development and Reform Commission (NDRC) AI platform; continue to expand in banking, airport transportation, and security sectors; obtain dominant shares in AI niche markets; and at the same time incubate AI enterprises using the national AI platform.\n\nCloudWalk was founded in April 2015 as a spinoff of the Chongqing Institute of Chinese Academy of Sciences focusing on computer vision and AI. Company founder Xi Zhou graduated postdoc from the University of Illinois at Urbana-Champaign (UIUC) and partnered with UIUC professor emeritus Dr. Thomas S. Huang, a leading image and pattern recognition expert, to back the company\u2019s AI research.\n\nCloudWalk\u2019s current products include a big data platform for public security; ID card optical character recognition systems; integrated biometric systems for facial, fingerprint, voiceprint, iris, and finger vein recognition; infrared binocular detection devices; smart door access systems; facial recognition vending machines; and other computer vision API services.\n\nThe company has research labs in UIUC, Silicon Valley, Chinese Academy of Sciences, Shanghai Jiaotong University, and additional centers in four other Chinese cities. CloudWalk also operates joint-labs with the Ministry of Public Security, China\u2019s big four banks, and the Civil Aviation Administration of China (CAAC) to set AI product benchmarks and develop national standards for facial recognition technology. The results are already being adopted for intercity travel.\n\nIn March 2017, the NDRC tasked CloudWalk with building an AI Infrastructure Public Service Platform,putting the company on China\u2019s national AI team and expediting AI technology adoption in the public sector.\n\n\u201cFacial recognition technology is ubiquitous now and China is at the forefront of this development, especially for commercial use. CloudWalk outcompetes similar companies in POC tests for finance customers and builds really good products that satisfy real customer needs. It has great infiltration in finance, security, and civil aviation sectors,\u201d says Puhua Capital Senior Vice President Nanhai Zhong."
    },
    {
        "url": "https://medium.com/syncedreview/vinci-sets-the-tone-for-smart-headphones-db76aa004319",
        "title": "Vinci Sets the Tone for Smart Headphones \u2013 SyncedReview \u2013",
        "text": "That\u2019s the question that inspired Max Wu back in 2014 as he stared at his non-electrical, phone-dependent tethered headphones. The idea of a smart wearable music-listening device excited the Massachusetts Institute of Technology MBA candidate, who partnered with friend and experienced entrepreneur David Zhu to found the AI-focused audio technology company Inspero.\n\nThere was no reference product back then. iWatch, which would evolve the wristwatch into a voice-controlled intelligent device, was still on Apple\u2019s drawing board. And so Inspero started from scratch, and released the world\u2019s first intelligent standalone headphones Vinci in 2016.\n\nVinci 1.0 is a fashionable headset equipped with dual screens. USA today said Vinci\u2019s audio quality \u201cnever sounded better\u201d, while Forbes lauded the headphones \u201cmagical\u201d voice-driven interface.\n\nA year later, Inspero is about to push the boundaries with its second generation headphones Vinci 2.0, which inherit their predecessor\u2019s features but are much smaller in size and aimed at the fitness community. \u201cJoggers have to tie a phone on the body parts while running outside, which is an annoying burden. This is the problem that Vinci 2.0 can solve,\u201d says Wu.\n\nVinci 2.0 is a workout-oriented headphone set with a unique pentagon shape that drapes around the neck. Its kickstarter campaign raised US$200,000 over the first 24 hours, 10 times its goal.\n\nThis week, Synced sat down with Wu, who is now Inspero\u2019s CTO, and tried out the Vinci headphones.\n\nVinci headphones include built-in 3G cellular and WiFi connectivity, meaning all of Vinci\u2019s features are available to users no matter whether their phones are nearby. While Vinci 1.0\u2019s dual screens were used for network setup, Vinci 2.0 sets up via a smartphone app.\n\nInspired by Wu\u2019s previous work as an Intel hardware engineer, Inspero engineers supercharged Vinci with Mediatek quad-core Cortex A7 processors (the same processor found in smartphones and tablets) with 1GB of RAM. The hardware enables features such as voice control, phone calls, calendar alerts, health and workout statistics, and third-party services like Spotify and Amazon Alexa.\n\nVinci headphones can run seven hours on a charge, which is reasonable for products running high computation tasks. Wu told Synced that the team made significant improvements on Vinci 2.0 to make it small without compromising the chips or battery performance.\n\nSynced was impressed by Vinci\u2019s incredible noise reduction \u2014 the smart noise cancelling mode that can silence different environments like home, office, train and airplane. In fact, Vinci\u2019s noise cancelling works so well that Inspero researchers realized users could miss phone calls or messages. A bluetooth connection allows smartphones to relay call or message notifications to the wearer.\n\nInspero sees music recommendation as the most important Vinci feature. The headphones can recommend music based on the wearer\u2019s heart rate, activity, and listening habits. Researchers also developed a conversational interface, enabling Vinci to respond to natural language requests like \u201cI want some Justin Bieber\u201d, or \u201cGive me classical music\u201d, or even \u201cI\u2019m not in a good mood.\u201d\n\n\u201cVinci will not help users order a pizza or book a flight to New York City, like Siri or Google assistant. Our focus is on a goal-oriented natural language processing system in the music domain,\u201d says Wu.\n\nActivated by voice control powered by speech recognition technology, users start conversations with the wake phrase \u201cHi Vinci.\u201d\n\nFitness freaks are likely to use Vinci 2.0 in noisy environments like a gym or outdoors, and this can stymie voice recognition accuracy. To solve the problem, Inspero developed bone conduction microphones which rest against the user\u2019s neck, receiving voice signals from bones instead of the mouth.\n\nThis technology optimizes separation of speech from ambient noise, improving speech recognition and voice services even in extremely noisy environments. Wu says Vinci is the first consumer electronic product equipped with such technology. However, only the Vinci 2.0 Supreme model, which costs US$160 more than the basic Vinci 2.0 Lite (US$89), has bone conduction microphones.\n\nWhile voice control is efficient, it is not the most practical interface to use when, for example, requesting the next song. This is why Vinci headphones also accept physical commands. Vinci 1.0 users can swipe the right headphone screen to adjust volume or switch songs. On Vinci 2.0, an infrared gesture sensor on the right earbud can recognize hand swipes.\n\nBuilding such a product wasn\u2019t easy for a startup with just 40 people, and work remains to be done. For example, 3G cellular networks might not be up to speed in a non-WiFi environment, especially for cloud-based AI-driven tasks. Wu told Synced that Vinci will add 4G cellular as early as next year.\n\nVinci 2.0 has only three third-party services thus far (Spotify, Soundcloud, Amazon Alexa). Vinci plans to add additional services like China\u2019s Xiami Music, which has millions of users. In addition, Google assistant will be added to Vinci 2.0.\n\nAlthough Vinci 2.0 is significantly smaller and lighter than its predecessor, Wu wants to further reduce the size, and release different types of headphones. Maybe the Vinci 3.0 will arrive looking just like regular headphones?\n\nWu is convinced Vinci is the answer to the question he asked himself back in 2014, and will open the era of intelligent standalone headphones. \u201cThe smartphone is not designed for listening to music, but headphones are. We have smart voice-controlled devices like Echo and iWatch that are hailed by hundreds of millions of users. So why not Vinci?\u201d\n\nNote: Synced tech analyst Alex Chen participated in the interview with Max Wu, and contributed to the above content."
    },
    {
        "url": "https://medium.com/syncedreview/rolling-out-the-tesla-semi-68674441fdfb",
        "title": "Rolling Out the Tesla Semi \u2013 SyncedReview \u2013",
        "text": "Electric, automated, and stormtrooper-shiny: the Tesla Semi debuted yesterday in Hawthorne, California.\n\nTesla CEO Elon Musk has always stressed the aesthetics of Tesla products, and how they would fit together like pieces of a puzzle to form his grand future vision. In this regard, the sleek form and game-changing function of the fully-electric Class 8 heavy-duty Tesla Semi make it the perfect addition to the Tesla family.\n\nIn the United States trucks handle over 70% of all freight transportation. But trucking is treacherous: last year 4,000 drivers lost their lives on American roads.\n\nSemi\u2019s autopilot function includes emergency braking, automatic lane keeping, forward collision warning. It will adjust the torque on each wheel automatically to stop jackknifing. Semi will even call 911 if the driver passes out or becomes unresponsive.\n\nThe truck is powered by four Tesla Model 3 motors, each dedicated to one of the drive wheels. It also has a bullet-shaped nose; dynamic side flaps which adjust to the trailer being pulled; and a smooth, flat bottom designed to lower air resistance. Musk boasts that Semi\u2019s drag coefficient of 0.36 is better than the US$2 million Bugatti Chiron sportscar at 0.38, and of course, far superior to conventional diesel trucks in the 0.6\u20130.7 range.\n\nDrivers sit in the center of a minimalist carbon fiber cockpit, flanked by two screens. Truck and driver are fully integrated with fleet management systems.\n\nSemi can accelerate to 60 mph (100kph) in 5 seconds, much faster than diesel-powered semi-trailers. Even hauling its max gross vehicle weight (GVW) of 80,000 pounds, it can accelerate to 60 mph in 20 seconds. Semi can reach a speed of 65 mph while climbing a 5% grade \u2014 an important performance boost compared to diesel.\n\nTo industry analysts\u2019 surprise, Tesla announced that Semi\u2019s battery can power the truck for 500 miles (aprox. 800km). This is a reasonable range for many logistics applications, covering, for example, the trip from LA to Silicon Valley without stopping. Drivers can grab a coffee at a Megacharger station while getting a full charge in just 30 minutes. It takes about 15 minutes to fuel a diesel truck.\n\nHow much will this shiny highway beast cost? That\u2019s a good question, which Musk skirted at the unveiling. Instead, he provided a cost analysis under certain pre-conditions, estimating $1.26/mile for the Tesla Semi compared to diesel trucks at $1.51/mile. If three Semis are platooned, their cost dips under that of rail transportation.\n\nThe Semi\u2019s glass is thermal nuclear explosion-proof and it comes with one million mile money back guarantee. Pre-orders have already come in from Walmart, Michigan supermarket chain Meijer, and transportation company J.B. Hunt.\n\nAlthough the reaction to Semi has been largely positive, there are three main questions on industry insiders\u2019 minds: How much will Semi actually cost? Will there be there sufficient charging stations en-route to practically accommodate the trucks? And, can Tesla deliver all orders as promised by 2019?"
    },
    {
        "url": "https://medium.com/syncedreview/will-iflytek-voice-inputs-98-accuracy-kill-the-keyboard-d28cee0a4d62",
        "title": "Will iFlytek Voice Input\u2019s 98% Accuracy Kill the Keyboard?",
        "text": "Human-machine interaction is rapidly evolving. Today, 12% of Chinese users opt for voice input over typing. iFlytek, the country\u2019s most popular voice input method, can translate English voice input directly into Mandarin or vice versa. It also translates Mandarin into Korean and Japanese and tailors voice input for over 22 different Chinese dialects.\n\nOn November 7, the Smartisan Nut Pro 2 smartphone was released with built-in iFlytek voice input method.\n\nFor years iFlytek strived to improve its speech recognition accuracy rate to the current 98%. Using natural language interaction, the Chinese voice recognition unicorn solved challenging problems such as homonyms and wrong words. But there\u2019s always a new target to aim for. \u201cIn three years, voice recognition for personalized voice users can reach 99% accuracy,\u201d predicts iFlytek Input Product Manager Jibo Zhai.\n\nCommercialization of voice recognition technology began in the late 1990s with IBM\u2019s trailblazer productVia Voice. But it was hardly a game-changer, and appraisals came mainly from industry insiders. Some claimed that it didn\u2019t work at all. Ten years later Motorola made the same endeavor with its A1200 phone to no avail.\n\nWhen touchscreens appeared on the market, companies like Google began redirecting R&D to voice input, giving birth to a milestone in Google Voice Search, which transcribed and then searched from voice inputs. The massive amount of incoming data helped to optimize the product, creating a feedback loop with fast iterations. IBM\u2019s Via Voice had lacked this connection to billions of network users.\n\n\u201cBig data, cloud computing, and machine learning frameworks satisfy the needs of input methods,\u201d explains Zhai.\n\niFlytek\u2019s voice input team occupies the entire 7th floor of the company headquarters in Hefei, a low-key city in central-eastern China. As a steady stream of keyboard clicks flows through the room, youngsters huddle around a table brainstorming. Happy photos from company field trips fill the bulletin board behind them.\n\nThe relative normalcy contrasts with media bombardment visited on the offices after iFlytex\u2019s 2016 debut in the Smartisan T3.\n\n\u201cThe IME project began from point zero when iOS and Android systems first launched,\u201d says Kun Cheng, one of the original trio of engineers who worked on the project.\n\nThe project began with Cheng and his colleagues multi-tasking without a clear division of labour, writing lines of code during the day and testing at night. The iFlytek input method officially launched two months later using the Hidden Markov Model. But the accuracy rate was only 60%, meaning 40 of every 100 words were wrong. Even if accuracy were to increase to 80%, the program would be barely usable.\n\nBy 2014, iFlytek voice recognition accuracy had reached 97%, and garnered 200 million users. Today the product has 500 million users in China. People are no longer surprised by the technology \u2014 voice input has become a norm.\n\niFlytek Input accommodates keyboard, handwriting, and voice input. If you want to write, start writing; if you want to type, tap the keyboard; if you want to speak, tap the speaker icon. \u2018We have a patent for this,\u201d Zhang Yuan, operation manager of iFlytek tells us while performing a demonstration.\n\n\u201cHow would you rate your product?\u201d I ask.\n\n\u201c75 out of 100,\u201d reckons Zhai. \u201cWe are good with generic words, but still fall short on customization.\u201d\n\n\u2018Two major changes have come to IME,\u201d Zhai explains. \u201cThe first was connecting to the internet, and the second was its augmentation with AI.\u201d Convolutional neural networks have helped to raise handwriting accuracy rate by 30% and shortened writing lag time to 0.15 seconds. \u201cColleagues from our AI research institute and input department meet regularly to exchange insights. They brief us on possible applications, and we give them user experience feedback.\u201d\n\nThe iFlytek IME team has grown to 100 members, and the entire company now has 8,000 employees. The company\u2019s China Sound Valley building has become a landmark, and the view from the 7th-floor window shows two new commercial buildings on the rise. \u201cWhen we first moved here, it was quite barren,\u201d says Yuan.\n\nClearly, the voice input business is also on the rise.\n\nThe QWERTY keyboard has been with us since 1873, and because people are creatures of habit it likely won\u2019t disappear overnight. But in the eyes of future generations accustomed to voice input, the keyboard may very well be a relic, as quaint and confusing as the video cassette recorder is today."
    },
    {
        "url": "https://medium.com/syncedreview/toward-multi-modal-understanding-and-multi-modal-intelligence-39d43afdf66e",
        "title": "Toward Multi-Modal Understanding and Multi-Modal Intelligence",
        "text": "Recently, there are three papers on ArXiv that drew my attention. All of them have a similar and related topic: learning multi-modal information. The aims of this article is\n\nThe first paper (Aytar et al.) [1] presents an architecture incorporating three sub-networks: sound, text and image networks. On top of these three networks, there is another network which is shared with the three modalities (Fig 1). The authors claim that the network can do alignment and learn a higher-level representation with these three modalities. After training, transfer of \u201cconcept\u201d between untrained modality pairs occurs as well.\n\nFig 1: The common shared representation layers are fully connected (blue), trained either by transfer loss or by ranking pair loss. The modality-specific representations (grays) is convolutional.\n\nAll of the three sub-networks use convolutional network to extract the multi-modal information. In the sound sub-network, the sound is encoded in a spectrogram format. The input spectrogram is a 500*257 signal (i.e. 257 channels (frequency bins) over 500 time steps). Three one dimensional convolutions with kernel size 11, 5 and 3 are used to extract the sound information. In the text sub-network, the text is first pre-processed with a pre-trained word2vec embedding layer with a dimension of 300. Then, a sentence is interpreted into a fixed length of 300*16 vector, with either filling in zeros or cropping longer sentences. Then three one-dimensional convolution layers are used to deal with the image information as well, following the standard AlexNet [2]. But the layers are only stacked until the pool5 layer.\n\nAlthough three individual CNNs extract enough statistical information on the higher level, as the usual CNN techniques do, the next target is that we should construct readily-available shared across all of the three modalities. One important technique to extract the synchronous representation between these modalities is to get the representation alignment via optimization methods.\n\nSpecifically, the alignment is done as follows: assume that the target of learning is to align between one modality x_i and the second modality y_i. If we denote f(xi) to be the representation of the modality x. The metric to examine the alignment can be related to the cosine similarity. In this paper, two approaches regarding the representation alignment were proposed.\n\nThe first approach is called model transfer, which was originally used in transfer learning. It uses KL-divergence\n\nDelta_{KL} is a KL-divergence, which is usually used to measure the distance between two probability distributions. In here, the objective of the KL-divergence is to optimize the student signal f(y), a representation of the second modality, i.e. let it approach teacher signal g(x), which is the first modality. Here the model transfer approach is used to train the upper representation of the student models for sound, vision and text to predict class probabilities, to approach the teacher signal. The authors used ImageNet model as a teacher model.\n\nThe second approach is called ranking. Different from the model transfer function which only align the upper representation (class probabilities), the ranking loss function is used here to align the hidden representation of different sub-nets and to let it be discriminative.\n\nwhere Delta is a margin hyper-parameter, Psi is a similarity function (e.g. cosine similarity), and j iterates over all negative samples. Therefore, the loss function tends to push paired samples together while move apart mismatched pair. It is used to apply on the last three hidden representation of the network to pair two modalities one by one:\n\nvision \u2192 text, text \u2192 vision, vision \u2192 sound, and sound \u2192 vision\n\nSo in the alignment, the paper examined pairs of images and sound (from videos) and pairs of images and text (from caption datasets). Since there are not enough training samples on sound/text pairs, the authors do not train those pairs. But the experiments show interesting alignment, using vision as a bridge to enable transfer between sound/text. The experiment details can be found in the original paper.\n\nThe most interesting part is in the visualization. As shown in Fig. 2 there are two interesting properties: Firstly, although the semantics are not explicitly trained on the hidden layers, they are emerged as pre-symbolic concepts in an unsupervised way. Secondly, many of the hidden units are able to detect objects appear independently in different modalities. So the alignment probability happens on the object level.\n\nThe second paper by Kaiser et al. [3] builds an unified model to do multi-modal information processing. We have witnessed many different deep-learning architectures accomplish different tasks, such as speech recognition, image classification, and translation. Compared to the previous deep multi-modal extraction tasks, which extract the common shared representation hierarchically, thanks to the computational resources from Google, this work attempts to solve a few problems simultaneously, some of which even incorporates a large number of datasets. Similar as the first paper, it tries to learn different modalities. But its main focus is to solve multiple machine learning tasks with a unique architecture. The common features representation is not its focus.\n\nFigure 3: The MultiModel architecture by Kaiser et al. [3].\n\nThese basic structures combine together to form the overall architecture as either an encoder or decoder block as shown in Figure 3. Different color blocks represent different modality sub-nets, all of which share the same encoder-decoder architecture. We will not introduce these three building blocks in detail, because there are numerous reviews on it. The figure below depicts the architecture of each building block one by one.\n\nRegarding the modality sub-nets, the four modality nets use slightly different architectures, but mainly based on word embedding (text) and convolutional network (audio and image) to extract the input features.\n\nAt the first experiment, a few benchmark tests (ImageNet and translation) were conducted which show comparable results to the state-of-the-art tests, although the the authors claim that the hyper-parameters were not really well-tuned. The second experiment compare with the joint training of 8 problems with a training of a single problem. As shown in Tab. 2, not surprisingly, the joint training performs better than the single problem. Although the reason is not analyzed in detail in this article, but I suspect it has similar reason as the first article we introduced: the higher-level concept is somehow represented between the shared representation.\n\nTab. 1. Comparisons with MultiModel and state-of-the-art in three tasks [4][5].\n\nTab. 2. Comparisons of MultiModel trained on joint 8-problem and single problem.\n\nThe third paper by Hristov et al. [6] tries to cope with the raw multi-modal inputs together with a robot manipulation task. Therefore, the motor action can be regarded as another modality. Furthermore, different from the previous two papers, the multi-modal inputs, as well as the output, are used as the grounding sources of the natural language symbols. Specifically, as shown in Fig. 5, the symbolic features are grounded with the color and size features which are presented as a normal distribution in that feature space (e).\n\nFig. 5. Overview of the full system pipeline of [6]. (a) Input to the system are language instructions, together with eye-tracking fixations and a top view of the camera. (b) Natural language are deterministically parsed to an abstract plan language \u00a9 Using the abstract plan, a set of labelled image patches is produced from the multi-modal data inputs (d) Doing Feature extraction from the image patches. (e) The parsed symbol is grounded with the features.\n\nThe action, on the other hand, is grounded via semantic parsing. Therefore, while doing end-to-end training, the actions are abstracted as the format (action target location) (b). One example of parsing can be found in Fig. 6\n\nThe grounding the (action target location) via probabilistic learning based on Fig. 7 after the objects were represented in multi-modal data from the image (Algorithm 1) as well as eye tracking. Just like for the previous two paper, the detailed learning algorithm 1 is not introduced. Readers with interests can refer to [6].\n\n6. Example of parsing using dependency graphs.\n\nFig 7. The probabilistic model used for symbol grounding based on image and eye-tracking data [7].\n\nAs we discussed a bit in the last session, the three papers concentrate on different perspectives of multi-modal learning. The first paper provides novel algorithms and architectures to align different modalities. The main finding is that the hierarchical network, which is somehow similar to our cognitive functions, extracts concepts, or the meaning of the objects to be specific, at a higher-level. This is also further proved by its trans-modality test (sound/text). This is the most interesting and novel part, which somehow mimics the brain cross-modality function. This work, to some extent, build a possibility that this concept can be further used as a meta-cognition.\n\nThanks to the massive computational power from Google, the second paper used a lot of effort to build and test a unified network model to accomplish various goals. It also reveals a few key neural structures which may be crucial in an universal AI machine. They are useful either to extract the features of different modalities, or to be adaptively select the best internal strategies to solve the problems encountered. At the end, I will not be surprised that Google uses the same architecture to finish most of its AI related services. And this architecture will be a milestone to build an AGI (Artificial General Intelligence) machine.\n\nThe third paper does not employ any fashionable deep learning algorithms. But I think it proposes another important aspect of modality \u2014 motor action, if our goal is to build a machine or robot that can move. Moreover, the grounding problem of symbols will be also necessary to connect the state-of-the-art image/sound recognition problems with the GOFAI (\u201cGood Old-Fashioned Artificial Intelligence\u201d). Although the grounding problem with hierarchical multi-modal networks is not new (e.g.[8]), the state-of-the-art multi-modal architecture and new algorithms (e.g. [1]) will be more helpful for us (and for the machines) to discover the meaning of the multi-modal world, and to further calculate using a symbolic representation on a higher cognitive level."
    },
    {
        "url": "https://medium.com/syncedreview/iphone-x-mask-dupe-raises-ai-security-concerns-71b91e55036",
        "title": "iPhone X Mask Dupe Raises AI Security Concerns \u2013 SyncedReview \u2013",
        "text": "Machines have already beaten humans in image recognition accuracy, but they are also susceptible to errors. Last Friday, Vietnamese company Bkav announced they had outsmarted the AI-powered image recognition machine that unlocks the new iPhone X.\n\nBkav broke Face ID using a composite face mask made of 3D-printed plastic, silicone, makeup and paper cutouts. The company released a video showing the experiment: When the demonstrator unveils their creepy mask to the front camera, the iPhone is immediately unlocked.\n\nObviously, Face ID should not have been so easily cracked by a mask. Apple\u2019s new face recognition tech is powered by a system called TrueDepth, which includes a dot projector, infrared camera and flood illuminator. The setup does not simply detect a 2D image, but actually projects a network of dots onto its subject, like a 3D contour mesh, to determine whether a presented face matches the User\u2019s recorded face model. Apple also employs machine learning algorithms and neural networks run by the new A11 Bionic chip to train Face ID.\n\nWhen Face ID made its debut, Apple lauded it as 20 times more secure than its predecessor, the fingerprint-based Touch ID. Apple boldly put the chances that someone could trick it at one in a million. More importantly, Apple actually claimed that masks would not stymie the robust tech.\n\nBkav\u2019s method was simple: they scanned a test subject\u2019s face, used a 3D printer to generate a face model, and affixed paper-cut eyes and mouth and a silicone nose. The total cost was only US$150.\n\nWhile fooling Face ID is a coup for Bkav\u2019s machine learning researchers, it is nightmare for AI security at Apple and has raised widespread concerns regarding deep learning\u2019s fragility in dealing with adversaries.\n\nWhen two different objects have an uncanny resemblance in features, this can confuse deep learning models. A well-known example in the AI community is differentiating between Chihuahuas and Muffins.\n\nMachines can also be fooled by handcrafted inputs called adversarial examples, which are designed to fool a neural network. By adding imperceptibly small perturbations, adversarial examples successfully tricked a neural network developed by a team of researchers from Google, Facebook, New York University, and the University of Montreal into classifying a school bus and a dog as an ostrich.\n\nSan Francisco-based AI research institute OpenAI explains why it is so hard to defend adversarial examples: \u201cit is difficult to construct a theoretical model of the adversarial example crafting process\u2026 adversarial examples are also hard to defend against because they require machine learning models to produce good outputs for every possible input. Most of the time, machine learning models work very well but only work on a very small amount of all the many possible inputs they might encounter.\u201d\n\n\u201cEven if iPhone X/FaceID is trained to be reject some types of face masks, an adversary can create a mask very different than what it was trained on,\u201d tweeted Andrew Ng from Deeplearning.ai.\n\nBkav\u2019s cracking Face ID made headlines and upped the stakes on consumer device privacy, and more generally on AI-powered security. It will also ignite the race between security developers and disrupters."
    },
    {
        "url": "https://medium.com/syncedreview/ai-biweekly-10-bits-from-nov-pt-1-cb021a356227",
        "title": "AI Biweekly: 10 Bits from Nov (Pt 1) \u2013 SyncedReview \u2013",
        "text": "US telecommunications giant AT&T introduces its artificial intelligence platform Acumos, which is open-source and hosted by the Linux Foundation. Developers can now build location tracking and facial recognition AI functionalities. The platform supports content curation, self-driving cars, and VR/AR capabilities; and is based on AT&T Indigo, a data-and-software-powered network with faster internet speeds.\n\nOver the next few months Alphabet\u2019s Waymo will test self-driving cars in Arizona, USA, with no human in the driver\u2019s seat. The road test will use a fleet of Fiat Chrysler Pacifica minivans, and try to avoid bad weather conditions. Initially, a Waymo employee will travel in the back seat but eventually the car will travel alone. The state of Arizona has accommodating self-driving car regulations. Waymo has eight years testing experience with self-driving cars, making it a leading international R&D company for this technology.\n\nAlphabet\u2019s Waymo is partnering with leading US automotive retailer AutoNation to repair its driverless cars. Self-driving cars are expensive to maintain due to their various sensors, chips, and software gadgets; and companies like Waymo need to balance operation costs if they seek to make money off the technology. AutoNation will start providing services for Waymo in California and Arizona.\n\nBroadcom made an offer to buy mobile chip maker Qualcomm for US $105 billion. At the time the offer was made, Qualcomm was locked in a lawsuit with Apple, which put the company at a financial disadvantage. In their rejection of the bid several days later Qualcomm suggested that Broadcom had significantly undervalued its current market share in mobile, IoT, and self-driving car industries.\n\nThe Department of Transportation (DoT) now permits broader testing of drone operations such as package delivery. The new policy allows drones to overfly pedestrians, operate at night, and fly beyond eyesight-distance. The three-year civil testing program will allow observation of drone operations on a case-by-case basis, and operational data will be used in future policy-making.\n\nSalesforce announces its myEinstein AI platform, with two new services: the Einstein Prediction Builder, which enables automatic creation of custom AI models; and the Einstein Bots, which can be trained to enhance customer service workflows by automating question-answering tasks. The new platform essentially provides Salesforce customers with pre-built AI apps for CRM, and allows them to build and customize their own with just a few clicks.\n\nSouth Korea Creative Content Agency and S.M. Entertainment host a competition with six teams composing music with AI. The compositions involve sentiment and melody training, lyrics from big data, and appropriateness to environment. As AI grows in Korea\u2019s entertainment industry, companies like S.M. Entertainment are also developing virtual chatbots to allow fans to interact with their favorite K-Pop idols.\n\nSpeaking at Google\u2019s Go North 17 conference on November 2, Prime Minister Trudeau says Canada wants to get ahead of AI and help shape it now because its development \u201chas never been this fast, and yet will never be this slow again.\u201d Trudeau believes Canada\u2019s values, openness and diversity provide a favourable framework for AI development, and the country needs to continue to attract global AI talent. The Canadian government has taken multiple steps in 2017 to support AI.\n\nDJI releases FlightHub, the industry\u2019s first enterprise-level drone management solution platform for businesses. The platform enables drone operators to remotely and securely manage drone fleet operations in real-time, and monitor flight data and pilot team activities. It also includes features such as Map View to enable geofence-based simultaneous flight monitoring, and Real-Time View to allow offsite teams to view live video feeds captured from the drones.\n\nAmazon announces its fourth German research hub will open in Tubingen, joining hubs in Berlin, Dresden and Achen. Tubingen will focus on visual AI research to provide a better online shopping experience with upgraded visual systems, and on R&D for Amazon Web Services and Alexa. Tubingen is also home to the Max Planck Institute for Intelligence Systems."
    },
    {
        "url": "https://medium.com/syncedreview/11-11-alibabas-ai-powered-shopping-extravaganza-2734b62e7f23",
        "title": "11.11: Alibaba\u2019s AI-Powered Shopping Extravaganza \u2013 SyncedReview \u2013",
        "text": "Every November 11, Chinese consumers embark on an online shopping spree that stops only when they \u201cchop off\u201d their hands, as the saying goes. This year, 11.11 sales totaled US$25.3 billion across e-commerce giant Alibaba\u2019s eight shopping platforms, mostly Taobao and Tmall. The figure is double that of Black Friday and Cyber Monday totals combined.\n\nUS$1.51 billion worth of sales were recorded in the first three minutes of 11.11, which is also known as \u201cSingles Day\u201d or \u201cDouble 11.\u201d Last year it took six minutes to reach the same amount. With so many sales in so little time, it\u2019s no surprise that artificial intelligence is playing a growing role in the largest online shopping event in human history.\n\nRong Jin, Dean of Alibaba Institute of Data Science and Technologies, says the application of deep learning at Alibaba can be categorized into four main areas: computer vision for visual search, image classification and cross-media retrieval; combinatorial optimization for 3D bin packing and automatic banner design; speech and NLP technology for acoustic models, dependency parsing and mimicked QA; and model simplification for model compression.\n\n11.11 is a day of non-stop action for the several thousand front-line Alibaba engineers in Hangzhou, whom colleagues have nicknamed the \u201csiege lions.\u201d In the quiet hours before Singles Day\u2019s midnight start, some opt for a stroll by the west lakes, while others try to relax by their computer screens with a cup of Dragon Well tea.\n\nIn late October, 200 engineers performed the first 11.11 \u2018stress testing\u2019 to determine whether Alibaba\u2019s hardware infrastructure could handle Singles Day\u2019s peak levels. Alibaba also partnered with 500 banks, enterprises, logistics companies and government agencies to conduct \u201cfull-link\u201d stress testing. \n\n \n\nFull-link stress testing is an 11.11 rehearsal with no users involved. The process simulates sales using a similar online environment, the number of users, and transaction scenarios and scale. It then initiates a tuning process to optimize system functionality for peak performance. The goal is to monitor generic metrics such as CPU, memory, hard drives, reaction time; and also to ensure the system is protected.\n\nAlibaba\u2019s Vanguard Program is responsible for the stress-testing and capacity tuning processes. This year\u2019s team required just three stress-testing sessions, down from 10 last year. The machines themselves can automatically make more than 50% of system repairs.\n\nTo ensure the billions of transactions run as planned, Alibaba uses AI external and internal feedback systems. DingTalk, an office communication tool similar to Slack, is used in conjunction with online chatbots to automatically log staff problems into the system. The system then applies machine learning algorithms to conduct a cluster analysis. \n\n \n\nThis automates the internal feedback loop and shortens time spent on communication. On Singles Day this basic application helps log and track all reported problems on shopping portals. Questions are processed by machines before they triage to DingTalk.\n\n \n\nOnline shoppers know the power of customer comments and ratings. While internal feedback systems monitor technical issues, external monitoring focuses on user comments. \n\n \n\nAlibaba\u2019s machine learning algorithm uses unsupervised clustering analysis for automatic comment monitoring, so problems are identified in real-time. The system de-noises incoming data, pre-processes it using business rules, then proceeds to use optimized K-medoids, CNN, and TextRank algorithms to do a clustering analysis. A final run through with AdaBoost meta-algorithm ensures generalization capabilities.\n\nThe E-commerce boom has transformed China\u2019s logistics industry, with more than 60% of delivery parcels now originating from online orders. The workload skyrockets on Singles Day, and Alibaba Group affiliate Cainiao logistics does the heavy lifting. \n\n \n\nA logistics niche applying deep learning technology is 3D bin packing, where one of the first questions is how to effectively pack a parcel using the smallest box possible. Using Deep-Q networks, Alibaba can determine the best fit, which has saved 5% of overall packaging costs.\n\nWhile a packaging savings of 5% may sound trivial, it adds up fast over the over 812 million orders made on Singles Day. Optimization efforts have also cut delivery time from an average of nine days in 2013 to three days last year.\n\nAmid the frenzy, Yang Lu, a Cainiao Logistics engineer in his early 20s, somehow found the time to complete an AI music composition program side project called MusicGo, which creates rap music in celebration of Single\u2019s Day.\n\nLu collected ten thousand rap lyrics online, preprocessed them to remove punctuation and special characters, then trained a seq2seq model to generate its own rap lyrics. The final composed song is called The Rap of T-mall 11.11, a title inspired by the Chinese reality TV show The Rap of China. \n\n \n\nHere\u2019s a Google translation of the lyrics using the latest state-of-the-art NLP technology with some editorial touch-ups:\n\nAs night fell on Singles Day, Nicole Kidman joined Alibaba CEO Jack Ma onstage at the 11.11 Celebration \u2014 a classy wrap to a crazy day that has become the world\u2019s biggest international shopping extravaganza superpowered by AI."
    },
    {
        "url": "https://medium.com/syncedreview/rvicarious-ai-is-teaching-robots-to-see-the-world-like-humans-8f0464202e5",
        "title": "Vicarious.ai is Teaching Robots to See The World Like Humans",
        "text": "The ultimate target in the AI research race is human-level artificial general intelligence (AGI). Although few researchers are audacious enough to predict how or when they might achieve that lofty goal, Dileep George and D. Scott Phoenix are confident they\u2019re on the right path with unique solutions combining machine learning and the human brain itself.\n\nThe Founders of Union City, California-based Vicarious.ai are teaching robots to tackle problems that humans are good at solving. Vicarious.ai\u2019s technology imitates the human visual cortex system to achieve significant results in computer vision tasks such as text and object recognition, robotic manipulation, and reasoning.\n\nFounded in 2010, Vicarious.ai surprised the AI community in 2013 when it announced its technology had solved text-based CAPTCHAs \u2014 the widely-used Turing-style web security test designed to distinguish humans from bots. Such CAPTCHAs are considered ineffective if a computer program can fool them at a rate above 1%. The Vicarious.ai model achieved astonishing rates of 66.6% on reCAPTCHAs, 64.4% on BotDetect, 57.4% on Yahoo, and 57.1% on PayPal.\n\nThis news however raised suspicions, especially as Vicarious.ai did not release details on their tech. Yann LeCun, a principal contributor to the development of Convolutional Neural Networks, called the Vicarious.ai announcement \u201ca textbook example of AI hype of the worst kind.\u201d\n\n\u201cWhen we broke CAPTCHA in 2013, we were very concerned about after-effect security, so we were careful about how we released the details,\u201d says George. Vicarious.ai only recently published a paper in Science Magazine introducing its Recursive Cortex Networks (RCNs), the tech behind the CAPTCA coup.\n\nUnlike deep learning models, RCNs adopt generative probabilistic models which can simulate and regenerate an object\u2019s features such as basic elements, corners, contours and shapes. Generative models have two distinct advantages over deep learning models: better performance in generalization, and the capability to deal with adversarial examples (handcrafted inputs that are used to fool a neural network).\n\nVicarious.ai researchers borrow insights from human brains to build the RCN. For example, the human visual system has a lateral connection that ensures humans can retain object contours in mind. When applying such human vision characteristics to RCNs, the lateral connections enforce the continuity of contours. A top-down attention mechanism, which enables humans to easily recognize overlapping items separately, is also employed in RCNs.\n\nThe AI community has a running disagreement regarding the best method for pursuing AGI. Last month, LeCun debated New York University\u2019s Gary Marcus on whether AI requires cognitive machinery like humans and animals. Marcus suggested AI researchers should apply cognitive science insights to machines; while LeCun countered that machines can be developed using only unsupervised deep learning, which is the major AI technology currently being adopted in industries.\n\nGeorge, however, told Synced he believes machines can only reach human-intelligence levels by referencing human brains. \u201cAny learning algorithm can be considered as a search algorithm eventually, but the search is too huge without any reference. You definitely need a structure, which we call \u2018scaffolding\u2019, from the brain.\u201d\n\nGeorge began studying human brains while doing his PhD in Computer Engineering at Stanford University. After graduating in 2005, he teamed up with Silicon Valley neuroscientist and entrepreneur Jeff Hawkins to found Numenta, a software company focused on machine intelligence.\n\nAt Numenta, George delved deeply into neuroscience-machine learning research, pioneering hierarchical temporal memory (HTM), a computational method based on principles of the neocortex. HTM technology is particularly suitable for problems such as streaming data, underlying patterns in data change over time, subtle patterns, and time-based patterns.\n\nIn 2010, George left Numenta and founded Vicarious.ai with D. Scott Phoenix, a tech entrepreneur who also regarded the human brain as the key to building human-like robots. \u201cWhat\u2019s magical about the human brain is it\u2019s a truly general-purpose learning architecture that can learn any tasks in the rich sensory world you and I are living in,\u201d Phoenix told Goldman Sachs.\n\nThe novel approach of Vicarious.ai caught the attention of Facebook angel investor and Paypal founder Peter Thiel, who financed the company\u2019s seed round in late 2010. By 2014, Vicarious.ai had raised US$40 million in Series B funding, from investors such as Facebook\u2019s founder & CEO Mark Zuckerberg, Y Combinator CEO Sam Altman, and Tesla founder Elon Musk. Total Vicarious AI financing has thus far exceeded US$130 million.\n\nIn recent years Vicarious has been ramping up research on how to apply RCN to robots, especially industrial robots. The company\u2019s Head of Commercialization Dr. Xinghua Lou told Synced that the technology can, for example, help flexible manufacturing systems react to changes whether predicted or unpredicted.\n\n\u201cVicarious.ai will provide intelligent modules in aspects of visions and controls for warehouse and manufacturing robots,\u201d says Dr. Lou. The company\u2019s current robot prototypes are being supplied by its investors ABB Group and Amazon.\n\nAlthough the road from industrial robots to AGI is bound to be a long and difficult one, Vicarious.ai is convinced it can get there by the year 2040. \u201cWe believe achieving advanced intelligence in AI is just as great as sending humans to the moon, and that motivates us to work,\u201d says George. \u201cI don\u2019t think other companies will solve AGI before us.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/dueros-project-prometheus-baidus-big-data-pledge-to-conversational-ai-6cc7b70705c7",
        "title": "DuerOS Project Prometheus & Baidu\u2019s Big Data Pledge to Conversational AI",
        "text": "The open source dataset is the very fuel driving AI engines. Datasets such as ImageNet have proved essential for image recognition. Chinese search giant Baidu now wants to bring the same power to conversational AI, the tech behind our rapidly-expanding voice-based human-machine interfaces.\n\nAt Silicon Valley-based GSV labs today, Baidu DuerOS launched Project Prometheus, which aims to advance state-of-the-art R&D in conversational AI. The project will release one of the world\u2019s largest open datasets for Mandarin conversational AI as early as next January, to boost the capabilities of AI products like voice assistants and smart speakers.\n\nBaidu is \u201cAll in on AI\u201d with a heavy investment in AI-driven fields, especially conversational AI. China has a huge market potential: the IDC forecasts that by 2020, 27% of Chinese households will have smart home systems, 51% will have smart cars, and 68% smartphones and wearables. \u201cVoice is increasingly becoming how we interact with our devices today,\u201d says Kaihua Zhu, CTO of Baidu\u2019s DuerOS.\n\nDuerOS is Baidu\u2019s Alexa or Siri \u2014 a platform for conversational AI. Launched earlier this year, DuerOS supports home appliances like TVs and smart speakers and mobile devices like phones or watches. Developers can access open-source SDKs and APIs to build third-party voice conversational services. And DuerOS\u2019 bot platform provides countless skills.\n\nDuerOS Project Prometheus\u2019 large-scale dataset is the last piece of the puzzle. Three large-scale sub-datasets will be released: far-field wake word detection, far-field speech recognition, and multi-turn dialogue.\n\nThe wake word dataset will collect training data for five to ten popular Chinese wake words, including \u201cXiaodu, Xiaodu\u201d, which activates DuerOS enabled devices. There are 100,000 different audio clips for each wake word. The evaluation data consists of real recordings of human voices with ambient sounds from different environments; and simulated recordings from TV, popular music, etc.\n\nThe speech recognition dataset will include 4,000 hours of Mandarin far-field speech recognition data, which enables hands-free devices to hear and process voice commands from afar or in noisy rooms. The dataset comes from multiple domains, including search, chatting, online comments, etc.\n\nThe multi-turn dialogue dataset will release 10,000 dialogues covering 10 different domains to promote the development of multi-turn conversation technology.\n\nDuerOS also encourages third parties such as universities, companies and domain communities to organize challenges using these datasets.\n\nMany AI researchers are excited by the project. Mei-Yuh Hwang, a leading speech recognition expert and CTO of Chinese AI unicorn Mobvoi, flew from Seattle to Silicon Valley to evaluate the dataset for Mobvoi\u2019s R&D. \u201cData for conversational AI, especially for multi-turn dialogue, is rare.\u201d\n\nHowever, acquiring such data is known to be time-consuming and expensive, so it remains uncertain whether Baidu DuerOS can deliver the dataset as promised. Chenchen Guo, Baidu\u2019s Principal Architect for DuerOS told Synced that Baidu DuerOS is looking for accountable data companies for quality control, and will also collect data directly for example by sending devices to vendors.\n\nIn addition to the datasets, DuerOS Project Prometheus features two other programs: a talent program, which will invest in conversational AI projects and foster talent; and a university program which will collaborate with universities and research organizations to conduct joint training, course design, and workshops.\n\nThe project includes advisors who are leading researchers in speech recognition and natural language processing at US universities, such as Sr. Manager of Amazon Machine Learning Dr. Bj\u00f6rn Hoffmeister, Director of Human Language Technology Center of Excellence at Johns Hopkins Dr. Sanjeev Khudanpur, and CTO and Co-founder of a Stealth Conversational AI Startup Dr. Antoine Raux.\n\nPrometheus is the Greek mythological figure who stole fire from Mount Olympus and gave it to mankind. Baidu believes DuerOS Project Prometheus can deliver a similar revolution in conversational AI."
    },
    {
        "url": "https://medium.com/syncedreview/berlin-breaking-the-wall-between-human-and-artificial-intelligence-9d989aad836b",
        "title": "Berlin: Breaking the Wall Between Human and Artificial Intelligence",
        "text": "Artificial intelligence is on every country\u2019s development agenda, alongside technologies such as robotics, neuroscience, and quantum computing. At the International Conference on Future Breakthroughs in Science and Society in Berlin, leading AI researcher Dr. J\u00fcrgen Schmidhuber spoke on the November 9 anniversary of the fall of Berlin Wall. Much like that epoch-making event 28 years ago, AI technology is breaking down limitations and opening up new opportunities.\n\nA long-time advocate of artificial general intelligence (AGI), Dr. Schmidhuber embarked on his research career in 1987 with a diploma thesis describing \u201cmeta-learning\u201d programs, which can revise or change their own learning algorithms. In 1997, he co-authored a paper with Sepp Hochreiter introducing memory functionality to enhance a neural network\u2019s functionality, and pioneering the methodology that would become known as \u201cLong Short-Term Memory\u201d or LSTM. The methodology was further developed in 2000 by Dr. Schmidhuber and his PhD students Felix Gers and Fred Cummins, which helped to improve improved speech and handwriting recognition capabilities. To date, this recurrent neural network has proved to be extremely efficient.\n\nDr. Schmidhuber says LSTM functions similarly to the human cortex. There are more than hundred billion neurons in our cerebral cortex, and each works like a small processor. Some take care of inputs, some handle image capturing and others are for processing thoughts. Humans also have nerves which capture pain and control muscles. These units are connected and intercommunicate when a task is executed. The strength of these connections fluctuates as the human learns. This is called \u201cpersistent connection,\u201d and it was the inspiration behind Dr. Schmidhuber\u2019s early LSTM research.\n\nDr. Schmidhuber notes that Google drastically improved its speech recognition functionality in Android phones and other devices by using an LSTM program trained with Connectionist Temporal Classification (CTC), a method first published by Dr. Schmidhuber\u2019s lab in 2006. Today, LSTM also powers QuickType and Siri on almost one billion iPhones. And of course, Facebook, which performs over 4.5 billion translations every day, also relies on this inbuilt model.\n\nPost-LSTM, Dr. Schmidhuber and his team moved on to all-purpose AI projects. As the industry has begun to reap the benefits of deep learning, ambitious researchers are shifting focus to more fundamental problems, and also zooming out to the big picture and the big prize in all of this: human-level artificial general intelligence.\n\nDr. Schmidhuber\u2019s view of artificial general intelligence rests on something much more broadening, which involves having artificial intelligence soaring into space and starting its own species. Is it too early for such optimism, or is it just the time for us to contemplate grander themes? For more of the conference visit: http://falling-walls.com/conference"
    },
    {
        "url": "https://medium.com/syncedreview/artificial-intelligence-glossary-78eae3531797",
        "title": "Artificial Intelligence Glossary \u2013 SyncedReview \u2013",
        "text": "This glossary currently includes a total of 743 specialized AI terms based on machine learning concepts and terminologies. Synced will continue to update the collection. Vocabulary updates proceed in two phases: 1) we will extract terminologies from textbooks or other credible academic sources; 2) we will extract uncommon terms found in compiled essays or other sources into the glossary.\n\nReader feedbacks and suggestions are vitally important for the success of this project, if you have any suggestions please don\u2019t hesitate to let us know in the comment section! To read the glossary in Chinese please see \u673a\u5668\u4e4b\u5fc3\u7ffb\u8bd1\u7ea2\u5b9d\u4e66 or visit us on GitHub!)"
    },
    {
        "url": "https://medium.com/syncedreview/video-understanding-is-a-new-vista-for-ai-13be04416f56",
        "title": "Video Understanding is a New Vista for AI \u2013 SyncedReview \u2013",
        "text": "AI-powered machines are now able to recognize images more accurately than human beings. This has prompted tech companies to explore the new field of video understanding, in which machines are trained to answer questions like \u201cWho is in this video?\u201d or \u201cWhat are the cats in the video doing?\u201d\n\nIn a session on video understanding at last week\u2019s AI Frontiers Conference, Google Principal Scientist Rahul Sukthankar, Facebook Manager of Computer Vision Manohar Paluri, and Alibaba iDST (Institute of Data Science and Technologies) Chief Scientist Xiaofeng Ren all agreed that video understanding has an unfathomable potential if energized by AI.\n\nDeep learning has certainly delivered better results than previous methods in video understanding research, said Sukthankar. Five years ago, multiple steps were required between input and output of training models, including manually designed descriptors and codebook histograms; now, deep learning offers end-to-end solutions by directly feeding data into the model. Deep-learned models can effect an 80% improvement in mean average precision over models using hand-crafted features.\n\nDeep learning is already being used to optimize YouTube services such as large-scale video annotation and automated thumbnail selection.\n\nSukthankar said Google is planning to use video understanding to train robots to learn human movements from videos. At the conference Google introduced its time-contrastive network, a neural network that simulates actions in a video and learns basic movements such as standing or bending.\n\nThe above mentioned research cannot be achieved without appropriately using large-scale open-sourced video datasets. Sukthankar said the characteristics of different video datasets correspond to different video understanding research fields. For example, Sports-1M and Youtube-8M are designed for video annotations; HUMOS, Kinetics, and Google\u2019s recently released dataset AVA are used for action recognition; while YouTube-BB And Open Images can be applied to train models for object recognition.\n\nPaluri from Facebook introduced the company\u2019s newly released open-source visual data platform, dubbed Lumos. Based on FBLearner Flow, Lumos is a platform for image and video understanding. Facebook engineers need not be trained in deep learning or computer vision to train and deploy a new model using Lumos.\n\nPaluri also announced the exciting news that Facebook will release two new datasets early next year: Scenes Objects & Actions (SOA) and Generic Motions.\n\nRen from Alibaba discussed application scenarios for video understanding, focusing on how to apply it to Alibaba\u2019s e-commerce business. For example, Alibaba is able to recognize objects in video content and connect to a shopping weblink at Taobao (an Amazon-like platform). This year, Alibaba began allowing allow Taobao sellers to upload promotional videos. Taobao can then analyze the video content to improve product search.\n\nIt\u2019s not just the tech giants that are achieving significant improvements in video understanding. Berlin-Montreal AI startup Twenty Billion Neurons GmbH (TwentyBN) introduced an AI system called Super Model that can observe actions in the real physical world and output a live caption of what it sees. Last year TwentyBN announced a funding round of US$2.5 million, and invited Dr. Yoshua Bengio to become an advisor.\n\nThanks to AI, machines are rapidly developing a clear and accurate perception of humans\u2019 dynamic physical environments. And it would seem the more they can understand humans, the more humanlike they can become."
    },
    {
        "url": "https://medium.com/syncedreview/david-wagner-on-adversarial-machine-learning-at-acm-ccs17-f75e729a96d8",
        "title": "David Wagner on Adversarial Machine Learning at ACM CCS\u201917",
        "text": "UC Berkley Professor David Wagner delivered a keynote speech to open the ACM Conference on Computer and Communications Security (CCS) 2017. The top tier computer security and privacy conference was held in Dallas, USA. Prof. Wagner joked this was a \u201ckindergarten level introduction\u201d to adversarial machine learning, covering topics such as model evasion, model extraction, model inversion, differential privacy and data pollution attack.\n\nInstead of starting by explaining what adversarial machine learning is, Prof. Wagner showed multiple sets of images that looked the same but were predicted with incorrect labels.\n\nFigure 1. An image of bus recognized as hummingbird after adding some noise (Szegedy 2014)\n\nFor example, in the figure above, an image of a bus (left) was perturbed by adding noise (middle) to form a new image (right) which looks the same but is then detected as a hummingbird by the model.\n\nAfter showing these adversarial examples, Prof. Wagner explained how they were generated, comparing the process to solving an optimization problem:\n\nHere x is the original input and delta stands for the perturbation added to x. The goal is to find a new input x+delta that labeled as target label t by the model, which is different from the original label. At the same time, we need to minimize the distance between the original input and the perturbed new input.\n\nFurther, if we can find a proper loss function to describe the optimization problem, gradient descent methods can be then applied to find the solutions.\n\nAfter showing how to find such adversarial examples, an audience member asked how to attack models without knowing the models\u2019 parameters. As in the previous examples mentioned, loss function of the original model is used to solve the optimization problem which makes it a white-box attack. However, in the real world, a black-box attack where the model is not known by the adversaries is a more common situation.\n\nProf. Wagner then introduced transferability of machine learning models: some examples can be predicted the same regardless of model parameters or even different algorithms (i.e. neuron networks, SVM, decision tree). Therefore, attackers can train a local model and find adversarial examples for it first, then transfer those examples to a black-box model to complete the attack.\n\nHow can we defend against such attacks? Prof. Wagner identified several possible solutions \u2014 such as protecting the parameters, adding randomness and making it non-differential \u2014 but none of these works due to transferability. \u201cNo known effective defense exists,\u201d he said, explaining that we can only improve the robustness against adversarial examples with adversarial retraining. The state-of-the-art paper on the subject will be published in July 2018 in conjunction with the ICML 2018 in Stockholm.\n\nAfter showing how machine learning models might be misled by crafted inputs, Prof. Wagner introduced another problem called model extraction. As machine learning models are trained using massive of data and resources, the model itself becomes an expensive property for its owners. However, recent research has shown that an adversary can \u201csteal\u201d the model by simply querying it (Tramer usenix 16). In 2017 researchers improved model extraction performance issues by reducing the query numbers (Papernot AsiaCCS 17). Besides being used to steal models, model extraction also facilitates black-box attacks since it learns a substitute model by just querying the black-box model. Attackers can use this learned substitute model as a white model instead of training a new one which can be difficult due to dataset limitations. There is no existing defense against model extraction either.\n\nMachine learning models also have privacy vulnerabilities. Research has shown that training data can be reconstructed from the model. The principle behind model inversion is simple: using features synthesized from the model to generate an input that maximizes the likelihood of being predicted with a certain label. This could be critical to privacy-sensitive data such as medical entries and government classified information.\n\nFigure 4. Reconstructed image of training data from the model (Fredrikson 2015)\n\nBesides model inversion, differential privacy is another topic related to privacy vulnerability of machine learning models. More efforts have been put into this problem, which could be defended against by adding noise in gradient descent or partitioning the data during the training process.\n\nFigure 5. Defense against differential privacy through partitioning the training process.\n\nDue to the huge volume of data required to train deep learning models, crowd-sourcing is necessary. This creates a new attack surface adversaries can use to pollute the training dataset. Researchers have found that by injecting polluted data into the training data, it is possible to affect accuracy and even insert a backdoor in the learning model.\n\nA discussion on safety issues regarding adversarial machine learning was canceled due to time issues.\n\nIn the Q&A, the author of DolphinAttack, Wenyuan Xu asked whether it would be possible to eliminate all the adversarial examples since these seem to be a property of the machine learning model itself. Prof. Wagner did not answer in the affirmative, leaving an open question for the community to consider."
    },
    {
        "url": "https://medium.com/syncedreview/jdd-2017-global-data-challenge-now-accepting-challengers-7c9982651b0a",
        "title": "JDD-2017 Global Data Challenge Now Accepting Challengers! Prize Up to USD 45,000!",
        "text": "The world is witnessing the revolutionary power of artificial intelligence, big data, and cloud computing. As one of the world\u2019s biggest economies and most populated country, China is developing future mega-metropolises powered by the AI engine. Businesses using AI technology to provide products or services are building a society of tomorrow.\n\nIf you are excited by the impending change just as we are, the time has arrived for you to take on the challenge: on November 6, JD Finance will launch the first JDD-2017 Global Data Challenge. Yes, we want you on board!\n\nYour insight, creativity, and most importantly, technical skills will be used to address real-world problems, competing side-to-side with members from the global tech community. Moreover, challenge winners will take home up to 45,000 USD cash prize and be offered positions at one of China\u2019s largest e-commerce companies.\n\nTeams can choose either Algorithm Track or Business Solution Track for one of the following four competitions.\n\nNote: all prizes are pretax, actual figures need to deduct tax\n\n45,000 USD per Competition Champion Team, final interview offer from JD Finance for technical positions (valid within 1 year)\n\nSubbarao Kambhampati, President of the Association for the Advancement of Artificial Intelligence (AAAI), Professor at Arizona State University\n\nSebastian Thrun, Co-Founder of Udacity, Co-Founder of the Google\u2019s Self-Driving Car Project"
    },
    {
        "url": "https://medium.com/syncedreview/ai-chip-explosion-cambricons-billion-device-ambition-9aba63da25c4",
        "title": "AI Chip Explosion: Cambricon\u2019s Billion-Device Ambition",
        "text": "On November 6 in Beijing, China\u2019s rising semiconductor company Cambricon released the Cambrian-1H8 for low power consumption computer vision application, the higher-end Cambrian-1H16 for more general purpose application, the Cambrian-1M for autonomous driving applications with yet-to-be-disclosed release date, and an AI system software named Cambrian NeuWare.\n\nCompany CEO Tianshi Chen also announced Cambricon\u2019s plan to launch the MLU machine learning series processor for cloud-based applications in 2018.\n\nSo far, industry giants Intel and NVIDIA enjoy a strong lead in the AI chip race. A successful series of GPU products has pushed NIVIDA\u2019s market value to over US$100 billion. Insiders like Will Yuan, Director of Strategic Business Development in Shanghai for British microprocessor leader Arm, believe the prohibitively high R&D and manufacturing costs of AI chips make this a marketplace only accessible to big semiconductor companies.\n\nThe high entry barrier has not however deterred audacious investing in chips. San Francisco startup Cerebras raised over US$112 million in three rounds of financing this year, while Wave Computing and Graphcore each raised US$60 million from investors including DeepMind\u2019s Demis Hassabis, OpenAI, and Uber\u2019s Zoubin Ghahramani.\n\nRiding the global wave, China\u2019s chip unicorn Cambricon attracted an impressive US$100 million in its Series A round this August, with much of the money coming from strategic investors like Alibaba and Lenovo. The current market value of Cambricon stands at US$1 billion.\n\nIn current computer architecture, CPUs are responsible for the general task and logic-controlled computing, while GPUs handle data-intensive, graphics-based vector tasks. However for AI processing which involves matrix multiplication and addition, both CPUs and GPUs fall short in instruction cycles. Cambricon\u2019s specially designed NPUs (neural processing units) tackle this problem.\n\nIn 2016 Cambricon launched its AI processor \u201c1A\u201d, designed especially for mobile devices, with hugely augmented performance per watt. By the end of 2016, Cambricon had booked CNY 100 million worth of orders. Huawei\u2019s newly-launched Kirin 970 chip is equipped with the 1A processor (NPU). The Kirin 970 chip installed on Huawei Mate10 has a HiAI mobile computing architecture comprised of four parts: CPU, GPU, ISP/DSP and a NPU.\n\nCompany CEO Tianshi Chen says Cambricon\u2019s NPU \u201cis at least two orders of magnitude\u201d better than CPUs or GPUs for image and voice recognition.\n\nIn order to succeed in an arena with well-established competitors, Cambricon is tapping into the niche market of deep learning processors for specialized applications. These ASIC (application-specific integrated circuit) microchips can achieve exceptional performance in areas not yet covered by the industry giant\u2019s generic processors.\n\nHowever, custom ASIC chips can only run fixed algorithms, they cannot adapt to more general applications. Chen says he does not want Cambricon to be pigeonholed as an ASIC chip company: \u201cNeural network processors is where we began, and we will be launching more general-purpose chips in the future.\u201d\n\n\u201cWe hope that Cambricon will soon occupy 30% of China\u2019s IP market and embed one billion device worldwide with our chips. We are working side-by-side with and are on the same page with global manufacturers on this,\u201d says Chen."
    },
    {
        "url": "https://medium.com/syncedreview/andrew-ng-says-enough-papers-lets-build-ai-now-6b52d24bcd58",
        "title": "Andrew Ng Says Enough Papers, Let\u2019s Build AI Now! \u2013 SyncedReview \u2013",
        "text": "While the scientific community continues looking for new breakthroughs in artificial intelligence, Andrew Ng believes the tech we need is already here. In his keynote speech Friday at the AI Frontiers Conference, the founder of Coursera & Deeplearning.ai encouraged AI talents to roll up their sleeves and start making a difference \u2014 whether with major industry players or through their own startups. \u201cWe have enough papers. Stop publishing, and start transforming people\u2019s lives with technology!\u201d\n\nThe three-day conference drew over 1,400 attendees from 17 different countries to the Santa Clara Convention Center. Ng\u2019s keynote speech was titled \u201cAI is the new electricity\u201d.\n\nThe number of papers submitted across arxiv-sanity categories such as machine learning, computer vision, and speech recognition has dramatically risen since 2012, says OpenAI\u2019s Senior Engineer Andrej Karpathy. Few, however, make significant or direct contributions to society.\n\nNg wants to quicken deep learning\u2019s transition from research to real world application. Over the last five years deep learning has achieved huge success in energizing computer vision and speech recognition. Ng believes natural language processing is the next major field deep learning will revolutionize.\n\nNg is however concerned that a shortage of AI talents will stall its implementation in society, and so has pioneered online deep learning education. In addition to five newly released specialization courses on Coursera, Deeplearning.ai is expected to soon launch an experimental deep learning camp aimed at training coders with machine learning skills in deep learning tasks.\n\nNg foresees an explosion of job openings for engineers with AI expertise. Market research firm Vanson Bourne\u2019srecent survey of 260 large organizations confirms the adoption of AI is growing remarkably. Many companies may even introduce a dedicated AI C-Suite position: \u201cThe movement to hire Chief AI Officers is taking off,\u201d tweeted Ng last month.\n\nDeeplearning.ai recently posted openings for full-stack engineers and software engineers with machine learning skills. Ng is also headhunting AI talent for other companies he\u2019s involved with, such as San Francisco autonomous driving startup Drive.ai (whose CEO is Ng\u2019s wife Carol Reiley); and Woebot, a chatbot startup where Ng chairs the board of directors.\n\nThe influx of AI is changing tech companies\u2019 workflow. For example, a software & product engineer can build a user interface prototype from a mockup; but cannot build a chatbot or other AI-driven application in this way \u2014 an appropriately designed machine learning model is also required.\n\nNg cautions that even if a technology company does R&D on neural networks, that does not make it an AI-driven company. True AI companies also prioritize the strategic data acquisition and the unified data analytics required to train the neural network.\n\nFollowing Ng\u2019s keynote speech, AI Frontiers invited over 50 leading figures from tech giants such as Apple, Google, Amazon and Microsoft to share recent research results and product development information from different trending applications.\n\nClick the link to read more insights from AI Frontiers."
    },
    {
        "url": "https://medium.com/syncedreview/canadas-big-push-on-ai-69fde664d0f9",
        "title": "Canada\u2019s Big Push on AI \u2013 SyncedReview \u2013",
        "text": "Canadian Prime Minister Justin Trudeau recently joined the Canadian machine learning community to celebrate the country\u2019s burgeoning AI ecosystem at the University of Toronto Rotman School of Management\u2019s Creative Destruction Lab (CDL). Held in late October, the event attracted scientists and investors, mostly from central Canada and the US east coast.\n\n\u201cI think we all understand, certainly in this room, the way the world is going,\u201d said Trudeau with regard to the wide-reaching changes artificial intelligence is bringing. \u201cSo let\u2019s be part of it and help shape it, and let\u2019s make sure we\u2019re benefiting from the innovations in design and the applications and jobs.\u201d\n\nTrudeau also said that like China and the United States, Canada regards the development of artificial intelligence technology from a top-down strategic perspective. This March, Canada announced a CDN$125 million \u201cPan-Canada AI Strategy\u201d; in September Trudeau shared a stage with Jack Ma at the Gateway \u201917 event to welcome Alibaba\u2019s venture into Canadian online shopping; and in October Trudeau personally called Amazon CEO Jeff Bezos to invite him to build his company\u2019s new headquarters in Canada. The city of Toronto also hooked up with Alphabet\u2019s urban innovation department Sidewalk Labs to redevelop 12 acres of prime lakefront land this year.\n\nGovernment top-down policy support is vitally important, but the Canadian industry\u2019s immediate task is providing AI based services and products that attract investors and sell in the market.\n\nDespite its strong R&D capabilities, Canada\u2019s native artificial intelligence business prospects remain uncertain. While many global AI companies are tackling big challenges in manufacturing and healthcare; most of Toronto\u2019s artificial intelligence companies are more narrowly focused on areas like real estate, marketing, entertainment, and fintech.\n\nMontreal startup Element AI helps businesses drive revenue through artificial intelligence. Co-founded by Yoshua Bengio, it received CDN$137.5 million in Round A financing this summer and is expected to become an artificial intelligence unicorn. However, additional Canadian AI unicorns have thus far failed to materialize.\n\nOne big positive for Canadian AI is the emerging northward exodus of companies and talents from Silicon Valley. AI legal search company Ross Intelligence is one example of this. Founded by Canadians, the company is shifting operations from California back to Canada. Founder Jimoh Ovbiagele recently returned to Silicon Valley to headhunt Chinese engineers, exploiting widespread industry fears of American visa issues in the wake of Donald Trump\u2019s election.\n\nA healthy AI ecosystem is built on regional R&D clusters, capital, and policy. The average time required to train a doctoral student is five to six years, and the PhD mentor has great influence on the direction of the student\u2019s research, and even how and where to develop it after graduation. In the coming few years, Canadian universities will graduate about 800 PhD students, who will have the advantage of studying under renowned scientists like Geoff Hinton, Yoshua Bengio, and Richard Sutton.\n\nCanada\u2019s commitment to AI is clear. British-Canadian researcher Geoffrey Hinton and Spanish-born Uber ATG Technology Director Raquel Urtasun have both said they regard Canada as the most suitable country for the development of generic artificial intelligence research. However, many still question whether a country with a population of just 36 million (about the same as California) can realistically attract and retain top talents in the field.\n\nFor many AI academics, the answer has less to do with Canada\u2019s population and more to do with professional ethics and national policies. In 1986 Hinton left the US to avoid military funding from DARPA (the US Defense Advanced Research Projects Agency). Over the past 60 years DARPA has invested heavily in Carnegie Mellon University, MIT, the California Institute of Technology and other STEM Universities, effectively pushing artificial intelligence researchers into the weapons arena. In 2015 the US military invested US$149 million in 30 research projects on automated weapons. This surge in lethal autonomous weapons research has been vigorously protested by scientists such as Stuart Russell at the University of California, Berkeley.\n\nAt the CDL\u2019s Q&A session, panelists were asked a simple question: \u201cWhich country do you think will win the artificial intelligence contest?\u201d New York University Professor Gary Marcus immediately replied \u201cChina 2030 will dominate the world,\u201d while Silicon Valley investor Steve Jurvetson said \u201cno doubt, China or the United States.\u201d Speaking with modesty befitting Canadian stereotypes, University of Alberta Professor Richard Sutton answered: \u201cMy hope is that no country will dominate this technology.\u201d"
    },
    {
        "url": "https://medium.com/syncedreview/bayesian-lstms-in-medicine-7af2ae13c976",
        "title": "Bayesian LSTMs in Medicine \u2013 SyncedReview \u2013",
        "text": "This paper gives a demonstration of using Bayesian LSTMs for classification of medical time series, which can improve the accuracy compared with standard LSTMs. First, I would like to give you a short introduction of RNNs (Recurrent Neural Networks), because LSTMs (Long Short-Term Memory Networks) is a specific type of RNN. For humans, our thought doesn\u2019t always start from scratch. For example, when you read an article, the preceding text (that you understand) can help you understand the text you are currently reading. But for some traditional neural networks, they just abandon the previous content and start from scratch. What they represent is that your memory is persistent, but the traditional neural network is not. RNNs can solve this problem theoretically, but the truth is that due to its simple structure (which usually has only one tanh layer), it cannot handle long-term dependencies. LSTMs is proposed with a more complicated structure, which has four neural network layers, to make it possible to solve the problem of long-term dependencies.\n\nIn medicine, doctors need to evaluate multiple parameters, and based on a complex mixture of assumptions and intuitions, make their decisions. RNNs have been proved to achieve some of the best results in reference [1, 2, 3, 4]. But due to the fact that RNNs\u2019 models cannot provide a certain measure of the practitioners\u2019 decisions, the effect of treatment for a given patient cannot be deterministic. Bayesian probability theory can help to reason a model\u2019s uncertainty in a mathematical way. There are two key benefits by using the Bayesian deep learning, one is to increase the classification accuracy of medical signals, and the other is to provide a measure of confidence in the model decisions. The authors also point out that even if the conventional Bayesian approaches introduce too much overhead, it can be implemented into the online classification in a clinical setting, which can help save the overall cost.\n\nBayesian LSTMs is a kind of LSTM that uses dropout to perform Bayesian inference. It uses the simple one, which consists of three gates (input, output, forget) and a cell unit. The gate uses a sigmoid activation function, while input and cell state usually use tanh to convert.\n\nIn this paper, the authors\u2019 implementation of LSTM is based on reference [5] using Tensorflow [6]. There are 4 gates in each cell of the LSTM: input i, output o, forget f, and input modulation gatesg. The functions are shown below:\n\nThe internal state ct is to represent as cell, and it is updated additively. \u03c3 represent the non-linear sigmoid activation. W\u2217 and U\u2217 are the input and hidden weight matrices, respectively, with biases b\u2217. After having these, we can compute the input to each gate\u2019s non-linearity by using the following single matrix multiplication:\n\nAnd according to reference [7], this approach to generate the matrix is a faster forward-pass.\n\nIn this part, the authors give a strict derivation process of Bayesian LSTM. I\u2019ll show you the key stages, and if you are interested in more details, it can be found in the paper.\n\nDropout method is leveraged to perform Bayesian inference with LSTMs. Given the observed labels Y and data X as in Equation 3.\n\nThe authors in this paper use variational inference to make an approximation of posterior distribution because it cannot be tracked in a general way. They minimize the reverse Kullback Leibler (KL) divergence between this approximate distribution and the full posterior to learn about the network\u2019s weights. In Equation 4, q(\u03c9) is a distribution on those matrices whose columns are randomly set to zero.\n\nFor LSTM, each matrix Wl has dimensions Kl\u22121 \u00d7 Kl. q(\u03c9) can be defined as:\n\nThe authors re-write the operation (omitting biases for brevity) by LSTM\u2019s definitions in Function 1:\n\nThe output can be defined as fy(hT ) = hT Wy + by. To emphasize the dependence on \u03c9, the functions are written as fh\u03c9 and fy\u03c9 . To approximate the posterior distribution q(\u03c9), the authors give the function:\n\nThis can be approximated with MC integration:\n\nThen the minimization objective can become as\n\nFor each layer l every weight matrix column wlk the approximating distribution is\n\nThe KL term in Equation 7 can be approximated as\n\nPredictions can be approximated by the standard forward pass for LSTMs like propagating the mean of each layer to the next (standard dropout approximations), or approximating the posterior with q(\u03c9) for a new input x\u2217 as shown in the following function:\n\nWith the use of naive dropout, the minimization objective can be Function 9:\n\nThe difference between variational and naive dropout approaches can be seen in Figure 1. Here the distributions are the hidden outputs in Equation 6, and it is plotted over 150 epochs for a model trained on MNIST dataset that I will describe later. It shows the percentiles of the hidden layer outputs, which are overall time steps for the same arbitrary input sample for each epoch. Table 1 shows that both approaches have similar performance. But according to the authors\u2019 hypothesis, the naive approach has a narrow distribution on the first layer, and in the second layer the variational approach has a concurrent narrow band of exploration. It was found that in the training simulations for the naive approach, the distributions would vary between different training simulations. But for the variational approach, it is the same for any training simulation.\n\nTo demonstrate the efficacy of Bayesian LSTMs, the authors use 5 datasets. For each dataset, they use the same LSTM model with a different architecture. A validation set was used, and Dropout was used on only the input and output LSTM connections. Optimization was performed with Adam [8], and the learning rate is 0.01, the minibatch size is 256. Here the authors provide the way how they configure the parameters for each dataset.\n\n1). MNIST\n\nit was processed in scanline order [9], There are 2 hidden layers with 128 units in each layer.\n\n2). MIT-BIH arrhythmia dataset\n\nIt contains 48 half-hour excerpts of electrocardiogram (ECG) recordings from 47 patients. Its percentage of train:test:validation is 50:40:10. The model is a single hideen layer of 128 units and the dropout probability is 0.3.\n\n3) Physionet/Compute in cardiology challenge 2016\n\nThere is 4,430 phonocardiogram (PCG) recordings in this dataset, and 3,126 were used for training. 301 validation samples were extracted from the training dataset. The data were provided with normal and abnormal heartbeats. The model has 2 hidden layers of 128 units, and the dropout rate is 0.25. The model will return a score that was evaluated by means of online submission.\n\n4) Neonatal intensive care unit dataset\n\nIt contains the first 48 hours of vital signs for 3 neonatal intensive care unit (NICU) patients. The signals contained ECG, blood pressure and oxygen saturation. The model has a single hidden layer of 64 units and the dropout rate is 0.1. The random split of 50:40:10 was used.\n\n5) Traumatic brain injury dataset\n\nThe data was from traumatic brain injury (TBI) patients. The dataset contains 19 variables recorded for 101 patients, of which 34 were females, and the age ranged from 15 to 76. The model has a single hidden layer of 128 units and the dropout rate is 0.4. A random split of 50:40:10 was used.\n\nTable 1 shows the results from the datasets analyzed. The values are the average rate for 10 times running. We can see in the table that Bayesian LSTM used for classification of medical time series provides an improvement.\n\nIn Figure 2, the authors juxtapose confident and uncertain Bayesian LSTM classified medical signals. It should be mentioned that only estimated class is produced as output in standard LSTMs. The figure shows that when the signals look noisy or abnormal, the model is uncertain. When this condition occurs, the practitioners should do further investigation.\n\nHere the authors give us mainly three points:\n\nIt shows that the conventional deep learning technique for time series have the following two advantages: (i) It helps perform the quantifying model decisions by providing a vital additional output. (ii) It can improve the model accuracy. Additionally, in medical machine learning, the methods for quantifying aleatoric uncertainty can also give valuable benefits.\n\n[1] Lipton, Zachary C., et al. \u201cLearning to diagnose with LSTM recurrent neural networks.\u201d arXiv preprint arXiv:1511.03677 (2015).\n\n[2] Choi, Edward, et al. \u201cDoctor ai: Predicting clinical events via recurrent neural networks.\u201d Machine Learning for Healthcare Conference. 2016.\n\n[3] Jagannatha, Abhyuday N., and Hong Yu. \u201cBidirectional rnn for medical event detection in electronic health records.\u201d Proceedings of the conference. Association for Computational Linguistics. North American Chapter. Meeting. Vol. 2016. NIH Public Access, 2016.\n\n[4] Harutyunyan, Hrayr, et al. \u201cMultitask Learning and Benchmarking with Clinical Time Series Data.\u201d arXiv preprint arXiv:1703.07771 (2017).\n\n[5] Hochreiter, Sepp, and J\u00fcrgen Schmidhuber. \u201cLong short-term memory.\u201d Neural computation9.8 (1997): 1735\u20131780.\n\n[6] Abadi, Mart\u00edn, et al. \u201cTensorflow: Large-scale machine learning on heterogeneous distributed systems.\u201d arXiv preprint arXiv:1603.04467 (2016).\n\n[7] Gal, Yarin. Uncertainty in deep learning. Diss. PhD thesis, University of Cambridge, 2016.\n\n[8] Kingma, Diederik, and Jimmy Ba. \u201cAdam: A method for stochastic optimization.\u201d arXiv preprint arXiv:1412.6980 (2014).\n\n[9] Cooijmans, Tim, et al. \u201cRecurrent batch normalization.\u201d arXiv preprint arXiv:1603.09025 (2016).\n\n[10] Neil, Daniel, Michael Pfeiffer, and Shih-Chii Liu. \u201cPhased LSTM: Accelerating recurrent network training for long or event-based sequences.\u201d Advances in Neural Information Processing Systems. 2016."
    },
    {
        "url": "https://medium.com/syncedreview/vector-adds-brains-d5cdf43416ed",
        "title": "Vector Adds Brains \u2013 SyncedReview \u2013",
        "text": "Toronto\u2019s Vector Institute announced today that it is adding 10 new faculty members, effectively doubling its size.\n\n\u201cVector is a pillar of the Canadian AI ecosystem and I\u2019m very excited that the team is expanding with highly sought-after talent, some of whom I\u2019ve had the pleasure of working with,\u201d said the Vector\u2019s Chief Scientific Advisor Geoffrey Hinton. \u201cIncreasingly, the world\u2019s most promising researchers in deep learning and other AI subfields are looking at Canada as a hub with many opportunities to collaborate, advance research and develop applications. This team will drive Vector\u2019s excellence in research, education and industry collaboration.\u201d\n\nMIT postdoctoral researcher and Facebook PhD Fellowship recipient Jimmy Ba; 2017 AMIA Summit on Clinical Research Informatics (CRI) Best Student Paper nominee Marzyeh Ghassemi; and Professor Pascal Poulart of the David R. Cheriton School of Computer Science at the University of Waterloo are among those joining the Vector. The newcomers\u2019 start dates range from autumn 2017 to August 2018.\n\nThe University of Toronto affiliated Vector Institute opened last spring with funding from the Canadian government\u2019s CDN$125 million Pan-Canadian Al Strategy, the Ontario provincial government, and the private sector.\n\n\u201cArtificial Intelligence is an essential building block in today\u2019s global economy \u2014 our government is ready to support Canada\u2019s leadership role in this area,\u201d said Canadian Minister of Innovation, Science and Economic Development Navdeep Bains. \u201cThe Vector Institute\u2019s top-quality work is exactly what we need to continue growing Canada\u2019s global reputation as a cutting-edge leader in AI research. Today\u2019s announcement is no exception to the remarkable progress the Vector Institute continues to make.\u201d\n\nOver the last 20 years, Canada has contributed invaluable research achievements to the AI boom. Toronto and Montreal have played key roles in the rise of deep learning, and produced thousands of AI talents. The Vector Institute bridges academia, industry and institutions in the transformative fields of machine learning, deep learning and AI while providing researchers with opportunities to work with existing data sets to solve real-world challenges."
    },
    {
        "url": "https://medium.com/syncedreview/ai-in-news-reporting-machines-are-now-writing-dialogues-q-as-and-news-articles-6e9e0da30a61",
        "title": "AI in News Reporting: Machines are Now Writing Dialogues, Q&As, and News Articles",
        "text": "Good afternoon everyone. It is my honour to be here and talk to professionals and experts in various fields about what artificial intelligence can do for natural language understanding. I am going to explain how to apply machine learning to natural language understanding, chatting with human users, and producing machine-written news articles. In order to complete these tasks, we need to know what machine learning tools and basic algorithm models are needed for. After that, I will talk about how we realized the technology. At the end, we will discuss artificial intelligence in general, including the current artificial intelligence technologies, the challenges we facec, and some of my thoughts.\n\nAt the beginning of 2016, Google DeepMind uses their program named AlphaGO to show that, machine learning has already reached or surpassed human beings in performing certain tasks. How did AlphaGO learn to play Go? Its algorithms consist of two parts: one from deep learning, and one from reinforcement learning or Monte Carlo tree search. I wil focus on the part relating to deep learning. Based on the achievements and development of neural networks and deep learning in the past twenty or thirty years, we know that deep learning is very good at solving problems like playing Go. This kind of problems is called supervised learning. What is supervised learning? It is like give data X, and hope you can use it to do some prediction, called Y. You need to use machine learning methods to find the mapping function that maps X to Y.\n\nLet\u2019s take an image to be the input, then our output is the label of this image, which will classify the content of this image. Is it a cat or a dog? This is an image classification problem. If input is an audio recording of a Chinese sentence, the output should be the audio of the corresponding English sentence. The process of converting Chinese to English is also machine translation, which is under supervised learning. The third example is that when we input an image, and then the program produces a text passage to describe it. When we were little, we all learned how to describe the pictures we saw. In theory, machines can do the same thing. We can model the scenario to be a supervised problem. The fourth example is that, we input an audio recording, and the output is the text corresponding to the audio. This is called speech recognition, which is also a supervised learning problem. We can also think about this problem from the opposite approach. Let our input be a passage of text, and make the output a corresponding audio. This is called speech synthesis, under supervised learning. As long as there are enough data and appropriate models, deep learning can do a great job in solving these kind of problems.\n\nYet, how did deep learning do this? The way how deep learning or artificial neural networks work is inspired by human brains. Human brains have huge amount of neurons. Each neuron can only do very simple tasks. However, they can do complicated things as a whole once they are connected.\n\nInspired by the neurons in human brain, the pioneers in artificial intelligence created artificial neurons. Artificial neurons also have some inputs. The input is processed by some nonlinear functions, and then output the result. These artificial neurons can accomplish complicated tasks if they are connected in certain ways.\n\nFor instance, if the input is an image, and the task is to identify the number from the image. Here is a single hidden layer of a neural network. You can increase the amount of the hidden layers to improve the recognition ability of this network.\n\nToutiao is a news platform that provides users the latest news on technology. The platform has three crucial production procedures: 1) how to produce high-quality content, 2) how to send these content to users that are interested, and 3) how to encourage users to make comments after they are done reading an article or watching a video. The core technology behind these three tasks all require the input of artificial intelligence.\n\nToday, I will talk about two major parts out of the three, including content production and content discussion \u2014 how we build machines to discuss content with users and how robots produce articles automatically. The major problem we have is language, which is quite different from the image problem I mentioned previously. The input of an image has a fixed size, while language does not. One sentence can be long or short. So here is the first problem: how to handle longer inputs? The deep learning models we build can handle longer input. Our initial idea is to increase memory units. In this model, there are some units for recording history information, which can remember information for a longer period of time, and make predictions.\n\nFor instance, here is a very simple recurrent neural network. Its input is X1, X2, X3 and X4, and each input is a vector. Its output, h, is what makes recurrent neural network different from the traditional convolutional neural network. Each h corresponds to not only the input of the current position, but also that of the previous position. In this way, all history information will be connected via a simple form. There is another form called Gated Recurrent Unit, which is a bit more complicated. Similar to adding switch to a human brain learning, this switch allows the system to memorize and forget information selectively. For instance, one of the switch is called Reset Gate, allowing the system to forget information selectively. There is another switch for controlling output, helping the system to control what information is from previous section at what position, and what information need to be saved for the next position.\n\nVia the neural networks with memory units, chatbots are capable to communicate with users automatically. For instance, here is a short passage of one of the conversations our chatbot had.\n\nOur chatbot can not only chat with users, but also determine the content\u2019s sentiment. Even if the input is very long, the system can still give a relatively correct response. Here is an excerpt of some famous movie lines, and the machine does give out a positive response.\n\nHow does the system generate response for the conversation? Here is a simple demonstration: starts with a recurrent neural network that has an initial state, which is the yellow rectangle in the picture. The initial state will take in the hidden information in the current state, predict what text should be output from the state, and then uses that new output information to generate the next state. After we have the predicted text, we use this information as the needed information for the input of the next state. This way, the system will keep generate the second, third, fourth words, until it reaches the end of the sentence.\n\nThat is a scenario that we do not need to worry about context. What if we have contexts and the system needs to respond to contexts? We can use recurrent neural network to build a model based on the previous sentence \u2014 the context. Each word in the previous sentence is treated as a vector. By putting all of the vectors together and processing them as a whole, the entire sentence will become a vector, which will be the input of the initial state in the recurrent neural network of the response. Then, the system will use the same method I mentioned above to generate words to form a complete response.\n\nOur robots can not only generate conversations, but also add sentiments to them. How does emotions come into play? We add extra excitation into the model, and this excitation can be an emotion excitation. If we want the conversation to be happy, angry or sad, we can add corresponding emotion excitation into the system. After that, the system will generate content with that specific emotion tone. However, what we have now is only a chatbot, it cannot answer questions that require more knowledge background.\n\nThe second thing I want to talk about is how to build a model, which enables the robot to answer deeper questions. In order to present knowledge in a way the computer can understand, we need a structured method. Use David Beckham as an example. We can format all the knowledge related to him in a graph. Each dot in the graph is an entity. There are some edges between entities and dots, corresponding to the relationship between entities. For example, Beckham being born in Leytonstone is a piece of knowledge about Beckham. We can put this information into a triad: the main entity, the guest entity, and the relationship between them.\n\nHow does the machine answer questions automatically? We need to find the corresponding knowledge triad in the knowledge database to answer questions like \u201cWhere was Beckham born?\u201d The triad will be <DavidBeckham, PlaceOfBirth, Leytonstone>, and the answer will be \u201cLeytonstone.\u201d\n\nWhy is it difficult for the computer to answer questions like this? The first challenge is that our language is very complicated. We can ask the same question in various ways. For instance, \u201cWhere was President Obama born\u201d is the same as \u201cWhat is President Obama\u2019s place of birth?\u201d This is the variety and complexity of our language. The second problem is ambiguity. Many entities in our database have the same or similar names. For example, if we ask \u201cWho is Michael Jordan?\u201d, some people might think of the famous basketball player Michael Jordan. But for the professionals in machine learning, there is also an expert called Michael Jordan in the field. The ambiguity caused by pronouns can be the second challenge. The third difficulty is from the data sparseness. We do have huge amount of data. After we filter the data by the needed triad, there are still 22 million knowledge triads. These data are from Google FreeBase. Overall, we have 0.1 million marked question-and-answer pairs. However, it is very difficult to use these 0.1 million marked data to answer questions based on the 22 million triads.\n\nRecently we built a CFO system, which is a deep learning system that can answer more complicated questions, such as \u201cWhere did Harry Potter go to school?\u201d We all know the answer is Hogwarts. However, beside Hogwarts, Harry also went to an elementary school, of which a lot of people did not know. Our system is able to come out with both answers.\n\nWe did an evaluation of the system using Facebook\u2019s open data sets. The result showed that our accuracy has reached 75.7%, which is 10% higher than the accuracy achieved by Facebook\u2019s 62.9%. How did we complete the task with such high accuracy? And how did we build this kind of chatbot?\n\nWe can start with a question, \u201cWho created the character Harry Potter?\u201d First, we need to determine what the key entities are in this question. \u201cHarry Potter\u201d is one entity. Second, we need to determine what relationship this question is looking for, which is \u201cCharacter_Created_By\u201d in this case. These two pieces of information can be used to find the corresponding answer to this question from the database.\n\nWe use sequence labeling, a deep learning model, to give all potential entities scores. By this scoring method, \u201cDavid Beckham\u201d is found to be the entity with highest probability for the question relate to Beckham. We also have another alternative model, which has a two-way recurrent neural networks. We use the input question to build a model through layers of two-way recurrent neural network, generating a vector as result. The model will then use this vector to predict which entity in the database corresponds to the given question, and what relationship the question is asking. At the end, the model will find \u201cHarry Potter\u201d is the entity the question asks. After finding the right entity, the model also find the correct answer.\n\nMy last topic is on the robot that can produce news automatically, named XiaomingBot. In 2008, before the Beijing Olympics, a newsbot was invented to write news articles. The robot wrote 450 news articles on topics like ping pong, badminton, soccer, and tennis. In 16 days, it gained about one million viewerships. Later, some research show that the views of a sport news article written by a professional reporter is about the same as those written by XiaomingBot; sometimes XiaomingBot might have even gotten higher views. Xiaomingbot generates not only short news messages, but also long articles. For example, the news about women\u2019s soccer is longer, with more detailed reporting on the game. Different from traditional newsbot, Xiaomingbot has two different features. The first one is that the robot is very fast \u2014 XiaomingBot will be able to generate and publish the corresponding news articles to readers in two seconds after the game finishes. The entire process is very brief since it is all taken care by the machine. This is also a feature of TouTiao. The second feature is that we produce short and long news content. Furthermore, XiaomingBot can add pictures about the game into the article. The generated content about the game will match the game timeline, especially for soccer games. The entire news generation process combines syntactic generation techniques with machine learning, which makes our news articles more professional.\n\nNow, we have have chatbots and new robots. The question is, will machines take over everything? Definitely not. But what are the defects of machines, and what can\u2019t they do? It is very easy for users to find that what they said will confuse the bot when they communicate with it. Even though our chatbot can answer technical questions with 75.7% accuracy, it still cannot handle more general questions like asking the mechanism behind the topic, procedure about completing a task, or explain a concept in detail. When you ask the bot \u201cwhat is the meaning of life,\u201d it will not know how to respond. We have good newsbot for generating sport news, however, if you want to change it into a more general text-generation robot, then we are not ready for that yet.\n\nWhy do machines have all of these limitations? At the very beginning of this talk, we mentioned that deep learning or machine learning are good ways to solve supervised learning problems. However, its effectiveness in solving supervised problems is also the limitation, due to the high demand of large amount of marked data. The price of marking these data can be really high sometimes."
    },
    {
        "url": "https://medium.com/syncedreview/baidu-leads-chinas-construction-of-the-upcoming-national-engineering-laboratory-of-deep-learning-999c763e1207",
        "title": "Baidu leads China\u2019s Construction of the Upcoming National Engineering Laboratory of Deep Learning\u2026",
        "text": "Recently, China\u2019s National Development and Reform Committee formally approved Baidu to lead the construction of national deep learning technology and application engineering laboratories, and further the development of deep learning technology and application alongside other researchers.\n\nChina needs to provide basic support to the development of AI technologies. Led by Baidu, this laboratory will focus on developing seven major areas, including deep learning, computer vision, computer auditory, biometric identification, human-computer interaction, standardization service, and deep learning intellectual property. It is determined to be an institution that is \u201cfirst in China and first-class in the world\u201d, making research breakthroughs, working with industries, transferring technologies, and cultivate new talents in order to enhance the overall competitiveness of China\u2019s artificial intelligence development.\n\nThe lab will be directed by Baidu deep learning laboratory director Yuan Ping Lin and chief scientist Wei Xu, Tsinghua University academician Bo Zhang, and Beijing University of Aeronautics and Astronaut academician Li Wei. As the lead, Baidu will work with Tsinghua University, Beijing University of Aeronautics and Astronautics, China Information and Communication Research Institute, China Electronic Technology Institute, and other institutions to develop deep learning technology and applications.\n\nAt the same time, Baidu will open-source to the laboratory its three world leading resources \u2014 computational resources, algorithm resources and big data resources, as well as the industry\u2019s most cutting-edge deep learning research focuses, which will attract top talents to participate in research, and use the laboratory as a training site for more researchers and postdoctoral candidates.\n\nBaidu\u2019s President and Chief Operating Officer Lu Qi issued a congratulatory note for the approval of laboratory:\n\nAs the leader of Artificial Intelligence in China, Baidu invested heavily in technology in recent years. The company\u2019s Institution of Deep Learning (IDL) was established in 2013, followed by the Big Data Laboratory (BDL) and Silicon Valley Artificial Intelligence Laboratory (SVAIL), (https://en.wikipedia.org/wiki/Augmented_reality) Augmented reality Laboratory (ARL), etc. Also, Baidu released a leading depth Learning platform (*Paddle Paddle*), and built a comprehensive artificial intelligence R&D system. In the field of voice, image recognition and unmanned technologies, Baidu\u2019s applications have reached the global leading level.\n\nNowadays, the day request volume of Baidu brain speech synthesis is 250 million, and speech recognition rate is up to 97%, especially the face recognition accuracy rate is as high as 99.7%. In 2016, Baidu voice technology is named \u201c2016 global breakthrough in the top ten technology\u201d by MIT Technology Review. In addition, Baidu has been selected as the world\u2019s second intelligent company.\n\nWhat\u2019s more, with Baidu\u2019s artificial intelligence, facial recognition technology has been applied to the face gates, financial wind control and other fields. In the 2016 World Internet Conference, Wuzhen attractions took a lead to apply Baidu\u2019s face recognition system, which increase the checking efficiency shapely. Other applications such as voice recognition and image recognition are also applied for indemnification, which effectively reduce the dim light, noise and other environmental factors on the accuracy of recognition, significantly enhance the recognition rate and prevent fraud.\n\nTotally 19 national engineering laboratories got approved this time. Baidu not only takes the lead in the construction of deep learning technology and application, but also participates in big data systems and brain intelligence technology building.\n\nAll in all, the development of artificial intelligence can significantly enhance the competitiveness of the country. Not surprisingly, deep learning technology has become one of the most important battle of science and technology competition."
    },
    {
        "url": "https://medium.com/syncedreview/an-exclusive-interview-with-maluuba-the-worlds-fastest-growing-ai-startup-59532c1f1fd4",
        "title": "An Exclusive Interview with Maluuba: The World\u2019s Fastest Growing AI Startup",
        "text": "Mohamed Musbah: We are very interested in industry application. Especially with customer service, Because it involves a lot of interaction with the real scenarios, and we need to take user experience into consideration. In a real-world scenario, when users report an error, they want their problems to be solved as quickly as possible. The question now is how we can automate and stabilize the machine and address the user\u2019s problems. Our latest paper is onmachine comprehension, in which we provide data to an artificial intelligence model to test whether the model understands the semantics. Research is still in progress, and more details will be publish in a few months. We are the most interested in conversational agent and dialogue systems.\n\nMohamed Musbah\uff1aWe are also very interested in products, and our Research Lab has done a lot of works on it, however it requires a lot of work to finish viable products. For the industry, this is a relatively new concept. But we\u2019re also working on other applications, such asconversational systems \u2014 and committed to develop systems that allow us to talk to our users. We are also expanding the existing API by increasing its functionality, which is the future direction of our efforts, and ultimately we want to achieve is that I mentioned before ,Machine understanding. For example, think about reading the car manuals. It often has 200 pages, and no one finishes reading it. We would like to put the contents into neural networks, which will be able to help you understand it. This involves natural language processing and is the key to Machine Understanding. In other words, we provide the user interface, users provide the content, and ultimately we will provide the analysis results. That is the API we are working on. In addition, we provide offline models to address situations when the network is unavailable.\n\nMohamed Musbah\uff1aI can\u2019t speak on behalf of a certain company for what their goals are, but I can definitely talk about the general trend of the industry and how we\u2019re still different. One thing we\u2019ve seen is that the studies of machine learning, even NPL, are applied to very narrowed fields.\n\nBut we don\u2019t think the future will be limited this way. If we see AI research moving into a certain area, it cannot be applied to anywhere else! They don\u2019t have the capacity to do one subject and apply that knowledge to another subject. What we\u2019re trying to do is take a different approach here: we don\u2019t teach the machine to do work in a narrow domain, instead, we teach them to understand people. The most important step is to endow machines with cognition, which is what people have. For example, when I read an article, I can understand it regardless of it being a fiction, magazine, or medical journals.\n\nIf we want machines mimic humans, we need to give them reasoning capabilities. The EpiReader article is a part of the new research into NPL, and we used CBT and CNN datasets to test its accuracy. Can we teach them to understand? Can we teach the machine to read a medical journal or Harry Potter book like a ten-year old? It doesn\u2019t have to delve into any specialty \u2014 \u2014 a ten year old can\u2019t understand medical jargons, but it can understand the basics.\n\nMohamed Musbah\uff1aOf course. As a company, we cannot get involved in every vertical field, but we can build a very generalized system to suit different needs. Everything will rely on the foundation of understanding human language, and set further challenges to teach the machine reasoning, memory, understanding, and the ability to have a conversation.\n\nMohamed Musbah\uff1aDuring 2010\u20132011, several of our computer science students set up the artificial intelligence lab at the University of Waterloo. Our initial goal was simple: create machines that communicate with humans. It was a crazy idea at that time, given that now artificial intelligence and natural language processing have developed a lot in the past five years. Now, we are glad to be able to work with many companies, and support them using our products and technology.\n\nMohamed Musbah: I don\u2019t have a specific number. One of our goals is to solve the fundamentals of language processing, and I need a good team for that. Fortunately, our Montreal research centre is working on this, and making a lot of progress.\n\nWe also want to build the deep thinking natural language lab in the world. This is a very special field. We want to meet more talents. We are proud of the team we have now and are looking forward to expand it. I think we are on the right track.\n\nMohamed Musbah\uff1aWe are very optimistic about the future of artificial intelligence. In the past few decades, artificial intelligence has been mainly used to perform operational tasks. The more intelligent the machine will be, the more reliable it . Self-driving cars are a good example. We used to think that it is very remote technology, and we are almost there now.\n\nAside from this, robotics are advancing at an exciting pace. Robots can execute many tasks, and liberate us from mundane tasks. For example, in order to drink a cup of coffee, we need to operate the complex coffee machine. But what if we make the coffee machine intelligent? It will learn you preferences, help you prepare, and simplify everything.\n\nMohamed Musbah\uff1aI am very glad to have these excellent consultants, Dr. Bengio is very famous in the field of artificial intelligence. We have all reached the consensus that natural language processing (NLP) is a critical domain. Artificial intelligence can drive a car now, but in understanding human language and semantic reasoning, it is still very limited. Dr. Bengio and I have been discussing a lot about how we can advance research in this field. Dr. Bengio is now a member of our team. Our research efforts are very alike.\n\nMohamed Musbah\uff1aWe have done a lot of research over the last 8 to 10 months, and we are fortunate that, with EpiReader as an example, we were able to beat the state-of-the-art results from companies like Google, Facebook and IBM. Now this is a really nascent area, which means that not every path you take will produce results. We are fortunate in terms of where we are and we have a lot of things in the pipeline right now that we are gonna announce in the next 6 months, yet this is still a completely uncharted territory both for Maluuba and for everybody else in this space. That means we are gonna solve challenges that we\u2019ve never seen before. Technically speaking, the idea of teaching machines to reason was crazy in 2010, was crazy in 2012, it is still crazy in 2016, but it is not as crazy. To get to that point is gonna be incredibly difficult, but even we got 20 or 30 percent the way there, we\u2019ve done a lot. And that is what we are starting to do. It is very open-ended. It is still really really early, but we are very excited with the progress we\u2019ve made so far and we believe there is still a lot more to be done.\n\nMohamed Musbah\uff1aThat is a good question. It\u2019s challenging for any startup to compete with resourceful, large organizations like Google, or whoever may call any of our researchers and say \u201ci\u2019d like to offer you one million dollars to leave Maluuba to come to our team\u201d(laugh). That\u2019s the reality and something we have to deal with. The first answer we have is: passion. When we bring in the team we explain graphically what we gonna do and how we\u2019re gonna try solve these problems. If you look at the larger companies, from the research point of it, it\u2019s obvious that we\u2019re in different areas. It\u2019s not just the difference between academic and industry. You can leverage a lot of expertise and bring leverage into the lab.\n\nThe other aspect is the team, and our combined vision: every single researcher plays a critical role in reaching it, and what we can offer them is to join the company. A lot of that comes down to a fundamental level of what the researcher wants to do: a researcher that wants to be paid a million dollars, handle a crazy amount of work may not necessarily fit in with Malluba. If the researcher wants to solve fundamental problems, they are aligned with our objectives. We\u2019re going to prove to them that it\u2019s worth it, and we think we can manage this in a startup company, then we\u2019re gonna keep growing.\n\nMohamed Musbah\uff1aWe certainly want to do that. The world is a globe. We\u2019re having this conversation when it\u2019s 10 in the morning for me and 9 at night for you, and I appreciate you taking the time to have conversation with me. From a research point of view, we would also love to do that. This is how it should work. Canada is where we are today, but that doesn\u2019t mean we\u2019re not traveling all over the world to attend conferences, or that we\u2019re not meeting smart people, or looking forward to collaborate with them. What we see happening in China is incredible. Maluuba is international and we\u2019ve been working with global companies. Even in China there are incredible companies doing amazing work that are strong in their areas. For Maluuba, we haven\u2019t explored the option to expand labs, but it is a goal.\n\nMohamed Musbah: I would like to say that at this point, it\u2019s exciting because there are a lot of exciting things to do. We are at the forefront of addressing these issues. The industry is very happy to support entrepreneurs to solve these problems, whether it is financial or strategic capacity. However, I would like to remind you some things. First, make sure you can distinguish between fact and fiction. A lot of information on artificial intelligence are too exaggerated, either due to the lack of basic understanding, or being too excited. Distinguishing fact from fiction can help you really understand current situations, to help you identify the problems accurately. Second, try to solve the issues no ones have solved before. In the few years later, I truly believe that the industry will filter people making brand new products and solving novel problems from the others."
    },
    {
        "url": "https://medium.com/syncedreview/google-brains-lukasz-kaiser-how-deep-learning-quietly-revolutionized-nlp-84723b6109c4",
        "title": "Google Brain\u2019s Lukasz Kaiser: How Deep Learning Quietly revolutionized NLP",
        "text": "On January 11th 2017, more than 20 world-class AI experts from both industry and academia came together in Santa Clara to participate in this year\u2019s AI Frontiers Conference.\n\nThis year\u2019s conference speakers include: Jeff Dean-Head of Google Brain, Li Deng-Chief AI Scientist at Microsoft, Adam Coates-Director of Baidu\u2019s AI Lab, and Alex Smola-Director of Machine Learning at Amazon. The speakers shared cutting-edge developments on Artificial Intelligence to over 1500 attendees.\n\nNLP or Natural Language Processing has changed tremendously in the recent years thanks to the developments of deep learning. NLP denotes a very broad term, encompassing speech, text, and the interaction between the two. For the purpose of his presentation, Lukasz talked specifically about text-to-text tasks, namely parsing, translation, language modeling, and etc.\n\nThese tasks seem easy (almost trivial) to anyone with a decent amount of education, but can a neural network achieve results similar to that of an average human? Most were skeptical until it actually happened, and when it did, everyone wanted to know how. How can a neural network understand a sentence? How can a neural network handle the complex tasks of language processing? Lukasz gave several explanations in the following slide:\n\n\u201cWhen neural networks first came out, it\u2019s built for image recognition to process inputs with the same dimension of pixels. Sentence are not the same as images.\u201d Lukasz said.\n\nThe amount of words a sentence contains can and will vary significantly, meaning that the dimension of the input is completely irregular. In order for the neural network to accommodate this, RNN (recurrent neural network) is a natural choice. The next step is to directly train the network, because if too many steps were built, the calculation burden would be too significant. In later developments, the idea of LSTM (long-short term memory) became viable solution to this problem.\n\nLSTM gave us the ability to train RNN. But in 1997, the developments of LSTM were plagued with issues: the sizes of LSTMs were too small and lacked proper hardware. The lack in supporting technology meant LSTM was only a theoretical breakthrough that people could not apply.\n\nIt was only in 2014 that LSTM became a viable application. Thanks to encoder-decoder architectures, now one can build not just a single layer, but a lot of layers of LSTM. After building the layers up, the larger model produced a much better result.\n\nLukasz gave an example on parsing. For humans, we have learned in school that in order to read a sentence, one needs to recognize verbs and nouns before looking at the grammar. This process is shown in the parsing tree below, and it is the old standard way to build a NLP model: first enter definitions, grammar, and sentence structure, then let the neural network train in order to understand and generate sentences.\n\nBut Lukasz\u2019s research team had a different approach: just write the tree as a sentence of a sequence of the simplest way they can imagine using brackets and symbols.\n\nUsing this approach, the network is trained only by writing the sequence, without knowing anything about grammar trees and brackets, or having any background knowledge. However, the problem with this approach is the shortage of data, since all the data (sequence) were written by researchers. Compared to the old training method using grammar or sentence structure, the new method seems weaker in providing background knowledge. However, in practice, the new method works much better, because the network can learn all these knowledge by itself.\n\nLSTM can also be applied to language models. The performance of language models is measured by perplexity: the lower the perplexity, the better the performance. Compared with past models, perplexity measurements are dropping rapidly, representing significant improvements. The best score achieved so far is 28 in 2016, compared to 67.6 in 2013. Results with such quality was once considered impossible. The decisive factor is model size.\n\nLukasz also gave some examples of applications of LSTMs in language modelling and sentence compression.\n\nThe most impressive improvement brought by LSTM is in the area of translation. Lukasz made a comparison: at school, we learn foreign language word by word. But what if we just learn through listening to people talk using that language? This is actually how children learn. As it turns out, this is how neural network learns as well. In this case, the size and number of training data are key factors.\n\nIn the graph below, translation performance is measured in BLEU scores, with higher score representing better performance. In the last two years, we have seen an improvement from 20.7 to 26.0. According to Lukasz, model size and tuning seem to play the decisive roles here as well.\n\nAbout two years ago, neural networks were still trained to match the level of \u201chand-made systems\u201d \u2014 a phrase system that translated phrase by phrase. By comparing the results ofPBMT ( old standard translation model) and GNMT( new model involving LSTM) translating a German sentence, we can see that the new model\u2019s result is obviously clearer and more understandable.\n\nThis result showed that the translation process doesn\u2019t require much hand engineering, just a big network and a lot of training. \u201cThis theory also holds true for many NLP tasks.\u201d said Lukasz.\n\nBut how good is it exactly? Can we quantify it?\n\nWith the recently launched neural network version of Google Translate, people were asked to evaluate translation results on a scale from 0 to 6 (0 means nonsense and 6 means perfect translation). Also to compare the results between old and new systems, Lukasz\u2019s team asked human translators (people that speak the language but are not professional linguists) to join the match, and added their results to the evaluation as well. The slide below shows the resulting scores for all three translation systems.\n\nBased on the results, we can see that the new system made huge improvements, and in some cases (English to Spanish) were almost as good as human translator. Obviously, studying larger databases helped to give better results.\n\nhere are still problems to be solved with sequence-to-sequence LSTM. Lukasz listed two of them:\n\nThese are large models, and due to their dependence on database size, a significant amount of calculations are involved. Processing speed becomes a big problem. In order to shorten process time, TPU is an important hardware that helps researchers serve these translations.\n\nAlso, since the translation process is very sequential, even if calculations are done quickly, it is still generating the result word by word. Process time can be slow even for a small sized task. To solve this problem, new and more parallel models (Neural GPU, ByteNet) are expected to help.\n\nSequence to sequence LSTMs require a lot of data. To solve this issue, attention and other new architectures that increase data efficiency is suggested. Another approach could be the use of regularizers such as dropout, confidence penalty and layer normalization.\n\nAlso, since translation process is very sequential, even if calculations are done quickly, it is still generating results word by word. Process time can be slow even for a small-size task. To solve this problem, new and more parallel models (Neural GPU, ByteNet) are expected to help."
    },
    {
        "url": "https://medium.com/syncedreview/top-10-ai-news-of-2016-ec9e43b91a2a",
        "title": "Top 10 AI News of 2016 \u2013 SyncedReview \u2013",
        "text": "Many companies are trying to out-compete Google in self-driving cars. The list includes big tech companies such as Baidu, Uber, and IBM, along with traditional car manufacturers like General Motors, Honda, and Tesla Motors. Independent research projects from prominent academic universities like MIT and Harvard are looking into the social impact, ethics and law of autonomous vehicles. By the beginning of 2016, the first self-driving bus hit the roads of Wageningen, Netherlands. The vehicle, named WePod, was designed by French car manufacturer EasyMile in conjunction with Citymobil2 , a pilot platform for automated road transport systems. WePod takes up to 6 passengers, yet can only travel 200m far at 8 miles/hr. nuTonomy , the world\u2019s first driverless taxi, had a road test in Singapore in August 2016. People interested in taking a ride could use smartphones to book seats. The nuTonomy project was initiated by researchers from MIT. While companies like Google and Volvo had privately tested their autonomous vehicles, nuTonomy became the first company to conduct a road test in public. Google made significant progress by the end of 2017, and separated its autonomous driving project into an independent company named Waymo. The rapid development of autonomous vehicles also caused issues. For example, Tesla\u2019s self-driving feature \u201cautopilot\u201d had caused several incidents in the past year, and one accident was lethal. Consequently, Tesla warned drivers not to rely completely on the autopilot function. Nevertheless, self-driving car technology as a whole improved greatly in 2016. Based on currently planning in China and the US, and provided that we manage to resolve the current safety hazards, we may be able to see these autonomous vehicles commonly on the road in less than ten years.\n\nIn March 2016, machines conquered the last besieged fortress of board games: Go. Google DeepMind developed a computer program named AlphaGo, which beat Lee Sedol, the international Go champion in Seoul, Korea with a score of 4\u20131. This attracted public attention on a great scale and raised discussions on the progress of AI. After the match, the Korea Baduk Association awarded AlphaGo with the highest Go master rank: \u201chonorary 9 dan\u201d. AlphaGo unseated the former human champion to become the world\u2019s number 1 ranked Go professional on July 27th. AlphaGo\u2019s development team is a British AI company named DeepMind. The company was acquired by Google in 2014, and soon became the most important company in the field. Under the leadership of genius researcher Demis Hassabis, DeepMind started to expand and attract talents from all over the world. DeepMind has made a number of important research breakthroughs, including winning Atari games, speech synthesis, lip reading, subway route planning, and building neural networks. In addition to research, DeepMind has also been making forays into product design. In July, Google announced that DeepMind\u2019s Deep Learning capabilities could potentially save its servers millions of dollars worth of electricity costs in the future. DeepMind\u2019s ongoing collaboration with the National Health Service (NHS) is seen as an important trailblazer in the medical application of AI. After AlphaGo\u2019s triumph, DeepMind turned its focus towards more complicated games requiring real-time strategies. In November, DeepMind collaborated with Blizzard in opening up the game StarCraft II to AI players and machine learning researchers around the world. This time, researchers will try to challenge the game from a human player perspective and adapt to real-time gaming speed.\n\nIn 2016, the idea that the continued development of AI is dangerous to human beings originally voiced by individuals like Stephen Hawking and Elon Musk continued to escalate. After the triumph of AlphaGo in March, these warnings started to sound more persuasive than usual to the general public. \n\n \n\nLater on in March, Microsoft\u2019s AI Chatbot \u201cTay\u201d also stirred up quite a scene on Twitter. Tay was originally designed to imitate the speaking habit of 19-year-old girls and learn through interacting with internet users. Within one day of its public launch, however, the internet taught Tay to say things that made the Chatbot a racist, Nazi supporter. \n\n \n\nGoogle also got into trouble this year: the company\u2019s image recognition program identified a black man as a gorilla, and its search ads suggested job advertisements which differed between men and women. Other news, such as the Dallas police using robots to bomb snipers and the effects of social networks on the U.S. presidential election, all contributed to public anxiety regarding AI. \n\n \n\nWill AI be a threat? Some researchers think AI can cause further social inequality and instability, and this includes using algorithms with biased loopholes or filtered data. These social reactions could lead to major public misunderstanding of AI.\n\n \n\nFurther education is definitely needed. Many organizations and researchers are working on various projects to change the phenomena. For instance, in September 2016, Google\u2019s DeepMind collaborated with Microsoft, Amazon, Facebook, and IBM to form an organization called \u201cPartnership on AI\u201d, which tries to explore the development of safe AI and promote public education. This is a good thing because there had been many publications by academics, the tech industry, and media that exaggerated public issues. If you are interested in learning more, check out articles by Microsoft on \u201cThe Partnership of the Future\u201d, the Allen Institute for Artificial Intelligence on \u201cDesigning AI systems that Obey Our Laws and Values\u201d, and the Nature International Weekly Journal of Science on \u201cMore Accountability for Big-Data Algorithms\u201d.\n\n \n\nThe future relationship between AI and human beings will be refined through careful research and practice rather than verbal debates. Governments and relevant organizations around the world are stepping in to make laws on AI relating practices. In the words of U.S. Council of Economic Advisors chairman Jason Furman, \u201cwe need AI for the future.\u201d\n\nThe White House released two reports in October 2016, \u201cPreparing for the Future of Artificial Intelligence\u201d and \u201cNational Artificial Intelligence Research and Development Strategic Plan\u201d, detailing the future challenge, development, and strategic planning of AI. These reports described the future opportunities and challenges posed by AI facing the American government. Venturebeat summarized the above reports in 7 takeaways from the White House report on AI:\n\nIn December, the White House released another report named \u201cArtificial Intelligence, Automation, and the Economy\u201d. This report forecasted the economic influence of AI and automation technology, while offering strategies to solve issues like technological unemployment. \n\n \n\nMeanwhile, the European, Chinese, Japanese, and Singaporean governments also signalled over the last year that they viewed AI as an important driver of national development moving forward. Li Keqiang, the premier of China, repeatedly emphasized in public statements that the Chinese government will be putting in a concerted effort to support tech developments and the robotics industry. \n\n \n\nMany governments and research labs paid extra attention to AI regulations in 2016. Researchers are working to address and preempt potential negative repercussions of automation such as social inequality, technological unemployment, and capital redistribution. They are also working on safeguards against potential abuse of AI technology. These are all important issues for which we have not found complete solutions to yet. \n\n \n\nOne report submitted by the Council of Economic Advisers to President Obama in July mentioned that an estimated 83% of jobs with wages under $20/hour will eventually be automated.The council suggested two basic strategies to deal with the situation: 1) allow flexible social experimentation; and 2) encourage alternative methods of employment. \n\n \n\nGovernments are learning to deal with the radical new changes that will be ushered in by AI. Their careful decisions and adaptive policies will directly determine the future of our societies.\n\nIn October, researchers from Microsoft published an article named \u201cAchieving Human Parity in Conversational Speech Recognition\u201d. Apparently, speech recognition systems now have an word-error rate (WER) of 5.9% , outperforming human professionals. The 6.9% WER historical record was broken in December. The development speed is definitely threatening to current human voice recording employees. \n\n \n\nOther industry players are also entering the field. Yonghao Luo, the CEO of Smartisan, demonstrated the company\u2019s new iFlytek voice input method at a product launch also in December. \n\n \n\nThe continued development of speech input methods will popularize speech assistants. At AWS re:invent 2016, Amazon launched various kinds of tools to help developers use the company\u2019s speech assistant Alexa more easily. Rohit Prasad, the vice president of Alexa\u2019s development team, disclosed that the AI application has more than 5,000 skills up to now. The Amazon Echo, which had Alexa installed, could easily help users with daily tasks like scheduling and music searching via speech commands. Also, big companies like Google, Microsoft, Samsung, and Apple are catching up. In a few years, machines will be around to wait for your speech commands.\n\nSince the fall of the \u201cTower of Babel\u201d, language unification has been an universal dream. Now, AI based on neural networks is providing us with the solution. \n\n \n\nIn September 2016, Google published a paper on arXiv introducing the Neural Machine Translation System (GNMT), an advanced training technique which maximizes the quality of machine translation. The system is currently being applied to Google\u2019s Chinese-English machine translation. Two months later, Google announced further breakthroughs in the realization of multi-lingual translation by neural machines and zero-shot translation. \n\n \n\nSougou\u2019s new real-time machine translation application was unveiled at the Wuzhen World Internet Conference in November 2016. Machine translation is becoming more and more of an established trend. It is now plausible that the goal of language unification will be achieved in our lifetimes.\n\n \n\nIn addition to the translation of text-to-text sequences between language pairs, 2016 also saw a noteworthy breakthrough in the \u201ctranslation\u201d of lip to text (video sequences to text sequences). In November, at Oxford University, Google\u2019s DeepMind and the Canadian Institute of Advanced Studies (CIFAR) jointly published an important paper describing the use of LipNet (a sentence-level lip reading AI) to actualize automatic lip-reading. The application currently has 93.4% accuracy, easily beating experienced human lip readers.\n\nAs deep learning algorithms become more complex, the data sets used grow as well, making demands on upscaled hardware. \n\n \n\nIn 2016, building platforms for artificial intelligence became a major new direction in the development of computational hardware. In addition to chip giants like Intel and NVIDIA making high-profile moves in AI, start-ups with core technologies are also trying to make pivotal changes (although high potential start-ups are acquired at faster rates). Even companies like Google are trying to make their own moves in this area. \n\n \n\nThe traditional chip manufacturer NVIDIA combined use of GPUs with deep learning algorithms to further its own development, helping its stock prices to soar and emerge notably as this year\u2019s biggest winner in AI computing. \n\n \n\nIntel, with a bigger market share, naturally did not wait for new markets to be claimed by new comers and sought to catch up through acquiring start-ups. In 2016, Intel acquired several AI start-ups such as Movidius (computer vision) and Nirvana (in-depth learning chip). By November, Intel announced its roadmap for AI with Nirvana and FPGA vendor Altera Plus, which was acquired in 2015, and introduced its corporate strategy and planned production ecosystem in AI chips. \n\n \n\nAMD emerged as a noteworthy competitor in this field in 2016, as the company announced its first VEGA GPU architecture-based machine learning chip. At the same time, DSP vendor CEVA, FPGA vendor Xilinx and processor technology vendors like Imagination also made their presences felt in the machine learning world."
    },
    {
        "url": "https://medium.com/syncedreview/interview-with-dr-richard-sutton-we-might-have-strong-ai-algorithms-by-2030-a1052332d878",
        "title": "Interview with Dr.Richard Sutton: we might have strong AI algorithms by 2030.",
        "text": "Synced visited University of Alberta and talked with the God Father of Reinforcement Learning\u2026\n\nDr. Sutton: It was always an obvious idea, a learning system wants something and some kind of learning is missing. In 1970s, Harry Klopf (1972,1975,1982) wrote several reports addressed the similar issues. He recognized the essential aspects of adaptive behavior were being lost as learning researchers came to focus almost exclusively on supervised learning. The missing part is the essential idea of trial-and -error learning.We tried to figure out what the basic idea, and found out that he is right. This idea has never been studied in any fields, especially in machine learning, not in Control Theory, not in Engineering, not in Pattern Recognition. All those fields overlooked this idea. You could see some earlier work in 50s, people talked about trial neuro but in the end it became supervised learning. It has targets and training sets and try to memorize, try to generalize from it. It\u2019s funny we\u2019re nowadays talking about Deep Learning and Reinforcement Learning. Way back to the beginning, it was the similar situation, trying to distinguish Reinforcement Learning from Supervised Learning. You need a system that can learn and that\u2019s all. So, Reinforcement Learning system finds a way in behaving or maximizing the world, where Supervised Learning just memorizes the example given to them, and generalizes new ones but they have to be told what to do. Now, Reinforcement Learning system can try different things. We must try different things, we must search actions and spaces or define learning to maximize the world. So, that idea has been lost and Andrew Barto and I gradually realize that it\u2019s not present in old works and it was needed. This is simplified view of why we\u2019re precursors.\n\n\uff08Editor\u2019s note: actually, Dr.Sutton has been developing and promoting Reinforcement Learning (RL) from late 1979. Like others, Dr.Sutton had a sense that reinforcement learning had been thoroughly explored in the early days of cybernetics and artificial intelligence. While reinforcement learning had clearly prompted some of the earliest computational studies of learning to develop, most of these researchers had shifted their focus to other things, such as pattern classification, supervised learning, and adaptive control, or they had abandoned the study of learning altogether. Also, the computing power of the computers at that time was very limited, so it was quite difficult to apply Reinforcement Learning to a real-world problem, since Reinforcement Learning involves a lot of trial-and-errors before converging to the optimal policy, which can take an extremely long time.\uff09\n\nDr.Sutton: I do not agree as you mentioned that Reinforcement Learning development is slow, but I do accept the fact that increasing computational resources have a big impact on this field. You have a time to coincide with the availability of hardware. Even though it is still a bit early for deep learning, it uses a lot of computation successfully because of its strength. It has been a long time that people said we will have computation power for strong AI in 2030. I think it is not just only depend on cheap hardware, but also algorithms.I don\u2019t think we have strong AI algorithms now but we might have them by 2030.\n\nDr. Sutton : It\u2019s a big question whether one of the hardware first or the software first. We have the software to test out hardware, and the availability of hardware pushes people to software. But it\u2019s not tremendously valuable for smartest guy researching or working in limited computational resources. Even in 2030 we may have adequate hardware, we may still need 10 years more for smartest guy to catch up with algorithms. Now you know my reasonings, you can reevaluate it or change it yourself.\n\nDr.Sutton: The basic reinforcement that trumps different learning has been found in the brain essentially. There are processes in the brain that are followed same rules and are well modelled by the rules of reinforcement learning.This is so called the standard model of world system in our brain. And I say it is standard model not because it is perfect but everyone can pick it. You knew you are succeeded when everyone chooses you , as well as the reward system in the brain. Thus, our brain is a good model of psychology learning and animal behavioural study. Meanwhile, the other major thing is the model is based on learning where you can do planning, that is responded from various notions of replay imagining circumstances. That is also a model reinforced how we plan , where we can learn the sequences from various demonstrations. With considering of both, AI researchers try to figure out the mind and deep reassuring behind that.\n\nReinforcement learning studies decision making and control, and how a decision making agent can learn to act optimally in a previously unknown environment. Deep reinforcement learning studies how neural networks can be used in reinforcement learning algorithms, making it possible to learn the mapping from raw sensory inputs to raw motor outputs, removing the need to hand-engineer this pipeline. Thus, nowadays, Deep Reinforcement Learning (DRL) which combines Reinforcement Learning with Deep Learning has become a very popular approach to solve many kinds of problems, such as game playing, decision problems, robotic control etc.\n\n\uff08Editor\u2019s note: Dr.Sutton agreed that combination of RL and DL is a really good improvement. As to the specific field , for an example Computer Vision ( CV), he stated that \u300cYou can certainly do a computer vision without reinforcement learning and practice normally to do it as how to prepare dataset mainly supervised example and then to learn from that. But I can say you couldn\u2019t have it without deep learning. But who would actually take some imaginations and do it with reinforcement learning. I think it would take some cleverness and imagination to do that. I\u2019d tend to think that would be a breakthrough to do computer vision with a degree of reinforcement.\u300d\uff09\n\nDr.Sutton (cont\u2019d): The winning feature of reinforcement learning is that you can learn during normal operation. Conventional deep learning learns from trained label training . (With Reinforcement learning) Whereas in principle you could learn from your normal operation. You could take some imagination to reform it because you don\u2019t have examples but you have much more experience than just normal use. And then you do(test) in the training examples."
    },
    {
        "url": "https://medium.com/syncedreview/interview-with-john-markoff-ai-surpasses-humans-and-take-away-jobs-2-47ea12c525e9",
        "title": "Interview with John Markoff : Does AI Surpass Humans and Take Away Jobs? (2)",
        "text": "It is Synced\u2019s pleasure to have John Markoff, a journalist at the New York Times and winner of the 2013 Pulitzer Prize for Explanatory Reporting, share his professional experience about Artificial Intelligence(AI).\n\nMarkoff: I guess, in terms of communities, it has to do what they focus on as researchers and designers. So the AI community has for a long time basically focused on building technologies that mimic human capabilities, everything from the physical robots, I consider robotics to be a subset of Artificial Intelligence and that was originally true, to all the intellectual things like cognition and reasoning, all the things humans do. IA focuses on using computing to extend humans\u2019 intellectual powers. It started with Engelbart\u2019s group, and Engelbart as a research scientist, basically decided to devote his life to building systems that would allow small group of human intellectual workers to collaborate effectively and basically \u201cbootstrap\u201d human knowledge. That was his passion. Like Internet. If you go back, so the internet is a set of protocol. Basically it\u2019s a set of documents, and they are called RFCs. If you go and read RFC Number 1, the reason for the ARPAnet which was the of the internet was to use Doug Engelbart\u2019s technology remotely. It was called oN-Line System, NLS. So in a sense, that was the first killer app, the very first killer app.\n\nI mean, that was the internet was for initially was to build a set of tools that would allow human knowledge we could use to work more collaboratively, which was a really nice idea. I\u2019ve watched these tools of all over a long time, so his ideas were first borrowed by the researchers at Xerox Palo Alto Research center. You know, Alan Kay worked there to develop personal computer. And then the ideas were coalesced to run the computer called Alto and the software run on Alto. And then both Microsoft and Apple borrowed these ideas and commercialize them. And that was how the computing world was reshaped.\n\nMarkoff: Examples of sort of IA approach, taking technology to extend the human mind. But you know, they are not like black and white ideas. First of all, there are these two communities that generally have philosophical orientation. The AI guys have their view and HCI (Human Computer Interaction) community has been a community that really wants to use computing tools to facilitate humans to work with computers. So I\u2019m not the only one taking this position. I mean, there are some other very interesting computer scientists who have written about this dichotomy. But it\u2019s a paradox, because if you augment the power of a human you also need fewer humans because humans are more powerful. So that was the puzzle I try to work on and try to understand in the book. And I have models basically. I think Siri is a great IA model. It is using AI technologies to extend the human power to basically build a partner for human. And I completely understand that it\u2019s not a black and white issue. The self-driving car issue is a really fascinating one because if you can basically make completely self-driving cars, you need fewer cars, you need less parking space, you can redesign cities. At the same time, you lose millions of millions of jobs. So there\u2019s this really interesting dichotomy and attention basically on the ideas. It\u2019s a puzzle basically; I don\u2019t think there\u2026. I think it\u2019s a good way of viewing the problem and thinking about the impact of technology. That\u2019s sort of what I\u2019m trying to frame.\n\nMarkoff: I hope that the designers of the systems were actually thinking about these questions. I was looking for a way to synthesize IA and AI and bring them back together. So yes, in a sense I hope that\u2019s true but it has to be done by the individual actions of designers. I know some of them very well. The two designers of Siri really are model from that kind of thinking. I mean there are deeply schooled AI researchers who want to use the techniques to do Engelbart style things, extend human power. That really excites me. So the potential is there.\n\nMarkoff: Jerry is a good friend of mine. We have this debate all the time. We have very different perspectives. Both of us have shifted our views. Books come out and time goes by. I finished the book at the end of 2013, so it\u2019s like almost two year. And the same is true for him. And now we think we both come to feel that the threat of automation is catastrophic. It\u2019s not gonna destroy all jobs anytime soon, but there is still an active debate. I\u2019ve been reading a lot about economists\u2019 writings. There\u2019s a growing interests in the field of economic inequality in America, becoming a very hot topic. Jerry makes this argument about technology and inequality and I think that the issue is controversial. The debate is fascinating to read, but I do not think it\u2019s clear. I think it\u2019s possible he is right but I\u2019m still reading on it. The best labor economist I read on this is David Autor. He has written some really thoughtful pieces about the limits of AI and in terms of automating jobs. He was the one who popularized the notion of \u201chollowing out\u201d the American economy. You know there\u2019s a great concern about the loss of middle class in America. Some thought that maybe in part technology is responsible for that. For example, there were many well-paid but repetitive white collar jobs and a lot of them have gone away in the last half decade to a decade. The people who did things in corporations that were repetitive have largely been replaced by software that just plugs things together. That\u2019s automation but I wouldn\u2019t call it AI. You can use based software and effectively automating interchange converse and things like that. And it takes away jobs. It\u2019s computer automation but it\u2019s not AI automation. I don\u2019t think it\u2019s super clear. I mean, Jerry is very adamant he is right. I just haven\u2019t seen the evidence. So we are still discussing that. There\u2019s a big debate among the economists about what\u2019s driven this growing inequality in American society. It might be changing tax policy and it can be other things expect science and technology.\n\nMarkoff: Part of the answer of the question over what time period. It depends. There is a very interesting report just done some MIT labor economists (Eric Brynjolfsson and Andrew McAfee) looking at the jobs of the lawyer. Actually they started off by citing my reporting in 2011 and 2010 when I reported about the use of e-discovery software which is document reading software. You have software that can do it demonstrably better job in reading documents than humans. A lot of people looked at that and said: \u201cOh, it\u2019s gonna affect the growth of the legal market.\u201d They looked at the actual job of a lawyer and it turns out that reading documents is only one part of probably I think they identified 12 things that lawyers do including counseling, going to trial, convincing juries. The only thing that the AI technologies have impact on is reading documents. So that\u2019s called task automation as opposed to job automation. We are seeing in American economy, that in the white-collar world, lots of examples of task automation for skilled jobs, doctors, lawyers, and people like that, programmers, journalists. For example, so much was made in neuroscience, that the fact that machines can write stories. But when will machines be able do interviews. Maybe someday, but not anytime soon. So once again, it\u2019s automating a part of the task, which is the writing task, not the complete task. So I was wondering one of the things that got me change my perspective on what fraction of the American work force, which is more spread out at risk been automated. There\u2019s an interesting recent McKinsey report. They have a very different approach. The debate is shifting in America now. I can see there\u2019s a growing consensus that the economy is not gonna be decimated by automation any time soon. The American economy recovered and there are more people working in America now than ever in history. There is something called labor participation rate, that is the percentage of the possible workers that actually working. In America, the rate is actually very low. It\u2019s not about technology; mostly it\u2019s about the aging American work force. As my generation begins to retire, they leave the work force and that lower the participation rate. So it\u2019s a complicated question."
    },
    {
        "url": "https://medium.com/syncedreview/master-talk-anca-dragan-algorithmic-human-robot-interaction-9b16df95ad68",
        "title": "Master Talk | Anca Dragan : Algorithmic Human-Robot Interaction",
        "text": "A: It\u2019s too young a field to have its own books, but I\u2019d recommend studying robotics, HCI, and cognitive science.\n\nA: Algorithmic HRI focuses on algorithms that move beyond robot function, to reasoning about people and interaction. It bridges the worlds of robotics and human-robot interaction, focusing on autonomously generating behavior for robots that leads to seamless interactions with people.\n\nA: In the next few years, a large focus is on autonomous driving. This is a domain where robots are becoming more and more capable, so they can drive on the road, reach their destination, avoid collisions. Coordinating with pedestrian and other drivers, as well as keeping the passengers feeling comfortable and safe, is I believe the next frontier in driving. This is where I think algorithmic HRI can have the most immediate impact.\n\nA: We\u2019ve seen quite a bit of progress in social robot navigation (moving among crowds), in learning from human demonstrations, and in generating more natural or more legible robot motion.\n\nA: I am hopeful that we will be able to treat interaction with the same level of capability as we treat function. That is, that we will be able to estimate and plan in a way that accounts for human state with similar success to the ways we are able to estimate and plan for physical state. Neither is a solved problem, but I think by reasoning over physical and human state, robots can generate behavior that is both functional and interactive.\n\nA: We make robots so that they can support real people. It seemed strange that our formal problem definitions in robotics rarely account for people at all. I saw a great opportunity to try and bring the field of robotics and the field of human-robot or human-computer interaction a bit closer together.\n\nA: We do both. We derive mathematical solutions, we turn them into algorithms, implement them on real robots, and test them in human subject studies. It\u2019d be hard to make any convincing statements about interaction by showing just the math, without showing that it actually works when robots and people interact.\n\nA: I am excited about all of our applications. We work on autonomous driving, and that\u2019s exciting in the shorter term. We work on assistive robotics, collaborating with the Children\u2019s Hospital Oakland. We also work on more fundamental problems, and there we test on more humanoid robots that have two arms, a head, etc. \u2014 these could be useful in homes, factories, etc.\n\nA: Absolutely. Safety is a big part of AHRI: not just tracking the human and stopping if you\u2019re about to collide with them, but anticipating what they are doing and planning ahead so that the system can preempt certain undesirable outcomes.\n\nA: I don\u2019t know what that means. I don\u2019t think we understand what we mean by consciousness. All an AI is trying to do is optimize whatever object it\u2019s given. The challenge is in making sure that we give it the right objective.\n\nA: I think in such a scenario, robots could be useful in caring for the elderly, or in helping people with disabilities. Robots could also be useful in assisting in surgery, manufacturing, and so on. I hope they will be useful tools, much like my computer is enormously useful to me. But secretly, I am also hoping they will have a bit of a personality and keeps things engaging.\n\nA: I think ultimately all AI is IA: AI is a tool for us to achieve more!"
    },
    {
        "url": "https://medium.com/syncedreview/interview-with-john-markoff-a-prophet-in-the-age-of-ai-1-33e49acd5d2c",
        "title": "Interview with John Markoff : A prophet in the age of AI (1)",
        "text": "Markoff: For my whole career, I think it\u2019s probably the microprocessor chip at the very beginning. It\u2019s a question of scale and the evolution of microprocessor chip. At certain point they can put all the basic components of a computer on a single piece of silicon. That really created a cost change that created a technical revolution that lead to personal computing. In my history, I think that has been the most significant thing. It has got more and more powerful and less and less expensive, just driving itself out to society.\n\nMarkoff: First of all, people have predicted the death of Moore\u2019s Law for many times, including me. We are getting to a point where Moore\u2019s Law seems to be slowing down. It\u2019s really interesting. It hasn\u2019t stopped, but for most of the industry in the last two or three years, the cost of per transistor stopped falling. That\u2019s very significant because it\u2019s about exponential. Things get faster and faster, cheaper faster. And the cheaper faster is what has driven the technology out into consumer applications. You turn the crank; you go from laptop, to tablet, to smart phones. If you can\u2019t get for free, it doesn\u2019t mean the progress stops, but it means you weigh on human innovation. So that\u2019s kind of exciting in a way, because there will be breakthroughs, but they won\u2019t come for free.\n\nAt the end, it will have less impact on society. For example, people like Ray Kurzweil have argued that you are gonna basically get a continuous explosion of technology. And I actually think it\u2019s more episodic. There are these rapid advances and plateau that require another technical breakthrough, maybe quantum computing. I watched very carefully, but workable quantum computers are still long way away. There\u2019s a debate. There\u2019s a Canadian company that claims to have a usable quantum computer called D-Wave. I follow the debate and I decide not to write about D-Wave until there are refereed papers in Science or someplace like that, because I don\u2019t entirely trust them.\n\nMarkoff: First of all, I can\u2019t tell how many stories I felt like I\u2019ve missed. Like, let me give you a good example. I was actually in the Google Garage, right after they moved to the garage and I didn\u2019t write about it. And I still irritated by that. And the reason I didn\u2019t write about it was there were six different search engines then. Maybe there were more than six. I had no idea which one was gonna be the successful one and I was too lazy. So it really irritates me. I knew these guys when they just starting out. It would look really good if I wrote about them. So I think part of it was timing in my case. I grew up with a generation of computer designers, ended up having a big impact on the world. In a sense, I was really lucky in terms of a technology writer. I spent a year at Byte Magazine in the early 1980s and Byte was sort of the technical hobbyist magazine. That was my familiar witch\u2019s degree that allow me to go very deep in my technical reporting and learn things about architecture. I think that was my advantage. I never got a computer science degree, but by reporting at Byte, I was able to get a technical advantage over other reporters. I was always more interested in the designers, so I spent my time literally hanging out with the guys who were doing the designs. They were my age. I came of an age as a reporter at the time the Homebrew Computer Club. It was a very exciting time. It was more actually about hobbyist culture than it was about industry. I mean, the personal computer first came out like a fantasy amplifier. There was a whole group of young people just wanted get their hands on this computer. They didn\u2019t even know what they wanted to do with it. It just seemed like the most exciting thing around. I was covering that. So it\u2019s timing more than anything else.\n\nMarkoff: There\u2019s probably a group of twenty years old out there doing the next. If I can start over, for example, I would focus on CRISPR. It was simultaneously invented in MIT and Berkeley. But I think CRISPR is gonna transform the world as early microprocessor did, for good or for ill. I just don\u2019t have the background of biological science, which is frustrating to me. I\u2019ve done some reports in gene sequencing, and I walked into those stories. You know I can walk into any IT situation, and ask intelligent questions. But when I walk into the biological labs, I got very nerves, because I don\u2019t have the grounding. It\u2019s frustrating.\n\nMarkoff: What\u2019s happening in America in the last ten years is this explosion of news organizations mostly on line. They cover technology. So there\u2019s a wealth of reporting. So quantity is not the problem. But it\u2019s uneven of the quality of the reporting. A lot of them focus on sort of business and consumer stuff and it\u2019s not focusing on technical detail. The art form is a good explanatory reporting on science and technology and I try as hard as I can. It\u2019s essential that, the society needs journalists actually watch these things and explain them. And I\u2019m not too pessimistic that I think there\u2019s such a vastly covered everything that sending to me how much has changed in the last decade.\n\nI expect to see a lot of sensationalism. AI is been so much covered by Hollywood. They are intense to shape our view of what machines do, and causes us basically our expectations are too high, largely because of the Hollywood movies. So I think one of the best things journalists could do is give us a real view of exactly the technology is."
    },
    {
        "url": "https://medium.com/syncedreview/alphago-may-already-have-found-a-different-beauty-of-go-beyond-human-imagination-3-9c32b38eab13",
        "title": "AlphaGo may already have found a different beauty of GO beyond human imagination (3)",
        "text": "Hui Fan: Logical thinking is a necessity for a GO player. But , the most important ability is perspective thinking, namely put yourself into your competitor\u2019s shoes. You always look before you leap. Now I don\u2019t pursue training top-level GO players, contrarily I prefer to make the GO game in popularity. I believe learning how to play GO is a self-learning process. GO is like a mirror which can point out all your shortcomings. Playing go doesn\u2019t require any special talent, hence everyone can play it even within 30 seconds.\n\nHui Fan: I don\u2019t agree that the only advantage in player\u2019s youth is computing ability, concentration should also be taken into consideration. When you are getting older, the errands in society will consume your attention step by step, unintentionally. You may be distracted by family and social problems while playing GO, and you may also be influenced by the reputation and money. Therefore your performance in GO games is not satisfying than you used to be.\n\nHui Fan: This involves a lot. Sometimes it\u2019s just talent, which is important and even essential in the GO game. Also , your life experience will be reflected to your GO playing honestly. Life experience are not always directly proportional to GO playing, sometimes less may be better for being simpler and more intuitive. Nevertheless some people consider the understanding and happiness from playing GO is more significant than pursuing perfection of skills. As for the sudden enlightenment in the Zen, I believe it is a sense of satisfaction and achievement, a sustainable spiritual joy, which also needs accumulation. Actually there is no sudden enlightenment, but a result of long-time efforts. So does the GO game. Some students are confused because they cannot feel any progress after one-year learning in the GO game. I always told them \u300c Haste makes waste \u300d\uff0clike water in a bottle, in which only enough amount of water can break the closure and then gush out. That is the concept of progress.\n\nHui Fan: From whatever aspects, either learning language or playing GO, children always learn faster than adults. The reason is simple. Children will always use whatever they just learn and correct mistakes immediately. However, adults will say I will use it but sometimes don\u2019t with many excuses. Coincidentally, Zen addresses the similar parable. A person is like a sponge, the more you experience, the more the water absorption will be. A child is a brand new sponge with less water , while an adult is a nearly full sponge which can\u2019t accept something new as easy as it used to be.\n\nHui Fan: Some people make it happen such as XunXuan Cao and Zhixun Zhao , who I admire most in the GO world . They could obtain so many achievements at their ages, which is nearly impossible in present. Nowadays the increasingly improvement of modern society lead it harder to be purity and concentration. It is inevitable for human beings to become complicated or inattentiveness at their forties because people are engaged in so many new things.\n\nHui Fan: Since I have stayed in Europe for years , not only teaching Go game, but also conveying the culture underneath. Like alphaGo , it is just matter of time , however , the Go game actually are trying to inherit an oriental thinking invented by Chinese people. The inner spirit of Go game is \u300che\u300d , a peaceful state of Taoism. Killing the king is not the ultimate objective of Go because each pieces are equal without any superiority . Meanwhile Go game is a definitely team-based game. In the Chess, less and less pieces are remain in the chess board during the battle between both sides, conversely, the number of pieces in hte GO game are continually increased. It matters to understand how to use the current piece to enhance former pieces as a group, which is called team spirit in the GO game.\n\nHui Fan: Of course. As a professional player , Go is my philosophy mentor. On my opinion, less is more, and simple is not easy . Everything in the world has its respective rules and regulations but eventually it will be normalized to a simple conception. Go game is too simple thus it is too complicated , it is an ultimate concept.\n\nHui Fan : I strongly agree with Zhe Li. From my perspective , any experiences in one\u2019s life will impact on his future . Honestly, It is hard to tell the impact on myself from this game ( the GO game between Huifan and AlphaGo) because it is just happened , and truly, I am not well aware of it.As to my technical playing skill, it might help , or do the opposite way. This is like a loop , which many top-level players have went through . After you entering a certain level , it must be a loop. Sometimes it is correlated with your mentality , you seems to know everything with strong confident , but suddenly , after stepping into it , you find yourself still in a uncharted path. Low self-esteem comes from uncertainty of the unknown. It happens to all of us. No matter how good you think you are, there is always someone out there that is better. It is useless to blow own\u2019s trumpet and see this world with one\u2019s superficial thought. That is called Taoism in GO game . Your nature is ever changing and is always the same. The path of understanding GO is simply accepting yourself. Live life and discover who you are."
    },
    {
        "url": "https://medium.com/syncedreview/interview-with-hui-fan-alphago-may-already-have-found-a-different-beauty-of-go-that-beyond-human-s-86b127b0225f",
        "title": "Interview with Hui Fan: AlphaGo may already have found a different beauty of GO that beyond human\u2019s\u2026",
        "text": "Hui Fan: Yes, it was about 10 years ago. A French team had developed a program called MOGO Go using Monte Carlo method, however, it was not advanced. At that time, a computer with Lyon server played against me based on a 9 x 9 board. It seemed that MOGO could \u201cthink\u201d because the program began to run faster when we entered the critical moment. Unfortunately I lost one, during the first 3 or 4 games, at the very beginning, I was not familiar with the smaller version of the board, but such as human nature, I quickly figured out the rules after the first two games. Thus, I pushed back and reign supreme.\n\nHui Fan: To some extent, yes. Since I have already known AlphaGo is a computer, I have a sense of substitution that try to explain its every move as a computer\u2019s calculating. However, there is less \u201ctricks\u201d (computer\u2019s strategy) than I thought when I review the entire game. In fact, it is not a calculated move, sometimes it is just a mistake or bad move. Certainly, computer can also make mistakes.\n\nHui Fan: To answer this question, we need to understand the essence of GO first. For the past years, I have taught GO in different countries, and at the same time, communicated the Asian culture behind the GO as well as the oriental philosophy. In China, GO is a sport competition, a wisdom for devising strategies. Go is an ancient board game which takes simple elements such as line and circle, black and white, combines them with simple rules and generates subtleties which have enthralled players for millennia. Thus, from my opinion, GO itself is a simple game, simple but beauty. Because GO lends itself to a uniquely reliable system of handicaps, players of widely disparate strengths can enjoy relatively even contests. So what is beauty? The definition may vary from individual to individual. AlphaGo may already have found a different beauty of GO that beyond human\u2019s imagination.\n\nHui Fan: Yes and No. As a GO player, I think GO is a discovery rather than invention. It has been existed in the universe as always and you just happen to find it. We see it, explore it and try to understand it. AlphaGo can\u2019t change the GO but may help people finding a better way to study it.\n\nHui Fan: The two sides are engaged in a fight of spiritual strength, no matter it is a computer program or human intelligence. Human players may bear a lot of pressure during the competition, but sometime, the spiritual strength may also inspire your desire and confidence. The instantaneous power is unexpected.\n\nHui Fan: There is a GO game called League Game, which allows two players against another two at one game, or even three to three. Its sways makes the GO game fun to watch. However, since every players have their own styles and strengths, cooperation seems to be unrealistic and challenge.\n\nHui Fan: This is an interesting question. The performance you provided in the game is unintentionally impacted by your daily experience, your failure and desires. If AlphaGo has its life, it is likely a \u300cWall\u300d. What is the \u300cWall\u300d? It does not move. Everything will bounce to yourself evenly when you try to chase it or stress it, no matter it is bless or curse. No feeling is the description I feel about AlphaGo\u2019s life.\n\nHui Fan: Certainly I do. It is not just me, every amateurs or professional players urge to play with AlphaGo. At that time, AlphaGo might look like the character Sai in Japanese comics Hikaru no Go . While exploring his grandfather\u2019s shed, Hikaru stumbles across a Go board haunted by the spirit of Sai, a Go master from Heian era. Hikaru is apparently the only person who can perceive him, Sai inhabits a part of Hikaru; s mind as a separate personality, coexisting, and starts to play Go again. Urged by Sai, Hikaru begins playing Go despite an initial lack of interest in the game. He begins by simply executing the moves Sai dictates to him, but Sai tells him to try to understand each move. In the final, Hikaru turns to be a truly Go master. At the end of the story, someone asks Hikaru for his reason for playing Go. His answer is to link the far past with the far future. The Go spirit between Sai and Hikaru impresses me most. In the future, AlphaGo could belong to everyone. When you want to play GO, AlphaGo can always company with you, just as Sai is always behind Hikaru.\n\nHui Fan: I might treat such technology with a more conservative attitude. I have read many novels and similar research progress, but I am not that into this concept. Players have a basic quality called control-ability, which means we tend to do things more controllable, not likely to do things beyond our control-ability such as trusting an uncertain chips.\n\nHui Fan: I agree with her, actually I do hear some negative concerns recently. But I insist that the development of AI will be helpful to us rather than devaluing the GO game."
    },
    {
        "url": "https://medium.com/syncedreview/interview-with-hui-fan-alphago-may-already-have-found-a-different-beauty-of-go-that-beyond-human-s-b59090e77a0a",
        "title": "Interview with Hui Fan: AlphaGo may already have found a different beauty of GO that beyond human\u2019s\u2026",
        "text": "Hui Fan: Last August, I came back to Bordeaux for vacation after the European Go Championship or Congress (EGC) in a a small town of Czech. Soon around late August or early September, I received an email invitation from Deepmind to visit the company. I had very little idea about this company at that time. However, I still accepted the invitation as I am the kind of people who are open to experience. \n\n \n\nI got to know that they were actually an Artificial Intelligence Research company owned by Google after a Skype call with them. During the call, they only mentioned that they were working on an exciting research project that is relevant to me, but nothing concert about the project or about the purpose of the trip, and of course, nothing about AlphaGo either. I signed an NDA with them. Then, it took place.\n\n \n\nAs for the NDA, I can not disclose some of the details. I guess the reason why they chose me is because of my performance on the European Go championship. I have been wining the European championship for the recent 3 years. Firstly, they need some one that is a professional Go player; and then this player better owning some nice title; preferably he\u2019s not on the other side of the planet. \n\n \n\nThe first game against AlphaGo was in October, 2015. The result is known on the spot, and I knew how powerful it is. The world know nothing about it and I can\u2019t tell anyone about it either. While I had to keep the silence, there were two thing became very interesting to me. One is that I went back to China to watch a Go program competition and find the system is totally not on the same level of AlphaGo.But I can\u2019t say anything; The other happens in November 2015. There was an amateur Go player of 7-dan rank from South Korea, who was also a top player in Europe. He said that he played with Crazystone a while a go, wining easily with a 3 stone handicap, and believe AI is far from wining human on Go even though it has made a lot of progress recently. I had to keep it to myself at that time. It is not just for him, most people believe Go programs have to take another 10 years to beat top human players. Well, they had made it already.\n\nHui Fan: No one can win every single game. Sure I have thought of the possibility that I might lose it. I decide to accept the invitation because I didn\u2019t thought there is much chance that I will lose; But even if I lose, so what? The game result does not mean anything to me. I am never afraid of losing. As long as you are playing the Game of Go, you must go with losing and winning. It doesn\u2019t matter who your opponent is. I just try my best. It is good if I can win, but it is just another game if I lost.\n\n \n\nWhen I was young, my father told me many similar stories, such as those about Lee Chang-ho. Lee worried about losing as well, but he keep a simple principle to make himself calm \u2014 just play the best for every single hand. In the end, wining or losing that game doesn\u2019t really matter for me. Actually for all these years I always play with this attitude in Europe. It is interesting that it was actually DeepMind who were very worried. They were afraid of me rejecting. It was just a research project for them, but for a professional player like me, I might lose my reputation. But I told them not to worry, as to a professional player, wining or losing is nothing special.\n\nHui Fan: No one would like to lose a game that has wining or losing, especially as a professional player. I tried my best to win. There are some posts on the internet suspecting that I lost the game intentionally. I don\u2019t want to comment on that, but I must admit that I did underestimated AlphaGo in the first game. As the opponent was an AI program, and we would play 5 games in total, I decided to play the first game simple and try something else in the later games. If I win the first game easily, then I know how to suppress him. If it didn\u2019t go well, I can still switch back. Another reason is that I believe integrity is the key in the game of Go. It is hard to evaluate the state of the plate clearly. If I play with simple strategy, it might be actually harder for it to evaluate. That\u2019s what I thought at that time. Its moves surprised me, \u2014 at least it never played the weird \u201ccomputer hand\u201d. It did pretty well on evaluating the plate. Before I made the mistake, I thought I would win. But after I made the mistake, I knew I was going to lose. Its control over the end game was amazing.\n\n \n\nThe first mistake is that I missed a Tesuji and caused a big loss. Another bad move was not hard to notice, but I was really careless because I thought I was leading and going to win. When that bad move appears, I know i might lose and there is no much chance to win. That was the first game, I didn\u2019t really care too much about it but DeepMind shows big interest.\n\n \n\nI was very upset after the first game. Why? It is simple, that was the historical moment that computer program beats a professional human player. It was the firs time and I lost it completely. Losing a game is not a problem, but I felt that I am incapable of beating him. I just made one mistake and I lost. \n\n \n\nThere are some more issue with my mentality in the later games. I changed my strategy starting from the second game. I designed some complex changes and trying to fight against it. The first Joseki was Avalanche, one of the most complex transformation in the game of Go. I caught some good chance at that time but then I made another mistake in the middle of the game. It caught me again, accurately as always. Once it caught me, the game fell into his hand and I could never turned it back. The later games went the same way, but it never made obvious mistakes.\n\nHui Fan: It will be very tough. But the biggest problem is that human make mistakes while machine make less. Therefore it became a burden later on. It doesn\u2019t really matter if i\u2019m leading or losing, I\u2019m afraid that I\u2019m gonna make mistake. But it is not a problem for AlphaGo. I actually lost my mind because of that. When I am leading, I didn\u2019t believe I could keep it up; When I was losing, I kept thinking that I was going to lose. Then it actually affect my performance. Why people don\u2019t think I was playing well at that time? Because to an experienced Go player, the performance in the game usually doesn\u2019t matches your ability due to psychological factors. For example. Lee Chang-ho, the famous player who has beated many top Go players in the world. He was known as \u201cthe Stone Buddha\u201d because of his poker face and stable mentality. You can\u2019t read anything from him. But what I want to point out is that who can be a \u201cStone Buddha\u201d comparing to computer? When facing the computer, no one can be more poker face than it. It does not has feeling like human players, and I suffered in the later games. It is void. You can\u2019t get him but he knows all about you. Totally imbalanced.\n\nHui Fan: Yes, it neglected some mistakes that I made but I don\u2019t think id didn\u2019t discover. It rather let me go as even it didn\u2019t catch me, it will win. It decided to let me took an area that I was supposed to lose. There would be more risk if it took that piece from me and the game will be more complex. It decided to not took my stone so that it could win the game easily. It chose a less risky strategy to win easily. Some Go TV show replicated the game between us in a later time, a professional Go player had the same view. As in the real game, I resigned soon after he let me took that area. I might continue fighting if it took my stone and that area. The game might be more complicated. Just like how Jie Ke said, he could not tell who is human and who is computer. No one can tell.\n\nHui Fan: I can\u2019t answer this question as it is covered in the NDA that I signed. You can ask other professional Go players about their opinions."
    },
    {
        "url": "https://medium.com/syncedreview/master-talk-randy-schekman-a-toy-microscope-leads-to-my-lifelong-fascination-with-microbiology-384fe535e1a7",
        "title": "Master Talk | Randy Schekman: A toy microscope leads to my lifelong fascination with microbiology",
        "text": "If you have seen his article, against the three prestigious journals Nature, Science, and Cell, in The Guardian, you probably still remember his straightforwardness. In his article, he claims that Nature, Science and Cell make them known as \u300cThe Luxuries\u300d, seriously distorting the progress of science development. They used \u300cinappropriate way to encourage scientists\u300d, which harm the improvement of science research. Sometimes, the high amount of times a journal got cited is just because it is eye-catching. \n\n \n\nIn the series of Cre8 Summit events held by Synced, we interviewed Professor Randy Schekman. In front of our camera, he kept his straightforwardness as always. Moreover, he talked about his personal story to encourage young people to follow their dreams this time. Below is Synced\u2019s interview video with Professor Randy Schekman.\n\nIn the video, he talked about an interesting story \u2014 \u2014 his science dream was ignited by a small toy microscope. When Randy was 10, he got a toy microscope as a gift. He scooped some silt and algae, put them under the microscope and observed. When he saw various microorganisms swimming in the lenses, he was amazed. Nonetheless, Randy\u2019s father said dismissively that it was just a toy. Randy was not satisfied, and wanted to have a real microscope. But a real microscope needed 100 dollars, which was a huge amount of money to him. Therefore, he started to find part-time jobs, including mowing, babysitting, helping out housework, delivering newspaper\u2026 However, his parents kept borrowing money from him. Finally one day, little Randy could not tolerate it anymore so he biked to the local police department, and told the policeman his \u300cstruggle\u300d: \u300cI want to buy a microscope, but my parents are always stealing my money\uff01\u300dHis parents were informed by the police and took him home directly. However, what\u2019s lucky about it, he got his first real Bausch & Lomb microscope in his life that afternoon. Since then, microscope became his best friend. \n\n \n\nAt the end of the video, Randy encouraged young people who are passionate about science to follow their dreams. He thinks, if you have your science dream, then you should definitely research on it independently, not just finish the tasks that the teacher gave you. He said that, \u300cGo explore yourself\u2026 Go to the libraries, read books, design experiments,examine creativity, do things independently.\u300d"
    },
    {
        "url": "https://medium.com/syncedreview/dr-yu-hu-the-development-path-of-speech-recognition-and-artificial-intelligence-2-b9be7c961b5f",
        "title": "Dr. Yu Hu : The development path of speech recognition and artificial intelligence \uff082\uff09",
        "text": "IFLYTEK shadows well with the history of speech recognition. Geoffrey Hinton and Microsoft researcher Deng Li become the leaders in the deep neural network industry, and IFLYTEK is quickly followed. Before long, It turns to be the first commercial-based company of deep nervous network application in China. At that time, Google is the earliest company who applies the deep neural network globally\uff0cand Google \u2018s Voice Search also pioneered speech recognition under Internet thinking. In this regard , IFLYTEK is inspired by Google profoundly. That probably why IFLYTEK can surpass other competitiers by using ripple effect in speech recognition.\n\nIFLYTEK initially relied on hidden Markov model, and then witched to the Internet. In 2009, IFLYTEK published a web demo, On the September of the same year, Android version was released , and IFLYTEK began the transformation of mobile Internet. In May 2010, IFLYTEK released demo that can be used on Android devices; in October 2010, IFLYTEK launched a voice input method and voice cloud .Cc\n\n \n\nThroughout the entire process, the most challenge moment is to prove the feasibility from even uncertainty.Those U.S companies survived and outstanded , so did IFLYTEK.\n\nVoice interaction is not a necessity in all scenarios. For example, it is a necessity that when one can put his or her cellphone aside when working on something else. Voice interaction is not necessity for everyone, for example, the younger generation who can easily use their cellphone. However, to elderly, it could be. The most valuable usage of voice interaction for the younger generation is for input, including input text, search, using map and take notes. In addition to that, it is a necessity in some specific scenarios, such as wearable computing, smart hardware with remote control, automotive electronics and home appliance. \n\n \n\nIFLYTEK has years of experience and achievement in the acoustic field and has solved the problem of long distance voice collecting. But beyond the long-distance problem, there is still natural interaction problems such as multiple round conversation and auto-correction. Firstly, most of the scenarios will need long-distance solutions, which need support by a series of technologies, including acoustic source localization, Beamforming, echo removal, anti-reverb, awakening etc, it is a system and can be more complicated than voice recognition. Secondly, it is about conversation control. Most voice assistants on mobile phones are single rounded, which mean you need to press the button once for every sentence, and it must be in full duplex mode ( Editor\u2019s notes: In a full duplex system, both parties can communicate with each other simultaneously) and can communicate simultaneously. The third is multiple round conversation, which is a process in rule-based conversation control. Another thing to support multiple round conversation is to understand user and their background. With all the above together, can it be a completed voice recognition system and can be called a core technology system innovation. One must achieve the end goal through a series of innovations all together to solve the problem in the previous scenarios.\n\n \n\nThe trends for voice assistant brought by Siri is obvious and it is vital to be able to understand which core technology system innovation or product micro-inovation can actually solve user\u2019s problem. In the field of voice recognition, IFLYTEK would like to bring not only products and technologies, but also value system. Preciseness, attitude and accuracy are tied to user\u2019s needs in the internet field. Thus, from user\u2019s perspective, it is no doubt that all technology must be created to solve problems and based on specific applications.\n\nVoice recognition is only part of IFLYTEK\u2019s landscape, some comprehensive researches they are working on is targeted for the AI industry such as cognition intelligence and perception intelligence.\n\n \n\nHuman spent years deciphering the flight of birds by imitating wings movements , but we failed. Later, aerodynamics is founded as an underlying contributing factor . Now, with the helping of flying machine, we performs even better than birds because the mechanical design can overcome some biotic limitations. When we recall the past of AI development, there is resemblance to the flying studies. We expect the AI to be capable of intelligence behavior, decision-making action but never possess self-awareness, just like robots. Asimov\u2019s lay of robotics were, and remain a fictional assumption that the self-awareness is the capacity for robotics come they have a certain intelligence.\n\n \n\nConfronted with this Black Swans effect, there are two questions we need to answer : does it really exist ? We cannot say not just because we have not seen it before. If yes, all the moral judgments and risks are under our consideration. Secondly , how to realize it ? Neuroscience may give you some clues.\n\n \n\nSome scientists state that we can create a synthetic brain by studying brain circuitry. The simulation doesn\u2019t consist simply of an artificial neural network, but involves a biologically realistic model of neuron. It is hoped that it eventually shed light the nature of consciousness. Another researchers support a new term called \u201cdynamics of intelligence \u201c. However, intelligence is not located somewhere in the brain, in the same way that \u201c athletic ability\u201d is not in some particular place in your body. Intelligence reflects differences between people, so it can\u2019t be in a person. Attempting to locate intelligence somewhere is asking the wrong question. \n\n \n\nNew conceptions and ideas are came out every second. If we can continue to generate new ideas , we would not mind to share these ideas out . Sometimes, sharing is even more important than debating. Let the world hear about our voice, which requires not only the passion, but also capability, resources, teamwork and persistence. That all IFLYTEK is pursuing, and now I believe it is also part of its destiny."
    },
    {
        "url": "https://medium.com/syncedreview/dr-yu-hu-the-development-path-of-speech-recognition-and-artificial-intelligence-1-290169da4839",
        "title": "Dr. Yu Hu : The development path of speech recognition and artificial intelligence \uff081\uff09",
        "text": "After electronic computer emerges, the earliest speech recognition system was created in Bell Lab ( Editor\u2019s notes: Audrey, developed by Bell Lab, can recognized 10 digits in English, which is the earliest speech recondition system that based on electronic computers). Later on, in the 50s\u2019 to 90s\u2019, several schools were developed in the speech recognition field, including IBM, Carnegie Mellon University and University of Cambridge, which turn to be the absolute dominators in the field of speech recognition.\n\n \n\n Hidden Markov Model has dominated speech recognition research for a very long time. (Editor\u2019s notes: HMM is a model describes a Markov process that with unobserved/hidden states. The hard thing about it is to draw the hidden states from observable states and then use the states to do further analysis, such as pattern recognition. The model was first developed by Leonard E.Baum and other authors in a series of statistic papers in later 1960s and was first being adopted in speech recognition in 1970s )\n\nThere are many theoretical researches on speech recognition in the past 30 years. A graduate from CMU, James K.Baker, who first started research in Continuos Speech Recognition Group in IBM, later founded Dragon System and become the first person created functioning application that used HMM in speech recognition. His company was not very successfully but he became famous in the history of Speech Recognition.\n\n \n\nAt the same time, speech recognition was researched in MIT as well; Bell Lab has developed Decision Making Theory and Training algorithms for speech recognition. After Bell Lab dismissed, University of Cambridge took over the flag and become the central of Speech Recognition. However, non of the solution is good enough; and the reason for it is because there is no Big Data and Ripple Effect.\n\nAny technology has a stage of accumulation and a stage of the outbreak , as well as speech recognition technology. It is derived from big data , along with the emergence of the Internet Ripple Effects and Deep Neural Networks. Ripple effect , referring to the role of the Internet thinking in improving the performance of the core technology . It was also known as optimization iteration , as to Andrew Ng\u2019s opinion (Chief Scientist of Baidu), researches, products and users are cycled as a closed loop of iterative optimization , which is an expression of Internet thinking to optimize and breakthrough . By this way, we can not only acquire data , but also gain learning experience, and understand better from user experience. \n\n \n\nSpeech recognition requires experience , data and user feedback to enhance the performance of joint action. With those feedback ,we can quickly high light user\u2019s characters and provide better performance. For example, some users are cut off somehow when they speaking , then we can fix this issue by adjusting some parameters. Data generating is only part of speech recognition, with the rate improvement , there are even more parameters such as senses, key points and experience. Like software iteration, rethinking matters. \n\n \n\nNow the reason why there are many misunderstanding regarding to speech recognition is that they even do not realize that, neither hidden Markov models, nor deep neural networks. They are just statistical machine learning tools. Some tools are good, some are just passable, and some latest tools could be better than the classical ones. That\u2019s probably why it is a critical to decide an appropriated machine learning method.Although the deep neural network is better than others, but it does not mean only the deep neural network can complete such a thing, hidden Markov model can do the job as well. For example, in the medical industry, Nuance, iFLYTEK had already recognized the fact before the deep neural network. Hidden Markov model has a limitation, especially with the amount of data increased. There is no significant enhancement of performance compared to the deep neural network, but in fact they all belong to statistical pattern recognition. \n\n \n\nIn the development of speech recognition, deep learning occurs simultaneously. To some extent, big data and the ripple effect, hidden Markov model is practical without the supporting of deep neural network. Since deep neural network reduce the threshold greatly, more people can be benefited. Also, under the application of ripple effect , the deep neural network algorithm could only be better. The more data we acquire , the better performance of deep neural networks will be.\n\nParticularly, the deep neural network is just a part of the whole theoretical framework of statistical pattern recognition machine, the statistical decision-making system is the core . Speech recognition is a decoding search system; and the deep neural network is a problem solution tool which is applied in acoustic and language models.\n\n \n\n Statistical decision theory value is more significant than the deep neural network system, but it was too professional. Accurately, Bayesian statistical decision theory , and the deep neural network is only used in statistical decision theory. However the statistical decision theory is a mathematical theory , it was proved that the speech recognition can be applied to this design, so decoding this method to get the best result , which proves the voice can be used to obtain optimal Bayesian decision model , which is statistical decision model is the most important point. Seldom people aware that because of its imfamous, but from the professional perspective , this is the most worthy field . Speech recognition can meet the Bayesian decision, in theory, be able to estimate the upper and lower limits and convergence conditions. Most practitioners, in the speech recognition field, just care about how rather than why.\n\n \n\nThus , Bayesian decision takes a decisive role in the entire statistical decision model. Deep neural network expand the effect more profound. At the initial stage, Markov model has been prosperity with mature decision theory for 30 years. Then, the emergence of the Internet has brought a ripple effect when the decision theory is reaching its maturity. Standing on the basis of all the theorie above,deep neural network maximize the influence of decision theory.The contribution of individuals can only be judged fairly until you have a whole picture of the history of speech recognition development."
    },
    {
        "url": "https://medium.com/syncedreview/yann-lecun-obstacles-on-the-path-to-ai-b7b8e7b8c14a",
        "title": "Yann LeCun: Obstacles on the path to AI \u2013 SyncedReview \u2013",
        "text": "Barely anyone connects more tightly with deep learning than Yann LeCun. He is a famous computer scientist with primary research in machine learning, computer vision, robotics, and computational neuroscience. LeCun is also best known for his research on optical character recognition and convolutional neural networks(CNN), who is also known as the father of convolutional nets.\n\nIn late 80s of last century, he joined Bell Lab, developed convolutional network method, which can be extremely useful to handwriting recognition systems: most of the check signatures in the United States are verified by this method. In the middle of 90s, even though neutral networking was in a recession, LeCun is one of the few scientists that remain confident. In 2003, he became a professor in New York University Center for Data Science, contributing a lot in deep learning. In 2013, he joined Facebook\u2019s new found lab called Facebook AI Research, and led the artificial intelligence research in Facebook, which focused on natural language processing(NLP), computer vision, and pattern recognition, etc. Synced had edited and translated a IEEE exclusive interview with LeCun, click here to read.\n\nOn October 2015, LeCun attended Bay Area Machine Learning Symposium in California, USA, and delivered a speech titled Obstacle on the path to AI.\n\nBay Area Machine Learning Symposium is a conference for all the machine learning scientists in San Francisco Bay Area, aiming at promoting community building between academic research and industrial institutions. The organizing committee includes Samy Bengio from Research at Google, Enda Wu from Baidu, Joaquin Quinonero Candela from Facebook, etc. \n\n \n\nLeCun gave his speech a subtitle, \u201cHow I learned to stop worrying and love unsupervised learning,\u201d and showed an optimism attitude toward unsupervised learning. He thinks it will be a powerful tool to establish Strong AI.\n\n1.Introduce to the corporation between representation learning , reasoning and decision-making. The goal of Representation learning is to seek for the better way to build appropriated model based on massive unmarked data.\n\n2. The disadvantage of reinforcement learning is the billions of parameters with representation learning that obviously slow down the machine learning. Reinforcement learning , a branch of machine learning , emphasizes to maximize imaginary profit in accordance with the time and the place. It is inspired by the behaviorist learning theory from psychology, that the relatively permanent change in behavior brought about as a result of experience or practice. The focus of the behavioral approach is on how the environment impacts over behavior. Yann metaphorically refer reinforcement learning to \u201c the cherry on the cake\u201d, which sounds like trolling, but there is no possible to learning billions of parameters within a reasonable amount of time.\n\n3. The application of unsupervised learning in natural language process( eg. phrase nesting , semantic combining , QA system ). There are two main leaning model of machine learning : supervised and unsupervised . Supervised is applied for marked database, for example , the image database with specific comments and description; the other is unsupervised learning , with which computer must to search for the unmarked subsets, subgroups from different panels.\n\n4.Memory Networks: how to simulate a hippocampus-liked memory module ? There are two hippocampus in the human\u2019s temporal lobe brain, but separate regions of two hemisphere. Hippocampus, as part of cerebral lambic system, applied the function of memory control and space orientation. LeCun states that, the current networks cannot remember things that long, the cortex only remember things for 20 seconds. Thus , we need a hippocampus as a separate memory model. Jason Weston \u2018s memory networks is suggested as a research review.\n\nFrom LeCun \u2018s perspective , unsupervised learning possesses a lot of advantages since it is modeled on a real world. This world is uncertain and vivid, rather than formalized and invariable. Thus, machine learning cannot be applied on every objectives. How to build an unsupervised learning with non-sample related is addressed LeCun\u2019s attention. In the Greek mythology , Sisyphus returns toward his rock, and the sorrow is back to the beginning. Now, confront with the non-sample related method , LeCun is the Sisyphus in unsupervised learning filed. Can he roll the stone up the hill of his expectation? Only time can tell."
    }
]