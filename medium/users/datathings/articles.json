[
    {
        "url": "https://medium.com/datathings/what-your-location-history-tells-about-you-5ebc9b3688d0?source=---------0",
        "title": "What your location history tells about you? \u2013 DataThings \u2013",
        "text": "At DataThings we are developing several reactive profiling tools (integrated into our GreyCat product) to better represent, visualise, dynamically query and analyse data in a fast way. In order to test and validate these profiling tools, we need a case study where we already understand the data, so the validation becomes easier.\n\nThe best case study that fits this need is to profile our own GPS positions. Google maps allows users to collect GPS positions and stores them in JSON format that can be downloadable at the following link. This file represents the entire history of you as seen from google servers point of view.\n\nAfter downloading the JSON file, you can use our tool: https://demo.datathings.com/lumo/ to check your most visited locations. The data will be processed locally using javascript on your computer. Nothing will be stored in our servers. In case you don\u2019t have google data collection enabled, or you\u2019re just curious to see the tool in action, you can use the \u201cdemo\u201d button to download and use a sample google location file.\n\nThe tool allows several query types to be executed on the data:\n\nThe time related query control panel allows you to select a specific date range, and specific days and hours of the week to be displayed on the map.\n\nIn the location related filters you can display the top 10 visited locations for example, or the locations you have visited at least 100 times. You can change as well the profiling precision radius to get more or less detailed view.\n\nAnother feature of the tool: is the dynamic heatmap display according to the zoom on the map. When you zoom-in or zoom-out, the heatmap is recalibrated dynamically to display the most visited places in the current map window. For instance, here is my heatmap above Luxembourg\n\nWhen you zoom in to the maximum level, you can view individual markers. Clicking on one of the markers, will open a popup with the weekly view profile. For instance, here is my weekly view profile at home and at work:\n\nThe tool automatically classifies, the top 7 most visited points to find if they are a home or work locations. The classification is based on the weekly profile and a root mean square distance to typical home and work profiles.\n\nIf you would like to give us your feedback or for any additional info, don\u2019t hesitate to contact us at: contact@datathings.com"
    },
    {
        "url": "https://medium.com/datathings/time-series-decomposition-over-open-weather-data-in-luxembourg-e7bc7074c565?source=---------1",
        "title": "Time series decomposition over weather data \u2013 DataThings \u2013",
        "text": "In order to see the results of the time series decomposition, we will use the open weather dataset of Luxembourg provided by Open Data Luxembourg. The dataset consists of the historical monthly average of temperature records in Luxembourg since January 1947 and can be downloaded for free from here. Here is the plot of the temperature data:\n\nDue to earth periodic nature of the yearly rotation of earth around the sun, it is very hard to see from this graph the temperature trend over the years. Time series decomposition helps extracting the periodic component from the signal.\n\nAfter passing it through a Gaussian profile, we can detect this periodic component. In January, the temperature average in Luxembourg is around 0.5 \u00b0C, it raises to 17.5 \u00b0C in August, then falls down back to 0.5 \u00b0C in average for the January of the next year. Here is the result (we only show 2 years here for simplicity):\n\nAfter filtering out this periodic signal, we can then fit a polynomial curve of the temperature trend over the years.\n\nAs you can see from the graph, the temperature trend curve was stable till 1980s, then it increased by 2 degrees the last 36 years.\n\nAfter removing the periodic and the trend signals from the original data, we are left with the random remainder. Here is what it looks like:\n\nThe interesting aspect about the random component is that it has an average value of 0. This is perfectly logical, since the time series decomposition goal is to move the average value of the signal to the periodic and linear trend.\n\nIn order to see the benefit of time series decomposition, we consider that our predictive model is the sum of the trend + periodic component. If we plot this predictive model (in orange) vs the real data in (blue) we get the following graph:\n\nFinally, to validate our results, we run the time series decomposition stl function in R and using facebook Prophet, the results are displayed in the figures below.\n\nThey both confirm our result of increase in temperature of 2 degrees over the last 36 years. However both processes in R and with facebook prophet took more time to execute than our live approach and require to load the full dataset before learning."
    },
    {
        "url": "https://medium.com/datathings/the-magic-of-lstm-neural-networks-6775e8b540cd?source=---------2",
        "title": "The magic of LSTM neural networks \u2013 DataThings \u2013",
        "text": "LSTM networks manage to keep contextual information of inputs by integrating a loop that allows information to flow from one step to the next. These loops make recurrent neural networks seem magical. But if we think about it for a second, as you are reading this post, you are understanding each word based on your understanding of the previous words. You don\u2019t throw everything away and start thinking from scratch at each word. Similarly, LSTM predictions are always conditioned by the past experience of the network\u2019s inputs.\n\nOn the other hand, the more time passes, the less likely it becomes that the next output depends on a very old input. This time dependency distance itself is as well a contextual information to be learned. LSTM networks manage this by learning when to remember and when to forget, through their forget gate weights. In a simple way, if the forget gate is just a multiplicative factor of 0.9, within 10 time steps this factor becomes: 0.9\u00b9\u2070=0.348 (or 65% of information forgotten), and within 30 steps -> 0.04 (96% forgotten).\n\nIn this post, I won\u2019t go more than this about the technical details of LSTM. This blog post instead, explains very nicely their mechanics. Instead I will present here a compilation of the different applications of LSTM.\n\nGenerating a text, like this one, can be converted to an LSTM task where each letter is generated taking into account all the previously generated letters. In fact, teaching an LSTM network the alphabet can be the first programming example to learn how to make LSTM learn something. Andrej Karpathy, shows different examples of text generation by LSTM depending on the training set you feed them. These are some of the automatically generated text:\n\nThis is an animation from Alex Graves, showing an LSTM network performing in live a handwriting recognition:\n\nAs an inverted experiment, here are some handwriting generated by LSTM.\n\nFor a live demo, and to automatically generate a LSTM-\u2019hand\u2019writing text yourself, visit this page.\n\nSince music, just like text, is a sequence of notes (instead of characters), it can be generated as well by LSTM by taking into account the previously played notes (or combinations of notes). Here you can find an interesting explanation of how to train LSTM on midi files. Otherwise, you can enjoy the following generated music (from a classical music training set):\n\nLanguage translation can be seen as a sequence-to-sequence mapping. A group of researchers, in collaboration with Nvidia published details on how to tame LSTM for such task (part1, part2, part3).\n\nIn a nutshell, they created a neural net with an encoder to compress the text to a higher abstract vectorial representation and a decoder to decode it back to the target language.\n\nFinally, the most impressive use of LSTM networks is to generate from an input image, a text caption describing the contents of the image. Microsoft research is progressing a lot in this area. Here are some sample demos of their results:\n\nYou can try their online demo yourself here: https://www.captionbot.ai/. Have fun!"
    },
    {
        "url": "https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e?source=---------3",
        "title": "Neural networks and backpropagation explained in a simple way",
        "text": "Obviously we can use any optimisation technique that modifies the internal weights of neural networks in order to minimise the total loss function that we previously defined. These techniques can include genetic algorithms or greedy search or even a simple brute-force search:\n\nIn our simple numerical example, with only one parameter of weight to optimize W, we can search from -1000.0 to +1000.0 step 0.001, which W has the smallest sum of squares of errors over the dataset.\n\nThis might works if the model has only very few parameters and we don\u2019t care much about precision. However, if we are training the NN over an array of 600x400 inputs (like in image processing), we can reach very easily models with millions of weights to optimise and brute force can\u2019t be even be imaginable, since it\u2019s a pure waste of computational resources!\n\nLuckily for us, there is a powerful concept in mathematics that can guide us how to optimise the weights called differentiation. Basically it deals with the derivative of the loss function. In mathematics, the derivative of a function at a certain point, gives the rate or the speed of which this function is changing its values at this point.\n\nIn order to see the effect of the derivative, we can ask ourselves the following question: how much the total error will change if we change the internal weight of the neural network with a certain small value \u03b4W. For the sake of simplicity will consider \u03b4W=0.0001. in reality it should be much smaller!.\n\nLet\u2019s recalculate the sum of the squares of errors when the weight W changes very slightly:\n\nNow as we can see from this table, if we increase W from 3 to 3.0001, the sum of squares of error will increase from 30 to 30.006. Since we know that the best function that fits this model is y=2.x, increasing the weights from 3 to 3.0001 should obviously create a little bit more error (because we are going further from the intuitive correct weight of 2. 3.0001 > 3 > 2 thus the error is higher) \n\nBut what we really care about is the rate of which the error changes relatively to the changes on the weight.\n\nBasically here this rate is the increase of 0.006 in the total error for each 0.0001 increasing weight -> that\u2019s a rate of 0.006/0.0001 = 60x!\n\nIt works in both direction, so basically if we decrease the weights by 0.0001, we should be able to decrease the total error by 0.006 as well!\n\nHere is the proof, if you run again the calculation, at W=2.9999 you get an error of 29.994. We managed to decrease the total error!\n\nWe could have guessed this rate by calculating directly the derivative of the loss function.\n\nThe advantage of using the mathematical derivative is that it is much faster and more precise to calculate (less floating point precision problems).\n\nHere is what our loss function looks like:\n\nIf we initialise randomly the network, we are putting any random point on this curve (let\u2019s say w=3) . The learning process is actually saying this:\n\n- Let\u2019s check the derivative.\n\n- If it is positive, meaning the error increases if we increase the weights, then we should decrease the weight.\n\n- If it\u2019s negative, meaning the error decreases if we increase the weights, then we should increase the weight.\n\n- If it\u2019s 0, we do nothing, we reach our stable point.\n\nIn a simple matter, we are designing a process that acts like gravity. No matter where we randomly initialise the ball on this error function curve, there is a kind of force field that drives the ball back to the lowest energy level of ground 0."
    },
    {
        "url": "https://medium.com/datathings/hello-world-6484c4f3a5ad?source=---------4",
        "title": "Hello from DataThings \u2013 DataThings \u2013",
        "text": "We are a startup of four passionate researchers holding PhDs in Computer Science.\n\nAfter several years of successful collaboration at the University of Luxembourg/SnT, we founded DataThings in 2017, to start the next chapter in live data analytics.\n\nWe aim at improving live analytics over complex and time-evolving data a reality in order to support your businesses in operational decision-making.\n\nThis blog will be kind of our diary on experiments and results we find through our adventure.\n\nHope you will enjoy it, feel free to follow us on twitter."
    }
]