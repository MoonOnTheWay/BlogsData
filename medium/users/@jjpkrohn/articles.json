[
    {
        "url": "https://insights.untapt.com/deep-reinforcement-learning-and-generative-adversarial-networks-tutorials-with-jupyter-notebooks-6ef4dc6957ea?source=user_profile---------1----------------",
        "title": "Deep Reinforcement Learning and Generative Adversarial Networks: Tutorials with Jupyter Notebooks",
        "text": "The first network \u2014 the Generator \u2014 attempts to create forgeries of some category of human-created images, say all the works of Claude Monet or Pablo Picasso. The second network \u2014 the Discriminator \u2014 does its best to distinguish the forgeries from the real images. With practice, the Generator learns to create low-quality forgeries, encouraging the Discriminator to develop some ability to discern them from the genuine article. Over countless iterations, this dynamic gradually results in compelling counterfeits: the Generator must produce ever-higher-quality forgeries in an effort to outfox the always-improving forgery-detecting instincts of the Discriminator .\n\nGANs are the brainchild of Ian Goodfellow . They are apparently the result of a tipsy, overnight coding splurge while Goodfellow was in graduate school at the University of Montr\u00e9al in 2014. At a high level, GAN involve two separate deep neural networks acting against each other as adversaries.\n\nBelow is a summary of what GANs and Deep Reinforcement Learning are, with links to the pertinent literature as well as links to my latest video tutorials , which cover both topics with comprehensive code provided in accompanying Jupyter notebooks .\n\nThe first are from a class of algorithms called Generative Adversarial Networks , which churn out stunning graphics \u2014 e.g., drawings, paintings, photographs \u2014 that are often indistinguishable from the real deal. The second category consists of Deep Reinforcement Learning models, which have exceeded human performance on complex problems, including Atari video games, the popular Asian board game Go, and safely driving an automobile.\n\nIn recent years, two families of Deep Learning architectures have yielded the lion\u2019s share of the most surprising \u201cartificial intelligence\u201d advances .\n\nWhile the first GANs created somewhat grainy, low-resolution images, recent developments are surreal. As an example, CycleGAN (paper, code) converted the subject of the video at the top of this blog post from a horse to a zebra frame-by-frame. While countless transitions facilitated by CycleGAN are provided on its creators\u2019 project page, many of the best are summarised in the single figure above, including:\n\nThe same Berkeley research group that developed CycleGAN created a fun, web-based tool called pix2pix that illustrates the power of GANs in real-time by allowing you to sketch cartoons that are automatically converted into (depending on the quality of your cartoon) real-ish-looking images. Here\u2019s a three-eyed cat I drew with pix2pix as a silly example:\n\nIf you\u2019re curious what the state-of-the-art is like in GANs today, the following clip from Tero Karras and his colleagues at the deep-learning-hardware company NVIDIA may be it. They fabricated countless 1024x1024-pixel \u201cphotographs\u201d of synthetic celebrity faces that are for the most part convincing:\n\nIn my video tutorials, we use Python to gradually build up TensorFlow-backed Keras code that assemble the Generator and the Discriminator networks, ultimately pitting them against each other as adversaries to produce realistic-looking \u201chand-drawn\u201d cartoons. The corresponding Jupyter notebook is available here.\n\nReinforcement Learning (RL) involves programming an agent to enable it to take complex sequences of actions within an elaborate environment so that it can obtain as many rewards as it can. A wide variety of complex tasks can be defined with rewards \u2014 e.g., obtaining a high point score in a video game, driving safely to a particular destination, or manipulating delicate objects with robotic arms \u2014 therefore a wide variety of complex tasks can be attempted by an RL algorithm.\n\nAn RL agent becomes a Deep RL agent when layers of artificial neural networks are leveraged somewhere within its algorithm. Google DeepMind is responsible for numerous headline-grabbing Deep RL implementations over the past couple of years, including:\n\nIn my tutorials, we focus on a particular type of Deep RL approach called Deep Q-Learning networks. In a Nature paper published in 2015, researchers at DeepMind unveiled a single Deep Q-Learning algorithm that could outperform expert human performance across a broad range of Atari video games. There are countless videos of this algorithm playing Atari games on YouTube \u2014 one of my favourites involves mastering Breakout, as shown below. The Jupyter notebook of the Deep Q-Learning algorithm we implement step-by-step over the course of my tutorials is available here.\n\nThrough the feedback from my existing interactive video tutorials (Deep Learning Fundamentals and Deep Learning for Natural Language Processing) as well as my in-class offering at the NYC Data Science Academy, it became clear that GANs and Deep RL are the topics that software developers and data scientists are most excited to learn about. So that\u2019s why I made these videos!\n\nThese videos are for you if you\u2019d like to learn how to:\n\nA detailed lesson-by-lesson breakdown of the tutorial content are provided in my corresponding GitHub respository. Here\u2019s a summary of what we cover:"
    },
    {
        "url": "https://insights.untapt.com/my-30-hour-deep-learning-course-demo-in-new-york-56e7f55739ee?source=user_profile---------2----------------",
        "title": "My 30-Hour Deep Learning Course: Demo in New York \u2013",
        "text": "My 30-Hour Deep Learning Course: Demo in New York\n\nOn Saturdays from March 3rd through to April 7th, I\u2019ll be offering an in-classroom-only Deep Learning course at the NYC Data Science Academy. All details, including the full curriculum, are available here. If you live in the area and are keen to experience a high-level demonstration of the course content \u2014 as well as to ask me any questions you might have \u2014 that\u2019s coming up on the evening of February 20th.\n\nThe course is an introduction to artificial neural networks that brings high-level theory to life with interactive labs featuring TensorFlow and Keras, the two Python libraries currently gaining the most popularity. The content of the course is the basis for my textbook, Deep Learning Illustrated, which is being published by Pearson and will appear on bookshelves later this year.\n\nOver five weekends, essential theory will be covered in a way that provides an intuitive understanding of Deep Learning\u2019s underlying foundations. Paired with hands-on code demos in Jupyter notebooks as well as strategies for overcoming common pitfalls, this foundational knowledge will empower individuals with no previous understanding of neural networks to build production-ready Deep Learning applications across all the major contemporary classes:\n\nIn addition, I\u2019ll guide you through the creation of your own Deep Learning project from conception through to completion. Together, we\u2019ll engage in the following five project stages:\n\nAs detailed on my testimonials page, students on the previous iteration of my course, held in late 2017, found it tremendously valuable. Mahipal, a software development director at KPMG, indicated the course \u201cwas exactly what I hoped it would be. It gave me a strong foundation in all of the core deep learning concepts\u2026 it motivated me to pivot my career into deep learning.\u201d Meanwhile Richard, an electrical engineer and former investment banker, said that he \u201chad a ton of fun with the class\u2026 Jon was able to illustrate the complex concepts with super-easy-to-understand visualizations.\u201d\n\nI\u2019m now excited to get started with the next cohort! I can\u2019t wait to share my unbridled enthusiasm for Deep Learning and to see the projects that everyone develops."
    },
    {
        "url": "https://insights.untapt.com/filming-deep-reinforcement-learning-and-generative-adversarial-network-livelessons-772e76828302?source=user_profile---------3----------------",
        "title": "Filming \u201cDeep Reinforcement Learning and Generative Adversarial Network LiveLessons\u201d",
        "text": "Following on the back of my Deep Learning with TensorFlow and Deep Learning for Natural Language Processing tutorials, I recently recorded a new set of videos covering Deep Reinforcement Learning and Generative Adversarial Networks. These two subfields of deep learning are arguably the most rapidly-evolving so it was a thrill both to learn about these topics and to put the videos together.\n\nGenerative Adversarial Networks (GANs) pit two deep learning networks against each other as adversaries. One of these networks is tasked with generating novel images while the adversary critiques these images, attempting to discriminate the real (e.g., a photograph, drawing, painting) from synthetic, machine-generated analogues. GANs produce stunning photorealistic images with flexible, user-specifiable features, and offer many of the most surprising advances in Deep Learning in recent years.\n\nDeep Reinforcement Learning has produced equally exciting results, including the bulk of the most widely-publicised recent \u201cArtificial Intelligence\u201d breakthroughs. Achievements on this list include AlphaGo defeating world-leading board-game players, a single algorithm passing human-level performance on a broad range of Atari games, and robots becoming capable of subtle manipulation tasks previously reserved for human hands alone. The underlying theme across Deep RL is the training of an \u201cagent\u201d to become adept in given \u201cenvironments\u201d by rewarding them when they exhibit particular, desired behaviours.\n\nOnce edited, the videos will be about six hours long. They should be available within Safari Books Online by March. Here\u2019s a breakdown of the tutorial\u2019s five lessons:\n\nThis lesson starts off by examining what the term \u201cAI\u201d means and how it relates to Deep Learning. It continues by discussing cutting-edge applications of Generative Adversarial Networks and Deep Reinforcement Learning algorithms that have recently revolutionized the field of machine learning. We then quickly review how to run the code in these LiveLessons on your own machine as well as the foundational Deep Learning theory that is essential for building these advanced-topics specializations upon.\n\nLesson Two begins by covering the high-level theory of what GANs are and how they are able to generate realistic-looking images. Next up is the \u201cQuick, Draw!\u201d game, which is used as the source of hundreds of thousands of hand-drawn images from a single class \u2014 say apples, rhinoceroses, or rainbows \u2014 for a GAN to learn to imitate. The bulk of the lesson is spent developing the intricate code for the three primary components of a GAN: the discriminator network, the generator network, and the adversarial network that pits them against each other. The completed Jupyter notebook is available here.\n\nLesson Three is the first of three lessons that explores Deep Reinforcement Learning algorithms. It commences by introducing a simple game called the Cartpole Game that is used throughout the rest of the lessons to train our Deep Reinforcement Learning algorithms. Next, the lesson delves a bit into the essential theory of Deep Reinforcement Learning as well as Deep Q-Learning Networks (DQNs), a popular type of Deep Reinforcement Learning agent. With that theory under your belt, you\u2019ll be able to understand at an intuitive level the code that you subsequently develop when you define your own DQN Agent and have it interact with The Cartpole Game within a handy library called OpenAI Gym. The completed Jupyter notebook is available here.\n\nIn the previous lesson, a Deep Q-Learning Network was used to master the Cartpole Game. This lesson builds upon those Deep Reinforcement Learning foundations by using Wah Loon Keng and Laura Graesser\u2019s OpenAI Lab both to visualize our DQN agent\u2019s performance in real-time and to straightforwardly modify its hyperparameters. You learn how to gauge your agent\u2019s overall fitness and automate the search through hyperparameters to optimize your agent\u2019s performance.\n\nThe previous two lessons covered Deep Reinforcement Learning largely through the lens of the Deep Q-Learning Network. This lesson introduces the Coach library to easily expand your arsenal of Deep Reinforcement Learning agents. We focus on the REINFORCE Policy Gradient algorithm and the Actor-Critic algorithm in particular. The lesson closes out these LiveLessons by covering why Deep Learning is reshaping software in general as well as by returning to the discussion of AI, specifically addressing the limitations of contemporary Deep Learning approaches in attaining Artificial General Intelligence."
    },
    {
        "url": "https://insights.untapt.com/the-happiest-employees-by-u-s-state-and-city-b3cc4e4a1591?source=user_profile---------4----------------",
        "title": "The Happiest Employees, by U.S. State and City \u2013",
        "text": "The Happiest Employees, by U.S. State and City\n\nWhen users join our site, untapt, they have the option to engage in an introductory conversation that provides us with data so we can serve them better. One question we ask them is, on a scale of one to ten, how happy are you in your current job?\n\nI recently finished reading Daniel Kahneman\u2019s \u201cThinking, Fast and Slow\u201d, a wide-reaching contemporary classic \u2014 one of those hard-copies I glimpse over and over on my New York subway commute. While nominally an economics text, Kahneman\u2019s landmark book is gripping because each chapter is filled with practical, actionable insight on blind-spots in human decision-making as well as strategies for overcoming them. One of the primary concepts Kahneman explicates is the existence of two, often oppositional selves within each of us: the experiencing self and the remembering self.\n\nFascinatingly, the experiencing self and the remembering self evaluate fundamental emotions like pain and happiness rather differently. While the experiencing self endures the pain of surgery or the joy of conversation across all of its individual moments, the remembering self tends to focus solely on the peak moment (e.g., the most painful instant in a surgical procedure) and the final moments of a given experience. This leads to errors. For example, given two different unpleasant experiences, one of which must be repeated, the remembering self is likely to choose the more painful of the two options \u2014 provided that its peak moment and final moment of pain is lower than the alternative. The remembering self makes this decision even if, overall, the alternative is objectively much less painful.\n\nAnother quirk is that people tend to use shortcuts and heuristics to evaluate their overall happiness. When asked how happy are you in your life?, humans tend to substitute this complex question with simpler ones like how is my day going? or what significant events recently occurred in my life? Ergo, when we ask a given user how happy are you in your current job?, we may be receiving responses that are colored by a simpler question like how were my social interactions with my colleagues today? or the user may be reminded of a particularly unpleasant recent commute into work in which they were stuck for twenty minutes on a stalled train while pressed up against an unwashed passenger.\n\nTo begin studying the question of happiness within untapt\u2019s own rich database, I have unearthed a handful of simple analytics by identifying where in the U.S. employees are most and least happy. First, the global summary metrics: The mean level of happiness that we observe across the U.S. is 5.10 (with a standard deviation of 2.28 for the statistics-minded among you).\n\nWhile our data are certainly biased (we primarily market to technologists, especially software engineers), the average difference in current-job happiness was broad across states. Our happiest state, Utah, had users report an average happiness of 5.64, while those in our least happy state, New Jersey, reported happiness of 4.77.\n\nHere are the five happiest states amongst our users:\n\nAt the other end, the five unhappiest states were:\n\nDelving a little bit further, the top-five happiest cities amongst our users were:\n\nOn the flip side, the top-five unhappy cities were:\n\nI made an effort to correlate commute times in these cities with unhappiness, but the results were disappointingly weak. In a forthcoming post, I\u2019ll dig deeper into these current-job happiness questions by addressing cross-referencing the findings with other data we have on hand, e.g., how highly users rate individual aspects of their role like compensation, work-life balance, and responsibility."
    },
    {
        "url": "https://insights.untapt.com/openai-lab-for-deep-reinforcement-learning-experimentation-6287867eb611?source=user_profile---------5----------------",
        "title": "\u201cOpenAI Lab\u201d for Deep Reinforcement Learning Experimentation",
        "text": "With SantaCon\u2019s blizzard-y carnage erupting around untapt\u2019s Manhattan office, the members of the Deep Learning Study Group trekked through snow and crowds of eggnog-saturated merry-makers to continue our Deep Reinforcement Learning journey (quick intro to Deep RL available here).\n\nWe were fortunate to have the session be led by Laura Graesser and Wah Loon Keng. Laura and Keng are the human brains behind OpenAI Lab, a Python library for automating experiments that involve Deep RL agents exploring virtual environments. The agents can be active in environments provided by the Unity gaming engine, but their primary environments \u2014 and indeed the library\u2019s namesake \u2014 are those provided by OpenAI, a leading AI research outfit with prominent backers.\n\nGiven the richness of Keng and Laura\u2019s experience with Deep RL and our study-group members\u2019 proclivity for question-asking, we held this session on a Saturday. This weekend workshop format enabled us to enjoy two hours of theory discussion with a further hour spent on an illustrative demo of the OpenAI Lab. My favourite elements of the library were:\n\nBroadly, the Deep RL topics we covered (slides here) were:\n\nFor our next session, we\u2019ll continue deepening our understanding of RL theory by studying along with Sergey Levine\u2019s Fall 2017 lectures at UC Berkeley. My detailed notes on the early talks from this course are in GitHub here and my notes on Serena Young\u2019s Deep Reinforcement Learning lecture from Stanford\u2019s CS231n Summer 2017 offering are here. Watch this space for ongoing updates as we progress."
    },
    {
        "url": "https://insights.untapt.com/deep-reinforcement-learning-our-prescribed-study-path-52b959a61f76?source=user_profile---------6----------------",
        "title": "Deep Reinforcement Learning: Our Prescribed Study Path",
        "text": "At our Deep Learning Study Group\u2019s most recent session (detailed notes available in GitHub here), we began greedily consuming introductory resources on Deep Reinforcement Learning (DRL), a rousing area of research that combines together:\n\nDRL has made a splash in the popular press over the past eighteen months for staggering advances, particularly:\n\nWe initially hoped to learn about DRL from Emma Brunskill by following the lectures and course materials from her CS234 curriculum at Stanford. I corresponded with teaching assistants from the course, who informed me that unlike CS231n or CS224n, CS234 video streams will not be released publicly (participants in the class decided democratically not to endure the hassle of the requisite, individual release-form paperwork that would facilitate this). Instead, we began to get our bearings by:\n\nWe found the former was well-suited to us, but the latter became too technical too quickly for our level of na\u00efvety with DRL subject matter.\n\nIn addition to discussing the above content, we thoroughly enjoyed Thomas Balestri\u2019s in-person exposition of the Reinforcement Learning chapter (Chapter 16) from Aur\u00e9lien G\u00e9ron\u2019s Hands-on Machine Learning book. Dr. Balestri kindly provided an overview of the chapter as well as live run-throughs of corresponding Jupyter notebooks.\n\nFor our upcoming study group session, we will continue to explore resources in our environment, but we will also begin to exploit them. The recommended preparatory work is:"
    },
    {
        "url": "https://insights.untapt.com/deep-learning-for-natural-language-processing-tutorials-with-jupyter-notebooks-ad67f336ce3f?source=user_profile---------7----------------",
        "title": "Deep Learning for Natural Language Processing: Tutorials with Jupyter Notebooks",
        "text": "At untapt, all of our models involve Natural Language Processing (NLP) in one way or another. Our algorithms consider the natural, written language of our users\u2019 work experience and, based on real-world decisions that hiring managers have made, we can assign a probability that any given job applicant will be invited to interview for a given job opportunity.\n\nWith the breadth and nuance of natural language that job-seekers provide, these are computationally complex problems. We have found deep learning approaches to be uniquely well-suited to solving them. Deep learning algorithms:\n\nTo share my love of deep learning for NLP, I have created five hours of video tutorial content paired with hands-on Jupyter notebooks. Following on from my acclaimed Deep Learning with TensorFlow LiveLessons, which introduced the fundamentals of artificial neural networks, my Deep Learning for Natural Language Processing LiveLessons similarly embrace interactivity and intuition, enabling you to rapidly develop a specialization in state-of-the-art NLP.\n\nThese tutorials are for you if you\u2019d like to learn how to:\n\nBelow is a summary of the topics covered over the course of my five Deep Learning for NLP lessons (full breakdown detailed in my GitHub repository):\n\nLesson One: Introduction to Deep Learning for Natural Language Processing"
    },
    {
        "url": "https://medium.com/@jjpkrohn/build-your-own-deep-learning-project-via-my-30-hour-course-e995ea025ba8?source=user_profile---------8----------------",
        "title": "Build Your Own Deep Learning Project via my 30-Hour Course",
        "text": "Build Your Own Deep Learning Project via my 30-Hour Course\n\nI\u2019m delighted to announce that I\u2019m offering a Deep Learning course at the NYC Data Science Academy, starting on October 14th. See all the details, including the full curriculum here.\n\nSpread over five Saturdays until mid-December, the course is an introduction to artificial neural networks that brings high-level theory to life with interactive labs featuring TensorFlow, the most popular Deep Learning library.\n\nEssential theory will be covered in a way that provides an intuitive understanding of Deep Learning\u2019s underlying foundations. Paired with hands-on code demos in Jupyter notebooks as well as strategies for overcoming common pitfalls, this foundational knowledge will empower individuals with no previous understanding of neural networks to build a production-ready Deep Learning applications.\n\nIndeed, I\u2019ll help you build your own specific Deep Learning project from conception through to completion. Together, we\u2019ll engage in the following five stages over the course of the, um, course:\n\nMy demo of this course last week proved popular, filling the venue (perhaps it was the wood-fired Waldy\u2019s Pizza that was provided to attendees). To get a sense of the content we discussed, here\u2019s a video from an introductory Deep Learning talk I gave to the NYC Open Data Meetup in February:\n\nI\u2019m excited to get started. I can\u2019t wait to share my unbridled enthusiasm for Deep Learning and to see the projects that everyone builds! If this excites you too, you can use the promotional code LearnWell on the course page when you sign up to get 10% off."
    },
    {
        "url": "https://insights.untapt.com/deep-learning-with-tensorflow-introductory-tutorials-with-jupyter-notebooks-23970fee6b06?source=user_profile---------9----------------",
        "title": "\u201cDeep Learning with TensorFlow\u201d Introductory Tutorials with Jupyter Notebooks",
        "text": "If you\u2019re a software engineer, data scientist, analyst, or statistician and you\u2019d like to:\n\n\u2026then my interactive Deep Learning with TensorFlow lessons may be a resource ideally-suited to you. The video tutorial, published on Pearson\u2019s Addison-Wesley imprint and available in Safari as of last week, focuses on providing an intuitive understanding of Deep Learning theory through hands-on Jupyter notebooks.\n\nOver the course of six hours, we gradually grow the \u201carsenal\u201d of tools available to you. Starting with example code for simple neural networks in the most popular Deep Learning library, TensorFlow (and its high-level API Keras), by the end of the lessons we are developing state-of-the-art Deep Learning architectures akin to those that underlie the bulk of the contemporary \u201cmachine intelligence\u201d spectrum \u2014 whether it be self-driving cars, voice recognition, or crushing humans at the game of Go."
    },
    {
        "url": "https://insights.untapt.com/how-to-understand-how-lstms-work-a5934e9d602d?source=user_profile---------10----------------",
        "title": "How to Understand How LSTMs Work \u2013",
        "text": "When modelling data that have an inherent sequential structure to them, such as the sequence of words in language or the sequence of milliseconds in financial market data, your first choice from the universe of Deep Learning algorithms is typically going to be a Recurrent Neural Network.\n\nRNNs enable information from previous time steps to influence the present one. The problem with vanilla RNNs, however, is that the influence quickly drops off. While the preceding step (e.g., the previous word or market-feed update) can have a great impact on the current time step, the information from, say, ten steps (ten words or ten market-feed updates) earlier is limited to having minimal or negligible impact on the current time step.\n\nThe prevailing solution to this vanishing gradient issue is to add gates to individual RNN units, enabling the important information (e.g., a verb or market-movement-predicting signal) from previous steps to be retained, while less consequential information (like a stop word or a boring market update) from previous steps is forgotten.\n\nThe most popular gated RNN unit is the Long Short-Term Memory unit. LSTMs were first described in 1997, but only since last year did the confluence of terabyte-sized training-data sets, efficient matrix multiplication on GPUs, and readily-scalable, low-cost (cloud) computing enable the utility of these units to outperform the heavyweight, rules-based algorithms that previously predominated.\n\nIn the blink of an eye, Deep Learning models incorporating LSTMs are everywhere. As reported by the New York Times, the Google Translate team overnight dropped decades of research and development on heavyweight machine-translation models in favour of the lightweight and unanticipatedly potent LSTMs. Any consumer devices you speak to (e.g., Apple\u2019s Siri, Amazon\u2019s Echo, Microsoft\u2019s Cortana) incorporated LSTM models to improve their speech recognition accuracy from 95% a couple years ago to 99% today. And, here at untapt, LSTMs help our natural language-processing algorithms exhibit a more human-like subtlety when they evaluate which jobs would be the most suitable fit for a given candidate\u2019s background."
    },
    {
        "url": "https://insights.untapt.com/filming-deep-learning-with-tensorflow-livelessons-for-oreilly-safari-50363ed4efad?source=user_profile---------11----------------",
        "title": "Filming \u201cDeep Learning with TensorFlow\u201d LiveLessons for Safari Books Online",
        "text": "My first video tutorial series \u2014 Deep Learning with TensorFlow \u2014 is slated to be released in the Safari platform in August. These videos will consist primarily of code walkthroughs (notebooks available for free in GitHub today) bolstered by whiteboards filled with essential deep learning theory and illustrative slides.\n\nOver the course of five hours, the \u201carsenal\u201d of tools available to the viewer grows gradually, facilitating an intuitive understanding of deep learning piece by piece. Starting with example code for simple neural networks in TensorFlow (and its high-level API Keras), by the end of the LiveLessons we are developing state-of-the-art deep learning architectures akin to those that underlie the bulk of the contemporary \u201cmachine intelligence\u201d spectrum \u2014 whether it be self-driving cars, voice recognition, or crushing humans at the game of Go."
    },
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-xi-recurrent-neural-networks-including-grus-and-lstms-22c17fa36deb?source=user_profile---------12----------------",
        "title": "Deep Learning Study Group XI: Recurrent Neural Networks, including GRUs and LSTMs",
        "text": "In our previous two sessions, we studied state-of-the-art mathematical and programming approaches for converting the free-form, natural language created by humans into a numeric representation that machines can ingest to identify patterns. Specifically, we dove into the nitty-gritty of the word2vec and GloVe algorithms. Once we have generated these numeric representations, we can leverage deep learning neural networks to build statistical models that perform at near-human or super-human accuracy on tasks like sentiment analysis and translation between languages. At our meeting last week (detailed notes here ), we focused on Recurrent Neural Networks (RNNs), which are a family of deep learning neural network architectures that are particularly adept at processing sequences of data like written or spoken language.\n\nThere are variants of Recurrent Neural Networks that deserve special mention and that garnered the lion\u2019s share of our attention, namely:\n\nWhile vanilla RNNs treat all elements of a sequence with equal weight, both GRUs and LSTMs possess the capability to \u201cremember\u201d or \u201cforget\u201d elements. This is powerful because it enables distant elements of a sentence or paragraph to influence each other. As a concrete example provided by Richard Socher in the eighth of his lectures for Stanford\u2019s CS224d (2017) course, GRUs and LSTMs can accurately complete the following paragraph, but vanilla RNNs cannot:\n\nJane walked into a room. John walked in too. It was late in the day. Jane said hi to __."
    },
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-10-word2vec-mania-generative-adversarial-networks-80922e962d1?source=user_profile---------13----------------",
        "title": "Deep Learning Study Group 10: word2vec Mania + Generative Adversarial Networks",
        "text": "The tenth iteration of our Deep Learning Study Group took place in late March. The recommended preparatory work and our detailed notes from the session can be found in our GitHub repository . In the session, we continued our progress through Salesforce Chief Scientist Richard Socher\u2019s CS224d course, which he teaches at Stanford, and is focused on Deep Learning applied to Natural Language Processing. Broadly, this topic involves leveraging techniques popularly identified as Artificial Intelligence to ingest human language (be it written or spoken) and process it in some useful way, such as:\n\nThe particular focus of this session was word2vec, an algorithm for taking a large body of text (e.g., all of Wikipedia, or all of the job applications that we receive at untapt) and deriving a location for each of the words within a high-dimensional (say, one hundred thousand-dimensional) vector space. In this space, words that are more closely related have a similar meaning. Mindblowingly, the dimensions of the vector space can represent concepts that are meaningful to humans (e.g., gender, verb tense, country-capital pairs, emotion) such that we can traverse the space mathematically and move gradually and accurately between the abstractions stored within language. The most famous example of this is that if we subtract the vector-space location for man from king and then add the vector-space location for woman, we end up at the word queen. That is: king \u2014 male + female = queen. This effect applies to more specific concepts as well, such as Mark Zuckerberg \u2014 Facebook + Amazon = Jeff Bezos.\n\nIn addition to covering this CS224d content and discussing tricks for optimising deep neural networks for natural language processing, we enjoyed perfectly topical, highly-interactive presentations from:"
    },
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-ix-natural-language-processing-ai-in-fashion-and-u-net-1a4726037806?source=user_profile---------14----------------",
        "title": "Deep Learning Study Group IX: Natural Language Processing, AI in Fashion, and U-Net",
        "text": "Session numero nine of our Deep Learning Study Group was held in early March and is detailed in our GitHub notes. In the session, we began reviewing Stanford\u2019s CS224d course, which is taught by Salesforce Chief Scientist Richard Socher and focuses on Deep Learning applied to Natural Language Processing. NLP, for those outside the field, has been a long-term focus of Artificial Intelligence researchers and involves converting the free-form (written or spoken) language of humans into a structured format that can be processed by machines.\n\nSince 2012, Deep Learning has emerged as a popular approach for enabling machines to recognise patterns in natural language. As a result, software programs suddenly possess human or near-human capability in previously confounding tasks such as:"
    },
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-viii-unsupervised-learning-regularisation-and-venture-capital-9aba67fc931c?source=user_profile---------15----------------",
        "title": "Deep Learning Study Group VIII: Unsupervised Learning, Regularisation, and Venture Capital",
        "text": "As detailed by my notes in our study group\u2019s GitHub repo here, last month we convened to wrap up our coverage of Stanford\u2019s CS231n. This course, led by Fei-Fei Li, but primarily taught in 2016 by Andrej Karpathy and Justin Johnson, focused on the Deep Learning algorithms that enable contemporary machine-vision applications like self-driving cars and the face recognition tools now commonplace in, for example, Apple\u2019s operating systems and Facebook. The most amusing use of these approaches I\u2019ve come across yet is the \u201cQuick, Draw!\u201d game \u2014you can play it for free here.\n\nThis was our third of three sessions covering CS231n, with our attention turning to the final few lectures of the course. We primarily explored:"
    },
    {
        "url": "https://insights.untapt.com/fundamental-deep-learning-code-in-tflearn-keras-theano-and-tensorflow-66be10a03227?source=user_profile---------16----------------",
        "title": "Fundamental Deep Learning code in TFLearn, Keras, Theano and TensorFlow",
        "text": "Yesterday evening, after years of attending the Open Statistical Programming Meetup in New York, I had the honour of giving a talk to the venerable institution on The Fundamentals of Deep Learning, replete with applications of the approach.\n\nMy full slides from the evening are available here. I leapt through the history of machine vision and natural language processing, then provided just enough of Deep Learning\u2019s underlying theory to facilitate intuition, leaving the focus of the hour on concrete implementations \u2014 e.g., Jupyter notebooks!\n\nTime and again, I referred back to this table of popular Deep Learning libraries:\n\nWith these relative strengths and weaknesses in mind, I used the high-level TensorFlow API TFLearn to demonstrate three Deep Learning models:\n\nSubsequently, to illustrate the added complexity (read: flexibility and functionality) of TensorFlow proper, I provided:\n\nEmploying TensorFlow\u2019s chief rival Theano, I elucidated the pros and cons of varying key attributes of deep nets:\n\nFinally, I explained that, at untapt, our preference is to build deep neural networks in Keras, a high-level API that (a.) calls on either TensorFlow or Theano while (b.) simultaneously offering more functionality than TFLearn:"
    },
    {
        "url": "https://insights.untapt.com/introductory-talk-on-deep-learning-a2de2f0a951e?source=user_profile---------17----------------",
        "title": "Introductory Talk on Deep Learning \u2013",
        "text": "The field of Machine Learning, which encompasses Deep Learning, sits at the intersection of statistics and computer science. It was unsurprising, then, that a sizeable portion of the attendees at the Deep Learning with Artificial Neural Networks talk I gave last week at Wilfrid Laurier University were students and faculty from those departments.\n\nWhile at times touching on theory and resources that were directly relevant to this academic audience, the primary purpose of the talk was to provide an introduction to deep learning that would resonate with any inquisitive party and so it assumes no prior expertise.\n\nDeep-learning algorithms are increasingly ubiquitous in our lives, enabling features as diverse as Facebook\u2019s face recognition, Tesla\u2019s Autopilot, Google Inbox\u2019s suggested replies, and Siri\u2019s voice recognition.\n\nDespite their broad real-world utility, some of these artificial intelligence techniques are not tremendously complex to grasp at a high level. With this in mind, the talk begins by delving into a case study on machine vision while evoking tangible analogies to the human visual experience, including parallels to biological neurons, the organisation of visual processing in mammals, and the evolution of vision in nature.\n\nThe second section of the talk introduces artificial neurons, the building block of deep learning approaches, and how they can be layered together to form neural networks. It continues by describing some of the fundamental emergent properties of these networks, including common issues that arise and approaches for mitigating them in practice.\n\nIn the third section, I described some of the most widespread varieties of deep neural networks with reference to specific applications. Namely, I covered:\n\nThe talk concluded with my theories on where machine intelligence techniques, including Deep Learning, may be headed, with discussion of the impact that this may have on society."
    },
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-6-a-history-of-machine-vision-6786d891b88c?source=user_profile---------18----------------",
        "title": "Deep Learning Study Group #6: A History of Machine Vision",
        "text": "A fortnight ago, our Deep Learning Study Group held a particularly fun and engaging session.\n\nThis was our first meeting since completing Michael Nielsen\u2019s Neural Networks and Deep Learning text. Nielsen\u2019s work provided us with a solid foundation for exploring more thoroughly the convolutional neural nets that are the de facto standard in contemporary machine-vision applications.\n\nIn Session 6, we dug into the first third of the lectures from the much-hyped Convolutional Neural Networks for Visual Recognition course led by Stanford\u2019s illustrious Fei-Fei Li, Andrej Karpathy and Justin Johnson.\n\nWith respect to theory, we covered:\n\nIn addition to theory, colourful study group member Dmitri Nesterenko, who is Director of Software Engineering at the XO Group downtown, went into considerable, helpful detail describing his adventures writing a k-Nearest Neighbours implementation from scratch with the NumPy library.\n\nFinally, with valuable suggestions from many folks in the room, we crafted a rough plan of the subject matter we\u2019ll be covering in future sessions:"
    },
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-5-how-deep-convolutional-neural-networks-work-and-how-to-improve-them-3d0db8c34f8f?source=user_profile---------19----------------",
        "title": "Deep Learning Study Group #5: How Deep Convolutional Neural Networks Work and How to Improve Them",
        "text": "On Thursday evening, our Deep Learning Study Group convened for the fifth time. The recommended preparatory work was reading the final chapter of Michael Nielsen\u2019s introductory text on neural networks. This particular chapter was focused on using convolutional neural nets for classifying images, e.g., the MNIST digits \u2014 or pretty well anything else!\n\nWe covered the three key properties of convolutional neural nets, i.e.:\n\nIn particular, we noted how these properties in concert facilitate the location-invariant detection of image features (e.g., edges, curves). In turn, feeding convolutional-pooling layers into a dense (fully-connected) layer enables the individual feature representations to be assimilated into more complex representations (e.g., the digit \u201c8\u201d, or \u2014 in deeper networks \u2014 a school bus, or a cat\u2019s face).\n\nWe went on to discuss architecture changes that can improve the image-classification accuracy of ConvNets. These include:\n\nI committed a Jupyter notebook to GitHub here to illustrate in detail how you can implement these improvements with Theano. Outputs include classification accuracy after each epoch of training.\n\nGiven the issues discussed in previous sessions of our study group (particularly the vanishing gradient problem), how does ConvNet training work? Well:\n\nIn addition to ConvNets, we touched on other popular deep neural network architectures:\n\nWe also discussed implementations of TensorFlow for Poets, which makes it trivial to leverage the powerful neural net image-classification architecture of Inception v3. A number of us worked through its Dockerized demo, with special mention to study-group member Thomas Balestri who quickly trained it into an image-classification tool for consumer products.\n\nWith this session, we completed Nielsen\u2019s textbook! Up next for us are the notes and lectures from the Stanford CS231n (Convolutional Neural Networks for Visual Recognition) class.\n\nFor curated data science resources, including suggested paths for getting started with deep learning, visit my site."
    },
    {
        "url": "https://insights.untapt.com/fundamentals-of-deep-learning-talk-to-data-science-fintech-meetup-2c3199bf363f?source=user_profile---------20----------------",
        "title": "\u201cFundamentals of Deep Learning\u201d talk to Data Science + FinTech Meetup",
        "text": "On Wednesday, I had the joy of presenting to a highly-engaged audience at the Data Science + FinTech meetup. From the starting gun, the sharp attendees quizzed me with insightful questions and thoughtful commentary on the topic of Deep Learning with Neural Networks, expanding the sixty-minute talk to nearly two hours.\n\nAll in all, we covered:\n\nI\u2019ve uploaded my slides from the evening here. Many thanks to Mansi Singhal and her team at qplum for their invitation and for bringing together such a bright community.\n\nFor curated data science resources, including suggested paths for getting started with deep learning, visit my site."
    },
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-session-4-proofs-of-key-neural-net-properties-4213b46a561c?source=user_profile---------21----------------",
        "title": "Deep Learning Study Group Session #4: Proofs of Key Neural Net Properties",
        "text": "Yesterday evening, untapt hosted yet another session of our Deep Learning Study Group.\n\nOur recommended preparatory reading this time around was the fourth and fifth chapters of Michael Nielsen\u2019s interactive textbook on neural networks. We took a break from applying techniques for this one session to speed our progress through the textbook theory, but we\u2019ll get back on track for the next session. Below are my notes on what we covered.\n\nNeural nets can compute any function (i.e., they are universal), assuming that (1) we accept they are an approximation (that can be improved by the inclusion of additional hidden neurons) as opposed to an exact solution, and (2) the function they are explaining is continuous (e.g., has no sharp jumps).\n\nFor the first time in our study session, we moved from whiteboarding to a projector to cover this content. This is because, in his fourth chapter, Michael Nielsen did a tremendous job of developing thematically-coherent, interactive Java applets that facilitate a clear visual understanding of this proof, and we wanted to be able to talk through these demonstrations.\n\nA fair bit of our discussion centered on the practicalities of expanding the proof beyond two inputs features into n-dimensional space.\n\nWe primarily discussed the causes, implications, and methods to mitigate unstable gradients, which in deep neural nets tend to vanish but in some cases can instead explode.\n\nWe also touched on other factors that can make deep nets difficult to train, e.g., the propensity for sigmoids to saturate, the risks of random weight initialization.\n\nThomas Balestri introduced us to Jason Yosinski\u2019s breathtaking Deep Visualization Toolbox for developing an understanding of how individual layers contribute to a convolutional NN.\n\nAt our next meeting, we\u2019ll discuss finishing off Nielsen\u2019s textbook and working through a TensorFlow implementation of a convolutional neural net that was built for visual recognition.\n\nFor curated data science resources, including suggested paths for getting started with deep learning, visit my site."
    },
    {
        "url": "https://medium.com/@jjpkrohn/three-themes-of-seth-moultons-inaugural-term-visualized-with-data-2d21b7b0df43?source=user_profile---------22----------------",
        "title": "Three Themes of Seth Moulton\u2019s Inaugural Term, Visualized with Data",
        "text": "Three Themes of Seth Moulton\u2019s Inaugural Term, Visualized with Data Leveraging statistical techniques for investigating trends in data, three major themes from Congressman Seth Moulton\u2019s first term in office emerge. First, Rep. Moulton displayed persistent leadership on complex domestic issues. Second, he was a central figure to several key international events. And, third, the former marine succeeded in passing bills with bipartisan support despite an unprecedentedly polarized U.S. House of Representatives. A network of news articles mentioning Seth Moulton, with related articles clustered together in colored groups. Image produced by Jon Krohn using Quid. My exploratory analysis began by generating clusters of related newspaper articles. From August 2013 through October 2016, there were 2168 items published by the American print media that mention \u201cSeth Moulton\u201d. In the diagram above, each point represents a unique article, and items with similar content are connected by a line. The shorter the distance between articles in the diagram, the more similar their subject matter is. An algorithm categorizes adjacent articles into thematic groups, which are shown as different colors. As examples, the prominent keywords in the eight largest clusters are: To begin to draw meaningful structure from these clustered data, it\u2019s instructive to chart the articles on a timeline:\n\nThis analytical method is optimized for long sections of written, natural language, so I\u2019ve limited the data to full-length newspaper articles to the exclusion of, for example, video and audio formats. This approach is therefore biased in favor of content from national print media conglomerates and effectively ignores Seth\u2019s regular contributions to local television and radio. Three sets of major clusters represent Seth\u2019s ongoing political efforts on domestic efforts. Articles from these clusters are spread relatively evenly across his inaugural two-year term. There were 260 articles related to Rep. Moulton\u2019s work on the House Armed Services Committee, his efforts to improve funding for Veterans Affairs health networks, and his strategies for handling terrorism:\n\nThere were also 136 articles involving firearm legislation (yellow) and July\u2019s shooting in Orlando (red). The yellow blocks in 2015 and early 2016 demonstrate Moulton discussing gun laws throughout his term, with a small burst of media activity in January. The tragedy in Florida generated the pronounced column of red-colored articles on June 12th. The yellow column that immediately follows is comprised of national articles covering Democrats\u2019 sit-in on gun violence legislation, which Seth participated in, and his New York Daily News cover story advocating for an assault rifle ban:\n\nOn top of ongoing legislative work, Seth received broad coverage related to exceptional events that played out on the international stage during his inaugural term. As in Theme 1, three sets of clusters are illuminating here. Seth championed support for refugees fleeing Middle Eastern conflicts, particularly the Syrian civil war (teal cluster). This included conspicuously inviting nine-year-old refugee Ahmad Alkhalaf to be his guest at the State of the Union address on January 12th (green). In aggregate, there were 95 articles across the two clusters:\n\nThirdly, there was a media splash around Barack Obama\u2019s historic visit to Cuba, the first by a sitting U.S. President since the 1950s. Seth participated as a member of the trip\u2019s Congressional delegation, and 25 articles on the topic were published over three days in March. Last year, Clio Andris and her colleagues published an expounding analysis of the ever-more-polarized U.S. House. The \u201cexponentially\u201d increasing partisanship from 1949 through 2012 is made stark by their network diagram in which red points denote Republican representatives, blue points denote Democrats, and a shorter distance between points relates to closer agreement on Congressional votes: Image from Andris et al. (2015) in PLoS ONE 10(4): e0123507 It would not be surprising if a 2015\u2013\u201916 update of this analysis showed Rep. Moulton as one of the rare nodes bridging the otherwise sharply-delineated blue and red clusters of recent sessions. Seth has been the second-most productive freshman Democrat in the 114th session, with two bills he sponsored passing the Republican-controlled House. Bucking the partisan trend, the first bill was co-sponsored by two Republicans and three Democrats, while the second was co-sponsored by seven Rs and sixteen Ds. Rep. Moulton is running unopposed in next month\u2019s election so I am anticipating his remarkable across-the-aisle effectiveness will continue through 2018. Given the trends from the current session, I expect this will involve a blend of ongoing leadership on domestic matters and growing influence around pivotal international events. Thank you to Quid for kindly providing their software for both data collection and analysis, as well as to Peter McCarthy for outstanding support on the tool, and to Christian Urrutia for the introduction to Seth in the Spring of 2014. For curated data science resources, visit my site here."
    }
]