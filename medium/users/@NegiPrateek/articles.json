[
    {
        "url": "https://medium.com/@NegiPrateek/starting-kafka-cluster-as-a-single-docker-service-a-fallacy-90736254e02c?source=user_profile---------1----------------",
        "title": "Starting Kafka Cluster As a Single Docker Service \u2014 A Fallacy ?",
        "text": "In the previous story, we created a Kafka cluster with 3 brokers. In that example, we setup each broker as a separate docker service targeted on specific nodes and exposing different ports.\n\nNext question is: Instead of setting each Kafka broker as separate docker service, can we set up Kafka broker as single service replicated in all swarm nodes?\n\nHere is the docker-compose.yml for that setup:\n\nLet\u2019s go through the details:\n\n2. Let\u2019s look at the more interesting definition: Kafka service\n\nLet\u2019s walk through what happens when we run the docker stack\n\nLet\u2019s test is out by starting it using Docker Stack.\n\nThe stack successfully starts the Zookeeper and Kafka service."
    },
    {
        "url": "https://medium.com/@NegiPrateek/wtf-setting-up-kafka-cluster-using-docker-stack-5efc68841c23?source=user_profile---------2----------------",
        "title": "WTF: Setting up Kafka Cluster using Docker Stack \u2013 Prateek \u2013",
        "text": "In the previous story Setting up Kafka Cluster using Docker Swam , we went through the journey of setting up a Kafka Cluster using Docker Swarm. That was most of the heavy lifting done there. We will do the same exercise using Docker Stack\n\nDocker Stack provides a configure docker swarm using the docker-compose files. You do not have to worry about issuing the individual docker swarm commands. Docker stack provides functionality over docker swarm the same way docker compose provides over core docker commands.\n\nThe details of the setup are provide in the previous story."
    },
    {
        "url": "https://medium.com/@NegiPrateek/wtf-setting-up-kafka-cluster-using-docker-swarm-6429bdb5784b?source=user_profile---------3----------------",
        "title": "WTF: Setting up Kafka cluster using Docker Swarm \u2013 Prateek \u2013",
        "text": "First, we will attempt to setup kafka cluster using docker swarm commands, so that we can understand how do we compose them.\n\nLet us expand on this:"
    },
    {
        "url": "https://towardsdatascience.com/statistics-is-freaking-hard-wtf-is-activation-function-df8342cdf292?source=user_profile---------4----------------",
        "title": "Statistics is Freaking Hard: WTF is Activation function",
        "text": "As with anything, I am not describing anything unique. There are lot of people who have approached this topic better than I can ever do. This story is for that other person in my head \ud83d\ude2c. I do talk to him often and he suddenly expressed interest in statistics and machine learning.\n\nSo, what is activation function? The neurons in the neural network are loosely modeled on our brain neurons. Aah ! Now, I see why it is named the same. The neurons in our brain fire based on inputs, which somehow makes us intelligent !!! Activation function in neurons of neural network is the function which decides whether the neuron should fire and with what intensity.\n\nLet\u2019s walk through different commonly used activation functions and their characteristics.\n\nThis is how a neuron looks like\n\nThe input value to the activation function will be between -\u221e and \u221e"
    },
    {
        "url": "https://towardsdatascience.com/statistics-is-freaking-hard-wtf-is-time-series-part-3-5ffdf77c52fb?source=user_profile---------5----------------",
        "title": "Statistics Is Freaking Hard : WTF Is Time Series \u2014 Part 3",
        "text": "In this part, I will be trying to explain some basic concepts for exponential smoothing. So before we delve deeper, let\u2019s try to understand our journey to exponential smoothing.\n\nIn part 1, we started with very simple model, where we gave all the previous observations equals weights, which is basically taking average of all previous observations. That meant that all the previous observation influenced the next predicted value equally.\n\nIn part 2, we moved to other extreme where we considered only n previous observation equally influencing the next result. We can further move the model to extreme where the last value is the predicted next value i.e. n = 1\n\nLet try to consider a middle ground, where later observations exponentially influence compared to earlier observations. This is where exponential smoothing comes into picture. In this part, we would be discussing simple exponential smoothing, also called single exponential smoothing.\n\nThe equation for simple exponential smoothing is:\n\nSn+1 = \ud835\udefcYn + (1-\ud835\udefc)Sn\n\nwhere, \n\nSn \u2192 predicted value of n\u1d57\u02b0 instance,\n\nYn \u2192 actual observed value of n\u1d57\u02b0 instance\n\nSn+1 \u2192 predicted value of n+1\u1d57\u02b0 instance \n\n\ud835\udefc \u2192 smoothing parameter (0 \u2264 \ud835\udefc \u2264 1)\n\nNow, this equation does not seems to be exponential. Lets expand this equation to try to understand its exponential nature\n\nSn+1 = \ud835\udefcYn + (1-\ud835\udefc)Sn \n\nSn+1 = \ud835\udefcYn + (1-\ud835\udefc)(\ud835\udefcYn-1 + (1-\ud835\udefc)Sn-1) \u2192 expand Sn \n\nSn+1 = \ud835\udefcYn + \ud835\udefc(1-\ud835\udefc)Yn-1 + (1-\ud835\udefc)\u00b2Sn-1\n\nSn+1 = \ud835\udefcYn + \ud835\udefc(1-\ud835\udefc)Yn-1 + (1-\ud835\udefc)\u00b2(\ud835\udefcYn-2 + (1-\ud835\udefc)Sn-2) \u2192 expand Sn-1\n\nSn+1 = \ud835\udefcYn + \ud835\udefc(1-\ud835\udefc)Yn-1 + \ud835\udefc(1-\ud835\udefc)\u00b2Yn-2 + (1-\ud835\udefc)\u00b3Sn-2\n\nWe can continue to expand till we reach S2, which is basically Y1\n\nSn+1 = \ud835\udefcYn + \ud835\udefc(1-\ud835\udefc)Yn-1 + \ud835\udefc(1-\ud835\udefc)\u00b2Yn-2 + \ud835\udefc(1-\ud835\udefc)\u00b3Yn-3 \u2026.. + \ud835\udefc(1-\ud835\udefc)\u207fY1\n\nNow, you can see the exponential nature of the equation. The weights are basically geometric progression. The summation of all the weights, believe me, is 1. The weights value exponentially reduces and hence exponential smoothing.\n\nThe value of \ud835\udefc which is between 0 and 1 determines the how the dampening of influence occurs. When \ud835\udefc is closer to 1, quicker is the dampening. When \ud835\udefc is closer to 0, slower is the dampening.\n\nThe observed values are in green. The predicted values are in purple. As you see simple exponential smoothing predicts better than the models in the previous posts.\n\nMaybe in the next post, I would address double and triple exponential smoothing."
    },
    {
        "url": "https://medium.com/@NegiPrateek/statistics-is-freaking-hard-wtf-is-time-series-part-2-e9c5d2e72564?source=user_profile---------6----------------",
        "title": "Statistics Is Freaking Hard : WTF Is Time Series \u2014 Part 2",
        "text": "I started this arduous journey in my previous post of explaining to myself and my friends my understanding of time series. I proudly shared the post with my friends and found that I miserably at explaining to them. In this post, the concepts become more complex. I am confident that this time I will fail to explain to my other audience: me.\n\nIn the previous post, we gave all the previous observations equal weights to predict the next value.\n\nThis means that all previous observations have equal influence on the next predicted value.\n\nLet\u2019s change the model such that later values have more influence that older values. That means the later values are given more weight than older values.\n\nWe can have different models, based on how we distribute the weight on the observations. One thing to note is that the sum of all weights is 1.\n\nOne model is to distribute the weight such that the weights are exponential in nature. The earlier weights have almost zero value and later weights have exponentially higher value. This method is called exponential smoothing. We will delve into this later, because as with any statistical topic it is a rabbit hole.\n\nAnother model could be to give last 3 observations 1/3 weight and all the other observations 0 (zero) weight. We are saying the values of the last 3 observations only influences the next predicted value. Each of the last 3 values influence the next predicted value equally. All the other older values have no influence in the next predicted value.\n\nThis model is referred as Single Moving Average.\n\nThis is how Single Moving Average would predict\n\nThe observed values are in black. The predicted values are in blue. If you look at it, then the predicted values follow closely to the observed value.\n\nAdding constant value makes the predicted value look much better\n\nThe observed values are in black. The predicted values are in orange. If you look at it, then the predicted values follow very closely to the observed value.\n\nIn the next post\u2026Yes ! We have not done here. This is just the tip of the iceberg. In the next post, we will be looking at exponential smoothing models."
    },
    {
        "url": "https://towardsdatascience.com/statistics-is-freaking-hard-wtf-is-time-series-part-1-22a44300c64b?source=user_profile---------7----------------",
        "title": "Statistics Is Freaking Hard : WTF Is Time Series \u2014 Part 1",
        "text": "Let\u2019s consider that we have data for sales for various time instances: T\u2080, T\u2081, T\u2082, T\u2083..Tn.\n\nLet\u2019s consider the most simplistic model to predict the value in any time instance T. The predicted value for any time instance T is the average of all observed values.\n\nThis model would suffice for an uninteresting data, where the data fluctuates around a value.\n\nThe observed values are in black and the predicted values are in red.\n\nIn this example, the value fluctuates around 20. The flat line is the average of all the observed values.\n\nThis approach does not work if the data is linear (but not flat in nature).\n\nThe observed values are in black. The predicted values are in red.\n\nThe observed value seem to have linear pattern but predicted value follow the flat average line\n\nWhat can we do to improve our model? So instead of predicted value being the average of all values, let\u2019s change the predicted value to be the average of observed values so far. In this case, the predicted value of next time instance n+1 is average of all n observations so far.\n\nThe observed values are in black and the predicted values are in red.\n\nThe predicted values drags behind the observed values. It definitely works better than previous model.\n\nOne way to think of this model is that average weighs all past observations equally. All observed value are given equal weight of 1/n to predict our value for the next time instance n+1.\n\nSince the prediction only takes into account the previous observations or predictions, let add some error to reduce the lag.\n\nThe observed values are in black. The previous predicted values are in red. The current predicted value with the constant added is in green.\n\nAdding the error constant definitely makes the predicted values look better."
    }
]