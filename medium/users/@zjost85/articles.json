[
    {
        "url": "https://towardsdatascience.com/sharing-your-sagemaker-model-eaa6c5d9ecb5?source=user_profile---------1----------------",
        "title": "Sharing Your SageMaker Model \u2013",
        "text": "AWS recently announced SageMaker, which helps you do everything from building models from scratch to deploying and scaling those models for use in production. This article will assume you already have a model running in SageMaker and want to grant access to another AWS account to make predictions with your model.\n\nAt a high level, you need to create a role that has access to invoke the endpoint, and then give the child account access to this role. For this, you\u2019ll need the 12-digit account id of the child account. Then, follow these steps:\n\nThis section will grant the ability to assume the role just created by the child account. You\u2019ll need the role arn from the last step above.\n\nNow that the permissions are configured, you\u2019ll need to be able to assume this role whenever you\u2019re trying to hit the endpoint. If you\u2019re using Boto3, the code will look something like this:"
    },
    {
        "url": "https://towardsdatascience.com/infogan-generative-adversarial-networks-part-iii-380c0c6712cd?source=user_profile---------2----------------",
        "title": "InfoGAN \u2014 Generative Adversarial Networks Part III \u2013",
        "text": "In Part I the original GAN paper was presented. Part II gave an overview of DCGAN, which greatly improved the performance and stability of GANs. In this final part, the contributions of InfoGAN will be explored, which apply concepts from Information Theory to transform some of the noise terms into latent codes that have systematic, predictable effects on the outcome.\n\nAs seen in the examples of Part II, one can do interesting and impressive things when doing arithmetic on the noise vector of the generator. In the example below from the DCGAN paper, the input noise vectors of men with glasses are manipulated to give vectors that result in women with sunglasses once fed into the generator. This shows that there are structures in the noise vectors that have meaningful and consistent effects on the generator output.\n\nHowever, there\u2019s no systematic way to find these structures. The process is very manual: 1) generate a bunch of images, 2) find images that have the characteristic you want, 3) average together their noise vectors and hope that it captures the structure of interest.\n\nThe only \u201cknob to turn\u201d to change the generator output is the noise input. And since it\u2019s noise, there\u2019s no intuition about how to modify it to get a desired effect. The question is: \u201cwhat if you wanted an image of a man with glasses \u2014 how do you change the noise?\u201d This is a problem because your representation is entangled. InfoGAN tries to solve this problem and provide a disentangled representation.\n\nThe idea is to provide a latent code, which has meaningful and consistent effects on the output. For instance, let\u2019s say you\u2019re working with the MNIST hand-written digit dataset. You know there are 10 digits, so it would be nice if you could use this structure by assigning part of the input to a 10-state discrete variable. The hope is that if you keep the code the same and randomly change the noise, you get variations of the same digit.\n\nThe way InfoGAN approaches this problem is by splitting the Generator input into two parts: the traditional noise vector and a new \u201clatent code\u201d vector. The codes are then made meaningful by maximizing the Mutual Information between the code and the generator output.\n\nThis framework is implemented by merely adding a regularization term (red box) to the the original GAN\u2019s objective function.\n\nLambda is the regularization constant and is typically just set to one. The I(c;G(z,c)) term is the mutual information between the latent code c and the generator output G(z,c).\n\nIt\u2019s not practical to calculate the mutual information explicitly, so a lower bound is approximated using standard variational arguments. This consists of introducing an \u201cauxiliary\u201d distribution Q(c|x), which is modeled by a parameterized neural network, and is meant to approximate the real P(c|x). P(c|x) represents the likelihood of code c given the generated input x. They then use a re-parameterization trick to make it such that you can merely sample from a user-specified prior (i.e. uniform distribution) instead of the unknown posterior.\n\nThe regularizer term above translates to the following process: Sample a value for the latent code c from a prior of your choice; Sample a value for the noise z from a prior of your choice; Generate x = G(c,z); Calculate Q(c|x=G(c,z)).\n\nThe final form of the objective function is then given by this lower-bound approximation to the Mutual Information:\n\nAs mentioned above, there\u2019s now a second input to the generator: the latent code. The auxiliary distribution introduced in the theory section is modeled by another neural network, which really is just a fully connected layer tacked onto the last representation layer of the discriminator. The Q network is essentially trying to predict what the code is (see nuance below). This is only used when feeding in fake input, since that\u2019s the only time the code is known.\n\nThere\u2019s one nuance here that can be difficult to understand. To calculate the regularization term, you don\u2019t need an estimation of the code itself, but rather you need to estimate the likelihood of seeing that code for the given generated input. Therefore, the output of Q is not the code value itself, but instead the statistics of the distribution you chose to model the code. Once you know the sufficient statistics of the probability distribution, you can calculate the likelihood.\n\nFor instance, if you have used a continuous valued code (i.e. between -1 and +1), you might model Q(c|x) as a Normal/Gaussian distribution. In that case, Q would output two values for this part of the code: the mean and standard deviation. Once you know the mean and standard deviation you can calculate the likelihood Q(c|x), which is what you need for the regularization term.\n\nInitial results were reported by training on the MNIST hand-written digit dataset. The authors specified a 10-state discrete code (hoping it would map to the hand-written digit value), as well as two continuous codes between -1 to +1. For comparison, they trained a regular GAN with the same structure but did not use the regularization term that maximizes the mutual information.\n\nThe images below show a process where a particular noise vector is held constant (each row), but the latent code is changed (each column). In part a you see the discrete code consistently changing the digit. Part b shows the regular GAN having essentially no meaningful or consistent change.\n\nParts c and d show the continuous codes being changed for InfoGAN. This clearly affects things like the tilt and width of the digit. Interestingly, they actually change from -2 to +2 even though training only used values from -1 to +1, which shows that these codes meaningfully extrapolate.\n\nHere are some results on face images. Please see the paper for additional results and explanation.\n\nIt\u2019s worth stressing that it was never specified in advance that tilt or digit thickness would be useful to separate as codes. The InfoGAN training procedure discovered these attributes by itself \u2014 i.e. in an unsupervised fashion. The only thing the research does is specify the structure of the latent code.\n\nWe\u2019ve seen that by simply adding a term that maximizes mutual information between part of the generator input and its output, the learning process disentangles meaningful attributes in the data and allocates them to this imposed latent code structure.\n\nI found the original repo difficult to run since its dependencies are very old. I\u2019ve updated the code so that you can run with modern Tensorflow APIs (version 1.3.0).\n\nIn this 3 part series we\u2019ve covered a few of the major contributions and seen GANs do amazing things. Even still, this barely scratches the surface. There are multiple github repos with a large and growing list of research papers. Here\u2019s one, and here\u2019s another. This is an exciting area of research that\u2019s only growing in maturity and effectiveness. If you find a particular paper you\u2019d like reviewed here, feel free to drop a note in the comments."
    },
    {
        "url": "https://towardsdatascience.com/generative-adversarial-networks-part-ii-6212f7755c1f?source=user_profile---------3----------------",
        "title": "Generative Adversarial Networks \u2014 Part II \u2013",
        "text": "In Part I of this series, the original GAN paper was presented. Although being clever and giving state of the art results at the time, much has been improved upon since. In this post I\u2019ll talk about the contributions from the Deep Convolutional-GAN (DCGAN) paper.\n\nPart I concluded with an enumeration of some problems with GAN. Chief among them was training stability. DCGAN makes significant contributions to this problem by giving specific network architecture recommendations.\n\nThese recommendations are targeted toward the computer vision domain, which has been one of the most successful application areas of deep learning. Specifically, the use of convolutional layers.\n\nLet\u2019s jump in to the architecture details. I\u2019ll assume basic familiarity with convolutional layers. If you need this background, check out this post. The recommended changes come directly from advances in the computer vision literature.\n\nThe generator is shown above, but the discriminator is essentially a mirror image. The 100-D noise input is fully connected the high level convolutional features. This layer then uses fractional-striding to double the size of the filters, but creates half the number. This process of doubling in size, halving the number is repeated until 128 filters of size 32x32 are created. This is then upscaled to a 64x64 image with 3 layers, representing the three color channels.\n\nHere are some generated images of bedrooms after 5 epochs of training on the LSUN bedrooms dataset. Pretty cool.\n\nTo further demonstrate that the generator was learning meaningful, high level features, they did an experiment where they did \u201cimage arithmetic\u201d.\n\nHere they have taken a man with glasses, subtracted out \u201cman\u201d-ness, added \u201cfemale\u201d-ness, and the results are a female with glasses. This suggests there are dedicated parts of the generator that control the presence of glasses and the gender. This was accomplished by doing these arithmetic operations on the generator noise input. So, you take the z-input vector for man with glasses, subtract the z-input vector for man with no glasses\u2026etc. The resultant vector is then fed into the generator to come up with the desired image. Multiple similar images were created by adding small, random changes to the input vector.\n\nA more systematic example of this was given by interpolating in the direction between faces that looked right and left. So you start with a vector representing right-looking faces and slowly move it in the direction of left-looking faces. This is the result:\n\nThis shows that by walking in the latent/noise-space z, you can have systematic control over features in the generated samples!\n\nFinally, they also demonstrated the quality of the discriminator by removing the real/fake classifier and feeding the convolutional features into a new classifier \u2014 i.e. the discriminator was a feature extractor. If it\u2019s true that useful, general features were learned, then it should be straight-forward to train a classifier by using these features. Using CIFAR-10, which has 10 different image classes, it had competitive accuracy at 83%. Interestingly, the DCGAN was not trained on the CIFAR-10 dataset itself, but on Imagenet-1k. This shows that the model learned general, useful features since it gave great performance on a totally different dataset.\n\nOne of the remaining problems is that the representation is entangled. This means the useful aspects of the input vector z are entangled with the raw noise. If one could separate the \u201clatent code\u201d from the noise, then generators would be more useful since you could control the output systematically and reliably without having to randomly walk the space. This problem and solution will be explored in Part III."
    },
    {
        "url": "https://towardsdatascience.com/overview-of-gans-generative-adversarial-networks-part-i-ac78ec775e31?source=user_profile---------4----------------",
        "title": "Overview of GANs (Generative Adversarial Networks) - Part I",
        "text": "The purpose of this article series is to provide an overview of GAN research and explain the nature of the contributions. I\u2019m new to this area myself, so this will surely be incomplete, but hopefully it can provide some quick context to other newbies.\n\nFor Part I we\u2019ll introduce GANs at a high level and summarize the original paper. Feel free to skip to Part II if you\u2019re already familiar with the basics. It\u2019s assumed you\u2019re familiar with the basics of neural networks.\n\nWhat is meant by generative? At a high level, a generative model means you have mapped the probability distribution of the data itself. In the case of images, that means you have a probability for every possible combination of pixel values. This also means you can generate new data points by sampling from this distribution (i.e. choosing combinations with large probability). If you\u2019re in a computer vision domain, that means your model can create new images from scratch. Here, for example, is a generated face.\n\nIn case this hasn\u2019t totally sunk in yet: this is not a real person, it\u2019s a computer-invented face. A GAN can do this because it was given a lot of images of faces to learn from and that resulted in a probability distribution. This image is one point taken from that distribution. With generative models you can create new stuff that didn\u2019t previously exist. Audio, text, images\u2026etc. Very, very cool stuff.\n\nThe original paper by Ian Goodfellow, et al outlined the basic approach, built the theoretical foundation and gave some example benchmarks.\n\nGANs did not invent generative models, but rather provided an interesting and convenient way to learn them. They are called \u201cadversarial\u201d because the problem is structured such that two entities are competing against one another, and both of those entities are machine learning models.\n\nThis is best explained with an example. Let\u2019s say you want to build a face-image generator. You start by feeding in a bunch of random numbers to a system, and it adds them and multiplies them and applies fancy functions. At the end of this, it outputs a brightness value for each pixel. This is your generative model \u2014 you give it noise and it generates data. Now, let\u2019s say you do this 10 different times and get 10 different fake images.\n\nNext, you grab 10 images of real faces. Then, you feed both the fake and real images into a different model called the discriminator. Its job is to output a number for each input image which tells you the probability that the image is real. In the beginning, the generated samples are just noise, so you might think this would be easy, but the discriminator is just as bad because it hasn\u2019t learned anything yet either.\n\nFor each mistake on a fake image, the discriminator gets penalized and the generator gets a rewarded. The discriminator is also penalized or rewarded based on classifying the real images correctly. This is why they\u2019re called adversarial \u2014 the discriminator\u2019s loss is the generator\u2019s gain. Over time, the competition leads to mutual improvement.\n\nFinally, the word \u201cnetworks\u201d is used because the authors use a neural network for modeling both the generator and discriminator. This is awesome because it provides an easy framework for using the penalties/rewards to tweak the network parameters such that they learn: the familiar back-propagation.\n\nI won\u2019t recreate all the gory details of the paper, but it\u2019s worth mentioning they show both:\n\nTo build intuition, in the above optimization objective V(D,G), the term D(x) is the discriminator\u2019s answer to the question: What\u2019s the probability that input x is from the real data set? If you plug G(z) in to this function, it\u2019s the discriminator\u2019s guess when you give it fake data. If you consider D and G separately you\u2019ll see that G wants V(D,G) to be small and D wants this to be large. This motivates the gradient ascent/descent technique in the algorithm. [The E means \u201cexpectation\u201d, which is just an average. The subscript shows you which probability distribution you\u2019re averaging over, either the real data or the noise that the Generator turns into fake images].\n\nHowever, their provided proof doesn\u2019t directly apply since we\u2019re indirectly optimizing these probability distributions by optimizing parameters of a neural networks, but it\u2019s nice to know that the foundation has theoretical guarantees.\n\nIt\u2019s worth noting that it\u2019s difficult to quantify the quality of fake data. How does one judge the improvement in fake-face generation? That aside, they have state of the art performance when it comes to generating realistic images, which is driving a lot of the buzz. Images generally look less blurry than alternative approaches.\n\nAlthough huge advancements have been made since the original paper (which will be covered in Part II), here are some examples from it:\n\nThe biggest problems with the original GAN implementation are:\n\nThese problems are addressed by refinements to the architecture and will be presented in future posts.\n\nUltimately, this framework allows us to use the normally supervised learning approach of neural networks in an unsupervised way. This is because our labels are trivial to generate since we know which data came from the training set and which data were generated. It\u2019s worth noting that in the above images of hand-written digits, the digit labels themselves were not used during training. Despite this, the generator and discriminator are both able to learn useful representations of the data, as demonstrated by the generator\u2019s capability of mimicking the data.\n\nIn Part II we\u2019ll discuss how to fix many of the training problems as well as make big improvements in realistic image generation."
    },
    {
        "url": "https://towardsdatascience.com/bayesian-additive-regression-trees-paper-summary-9da19708fa71?source=user_profile---------5----------------",
        "title": "\u201cBayesian Additive Regression Trees\u201d paper summary \u2013",
        "text": "This paper develops a Bayesian approach to an ensemble of trees. It is extremely readable for an academic paper and I recommend taking the time to read it if you find the subject interesting.\n\nBayesian Additive Regression Trees (BART) are similar to Gradient Boosting Tree (GBT) methods in that they sum the contribution of sequential weak learners. This is opposed to Random Forests, which average many independent estimates. But instead of multiplying each sequential tree by a small constant (the learning rate) as in GBT, the Bayesian approach is to use a prior.\n\nBy using a prior and likelihood to get a posterior distribution of our prediction, we\u2019re given a much richer set of information than the point estimates of classical regression methods. Furthermore, the Bayesian framework has a built-in complexity penalty, which means we no longer have to make empirical choices about regularization, max tree-depth and the plethora of other options we normally tune via cross-validation.\n\nIt also turns out that this method outperforms all the others that were compared, including GBM and Random Forests, on the 42 different datasets evaluated.\n\nThe novel thing in this paper is really the combination of three previous works: the Bayesian framing of individual trees in the Bayesian Treed Models paper; the idea of Gradient Boosting Trees; and the use of Bayesian Backfitting to do MCMC sampling from a general additive model\u2019s posterior distribution. If you can make one tree Bayesian and you know how to sample from any model that\u2019s a sum of base learners, then you have BART.\n\nThe task of framing the tree problem in such a way that you can coherently define prior parameter distributions is not trivial. This work was mostly done in the previously linked Treed Models paper. Essentially, the overall prior is split into three sub-priors: one for the tree structure (depth, splitting criteria), another for the values in the terminal nodes conditional on the tree structure, and a final one for the residual noise\u2019s standard deviation. Some clever, but straight-forward arguments are made to lead to reasonable default recommendations. For instance, it\u2019s argued that the mean of the function is very likely between y_min and y_max of your training data, so it\u2019s framed such that most of the prior\u2019s mass is in this region.\n\nThe choice on the number of trees, m, is interestingly not given a prior, and this is due to computational concerns. The reality is that the results were incredibly robust to this parameter, so he recommended setting it to 200 and moving on. In fact, the results seem very robust to reasonable prior choices in general and he mostly recommends just using the specified defaults.\n\nThe second important part is sampling from the model\u2019s posterior. Like many problems, this is done by a Metropolis-Hastings algorithm where you generate a sample from a distribution and then keep/reject it based on how well it performs. In this case, it boils down to mostly this: pick a tree in the additive sequence, morph it by randomly choosing among some rules (PRUNE, GROW\u2026etc), from this new tree, sample from the terminal node value distribution, then choose whether to keep this new tree or the original according to their ratio of posterior probabilities. In this way, the trees are continually changed to balance their complexity and ability to explain the data. The priors are chosen to naturally favor simpler trees, so deeper are only chosen if it\u2019s necessary to explain the data.\n\nThis framework was applied to 42 different real datasets and compared to other popular approaches: linear regression with Lasso/L1 regularization, Gradient Boosting Trees, Random Forests, and Neural Networks with one hidden layer. These models were tuned over a variety of parameter choices. The punchline is: BART wins. You get the best performance if you perform cross-validation for hyper-parameter tuning, similar to how you would with all the other models to tune parameters, but you can avoid all of that and get highly competitive results just using BART with the default values.\n\nFurther, the results were shown to be highly robust to changes in the choice of prior. And in all its Bayesian glory, it was shown that even when the predictors were thrown in with many useless random variables (as would often be the case when we\u2019re building models without being clear which ones are most important), it performed very well as compared to the other methods.\n\nThe author also suggested a variable selection technique that involved artificially limiting the number of trees and then counting the prevalence of the variables in the splitting rules of the resulting trees. Although this might be a nice heuristic, the question of variable selection is a complicated one and it wasn\u2019t explored with any rigor.\n\nThis was a super cool paper that gave very impressive results. Although I had difficulty understanding the details of the Bayesian Backfitting algorithm and how the Gibbs sampling is actually achieved, the basics seem mostly in line with other Metropolis-Hastings approaches.\n\nUltimately, this approach provides an \u201cout-of-the-box\u201d set of defaults that are statistically robust, give best-in-class performance, and are resilient to changes in hyper-parameter choices. I think it\u2019s an excellent demonstration of how the Bayesian probabilistic framework can avoid many of the problems with traditional, ad-hoc approaches that mostly rely on maximum likelihood.\n\nThe most disappointing fact is that I could not find a Python implementation of this algorithm. The authors created an R package (BayesTrees) that had some obvious problems \u2014 mostly the lack of a \u201cpredict\u201d function \u2014 and another, more widely used implementation called bartMachine was created.\n\nIf you have experience implementing this technique or know of a Python library, please leave a link in the comments!"
    },
    {
        "url": "https://towardsdatascience.com/getting-started-with-python-for-data-analysis-64d6f6c256b2?source=user_profile---------6----------------",
        "title": "Getting Started with Python for Data Analysis \u2013",
        "text": "A friend recently asked this and I thought it might benefit others if published here. This is for someone new to Python that wants the easiest path from zero to one.\n\nHere\u2019s a quick summary of the important libraries you\u2019ll interact with frequently."
    },
    {
        "url": "https://towardsdatascience.com/estimation-and-inference-of-heterogeneous-treatment-effects-using-random-forests-paper-review-c26fb97c96b7?source=user_profile---------7----------------",
        "title": "\u201cEstimation and Inference of Heterogeneous Treatment Effects using Random Forests\u201d paper review",
        "text": "I\u2019ve recently been reading a lot of technical papers and thought it would be nice to summarize them in less formal language than academic papers. I may or may not do this more than once.\n\nThis paper is about trying to do causal analysis using Random Forests (RF). RF are very popular for building classification or regression predictive models, but it\u2019s not trivial to make classical statistical claims about the results. For instance, what are your confidence intervals? How do you get p-values?\n\nFurthermore, this paper wants to make claims on causal impact, or the effect of a treatment. For instance, what\u2019s the impact of college on income? This is hard to do for many reasons, but most fundamentally you don\u2019t have the data you exactly need, which is data for each individual on what happened when they both went to college and did not. This is impossible of course, because in reality the individual either went to college or they did not and you don\u2019t know what would have happened if the other situation occurred \u2014 i.e. the counterfactual. This way of framing the problem is called the \u201cpotential outcomes framework\u201d and essentially supposes that each person has multiple potential outcomes depending on whether or not they received the treatment.\n\nA key assumption in this paper, and causal estimation techniques of this type generally, is one of unconfoundedness. This means that once you control for the variables of interest, whether or not a person received the treatment or not is random. This enables us to treat nearby points as mini-randomized experiments. In the case of a drug trial you can randomly assign treatment, so this isn\u2019t a problem, but if you\u2019re analyzing observational data the treatments are already assigned \u2014 a person went to college or they didn\u2019t.\n\nFor unconfoundedness to be upheld you would need to choose and measure all of the variables that would affect the treatment assignment. For the causal impact of a 4-year degree on income, one of the covariates you\u2019d likely want to choose is family income since whether or not someone goes to college is probably correlated with their family\u2019s income. Age might be another one since an 18 year-old is a lot more likely to go to college than a 50 year-old. The idea is that when you look at two neighbor points that are plotted in your family-income/age plot, the decision of whether or not to go to college should be random. This is valuable because then you can take the income of the no-college-person and subtract it from the college-person and you have an estimate of the impact of college at that point in the family-income/age space.\n\nBut this is hard, because you might forget an important variable, or maybe you just don\u2019t have the data for it. So it\u2019s an assumption that surely isn\u2019t exactly true, but it might be true enough to give useful answers.\n\nBut assuming you have the right covariates, the authors use a Random Forest to split the data into self-similar groups. The Random Forest is an adaptive nearest-neighbor method in that it decides which portions of the space are similar for you, whereas most nearest-neighbor techniques tend to treat all distances equal. They then add constraints that the resultant leaves have a minimum of both treatment and no-treatment classes and then the causal impact for each leaf can be calculated by subtracting the averages as if it were a randomized experiment. Once complete for the individual trees, the estimates from each tree can be averaged. They call this implementation a Causal Forest.\n\nAs mentioned above, to make robust statistical claims in traditional statistics, you need things like p-values and confidence intervals. This requires knowing the asymptotic sampling distribution of the statistic. In this case, that means we need to know what the distribution would look like if we sampled the average treatment effect from the Random Forest estimator over an infinite number of trees/data. If we have that, then we can say things like: \u201cthe average treatment is 0.3 and the likelihood that this came from random chance is less than 0.1%\u201d\n\nThe built-in assumption here is that we can derive properties for the asymptotic/infinite data case and apply them to our real-world case of finite samples, but that\u2019s often the case in traditional statistics.\n\nPrior work has been done to enable asymptotic analysis of Random Forests, but this paper establishes the constraints needed to apply them to their Causal Forests. One of the constraints requires \u201chonest trees\u201d, which they present two algorithms for growing.\n\nThe full proof is very complicated and I won\u2019t attempt to recreate it here, but I\u2019ll briefly outline some constraints and what they enable.\n\nFirst, an asymptotic theory is recreated from previous work for traditional Random Forests that shows that it is an asymptotically Gaussian estimator with a mean of zero, meaning it\u2019s unbiased. They also mention a technique called the infinitesimal jackknife for estimating the variance, which includes a finite-sample correction.\n\nThe authors are able to leverage this previous work by including the concept of the \u201chonest tree\u201d. The basic idea is that you cannot use the outcome variable to both do the splitting and estimate the average impact \u2014 you have to choose one or the other. They present two ways to do this. The first is the double-sample tree where you split your data in two: half for estimating the impact and the other for placing the splits. The splitting criteria is to minimize the MSE for the outcome variable. In the double-sample case it might seem you\u2019re throwing away half your data, but this condition is for a single tree and the Random Forest is sampling new training sets for each tree, so you\u2019ll end up using all of the data for both splitting and estimation.\n\nThe other method is by growing \u201cpropensity trees\u201d which are classification trees that aim to predict the treatment class instead of the outcome, and then the outcome variable is only used for estimating the impact within each leaf. They impose a stopping criteria such that you stop splitting to maintain a minimum of each treatment class in any leaf. This is necessary so that you have outcomes to compare to estimate the effect.\n\nBy using honest trees and relying on assumptions of unconfoundedness and treatment class overlap within leaves, they\u2019re able to slightly modify the traditional treatment to give the same unbiased gaussian asymptotic results.\n\nTheir baseline is using the k-NN algorithm. They create some simulation experiments with known conditions that mimic common problems and then apply the Causal Forest and k-NN methods.\n\nThe first experiment holds the true treatment effect at zero for all x, but establishes a correlation between the outcome and the treatment assignment, thereby testing the ability of the algorithm to correct the covariates to eliminate the bias. This is like having the algorithm automatically figure out that age and family are important as well as how to split them up. They run the experiment many times at a varying training data dimension. They reported MSE and the coverage, which was how often the true value was within the 95% confidence interval of the estimator. The Causal Forest had an order of magnitude improvement over 10-NN and a factor of 5 improvement over 100-NN. CF maintained ~0.95 coverage up to 10 dimensions and then began to degrade. 10-NN maintained reasonable coverage in the 0.9 range and 100-NN performed very poorly. It\u2019s worth noting the confidence intervals were much wider for k-NN than CF, so the improved coverage is that much more impressive.\n\nThe 2nd experiment had constant main effect and propensity, but had the true treatment effect depend on only two covariates. They then scaled the number of irrelevant covariates to understand the ability of the algorithm to find this heterogeneous treatment effect in the presence of irrelevant covariates. Surprisingly, CF did better at higher dimension than low dimension. They explain this by noting the variance of the forest depends on the correlation between trees and suggest that the correlation between trees and therefore ensemble variance is reduced at the higher dimension. The results are similar to experiment 1 in that MSE is much better or at least on par with more consistent coverage that scales with dimension.\n\nIt was noted that the confidence intervals\u2019 coverage begins to degrade at the edge of the feature space, particularly for high dimension. This is explained as being a situation dominated by bias that would disappear in the asymptotic limit of infinite data. It is noted that bias at the boundaries is typical of trees specifically, and nearest-neighbor non-parametric estimators generally.\n\nAlthough causal analysis is not my expertise, it seems this is a nice advancement for nearest-neighbor methods with the assumption of unconfoundedness. The dramatic improvement in MSE while maintaining nominal coverage is impressive.\n\nI found several aspects of the paper confusing, however. Specifically, those related to splitting criteria of the trees. In the case of propensity trees, they\u2019re training a classifier to separate treatment classes, but they\u2019re conversely requiring a constraint of heterogeneity of classes in each leaf, which is directly opposed to the split criteria.\n\nSimilarly in the double-sample framework, they\u2019re splitting to minimize the outcome MSE, which groups points with similar outcome values. But the entire point is that after separating the points by treatment classification, the outcomes are different and that difference is the average treatment effect. Once again the splitting criteria seems opposed to the end-goal. To this end they reference a paper (Athey and Imbens [2016]) that may contain clarification.\n\nI\u2019m not sure if this is operational overhead, or something more fundamental.\n\nWhat do you think?"
    }
]