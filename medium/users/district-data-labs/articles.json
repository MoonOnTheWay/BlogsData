[
    {
        "url": "https://medium.com/district-data-labs/entity-resolution-on-voter-registration-data-db13caf8517c?source=---------0",
        "title": "Entity Resolution on Voter Registration Data \u2013 District Data Labs \u2013",
        "text": "Entity resolution is a field that aims to find records in data sets that refer to the same entity by grouping and linking. Entity resolution is also called deduplication, merge purge, patient matching, etc. depending on the application. In voter registration, it is a useful technology to make sure voter rolls are up to date and can be used to see if a voter is registered in multiple states.\n\nThere are many challenges to applying entity resolution. Different data sources have varying schema, collection standards and methodologies. Even the task of standardizing the data to link entities from these sources can be a cumbersome task. To further complicate matters, there can be spelling errors, transposed characters, missing values, and other anomalies.\n\nThere has been a lot of discussion around voter fraud during the most recent Presidential election. The following article stated that there were 3 million people who were registered to vote in multiple states. Entity resolution is a tool that can be used to clean up voter registration data, ensure integrity in the election process, and prevent voter fraud, thereby helping to restore people\u2019s confidence in the voting process.\n\nFor the purposes of this project, DC voter registration data from 2014 was used. The data is available at this website.\n\nAll of the information was compiled into a zipped-up csv file that contained a header row and 469,610 rows of voter registration data. The following are the most significant fields related to a particular voter:\n\nSince the data was provided as a csv file, it was an easy task to import the records into a table in a MySQL database. The data was imported into a database called dcvoterinfo and the table was named voterinfo. To make the task of entity resolution easier, certain fields were combined to form new fields based on logical groups. The fields LASTNAME, FIRSTNAME, MIDDLE, and SUFFIX were combined to create a new field called name. Similarly, the RES_HOUSE, RES_FRAC, RES_APT, RES_STREET, RES_CITY, RES_STATE, RES_ZIP, and RES_ZIP4 fields were combined to create a field called address.\n\nThere was one record with a value in the REGISTERED field where the recorded year was 2223, which was clearly an error. Also, there were 40 records with REGISTERED year 1900, all with the date 01/01/1900, which was most likely a collection error. Grouping records by year reveals that prior to 1968, there was very little data. In fact, the total number of records prior to 1968 was 123, while the number of records in just 1968 alone was 8,628. These were left in the data set and did not appear to cause any issues with the results.\n\nStudying the data set revealed that the following fields were most likely to identify a unique entity: name, address, REGISTERED. Fields such as precinct and ward are dependent on the address field and are, therefore, redundant. One quick and dirty way to determine if there were possible duplicates was to use a SQL group by query to see if there was more than one record that matched an attribute or a set of attributes. The following is a query that was run to identify potential duplicates using the name and address fields:\n\nThe above query generated 703 results \u2014 most resulted in two matches per group, and there were four groups that had three matches. The following is a snapshot of the results:\n\nLooking at the records for RUSSELL D SIMMONS revealed that the value for REGISTERED was different in these records: the first REGISTERED date was 3/23/1968, the second was 10/12/1996, and the third was 5/9/2006. It is possible that a father and son lived at the same address, but since we had three matches, it was possible that two of the records refer to the same entity.\n\nAdding REGISTERED to the sql query found 10 results with the same values for name, address and REGISTERED. The following are the records:\n\nThe records for AKIL A LASTER showed that there were two records with a REGISTERED date of 11/4/2014 and another record with a REGISTERED date of 11/12/2014. It was very likely that these three records refered to the same person.\n\nWithout domain knowledge regarding the data set, we had to make some assumptions on how to identify duplicate records. Since the data spans several years, it was possible for someone to have moved and to have more than one address. But in the absence of the residential address history and to simplify the analysis, we made the assumption that name and address will uniquely identify an entity.\n\nWhile the sql queries revealed duplicates, the data set probably had other duplicates that were not caught because of misspellings in the name and/or address fields. This was where Dedupe came in. Dedupe is a Python library that performs entity resolution.\n\nSince the data set is in MySQL, it was a pretty straightforward task to use the mysql_example.py code from the dedupe-examples package. Some changes had to be made to the code, namely the data source, table name, fields etc.\n\nThe following were the fields selected from the voterinfo table:\n\nRunning the code against the data set resulted in 8,111 clusters. Several of these clusters contained 5 or more duplicate entities, which seemed excessive. It also took several hours to run. Upon examining the results, it was clear that name was over weighted. All the clusters had either the exact same name or names that were very close (such as Craig C Johnson Sr and Craig C Jonhson) regardless of the value in the address field. The following is a sample of the results:\n\nForest Gregg, one of the creators of Dedupe, suggested installing and using dedupe-variable-name as the data set contained American names and would perform better in this example because there were multiple people who share the same address. He also suggested removing the Interaction variable.\n\nFor people with Mac operating systems who have trouble installing dedupe-variable-name because of a gcc error, the following modification can be made to get past the error:\n\nIn order to use dedupe-variable-name, the following additional import statement was added to the code and the name field was modified to use the \u201cName\u201d type from this package.\n\nThis iteration produced 1,250 clusters and took 52 minutes to run, which was a significant improvement in performance.\n\nSince Dedupe also provides a special variable type called dedupe-variable-address, in the third iteration, the code was modified to include this package to see if the results can be further improved. The address field was modified to use the \u201cAddress\u201d type.\n\nAdding dedupe-variable-address resulted in 1,337 clusters and took 50 minutes to run, which was somewhat similar to the previous example.\n\nRemember that we made an assumption earlier that name and address will uniquely identify an entity.\n\nTaking that approach, the first set of results (produced without using dedupe-variable-name and dedupe-variable-address) was not meaningful because it identified duplicates solely on name.\n\nThe second and third sets of results were very similar \u2014 both matched on 1,100 clusters containing a total of 2,224 records. There were 150 clusters in the second result set that were not in the third result set while there were 237 clusters in the third result set that were not in the second result set.\n\nOverall, both did a pretty good job in identifying records that had the same name and address, such as the following:\n\nThere were also a number of records that they identified where there were spelling mistakes and inverted name mistakes. The following are a couple of examples.\n\nHowever, there were also a number of records where names that had a suffix were matched up with names that did not. The following are a few cases.\n\nThere were also cases where records matched on last name and address but had completely different first names that were put together in a cluster.\n\nLooking at the results that were only in the second result set, it appeared that it contained a number of records that had similarities in name but were not exact. The following figure shows some examples. All these records should have been in different clusters.\n\nWhat was interesting was that there were some exact matches on name and address (but different values for REGISTERED) that were identified as belonging to the same cluster in the second result set but not so in the third result set. The following is an example:\n\nThere were similar patterns in the records that were only in the third result set. Examples include: clusters with similarities in name, and clusters that matched on name and address but had different values for REGISTERED. In other words, there were cases where the third result set identified clusters while the second result set did not.\n\nThe third result set identified a couple of clusters where the address was the same but the names were not the same. The following is an example.\n\nOverall, Dedupe does a very good job of identifying duplicate entities based on name and address. It is able to effectively identify slight variations in the name such as spelling errors and inverted first name and last name. In that sense, it does a much better job than simply identifying exact matches using a group by SQL query or an Excel remove duplicates function. It appears, however, that it can use some improvement in discarding cases where there is more variation, such as examples where the first name is clearly different. The dedupe-variable-name package accounts for suffixes but it tends to cluster names with suffixes along with names that don\u2019t. A potential enhancement can be made to prevent such entities from getting clustered together.\n\nThere have been several articles such as this one, which stated that people were registered to vote in more than one state. This was possible as people moved they failed to tell the voter registration agency in their old state that they have moved. The results that Dedupe produced seem to suggest that people have also registered to vote more than once in the same state. Further investigation needs to be conducted to determine if this was truly the case. If so, states can begin to use entity resolution to take measures to prevent this double-registering from happening in the future. In addition, entity resolution tools such as Dedupe can be combined with record linkage to identify voters who are registered in multiple states. Utilizing entity resolution to clean up our voter registries would be an important first step towards making our elections more fair and transparent."
    },
    {
        "url": "https://medium.com/district-data-labs/applied-data-science-ai-round-up-february-2018-edition-2407336d0217?source=---------1",
        "title": "Applied Data Science & AI Round-Up: February 2018 Edition",
        "text": "This month, we continue our Applied Data Science & AI Round-Up series where we highlight some of the interesting applications of data science and machine learning we\u2019ve read about. In February, we saw several interesting examples of applications in the education, cybersecurity, manufacturing, and finance industries. Check them out below!\n\nNew technologies such as AI are giving us the ability to enhance and accelerate the learning process, which will enable streamlining everything from admissions and grading to student access to vital resources.\n\nCybersecurity teams constantly struggle with having information that is either timely, but not meaningful or vice versa. Data science can help solve that problem by utilizing computational and analytical techniques to deliver high value, actionable insights that will enable Chief Security Information Officers and Security Control Managers to make better decisions.\n\nBoeing is using machine learning software to alter the alloys they use for their jets, all the way down to the atoms. The machine learning models are able to discern the best recipe for the metal much quicker than would be possible using standard practices. Combining this with 3D printing technology, Boeing is able to create brand new, custom parts for their planes.\n\nThe financial industry is increasingly finding value in artificial intelligence applications, especially in conversational AI and consumers interacting with intelligent voice products. Computer science professor Dr. Jason Mars expects to see more data and computer scientists applying their talents to the financial space, producing effects similar to what has already occurred in tech.\n\nAI-powered algorithms are giving screenwriters that lack the big Hollywood budgets the ability to generate videos simply by consuming a short script. While the videos created are still quite primitive, this technology is expected to be able to help outside of entertainment such as helping a witness reconstruct a crime scene.\n\nThat\u2019s it for this month! If you come across some interesting articles that highlight the application of data science and AI to an industry or domain, make sure to let us know by tagging us on social or emailing us a link at news@districtdatalabs.com."
    },
    {
        "url": "https://medium.com/district-data-labs/applied-data-science-ai-round-up-january-2018-edition-e79a979f2a81?source=---------2",
        "title": "Applied Data Science & AI Round-Up: January 2018 Edition",
        "text": "For 2018, we are starting a new monthly round-up series on the DDL blog that highlights examples of data science and artificial intelligence being applied in different fields, industries, and domains. We\u2019ve noticed that people who don\u2019t work closely with data, or haven\u2019t worked with data scientists, often have trouble envisioning how these methods and technologies are applicable to them and can impact the work they do. We hope this series will help further the conversation about how these technologies can be applied and spark some ideas for how you can apply them in your own business or domain.\n\nChild abuse and neglect hotlines around the country screen millions of calls a year attempting to discern which cases are serious. Allegheny County in Pennsylvania is the first jurisdiction in the United States to use a predictive analytics algorithm when screening calls to their child abuse and neglect hotline to better help determine which families require intervention. The algorithm is capable of analyzing multiple databases and historical files in seconds, whereas it would have taken human employees hours to perform the same tasks.\n\nIn this in-depth interview with Forbes, current Walmart CIO Clay Johnson goes over his big priorities for the IT department this year, which includes developing a product model for IT to facilitate end-to-end ownership of different product areas created, as well as process automation, facilitated at least in part through artificial intelligence.\n\nWith more refugees in the world than ever before, this growing global crisis is often considered an overwhelming challenge. However, solutions are possible with the help of predictive analytics. By using migration data that is already being collected, experts can predict where refugees are most likely to head next so that countries can have time to react and prepare.\n\nFully tapping into the power of machine learning may require relying on results that are impossible to explain to the human mind. Modifying the technology to explain its conclusions in every case can lead to mistakes like failing to diagnose diseases, overlooking significant causes of climate change, or making an educational system that\u2019s one-size fits all.\n\nHistorians and cryptographers have been attempting to decipher the Voynich manuscript since before WWII and all have failed until scientists at the University of Alberta enlisted the help of artificial intelligence.\n\nThat\u2019s it for this month! If you come across some interesting articles that highlight the application of data science and AI to an industry or domain, let us know by tagging us on social or emailing us a link at news@districtdatalabs.com."
    },
    {
        "url": "https://medium.com/district-data-labs/data-exploration-with-python-part-3-dd6007bb3ae7?source=---------3",
        "title": "Data Exploration with Python, Part 3 \u2013 District Data Labs \u2013",
        "text": "This is the third post in our Data Exploration with Python series. Before reading this post, make sure to check out Part 1 and Part 2!\n\nPreparing yourself and your data like we have done thus far in this series is essential to analyzing your data well. However, the most exciting part of Exploratory Data Analysis (EDA) is actually getting in there, exploring the data, and discovering insights. That\u2019s exactly what we are going to start doing in this post.\n\nWe will begin with the cleaned and prepped vehicle fuel economy data set that we ended up with at the end of the last post. This version of the data set contains:\n\nNow, without further ado, let\u2019s embark on our insight-finding mission!\n\nOne of the fundamental ways to extract insights from a data set is to reduce the size of the data so that you can look at just a piece of it at a time. There are two ways to do this: filtering and aggregating. With filtering, you are essentially removing either rows or columns (or both rows and columns) in order to focus on a subset of the data that interests you. With aggregation, the objective is to group records in your data set that have similar categorical attributes and then perform some calculation (count, sum, mean, etc.) on one or more numerical fields so that you can observe and identify differences between records that fall into each group.\n\nTo begin filtering and aggregating our data set, we could write a function like the one below to aggregate based on a that we provide, counting the number of rows in each group. To make things more intuitive and easier to interpret, we will also sort the data from most frequent to least and format it in a pandas data frame with appropriate column names.\n\nNow that we have this function in our toolkit, let\u2019s use it. Suppose we were looking at the Vehicle Category field in our data set and were curious about the number of vehicles in each category that were manufactured last year (2016). Here is how we would filter the data and use the function to transform it to show what we wanted to know.\n\nThis gives us what we want in tabular form, but we could take it a step further and visualize it with a horizontal bar chart.\n\nNow that we know how to do this, we can filter, aggregate, and plot just about anything in our data set with just a few lines of code. For example, here is the same metric but filtered for a different year (1985).\n\nIf we wanted to stick with the year 2016 but drill down to the more granular Vehicle Class, we could do that as well.\n\nWe could also look at vehicle counts by manufacturer.\n\nWhat if we wanted to filter by something other than the year? We could do that by simply creating a different filtered data frame and passing that to our function. Below, instead of filtering by Year, I've filtered on the Fuel Efficiency field, which contains the fuel efficiency quintiles we generated in the last post. Let's choose the Very High Efficiency value so that we can see how many very efficient vehicles each manufacturer has made.\n\nWhat if we wanted to perform some other calculation, such as averaging, instead of counting the number of records that fall into each group? We can just create a new function called that calculates the mean of a designated numerical field.\n\nWe can then simply swap out the function with our new function and indicate what field we would like to use for our calculation. Below is an example showing the average fuel efficiency, represented by the Combined MPG field, by vehicle category.\n\nUp until this point, we\u2019ve been looking at our data at a pretty high level, aggregating up by a single variable. Sure, we were able to drill down from Vehicle Category to Vehicle Class to get a more granular view, but we only looked at the data one hierarchical level at a time. Next, we\u2019re going to go into further detail by taking a look at two or three variables at a time. The way we are going to do this is via pivot tables and their visual equivalents, pivot heatmaps.\n\nFirst, we will create a function, similar to the function we created earlier, that will transform whatever data frame we feed it into a pivot table with the rows, columns, and calculated field we specify.\n\nWe will then use this function on our data frame and pivot it out with the Fuel Efficiency quintiles we created in the last post representing the rows, the Engine Size quintiles representing the columns, and then counting the number of vehicles that had a Combined MPG value.\n\nThis is OK, but it would be faster to analyze visually. Let\u2019s create a heatmap that will color the magnitude of the counts and present us with a more intuitive view.\n\nJust like we did earlier with our horizontal bar charts, we can easily filter by a different year and get a different perspective. For example, here\u2019s what this heatmap looks like for 1985.\n\nWith these pivot heatmaps, we are not limited to just two variables. We can pass a list of variables for any of the axes (rows or columns), and it will display all the different combinations of values for those variables.\n\nIn this heatmap, we have Engine Size and Fuel Efficiency combinations represented by the rows, and we\u2019ve added a third variable (the Vehicle Category) across the columns. So now we can see a finer level of detail about what types of cars had what size engines and what level of fuel efficiency last year.\n\nAs a final example for this section, let\u2019s create a pivot heatmap that plots Make against Vehicle Category for 2016. We saw earlier, in the bar chart that counted vehicles by manufacturer, that BMW made the largest number of specific models last year. This pivot heatmap will let us see how those counts are distributed across vehicle categories, giving us a better sense of each auto company\u2019s current offerings in terms of the breadth vs. depth of vehicle types they make.\n\nSo far in this post, we\u2019ve been looking at the data at given points in time. The next step is to take a look at how the data has changed over time. We can do this relatively easily by creating a function that accepts a data frame and x/y fields and then plots them on a multiline chart.\n\nLet\u2019s use this function to visualize our vehicle categories over time. The resulting chart shows the number of vehicles in each category that were manufactured each year.\n\nWe can see from the chart that Small Cars have generally dominated across the board and that there was a small decline in the late 90s that then started to pick up again in the early 2000s. We can also see the introduction and increase in popularity of SUVs starting in the late 90s, and the decline in popularity of trucks in recent years.\n\nIf we wanted to, we could zoom in and filter for specific manufacturers to see how their offerings have changed over the years. Since BMW had the most number of vehicles last year and we saw in the pivot heatmap that those were mostly small cars, let\u2019s filter for just their vehicles to see whether they have always made a lot of small cars or if this is more of a recent phenomenon.\n\nWe can see in the chart above that they started off making a reasonable number of small cars, and then seemed to ramp up production of those types of vehicles in the late 90s. We can contrast this with a company like Toyota, who started out making a lot of small cars back in the 1980s and then seemingly made a decision to gradually manufacture less of them over the years, focusing instead on SUVs, pickup trucks, and midsize cars.\n\nThe final way we are going to explore our data in this post is by examining the relationships between numerical variables in our data. Doing this will provide us with better insight into which fields are highly correlated, what the nature of those correlations are, what typical combinations of numerical values exist in our data, and which combinations are anomalies.\n\nFor looking at relationships between variables, I often like to start with a scatter matrix because it gives me a bird\u2019s eye view of the relationships between all the numerical fields in my data set. With just a couple lines of code, we can not only create a scatter matrix, but we can also factor in a layer of color that can represent, for example, the clusters we generated at the end of the last post.\n\nFrom here, we can see that there are some strong positive linear relationships in our data, such as the correlations between the MPG fields, and also among the fuel cost, barrels, and CO2 emissions fields. There are also some hyperbolic relationships in there as well, particularly between the MPG fields and engine displacement, fuel cost, barrels, and emissions. Additionally, we can also get a sense of the size of our clusters, how they are distributed, and the level of overlap we have between them.\n\nOnce we have this high-level overview, we can zoom in on anything that we think looks interesting. For example, let\u2019s take a closer look at Engine Displacement plotted against Combined MPG.\n\nIn addition to being able to see that there is a hyperbolic correlation between these two variables, we can see that our Small Very Efficient cluster resides in the upper left, followed by our Midsized Balancedcluster that looks smaller and more compact than the others. After that, we have our Large Moderately Efficient cluster and finally our Large Inefficient cluster on the bottom right.\n\nWe can also see that there are a few red points at the very top left and a few purple points at the very bottom right that we may want to investigate further to get a sense of what types of vehicles we are likely to see at the extremes. Try identifying some of those on your own by filtering the data set like we did earlier in the post. While you\u2019re at it, try creating additional scatter plots that zoom in on other numerical field combinations from the scatter matrix above. There are a bunch of other insights to be found in this data set, and all it takes is a little exploration!\n\nWe have covered quite a bit in this post, and I hope I\u2019ve provided you with some good examples of how, with just a few tools in your arsenal, you can embark on a robust insight-finding expedition and discover truly interesting things about your data. Now that you have some structure in your process and some tools for exploring data, you can let your creativity run wild a little and come up with filter, aggregate, pivot, and scatter combinations that are most interesting to you. Feel free to experiment and post any interesting insights you\u2019re able to find in the comments!\n\nAlso, make sure to stay tuned because in the next (and final) post of this series, I\u2019m going to cover how to identify and think about the different networks that are present in your data and how to explore them using graph analytics."
    },
    {
        "url": "https://medium.com/district-data-labs/basics-of-entity-resolution-with-python-and-dedupe-bc87440b64d4?source=---------4",
        "title": "Basics of Entity Resolution with Python and Dedupe \u2013 District Data Labs \u2013",
        "text": "Entity resolution (ER) is the task of disambiguating records that correspond to real world entities across and within datasets. The applications of entity resolution are tremendous, particularly for public sector and federal datasets related to health, transportation, finance, law enforcement, and antiterrorism.\n\nUnfortunately, the problems associated with entity resolution are equally big \u2014 as the volume and velocity of data grow, inference across networks and semantic relationships between entities becomes increasingly difficult. Data quality issues, schema variations, and idiosyncratic data collection traditions can all complicate these problems even further. When combined, such challenges amount to a substantial barrier to organizations\u2019 ability to fully understand their data, let alone make effective use of predictive analytics to optimize targeting, thresholding, and resource management.\n\nLet us first consider what an entity is. Much as the key step in machine learning is to determine what an instance is, the key step in entity resolution is to determine what an entity is. Let\u2019s define an entity as a unique thing (a person, a business, a product) with a set of attributes that describe it (a name, an address, a shape, a title, a price, etc.). That single entity may have multiple references across data sources, such as a person with two different email addresses, a company with two different phone numbers, or a product listed on two different websites. If we want to ask questions about all the unique people, or businesses, or products in a dataset, we must find a method for producing an annotated version of that dataset that contains unique entities.\n\nHow can we tell that these multiple references point to the same entity? What if the attributes for each entity aren\u2019t the same across references? What happens when there are more than two or three or ten references to the same entity? Which one is the main (canonical) version? Do we just throw the duplicates away?\n\nEach question points to a single problem, albeit one that frequently goes unnamed. Ironically, one of the problems in entity resolution is that even though it goes by a lot of different names, many people who struggle with entity resolution do not know the name of their problem.\n\nThe three primary tasks involved in entity resolution are deduplication, record linkage, and canonicalization:\n\nEntity resolution is not a new problem, but thanks to Python and new machine learning libraries, it is an increasingly achievable objective. This post will explore some basic approaches to entity resolution using one of those tools, the Python Dedupe library. In this post, we will explore the basic functionalities of Dedupe, walk through how the library works under the hood, and perform a demonstration on two different datasets.\n\nDedupe is a library that uses machine learning to perform deduplication and entity resolution quickly on structured data. It isn\u2019t the only tool available in Python for doing entity resolution tasks, but it is the only one (as far as we know) that conceives of entity resolution as it\u2019s primary task. In addition to removing duplicate entries from within a single dataset, Dedupe can also do record linkage across disparate datasets. Dedupe also scales fairly well \u2014 in this post we demonstrate using the library with a relatively small dataset of a few thousand records and a very large dataset of several million.\n\nEffective deduplication relies largely on domain expertise. This is for two main reasons: first, because domain experts develop a set of heuristics that enable them to conceptualize what a canonical version of a record should look like, even if they\u2019ve never seen it in practice. Second, domain experts instinctively recognize which record subfields are most likely to uniquely identify a record; they just know where to look. As such, Dedupe works by engaging the user in labeling the data via a command line interface, and using machine learning on the resulting training data to predict similar or matching records within unseen data.\n\nGetting started with Dedupe is easy, and the developers have provided a convenient repo with examples that you can use and iterate on. Let\u2019s start by walking through the csv_example.py from the dedupe-examples. To get Dedupe running, we\u2019ll need to install , , and .\n\nIn your terminal (we recommend doing so inside a virtual environment):\n\nThen we\u2019ll run the csv_example.py file to see what dedupe can do:\n\nLet\u2019s imagine we own an online retail business, and we are developing a new recommendation engine that mines our existing customer data to come up with good recommendations for products that our existing and new customers might like to buy. Our dataset is a purchase history log where customer information is represented by attributes like name, telephone number, address, and order history. The database we\u2019ve been using to log purchases assigns a new unique ID for every customer interaction.\n\nBut it turns out we\u2019re a great business, so we have a lot of repeat customers! We\u2019d like to be able to aggregate the order history information by customer so that we can build a good recommender system with the data we have. That aggregation is easy if every customer\u2019s information is duplicated exactly in every purchase log. But what if it looks something like the table below?\n\nHow can we aggregate the data so that it is unique to the customer rather than the purchase? Features in the data set like names, phone numbers, and addresses will probably be useful. What is notable is that there are numerous variations for those attributes, particularly in how names appear \u2014 sometimes as nicknames, sometimes even misspellings. What we need is an intelligent and mostly automated way to create a new dataset for our recommender system. Enter Dedupe.\n\nWhen comparing records, rather than treating each record as a single long string, Dedupe cleverly exploits the structure of the input data to instead compare the records field by field. The advantage of this approach is more pronounced when certain feature vectors of records are much more likely to assist in identifying matches than other attributes. Dedupe lets the user nominate the features they believe will be most useful:\n\nDedupe scans the data to create tuples of records that it will propose to the user to label as being either matches, not matches, or possible matches. These are identified using a combination of blocking , affine gap distance, and active learning.\n\nBlocking is used to reduce the number of overall record comparisons that need to be made. Dedupe\u2019s method of blocking involves engineering subsets of feature vectors (these are called \u2018predicates\u2019) that can be compared across records. In the case of our people dataset above, the predicates might be things like:\n\nRecords are then grouped, or blocked, by matching predicates so that only records with matching predicates will be compared to each other during the active learning phase. The blocks are developed by computing the edit distance between predicates across records. Dedupe uses a distance metric called affine gap distance, which is a variation on Hamming distance that makes subsequent consecutive deletions or insertions cheaper.\n\nTherefore, we might have one blocking method that groups all of the records that have the same area code of the phone number. This would result in three predicate blocks: one with a 202 area code, one with a 334, and one with NULL. There would be two records in the 202 block (IDs 452 and 821), two records in the 334 block (IDs 233 and 699), and one record in the NULL area code block (ID 720).\n\nThe relative weight of these different feature vectors can be learned during the active learning process and expressed numerically to ensure that features that will be most predictive of matches will be heavier in the overall matching schema. As the user labels more and more tuples, Dedupe gradually relearns the weights, recalculates the edit distances between records, and updates its list of the most uncertain pairs to propose to the user for labeling.\n\nOnce the user has generated enough labels, the learned weights are used to calculate the probability that each pair of records within a block is a duplicate or not. In order to scale the pairwise matching up to larger tuples of matched records (in the case that entities may appear more than twice within a document), Dedupe uses hierarchical clustering with centroidal linkage. Records within some threshold distance of a centroid will be grouped together. The final result is an annotated version of the original dataset that now includes a centroid label for each record.\n\nYou can see that is a command line application that will prompt the user to engage in active learning by showing pairs of entities and asking if they are the same or different.\n\nActive learning is the so-called special sauce behind Dedupe. As in most supervised machine learning tasks, the challenge is to get labeled data that the model can learn from. The active learning phase in Dedupe is essentially an extended user-labeling session, which can be short if you have a small dataset and can take longer if your dataset is large. You are presented with four options:\n\nYou can experiment with typing the y, n, and u keys to flag duplicates for active learning. When you are finished, enter f to quit.\n\nAs you can see in the example above, some comparisons decisions are very easy. The first contains zero for zero hits on all four attributes being examined, so the verdict is most certainly a non-match. On the second, we have a 3/4 exact match, with the fourth being fuzzy in that one entity contains a piece of the matched entity; Ryerson vs. Chicago Public Schools Ryerson. A human would be able to discern these as two references to the same entity, and we can label it as such to enable the supervised learning that comes after the active learning.\n\nThe csv_example also includes an evaluation script that will enable you to determine how successfully you were able to resolve the entities. It\u2019s important to note that the blocking, active learning and supervised learning portions of the deduplication process are very dependent on the dataset attributes that the user nominates for selection. In the csv_example, the script nominates the following four attributes:\n\nA different combination of attributes would result in a different blocking, a different set of , a different set of features to use in the active learning phase, and almost certainly a different result. In other words, user experience and domain knowledge factor in heavily at multiple phases of the deduplication process.\n\nIn order to try out Dedupe with a more challenging project, we decided to try out deduplicating the White House visitors\u2019 log. Our hypothesis was that it would be interesting to be able to answer questions such as \u201cHow many times has person X visited the White House during administration Y?\u201d However, in order to do that, it would be necessary to generate a version of the list that contained unique entities. We guessed that there would be many cases where there were multiple references to a single entity, potentially with slight variations in how they appeared in the dataset. We also expected to find a lot of names that seemed similar but in fact referenced different entities. In other words, a good challenge!\n\nThe data set we used was pulled from the WhiteHouse.gov website, a part of the executive initiative to make federal data more open to the public. This particular set of data is a list of White House visitor record requests from 2006 through 2010. Here\u2019s a snapshot of what the data looks like via the White House API.\n\nThe dataset includes a lot of columns, and for most of the entries, the majority of these fields are blank:\n\nUsing the API, the White House Visitor Log Requests can be exported in a variety of formats to include, .json, .csv, and .xlsx, .pdf, .xlm, and RSS. However, it\u2019s important to keep in mind that the dataset contains over 5 million rows. For this reason, we decided to use .csv and grabbed the data using :\n\nOnce downloaded, we can clean it up and load it into a database for more secure and stable storage.\n\nNext, we\u2019ll discuss what is needed to tailor a example to get the code to work for the White House visitors log dataset. The main challenge with this dataset is its sheer size. First, we'll need to import a few modules and connect to our database:\n\nThe other challenge with our dataset are the numerous missing values and datetime formatting irregularities. We wanted to be able to use the datetime strings to help with entity resolution, so we wanted to get the formatting to be as consistent as possible. The following script handles both the datetime parsing and the missing values by combining Python\u2019s module and PostgreSQL's fairly forgiving 'varchar' type.\n\nThis function takes the csv data in as input, parses the datetime fields we\u2019re interested in (\u2018lastname\u2019,\u2019firstname\u2019,\u2019uin\u2019,\u2019apptmade\u2019,\u2019apptstart\u2019,\u2019apptend\u2019, \u2018meeting_loc\u2019.), and outputs a database table that retains the desired columns. Keep in mind this will take a while to run.\n\nAbout 60 of our rows had ASCII characters, which we dropped using this SQL command:\n\nFor our deduplication script, we modified the PostgreSQL example as well as Dan Chudnov\u2019s adaptation of the script for the OSHA dataset.\n\nInitially, we wanted to try to use the datetime fields to deduplicate the entities, but was not a big fan of the datetime fields, whether in isoformat or ordinal, so we ended up nominating the following fields:\n\nWe modified a function Dan wrote to generate the predicate blocks:\n\nAnd we adapted the method from the dedupe-examples repo to handle the active learning, supervised learning, and clustering steps:\n\nWe observed a lot of uncertainty during the active learning phase, mostly because of how enormous the dataset is. This was particularly pronounced with names that seemed more common to us and that sounded more domestic since those are much more commonly occurring in this dataset. For example, are two records containing the name Michael Grant the same entity?\n\nAdditionally, we noticed that there were a lot of variations in the way that middle names were captured. Sometimes they were concatenated with the first name, other times with the last name. We also observed what seemed to be many nicknames or that could have been references to separate entities: KIM ASKEW vs. KIMBERLEY ASKEW and Kathy Edwards vs. Katherine Edwards (and yes, does preserve variations in case). On the other hand, since nicknames generally appear only in people's first names, when we did see a short version of a first name paired with an unusual or rare last name, we were more confident in labeling those as a match.\n\nOther things that made the labeling easier were clearly gendered names (e.g. Brian Murphy vs. Briana Murphy), which helped us to identify separate entities in spite of very small differences in the strings. Some names appeared to be clear misspellings, which also made us more confident in our labeling two references as matches for a single entity (Davifd Culp vs. David Culp). There were also a few potential easter eggs in the dataset, which we suspect might actually be aliases (Jon Doe and Ben Jealous).\n\nOne of the things we discovered upon multiple runs of the active learning process is that the number of fields the user nominates to Dedupe for use has a great impact on the kinds of predicate blocks that are generated during the initial blocking phase. Thus, the comparisons that are presented to the trainer during the active learning phase. In one of our runs, we used only the last name, first name, and meeting location fields. Some of the comparisons were easy:\n\nWhat we realized from this is that there are two different kinds of duplicates that appear in our dataset. The first kind of duplicate is one that generated via (likely mistaken) duplicate visitor request forms. We noticed that these duplicate entries tended to be proximal to each other in terms of visitor_id number, have the same meeting location and the same uin (which confusingly, is not a unique guest identifier but appears to be assigned to every visitor within a unique tour group). The second kind of duplicate is what we think of as the frequent flier \u2014 people who seem to spend a lot of time at the White House like staffers and other political appointees.\n\nDuring the dedupe process, we computed there were 332,606 potential duplicates within the data set of 1,048,576 entities. For this particular data, we would expect these kinds of figures, knowing that people visit for repeat business or social functions.\n\nIn this beginners guide to Entity Resolution, we learned what it means to identify entities and their possible duplicates within and across records. To further examine this data beyond the scope of this blog post, we would like to determine which records are true duplicates. This would require additional information to canonicalize these entities, thus allowing for potential indexing of entities for future assessments. Ultimately we discovered the importance of entity resolution across a variety of domains, such as counter-terrorism, customer databases, and voter registration."
    },
    {
        "url": "https://medium.com/district-data-labs/data-exploration-with-python-part-2-4712930d16d6?source=---------5",
        "title": "Data Exploration with Python, Part 2 \u2013 District Data Labs \u2013",
        "text": "This is the second post in our Data Exploration with Python series. Before reading this post, make sure to check out Data Exploration with Python, Part 1!\n\nWhen performing exploratory data analysis (EDA), it is important to not only prepare yourself (the analyst) but to prepare your data as well. As we discussed in the previous post, a small amount of preparation will often save you a significant amount of time later on. So let\u2019s review where we should be at this point and then continue our exploration process with data preparation.\n\nIn Part 1 of this series, we were introduced to the data exploration framework we will be using. As a reminder, here is what that framework looks like.\n\nWe also introduced the example data set we are going to be using to illustrate the different phases and stages of the framework. Here is what that looks like.\n\nWe then familiarized ourselves with our data set by identifying the types of information and entities encoded within it. We also reviewed several data transformation and visualization methods that we will use later to explore and analyze it. Now we are at the last stage of the framework\u2019s Prep Phase, the Create stage, where our goal will be to create additional categorical fields that will make our data easier to explore and allow us to view it from new perspectives.\n\nBefore we dive in and start creating categories, however, we have an opportunity to improve our categorization efforts by examining the columns in our data and making sure their labels intuitively convey what they represent. Just as with the other aspects of preparation, changing them now will save us from having to remember what or mean when they show up on a chart later. In my experience, these small, detail-oriented enhancements to the beginning of your process usually compound and preserve cognitive cycles that you can later apply to extracting insights.\n\nWe can use the code below to rename the columns in our vehicles data frame.\n\nNow that we have changed our column names to be more intuitive, let\u2019s take a moment to think about what categorization is and examine the categories that currently exist in our data set. At the most basic level, categorization is just a way that humans structure information \u2014 how we hierarchically create order out of complexity. Categories are formed based on attributes that entities have in common, and they present us with different perspectives from which we can view and think about our data.\n\nOur primary objective in this stage is to create additional categories that will help us further organize our data. This will prove beneficial not only for the exploratory analysis we will conduct but also for any supervised machine learning or modeling that may happen further down the data science pipeline. Seasoned data scientists know that the better your data is organized, the better downstream analyses you will be able to perform and the more informative features you will have to feed into your machine learning models.\n\nIn this stage of the framework, we are going to create additional categories in 3 distinct ways:\n\nNow that we have a better idea of what we are doing and why, let\u2019s get started.\n\nThe first way we are going to create additional categories is by identifying opportunities to create higher-level categories out of the variables we already have in our data set. In order to do this, we need to get a sense of what categories currently exist in the data. We can do this by iterating through our columns and printing out the name, the number of unique values, and the data type for each.\n\nFrom looking at the output, it is clear that we have some numeric columns (int64 and float64) and some categorical columns (object). For now, let\u2019s focus on the six categorical columns in our data set.\n\nWhen aggregating and summarizing data, having too many categories can be problematic. The average human is said to have the ability to hold 7 objects at a time in their short-term working memory. Accordingly, I have noticed that once you exceed 8\u201310 discrete values in a category, it becomes increasingly difficult to get a holistic picture of how the entire data set is divided up.\n\nWhat we want to do is examine the values in each of our categorical variables to determine where opportunities exist to aggregate them into higher-level categories. The way this is typically done is by using a combination of clues from the current categories and any domain knowledge you may have (or be able to acquire).\n\nFor example, imagine aggregating by Transmission, which has 43 discrete values in our data set. It is going to be difficult to derive insights due to the fact that any aggregated metrics are going to be distributed across more categories than you can hold in short-term memory. However, if we examine the different transmission categories with the goal of finding common features that we can group on, we would find that all 43 values fall into one of two transmission types, Automatic or Manual.\n\nLet\u2019s create a new Transmission Type column in our data frame and, with the help of the method in pandas, assign it a value of Automatic where the first character of Transmission is the letter A and a value of Manual where the first character is the letter M.\n\nWe can apply the same logic to the Vehicle Class field. We originally have 34 vehicle classes, but we can distill those down into 8 vehicle categories, which are much easier to remember.\n\nNext, let\u2019s look at the Make and Model fields, which have 126 and 3,491 unique values respectively. While I can\u2019t think of a way to get either of those down to 8\u201310 categories, we can create another potentially informative field by concatenating Make and the first word of the Model field together into a new Model Type field. This would allow us to, for example, categorize all Chevrolet Suburban C1500 2WD vehicles and all Chevrolet Suburban K1500 4WD vehicles as simply Chevrolet Suburbans.\n\nFinally, let\u2019s look at the Fuel Type field, which has 13 unique values. On the surface, that doesn\u2019t seem too bad, but upon further inspection, you\u2019ll notice some complexity embedded in the categories that could probably be organized more intuitively.\n\nThis is interesting and a little tricky because there are some categories that contain a single fuel type and others that contain multiple fuel types. In order to organize this better, we will create two sets of categories from these fuel types. The first will be a set of columns that will be able to represent the different combinations, while still preserving the individual fuel types.\n\nAs it turns out, 99% of the vehicles in our database have gas as a fuel type, either by itself or combined with another fuel type. Since that is the case, let\u2019s create a second set of categories \u2014 specifically, a new Gas Type field that extracts the type of gas (Regular, Midgrade, Premium, Diesel, or Natural) each vehicle accepts.\n\nAn important thing to note about what we have done with all of the categorical fields in this section is that, while we created new categories, we did not overwrite the original ones. We created additional fields that will allow us to view the information contained within the data set at different (often higher) levels. If you need to drill down to the more granular original categories, you can always do that. However, now we have a choice whereas before we performed these category aggregations, we did not.\n\nThe next way we can create additional categories in our data is by binning some of our continuous variables \u2014 breaking them up into different categories based on a threshold or distribution. There are multiple ways you can do this, but I like to use quintiles because it gives me one middle category, two categories outside of that which are moderately higher and lower, and then two extreme categories at the ends. I find that this is a very intuitive way to break things up and provides some consistency across categories. In our data set, I\u2019ve identified 4 fields that we can bin this way.\n\nBinning essentially looks at how the data is distributed, creates the necessary number of bins by splitting up the range of values (either equally or based on explicit boundaries), and then categorizes records into the appropriate bin that their continuous value falls into. Pandas has a method that makes binning extremely easy, so let's use that to create our quintiles for each of the continuous variables we identified.\n\nThe final way we are going to prepare our data is by clustering to create additional categories. There are a few reasons why I like to use clustering for this. First, it takes multiple fields into consideration together at the same time, whereas the other categorization methods only consider one field at a time. This will allow you to categorize together entities that are similar across a variety of attributes, but might not be close enough in each individual attribute to get grouped together.\n\nClustering also creates new categories for you automatically, which takes much less time than having to comb through the data yourself identifying patterns across attributes that you can form categories on. It will automatically group similar items together for you.\n\nThe third reason I like to use clustering is because it will sometimes group things in ways that you, as a human, may not have thought of. I\u2019m a big fan of humans and machines working together to optimize analytical processes, and this is a good example of value that machines bring to the table that can be helpful to humans. I\u2019ll write more about my thoughts on that in future posts, but for now, let\u2019s move on to clustering our data.\n\nThe first thing we are going to do is isolate the columns we want to use for clustering. These are going to be columns with numeric values, as the clustering algorithm will need to compute distances in order to group similar vehicles together.\n\nNext, we want to scale the features we are going to cluster on. There are a variety of ways to normalize and scale variables, but I\u2019m going to keep things relatively simple and just use Scikit-Learn\u2019s , which will divide each value by the max absolute value for that feature. This will preserve the distributions in the data and convert the values in each field to a number between 0 and 1 (technically -1 and 1, but we don't have any negatives).\n\nNow that our features are scaled, let\u2019s write a couple of functions. The first function we are going to write is a function that will k-means cluster a given data frame into a specified number of clusters. It will then return a copy of the original data frame with those clusters appended in a column named Cluster.\n\nOur second function, called is going to count the number of vehicles that fall into each cluster and calculate the cluster means for each feature. It is going to merge the counts and means into a single data frame and then return that summary to us.\n\nWe now have functions for what we need to do, so the next step is to actually cluster our data. But wait, our function is supposed to accept a number of clusters. How do we determine how many clusters we want?\n\nThere are a number of approaches for figuring this out, but for the sake of simplicity, we are just going to plug in a couple of numbers and visualize the results to arrive at a good enough estimate. Remember earlier in this post where we were trying to aggregate our categorical variables to less than 8\u201310 discrete values? We are going to apply the same logic here. Let\u2019s start out with 8 clusters and see what kind of results we get.\n\nAfter running the couple of lines of code above, your should look similar to the following.\n\nBy looking at the Count column, you can tell that there are some clusters that have significantly more records in them (ex. Cluster 7) and others that have significantly fewer (ex. Cluster 3). Other than that, though, it is difficult to notice anything informative about the summary. I don\u2019t know about you, but to me, the rest of the summary just looks like a bunch of decimals in a table.\n\nThis is a prime opportunity to use a visualization to discover insights faster. With just a couple import statements and a single line of code, we can light this summary up in a heatmap so that we can see the contrast between all those decimals and between the different clusters.\n\nIn this heatmap, the rows represent the features and the columns represent the clusters, so we can compare how similar or differently columns look to each other. Our goal for clustering these features is to ultimately create meaningful categories out of the clusters, so we want to get to the point where we can clearly distinguish one from the others. This heatmap allows us to do this quickly and visually.\n\nWith this goal in mind, it is apparent that we probably have too many clusters because:\n\nFrom the way our heatmap currently looks, I\u2019m willing to bet that we can cut the number of clusters in half and get clearer boundaries. Let\u2019s re-run the clustering, summary, and heatmap code for 4 clusters and see what kind of results we get.\n\nThese clusters look more distinct, don\u2019t they? Clusters 1 and 3 look like they are polar opposites of each other, cluster 0 looks like it\u2019s pretty well balanced across all the features, and cluster 2 looks like it\u2019s about half-way between Cluster 0 and Cluster 1.\n\nWe now have a good number of clusters, but we still have a problem. It is difficult to remember what clusters 0, 1, 2, and 3 mean, so as a next step, I like to assign descriptive names to the clusters based on their properties. In order to do this, we need to look at the levels of each feature for each cluster and come up with intuitive natural language descriptions for them. You can have some fun and can get as creative as you want here, but just keep in mind that the objective is for you to be able to remember the characteristics of whatever label you assign to the clusters.\n\nNow that we have come up with these descriptive names for our clusters, let\u2019s add a Cluster Namecolumn to our data frame, and then copy the cluster names over to our original data frame.\n\nIn this post, we examined several ways to prepare a data set for exploratory analysis. First, we looked at the categorical variables we had and attempted to find opportunities to roll them up into higher-level categories. After that, we converted some of our continuous variables into categorical ones by binning them into quintiles based on how relatively high or low their values were. Finally, we used clustering to efficiently create categories that automatically take multiple fields into consideration. The result of all this preparation is that we now have several columns containing meaningful categories that will provide different perspectives of our data and allow us to acquire as many insights as possible.\n\nNow that we have these meaningful categories, our data set is in really good shape, which means that we can move on to the next phase of our data exploration framework. In the next post, we will cover the first two stages of the Explore Phase and demonstrate various ways to visually aggregate, pivot, and identify relationships between fields in our data."
    },
    {
        "url": "https://medium.com/district-data-labs/forward-propagation-building-a-skip-gram-net-from-the-ground-up-9578814b221?source=---------6",
        "title": "Forward Propagation: Building a Skip-Gram Net From the Ground Up",
        "text": "This post is part of a series based on the research conducted in District Data Labs\u2019 NLP Research Lab. Make sure to check out the other posts in the series so far:\n\nLet\u2019s continue our treatment of the Skip-gram model by traversing forward through an single example of feeding forward through a Skip-gram neural network; from an input target word, through a projection layer, to an output context vector representing the target word\u2019s nearest neighbors. Before we get into our example, though, let\u2019s revisit some fundamentals on neural networks.\n\nNeural networks originally got their name from borrowing concepts observed in the functioning of the biological neural pathways in the brain. At a very basic level, there is a valid analogy between a node in a neural network and the neurons in a biological brain worth using to explain the fundamental concepts.\n\nThe biological model of a neural pathway consists of specialized cells called dendrites that receive chemical signals in one end that build up an electric potential until a threshold is reached (called activation), resulting in more or different chemical signals to be released on the other end. These output chemicals can then act as input to other neurons, causing them to activate as well. In the biological model, a combination of activated neurons can be interpreted in such a way to cause an action on behalf of the organism.\n\nThe computational model of a neural network represents this process mathematically by propagating input data in a particular way through a graph structure containing nodes inside an input layer, hidden layer, and output layer. The input layer represents the input data, analogous to the incoming chemical signals of a neuron. The hidden layer represents the neurons receiving the input signal. The output layer is a simplification of the decision interpretation of the biological model; it represents the decision or action a given activated pathway indicates. In other words, the output layer is a classifier.\n\nThe input data, which in the biological system are chemical signals but in the computational model are numerical quantities, are made available to all potential hidden layer nodes through a linear transformation of a weight layer. This provides a way for the computational model to represent that not all nodes receive the input data evenly or with the same affinity.\n\nThe hidden layer, representing our neurons, receives this input and then applies a non-linear activation function to simulate neurons\u2019 activation. The simplest activation function is the Heaviside step function which simply turns a hidden layer node to on (1) or off (0) (and a neural network that uses this type of activation function is nicknamed a perceptron). More common activation functions in language models are the logit function, the hyperbolic tangent, and their variants.\n\nThe output layer is produced by a linear transformation from the hidden layer through another weight matrix. This provides a way for the computational model to represent that activated nodes have varying effects on the final decision interpretation.\n\nBelow is a figure from the Appendix of the paper \u201cword2vec Parameter Learning Explained\u201d by Rong et al that represents this process. The input layer = is linearly transformed through a weight matrix . That input is activated with an activation function to produce the hidden layer = . The resulting hidden layer is linearly transformed through another weight matrix to finally result in the output layer = .\n\nThe learning part of both a biological brain and a computational neural network has to do with strengthening or weakening certain paths given certain input data. The biological system to achieve this is well beyond my ability to explain, but on the computational side we simulate it by modifying our weight layers in reaction to some objective function (a term often conflated with a related term cost function and which for our purposes are synonymous). Knowing what our input layer looks like and what our output layer should represent, we pick some objective function that allows us to compare our output layer classifier with the expected result for a given input. In other words, we pick something the output layer should represent (like a probability) that we can compare to the subset of reality represented by our training data (like an observed frequency).\n\nBy iteratively training examples and penalizing the weight layers for the error observed based on our interpretation of the output layer, the net eventually learns how to output the values that mean what we want. We use a mathematical technique called the chain rule to estimate the effect of different parts of the network (e.g. only the second weight layer, or only the first weight layer) on the total error. We then modify each piece accordingly using an algorithmic technique called gradient descent (or ascent, depending on whether you are maximizing or minimizing your objective function). In practice, we use a less computationally expensive version of gradient descent called stochastic gradient descent that randomly applies the chain rule as opposed to after every single training example.\n\nOnce you have reached a predefined threshold for your objective function (or got tired of running your network for so long), your model is complete. You would now be able to submit new input data and receive a classification result based on all the examples you trained with in the first place.\n\nNow that we\u2019ve reestablished some fundamentals, let\u2019s set up the specific architecture of Skip-gram. Recall that the goal of Skip-gram is to learn the probability distribution of words in our vocabulary being within a given distance (context) of an input word. We are effectively choosing our output layer to represent that probability distribution. Once we\u2019ve trained the network with this goal, instead of using the model to predict the context probability distribution for future words, we\u2019ll derive distributed representations of words we saw (word embeddings) from the input weight layer of the final model.\n\nAs you may recall from our previous post, our example corpus consisted of the following sentence.\n\nIn that post, we preprocessed this sentence by removing punctuation and capitalization, dropping stopwords, and stemming the remaining words. This left us with \u201cduct tape work anywher duct tape magic worship.\u201d\n\nLet\u2019s start by defining some terminology and the variables you\u2019ll see associated with them throughout this post.\n\nA vocabulary (v), is a deduplicated, ordered list of all the distinct words in our corpus. The specific order of the words doesn\u2019t matter, as long as it stays the same throughout the entire process.\n\nA context \u00a9 is a zone, or window, of words before and after a target word that we want to consider \u201cnear\u201d the target word. We can select a context of our choice, which is 2 in the example below.\n\nWe have another selection to make, this time about the projection layer. We can specify the size of the projection layer in nodes (n). The higher the number of nodes, the higher dimensionality the projection between layers will be and the higher dimensionality your final word embeddings will be. We\u2019ll look at this in more detail a little later. For now, let\u2019s just remember that n is a tunable parameter, and let\u2019s set it to 3.\n\nBefore we actually get to working through the neural network, let\u2019s reconsider how we will provide each input word to it.\n\nWhat do each of these words look like when they go into the neural network? To perform transformations on them through each layer, we have to represent the words numerically. Additionally, each word\u2019s representation must be unique. There are many vector-based approaches to represent words to a neural network, as discussed in the first post in this series. In the case of Skip-gram, each input word is represented as a one-hot encoded vector. This means that each word is represented by a vector of length , where the index of the target word in v contains the value 1 and all other indices contain the value 0.\n\nWe would expect to represent each word in our vocabulary with the following one-hot encoded vectors:\n\nThis selection of one-hot encoded vectors, as the representation of words to the Skip-gram net, is actually going to become quite important later. But let\u2019s get to some forward propagation now.\n\nThe Skip-gram neural network is a shallow neural network consisting of an input layer, a single linear projection layer, and an output layer. The input layer is a one-hot encoded vector representing the input word, and the output layer is a probability distribution of what words are likely to be seen in the input word\u2019s context. The objective function of the net attempts to maximize the output layer probability distribution against what is known from the source corpora about a given word\u2019s context frequency distribution.\n\nThat is quite dense, and several of these parts are important, but I want to point out a major divergence from the classic neural network model that we just described. Classic neural nets have hidden layers transformed with a non-linear activation function as described in the neural net refresher above \u2014 Heaviside step functions, logit, hyperbolic tangent. One of the major computational tradeoffs Mikolov et al made to make Skip-gram and CBOW feasible whereas earlier neural nets were prohibitively limited by training time was to completely remove the activation step from the hidden layer. From the paper Efficient Estimation of Word Representations in Vector Space, they state:\n\nTherefore, what a classical neural net would term a \u2018hidden layer\u2019 is generally regarded instead as a projection layer for the purposes of Skim-gram and CBOW. In Mikolov et al, they distinguish these two models from classic, non-linearly activated neural nets by calling them log-linear models.\n\nTo train the net, we scan through our source corpus, submitting each word we encounter once for each output context. The input word is projected through a weight layer and then transformed through another weight layer into an output context. Each output node is of size v and contains at each index a score for that index's word, estimating the word's likelihood of appearing in that context location.\n\nBelow is a reproduction of the architecture diagram from Google\u2019s introductory Skip-gram paper, Efficient Estimation of Word Representations in Vector Space.\n\nLet\u2019s break that down again using the terms in the diagram. The Skip-gram neural net iterates through a series of words one at a time, as input w(t). Each input word w(t) is fed forward through the network times, once for each output context vector. Each time w(t) is fed through the network, it is linearly transformed through two weight matrices to an output layer that contains nodes representing a context location: where , those context locations are one of w(t-2) to w(t+2). The output nodes, each the size of the vocabulary, contain scores at each index estimating the likelihood that a word in the vocabulary would appear in that context position. For example, with the input word \"tape\" the score for the w(t-2) vector of length v at its second index (w(t-2)[2]) would be a score for the likelihood that the word \"work\" would appear in context two words behind the input word \"tape.\" Those raw scores are later turned into actual probabilities using the softmax equation.\n\nFor each given training instance, the net will calculate its error between the probability generated for each word in each context location and the observed reality of the words in the context of the training instance. For example, the net may calculate that \u201cwork\u201d has a 70% chance of showing up two words before the word \u201ctape\u201d, but we can determine from the source corpus that the probability is really 0%. Through the process of backpropagation, the net will modify the weight matrices to change how it projects the input layer through to the output layer in order to minimize its error: for example, to minimize the error between the calculated 70% and the observed 0%. Then the next word in the corpus will be sent as an input times, then the next, and so on.\n\nOnce the net is done training, the first weight layer will contain the vector representations of each word in the vocabulary, in order. After we go through all of feed-forward and backprop, you\u2019ll see why, but for now while we\u2019ve got the gist of the architecture, let\u2019s try plugging in some numbers.\n\nTime for some code! Let\u2019s get a more detailed, color-coded diagram going:\n\nThe figure above is an extended depiction of a Skip-gram network for a training example feeding forward to output context vectors representing a context window . The input word is a one-hot encoded vector the size of the vocabulary. This is connected to a projection layer defined by our node size parameter to be of size len(n). The input layer is projected via linear transformation through the input weight matrix p, of v x n dimensions. With a context size of 2, each training example will feed forward 4 times to 4 output vectors: w(t-2), w(t-1), w(t+1) and w(t+2). The projection layer is connected to the output layers by the output weight matrix p', which has n x v dimensions.\n\nFrom left to right, each subsequent layer is constructed mathematically simply by taking the dot product of a layer and its weight matrix.\n\nThe weight matrices p and p\u2019 can be initialized in many ways, and will eventually be tuned using backpropagation. For now, let\u2019s initialize the net simply with random numbers. Let\u2019s expand our example with those randomized matrices and calculate the transformations from the input word to the projection layer and from the projection layer to the output layer. Let\u2019s consider a forward pass through the net where our input word is the second word in our vocabulary, \u201ctape.\u201d\n\nFirst let\u2019s import our packages and set up our first two pieces, the input array for \u201ctape\u201d (which you\u2019ll recall is one-hot encoded), and our randomized first weight layer p:\n\nNow, let\u2019s calculate from the input layer to the projection layer. You\u2019ll see here that the effect of a linear transformation from the one-hot encoded layer through the weight layer means we are simply projecting a row of the weight matrix matching the index of the one-hot encoded input word through to the next layer. That is why we keep calling this process \u2018projection\u2019 and are shying away from the terms \u2018hidden\u2019 or \u2018activation\u2019 layer (though those terms are still sometimes used to describe this process). You can compare the output vector below to the matrix above and see that clearly \u2014 we\u2019ve simply picked out the vector from the second matrix row of p during this projection process.\n\nWe\u2019ll use that to calculate from the projection layer to the first output context vector w(t-2).\n\nAt this point, we have the first output vector after we\u2019ve performed calculations against the randomized weight layers for p and the p\u2019. For the purposes of the diagram I have rounded all the indices in our matrices to their tenths place, but let\u2019s take a look to re-establish where we are.\n\nNow that we have this context vector, let\u2019s dive in again to what the context vectors really are. Each context vector is the length of the vocabulary, and since the vocabulary is an ordered list, each index in the context vector can be traced back to a certain word in the vocabulary. The meaning of the value at each index in the context vector is an estimation of the likelihood of appearing in that context window for the word in the vocabulary that index traces back to.\n\nLet\u2019s unpack that some more with the following code snippets. The variable is the output vector for that orange colored context w(t-2) we just calculated given the input word \"tape\". The value at each index of that vector represents each vocabulary word's estimated likelihood to be 2 words behind \"tape\", per the vocabulary order of our original vocabulary vector .\n\nWe know the vocabulary is made up of this ordered list.\n\nIf we zip those together, we annotate each index in the context vector that estimates the likelihood of being 2 words behind \u201ctape\u201d with what word in the vocabulary we\u2019re estimating for:\n\nSince this whole net at this point was trained with the input word \u201ctape,\u201d we can extrapolate from this output vector for w(t-2) that the word \u201cduct\u201d is estimated to be two words behind our input word \u201ctape\u201d with a likelihood score of 0.42451663675598933, that that the word \u201ctape\u201d is estimated to be two words behind our input word \u201ctape\u201d with a likelihood score of 0.32344971050993732, and so on.\n\nIntuitively, we can see that we have very similar scores right now for each word, though each word is not equally likely to appear at position w(t-2), aka two spots before our target word, since we\u2019ve seen our corpus. We will use backpropagation to calculate our error here against the known likelihood of these words in context and adjust the weight vectors to reduce our error. However, there is one more processing step to these output vectors before we\u2019re ready to have the net begin backpropagation: applying the softmax equation. The softmax will normalize the values in each context vector to represent probabilities, so we can directly compare them during backpropagation to the known frequency distributions in the source corpus. Softmax will be the topic of the second half of this feedforward post series, so stay tuned!"
    },
    {
        "url": "https://medium.com/district-data-labs/data-exploration-with-python-part-1-643fda933479?source=---------7",
        "title": "Data Exploration with Python, Part 1 \u2013 District Data Labs \u2013",
        "text": "Exploratory data analysis (EDA) is an important pillar of data science, a critical step required to complete every project regardless of the domain or the type of data you are working with. It is exploratory analysis that gives us a sense of what additional work should be performed to quantify and extract insights from our data. It also informs us as to what the end product of our analytical process should be. Yet, in the decade that I\u2019ve been working in analytics and data science, I\u2019ve often seen people grasping at straws when it comes to exploring their data and trying to find insights.\n\nHaving witnessed the lack of structure in conventional approaches, I decided to document my own process in an attempt to come up with a framework for data exploration. I wanted the resulting framework to provide a more structured path to insight discovery: one that allows us to view insight discovery as a problem, break that problem down into manageable components, and then start working toward a solution. I\u2019ve been speaking at conferences over the last year about this framework. It has been very well-received, so I wanted to share it with you in the form of this blog post series.\n\nThe framework I came up with, pictured below, consists of a Prep Phase and an Explore Phase. Each phase has several steps in it that we will walk through together as we progress through this series.\n\nThe series will have four parts. The first two posts will cover the Prep Phase, and the second two posts will cover the Explore Phase.\n\nThere is an important point about the framework\u2019s two phases that I would like to stress here. While you can technically complete the Explore Phase without the Prep Phase, it is the Prep Phase that is going to allow you to explore your data both better and faster. In my experience, I have found that the time it takes to complete the Prep Phase is more than compensated for by the time saved not fumbling around in the Explore Phase. We are professionals, this is part of our craft, and proper preparation is important to doing our craft well.\n\nI hope you enjoy the series and are able to walk away from it with an intuitive, repeatable framework for thinking about, analyzing, visualizing, and discovering insights from your data. Before we jump into looking at our example data set and applying the framework to it, however, I would like to lay some foundation, provide some context, and share with you how I think about data.\n\nAt the most basic level, we can think of data as just encoded information. But the critical thing about this is that information can be found everywhere. Everything you know, everything you can think of, everything you encounter every moment of every day is information that has the potential to be captured, encoded, and therefore turned into data. The ubiquity of these potential data sources turns our world into a web of complexity. We can think of each data set we encounter as a slice of this complexity that has been put together in an attempt to communicate something about the world to a human or a machine.\n\nAs humans, we have an inherent ability to deal with this complexity and the vast amounts of information we are constantly receiving. The way we do this is by organizing things and putting things in order. We create categories, hierarchical classifications, taxonomies, tags, and other systems to organize our information. These constructs help provide order and structure to the way we view the world, and they allow us to be able to look at something, recognize the similarities it has to a group of other things, and quickly make decisions about it.\n\nThe fact that we have this ability to create order out of complexity is wonderful, and we use it all the time without ever thinking about it. Unfortunately, that includes when we\u2019re analyzing data, and that often makes our processes not reproducible, repeatable, or reliable. So I wanted my data exploration framework to explicitly take advantage of this ability and help people make better use of it in their workflows.\n\nThe data set we will be applying this framework to throughout this series is the Environmental Protection Agency\u2019s Vehicle Fuel Economy data set. Here is what the data looks like after some light clean-up.\n\nTo get the data looking like this, we first need to download the data to disk.\n\nFrom there, we are going to load it into a pandas data frame.\n\nAnd finally, we are going to clean it up by dropping columns we don\u2019t need, removing vehicles that are coming out in the future, removing any duplicate records, and then sorting the data by make, model, and year.\n\nNow that we have a clean data set, let\u2019s jump into the framework, beginning with the Prep Phase. The first thing we\u2019re going to do is identify the types of information contained in our data set, which will help us get to know our data a bit better and prepare us to think about the data in different ways. After that, we will identify the entities in our data set so that we are aware of the different levels to which we can aggregate up or drill down.\n\nThere are a few distinct types of information that jump out at us just from taking a quick look at the data set.\n\nThere are also some other types of information in our data that may not be as obvious. Since we have the year the vehicle was manufactured, we can observe changes in the data over time. We also have relationship information in the data, both between fields and between the entities. And since we have both a time variable as well as information about relationships, we can learn how those relationships have changed over time.\n\nThe next step in the Prep Phase is to identify the entities in our data. Now, what exactly do I mean by entities? When I refer to entities, I\u2019m referring to the individual, analyzable units in a data set. To conduct any type of analysis, you need to be able to distinguish one entity from another and identify differences between them. Entities are also usually part of some hierarchical structure where they can be aggregated into one or more systems, or higher-level entities, to which they belong. Now that we have defined what an entity is, let\u2019s take a look at the different levels of them that are present in our data set.\n\nBeginning at Level 1 (which is the most granular level in the data) \u2014 you can see the year and specific model of vehicle. The next level we can aggregate up to from there is year and model type, which is slightly less granular. From there, we have a few different directions we can pursue: year and vehicle class, year and make, or we can remove year and only keep model type. Finally, at Level 4, we can further aggregate the data to just the vehicle classes, the years, or the makes.\n\nTo illustrate even further, here are some actual examples of entities in our data set.\n\nAt Level 1, which was the year and the model, we have a 2016 Ford Mustang with a 2.3 liter V4 engine, automatic transmission, and rear-wheel drive. At Level 2, we can roll things up and look at all 2016 Ford Mustangs as one entity that we\u2019re analyzing. Then at Level 3, we can either make our entities all 2016 Subcompact Cars, all 2016 Fords, or all Ford Mustangs regardless of the year they were manufactured. From there, we can continue going up the hierarchy.\n\nAgain, doing this is important, and it will help you think about all the things you can do to the data and all the different ways you can look at it later on. I see a lot of people that are new to data science who don\u2019t do this. They don\u2019t think about their data this way, and because of that, they end up missing valuable insights that they would have otherwise discovered. I hope that these examples help make it easier to think about data this way.\n\nThe next step in the Prep Phase is to review some transformation and visualization methods. Doing this will ensure that we are aware of the tools we have in our analytic arsenal, what they should be used for, and when to utilize each one.\n\nThe first methods we will cover are the transformation methods. Let\u2019s take a look at some of my favorite ways to transform data.\n\nThe first method I have listed here is Filtering, which is making the data set smaller by looking at either fewer rows, fewer columns, or both. The next method on the list is Aggregation/Disaggregation. This is the process of changing the levels at which you are analyzing the data, getting either more or less granular. Then we have Pivoting, which is the process of aggregating by multiple variables along two axes \u2014 the rows and the columns. Finally, we have Graph Transformation, which is the process of linking your entities based on shared attributes and examining how they relate to one another.\n\nBy transforming the data, you are ultimately altering its structure, which allows you to look at it from several perspectives. And just like looking at anything else from different perspectives, you will learn something new from each way that you view it. The remarkable thing about this is that the number of ways you can transform the data is limited only by your creativity and your imagination. This, for me, is one of the most exciting things about working with data \u2014 all the things you can do to it and all the creative ways that you can transform it.\n\nIn addition to transforming the data, I also like to go a step further and visualize it, as sometimes the transformations you perform can be difficult to interpret. Converting your transformations to visualizations allows you to bring the human visual cortex into your analytical process, and I often find that this helps me find more insights faster, since the visual component of it makes the insights jump right out at me.\n\nBecause of this, transformation and visualization go hand-in-hand. Since there are a variety of ways to transform data, there are also several ways you can visualize it. I like to keep things relatively simple, so here are some of the visualization methods I use most often.\n\nThe first visualization method on the list is Bar charts, which help you intuitively view aggregations by comparing the size or magnitude of higher-level entities in the data. Bar charts are simple, but they can be very useful, which is why they are one of the most popular types of visualization methods. Next, we have Multi-line Graphs, which are usually used to show changes over time or some other measure, where each line typically represents a higher-level entity whose behavior you are comparing.\n\nThe third method on the list is a combination of Scatter Plots and Scatter Matrices. Using scatter plots, you can view relationships and correlations between two numeric variables in your data set at a time. Scatter matrices are simply a matrix of scatter plots, so they allow you to view the relationships and correlations between all your numeric variables in a single visualization.\n\nThe fourth visualization method listed are Heatmaps, which allow you to view the concentration, magnitude, or other calculated value of entities that fall into different combinations of categories in your data. Last, but certainly not least, we have Network Visualizations, which bring graph transformations to life and let you visualize relationships between the entities in your data via a collection of visual nodes and edges.\n\nWe will cover all of these visualization methods in more depth and show examples of them in the other posts in this series.\n\nIn this post, we have developed a way of thinking about data, both in general and for our example data set, which will help us explore our data in a creative but structured way. By this point, you should have foundational knowledge about your data set, as well as some transformation and visualization methods available to you, so that you can quickly deploy them when necessary.\n\nThe goal of this post was to prepare you, the analyst or data scientist, for exploring your data. In the next post, we will prepare the data itself to be explored. We will move to the last step of the Prep Phase and come up with ways to create additional categories that will help us explore our data from various perspectives. We will do this in several ways, some of which you may have seen or used before and some of which you may not have realized were possible. So make sure to stay tuned!"
    },
    {
        "url": "https://medium.com/district-data-labs/exploring-bureau-of-labor-statistics-time-series-1917d79bb01f?source=---------8",
        "title": "Exploring Bureau of Labor Statistics Time Series \u2013 District Data Labs \u2013",
        "text": "Machine learning models benefit from an increased number of features \u2014 \u201cmore data beats better algorithms\u201d. In the financial and social domains, macroeconomic indicators are routinely added to models particularly those that contain a discrete time or date. For example, loan or credit analyses that predict the likelihood of default can benefit from unemployment indicators or a model that attempts to quantify pay gaps between genders can benefit from demographic employment statistics.\n\nThe Bureau of Labor Statistics (BLS) collects information related to the labor market, working conditions, and prices into periodic time series data. Moreover, BLS provides a public API making it very easy to ingest essential economic information into a variety of analytics. However, while they provide raw data and even a few reports that analyze employment conditions in the United States, the tables they provide are more suited towards specialists and the information can be difficult to interpret at a glance.\n\nIn this post, we will review simple data ingestion of BLS time series data, enabling routine collection of data on a periodic basis so that local models are as up to date as possible. We will then visualize the time series using pandas and matplotlib to explore the series provided with a functional methodology. At the end of this post, you will have a mechanism to fetch data from BLS and quickly view and explore data using the BLS series id key.\n\nThe BLS API currently has two versions, but it is strongly encouraged to use the V2 API, which requires registration. Once you register, you will receive an API Key that will authorize your requests, ensuring that you get access to as many data sets as possible at as high a frequency as possible.\n\nThe API is organized to return data based on the BLS series id, a string that represents the survey type and encodes which version or facet of the data is being represented. To find series ids, I recommend going to the data tools section of the BLS website and clicking on the \u201ctop picks\u201d button next to the survey you\u2019re interested in, the series id is provided after the series title. For example, the Current Population Survey (CPS), which provides employment statistics for the United States, lists their series; here are a few examples:\n\nThe series id, in this case, starts with LNS or LNU: , , , and . There are two methods to fetch data from the API. You can GET data from a single series endpoint, or you can POST a list of up to 25 ids to fetch multiple time series at a time. Generally, BLS data sets are fetched in groups, so we'll look at the multiple time series ingestion method. Using the module, we can write a function that returns a JSON data set for a list of series ids:\n\nThis script looks up your API key from the environment, a best practice for handling keys which should not be committed to GitHub or otherwise saved in a place that they can be discovered publicly. You can either change the line to hard code your API key as a string, or you can export the variable in your terminal as follows:\n\nThe function accepts a list of series ids and a set of generic keyword arguments which are stored as a dictionary in the variable. The first step of the function is to ensure that we have between 1 and 25 series passed in (otherwise an error will occur). If so, we create our request headers to pass and receive JSON data as well as construct a payload with our request parameters. The payload is constructed with the keyword arguments as well as the registration key from the environment and the list of series ids. Finally, we POST the request, check to make sure it returned successfully, and return the parsed JSON data.\n\nTo run this function for the series we listed before:\n\nYou should see something similar to the following result:\n\nFrom here it is a simple matter to operationalize the routine (monthly) ingestion of new data. One method is to store the data in a relational database like PostgreSQL or SQLite so that complex queries can be run across series. As an example of database ingestion and wrangling, see the github.com/bbengfort/jobs-report repository. This project was a web/D3 visualization of the BLS time series data, but it utilized a routine ingestion mechanism as described in the README of the ingestion module. To simplify data access, we\u2019ll use a database dump from that project in the next section, but you can also use the data downloaded as JSON from the API if you wish.\n\nFor this section, we have created a database of BLS data (using the API) that has two tables: a table that has information describing each time series and a table where each row is essentially a tuple of records. This allows us to aggregate and query the timeseries data effectively, particularly in a DataFrame. For this section we've dumped out the two tables as CSV files, which can be downloaded here: BLS time series CSV tables.\n\nThe first step is to create a data frame from the file such that we can query information about each time series without having to store or duplicate the data.\n\nWorking backward, we can create a function that accepts a BLS ID and returns the information from the info table:\n\nI utilize this function a fair amount to check if I have a time series in my dataset or to lookup seemingly related time series. In fact, we can see a pattern starting to emerge from function and the API fetch function from the last section. Our basic methodology is going to be to create functions that accept one or more BLS series ids and then perform some work on them. Unifying our function signatures in this way and working with our data on a specific key type dramatically simplifies exploratory workflows.\n\nHowever, the BLS ids themselves aren\u2019t necessarily informative, so like the previous function, we need an ability to query the data frame. Here are a few example queries:\n\nThis query returns all of the time series whose source is the Local Area Unemployment Statistics (LAUS) program, which breaks down unemployment by state. However, you\u2019ll notice from the previous section that the prefixes of the series seem to be related but not necessarily to the source. We could also query based on the prefix to find related series:\n\nCombining queries like these into a functional methodology will easily allow you to explore the 3,368 series in this dataset and more as you continue to ingest series information using the API!\n\nThe next step is to load the actual time series data into Pandas. Pandas implements two primary data structures for data analysis \u2014 the and objects. Both objects are indexed, meaning that they contain more information about the underlying data than simple one or two-dimensional arrays (which they wrap). Typically the indices are simple integers that represent the position from the beginning of the series or frame, but they can be more complex than that. For time series analysis, we can use a index, which indexes the series values by a granular interval (by month as in the BLS dataset). Alternatively, for specific events you can use the index, but periods do well for our data.\n\nTo load the data from the file, we need to construct a per time series data structure, creating a collection of them. Here's a function to go about this:\n\nIn this function we use the module, part of the Python standard library, to read and parse each line of our opened CSV file. The generates rows as dictionaries whose keys are based on the header row of the csv file. Because each record is in the format (with some extra information as well), we can the . This does require the file to be sorted by since the function simply scans ahead and collects rows into the variable until it sees a new .\n\nOnce we have our rows grouped by we can load them into memory and sort them by time period. The period value is a string in the form , which is a sortable format; however, if we create a from this string the period will have the day granularity. For each row, we create a period using the to transform the period into the month granularity. Finally, we parse our values into floating points for data analysis and construct a object with the values, the monthly period index, and assign a name to it \u2014 the string blsid, which we will continue to use to query our data.\n\nThis function uses the statement to return a generator of objects. We can collect all series into a single data frame, indexed correctly as follows:\n\nIf you\u2019re using our data, the data frame should be indexed by period, and there should be roughly 183 months (rows) in our dataset. There are also 3366 time series in the data frame represented as columns whose column id is the BLS ID. If any of the series did not have a period matched by the global period index, the function correctly fills in that value as . As you can see from a simple : the data frame contains a wide range of data, and the domain of every series can be dramatically different.\n\nNow that we\u2019ve gone through the data wrangling hoops, we can start to visualize our series using matplotlib and the plotting library that comes with Pandas. The first step is to create a function that takes a as input and uses the and data frames to create a visualization:\n\nThe first thing this function does is look up the title of the series using the series info data frame. To do this it uses the method of the data frame which will return the value of a particular column for a particular row by index. To look up the title by , we will have to query the info data frame for that row, , then fetch the index, and then we can then use that to get the 'title' column. After that we can simply plot the series, fetching it directly from the series data frame and plotting it using Pandas.\n\nWarning: Don\u2019t try , which will try to plot a line for every series (all 3366 of them); I've crashed a few notebooks that way!\n\nThis function is certainly an enhancement of the function from before, allowing us to think more completely about the domain, range, and structure of the time series data for a single . I typically use this function when adding macroeconomic features to datasets to decide if I should use a simple magnitude, or if I should use a slope or a delta, or some other representation of the data based on its shape. Even better though would be the ability to compare a few time series together:\n\nIn this function instead of providing a single , the argument is a list of strings. For each series, we plot them but add their title as a label. This allows us to create a legend with all the series names. Finally, we add a title that indicates the series for reference later. We can now start making visual series comparisons:\n\nOne thing to note is that comparing these series worked because they had the approximately the same range. However, not all series in the BLS data set are in the same range and can be orders of magnitude different. One method to combat this is to provide a argument to the function and then use a normalization method to bring the series into the range [0,1].\n\nThe addition of macroeconomic features to models can greatly expand their predictive powers, particularly when they inform the behavior of the target variable. When instances have some time element that can be mapped to a period, then the economic data collected by Census and BLS can be easily incorporated into models.\n\nThis post was designed to equip a workflow for ingesting and exploring macroeconomic data from BLS in particular. By centering our workflow on the of each time series, we were able to create functions that accepted an id or a list of ids and work meaningfully with it. This allows us to connect exploration both on the BLS data explorer, as well as in our data frames.\n\nOur exploration process was end-to-end with respect to the data science pipeline. We ingested data from the BLS API, then stored and wrangled that data into a database format. Computation on the timeseries involved the and objects, which were then aggregated into a single data frame. At all points we explored querying and limiting the data, culminating with visualizing single and multiple timeseries using matplotlib.\n\nNicole Donnelly reviewed and edited this post, Tony Ojeda helped wrangle the datasets."
    },
    {
        "url": "https://medium.com/district-data-labs/the-trends-behind-whats-trending-5f2cf0524b12?source=---------9",
        "title": "The Trends Behind What\u2019s Trending \u2013 District Data Labs \u2013",
        "text": "This article highlights one of the capstone projects from the Georgetown Data Science Certificate program, where several of the DDL faculty teach. We\u2019ve invited groups with interesting projects to share an overview of their work here on the DDL blog. We hope you find their projects interesting and are able to learn from their experiences.\n\nProducing online content that goes viral continues to be more art than science. Often, the virality of content depends heavily on cultural context, relevance to current events, and the mercurial interest of the target audience. In today\u2019s dynamic world of constantly shifting tastes and interests, reliance on the experience and intuition of the editing staff is no longer sufficient to generate high-engagement digital content.\n\nBased on this understanding of the media landscape, we used this project to ask the question: is it possible to optimize the likelihood that an article\u2019s headline is clicked on? Taking a data science approach, we used Natural Language Processing and Machine Learning methodologies to analyze a summer\u2019s worth of BuzzFeed article data. We hypothesized that with sufficient data, it would be possible to identify and evaluate the most common features of successful articles.\n\nBuzzFeed is a global network of social news and entertainment content that is currently published in 11 countries, each with its own landing page. The website is frequently accessed outside of these 11 countries, though it does not target them directly. Of the 11 countries, 5 are English speaking: Australia, Canada, India, the United Kingdom, and the United States.\n\nBuzzFeed was selected for this project because it might be considered a successful digital media company, one which has very much become an example of how to commodify the popularity of digital content. In fact, its entire business model is built on this very premise. However, despite this fact, it still finds itself prioritizing quantity over quality \u2014 acting as what might be referred to as a \u201ccontent machine gun\u201d that churns out as many articles as possible in the hopes that something will stick with the audience. Additionally, BuzzFeed\u2019s trending article API was free and open to the public and therefore allowed us to query their data on a regular basis without needing to request and frequently update developer keys or credentials.\n\nThe number of visitors to the BuzzFeed site has been rising steadily since 2013. In the first eight months of 2016, the monthly number of unique visitors has ranged from 180 million to 210 million, generating between 440 million to 500 million visits to the site. On average, the United States is responsible for approximately 50% of this traffic.\n\nThe project\u2019s approach was formed based on two central hypotheses:\n\nIn order to collect data from five different BuzzFeed API queries (we hit each English site\u2019s trending article list), we set up an Ubuntu instance through Amazon Web Services EC2 tool, which allowed us to schedule a crontab job to run our Python query scripts every hour. We then saved the API response as a JSON file in a WORM directory. Each call returned data points on the top 25 articles for each location. We then parsed the JSON files into a relational format and loaded each article as a row into a Postgres database, which allowed us to see what type of data we had very quickly.\n\nBelow is a visual glimpse of a portion of our final data set. On the left, you\u2019ll see a cluster of the data pulled from the US API. To better understand what we had, we color-coded the articles based on the country of origin and weighted the size based on its frequency (i.e. the number of occurrences) within the dataset. On the right-side is the same visualization, but for data pulled from the India API.\n\nOnce we had a sufficient amount of instances, we realized that our analysis would benefit from generating some additional data points. In particular, because BuzzFeed\u2019s methodology for determining what articles appear on their trending lists is proprietary and was not available for our consideration, we quickly determined that we needed to generate a metric to indicate what we meant when we used the term virality.\n\nUltimately, we defined our own heuristic function:\n\nWe then used this metric to generate a new category for our data under the assumption that our target was a solution to a classification problem. What we came up with was essentially a virality scale from 1\u20133, where one was the least viral and three was the most viral.\n\nApplying a Machine Learning solution to a problem that consists mostly of textual and categorical data can be tricky. We leaned heavily on the open source Scikit-Learn Python library to do the heavy lifting.\n\nFor each article, we vectorized the entire data set, linearly transforming our textual values into numerical vectors. In order to perform this vectorization, we used the CountVectorizer module available through Scikit-Learn. Another useful tool we relied on heavily was the pipeline module, which allowed us to consistently transform our data and solicit human-readable outputs, or predictions, from our models.\n\nOnce our pipeline was fine-tuned, we trained two classification models on our data for our predictors: Multinomial Naive Bayes and Logistic Regression. With our pipeline, Logistic Regression performed best with a cross-validated accuracy score of 0.864645.\n\nMost capstone projects tend to be an exercise in building a large machine and driving it simultaneously, and ours was no exception. Throughout the project, we were constantly faced with the need to ramp up knowledge in an unfamiliar area in order to make progress. In particular, we felt this acutely when attempting to apply a Machine Learning solution to a large amount of textual data, a practice that requires both a unique approach and at least basic familiarity with vectorization methodologies.\n\nAdditionally, we realized too late into the project that we had made the mistake of spreading our resources too thin by attempting to work on other problems at the same time we were working on our trending prediction model. Had the entire focus of the project been on this one issue, we may have determined a better way to address some of the setbacks we encountered.\n\nPulling data from a public API that is not well documented and includes publication-specific variables and terminology caused us to have a significant gap in our domain knowledge, despite one team member\u2019s background in web analytics.\n\nAs we made progress collecting and augmenting our dataset, we quickly realized that the textual data we collected might benefit from also collecting the contents of each article. However, because we were using BuzzFeed as our primary data source, a significant number of articles were comprised of aggregated content \u2014 that is, content pulled in from other web platforms such as Twitter, Tumblr, YouTube, etc. Moreover, the API data did not contain a direct link to the articles themselves, though this issue might have been sidestepped by mixing and mashing \u201cusername\u201d and \u201curi\u201d data points.\n\nUltimately, incorporating additional article data would have required a significant level of time and effort that was simply not available for this project. Nevertheless, a more thorough approach might have found a way to web-scrape the content of the articles for which we were pulling API data.\n\nPractically speaking, a project like this might allow news media organizations to evaluate the potential popularity of a given article based on its textual inputs. This would not act as a stand in for editorial judgment, but rather a tool to supplement and refine the process as it currently stands.\n\nRather than churning out excessive digital content and hoping that something will stick, further progress in this area would allow editors and content authors to see a macro-level view of what content is successful for each region and refine their efforts based on this perspective. These efforts could be improved even further if one were to train the models categorically to generate predictions that are contingent on the audience that each category serves."
    },
    {
        "url": "https://medium.com/district-data-labs/python-exception-handling-basics-b29435a8354a",
        "title": "Python Exception Handling Basics \u2013 District Data Labs \u2013",
        "text": "Exceptions are a crucial part of higher level languages, and although exceptions might be frustrating when they occur, they are your friend. The alternative to an exception is a panic \u2014 an error in execution that at best simply makes the program die and at worst can cause a blue screen of death. Exceptions, on the other hand, are tools of communication; they allow the program to tell you what, why, and how something went wrong and then they gracefully terminate your program without destruction. Learning to understand exceptions and throw some of your own is a crucial next step in programming mastery, particularly in Python.\n\nIn this short post, we will demonstrate the basics of exception handling in the Python programming language. We will look at how to read exception traceback reports and how to diagnose exceptions. Finally, we\u2019ll look at the use of context management to handle standard exception cases. Hopefully, this post will not only make you feel comfortable handling exceptions but also raising them yourself. Eventually, you\u2019ll be creating your own exception hierarchies in your code and throwing them at will!\n\nExceptions are a tool that programmers use to describe errors or faults that are fatal to the program; e.g. the program cannot or should not continue when an exception occurs. Exceptions can occur due to programming errors, user errors, or unexpected conditions like no internet access. Exceptions themselves are objects that contain information about what went wrong. Exceptions are usually defined by their - which describes broadly the class of exception that occurred, and by a - a string that says specifically what happened. Here are a few common exception types:\n\nExceptions are defined in a class hierarchy \u2014 e.g. every exception is an object whose class defines its type. The base class is the object. All objects are initialized with a message - a string that describes exactly what went wrong. Constructed objects can then be \"raised\" or \"thrown\" with the keyword:\n\nThe reason the keyword is is because Python program execution creates what's called a stack as functions call other functions, which call other functions, etc. When a function (at the bottom of the stack) raises an Exception, it is propagated up through the call stack so that every function gets a chance to handle the exception (more on that later). If the exception reaches the top of the stack, then the program terminates and a traceback is printed to the console. The traceback is meant to help developers identify what went wrong in their code.\n\nLet\u2019s take a look at a simple example that creates an artificial call stack:\n\nThe above example represents a fairly complex piece of code that has lots of functions that call lots of other functions starting from the function. The primary question for exception handling is how do we know where our code went wrong? The answer is reported in the traceback which delineates exactly the functions through which the exception was raised. Let's trigger the exception and the traceback:\n\nTo read the traceback, start at the very bottom. As you can see it indicates the type of the exception, followed by a colon, and then the message that was passed to the exception constructor. This information is often enough to figure out what is going wrong. However, if we\u2019re unsure where the problem occurred, we can step back through the traceback in a bottom to top fashion.\n\nIn the example above, we can see that the exception was raised in the function, which was called by at line 46, which was called by at line 30, which was called by at line 14.\n\nThe first part of the traceback indicates the exact line of code and file where the exception was raised, as well as the name of the function in which it was raised. If you called this indicates that is the function where the problem occurred. If you wrote this function, perhaps that is the place to change your code to handle the exception.\n\nHowever, many times you\u2019re using third party libraries or Python standard library modules, meaning the location of the exception raised is not helpful, since you can\u2019t change that code. Therefore, you will continue up the call stack until you discover a file/function in the code you wrote. This will provide the surrounding context for why the error was raised, and you can use or even just statements to debug the variables around that line of code. Alternatively, you can handle the exception, which we'll discuss shortly.\n\nIf a programming error caused the exception, the developer can simply change the code to make it correct. However, if the exception was created by bad user input, or by a bad environmental condition (e.g. the wireless is down), then you don\u2019t want to crash the program. Instead, you want to provide feedback and allow the user to fix the problem or try again. Therefore, in your code, you can catch exceptions at the place they occur using the following syntax:\n\nWhat we\u2019re basically saying is to run the code in the first block - hopefully, it works. If it raises an save that exception in a variable called (the syntax) then we will deal with that exception in the block. Then run the code in the block even if an exception occurs. By specifying exactly the type of exception we want to catch ( in this case), we will not catch all exceptions, only those that are of the type specified, including subclasses. If we want to catch all exceptions, you can use one of the following syntaxes:\n\nHowever, it is best practice to capture only the type of exception you expect to happen, because you could accidentally create the situation where you\u2019re capturing fatal errors but not handling them appropriately. If you want to catch multiple types of exceptions, you can pass them as a tuple:\n\nOr you can pass multiple except blocks for different handling:\n\nIf an exception is raised that is not an instance or a subclass of ValueError or TypeError then it continues to be propagated up the call stack. You can also catch custom exceptions that you define yourself. Here is a complete example of exception handling with real code that randomly generates exceptions:\n\nThis code snippet demonstrates a couple of things. First, you can define your own program-specific exceptions by defining a class that extends . We have done so and created our own exception class. Next, we have a function that raises a with some likelihood which is an argument to the function. Then we have our exception handling block that calls the function and handles it.\n\nTry adapting the code snippet to see how exception handling changes in the following cases:\n\nMake sure you run the code multiple times since the error does occur randomly!\n\nIn Python, exception handling is the standard way of dealing with runtime errors. You may wonder why you must use a block to handle exceptions. Couldn't you simply do a check that the exception won't occur before it does? For example, consider the following code:\n\nThis code checks if a key exists in the dictionary before using it, then uses an else block to handle the \u201cexception.\u201d This is an alternative to the following code:\n\nBoth blocks of code are valid. In fact, they have names:\n\nFor a variety of reasons, the second example (EAFP) is more pythonic \u2014 that is the preferred Python Syntax, commonly accepted by Python developers. For more on this, please see Alex Martelli\u2019s excellent PyCon 2016 talk, Exception and error handling in Python 2 and Python 3.\n\nPython also provides a syntax for embedding common blocks in an easy to read format called context management. To motivate the example, consider the following code snippet:\n\nThis is a very common piece of code that opens a file and reads data from it. If the file doesn\u2019t exist, we simply alert the user that the required file is missing. No matter what, the file is closed. This is critical because if the file is not closed properly, it can be corrupted or not available to other parts of the program. Data loss is not acceptable, so we need to ensure that no matter what the file is closed when we\u2019re done with it. So we can do the following:\n\nThe syntax implements context management. On , a function called the function is called to do some work on behalf of the user (in this case open a file), and the return of that function is saved in the variable. When this block is complete, the finally is called by implementing an function. (Note that the part is not implemented in this particular code). In this way, we can ensure that the for opening and reading files is correctly implemented.\n\nWriting your own context managers is possible, but beyond the scope of this post (though I may write something on it shortly). Suffice it to say, you should always use the syntax for opening files!\n\nThis has been a whirlwind tour of exception handling basics in Python. Hopefully, we covered enough to make exceptions feel less terrifying. Remember, exceptions are your friend: they are tools of communication that protect you and your users from critical damage. By understanding how to read the syntax of exceptions we hope that they start to feel more friendly and less frustrating, they are a map to guide you to solutions, not impediments to success!\n\nThis is only the beginning of exception management though \u2014 handling exceptions that are thrown by the Python interpreter and maybe throwing a few of your own. Exceptions are such a critical tool in programming that most large programs have their own exception hierarchies and throw and handle their own exceptions. If you use larger libraries like Scikit-Learn or Numpy, you\u2019ve probably already noticed this. I hope you\u2019ll soon agree that exceptions are critical to ensuring that things go smoothly in programs written in Python!\n\nThank you so much to Nicole Donnelly for reviewing and editing this post!"
    },
    {
        "url": "https://medium.com/district-data-labs/getting-started-in-open-source-ac2f302ae596",
        "title": "Getting Started in Open Source \u2013 District Data Labs \u2013",
        "text": "The phrase \u201copen source\u201d evokes an egalitarian, welcoming niche where programmers can work together towards a common purpose \u2014 creating software to be freely available to the public in a community that sees contribution as its own reward. But for data scientists who are just entering into the open source milieu, it can sometimes feel like an intimidating place. Even experienced, established open source developers like Jon Schlinkert have found the community to be less than welcoming at times. If the author of more than a thousand projects, someone whose scripts are downloaded millions of times every month, has to remind himself to stay positive, you might question whether the open source community is really the developer Shangri-la it would appear to be!\n\nAnd yet, open source development does have a lot going for it:\n\nSo why start a blog post for open source noobs with a quotation from an expert like Jon, especially one that paints such a dreary picture? It\u2019s because I want to show that the bar for contributing is\u2026 pretty low.\n\nAsk yourself these questions: Do you like programming? Enjoy collaborating? Like learning? Appreciate feedback? Do you want to help make a great open source project even better? If your answer is \u2018yes\u2019 to one or more of these, you\u2019re probably a good fit for open source. Not a professional programmer? Just getting started with a new programming language? Don\u2019t know everything yet? Trust me, you\u2019re in good company.\n\nBecoming a contributor to an open source project is a great way to support your own learning, to get more deeply involved in the community, and to share your own unique thoughts and ideas with the world. In this post, we\u2019ll provide a walkthrough for data scientists who are interested in getting started in open source \u2014 including everything from version control basics to advanced GitHub etiquette.\n\nBy our very nature, as data scientists we are already significantly involved in the open source community, even if we don\u2019t realize it. Nearly every single one of the tools we use \u2014 Linux, Git, Python, R, Julia, Java, D3, Hadoop, Spark, PostgreSQL, MongoDB \u2014 is open source. We are used to going to StackOverflow and StackExchange to find answers to our programming questions, grabbing code snippets from blog posts, and leveraging useful packages from places like CRAN and the Python Package Index (PyPI). But our Google foo/copy-and-paste/ approach is heavily dependent on the contributions and thought leadership of others, and at some point, you are likely to encounter at least one of the three following scenarios:\n\nThese scenarios should be signals to you to get engaged in contributing to open source. In the first case, you could just submit a bug report, or you could go a step further and actually fix the bug. In the second case, you\u2019ve identified a new feature, which you could suggest to the maintainer of the code, or implement yourself and give back to the code base. In the third case, you\u2019re very close to having developed a standalone library that you could refine, package, and then share with the world. This post will be primarily concerned with contributing under one of the first two scenarios.\n\nIn order to become an open source contributor, you have to buy into version control. Version control is a way of managing files as they evolve, and offers a methodology for recording changes to a file or a whole project over time so that historic versions can later be retrieved. Version control is important to developers of all languages, and is especially important to open source development.\n\nOne of the most popular tools for version control is Git, which is itself an open source project. Git was developed by the Linux community and first released in 2005. Git is a distributed version control system that\u2019s fast and simple and supports non-linear development of small and large projects. If you don\u2019t already have Git, download and install the latest version here. Then configure Git with your name and email by typing the following into your terminal:\n\nNext, take a crash course on Git (try this one or this one or just read this) and figure out the main Git commands. Make sure you know how to use these ones:\n\nGithub is a popular web-based Git repository hosting service that offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own cool features like metadata & reporting tools. If you haven\u2019t already, create a Github account \u2014 in addition to being useful for open source development, your Github page is your programming portfolio, the place you can point potential teammates and employers to show them what you\u2019ve worked on and how you approach problem-solving with code.\n\nAn additional tool I like to use is Waffle.io. Waffle is a lightweight project management tool for GitHub repositories. For open source developers who use Github, it\u2019s particularly convenient because Waffle\u2019s source of record is GitHub. GitHub issues and pull requests transform into cards on a board (and likewise, adding a card to the waffle creates a new issue in Github), making it easier to plan, organize, and track work across one or many repositories. Create your own Waffle.io account to facilitate collaborative coding and good project management practices.\n\nOnce you\u2019ve identified a project that you want to contribute to, and you\u2019ve learned the Git/Github basics, the next step is to find the project README. Usually written in a simple text or markdown file, a README is the author\u2019s way of communicating to users what the package is, how to get and install it, what it looks like, and how to use it.\n\nIn the Art of README, Stephen Whitmore writes, \u201cYour documentation is complete when someone can use your module without ever having to look at its code.\u201d As the package\u2019s public-facing documentation (as opposed to documentation that appears in comments and docstrings inside the source code, for instance) the README is a critical first point of entry for anyone interested in contributing to a project.\n\nThis documentation lays out what the module is supposed to do at a high level: how the installation should work, how the package is meant to be used, and importantly, how the application programming interface (API) is meant to work. This is particularly important for those who want to modify or contribute to the code, since whatever changes you make should be compatible with the API.\n\nNow that you have a sense of what you\u2019d like to contribute to the project, you should read the existing issues. Most software projects have a bug or proposed feature tracker of some kind, and in GitHub it\u2019s called issues. Every Github repository has its own section for issues. Issues can be categorized using labels and tags to denote their type (task, enhancement, question, bug), their degree of importance, the level expertise required to address them, the project milestone they\u2019re associated with, etc. For any public repo, the issues are also public, and if you are thinking of reporting a bug or suggesting a new feature, you should first read through the existing issues to make sure that the bug you\u2019ve identified, or the feature you\u2019d like to suggest, has not already be submitted and added to the backlog.\n\nYou can also go look at the project\u2019s Waffle to help you visualize the core developers\u2019 current workflow, backlog, and priorities.\n\nGenerally maintainers of popular open source projects provide some guidelines to other about how to contribute. At District Data Labs, we encourage contributions in the following ways:\n\nThe labels in the Github issues for our projects are defined in the blog post: How we use labels on GitHub Issues at Mediocre Laboratories.\n\nWhen you want to create your own copy of a repository on GitHub, you fork it. You can find the fork button in the top-right corner of the page. Your forked version of the project is yours; its a place where you can freely experiment, modify the code, add and change things as you like without changing anything about the original repository. In open source, forking is the typical method for contributing to an open source project.\n\nNow that you have identified a novel feature or a suggestion for a bug fix, you can fork the repo, add the new feature or fix the bug, commit the changes and push them back to your forked version of the repo, and then submit a pull request to the maintainers of the project. They\u2019ll receive a notification, and can review your pull request and either merge it into the source code or provide you with feedback to help guide you toward contribution. Keep in mind that your pull request might not get pulled in immediately for a number of reasons \u2014 everything from a potential complication with your proposed fix/addition that will require the creation of a new test to the maintainers having a busy work week. Patience is appreciated, but for the most part, maintainers will likely be pretty stoked that you like their code enough to contribute, and they\u2019ll be inclined to coach you through the process to get a successful contribution merged into master.\n\nSteps 1\u20136 will get you operational as a open source contributor to an existing project. However, if you\u2019re at the stage of converting one of your own custom projects into an open source package, there are a couple additional things to consider, two of which are Git branching workflows and semantic versioning.\n\nMost version control tools have a way of supporting project branching. Branching means diverging from the main line of development in order to make changes, experiment, and explore without breaking the main line. Fortunately, Git makes branching very easy and lightweight by facilitating smooth switching back and forth between branches. Git encourages a workflow that branches and merges often, even multiple times in a day.\n\nA typical production/release/development cycle is described in A Successful Git Branching Model. A typical workflow is as follows:\n\nSelect a card from the dev board \u2014 preferably one that is \u201cready\u201d then move it to \u201cin-progress.\u201d\n\nCreate a branch off of develop called \u201cfeature-[feature name]\u201d, work and commit into that branch.\n\nOnce you are done working (and everything is tested) merge your feature into develop.\n\nRepeat. Releases will be routinely pushed into master via release branches, then deployed to the server.\n\nSemantic versioning is a way of encoding information about the stage and changes to code over time in the version numbers. Semantic versioning is very important to the open source community because it provides another tool for developers and users to communicate about things like package dependencies, changes in the application programming interface, and backwards compatibility.\n\nUnder semantic versioning, package version numbers are assigned and incremented as follows: Once a public API is declared (either within the documentation, the code, or both), changes to the API are denoted with specific increments to the version number. As explained in Semantic Versioning 2.0.0: \u201cConsider a version format of X.Y.Z (Major.Minor.Patch). Bug fixes not affecting the API increment the patch version, backwards compatible API additions/changes increment the minor version, and backwards incompatible API changes increment the major version.\u201d\n\nIn addition to semantic versioning, calendar versioning is a recommended convention that dictates a project\u2019s release calendar using semantic rules rather than arbitrary numbers.\n\nIf you\u2019re interested in getting into open source, you can get started by checking out the District Data Labs organization on GitHub, and looking through the projects we have in active development. I hope you enjoyed this post and that you will consider becoming a contributor!"
    },
    {
        "url": "https://medium.com/district-data-labs/principal-component-analysis-with-python-4962cd026465",
        "title": "Principal Component Analysis with Python \u2013 District Data Labs \u2013",
        "text": "The amount of data generated each day from sources such as scientific experiments, cell phones, and smartwatches has been growing exponentially over the last several years. Not only are the number data sources increasing, but the data itself is also growing richer as the number of features in the data increases. Datasets with a large number of features are called high-dimensional datasets.\n\nOne example of high-dimensional data is high-resolution image data, where the features are pixels, and which increase in dimensionality as sensor technology improves. Another example is user movie ratings, where the features are movies rated, and where the number of dimensions increases as the user rates more of them.\n\nDatasets that have a large number features pose a unique challenge for machine learning analysis. We know that machine learning models can be used to classify or cluster data in order to predict future events. However, high-dimensional datasets add complexity to certain machine learning models (i.e. linear models) and, as a result, models that train on datasets with a large number features are more prone to producing error due to bias.\n\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional datasets into a dataset with fewer variables, where the set of resulting variables explains the maximum variance within the dataset. PCA is used prior to unsupervised and supervised machine learning steps to reduce the number of features used in the analysis, thereby reducing the likelihood of error.\n\nConsider data from a movie rating system where the movie ratings from different users are instances, and the various movies are features.\n\nIn this dataset, you might observe that users who rank Star Wars Episode IV highly might also rank Rogue One: A Star Wars Story highly. In other words, the ratings for Star Wars Episode IV are positively correlated with the ones for Rogue One: A Star Wars Story. One could image that all movies ranked by certain users in a similar way might all share similar attributes (ex. all movies with these ratings are classic sci-fi movies), and could ultimately be grouped together to form a new feature.\n\nThis is an intuitive way of grouping data, but it would take quite some time to read through all of the data and group it according to similar attributes. Fortunately, there are algorithms that can automatically group features that vary in a similar way within high-dimensional datasets, such as Principal Component Analysis.\n\nThe overall goal of PCA is to reduce the number of d dimensions (features) in a dataset by projecting it onto a k dimensional subspace where k < d. The approach used to complete PCA can be summarized as follows:\n\nIn this post, we\u2019ll use a high-dimensional movie rating dataset to illustrate how to apply Principal Component Analysis (PCA) to compress the data. This tutorial picks up after having created csv files from the data. If you want to pick up where this tutorial starts, you can find the pre-made csv files here.\n\nFirst we\u2019ll load the data and store it in a pandas dataframe. The data set contains ratings from 718 users (instances) for 8,913 movies (features). Even though all of the features in the dataset are measured on the same scale (a 0 through 5 rating), we must make sure that we standardize the data by transforming it onto a unit scale (mean=0 and variance=1). Also, all null (NaN) values were converted to 0. It is necessary to transform data because PCA can only be applied on numerical data.\n\nNext, a covariance matrix is created based on the standardized data. The covariance matrix is a representation of the covariance between each feature in the original dataset.\n\nThe covariance matrix can be found as follows:\n\nAlternatively, you can also create the same covariance matrix with one line of code.\n\nAfter the covariance matrix is generated, eigendecomposition is performed on the covariance matrix. Eigenvectors and eigenvalues are found as a result of the eigendceomposition. Each eigenvector has a corresponding eigenvalue, and the sum of the eigenvalues represents all of the variance within the entire dataset.\n\nThe eigendecomposition can be performed as follows:\n\nEigenvectors, or principal components, are a normalized linear combination of the features in the original dataset. The first principal component captures the most variance in the original variables, and the second component is a representation of the second highest variance within the dataset.\n\nFor example, if you were to plot data from a dataset that contains two features, the following illustrates that principal component 1 (PC1) represents the direction of the most variation between the two features and principal component 2 (PC2) represents the second most variation between the two plotted features. Our movies dataset contains over 8,000 features and would be difficult to visualize which is why we used eigendecomposition to generate the eigenvectors.\n\nThe eigenvectors with the lowest eigenvalues describe the least amount of variation within the dataset. Therefore, these values can be dropped. First, lets order the eigenvalues in descending order:\n\nTo get a better idea of how principal components describe the variance in the data, we will look at the explained variance ratio of the first two principal components.\n\nThe first two principal components describe approximately 14% of the variance in the data. In order gain a more comprehensive view of how each principal component explains the variance within the data, we will construct a scree plot. A scree plot displays the variance explained by each principal component within the analysis.\n\nOur scree plot shows that the first 480 principal components describe most of the variation (information) within the data. This is a major reduction from the initial 8,913 features. Therefore, the first 480 eigenvectors should be used to construct the dimensions for the new feature space.\n\nTry it on your own with a larger data set. What impact do you think a data set with more instances, but similar number of features would have on the resulting number of principal components (our kvalue)? What impact would a dataset with a larger number of features have on the resulting number of principal components?"
    },
    {
        "url": "https://medium.com/district-data-labs/nlp-research-lab-part-2-skip-gram-architecture-overview-1c48491144f7",
        "title": "NLP Research Lab Part 2: Skip-Gram Architecture Overview",
        "text": "This post is part of a series based on the research conducted in District Data Labs\u2019 NLP Research Lab.\n\nChances are, if you\u2019ve been working in Natural Language Processing (NLP) or machine learning, you\u2019ve heard of the class of approaches called Word2Vec. Word2Vec is an implementation of the Skip-Gram and Continuous Bag of Words (CBOW) neural network architectures. At its core, the skip-gram approach is an attempt to characterize a word, phrase, or sentence based on what other words, phrases, or sentences appear around it. In this post, I will provide a conceptual understanding of the inputs and outputs of the skip-gram architecture.\n\nThe purpose of the Skip-Gram Architecture is to train a system to represent all the words in a corpus as vectors. Given a word, it aims to find the probability that the word will show up near another word. From this kind of representation, we can calculate similarities between words or even the correct response to an analogy test.\n\nFor example, a typical analogy test might consist of the following:\n\nIn this case, an appropriate response for the value of X might be \u201cSlippery.\u201d The output for this model, which is described in detail below, results in a vector of the length of the vocabulary for each word. A practitioner should be able to calculate the cosine distance between two word vector representations to determine similarity.\n\nHere is a simple Python example, where we assume the vocabulary size is 6 and are trying to compare the similarity between two words:\n\nIn this case, the cosine distance ends up being 0.1105008200066786.\n\nGuessing the result of an analogy simply uses vector addition and subtraction and then determines the closest word to the resulting vector. For example, to calculate the vector in order to guess the result of an analogy, we might do the following:\n\nThen you can simply find the closest word (via cosine distance or other) to the , and that would be your prediction. Both of these examples should give you a good intuition for why skip-gram is incredibly useful. So let's dig into some of the details.\n\nTo make things easy to understand, we are going to take a look at another example.\n\nIn the real world, a corpus that you want to train will be large; at least tens of thousands of words if not larger. If we trained the example above in the real world, it wouldn\u2019t work because it isn\u2019t large enough, but for the purposes of this post, it will do.\n\nBefore you get to the meat of the algorithm, you should be doing some preparatory work with the content, just as you would do for most other NLP-oriented tasks. One might think immediately that they should remove stopwords, or words that are common and have little subject oriented meaning (ex. the, in, I). This is not the case in skip-gram, as the algorithm relies on understanding word distance in a paragraph to generate the right vectors. Imagine if we removed stop words from the sentence \u201cI am the king of the world.\u201d The original distance between king and world is 3, but by removing stopwords, the distance between those two words changes to 1. We\u2019ve fundamentally changed the shape of the sentence.\n\nHowever, we do probably want to conduct stemming in order to get words down to their core root (stem). This is very helpful in ensuring that two words that have the same stem (ex. \u2018run\u2019 and \u2018running\u2019) end up being seen as the same word (\u2018run\u2019) by the computer.\n\nA simple example using NLTK in Python is provided below.\n\nThe result is as follows:\n\nTo build the final vocabulary that will be used for training, we generate a list of all the distinct words in the text after we have stemmed appropriately. To make this example easier to follow, we will sort our vocabulary alphabetically. Sorting the vocabulary in real life provides no benefit and, in fact, can just be a waste of time. In a scenario where we have a 1 billion word vocabulary, we can imagine the sorting taking a long time.\n\nSo without any further delay, our vocabulary ends up becoming the following:\n\nThe following stopwords would also be included: [is, and, should, be]. I\u2019m leaving these out to keep this example simple and small, but in reality, those would be in there as well.\n\nJust like with other statistical learning approaches, you\u2019ll need to develop some methodology for splitting your data into training, validation, and testing sets. In our specific example, we\u2019ll make 2/3 of the total vocabulary our training set through a random selection. So lets suppose our training set ends up being the following after a random selection:\n\nWhich means we have 4 training samples t1 through t4 (T={t1,t2,t3,t4}). The vectors used to feed the input layer of the network are as follows:\n\nSuppose your only goal was to find the probability that \u201cwork\u201d shows up near \u201ctape.\u201d You can\u2019t just throw one example at the Neural Network (NN) and expect to get a result that is meaningful. When these systems (NN) are trained, you will eventually be pushing the bulk of the vocabulary (your training set) into the input layer and training the system regardless of the specific question you may be asking.\n\nOur input layer is a vector that is the length of the vocabulary (V) and we have four training samples, one for each word. So the total set of data pushed through the NN during training-time is of size VxT (6x4). During training time, one of the samples in T is input into the system at a time. It is then up to the practitioner to decide if they want to use online training or batch inputs before back-propagating. Back-propagation is discussed in our back-propagation blog post, which will be published soon. For now, don\u2019t worry about those details. The point here is to conceptually grasp the approach.\n\nThe insertion into the input layer looks something like the following diagram:\n\nEach sample of array length V (6) represents a single word in the vocabulary and its index location in the unique word vocabulary.\n\nSo let\u2019s review our objective here. The objective of the Skip-gram model, in the aggregate, is to develop an output array that describes the probability a word in the vocabulary will end up \u201cnear\u201d the target word. \u201cNear\u201d defined by many practitioners in this approach is a zone of c words before and after the target word. This is referred to as the context area or context zone. So in the example below, if the context size is 2 (c=2) and our target word is \u201cmagic,\u201d the words in the context area are C shown below.\n\nTo get to the point where we get a single array that represents the probability of finding any other word in the vocabulary in the context area of the target word, we need to understand exactly what the NN output looks like. To illustrate, I\u2019ve provided the diagram below.\n\nIn this diagram, if we choose a context size of one, it means we care about words that appear only directly before and directly after the target word.The output layer includes two distinct sets of output values. If our context size was 5, we\u2019d end up with 60 output values.\n\nEach word in the vocabulary receives two values for our context size of one. To get the score for an individual word in the context area, we can simply sum up the values. So for example, the score for \u201cduct\u201d (v2) showing up within the context area is 0.22 (0.2 + 0.02).\n\nYou may have noticed we are calling the results scores instead of probabilities. That is because the raw output of the skip-gram architecture does not produce probabilities that add up to 1. To convert the scores to probabilities, you must conduct a softmax calculation to scale them. The purpose of this post isn\u2019t to describe softmax, so we are just going to pretend the values in the diagram are probabilities.\n\nAt the end of forward propagation (stay tuned for the forward-propagation blog we have coming up next), you need to calculate an error in order to backpropagate. So how do you do that? It\u2019s actually pretty easy. We already know what the actual probabilities are of finding a word in the context area of a target word based on our corpus. So for example, if we wanted to know the error in the probability of finding \u201cduct\u201d given \u201cmagic\u201d as the target word, we would do the following.\n\nIn our corpus, the actual probability of finding \u201cduct\u201d in the context area around \u201cmagic\u201d is 100% because \u201cmagic\u201d is only used once and \u201cduct\u201d is within the context zone. So the absolute error in probability is 1\u20130.22 = 0.78, and the mean squared error (MSE) is 0.61. This error is used in backpropagation which re-calculates the input and output weight matrices.\n\nWhat I have given you is a conceptual understanding of what the input vectors look like and what the output vectors look like, but there are other components of the algorithm that will be explained in upcoming blog posts.\n\nThe input weight matrix (1) is the matrix that becomes the vectors for each word where each row is a word and the vector is of length H (H is the number of nodes in the hidden layer). The output vectors simply give the score for a word being in the context zone and are really not used for anything other than training and error calculation.\n\nIt is important to understand that a practitioner can choose any number of H nodes for the hidden layer; it is a hyperparameter in training. Generally, the more hidden layer nodes you have, the more expressive (but also the more computationally expensive) a vector is. The output weight matrices are not used outside of the context of training. To learn more about these details and what the process of forward propagation is, please check out our forward propagation blog post, which is coming up next."
    },
    {
        "url": "https://medium.com/district-data-labs/nlp-research-lab-part-1-distributed-representations-b7296b522d38",
        "title": "NLP Research Lab Part 1: Distributed Representations",
        "text": "This post is part of a series based on the research conducted in District Data Labs\u2019 NLP Research Lab.\n\nThis post is about Distributed Representations, a concept that is foundational not only to the understanding of data processing in machine learning, but also to the understanding of information processing and storage in the brain. Distributed representations of data are the de-facto approach for many state-of-the-art deep learning techniques, notably in the area of Natural Language Processing, which will be the focus of this blog post.\n\nIf you\u2019re anything like me, you probably feel somewhat overwhelmed by the technical jargon and content of machine learning publications; every paper or article you read seems to require an understanding of many underlying concepts (\u201cprerequisites\u201d). Distributed representations is one of those concepts, and the modest aim of this blog post is to hopefully check this prerequisite off your list and move one step closing to a stronger command of machine learning.\n\nWe\u2019re going to start with a thought experiment: let\u2019s imagine that my brain has 3 neurons, or 3 information processing units. My wife tells me that sometimes when she\u2019s talking to me, it feels to her like my brain is mostly turned off, so perhaps this image is more apt than we think. Let\u2019s also imagine that my brain is a complete blank slate; in other words, I have no knowledge learned/stored in my brain yet. My wife also believes this about my brain, but we digress. Now let\u2019s say that I\u2019m staring out of my window and I see a small red car driving by. The concept \u201csmall red car\u201d is completely new to me, and so my brain decides to assign one of its units to the representation of this concept. This means that, from now on, I can recall the concept of \u201csmall red car\u201d by retrieving it from this one neuron. Let\u2019s write this in somewhat \u201cmathy\u201d terms, shall we:\n\nIn this example, 1 designates the neuron \u201cfiring\u201d and 0 represents the neuron not \u201cfiring.\u201d As a side corollary, we can imagine a blank [ ] vector representing my brain knowing absolutely nothing.\n\nNow, I\u2019m still staring out of my window, and I see a second car pass by, this time it\u2019s a Large Blue SUV. In similar fashion, my brain assigns this new concept to a second processing unit, and we end up with the following updated knowledge representations:\n\nWe now have two processing units (neurons), each either on or off (firing or not firing). Note that it\u2019s theoretically possible to have [ 0 0 ], representing the notion \u201cNOT a small red car AND NOT a large blue SUV,\u201d but it is not possible to have [ 1 1 ] because then a small red red would also be a large blue SUV, and car commercials would suddenly become very confusing.\n\nContinuing on with our window staring (it\u2019s a slow day), one more vehicle passes by and it\u2019s a Large Red SUV. Three concepts in total, 3 available processing units: perfect. We end up with the following knowledge representations:\n\nWith this type of representation, one thing immediately become apparent: 3 processing units only allow me to store 3 pieces of information. If I were to keep staring out my window, and see a large red car pass by, I would have no room for that new concept. I would either have to discard it, or replace an existing one.\n\nA second, less dramatic result of this representation is the fact that each processing unit can contribute to only one concept. In other words, unit 1 only \u201cfires\u201d for a small red car, and doesn\u2019t involve itself in representing anything else. This way of representing information is called a localist representation. It is a simple and accessible way of representing data, in the sense that you could point at some location in my brain and say \u201cthat neuron is where this concept is stored.\u201d Similarly, if a localist approach is used to represent data or state in a machine, it would offer some advantages in terms of identifying data (just find the one hardware unit that is on), and changing data (turn one unit off, turn one unit on).\n\nIf you\u2019ve worked with machine learning within the context of NLP, there\u2019s a chance you\u2019ve come across a localist representation called One-Hot Encoding. For example: you have a vocabulary of n words and you represent each word using a vector that is n bits long, in which all bits are zero except for one bit that is set to 1. Different words get a different bit \u201cassigned.\u201d For example, let\u2019s consider this quote from Andy Weir\u2019s \u201cThe Martian\u201d:\n\nThe above sentence is made up of 12 words, of which 10 are unique (\u201cduct\u201d and \u201ctape\u201d are repeated). Note that for this tutorial\u2019s purposes, we are currently ignoring letter case. Our vocabulary is therefore made up of 10 words: \u201cduct\u201d, \u201ctape\u201d, \u201cworks\u201d, \u201canywhere\u201d, \u201cis\u201d, \u201cmagic\u201d, \u201cand\u201d, \u201cshould\u201d, \u201cbe\u201d, \u201cworshiped.\u201d We can use a vector of 10 bits to represent any of these words, by simply making the 1st bit represent the word \u201cduct\u201d, the second bit the word \u201ctape\u201d, etc. As follows:\n\nOne-Hot Encoding is very common in natural language processing, because it involves a simple localist representation with all the advantages that entails. However, OHE has its limitations:\n\nBelow is an example function in Python that performs One-Hot Encoding. The input to this function is the vocabulary and the word we want to One-Hot Encode. The function returns an OHE vector.\n\nLet\u2019s go back to our original thought experiment with small red cars and large blue SUVs. Let\u2019s also add a few more concepts into the mix \u2014 the assumption here is that I went to sleep and my brain grew a few more neurons, then I woke up and watched some YouTube videos, so my knowledge map now looks as follows:\n\nHere we face the same problems we mentioned in our One-Hot Encoding discussion: high computational cost, high dimensionality, and the concepts are equally similar. For example, to go from any concept to another, you go the same distance: -1 in one dimension, +1 in a second dimension. As far as this representation goes, a large blue SUV is as similar to a large red SUV as it is to a banana; not exactly nuanced. But we can do better.\n\nEver since the mid 1980\u2019s (see Rumelhart, McClelland and Hinton \u2014 1986), the connectionist (read \u201canti-localist\u201d) crowd has advocated something called Distributed Representations, which offer some advantages over the localist approaches and help us navigate around their limitations. The name \u201cdistributed representation\u201d is mainly driven by the fact that the representation of any single concept is distributed over many, if not all, processing units. In many cases, the unit values in the vectors are continuous values, instead of just 1\u2019s and 0\u2019s.\n\nLet\u2019s apply this type of representation on my brain knowledge and see if we can make me smarter (my wife has already despaired, so it\u2019s all on you now, no pressure):\n\nWhat do we have here?\n\nThe last point above is crucial and has a wonderful implication when it comes to learning new concepts we haven\u2019t seen before. If I see some new object that I don\u2019t recognize, let\u2019s say a river trout, and it has the following representation [ 0.144 0.187 0.439 0.606 ], I can quickly find which existing concept\u2019s vector it is most similar to (the small fish vector) and I can guess that it must be a fish-like object, without any additional information about it!\n\nFurthermore \u2014 and here\u2019s why this type of representation is so powerful \u2014 we are able to generalize. I don\u2019t need all the units in the vector to have values, I can \u201cguess\u201d what concept category they might fall into with only a partial representation. For example, [ 0.158 0.030 \u2014 \u2014 ] is of similar characteristics to a Green Apple. It could be a Green Pear, or a Yellow Lemon, but it\u2019s very unlikely to be a Tall Building.\n\nWe looked at One-Hot Encoding as a localist type of representation, now let\u2019s look at Word Embeddings, an example of distributed representation used in Deep Learning for NLP.\n\nThe idea of Word Embeddings is to take a corpus of text, and figure out distributed vector representations of words that retain some level of semantic similarity between them. When that happens, the words \u201cduct\u201d and \u201ctape\u201d are \u201ccloser\u201d to each other than they are to \u201cmagic\u201d (which we couldn\u2019t do with One-Hot Encoding). This can be applied to any corpus, whether it\u2019s a collection of novels, or documents within a certain scientific discipline, or many other applications. From here, these word embeddings can be used as inputs to additional models such as an SVM or recurrent neural network. This input is usually in the form of a matrix of vectors, just like we saw in the previous section.\n\nIn the following posts in this series, we will be exploring one popular implementation of a word embedding model: word2vec. Based on a recent paper (See Mikolov \u201813), word2vec turns one-hot-encoded vectors into embedded vectors, and achieves outstanding results along the way. It manages to produce vectors that allow us to do things like W(\u201cKing\u201d) - W(\u201cMan\u201d) + W(\u201cWoman\u201d) = W(\u201cQueen\u201d)! How does it do that? We\u2019ll find out in the next post!"
    },
    {
        "url": "https://medium.com/district-data-labs/beyond-the-word-cloud-428e3c25b59c",
        "title": "Beyond the Word Cloud \u2013 District Data Labs \u2013",
        "text": "In this article, we explore two extremely powerful ways to visualize text: word bubbles and word networks. These two visualizations are replacing word clouds as the defacto text visualization of choice because they are simple to create, understandable, and provide deep and valuable at-a-glance insights. In this post, we will examine how to construct these visualizations from a non-trivial corpus of news and blog RSS feeds. We begin by investigating the importance of text visualization. Next, we discuss our corpus and how to wrangle it. Finally, we will present word bubbles and word networks.\n\nEdward Tufte examines \u201cthe unification of text and image\u201d within Centaur, a ninth century manuscript depicting the constellation Centaurus filled with Latin text that elaborates on 24 of the stars depicted. The words within the image not only add information about the constellation, but also allow the reader to clearly see the centaur\u2019s image. This example shows the long history of text visualization and the importance of \u201ccontinuity of showing and seeing.\u201d In this case, text is used as a powerful tool to inform the viewer of the \u201cnarrative metaphor grouping the stars into a constellation.\u201d Today, we see similar images created using large corpuses to outline images. (Beautiful Evidence p. 85)\n\nThe most interesting data is often unstructured text in news articles, blog posts, emails, Twitter posts, log files, and countless other publications. However, as the amount of data grows, the task of applying analytics to extract meaningful information from the data becomes more challenging. While there is a lot of valuable information to be extracted from such data, it is not natively in a computer understandable format, which makes it difficult to present insights while keeping the natural narrative intact.\n\nModern text visualization provides a solution to this challenge. Text visualization takes a media that we\u2019re very familiar with \u2014 language, and transforms the flood of information into a snapshot of the data that is more easily interpreted than painstakingly reading through hundreds of documents. David McCandless, a London based data journalist, gave a TED Talk entitled \u201cThe beauty of data visualization.\u201d In the talk, he noted that our sense of sight is the fastest of the five senses and that the \u201ceye is exquisitely sensitive to patterns in variations in color, shape and pattern.\u201d He goes on to say that combining \u201cthe language of the eye\u201d and the \u201clanguage of the mind\u201d provide a powerful combination to gather insights from data.\n\nHow to acheive a similar high quality \u201cof display with text is an open question,\u201d Hearst states in Search User Interfaces. She continues, \u201cPlacing nominal data like words or names along an axis, unfortunately, is much like scrambling the years in a plot of change over time\u2026.Unfortunately, because text is nominal, many attempts to visualize text result in nonsensical graphs\u2026\u201d Through our two visualization suggestions in this post, we hope to present instructions for clear alternatives to summarize corpus content.\n\nIn order to demonstrate the effectiveness of text visualization, we have provided some examples of visualizations of an analysis on a non-trivial corpus. The source of these analyses is the Baleen corpus, a service that ingests news and blog posts from RSS feeds on a daily basis. The service has been collecting feeds since March 3, 2016 and has more than 52,000 articles in HTML format. The feeds span a variety of categories including news, politics, business, tech, and cooking. The following dendrogram chart shows the categories and feeds that are ingested into Baleen.\n\nThe articles are ingested into a hefty MongoDB database that is over 10GB. The database contains three primary collections:\n\nFor the purposes of this post, we will focus on the feeds and posts collections. If you would like a snapshot of the Baleen corpus, please request it via direct message on Twitter!\n\nIn order to extract the source data for analysis, the and collections were joined to create a new collection called . Because the data set was so large, we used MongoDB's built in map/reduce functionality to create the new collection as an aggregation. The MongoDB code to join the posts is below:\n\nThe fields of interest in the joined collection are:\n\nNow that the data set was aggregated in the collection, we extracted the data category as JSON files using . For example, below is a query that pulls out all the article information from the \"business\" category:\n\nThe JSON files were piped through Python code using a library called Newspaper to perform basic data wrangling. Note that it is possible to have simply parsed the HTML content in the posts collection of the Baleen database using a package such as Beautiful Soup. However, we chose not to do so because we found Newspaper to be simpler and more effective in parsing HTML content. The parsing step in Newspaper is just one line of code and it worked for the majority of the articles that we parsed. The input for Newspaper is a URL (in this case, the article URL was used). The URL is used to go the specific web page, download the HTML document, and parse the document into a readable text file. The code can be modified based on how the text files need to be organized.\n\nThe code below organizes the files in the following manner in the file system:\n\nSince the early days of text visualization, word clouds have been used exhaustively as a means to represent text data. The idea behind word clouds is to use font size to denote frequency of usage of a given word. However, word clouds are becoming increasingly unpopular among data analysts as interest in Natural Language Processing grows. Jacob Harris, former NYTimes senior software architect, called word clouds \u201ca shoddy visualization\u201d that \u201csupport only the crudest sorts of text analysis.\u201d Word clouds are often confusing, difficult to read, and do not help convey any information about the text. The word cloud below haphazardly places text; some words are placed horizontally, while others are placed vertically. The reader has to spend effort to make out the words, which deters from understanding what the visual is about. Some word clouds add color to the words, which make the visual look pretty, but do little else.\n\nUtilizing the and R libraries, the Baleen corpus was stripped of whitespace, custom stopwords, and stemmed to create the default word cloud shown below:\n\nRather than using word clouds, we prefer to use word bubbles. The visual below shows the top 25 words used in Baleen\u2019s news feeds in May 2016. The words are enclosed in bubbles, which vary in size based on the word\u2019s frequency. The words are all presented horizontally, allowing the reader to focus on the bubble size to make comparisons. In addition, the visualization uses tooltips. A tooltip is a great tool to provide additional information without adding clutter to a data visualization. The reader simply needs to hover over a bubble of interest and the tooltip provides more detail about the data the bubble represents.\n\nIn this visualization, the word and its frequency are displayed. The bubbles are randomly filled with colors to beautify the image. A potential enhancement to the visual is to use shades of a single color to differentiate between the bubbles. For example, the smallest bubble can have the lightest shade and the largest bubble can have the darkest shade. With this change, both bubble size and color can be used to make comparisons between different words and their usage.\n\nThe Javascript code leverages d3.js to generate the word bubbles. The top 25 words were generated by month from February 2016 to May 2016 and stored as csv files and placed in directories by month. The script takes these files as input and dynamically generates the visualization based on the month chosen by the user.\n\nThe word bubbles are a well-organized alternative to show frequently appearing content within the corpus, but still miss connections between words; therefore, we will explore word networks next.\n\nIn order to create the network visualization, we use Python\u2019s NLTK library to find the top 30 one-word, non-plural people/organizations using named-entity recognition. Using these entities, we identify similar words from immediate context counts. In particular, if a word appears directly before or after the person/organization of interest multiple times, this is considered a similar word. The created Python dictionary is transformed into a JSON file where each node is a word, which links to a target, colored by group according to the base named-entity.\n\nExample code for utilizing NLTK for this purpose is shown below. The corpus was also processed (tolower, custom stopwords removed, etc.) after it was read into Python. The dictionary was later used to create the JSON for d3, which was modelled using this JSON.\n\nThe network graph should give a more comprehensive depiction of the conversation happening within the text. Using context to link important people and organizations creates an understanding of the content, relationships, and the message within the text. For example, in early May, Leicester City Football Club won the Premier League title. Our network includes \u201cLeicester,\u201d which links to words like \u201cBBC,\u201d \u201cCNN,\u201d \u201cself motivate,\u201d \u201cwork,\u201d \u201cManchester United,\u201d and \u201cstreets.\u201d Thus, the network graph tells part of the story of Leicester City, a self-motivated, hardworking team whose title-decider against Manchester United helped them claim victory. The Cinderella story reported both in the UK and in the US sparked celebrations in the streets of Leicester.\n\nThis force-directed network graph depicts the Baleen corpus as a narrative. Some of the proper nouns relate directly to one another (i.e. \u201cTrump\u201d and \u201cCruz\u201d). The chart also shows how people and organizations within the text relate to each other through their similar words. Words that are shared by two frequently used proper nouns connect the node clusters together. When the similar word links to two proper nouns, it can show conceptual similarities between two topics of interest. By examining nodes within each cluster, we can gain insight into understanding each word of interest. Through the connections, we can find similarities between each cluster. The code used to generate the network graph can be viewed here.\n\nAs the scope of what we consider to be data increases, the role of the data visualizer becomes more important. Visualizations must be well thought out, maintaining a self-explanatory story while utilizing a greater number of dimensions. We focus on text data, extracted from the Baleen corpus, to achieve this goal. This was done by combining relevant data from two of the collections from the MongoDB database, outputting the results into JSON files, and using Python\u2019s Newspaper package to convert the HTML content into text. After extracting the data, we created two visualizations. Our word bubble chart uses bubble size to compare frequency of word usage, but also uses a tooltip to display the exact frequency. The network visualization shows potential relationships between different frequently used proper nouns in the data in an attempt to visualize the greater conversation within the corpus. These are just two ways to improve text visualization, to move beyond the word cloud."
    },
    {
        "url": "https://medium.com/district-data-labs/visual-diagnostics-for-more-informed-machine-learning-7ec92960c96b",
        "title": "Visual Diagnostics for More Informed Machine Learning",
        "text": "Welcome back! In this final installment of Visual Diagnostics for More Informed Machine Learning, we\u2019ll close the loop on visualization tools for navigating the different phases of the machine learning workflow. Recall that we are framing the workflow in terms of the \u2018model selection triple\u2019 \u2014 this includes analyzing and selecting features, experimenting with different model forms, and evaluating and tuning fitted models. So far, we\u2019ve covered methods for visual feature analysis in Part 1 and methods for model family and form exploration in Part 2. This post will cover evaluation and tuning, so we\u2019ll begin with two questions:\n\nLet\u2019s start with the first question.\n\nYou\u2019ve probably heard other machine learning practitioners talking about their F1 scores or their R-Squared value. Generally speaking, we do tend to rely on numeric scores to tell us when our models are performing well or poorly. There are a number of measures we can use to evaluate our fitted models.\n\nAs you\u2019ve probably guessed, I\u2019m going to propose using visualizations in combination with the numeric scores to build better intuition around performance. In the next few sections, I\u2019ll share some of the tools I have found useful in my own learning. The overarching idea I want to convey is that that a single score, or even a single plot, is not enough. It isn\u2019t useful to think of machine learning models as \u2018good\u2019 or \u2018bad\u2019 devoid of context. A model is good if it manages to use the smallest set of features that produce the most predictive model. A fitted model is good if it\u2019s better than the results you get from fitting another model form or if it\u2019s better than the model you used to make predictions yesterday. And later in the post, when we move to answering the second of our two questions, we\u2019ll say that a model is good when it\u2019s instantiated using the specific combination of hyperparameters that result in its best performing version.\n\nIn the previous section, when we ran each of our models, the outputs were some of the standard metrics ( , , ) that are routinely used to determine how well a classifier or regressor is performing. But those are only a few of the options available. Below are a few of the most common ones, along with their interpretations and function calls in Scikit-Learn,\n\nNext we\u2019ll delve into these metrics with a bit more depth and explore how to deploy visualization tools from Scikit-Learn to better \u2018see\u2019 how our models are performing.\n\nWhen we evaluate our classifiers for room occupancy and credit card default, the implicit question we are asking is how well our predicted values matched the actual labeled values for both sets. But within this notion of \u2018how well\u2019 there are a lot of possibilities. For instance, there are two ways to be right: we can be right if our classifier correctly identifies cases where a room was occupied or when a credit card customer defaulted on their payment. These are called True Positives. But we can also be right by correctly identifying vacant rooms and cases where credit card customers successfully paid off their debt (i.e. True Negatives).\n\nLikewise, there is more than one way to be wrong \u2014 by labeling a room vacant that is actually occupied (or by labeling a client as a defaulter when they actually paid), or by labeling a room occupied when it is actually empty (or by identifying a customer as having paid their bill when they actually defaulted). The first type of error is a False Positive (or sometimes Type I Error), and the second is a False Negative (or Type II Error).\n\nThere are going to be cases where we will care about being wrong in one way more than in another. But in a generalized case, we\u2019d ideally like to be able to tell how we did along all of those parameters. For this, we use a confusion matrix.\n\nScikit-Learn comes with a built-in function for generating confusion matrices, , which takes as an argument the actual values from the dataset and the predicted values generated by the fitted model, and outputs a confusion matrix. Our function from Part 2 is a wrapper for the Scikit-Learn function, and when you run the code you end up with something like looks like this:\n\nTrue to their name, confusion matrices can sometimes be a bit difficult to unpack, particularly the more classes you have. So instead, I often prefer using a classification report.\n\nClassification reports include the same basic information as in a confusion matrix, but with several added advantages. First, where the confusion matrix merely labels whether instances have be classified properly or improperly, a classification report provides three different evaluation metrics: precision, recall, and F1 score. Moreover, the classification report can conveniently include the names of the labels for each of the classes, which helps a lot with interpretability. Making one is as simple as:\n\nHowever, with some gentle manipulation of the built-in classification report metric from Scikit-Learn, we can also integrate a color-coded heatmap that will help guide our eye towards our predictive successes (the oranges) and weaknesses (the greys). In my code below I\u2019ve used a custom colormap, but you can use whichever colors you find most informative.\n\nEasier to interpret, right? One of the things that I find helpful about the classification heatmap is that it makes me reflect on my relative tolerance for Type I vs. Type II errors for the given problem. On the other hand, one thing that the code above does not do is allow for comparison across models, which is important to evaluating the performance of a fitted model. For that reason, in this post we\u2019ll be using a slightly different version of the prediction functions ( and ) that we developed in Part 2.\n\nThe advantage of the function below is that it outputs a tuple with the actual labeled (e.g. expected) values and the predicted values generated by the fitted model. This tuple form will make it easier for us to do different kinds of visual comparisons across fitted models so that we can decide for ourselves which is the best.\n\nAnother way to examine the performance of our classifiers is with the Receiver Operating Characteristic (ROC). We can import from the Scikit-Learn metrics module and in order to get a numeric calculation of the true positive and false positive rates, as well as the thresholds. Even better, we can plot the ROC to visualize the tradeoff between our classifier's sensitivity (how well it is optimized to find true positives) and its specificity (how well it is optimized to avoid false positives).\n\nIn the plot below, the x-axis indicates the False Positive Rate and the y-axis shows the True Positive Rate. We have the added advantage of being able to compare the performance of two different fitted models, and we can see that the has outperformed the classifier.\n\nGenerally speaking, if your ROC curve is a straight horizontal line, your classifier is perfect (which should make you a bit skeptical about your data). If your curve is pulling a lot toward the upper left corner, your classifier has good accuracy. If your curve is exactly aligned with the diagonal, your classifier is about as effective as a random coin toss.\n\nWe have also calculated the area under curve (AUC) and integrated that into our plot. As I mentioned above, what matters most is relative AUC (i.e. how much better or worse the calculated AUC is for one model compared to another). But generally speaking, if the AUC is greater than 0.80, I have the sense that my classifier is very strong. If my AUC is between 0.60\u20130.80, my classifier is good, but might be better if I kept tuning or changed model forms. An AUC of less than 0.60 might lead me to question whether the features I am using are actually predictive.\n\nLet\u2019s say we want to experiment with a few different models for our concrete dataset and then determine which one performs the best. In the Part 2 examples, the output for was presented in terms of the mean squared errors and coefficients of determination (R-Squared), which looked something like this:\n\nThe numeric scores are helpful, particularly when we are able to compare mean squared error and R across different fitted models. But they don\u2019t give us a good feel for why a certain model is outperforming another, and they don\u2019t tell us how to tune the parameters of our models so that we can improve the scores. Next we\u2019ll see two visual evaluation techniques that can help us with diagnosing regression model deficiencies: prediction error plots and residual plots.\n\nTo get a sense of how often our model is predicting values that are close to the expected values, we\u2019ll plot the actual labels from the concrete dataset (which indicates the strength of the concrete) against the predicted value generated by each of our models. Below is a plot of the error for three different fitted regression models on our concrete dataset: , , and a wildcard .\n\nWhat we\u2019re looking for here is a clear relationship between the predicted and actual values. We can see that both of the linear models perform fairly well (though not identically), while the support vector machine model is a flop.\n\nA residual is the difference between the labeled value and the predicted value for each instance in our dataset. We can plot residuals to visualize the extent to which our model has captured the behavior of the data. By plotting the residuals for a series of instances, we can check whether they\u2019re consistent with random error; we should not be able to predict the error for any given instance. If the data points appear to be evenly (randomly) dispersed around the plotted line, our model is performing well.\n\nWhat we\u2019re looking for is a mostly symmetrical distribution with points that tend to cluster towards the middle of the plot, ideally around smaller numbers of the y-axis. If we observe some kind of structure that does not coincide with the plotted line, we have failed to capture the behavior of the data and should either consider some feature engineering, selecting a new model, or an exploration of the hyperparameters.\n\nEvery estimator has advantages and drawbacks, which we can think of in terms of its bias (or average error for different training sets), its variance (e.g. how sensitive it is to varying training sets). We can diagnose bias and variance by looking to the particular kinds of structures we observe in plots of non-random residuals.\n\nFor example, an average value of y that is not zero given thin vertical strips of the graph is indicative of bias. A plot with points that are not even distributed across the x-axis is evidence of heteroscedastic residuals. Both can often be addressed through hyperparameter tuning (which we\u2019ll discuss in the next section) to strengthen the predictive power of the model.\n\nIf we notice our fitted model has high bias, we can try to add more features, add complexity by picking a more sophisticated model form, or we can decrease the amount of penalty or regularization. If we notice our fitted model has high variance, we can try using fewer features, training on more samples, or increasing regularization. For an excellent visual walkthrough of the bias-variance tradeoff, check out this essay by Scott Fortmann-Roe.\n\nThis kind of evaluation of our models should flow directly into a reflection on the models we initially selected, in some cases leading us to choose different models. Our model evaluations should also prompt us to consider tuning, which we\u2019ll get to in the next section.\n\nWe started off by asking how we know when a machine learning model is working and how we can make it work better. Now that we\u2019ve reviewed some methods for assessing how well a model is working, let\u2019s next consider what it takes to make a model perform better. As you may have noticed, for every model we have used so far, we have accepted the default Scikit-Learn parameters. For most of our fitted models, the scores were fairly high, so the defaults served us pretty well. But we will not always be so lucky, and getting good at tuning a model by adjusting its parameters, or hyperparameter tuning, is the next step to getting good at machine learning.\n\nHow do you pick the best parameters? One method is to use validation curves to visualize training and validation scores of a model through different values of a single hyperparameter. Let\u2019s experiment with a classifier for our credit card default data set. We'll make a validation curve by plotting the training scores and validation scores along different values of the parameter gamma.\n\nWhat we\u2019re looking for is the spot with the highest value for both the training and the validation scores. If both scores are low, it\u2019s an indication of underfit. If the training score is high but the validation score is low, it\u2019s an indication of overfit.\n\nNote that in the code provided below, I\u2019ve illustrated the execution of validation curves using just the first few columns of the credit default dataset. This was done for speed\u2019s sake because the dataset is so high dimensional and because I\u2019m using 6-fold cross-validation. Be prepared for this to take a while IRL.\n\nWhen it comes to hyperparameter tuning, most people use grid search. Grid search is a brute force mechanism for trying all possible combinations of the algorithm\u2019s parameters. By establishing a range of discrete values for multiple hyperparameters, each can be seen as an axis, and the grid is just the set of points representing every combination. Each combination is tested individually and the best is returned.\n\nIn our model for the credit data set, we can try to improve our accuracy by experimenting with different values for the kernel coefficient gamma of the radial basis function, as well as for C, the penalty parameter of the error term:\n\nThe upside of this approach is that it meets our \u2018models are only good by comparison\u2019 criterion, because it allows us to explore a multitude of different fitted models and pick the very best one. The downside of this approach is that it is a blind search. The best case scenario is that you end up with a better performing model but no additional intuition around its tuning. A common scenario for beginners is ending up without a better model or more intuition.\n\nHere\u2019s the problem: with grid search, the effective selection of the initial search range for the parameters requires some understanding of what parameters are available, what those parameters mean, what impact they can have on a model, and what a reasonable search space might be.\n\nHere again, visualizations can offer some help, and instead of using the function, we can create a .\n\nIn the heatmap (coolmap?) above, we can see the combinations of values of C and gamma where our model is doing best. We can compare those combinations with other places on the map where our fitted model performed less well. We can start to see that for our credit card default dataset, there\u2019s a sweet spot around the area where C is set to 0.01, which reaches peak performance when gamma is around 1e-05.\n\nMethods like can help sensitize us to the relationships between a model's accuracy scores and it's different hyperparameter values. Yes, hyperparameter tuning is still hard. Some folks spend years in school studying and investigating the complexities of different model parameters. Spinning up that kind of hard-won intuition isn't going to happen overnight, but visualizations can add insight and take grid searching out of the black box.\n\nAlthough many of us tend to think of graphs and diagrams as the end phase of the pipeline, visualization has a critical role to play throughout the machine learning process. Many tools are available and already implemented in Scikit-Learn, Matplotlib, Pandas, Bokeh, and Seaborn. Now that you\u2019ve seen them in action, I hope you\u2019ll enjoy experimenting with them and iterating on them for your own data sets!\n\nOf course, there are also many tools that don\u2019t exist yet \u2014 particularly ones that enable interactive visual steering and feature analysis (like smooth zoom-and-filter implementations of hierarchical aggregation for multi-dimensional data) and hyperparameter tuning (like slick parameter sliders). But these tools are likely not far off, given the explosive growth in machine learning-based applications and the growing demand for data products, so keep your eyes peeled!"
    },
    {
        "url": "https://medium.com/district-data-labs/visual-diagnostics-for-more-informed-machine-learning-33c9c69ef5ec",
        "title": "Visual Diagnostics for More Informed Machine Learning",
        "text": "When it comes to machine learning, ultimately the most important picture to have is the big picture. Discussions of (i.e. arguments about) machine learning are usually about which model is the best. Whether it\u2019s logistic regression, random forests, Bayesian methods, support vector machines, or neural nets, everyone seems to have their favorite! Unfortunately these discussions tend to truncate the challenges of machine learning into a single problem, which is a particularly problematic misrepresentation for people who are just getting started with machine learning. Sure, picking a good model is important, but it\u2019s certainly not enough (and it\u2019s debatable whether a model can actually be \u2018good\u2019 devoid of the context of the domain, the hypothesis, the shape of the data, and the intended application. But we\u2019ll leave that to another post.\n\nIn this post we\u2019ll discuss model selection in the context of the big picture, which I\u2019ll present in terms of the model selection triple, and we\u2019ll explore a set of visual tools for navigating the triple.\n\nProducing a fitted model that is well-suited to the data, predictive, and also performant is critically dependent on feature selection and tuning as well as model selection. Kumar et al. refer to this trio of steps as the model selection triple. As they explain\u2026\n\nIn other words, this is the part that makes machine learning hard. The process is complex, iterative, and disjointed, often with many missteps and restarts along the way. And yet these iterations are central to the science of machine learning \u2014 optimization is not about limiting those iterations (e.g. helping you pick the best model on the first try every time), but about facilitating them. For that reason, let\u2019s begin with the visualization I think is the most important of all: a view of the workflow that I use to put together all of the steps and visual diagnostics described throughout Parts 1, 2, and 3 of this post.\n\nAs shown in the diagram below, I begin with data stored on disk and take a first pass through feature analysis using histograms, scatterplots, parallel coordinates and other visual tools. My analysis of the features often leads back to the data, where I take another pass through to normalize, scale, extract, or otherwise wrangle the attributes. After more feature analysis has confirmed I\u2019m on the right track, I identify the category of machine learning models best suited to my features and problem space, often experimenting with fit-predict on multiple models. I iterate between evaluation and tuning using a combination of numeric and visual tools like ROC curves, residual plots, heat maps and validation curves. Finally, the best model is stored back to disk for later use.\n\nIn Part 1 of this post, we covered the feature analysis tools, and we\u2019ll explore evaluation and tuning later in Part 3. Here in Part 2, we\u2019ll be focusing on the decision-making process that goes into choosing the set of algorithms to use for a given dataset. As in Part 1, we\u2019ll be using three different datasets from the UCI Machine Learning Repository \u2014 one about room occupancy, one about credit card default, and one about concrete compressive strength. Using those three datasets, we\u2019ll explore a range of models, some of which will work better than others, and none of which will work equally well for all three. This is normal, which is why the goal is not to get good enough at machine learning that we can pick the best model on the first try every time, but to adopt a process that will facilitate exploration and iteration. Because we want to be doing informed machine learning, we do want some kind of process, and the flowcharts and maps we\u2019ll explore below can serve as a guide.\n\nThose who have used Scikit-Learn before will no doubt already be familiar with the Choosing the Right Estimator flow chart. This diagram is handy for those who are just getting started, as it models a simplified decision-making process for selecting the machine learning algorithm that is best suited to one\u2019s dataset.\n\nLet\u2019s try it together. First we are asked whether we have more than 50 samples for each of our datasets.\n\nWe do! Next we\u2019re asked if we\u2019re predicting a category. For the occupancy and credit datasets, the answer is yes. For occupancy, we are predicting whether a room is occupied (0 for no, 1 for yes), and for credit, we are predicting whether the credit card holder defaulted on their payment (0 for no, 1 for yes). For the concrete dataset, the labels for the strength of the concrete are continuous, so we are predicting a quantity, not a category. Therefore, we will be looking for a classifier for our occupancy and credit datasets, and for a regressor for our concrete dataset.\n\nSince both of our categorical datasets have fewer than 100,000 instances, we are prompted to start with (which will map the data to a higher dimensional feature space), or failing that, (which will assign instances to the class most common among its k nearest neighbors). In our feature exploration of the occupancy dataset, you'll remember that the different attributes were not all on the same scale, so in addition to the other steps, we import so that we can standardize all the features before we run fit-predict:\n\nOk, let\u2019s use the same function to model the credit default dataset next. As you'll remember from our visual exploration of the features, while there are two classes in this dataset, there are very few cases of default, meaning we should be prepared to see some manifestations of class imbalance in our classifier.\n\nMeanwhile for our concrete dataset, we must determine whether we think all of the features are important, or only a few of them. If we decide to keep all the features as is, the chart suggests using (which will identify features that are less predictive and ensure they have less influence in the model) or possibly with a linear kernel (which is similar to the LinearSVC classifier). If we guess that some of the features are not important, we might decide instead to choose (which will drop out any features that aren't predictive) or (which will try to find a happy medium between the Lasso and Ridge methods, taking the linear combination of their L1 and L2 penalties).\n\nLet\u2019s try a few because, why not?\n\nAs illustrated in the code above, the Scikit-Learn API allows us to rapidly deploy as many models as we want. This is an incredibly powerful feature of the Scikit-Learn library that cannot be understated. After all, being able to iterate and experiment is why we all got into science in the first place, right? As we learned in the beginning of this post, the workflows we use to navigate the model selection triple tend to be highly non-linear, and iteration and experimentation are particularly key to model selection.\n\nThe Scikit-Learn flowchart is useful because it offers us a map, but it doesn\u2019t offer much in the way of transparency about how the various models are functioning. For that kind of insight, there are two images that have become somewhat canonical in the Scikit-Learn community: the classifier comparison and cluster comparison plots.\n\nBecause unsupervised learning is done without the benefit of a ground truth to inform us when we have labeled data properly, this plot of small multiples is a useful way to compare different clustering algorithms across different datasets:\n\nSimilarly, the classifier comparison plot below is a helpful visual comparison of the performance of nine different classifiers across three different toy datasets:\n\nGenerally these images are used just to demonstrate the substantial differences in the performance of various models across different datasets. Unfortunately, the datasets used are synthetic, and while it would be exciting to be able to operationalize the code as a visual tool for model selection and exploration, the curse of dimensionality will pose problems for most real-world datasets. Nonetheless, I find it helpful to able to picture the behavior of different models in the same dataspace. For example, in the image above we can see a difference in the way that and divide the data up, which provides some useful insight into how the two algorithms are operating with our room occupancy and credit card default datasets.\n\nVisualizations and flow diagrams of the model selection process like Scikit-Learn\u2019s \u201cChoosing the Right Estimator\u201d can be helpful, especially when you\u2019re just getting started with machine learning. But what do you do when you\u2019ve exhausted all those options? There are a lot more models available in Scikit-Learn, and the estimator flowchart barely scratches the surface. It is possible to use an exhaustive approach to essentially test the entire catalog of Scikit-Learn models in order to find the one that works the best on your dataset. But if our goal is to be more informed machine learning practitioners, then we care not only about whether our models are working, but also about why they are (or are not) working. For that matter, we still haven\u2019t addressed what we mean when we say a model is \u2018working\u2019. The outputs of our and functions above only give us a very small picture of what's happening, and can mask a lot of problems. We'll discuss more about model evaluation in Part 3.\n\nFor now, we\u2019re looking for a more systematic way to experiment with different kinds of models once we\u2019ve exhausted the options proposed to us through the Scikit-Learn flow chart. To do that, let\u2019s first take a step back and reconsider what we mean when we say \u2018model\u2019. As Hadley Wickham points out, the word \u2018model\u2019 is an overloaded term because we use it to mean at least three different things:\n\nAs Wickham explains, model family is largely determined by the problem space, whereas model form is chosen through experimentation and statistical testing (or sometimes based on the preference of the practitioner), and a fitted model is generated through a combination of human parameter tuning and machine computation.\n\nFor our purposes, experimentation within the model forms (and as we\u2019ll discuss later, hyperparameters), is probably the place where we can expect to get the most return on our investment. Model form specifies how our features are related within the framework of the model family. In the context of our concrete model, the forms of Ridge, Lasso, and ElasticNet specify that the strength variable is the target, whereas cement content, slag, ash, water, superplasticity, coarseness, fineness, and age are the predictors. They also specify how those predictors are related to each other and to the target.\n\nOne tool I like for model exploration is Dr. Saed Sayad\u2019s interactive data mining map, because it is much more comprehensive than the Scikit-Learn flow chart and integrates a lot of the ideas of model family and form. Moreover, in addition to predictive methods, Sayad\u2019s map includes a separate section on statistical methods for explaining the past.\n\nBelow is an iteration on a genealogical chart that we\u2019ve been exploring at District Data Labs. It aims to present the same broad coverage of predictive methods as Sayad\u2019s (representing some, like reinforcement learning, not represented in original), while integrating the Scikit-Learn model classesthat correspond to the model forms. Color and hierarchy designate the model forms and model families:\n\nWhile our map isn\u2019t comprehensive (and more importantly, not yet interactive), we envision it becoming the intermediate version of the Scikit-Learn estimator flow chart \u2014 a visual selection tool that can be integrated seamlessly into the model selection triple workflow.\n\nIn Part 1 we began our visual journey at the feature analysis and feature selection phases of the model selection triple, and in Part 2, we\u2019ve moved to the model selection phase. It is worth repeating that for many machine learning practitioners, the traversal of the phases is iterative and non-linear. Within the model family that is appropriate to your problem space, it is useful (and easy) to explore multiple model forms, though I do recommend using the visualizations, as well as the other tools provided in this post (like the and wrapper functions) to experiment in as strategic a way as possible.\n\nBy comparing and contrasting the performance of different models on a single dataset, and by doing this in a repeatable way over time with a number of datasets, we can begin building intuition around the model forms likely to outperform their siblings in the model family. In Part 3, we\u2019ll move into the next phases of the model selection triple, exploring a suite of visual tools for evaluating fitted models and for tuning their parameters to improve their performance."
    },
    {
        "url": "https://medium.com/district-data-labs/visual-diagnostics-for-more-informed-machine-learning-b7ea4371bb14",
        "title": "Visual Diagnostics for More Informed Machine Learning",
        "text": "Python and high level libraries like Scikit-learn, TensorFlow, NLTK, PyBrain, Theano, and MLPY have made machine learning accessible to a broad programming community that might never have found it otherwise. With the democratization of these tools, there is now a large, and growing, population of machine learning practitioners who are primarily self-taught. At the same time, the stakes of machine learning have never been higher; predictive tools are driving decision-making in every sector, from business, art, and engineering to education, law, and defense.\n\nHow do we ensure our predictions are valid and robust in a time when these few lines of Python can instantiate and fit a model?\n\nHow do you build intuition around what initial model to select? Which features do you use? Which should you normalize? How do you identify problems like local minima and overfit? Can you get a weak model to perform better?\n\nTo help us think through these questions, let\u2019s take a look at the following four 2-dimensional arrays, imagining that we want to produce predictive models for each:\n\nWhat kind of model should we use to fit our data? Let\u2019s compute some statistical properties for each: the mean and variance, the correlation coefficient, and the slope and intercept of their linear regression.\n\nWhen you run the above code, you discover that the four arrays have the same descriptive statistical properties. This might lead us to decide to use a single model for each, maybe ? And yet, if we were to plot the points for each of the datasets, we would see that they are not at all alike:\n\nMore importantly, a simple linear regression model is not going to perform equally well on each. While we can see a linear relationship in i and iii, their regression lines are substantially different. In the ii plot, we can see that the variables are related but not linearly correlated, and also that they are not normally distributed. Moreover, both the iii and the iv datasets contain outliers big enough to strongly influence the correlation coefficients.\n\nAssembled by English statistician Frank Anscombe in 1973, the takeaway from these four datasets, known as Anscombe\u2019s Quartet, and their corresponding visualizations is that of all of the analytical tools at our disposal, sometimes our eyes are the most important. In data science, visual diagnostics are a powerful but frequently underestimated tool. Visualizations don\u2019t have to be the end of the pipeline. They can allow us to find patterns we simply cannot see by looking at raw data alone. Where static outputs and tabular data may render patterns opaque, human visual analysis can uncover volumes and lead to more robust programming and better data products.\n\nIn machine learning, where lots of things can cause trouble (messy data, overtraining, undertuning, the curse of dimensionality, etc.) visual diagnostics can mean the difference between a model that crashes and burns and one that predicts the future. In this series of posts on \u2018Visual Diagnostics for More Informed Machine Learning,\u2019 I\u2019d like to show how visualization tools can offer analytical support at several key stages in the machine learning process. I\u2019ll demonstrate how to deploy some of the visualization tools from the standard Scikit-Learn and Matplotlib libraries (along with a few tricks from Pandas, Bokeh, and Seaborn) and illustrate how these diagnostics can support the machine learning workflow, including feature analysis, model selection, and parameter tuning.\n\nIn order to explore these visualization methods in a variety of contexts, we\u2019ll be using a few different datasets from the UCI Machine Learning Repository:\n\nHere is a simple script that uses the Python module to go to the UCI page to fetch all three:\n\nBy running this script you should find a directory called in your current working directory, containing two XLS (Excel) files, a zip file, and a directory containing the unzipped room occupancy data.\n\nFeature selection is key to successful machine learning. It\u2019s a safe bet that with our three sample datasets, some feature selection had already been done before they were uploaded to the UCI repository. But when doing machine learning in the wild, we often have to do that feature selection from scratch using a combination of statistical and other methods (e.g. talking with domain experts, using visual analysis). In these real-world scenarios, we anticipate that perhaps only some attributes may be predictive (and only if we are lucky!) and that others may not be predictive at all. We also anticipate situations where there is redundancy across attributes (e.g. one is a linear combination of two others).\n\nWith feature selection, our goal is to find the smallest set of the available features such that the fitted model will reach it\u2019s maximal predictive value. Why? Firstly, minimizing the number of features we include lowers the complexity of the model, in turn reducing bias. Secondly, lower dimensional data takes a lot less computation time. Finally, in practice, models based on smaller sets of variables are frequently also more interpretable.\n\nStatistical measures like mean and variance are a very useful first step to unpacking the features to prepare for selection. Now that we\u2019ve got our data, let\u2019s import , load each into a data frame and take a quick look:\n\nWe can start to get a feel for the differences across our three datasets from the output of the statements above. For example, in the occupancy dataset, the standard deviations for light and CO2 emissions are two orders of magnitude greater than they are for temperature and humidity, meaning that some scaling may be necessary. In the credit card default dataset, the distribution of the labels (0 for credit card holders who did not default on the payment and 1 for those who did) appears uneven, which can be an indicator of possible class imbalance.\n\nHowever, if you had to select which features were most likely to be predictive based solely on the descriptive tables, it would be pretty tough, especially without domain expertise (what the heck is superplasticity?). At this point, those with some experience with predictive modeling will often begin to visualize the data so that they can see the behavior of the different feature vectors. Below we\u2019ll explore our three datasets using some common approaches to visualizing features, including:\n\nAs we\u2019re exploring the graphs, the two key things we want to be looking for are signal (e.g. patterns, separability, relationships between our features and our targets, relationships between different features, etc.) and volatility (e.g. amount of noise, distribution of data, etc.).\n\nBoxplots (or \u2018box-and-whisker\u2019 plots) enable us to look at the central tendency of the data, see the distribution, and examine outliers.\n\nIn the example above, each feature of the concrete dataset is listed out on the x-axis and for each feature, we get to visualize the data\u2019s behavior. The boxes indicate the upper and lower quartiles of the data, the black line in the center of each box indicates the median, the whiskers show the biggest and smallest values (with the outliers excluded), and the diamonds show the outliers. A boxplot of our concrete dataset (shown below, along with the code needed to generate it) shows us that most of the features are on a similar scale, with the exception of \u2018coarse\u2019 and \u2018fine\u2019. This suggests that we may want to rescale our features before we begin modeling.\n\nViolinplots are a nice alternative to traditional box-and-whiskers, because they provide the same information but also reflect relative kernel density estimates, which can be useful when looking for separability. Additionally, the two sides of the violin can be used to show relative distributions of categorical variables (particularly useful for binary classifiers)! Use instead of to generate these types of figures.\n\nHistograms enable us to bin values of individual features into buckets and visualize the domain of the feature by exposing the frequency of values as the relative size of each bucket. Below is a histogram which plots the age feature of the credit card default dataset, as well as the code needed to generate the graph.\n\nOne notable observation from this visualization is that most of the people represented in the data are under the age of 40.\n\nScatterplot matrices (or \u2018sploms\u2019) are one of my favorite feature analysis tools. With sploms, we plot all of the pairwise scatterplots of the features in a single matrix, where the diagonal is generally left blank or used to display kernel density estimates, histograms, or feature labels. Sploms are a way to check the pairwise relationships between features. When we look at a scatterplot matrix, we are looking for covariance, for relationships that appear to be linear, quadratic, or exponential, and for either homoscedastic or heteroscedastic behavior that will tell us how the features are dispersed relative to each other. In the scatterplot for the concrete dataset below, we can see what appears to be heteroscedastic behavior in the pairwise plot of strength and cement content.\n\nNote that the Seaborn function for a scatterplot matrix is called :\n\nRadial visualizations are based on a spring tension minimization algorithm. The features of the dataset are equally spaced on a unit circle and the instances are dropped into the center of the circle. The features then \u2018pull\u2019 the instances towards their position on the circle in proportion to their normalized numerical value for that instance.\n\nAs of this writing, radial visualizations are not yet available in Seaborn, so we can use the Pandas function together with the Seaborn to maintain a consistent look:\n\nIn the radviz graph for the occupancy dataset above, we can see that there is some definite separation between the rooms that are labeled as occupied and those that are vacant. Moreover, it appears that temperature seems to be one of the more predictive features, given how strongly the green dots (the unoccupied rooms) are being \u2018pulled\u2019 toward that part of the circle.\n\nParallel coordinates, like radviz plots, are a way to visualize clusters in data. Data points are represented as connected line segments. The x-axis units are not meaningful, and instead, each vertical line represents one attribute. One set of connected line segments represents one instance. Points that tend to cluster will appear closer together, and we look for thick chords or braids of lines of the same color that would indicate good class separability.\n\nAs with radial visualization, we must resort to using the Pandas function :\n\nFeature analysis can be a big challenge as the dimensionality of the data increases, even for experts. Frankly, there aren\u2019t a lot of tools out there for dealing with high-dimensional data. As they are currently implemented in Python, neither radviz nor parallel coordinates scale particularly well to very high dimensional data (although, making them more interactive and enabling the user to drag features in and out of the graph would help!).\n\nGenerally speaking, the number of dimensions must be reduced through techniques such as hierarchical aggregation, dimensionality reduction (like PCA and LDA), and dimensional subsetting. For dimensional subsetting, one visual tactic is to use the scatterplot matrix approach to generate small multiples. Another is to do a series of independent jointplots to examine the relationships and correlations between each possible pair of features.\n\nIn the jointplot below, we can examine the relationship between the amounts of individuals\u2019 first bill in April and their last bill in September.\n\nFeature analysis is a critical part of machine learning, and it does increase significantly in complexity as the number of potential features increases. But as I hope I\u2019ve illustrated, feature selection doesn\u2019t have to be mysterious. Statistical tools like correlation coefficients (seen in the jointplot above) and LASSO (which we\u2019ll explore a bit more in Part 2) are very useful tools for identifying the smallest set of maximally predictive features. The advantage of using tools like boxplots, histograms, and sploms in concert with statistical methods is that they enable us to incorporate visual analysis to help build intuition. Just as Anscombe\u2019s quartet illustrates the power of visualizations to unlock insights about data, visual feature selection opens up insight into our data. This is particularly useful for those getting started with machine learning (and can also help with seasoned machine learning practitioners who are modeling data in a new domain space).\n\nThrough the course of our visual exploration of the datasets on room occupancy, credit card default, and concrete we\u2019ve developed a much better sense of what we want to be predicting, and also of which features will help us to do that. In addition to being a practical and accessible way to augment the feature analysis process, visual tools can help guide us toward selecting the right machine learning algorithm for the job. In Part 2 of this series, we\u2019ll continue exploring our three datasets and discuss how visualizations can facilitate the model selection process."
    },
    {
        "url": "https://medium.com/district-data-labs/named-entity-recognition-and-classification-for-entity-extraction-6f23342aa7c5",
        "title": "Named Entity Recognition and Classification for Entity Extraction",
        "text": "The overwhelming amount of unstructured text data available today from traditional media sources as well as newer ones, like social media, provides a rich source of information if the data can be structured. Named Entity Extraction forms a core subtask to build knowledge from semi-structured and unstructured text sources. Some of the first researchers working to extract information from unstructured texts recognized the importance of \u201cunits of information\u201d like names (such as person, organization, and location names) and numeric expressions (such as time, date, money, and percent expressions). They coined the term \u201cNamed Entity\u201d in 1996 to represent these.\n\nConsidering recent increases in computing power and decreases in the costs of data storage, data scientists and developers can build large knowledge bases that contain millions of entities and hundreds of millions of facts about them. These knowledge bases are key contributors to intelligent computer behavior. Not surprisingly, Named Entity Extraction operates at the core of several popular technologies such as smart assistants (Siri, Google Now), machine reading, and deep interpretation of natural language.\n\nThis post explores how to perform Named Entity Extraction, formally known as \u201cNamed Entity Recognition and Classification (NERC). In addition, the article surveys open-source NERC tools that work with Python and compares the results obtained using them against hand-labeled data.\n\nThe information extraction concepts and tools in this article constitute a first step in the overall process of structuring unstructured data. They can be used to perform more complex natural language processing to derive unique insights from large collections of unstructured data.\n\nIn order to follow along with the work in this article, we recommend using Anaconda, which is an easy-to-install, free, enterprise-ready Python distribution for data analytics, processing, and scientific computing. With a few lines of code, you can have all the dependencies used in this post with, the exception of one function (email extractor).\n\nIf you use an alternative method to set up a virtual environment, make sure you have all the files installed from the yml file. The one dependency not in the yml file is the email extractor. Cut and paste the function from this Gist, save it to a .py file, and make sure it is in your sys.path or environment path.\n\nThe proceedings from the Knowledge Discovery and Data Mining (KDD) conferences in New York City (2014) and Sydney, Australia (2015) serve as our source of unstructured text and contain over 200 peer reviewed journal articles and keynote speaker abstracts on data mining, knowledge discovery, big data, data science, and their applications. The full conference proceedings can be purchased for $60 at the Association for Computing Machinery\u2019s Digital Library (includes ACM membership).\n\nThis post will work on a data set that is equivalent to the combined conference proceedings, but only use abstracts and extracts from the text, rather than the full proceedings, a data set that can be found on the ACM website. We will explore reading PDF data and discuss follow-on analytics if the full proceedings are available to you.\n\nVisual inspection reveals that the target filenames begin with a \u201cp\u201d and end with \u201cpdf.\u201d As a first step, we determine the number of files and the naming conventions by using a loop to iterate over the files in the directory and printing out the filenames. Each filename also gets saved to a list, and the length of the list tells us the total number of files in the dataset.\n\nA total of 253 files exist in the directory. Opening one of these reveals that our data is in PDF format and that it is semi-structured (follows journal article format with separate sections for \u201cabstract\u201d and \u201ctitle\u201d). While PDFs provide an easily readable presentation of data, they are extremely difficult to work with in data analysis. In your work, if you have an option to get to data before conversion to a PDF format, be sure to take that option.\n\nWe used several Python tools to ingest our data, including the following libraries:\n\nOur task begins by iterating over the files in the directory with names that begin with \u201cp\u201d and end with \u201cpdf.\u201d This time, however, we will strip the text from the pdf file, write the .txt file to a newly created directory, and use the variable to name the files we write to disk. Keep in mind that this task may take a few minutes depending on the processing power of your computer.\n\nNext, we build a custom NLTK corpus. Having our target documents loaded as an NLTK corpus brings the power of NLTK to our analysis goals.\n\nWe now have a semi-structured dataset in a format that we can query and analyze. First, let\u2019s see how many words (including stop words) we have in our entire corpus and the vocabulary of the corpus.\n\nThe NLTK book has an excellent section on processing raw text and unicode issues. It provides a helpful discussion of some problems you may encounter.\n\nTo begin our exploration of regular expressions (a.k.a. \u201cregex\u201d), it\u2019s important to point out some good resources for those new to the topic. An excellent resource may be found in Johns Hopkins University\u2019s Coursera video titled Getting and Cleaning Data.\n\nAs a simple example, let\u2019s extract titles from the first 10 documents.\n\nThe result is as follows:\n\nThis code extracts the titles, but some author names get caught up in the extraction as well.\n\nFor simplicity, let\u2019s focus on wrangling the data to use the NERC tools on two sections of the paper: the \u201ctop\u201d section and the \u201creferences\u201d section. The \u201ctop\u201d section includes the names of authors and schools. This section represents all of the text above the article\u2019s abstract. The \u201creferences\u201d section appears at the end of the article. The regex tools of choice to extract sections are the and expressions. Using these, we will build two functions designed to extract the \u201ctop\u201d and \u201creferences\u201d sections of each document.\n\nIn addition to extracting the relevant sections of the documents, our two functions will obtain a character count for each section, extract emails, count the number of references and store that value, calculate a word per reference count, and store all the above data as a nested dictionary with filenames as the key.\n\nThe above code also makes use of the tool to create the \"word per reference\" statistic (takes time to run).\n\nI want to take an opportunity here to say few words about the data. When working with natural language, one should always be prepared to deal with irregularities in the data set. This corpus is no exception. It comes from a top-notch data mining organization, but human error and a lack of standardization makes its way into the picture. For example, in one paper the header section is entitled \u201cCategories and Subject Descriptors,\u201d while in another the title is \u201cCategories & Subject Descriptors.\u201d While that may seem like a small difference, these types of differences cause significant problems. There are also some documents that will be missing sections altogether, i.e. keynote speaker documents do not contain a \u201creferences\u201d section. When encountering similar issues in your work, you must decide whether to account for these differences or ignore them. I worked to include as much of the 253-document corpus as possible.\n\nNext, let\u2019s test the \u201creferences\u201d extraction function and look at the output by obtaining the first 10 entries of the dictionary created by the function. This dictionary holds all the extracted data and various calculations. The module is a great tool to visualize descriptive outputs in table format.\n\nThe output is as follows:\n\nAs you can see, this is a good start to performing bibliographic analysis with Python.\n\nNow that we have a method to obtain the corpus from the \u201ctop\u201d and \u201creferences\u201d sections of each article in the dataset, we are ready to perform the named entity extractions. In this post, we examine three popular, open source NERC tools. The tools are NLTK, Stanford NER, and Polyglot. A brief description of each follows.\n\nWe can now test how well these open source NERC tools extract entities from the \u201ctop\u201d and \u201creference\u201d sections of our corpus. For two documents, I hand labeled authors, organizations, and locations from the \u201ctop\u201d section of the article and the list of all authors from the \u201creferences\u201d section. I also created a combined list of the authors, joining the lists from the \u201ctop\u201d and \u201creferences\u201d sections. Hand labeling is a time consuming and tedious process. For just the two documents, this involved 295 cut-and-pastes of names or organizations.\n\nAn easy test for the accuracy of a NERC tool is to compare the entities extracted by the tools to the hand-labeled extractions. Before beginning, we take advantage of the NLTK functionality to obtain the \u201ctop\u201d and \u201creferences\u201d sections of the two documents used for the hand labeling:\n\nFor each NERC tool, I created functions to extract entities and return classes of objects in different lists.\n\nIn this next block of code, we will apply the NLTK standard chunker, Stanford Named Entity Recognizer, and Polyglot extractor to our corpus. We pass our data, the \u201ctop\u201d and \u201creferences\u201d section of the two documents of interest, into the functions created with each NERC tool and build a nested dictionary of the extracted entities \u2014 author names, locations, and organization names. This code may take a bit of time to run (30 secs to a minute).\n\nWe will focus specifically on the \u201cpersons\u201d entity extractions from the \u201ctop\u201d section of the documents to estimate performance. However, a similar exercise is possible with the extractions of \u201corganizations\u201d entity extractions or \u201clocations\u201d entity extractions too, as well as from the \u201creferences\u201d section.\n\nTo get a better look at how each NERC tool performed on the named person entities, we will use the dataframe. is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. The dataframe provides a visual comparison of the extractions from each NERC tool and the hand-labeled extractions. Just a few lines of code accomplish the task:\n\nThe above dataframe illustrates the mixed results from the NERC tools. NLTK Standard NERC appears to have extracted 3 false positives while the Stanford NER missed 2 true positives and the Polyglot NERC extracted all but one true positive (partially extracted; returned first name only).\n\nHere is a quick figure summarizing overall model performance.\n\nThe following function calculates the metrics for the three NERC tools:\n\nNow let\u2019s pass our values into the function to calculate the performance metrics:\n\nThe basic metrics above reveal some quick takeaways about each tool based on the specific extraction task. The NLTK Standard Chunker has perfect accuracy and recall but lacks in precision. It successfully extracted all the authors for the document, but also extracted 3 false entities. NLTK\u2019s chunker would serve well in an entity extraction pipeline where the data scientist is concerned with identifying all possible entities\n\nThe Stanford NER tool is very precise (specificity vs sensitivity). The entities it extracts were 100% accurate, but it failed to identify half of the true entities. The Stanford NER tool would be best used when a data scientist wanted to extract only those entities that have a high likelihood of being named entities, suggesting an unconscious acceptance of leaving behind some information.\n\nThe Polyglot Named Entity Recognizer identified five named entities exactly, but only partially identified the sixth (first name returned only). The data scientist looking for a balance between sensitivity and specificity would likely use Polyglot, as it will balance extracting the 100% accurate entities and those which may not necessarily be a named entity.\n\nIn our discussion above, we notice the varying levels of performance by the different NERC tools. Using the idea that combining the outputs from various classifiers in an ensemble method can improve the reliability of classifications, we can improve the performance of our named entity extractor tools by creating an ensemble classifier. Each NERC tool had at least 3 named persons that were true positives, but no two NERC tools had the same false positive or false negative. Our ensemble classifier voting rule is very simple: Return all named entities that exist in at least two of the true positive named entity result sets from our NERC tools.\n\nWe implement this rule using the module. We first do an operation of the NERC results vs the hand labeled entities to get our \"true positive\" set.\n\nHere is our code to accomplish the task:\n\nTo get a visual comparison of the extractions for each tool and the ensemble set side by side, we return to our dataframe from earlier. In this case, we use the operation in pandas to append the new ensemble set to the dataframe.\n\nFirst, a quick visual to see how performance improved.\n\nAnd we get a look at the performance metrics to see if we push our scores up in all categories:\n\nExactly as expected, we see improved performance across all performance metric scores and, in the end, get a perfect extraction of all named persons from this document.\n\nBefore we go any further, the idea of moving from \u201cokay\u201d to \u201cperfect\u201d is unrealistic. Moreover, this is a very small sample and only intended to show the application of an ensemble method. Applying this method to other sections of the journal articles will not lead to a perfect extraction, but it will indeed improve the performance of the extraction considerably.\n\nA good rule for any data analytics project is to store the results or output in an open file format. I selected JavaScript Object Notation (JSON), which is an open standard format that uses human-readable text to transmit data objects consisting of attribute\u2013value pairs.\n\nLet\u2019s take our list of persons from the ensemble results, store it as a Python dictionary, and then convert it to JSON. Alternatively, we could use the function from the module to return dictionaries, and ensure we get the open file format at every step. This way, other data scientists or users could pick and choose what portions of code to use in their projects.\n\nIn this post, we\u2019ve covered the entire data science pipeline in a natural language processing job that compared the performance of three different NERC tools. A core task in this pipeline involved ingesting plaintext into an NLTK corpus so that we could easily retrieve and manipulate the corpus. Then we used the results from the various NERC tools to create a simplistic ensemble classifier that improved the overall performance.\n\nThe techniques in this post can be applied to other domains, larger datasets or any other corpus. Everything I used in this post (with the exception of the Regular expression resource from Coursera) was not taught in a classroom or structured learning environment. It all came from online resources, posts from others, and books (that includes learning how to code in Python). If you have the motivation, you can do it.\n\nThroughout the article, there are hyperlinks to resources and reading materials for reference, but here is a central list:"
    },
    {
        "url": "https://medium.com/district-data-labs/building-a-classifier-from-census-data-18f996c4d7cf",
        "title": "Building a Classifier from Census Data \u2013 District Data Labs \u2013",
        "text": "One of the machine learning workshops given to students in the Georgetown Data Science Certificateis to build a classification, regression, or clustering model using one of the UCI Machine Learning Repository datasets. The idea behind the workshop is to ingest data from a website, perform some initial analyses to get a sense for what\u2019s in the data, then structure the data to fit a Scikit-Learn model and evaluate the results. Although the repository does give advice as to what types of machine learning might be applied, this workshop still poses a challenge, especially in terms of data wrangling.\n\nIn this post, I\u2019ll outline how I completed this workshop alongside my students this past weekend. For those new to machine learning or to Scikit-Learn, I hope this is a practical example that may shed light on many challenges that crop up developing predictive models. For more experienced readers, I hope that I can challenge you to try this workshop, and to contribute iPython notebooks with your efforts as tutorials!\n\nThe first part of the workshop is to use the UCI Machine Learning Repository to find a non-trivial dataset with which to build a model. While the example datasets included with Scikit-Learn are good examples of how to fit models, they do tend to be either trivial or overused. By exploring a novel dataset with several (more than 10) features and many instances (more than 10,000), I was hoping to conduct a predictive exercise that could show a bit more of a challenge.\n\nThere are around 350 datasets in the repository, categorized by things like task, attribute type, data type, area, or number of attributes or instances. I ended up choosing a Census Income dataset that had 14 attributes and 48,842 instances. The task listed was a binary classifier to build a model that could determine from census information whether or not the person made more than $50k per year.\n\nEvery dataset in the repository comes with a link to the data folder, which I simply clicked and downloaded to my computer. However, in an effort to make it easier for you to follow along, I\u2019ve included a simple function that uses to fetch the data.\n\nThis code also helps us start to think about how we\u2019re going to manage our data on disk. I\u2019ve created a folder in my current working directory to hold the data as it's downloaded. In the data management section, we'll expand this folder a bit further to be loaded as a object.\n\nThe very first thing to do is to explore the dataset and see what\u2019s inside. The three files that downloaded do not have a file extension, but they are simply text files. You can change the extension to for easier exploration if that helps. By using the and commands on the command line, our files appear to be as follows:\n\nClearly this dataset is intended to be used for machine learning, and a test and training data set has already been constructed. Similar types of split datasets are used for Kaggle competitions and academic conferences. This will save us a step when it comes to evaluation time.\n\nSince we already have a csv file, let\u2019s explore the dataset using Pandas:\n\nBecause the CSV data doesn\u2019t have a header row, I had to supply the names directly to the function. To get these names, I manually constructed the list by reading the file. In the future, we'll store these names as a machine readable JSON file so that we don't have to manuually construct it.\n\nBy glancing at the first 5 rows of the data, we can see that we have primarily categorical data. Our target, is also currently constructed as a categorical field. Unfortunately, with categorical fields, we don't have a lot of visualization options (quite yet). However, it would be interesting to see the frequencies of each class, relative to the target of our classifier. To do this, we can use Seaborn's function to count the occurrences of each data point. Let's take a look at the counts of and \u2014 two likely predictors of income in the census data:\n\nThe function accepts either an or a argument to specify if this is a bar plot or a column plot. I chose to use the argument so that the labels were readable. The argument specifies a column for comparison; in this case we're concerned with the relationship of our categorical variables to the target income. Go ahead and explore other variables in the dataset, for example and to see if those values are predictive of the level of income or not!\n\nNow that we\u2019ve completed some initial investigation and have started to identify the possible feautures available in our dataset, we need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. My proposal is to use the object to load the data into and attributes respectively, similar to how Scikit-Learn's toy datasets are structured. Using this object to manage our data will mirror the native API and allow us to easily copy and paste code that demonstrates classifiers and technqiues with the built in datasets. Importantly, this API will also allow us to communicate to other developers and our future-selves exactly how to use the data.\n\nIn order to organize our data on disk, we\u2019ll need to add the following files:\n\nI constructed a pretty simple in Markdown that gave the title of the dataset, the link to the UCI Machine Learning Repository page that contained the dataset, as well as a citation to the author. I simply wrote this file directly using my own text editor.\n\nThe file, however, we can write using the data frame that we already have. We've already done the manual work of writing the column names into a variable earlier, there's no point in letting that go to waste!\n\nThis code creates a file by inspecting the data frame that we have constructued. The column, is just the two unique values in the series; by using the method - we're guarenteed to spot data errors if there are more or less than two values. The is simply the names of all the columns.\n\nThen we get tricky \u2014 we want to store the possible values of each categorical field for lookup later, but how do we know which columns are categorical and which are not? Luckily, Pandas has already done an analysis for us, and has stored the column data type, , as either or . Here I am using a dictionary comprehension to create a dictionary whose keys are the categorical columns, determined by checking the object type and comparing with , and whose values are a list of unique values for that field.\n\nNow that we have everything we need stored on disk, we can create a function, which will allow us to load the training and test datasets appropriately from disk and store them in a :\n\nThe primary work of the function is to locate the appropriate files on disk, given a root directory that's passed in as an argument (if you saved your data in a different directory, you can modify the root to have it look in the right place). The meta data is included with the bunch, and is also used split the train and test datasets into and variables appropriately, such that we can pass them correctly to the Scikit-Learn and estimator methods.\n\nNow that our data management workflow is structured a bit more like Scikit-Learn, we can start to use our data to fit models. Unfortunately, the categorical values themselves are not useful for machine learning; we need a single instance table that contains numeric values. In order to extract this from the dataset, we\u2019ll have to use Scikit-Learn transformers to transform our input dataset into something that can be fit to a model. In particular, we\u2019ll have to do the following:\n\nWe will explore how to apply these transformations to our dataset, then we will create a feature extraction pipeline that we can use to build a model from the raw input data. This pipeline will apply both the imputer and the label encoders directly in front of our classifier, so that we can ensure that features are extracted appropriately in both the training and test datasets.\n\nOur first step is to get our data out of the object data type land and into a numeric type, since nearly all operations we\u2019d like to apply to our data are going to rely on numeric types. Luckily, Sckit-Learn does provide a transformer for converting categorical labels into numeric integers: . Unfortunately it can only transform a single vector at a time, so we'll have to adapt it in order to apply it to multiple columns.\n\nLike all Scikit-Learn transformers, the has and methods (as well as a special all-in-one, method) that can be used for stateful transformation of a dataset. In the case of the , the method discovers all unique elements in the given vector, orders them lexicographically, and assigns them an integer value. These values are actually the indices of the elements inside the attribute, which can also be used to do a reverse lookup of the class name from the integer value.\n\nFor example, if we were to encode the column of our dataset as follows:\n\nWe can then transform a single vector into a numeric vector as follows:\n\nObviously this is very useful for a single column, and in fact the really was intended to encode the target variable, not necessarily categorical data expected by the classifiers.\n\nNote: Unfortunately, it was at this point that I realized the values all had a space in front of them. I\u2019ll address what I might have done about this in the conclusion.\n\nIn order to create a multicolumn LabelEncoder, we\u2019ll have to extend the in Scikit-Learn to create a transformer class of our own, then provide and methods that wrap individual for our columns. My code, inspired by the StackOverflow post \u201cLabel encoding across multiple columns in scikit-learn\u201d, is as follows:\n\nThis specialized transformer now has the ability to label encode multiple columns in a data frame, saving information about the state of the encoders. It would be trivial to add an method that accepts numeric data and converts it to labels, using the method of each individual on a per-column basis.\n\nAccording to the file, unknown values are given via the string. We'll have to either ignore rows that contain a or impute their value to the row. Scikit-Learn provides a transformer for dealing with missing values at either the column level or at the row level in the library called the Imputer.\n\nThe requires information about what missing values are, either an integer or the string, for data types, it then requires a strategy for dealing with it. For example, the can fill in the missing values with the mean, median, or most frequent values for each column. If provided an axis argument of 0 then columns that contain only missing data are discarded; if provided an axis argument of 1, then rows which contain only missing values raise an exception. Basic usage of the is as follows:\n\nUnfortunately, this would not work for our label encoded data, because 0 is an acceptable label \u2014 unless we could guarentee that 0 was always , then this would break our numeric columns that already had zeros in them. This is certainly a challenging problem, and unfortunately the best we can do, is to once again create a custom Imputer.\n\nOur custom imputer, like the transformer takes a set of columns to perform imputation on. In this case we only wrap a single as the is multicolumn \u2014 all that's required is to ensure that the correct columns are transformed. I inspected the encoders and found only three columns that had missing values in them, and passed them directly into the customer imputer.\n\nI had chosen to do the label encoding first, assuming that because the required numeric values, I'd be able to do the parsing in advance. However, after requiring a custom imputer, I'd say that it's probably best to deal with the missing values early, when they're still a specific value, rather than take a chance.\n\nNow that we\u2019ve finally acheived our feature extraction, we can continue on to the model build phase. To create our classifier, we\u2019re going to create a that uses our feature transformers and ends in an estimator that can do classification. We can then write the entire pipeline object to disk with the , allowing us to load it up and use it to make predictions in the future.\n\nA pipeline is a step-by-step set of transformers that takes input data and transforms it, until finally passing it to an estimator at the end. Pipelines can be constructed using a named declarative syntax so that they\u2019re easy to modify and develop. Our pipeline is as follows:\n\nThe pipeline first passes data through our encoder, then to the imputer, and finally to our classifier. In this case, I have chosen a , a regularized linear model that is used to estimate a categorical dependent variable, much like the binary target we have in this case. We can then evaluate the model on the test data set using the same exact pipeline.\n\nAs part of the process in encoding the target for the test data, I discovered that the classes in the test data set had a appended to the end of the class name, which I had to strip in order for the encoder to work! However, once done, I could predict the y values using the test dataset, passing the predicted and true values to the classifier report.\n\nThe classifier I built does an ok job, with an F1 score of 0.77, nothing to sneer at. However, it is possible that an SVM, a Naive Bayes, or a k nearest neighbor model would do better. It is easy to construct new models using the pipeline approach that we prepared before, and I would encourage you to try it out! Furthermore, a grid search or feature analysis may lead to a higher scoring model than the one we quickly put together. Luckily, now that we\u2019ve sorted out all the pipeline issues, we can get to work on inspecting and improving the model!\n\nThe last step is to save our model to disk for reuse later, with the module:\n\nYou should also dump meta information about the date and time your model was built, who built the model, etc. But we\u2019ll skip that step here, since this post serves as a guide.\n\nNow it\u2019s time to explore how to use the model. To do this, we\u2019ll create a simple function that gathers input from the user on the command line, and returns a prediction with the classifier model. Moreover, this function will load the pickled model into memory to ensure the latest and greatest saved model is what\u2019s being used.\n\nThe hardest part about operationalizing the model is collecting user input. Obviously in a bigger application this could be handled with forms, automatic data gathering, and other advanced techniques. For now, hopefully this is enough to highlight how you might use the model in practice to make predictions on unknown data.\n\nThis walkthrough was an end-to-end look at how I performed a classification analysis of a dataset that I downloaded from the Internet. I tried to stay true to my exact workflow so that you could get a sense for how I had to go about doing things with little to no advanced knowledge. As a result, there are definitely some things I might change if I was going to do this over.\n\nOne place that I struggled with was trying to decide if I should write out wrangled data back to disk, then load it again, or if I should maintain a feature extraction of the raw data. I kept going back and forth, particularly because of silly things like the spaces in front of the values. This could be fixed by loading the data as follows:\n\nUsing a regular expression for the seperator that would automatically strip whitespace. However, I\u2019d already gone too far to make these changes!\n\nI also had problems with the ordering of the label encoding and the imputation. Given another chance, I think I would definitely wrangle and clean both datasets and save them back to disk. Even just little things like the \u201c.\u201d at the end of the class names in the test set were annoyances that could have been easily dealt with.\n\nNow that you\u2019ve had a chance to look at my walkthrough, I hope you\u2019ll try a few on your own and send your workflows and analyses to us so that we can post them as well!"
    },
    {
        "url": "https://medium.com/district-data-labs/graph-analytics-over-relational-datasets-with-python-89fb14587f07",
        "title": "Graph Analytics Over Relational Datasets with Python",
        "text": "The analysis of interconnection structures of entities connected through relationships has proven to be of immense value in understanding the inner-workings of networks in a variety of different data domains including finance, health care, business, computer science, etc. These analyses have emerged in the form of Graph Analytics \u2014 the analysis of the characteristics in these graph structures through various graph algorithms. Some examples of insights offered by graph analytics include finding clusters of entities closely connected to each-other, calculating optimal paths between entities (the definition of optimal depending on the dataset and use case), understanding the hierarchy of entities within an organization as well as figuring out the impact each entity has inside the network.\n\nGraph structured data is a specialized type of dataset in terms of the way we need to access it; therefore it needs to be stored in ways that complements these access patterns. This has sparked the emergence of a wide variety of specialized graph databases such as Neo4j, OrientDB, Titan etc. These systems are highly optimized specifically for efficient graph storage, traversal, and analysis.\n\nDespite the existence of these graph database management systems however, users do not typically store their data in a graph database unless they are strictly dealing with graph workloads. The reason for this is that while graph structured data may complement graph analysis, it ultimately undermines traditional relational analytics through the use of declarative languages like SQL that have been heavily leveraged by companies and organizations for decades. Users thus typically store their data in Relational Database Management System that backed by many years of research and development, are highly robust and optimized for these generalized analyses and a wide variety of different types of queries.\n\nSo how do we get the best of both worlds?\n\nGraphGen (currently under development at the University of Maryland), is a system that enables users to efficiently conduct graph analytics on many different types of graphs that exist inside of relational datasets. The main idea behind GraphGen is to enable users to effortlessly leverage graph analytics on top of their data, without the need to go through a labor-intensive and time-consuming Extract, Transform and Load (ETL) process. It allows users to declaratively express any set of entities and relationships between them that exists in the underlying dataset, and extract it into memory where it can be analyzed \u2014 all this through a simple language abstraction! For more information on the project please visit the GraphGen project webpage\n\nIn this blog post, we will go through the entire process of writing an extraction query, to extracting and analyzing the extracted graph from within a relational database. GraphGen is natively written in and includes many native optimizations for efficient extraction and memory usage, but for the purposes of this post we will use , a Python wrapper over the GraphGen system, that enables quickly extracting and serializing the extracted graph to disk instead. We will demonstrate an end-to-end analysis on the openfootball database, restricted to the 2014 World Cup, from extraction to analysis of a graph between players. You can download the pre-built version of the sqlite database for the 2014 World Cup that we'll be using in this tutorial from here: .\n\nTo install onto your system, simply download and uncompress the packaged zip file linked in the GraphGen webpage, into your workspace, and enter the folder.\n\nIf you\u2019re using a virtual environment ( ) -- which we highly recommend -- then you can simply\n\nNote: You may need to use or Administrator privileges if you're installing to your global site-packges.\n\nIf you\u2019d prefer to try out in your local workspace without having to install it simply first install the requirements using\n\nand then export your to include the folder in your local workspace\n\nAfter that you can immediately to begin exploring graphs in your relational datasets.\n\nRelational datasets, as their name suggests, often include a wide variety of underlying entities and relationships between them. One of the biggest strong points of GraphGen is that it enables users to explore any type of graph that may exist within the dataset, be it graphs with different types of entities (nodes), or different ways in which they are connected to each-other (edges).\n\nFor the sake of this example, we will extract a graph of players where there is an edge between them if they\u2019ve played against each-other.\n\nFirst, we need to inspect the database schema, in order to find which relations (tables) we need to utilize in order to extract this information. A subset of the schema can be seen below\n\nAs we can see here, the tables that hold the information about the ways we want the player nodes to be connected in this graph are included in the , and tables. The table holds the data for each player, the table tells us which team each player is a part of, and the table holds all the data about each game that occurred; including which teams played against each-other.\n\nGraphGen uses a custom written declarative language (that gets translated to SQL queries under the hood). The query that extracts the graph we want in this language will look like this:\n\nThe intuition behind the above query is that we are declaring how we want the nodes and edges in this graph to be defined in terms of the tables in this database. Here, we are declaring that we want the nodes to have two properties, an and a , which should be taken from the table. We are also stating that we want node with and node with , to be connected to each other, if there is a game in the table, where id1's team has played against id2's team. For a more in-depth analysis on the language and on writing queries for GraphGen, please refer to the GraphGen Project webpage\n\nThe python code that allows us to do this is quite simple\n\nNote 1: By definition, this query will include all individuals in the \u201cpersons\u201d table in the database, even if they did not participate in any of the games in the table.\n\nNote 2: Currently, the language is limited to only extracting undirected graphs with no edge properties.\n\nWe have now extracted the graph we wanted in a matter of seconds, and serialized it onto disk in a standard graph format \u2014 all without the need to write a single SQL query or script for wrangling the data into a graph format.\n\nMost importantly however, the capabilities of the above language, allow us to essentially view this relational dataset from any angle, and explore all the types of graphs, that we may think of, and find interesting within.\n\nNow we can go ahead and conduct analysis on the extracted graph.\n\nAs mentioned before, there exist a wide diversity of graph databases, and graph analytics engines that are optimized for graph traversal and analysis, and we are now in the position to import our extracted graph and utilize any of them. For the purposes of this post, we will use a widely adopted graph analysis library written and distributed in Python, known as NetworkX.\n\nNetworkX is a very rich library that implements a wide array of graph algorithms for many different types of analyses including clustering, communities, centrality, distance measures and many more. It also parses several different types of standard graph formats; for our purposes we will use the format as we extracted in the previous section's code.\n\nHere\u2019s how we import the graph into a Graph object:\n\nOne classic type of analysis on any graph is to identify the key or most \u201cimportant\u201d entities in the graph. To do this, we will run the PageRank algorithm on top of the graph. This will assign a score to each node based on the structure of the incoming edges \u2014 we can then find the node with the highest PageRank score.\n\nAnother interesting aspect that we may want to analyze is finding the key intermediary nodes in terms of how information may flow inside the graph, commonly known as Betweenness Centrality. Vertices with high betweenness centrality, means that they have a large influence in the connectivity of their neighbors with the other nodes in the graph. The code here is identical to the way we ran PageRank\n\nAnother common analysis is how many \u201ctriangles\u201d does each node in the graph participate in if any. A triangle is a subgraph that includes exactly 3 nodes connected via exactly 3 edges (or a complete subgraph with 3 nodes). Triangle counting is used in a wide variety of graph mining and analysis algorithms, and can be done using .\n\nIt\u2019s interesting to see that in this specific graph, there actually isn\u2019t a single triangle; every node in the graph is part of exactly 0 triangles.\n\nIn many cases, visual analysis of a graph can also provide great value, as it allows for including the human factor into the mix, which is essential in pinpointing various patterns in the graph structure. Visualizations also allow for collaborative discussions on the outcomes of the analysis and allows for sharing and easily communicating results and ideas to others.\n\nThe simplest way to visualize a graph extracted using graphgen would be using itself. The library provides a variety of ways to easily draw the input graph using . It also provides a wide array of graph layouts for laying out the nodes and edges in non-overlapping ways adequate for enabling maximal visibility of the information in the graph, and simplifying visual analysis.\n\nAn example of the code we would need to write for drawing the graph we extracted in the previous sections is:\n\nAnother very effective way to visualize graphs in a standard format is by using a third party tool called Gephi. Gephi provides a great Graphical User Interface (GUI) for manually applying different layouts, node sizes, colors and various techniques for drawing edges. Note that most of the things Gephi does, are also possible through but would require significant amounts of complex code to accomplish.\n\nBelow is an example of using Gephi to load in the extracted file and changing the shade of color of each node depending on its degree. The layout algorithm used here is called Yifan Hu.\n\nGephi makes it effortless to re-size labels, choose and re-apply different layouts and play around with different node and edge sizes and colors. Below is an example of the same graph, but using the Fruchterman\u2013Reingold algorithm and including the names of the players in small font.\n\nGephi produces vector graphics so it naturally allows for zooming into details you\u2019d want to explore further and allows for manually clicking and dragging nodes of special interest to specific positions; something significantly more complex to do using and .\n\nAnalysis of graph structured data has proven its worth time and again, being able to provide invaluable insights about the relationships between entities, as well as enable optimizations over a network of interconnected objects. Graph analytics however is but one direction in which we\u2019d like to leverage the information in our datasets, and typically do not want to center our entire data collection workflow around graph analysis. A majority of users and businesses rely on the robust and mature features and analytical capabilities provided by relational databases. GraphGen brings the best for both worlds, by enabling effortless extraction of many different types of graphs that exist inside of a relational dataset, thus opening graph analytics up to any dataset that exists inside a SQL-powered database engine."
    },
    {
        "url": "https://medium.com/district-data-labs/a-practical-guide-to-anonymizing-datasets-with-python-faker-ecf15114c9be",
        "title": "A Practical Guide to Anonymizing Datasets with Python & Faker",
        "text": "In order to learn (or teach) data science you need data (surprise!). The best libraries often come with a toy dataset to illustrate examples of how the code works. However, nothing can replace an actual, non-trivial dataset for a tutorial or lesson, because only that can provide for deep and meaningful exploration. Unfortunately, non-trivial datasets can be hard to find for a few reasons, one of which is that many contain personally identifying information (PII).\n\nA possible solution to dealing with PII is to anonymize the dataset by replacing information that would identify a real individual with information about a fake (but similarly behaving or sounding) individual. Unfortunately, this is not as easy at it sounds. A simple mapping of real data to randomized data is not enough, because in order to be used as a stand in for analytical purposes, anonymization must preserve the semantics of the original data. As a result, issues related to entity resolution, like managing duplicates or producing linkable results, frequently come into play.\n\nThe good news is that we can take a cue from the database community, who routinely generate simulated data to evaluate the performance of a database system. This community has developed plenty of tools for generating very realistic data for a variety of information types. For this post, I\u2019ll explore using the Faker library to generate a realistic, anonymized dataset that can be utilized for downstream analysis.\n\nThe goal: given a target dataset (for example, a CSV file with multiple columns), produce a new dataset such that for each row in the target, the anonymized dataset does not contain any personally identifying information. The anonymized dataset should have the same amount of data and maintain its analytical value. As shown in the figure below, one possible transformation simply maps original information to fake and therefore anonymous information but maintains the same overall structure.\n\nThis post will study a simple example that requires the anonymization of only two fields: full name and email. Sounds easy, right? The difficulty is in preserving the semantic relationships and distributions in our target dataset so that we can hand it off to be analyzed or mined for interesting patterns. What happens if there are multiple rows per user? Since CSV data is naturally denormalized (e.g. contains redundant data like rows with repeated full names and emails), we will need to maintain a mapping of profile information.\n\nNote: Since we\u2019re going to be using Python 2.7 in this example, you\u2019ll need to install the module with . Additionally you'll need the Faker library:\n\nThe following example shows a simple function that maintains this mapping and also shows how to generate data with Faker. We'll also go a step further by reading the data from a source CSV file and writing the anonymized data to a target CSV file. The end result is a file very similar in terms of length, row order, and fields, except that the names and emails have been replaced with fake names and emails.\n\nThe entry point for this code is the function itself. It takes as input the path to two files: the , where the original data is held in CSV form and , a path to write out the anonymized data to. Both of these paths are opened for reading and writing respectively. The module is used to read and parse each row, transforming them into Python dictionaries. Those dictionaries are passed into the function, which transforms and each row to be written by the CSV writer to disk.\n\nThe function takes any iterable of dictionaries which contain and keys. It loads the fake factory using \u2014 a class function that loads various providers with methods that generate fake data (more on this later). We then create two instances to map real names to fake names and real emails to fake emails.\n\nThe Python module provides , which is similar to a regular except that if the key does not exist in the dictionary, a default value is supplied by the callable passed in at instantiation. For example, would provide a default value of 0 for every key not already in the dictionary. Therefore when we use we're saying that for every key not in the dictionary, create a fake name (and similar for email). This allows us to generate a mapping of real data to fake data, and to make sure that the real value always maps to the same fake value.\n\nFrom there, we simply iterate through all the rows, replacing data as necessary. If our target CSV file looked like this (imagine clickstream data from an email marketing campaign):\n\n\u2026it would be transformed into something like:\n\nWe now have a new wrangling tool in our toolbox that will allow us to transform CSVs with name and email fields into anonymized datasets! This naturally leads us to the question: what else can we anonymize?\n\nThere are two third-party libraries for generating fake data with Python that come up on Google search results: Faker by @deepthawtz and Fake Factory by @joke2k, which is also called \u201cFaker\u201d. Faker provides anonymization for user profile data, which is completely generated on a per-instance basis. Fake Factory (used in the example above) uses a providers approach to load many different fake data generators in multiple languages. I typically prefer Fake Factory over Faker because it has multiple language support and a wider array of fake data generators. Next we\u2019ll explore Fake Factory in detail (for the rest of this post, when I refer to Faker, I\u2019m referring to Fake Factory).\n\nThe primary interface that Faker provides is called a . Generators are a collection of instances which are responsible for formatting random data for a particular domain. Generators also provide a wrapper around the module, and allow you to set the random seed and other operations. While you could theoretically instantiate your own Generator with your own providers, Faker provides a to automatically load all the providers on your behalf:\n\nIf you inspect the object, you'll see around 158 methods (at the time of this writing), all of which generate fake data. Try the , , and methods to name a few, just to get a sense of the variety of generators that exist.\n\nImportantly, providers can also be localized using a language code. This is probably the best reason to use the object \u2014 to ensure that localized providers, or subsets of providers, are loaded correctly. For example, to load the French localization:\n\nAnd for fun, some Chinese:\n\nThe Faker library has the most comprehensive set of data generators I\u2019ve ever encountered for a variety of domains. Unfortunately there is no single provider listing; the best way to explore all the providers in detail is simply to look at the providers package on GitHub.\n\nAlthough the Faker library has a comprehensive array of providers, occasionally you need a domain specific fake data generator. In order to add a custom provider, you will need to subclass the and expose custom faker methods as class methods using the decorator. One very easy approach is to create a set of random data you'd like to expose, and simply randomly select it:\n\nIn order to change the likelihood or distribution with which oceans are selected, simply add duplicates to the list so that each name has the probability of selection that you'd like. Then add your provider to the object:\n\nIn routine data wrangling operations, you can create a package structure with localization similar to Faker\u2019s and load things on demand. Don\u2019t forget \u2014 if you come up with a generic provider that may be useful to many people, submit it back as a pull request!\n\nNow that we understand the wide variety of fake data we can generate, let\u2019s get back to our original example of creating user profile data with just name and email address. First, if you look at the results in the Anonymizing section above, we can make a few observations:\n\nBasically we want to improve our user profile to include email addresses that are similar to the names (or a non-name based username), and we want to ensure that the domains are a bit more realistic for work addresses. We also want to include aliases, nicknames, or different versions of the name. Faker does include a profile provider:\n\nBut as you can see, it suffers from the same problem. In this section, we\u2019ll explore different techniques that allow us to modify our fake data generation such that it matches the distributions we\u2019re seeing in the original data set. In particular we\u2019ll deal with the domain, create more realistic fake profiles, and add duplicates to our data set with fuzzy matching.\n\nOne idea to maintain the distribution of domains is to do a first pass over the data and create a mapping of real domain to fake domain. Moreover, many domains like gmail.com can be whitelisted and mapped directly to themselves (we just need a fake username). Additionally, we can also preserve capitalization and spelling via this method, e.g. \u201cGmail.com\u201d and \u201cGMAIL.com\u201d which might be important for data sets that have been entered by hand.\n\nIn order to create the domain mapping/whitelist, we\u2019ll need to create an object that can load a whitelist from disk, or generate one from our original dataset. For example:\n\nThat\u2019s quite a lot of code all at once, so let\u2019s break it down a bit. First, the class extends which is an abstract base class (ABC) in the module. The ABC gives us the ability to make this class act just like a object. All we have to do is provide , , , and methods, and all other dictionary methods like , or will work on our behalf. Here, we're just wrapping an inner dictionary called .\n\nThe thing to note about our method is that it acts very similar to a \u2014 that is, if you try to fetch a key that is not in the mapping, it generates fake data on your behalf. This way, any domains that we don't have in our whitelist or mapping will automatically be anonymized.\n\nNext, we want to be able to and this data to a JSON file on disk, that way we can maintain our mapping between anonymization runs. The method is fairly straightforward; it just takes an open file-like object, parses it using the module, instantiates the domain mapping, and returns it. The method is a bit more complex. It has to break down the whitelist and mapping into separate objects, so that we can easily modify the data on disk if needed. Together, these methods will allow us to load and save our mapping into a JSON file that will look similar to:\n\nThe final method of note is the method. The generate method allows you to do a first pass through a list of emails, count the frequency of the domains, then propose domains to the user in order of frequency to decide whether or not to add it to the whitelist. For each domain in the emails, the user is prompted as follows:\n\nNote that the prompt includes a progress indicator (this is prompt 1 of 245) as well as a method to quit early. This is especially important for large datasets that have a lot of unique domains; if you quit, the domains will still be faked, and the user only sees the most frequent examples for whitelisting. The idea behind this mechanism is to read through your CSV once, generate the whitelist, then save it to disk so that you can use it for anonymization on a routine basis. Moreover, you can modify domains in the JSON file to better match any semantics you might have (e.g. such as including .edu or .gov domains, which are not generated by the internet provider in Faker).\n\nTo create realistic profiles, we\u2019ll create a provider that uses the domain map from above and generates fake data for every combination we see in the dataset. This provider will also provide opportunities for mapping multiple names and email addresses to a single profile, so that we can use the profile for creating fuzzy duplicates in the next section. Here is the code:\n\nAgain, this is a lot of code, make sure you go through it carefully to understand what is happening. First off, a profile in this case is the combination of mapping names to fake names and emails to fake emails. The key is that the names and emails are related to the original data somehow. Here the relationship is through case such that \u201cDANIEL WEBSTER\u201d is faked to \u201cJAKOB WILCOTT\u201d instead of to \u201cJakob Wilcott\u201d. Additionally through our domain mapping, we also maintain the relationship of the original email domain to the fake domain mapping, e.g. everyone with the email domain \u201c@districtdatalabs.com\u201d will be mapped to the same fake domain.\n\nIn order to maintain the relationship of names to emails (which is very common), we need to be able to access the name more directly. We have a name parts generator which generates fake first, middle, and last names. We then randomly generate names of the form \u201cfirst last\u201d, \u201cfirst middle last\u201d, or \u201cfirst i. last\u201d. The email can take a variety of forms based on the name parts as well. Now we get slightly more realistic profiles:\n\nImportantly this profile object makes it easy to map multiple names and emails to the same profile object to create \u201cfuzzy\u201d profiles and duplicates in your dataset. We will discuss how to perform fuzzy matching in the next section.\n\nIf you noticed in our original dataset we had a clear entity duplication: same email, but different names. In fact, the second name was simply the first initial and last name, but you can imagine other duplication scenarios like nicknames (\u201cBill\u201d instead of \u201cWilliam\u201d), or having both work and personal emails in the dataset. The fuzzy profile objects we generated in the last section allow us to maintain a mapping of all name parts to generated fake names, but we need some way to be able to detect duplicates and combine their profile: enter the module.\n\nSimilarly to our domain mapping approach, we\u2019re going to pass through the entire dataset and look for similar name-email pairs to propose to the user. If the user thinks they\u2019re duplicates, then we\u2019ll merge them together into a single profile, and use the mappings as we anonymize. This is also something you can save to disk and load on demand for multiple anonymization passes and to include user-based edits.\n\nThe first step is to get pairs and eliminate exact duplicates. To do this we\u2019ll create a hashable data structure for our profiles using a .\n\nThe is an immutable data structure that is compact, efficient, and allows us to access properties by name. Because it is immutable, it is also hashable (unlike mutable dictionaries), meaning we can use it for keys in sets and dictionaries. This is important, because the first thing our function does is eliminate exact matches by creating a set of tuples. We then use the function in to generate every pair without replacement.\n\nThe next step is to figure out how similar each pair is. To do this we\u2019ll use the library to come up with a partial ratio score: the mean of the similarity of the names and the emails for each pair:\n\nThe score will be between 0 (no similarity) and 100 (exact match), though hopefully you won\u2019t get any scores of 100 since we eliminated exact matches above. For example:\n\nThe fuzzing process will go through our entire dataset, create pairs of people, and compute their similarity score. We can then filter out all pairs with scores below a certain threshold (say, 50) and propose the results to the user to decide if they\u2019re duplicates in descending score order. When a duplicate is found, we can merge the profile object to map the new names and emails together.\n\nAnonymization of datasets is a critical method to promote the exploration and practice of data science through open data. Fake data generators that already exist give us the opportunity to ensure that private data is obfuscated. The issue becomes how to leverage these fake data generators while still maintaining and preserving a high quality dataset with semantic relations for further analysis. As we\u2019ve seen throughout the post, even the anonymization of just two common fields like name and email can lead to potential problems.\n\nThis problem, and the code in this post are associated with a real case study. For District Data Labs\u2019 Entity Resolution Research Lab, I wanted to create a dataset that removed PII of members while maintaining duplicates and structure to study entity resolution. The source dataset was 1,343 records in CSV form and contained names and emails that I wanted to anonymize.\n\nUsing the strategy I described for domain name mapping, the dataset contained 245 distinct domain names, 185 of which appeared only once. There was a definite long tail, as the first 20 or so most frequent domains were the majority of the records. Once I generated the whitelist as described above, I manually edited the mappings to ensure that there were no duplicates and that major work domains were sufficiently \u201cprofessional.\u201d\n\nUsing the fuzzy matching process was also a bear. It took, on average, 28 seconds to compute the pairwise scores. Using a threshold score of 50, I was proposed 5,110 duplicates (out of a possible 901,153 combinations). I went through 354 entries (until the score was below 65) and was satisfied that I had covered many of the duplicates in the dataset.\n\nThe resulting anonymized dataset was of a high quality and obfuscated personally identifying information like name and email. Of course, you could reverse some of the information in the dataset. For example, I\u2019m listed in the dataset, and one of the records indicates a relationship between a fake user and a blog post, which I\u2019m on record as having written. However, even though you can figure out who I am and what else I\u2019ve done through the dataset, you wouldn\u2019t be able to use it to extract my email address, which was the goal.\n\nIn the end, even though anonymizing data requires a lot of data wrangling effort and considered thought, the benefits of open data are invaluable. Only by sharing data, resources, and tools can use many eyes to provide multiple insights and to drive the field of data science forward.\n\nI would like to thank Michal Haskell and Rebecca Bilbro for their help editing and preparing this post. This discussion was a challenge, and they cleaned up my bleary eyed writing to make the article readable. A special thank you to Rebecca Bilbro as well for drawing the figure used to describe the anonymization process.\n\n1.Anonymize: remove identifying particulars from (test results) for statistical or other purposes.\n\n2.Entity Resolution: tools or techniques that identify, group, and link digital mentions or manifestations of some object in the real world.\n\n3.DDL Research Labs is an applied research program intended to develop novel, innovative data science solutions towards practical applications."
    },
    {
        "url": "https://medium.com/district-data-labs/an-introduction-to-machine-learning-with-python-92bcaf651ac4",
        "title": "An Introduction to Machine Learning with Python \u2013 District Data Labs \u2013",
        "text": "The impulse to ingest more data is our first and most powerful instinct. Born with billions of neurons, as babies we begin developing complex synaptic networks by taking in massive amounts of data \u2014 sounds, smells, tastes, textures, pictures. It\u2019s not always graceful, but it is an effective way to learn.\n\nAs data scientists, the trick is to encode similar learning instincts into applications, banking more on the volume of data that will flow through the system than on the elegance of the solution (see also these discussions of the Netflix prize and the \u201cunreasonable effectiveness of data\u201d). Building data products \u2014 or products that use data and generate more data in return \u2014 requires the special ingredients of encoded learning and the ability to independently predict outcomes of new situations.\n\nFortunately Python and high level libraries like Scikit-learn, NLTK, PyBrain, Theano, and MLPy have made machine learning a lot more accessible than it would be otherwise. For better or worse, you don\u2019t have to have an academic background in predictive methods to dip your toe into the machine learning waters (that said, we do encourage you to ML responsibly and have provided a list of helpful links and suggested further readings at the end). This post aims to provide a basic introduction to machine learning: what it is, how it works, and how to get started with machine learning in Python using the Scikit-learn API.\n\nMachine learning is, as artificial intelligence pioneer Arthur Samuel said, a way of programming that gives computers \u201cthe ability to learn without being explicitly programmed.\u201d In particular, it is a way to program computers to extract meaningful patterns from examples, and to use those patterns to make decisions or predictions. In practice, this amounts to, first, creating a model with tunable parameters that can adjust themselves to the available data, then introducing new data and letting the model extrapolate. We also help the model balance between precisely capturing the behavior of known data and generalizing enough to predict the behavior of unknown data.\n\nIn general, a learning problem considers a set of n known samples (we tend to call them instances). Instances are vector representations of things in the real world. In a multidimensional decision space, we call each property of the vector representation an attribute or feature.\n\nMachine learning essentially falls into three categories: supervised, unsupervised, and reinforcementlearning. The appropriate category is determined by the type of data at hand, and depends largely on whether it is labeled or unlabeled.\n\nLabeled data has, in addition to other features, a parameter of interest that can serve as the \u2018correct\u2019 answer. Unlabeled data does not have answers. With labeled data, we use supervised learning methods. If the data does not have labels, we use unsupervised methods. For example, here is a well-known machine learning dataset, the Haberman survival data, a record of cases from a study of the survival of breast cancer surgery patients between 1958 and 1970 at the University of Chicago\u2019s Billings Hospital.\n\nReinforcement learning methods such as swarms and genetic algorithms \u2014 where models interact with the environment for some reward \u2014 comprise a fascinating third category of machine learning, albeit one beyond the scope of this post.\n\nBelow we\u2019ll explore several common techniques for supervised and unsupervised learning.\n\nWe can break supervised learning down further into two subcategories, regression problems and classification problems. As with supervised and unsupervised learning, the decision to use regression or classification is largely dictated by the type of data you have. If the labels in your dataset are continuous (e.g. percentages that indicate the probability of rain or fraud), your machine learning problem likely calls for a regression technique. If the labels are discrete and the predictions should fall into categories (e.g. \u201cmale\u201d or \u201cfemale\u201d, \u201cred\u201d, \u201cgreen\u201d or \u201cblue\u201d), it\u2019s a classification problem.\n\nIn the Haberman dataset shown above there are labels in addition to the three numerical features (the age of the patient at the time of her operation, the two-digit year of the patient\u2019s operation, and the number of positive axillary nodes detected). The labels are categorical \u2014 a \u20181\u2019 indicates that the patient survived for more than five years after undergoing surgery to treat their cancer; a \u20182\u2019 indicates that the patient died within five years of their surgery. So, if the goal is to predict a patient\u2019s five-year post-operative survival based on age, surgery year, and number of nodes, the best approach would be to build a classifier. However, if the labels were continuous (e.g. giving the number of months of survival post operation), the problem might be better suited to regression.\n\nUnsupervised learning is a way of discovering hidden structure in unlabeled data. Recall that with supervised learning, labels are the \u2018correct answers\u2019 that enable the model to adjust its parameters. Unsupervised learning models have no such signal. Instead, unsupervised learning attempts to organize datasets into groups of similar data points, ensuring the groups are meaningfully dissimilar from each other. While an unsupervised learning algorithm is unlikely to render clusters via a simple, elegantly parameterized equation like a human might, they can be strikingly successful at detecting patterns within datasets for which no human would even think to look.\n\nThere are two main approaches to clustering, partitive (or centroidal) and hierarchical, both of which separate data into groups whose members share maximum similarity as defined (typically) by a distance metric. In partitive clustering, clusters are represented by central vectors, distributions, or densities. There are numerous variations of partitive clustering algorithms, but some of the most common techniques include k-means, k-medoids, OPTICS, and affinity propagation. Hierarchical clustering involves creating clusters that have a predetermined ordering from top to bottom, and can be either agglomerative (clusters begin as single instances and iteratively aggregate by similarity until all belong to a single group) or divisive (the dataset is gradually partitioned, beginning with all instances and finishing with single instances).\n\nScikit-learn is one of the extensions of SciPy (Scientific Python) that provides a wide variety of modern machine learning algorithms for classification, regression, clustering, feature extraction, and optimization. It sits atop C libraries, LAPACK, LibSVM, and Cython, and provides extremely fast analysis for small- to medium-sized data sets. It is open source, commercially usable and is probably the best generalized machine learning framework currently available. Some of Scikit-learn\u2019s features include: cross-validation, transformers, pipelining, grid search, model evaluation, generalized linear models, support vector machines, Bayes, decision trees, ensembles, clustering and density algorithms, and best of all, a standard Python API. For this reason, Scikit-learn is often one of the first tools in a data scientist\u2019s toolkit.\n\nNext we\u2019ll explore some regression, classification, and clustering models, but first install using :\n\nThe Scikit-learn API is an object-oriented interface centered around the concept of an \u2014 broadly any object that can learn from data, be it a classification, regression or clustering algorithm, or a transformer that extracts useful features from raw data. Each estimator in Scikit-learn has a fit and a predict method.\n\nThe method sets the state of the estimator based on the training data. Usually, the data is comprised of a two-dimensional numpy array X of shape (nsamples, npredictors) that holds the feature matrix, and a one-dimensional numpy array y that holds the labels. Most estimators allow the user to control the fitting behavior, setting parameters when the estimator is instantiated or modifying them later.\n\ngenerates predictions: predicted regression values in the case of regression, or the corresponding class labels in the case of classification. Classifiers that can predict the probability of class membership have a method that returns a two-dimensional numpy array of shape (nsamples, nclasses) where the classes are lexicographically ordered.\n\nA is a special type of estimator that transforms input data by selecting a subset of the available features or extracting new features based on the original ones. Transformers can be used to normalize or scale features, or to impute missing values.\n\nScikit-learn also comes with a few datasets that can demonstrate the properties of classification and regression algorithms, as well as how the data should fit. The datasets module also contains functions for loading data from the mldata.org repository as well as for generating random data. Coupled with the API, which allows you to rapidly deploy any number of models, these small toy datasets are helpful for getting started with machine learning.\n\nRegressions are a type of supervised learning algorithm where, given continuous input features, the object is to predict the continuous target values.\n\nGet started by loading some practice datasets from the Scikit-learn repository, on glucose and insulin levels of diabetes patients and median home values in Boston:\n\nBecause they have continuous labels, both of these datasets lend themselves to regression. As we explore each of the predictive models below, we should be asking ourselves which performs the best for each of the two sample datasets. To help with our evaluation, let\u2019s import some handy built-in tools from Scikit-learn; mean squared error, which indicates a regression model\u2019s error in terms of precision (variance) and accuracy (bias), and coefficient of determination or R2 score (a ratio of explained variance to total variance), which illustrates how well the prediction fits the data:\n\nA simple linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observations and the predictions.\n\nNow we evaluate the fit of the model:\n\nTry the same linear model with the Boston housing prices data:\n\nRegularization methods penalize the complexity of a model to limit overfitting and help with generalization. For example, ridge regression, also known as Tikhonov regularization, penalizes a least squares regression model by shrinking the value of the regression coefficients. Compared to a standard linear regression, the slope will tend to be more stable and the variance smaller.\n\nTry the same regularization method with the Boston housing prices data:\n\nRandom forest is an ensemble method that creates a number of decision trees using the CART algorithm, each on a different subset of the data. The general approach to creating the ensemble is bootstrap aggregation of the decision trees (also known as \u2018bagging\u2019).\n\nAnd for the Boston dataset:\n\nGiven labeled input data (with two or more possible labels), classification aims to fit a function that can predict the discrete class of new input.\n\nFor our exploration of a few of the classification methods available in Scikit-learn, let\u2019s pick a new dataset to work with. Once you\u2019ve exhausted the toy datasets available through the Scikit-learn API, the next place to explore is the machine learning repository maintained by the University of California, Irvine. For the following few examples, we\u2019ll be using the Haberman survival dataset we explored at the beginning of the post.\n\nIn supervised machine learning, data are divided into training and test sets. But what if certain chunks of the data have more variance than others? It\u2019s important to get into the habit of using cross validation (we like to use 12 folds) to ensure that your models perform just as well regardless of the particular way the data are divided up.\n\nTo do this in , we'll import :\n\nAs we explore different classifiers, we\u2019ll again be asking which model performs the best with our dataset. To help us evaluate, let\u2019s import .\n\nThis will give us the precision, accuracy and recall scores for each classifier. Precision is the number of correct positive results divided by the number of all positive results. Recall is the number of correct positive results divided by the number of positive results that should have been returned. The F1 scoreis a measure of a test\u2019s accuracy. It considers both the precision and the recall of the test to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.\n\nA logistic regression mathematically calculates the decision boundary between the possibilities. It looks for a straight line that represents a cutoff that most accurately represents the training data.\n\nSupport vector machines (SVMs) use points in transformed problem space that separate the classes into groups. For data that is not linearly separable, mapping to a higher dimensional feature space by computing the kernels can render the decision space more straightforward (this is called the \u201ckernel trick\u201d). Support vector machine models are versatile because they can be parameterized with a variety of different kernel functions including linear, polynomial, sigmoid, and radial basis.\n\nWe used a Random Forest regressor in the previous section on Regression, but this ensemble method can also be used in classification. In fact, many supervised models in Scikit-learn have both a regressor and a classifier version.\n\nClustering algorithms attempt to find patterns in unlabeled data. They are usually grouped into two main categories: centroidal (to find the centers of clusters) and hierarchical (to find clusters of clusters).\n\nIn order to explore clustering, let\u2019s pull another dataset from the UCI repository, this one on grocery store customer spending behavior:\n\nK-Means Clustering partitions N samples into k clusters, where each sample belongs to a cluster to which it has the closest mean of its neighbors. This problem is \u201cNP-hard\u201d, but there are good estimations.\n\nThe black dots are our data points and the x\u2019s represent the centers of the clusters that were identified by our model:\n\nScikit-learn includes a lot of useful clustering models, which, as you experiment, you\u2019ll find perform better and worse depending on the number of clusters, density of the data, dimensionality, and desired distance metric.\n\nWhen we talk about data science and the data science pipeline, we are typically talking about the management of data flows for a specific purpose \u2014 the modeling of some hypothesis. Those models we construct can then be used in data products as an engine to create useful new data, as shown in the below pipeline:\n\nThe general model workflow for conducting machine learning tasks is as follows. Keep in mind that the best results will often be obtained through numerous iterations of steps 2\u20136.\n\nThe wide range of toy datsets in the UCI Machine Learning Repository make it pretty easy (almost too easy\u2026) for those new to machine learning to get started. In order to fully operationalize Scikit-learn, most datasets will require significant preprocessing, including the transformer estimators mentioned in the API discussion, as well as substantial data exploration, wrangling and feature engineering. For instance, if you\u2019re trying to use machine learning methods with text data (perhaps to do sentiment analysis), you\u2019ll have to convert the words to vectors.\n\nNevertheless, the Scikit-learn API makes machine learning very straightforward, and facilitates exploration with many different types of models. Depending on the dataset, some models will dramatically outperform others. The best approach, particularly for those just getting started with machine learning in Python, is to check out Scikit-learn\u2019s handy flowchart, experiment with grid search, and try out many different models (and the available parameters) until you begin to develop a bit of intuition around model selection and tuning. Coupled with the further readings listed below, you should be dangerous in no time.\n\nWondering how to get your (non-toy) dataset into better shape for modeling? Looking for more on determining which model is best for your data? Stay tuned for upcoming posts exploring pipelining with Scikit-learn and visual diagnostics for model evaluation!\n\nAs always, if you liked this post, please subscribe to the DDL blog and make sure to follow us on Facebook and Twitter. And finally, a sincere thank you to Benjamin Bengfort and Gianna Capezio for their thoughtful editorial guidance and contributions to this post!"
    },
    {
        "url": "https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce",
        "title": "Parameter Tuning with Hyperopt \u2013 District Data Labs \u2013",
        "text": "This post will cover a few things needed to quickly implement a fast, principled method for machine learning model parameter tuning. There are two common methods of parameter tuning: grid search and random search. Each have their pros and cons. Grid search is slow but effective at searching the whole search space, while random search is fast, but could miss important points in the search space. Luckily, a third option exists: Bayesian optimization. In this post, we will focus on one implementation of Bayesian optimization, a Python module called .\n\nUsing Bayesian optimization for parameter tuning allows us to obtain the best parameters for a given model, e.g., logistic regression. This also allows us to perform optimal model selection. Typically, a machine learning engineer or data scientist will perform some form of manual parameter tuning (grid search or random search) for a few models \u2014 like decision tree, support vector machine, and k nearest neighbors \u2014 then compare the accuracy scores and select the best one for use. This method has the possibility of comparing sub-optimal models. Maybe the data scientist found the optimal parameters for the decision tree, but missed the optimal parameters for SVM. This means their model comparison was flawed. K nearest neighbors may beat SVM every time if the SVM parameters are poorly tuned. Bayesian optimization allow the data scientist to find the best parameters for all models, and therefore compare the best models. This results in better model selection, because you are comparing the best k nearest neighbors to the best decision tree. Only in this way can you do model selection with high confidence, assured that the actual best model is selected and used.\n\nTopics covered are in this post are:\n\nTo use the code below, you must install and .\n\nSuppose you have a function defined over some range, and you want to minimize it. That is, you want to find the input value that result in the lowest output value. The trivial example below finds the value of that minimizes a linear function .\n\nThe function first takes a function to minimize, denoted , which we here specify with an anonymous function . This function could be any valid value-returning function, such as mean absolute error in regression.\n\nThe next parameter specifies the search space, and in this example it is the continuous range of numbers between 0 and 1, specified by . is a built-in function that takes three parameters: the name, , and the lower and upper bound of the range, and .\n\nThe parameter takes a search algorithm, in this case which stands for tree of Parzen estimators. This topic is beyond the scope of this blog post, but the mathochistic reader may peruse this for details. The parameter can also be set to , but we do not cover that here as it is widely known search strategy. However, in a future post, we can.\n\nFinally, we specify the maximum number of evaluations the function will perform. This function returns a python dictionary of values.\n\nAn example of the output for the function above is .\n\nHere is the plot of the function. The red dot is the point we are trying to find.\n\nHere is a more complicated objective function: . This time we are trying to minimize a quadratic equation . So we alter the search space to include what we know to be the optimal value ( ) plus some sub-optimal ranges on either side: .\n\nNow we have:\n\nThe output should look something like this:\n\nHere is the plot.\n\nInstead of minimizing an objective function, maybe we want to maximize it. To to this we need only return the negative of the function. For example, we could have a function :\n\nHow could we go about solving this? We just take the objective function and return the negative, giving or just .\n\nHere is one similar to example 1, but instead of minimizing, we are trying to maximize.\n\nHere is a function with many (infinitely many given an infinite range) local minima, which we are also trying to maximize:\n\nThe module includes a few handy functions to specify ranges for input parameters. We have already seen . Initially, these are stochastic search spaces, but as learns more (as it gets more feedback from the objective function), it adapts and samples different parts of the initial search space that it thinks will give it the most meaningful feedback.\n\nThe following will be used in this post:\n\nOthers are available, such as , , , but we will not use them here.\n\nTo see some draws from the search space, we should import another function, and define the search space.\n\nAn example output is:\n\nTry running this a few times and to see the different samples.\n\nIt would be nice to see exactly what is happening inside the black box. The object allows us to do just that. We need only import a few more items.\n\nThe and imports are new. The object allows us to store info at each time step they are stored. We can then print them out and see what the evaluations of the function were for a given parameter at a given time step.\n\nHere is an example output of the code above:\n\nThe trials object stores data as a object, which works just like a object. is from the module. We will not discuss the details here, but there are advanced options for that require distributed computing using , hence the import.\n\nBack to the output above. The is the time id, that is, the time step, which goes from to . It increases by one each iteration. is in the key, which is where your parameters are stored for each iteration. is in the key, which gives us the value for our objective function at that iteration.\n\nLet\u2019s look at this in another way.\n\nWe\u2019ll go over two types of visualizations here: val vs. time, and loss vs. val. First, val vs. time. Below is the code and sample output for plotting the data described above.\n\nThe output should look like this, assuming we change to 1000.\n\nWe can see that initially the algorithm picks values from the whole range equally (uniformly), but as time goes on and more is learned about the parameter\u2019s effect on the objective function, the algorithm focuses more and more on areas in which it thinks it will gain the most \u2014 the range close to zero. It still explores the whole solution space, but less frequently.\n\nNow let\u2019s look at the plot of loss vs. val.\n\nThis gives us what we expect, since the function is deterministic.\n\nTo wrap up, let\u2019s try a more complicated example, with more randomness and more parameters.\n\nIn this section, we\u2019ll walk through 4 full examples of using for parameter tuning on a classic dataset, Iris. We will cover K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Trees, and Random Forests. Note that since we are trying to maximize the cross-validation accuracy ( in the code below), we must negate this value for , since only knows how to minimize a function. Minimizing a function is the same as maximizing the negative of .\n\nFor this task, we\u2019ll use the classic Iris data set, and do some supervised machine learning. There are 4 input features, and three output classes. The data are labeled as belonging to class 0, 1, or 2, which map to different kinds of Iris flower. The input has 4 columns: sepal length, sepal width, petal length, and pedal width. Units of the input are centimeters. We will use these 4 features to learn a model that predicts one of three output classes. Since the data is provided by , it has a nice DESCR attribute that provides details on the data set. Try the following for more details.\n\nLet\u2019s get to know the data a little better through visualization of the features and classes, using the code below. Don\u2019t forget to if you have not already.\n\nHere is the plot:\n\nWe now apply to finding the best parameters to a K-Nearest Neighbor (KNN) machine learning model. The KNN model classifies a data point from the test set based on majority class of the k nearest data points in the training data set. More information on this algorithm can he found here. \n\nThe code below incorporates everything we have covered.\n\nNow let\u2019s see the plot of the output. The y axis is the cross validation score, and the x axis is the value in k-nearest-neighbors. Here is the code and its image:\n\nAfter is greater than 63, the accuracy drops precipitously. This is due to the number of each class in the dataset. There are only 50 instances of each of the three classes. So let's drill down by limiting the values of to smaller values.\n\nHere is what we get when we run the same code for visualization:\n\nNow we can see clearly that there is a best value for , at = 4.\n\nThe model above does not do any preprocessing. So let\u2019s normalize and scale our features and see if that helps. Use this code:\n\nAnd plot the parameters like this:\n\nWe see that scaling and/or normalizing the data does not improve predictive accuracy. The best value of remains 4, which results in 98.6 % accuracy.\n\nSo this is great for parameter tuning a simple model, KNN. Let\u2019s see what we can do with Support Vector Machines (SVM).\n\nSince this is a classification task, we\u2019ll use 's class. Here is the code:\n\nHere is what we get:\n\nAgain, scaling and normalizing do not help. The first choice of kernel funcion is the best ( ), the best value is , and the best is . This set of parameters results in 99.3 % classification accuracy.\n\nWe will only attempt to optimize on a few parameters of decision trees. Here is the code.\n\nThe output is the following, which gives 97.3 % accuracy.\n\nHere are the plots. We can see that there is little difference in performance with different values of , , and .\n\nLet\u2019s see what\u2019s happending with an ensemble classifier, Random Forest, which is just a collection of decision trees trained on different even-sized partitions of the data, each of which votes on an output class, with the majority class being chosen as the prediction.\n\nAgain, we only get 97.3 % accuracy, same as decision tree.\n\nHere is the code to plot the parameters:\n\nWhile it is fun and instructive to automatically tune the parameters of one model \u2014 SVM or KNN, for example \u2014 it is more useful to tune them all at once and arrive at a best model overall. This allows us to compare all parameters and all models at once, which gives us the best model. Here is the code.\n\nThis code takes a while to run since we increased the number of evaluations: . There is also added output to update you when a new accuracy is found. Curious as to why using this method does not find the best model that we found above: with , , and .\n\nWe have covered simple examples, like minimizing a deterministic linear function, and complicated examples, like tuning random forest parameters. The documentation for is here. Another good blog on hyperopt is this one by FastML. A SciPy Conference paper by the authors is Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms, with an accompanying video tutorial. A more techical treatment of the engineering ins and outs is Making a Science of Model Search.\n\nThe techniques in this post can be used in many domains other than machine learning, such as tuning the parameter in an epsilon-greedy multi-armed bandit, or the parameters passed to a graph generator to make a synthetic network that has certain properties. We will write more on that later."
    },
    {
        "url": "https://medium.com/district-data-labs/time-maps-visualizing-discrete-events-across-many-timescales-6bb307fe200c",
        "title": "Time Maps: Visualizing Discrete Events Across Many Timescales",
        "text": "Discrete events pervade our daily lives. These include phone calls, online transactions, and heartbeats. Despite the simplicity of discrete event data, it\u2019s hard to visualize many events over a long time period without hiding details about shorter timescales.\n\nThe plot below illustrates this problem. It shows the number of website visits made by a certain IP address over the course of 7 months. It was built from discrete event data. The height of each bar is the number of events that occurred in each time bin.\n\nWhile the plot displays the overall behavior, details about the timing of events within each time bin are completely lost. One can always zoom into a certain portion of the histogram and re-bin the data, but what zoom level should we choose, and what timespan should we examine? For example, if we want to zoom into individual days, we would have roughly 210 days to look at. Searching for patterns and outliers at various zoom levels would be very time consuming.\n\nIn this blog post, I\u2019ll describe a technique for visualizing many events across multiple timescales in a single image, where little or no zooming is required. It allows the viewer to quickly identify critical features, whether they occur on a timescale of milliseconds or months. It is adopted from the field of chaotic systems, and was originally conceived to study the timing of water drops from a dripping faucet. The visualization has gone by various names: return map, return-time map, and time vs. time plot. For conciseness, I will call them \u201ctime maps.\u201d Though time maps have been used to visualize chaotic systems, they have not been applied to information technology. I will show how time maps can provide valuable insights into the behavior of Twitter accounts and the activity of a certain type of online entity, known as a bot.\n\nThis blog post is a shorter version of a paper I recently wrote, but with slightly different examples. The paper was accepted to the 2015 IEEE Big Data Conference. The end of the blog also contains sample Python code for creating time maps.\n\nBuilding a time map is easy. First, imagine a series of events as dots along a time axis. The time intervals between each event are labeled as t1, t2, t3, t4, \u2026\n\nA time map is simply a two-dimensional scatterplot, where the xy coordinates of the events are: (t1,t2), (t2, t3), (t3, t4), and so on. On a time map, the purple dot would be plotted like this:\n\nIn other words, each point in the scatterplot represents an event. The x-coordinate of an event is the time between the event itself and the preceding event. An event\u2019s y-coordinate is the time between the event itself and the subsequent event. The only points that are not displayed in a time map are the first and last events of the dataset.\n\nBelow are two simple examples:\n\nFor the sequence of evenly spaced events (A), the time map is effectively a single point, since the xy coordinates of the events are all identical. Sequence B is the same as A, except the timing of one event is slightly shifted. The resulting time map consists of effectively 4 points, making the shift easy to notice. Unless a very small bin size is chosen, the histogram over time for the two sequences would be exactly the same, concealing the changes in timing.\n\nTo get a more intuitive feel for time maps, here\u2019s a heuristic diagram, which is divided into quadrants:\n\nWithin each quadrant is a picture of events along a time axis. The upper-right and lower-left quadrants contain events that are equally spaced between preceding and subsequent events. So both of these quadrants are \u201csteady.\u201d Since there is less time between events in the lower-left quadrant, it\u2019s called \u201cfast and steady.\u201d The upper-right has more time between events, so it\u2019s \u201cslow and steady.\u201d In the lower-right, an event occurs, there is a long waiting time, and then two events happen in rapid succession. So that process is \u201cspeeding up.\u201d In the upper-left, two events quickly occur, followed by a long waiting time. In this case, the events are \u201cslowing-down.\u201d Viewing a time map is a bit like reading a map of a new city. While unfamiliar at first, it eventually becomes second nature.\n\nThe beauty of time maps is that long timelines are no longer an issue, since only the times between events are plotted. In cases where inter-event timing varies by orders of magnitude, we can scale the axes logarithmically. This allows the viewer to see time intervals that range from milliseconds to months in a single picture. Let\u2019s see some real-world examples!\n\nTwitter API allows you to gather the 3,200 most recent tweets written by a user. Using Twython, I downloaded tweets from the @WhiteHouse, which are mostly written by the president\u2019s staff. Here is the time map for tweets written between January and September 2015:\n\nEach tweet is color-coded based on the time of day, and the time axes are scaled logarithmically. Two clusters correspond to the beginning and end of the workday, at least in terms of tweets. The first tweet of the day usually occurs between 9 AM and noon. The last tweet of the day can take place over a much wider time window.\n\nInterestingly, there are two clusters that represent different modes of behavior. In the cluster I call \u201cbusiness as usual\u201d, tweets are written roughly once per hour. The tweets in the lower left occur in rapid succession, and correspond to major events.\n\nSince it\u2019s hard to count the number of points in each cluster, a \u201cheated time map\u201d can be useful to show higher densities of points in red.\n\nIn a heated time map, we can still see isolated points. The added benefit is that we can also see densities when points are close together. It looks like the \u201cbusiness as usual\u201d cluster has the most tweets, which justifies calling it \u201cusual.\u201d\n\nA public relations staff manages the Twitter account of the @WhiteHouse. What do the tweets from a personal Twitter account look like? Nicholas Felton is a graphic designer who specializes in information visualization. His work includes annual reports that compile graphics based on data about his life. Below is a heated time map of his tweets:\n\nIt does not contain the distinct clusters exhibited by the @WhiteHouse. Unlike a public relations account, tweets from personal accounts often do not adhere to a strict schedule. However, the time map still captures overall trends. Many tweets are written before and following 24-hour lulls. Also, a large number of points follow a \u201cslow and steady\u201d pattern, in which tweets are written roughly once per day.\n\nInternet bots are computer programs that execute automated tasks on the web. Bots are unwittingly installed on millions of personal computers when users click on certain links. A bot could even be operating from your computer as you read this sentence! I analyzed data from a company that provides monitoring services for various websites. This brings us back to the plot shown at the beginning of this blog:\n\nIt shows the number of website visits originating from a specific IP address (this IP address is different from the IP examined in my paper). While the histogram does contain important information about overall behavior, here is what the time map reveals:\n\nThese patterns of activity cannot be achieved by a mere mortal. A few prominent features immediately jump out: a block of \u201cfast and steady\u201d points, bars of points that correspond to \u201cspeeding up\u201d and \u201cslowing down,\u201d and finally, some sparse points beyond them.\n\nThe sparse points in the lower-right and upper-left represent long dormant periods between high levels of activity. This is consistent with the long gaps seen in the histogram. The bars of points correspond to 8-minute lulls between bursts of rapid activity. The bursts themselves are represented by the large block in the lower left, in which websites are visited in quick succession. The presence of bursts can be confirmed by examining an extremely zoomed-in histogram of the data:\n\nJust as expected, there are bursts of activity separated by roughly 8-minute lulls. Though the information contained in the zoomed in histogram is consistent with the time map, who would have thought to zoom in at this level?! The time map immediately revealed the bot\u2019s regular bursting behavior, and unlike the histogram, no zooming was needed.\n\nBelow is a simple Python program for creating a time map. It\u2019s based on randomly generated data.\n\nNow let\u2019s create a heated time map. It builds off the program from above. Basically, a two-dimensional histogram is made, which counts the number of events within each grid-square of the time map. The histogram is treated like an image. Gaussian blur is then applied to the image, which smoothens out abrupt changes.\n\nPython files for downloading tweets and creating time maps can also be found on my github page.\n\nI\u2019ve given three examples that demonstrate the exploratory value of time maps. In all cases, they immediately revealed underlying patterns which would be extremely time-consuming to discover with histograms. Nevertheless, time maps certainly do not replace other time visualizations; they augment them. The best visual explorations will involve time maps and other graphics as well. For instance, both the time map and the histogram display important aspects of the data, and should be used in concert with each other.\n\nPicasso captured the full extent of an object by displaying it from multiple perspectives within the same painting. Similarly, time maps distill a dataset across multiple timescales within a single image. Given their easy implementation and ability to reveal hidden structure, time maps should become an invaluable tool in our increasingly data-driven world."
    },
    {
        "url": "https://medium.com/district-data-labs/the-age-of-the-data-product-e92a131b1883",
        "title": "The Age of the Data Product \u2013 District Data Labs \u2013",
        "text": "We are living through an information revolution. Like any economic revolution, it has had a transformative effect on society, academia, and business. The present revolution, driven as it is by networked communication systems and the Internet, is unique in that it has created a surplus of a valuable new material \u2014 data \u2014 and transformed us all into both consumers and producers. The sheer amount of data being generated is tremendous. Data increasingly affects every aspect of our lives, from the food we eat, to our social interactions, to the way we work and play. In turn, we have developed a reasonable expectation for products and services that are highly personalized and finely tuned to our bodies, our lives, and our businesses, creating a market for a new information technology \u2014 the data product.\n\nThe rapid and agile combination of surplus data sets with machine learning algorithms has changed the way that people interact with everyday things and each other because they so often lead to immediate and novel results. Indeed, the buzzword trend surrounding \u201cbig data\u201d is related to the seemingly inexhaustible innovation that is available due to the large number of models and data sources.\n\nData products are created with data science workflows, specifically through the application of models, usually predictive or inferential, to a domain-specific data set. While the potential for innovation is great, the scientific or experimental mindset that is required to discover data sources and correctly model or mine patterns is not typically taught to programmers or analysts. Indeed, it is for this reason that it\u2019s cool to hire PhDs again \u2014 they have the required analytical and experimental training that, when coupled with programming foo, leads almost immediately to data science expertise. Of course, we can\u2019t all be PhDs. Instead, this article presents a pedagogical model for doing data science, and serves as a foundation for architecting applications that are, or can become, data products (minus the expensive and lengthy degree).\n\nThe traditional answer to this question is usually \u201cany application that combines data and algorithms.\u201d But to be frank, if you\u2019re writing software and you\u2019re not combining data with algorithms, then what are you doing? After all, data is the currency of programming! More specifically, we might say that a data product is the combination of data with statistical algorithms that are used for inference or prediction. Many data scientists are former statisticians, and statistical methodologies are central to data science.\n\nArmed with this definition, you could cite Amazon recommendations as an example of a data product. They examine items you\u2019ve purchased, and based on those interests, they make recommendations. In this case, order history data is combined with recommendation algorithms to make predictions about what you might purchase in the future. You might also cite Facebook\u2019s People You May Know because this product \u201cshows you people based on mutual friends, work and education information \u2026 [and] many other factors\u201d \u2014 the combination of social network data with graph algorithms to infer members of communities.\n\nThese examples are certainly revolutionary in their own domains of retail and social networking, but they don\u2019t necessarily seem different from other web applications. Indeed, defining data products as simply the combination of data with statistical algorithms seems to limit data products to single software instances like a web application, which hardly seems a revolutionary economic force. Although we might point to Google or others as large scale economic forces, it is not simply the combination of web crawler and PageRank that has had a larger effect on the economy. Since we know what an important role search plays in economic activity, something must be missing from this first definition.\n\nMike Loukides argues that a data product is not simply another name for a \u201cdata-driven app.\u201d Although blogs, eCommerce platforms, and most web and mobile apps rely on a database and data services such as RESTful APIs, they are merely using data. That alone it is not a data product. Instead, in his excellent article, What is Data Science?, he defines a data product as follows:\n\nThis is the revolution. A data product is an economic engine. It derives value from data and then produces more data, more value, in return. The data that it creates may fuel the generating product (we have finally achieved perpetual motion!) or it might lead to the creation of other data products that derive their value from that generated data. This is precisely what has led to the surplus of information and the resulting information revolution. More importantly, it is the generative effect that allows us to achieve better living through data, because more data products mean more data, which means even more data products, and so forth.\n\nArmed with this more specific definition, we can go farther to describe data products as systems that are self adapting, learning, and broadly applicable. Under this definition, the Nest thermostat is a data product. It derives its value from sensor data and adapts, as a result, its own newly generated data. Autonomous vehicles such as those being produced by Stanford\u2019s Autonomous Driving Team also fall into this category. Their machine vision and pilot behavior simulation are the result of algorithms, so when the vehicle is in motion, it produces more data in the form of navigation and sensor data that can be used to improve their driving platform. The advent of the quantified self produced by Fitbit, Withings, and many others means that data affects human behavior; the smart grid means that data effects your utilities.\n\nData products are not merely web applications and are rapidly becoming an essential component of almost every single domain of economic activity of the modern world. Because they are able to discover individual patterns in human activity, they drive decisions, whose resulting actions and influences are also recorded as new data.\n\nAn oft-quoted tweet by Josh Wills informs us that a Data Scientist is:\n\nCertainly this definition fits in well with the idea that a data product is simply the combination of data with statistical algorithms. Both software engineering and statistical knowledge are essential to data science. However, in an economy that demands products that derive their value from data and generate new data in return, we should say instead that as data scientists, it is our job to build data products.\n\nHarlan Harris, provides more detail about the incarnation of data products: they are built at the intersection of data, domain knowledge, software engineering, and analytics. Because data products are systems, they require an engineering skill set, usually in software, in order to build them. They are powered by data, so having data is a necessary requirement. Domain knowledge and analytics are the tools used to build the data engine, usually via experimentation, hence the \u201cscience\u201d part of data science.\n\nBecause of the experimental methodology required, most data scientists will point to this typical analytical workflow: ingestion \u2192 wrangling \u2192 modeling \u2192 reporting and visualization. Yet this so-called \u201cdata science pipeline\u201d is completely human-powered, augmented by the use of scripting languages like R and Python. Human knowledge and analytical skill are required at every step of the pipeline, which is intended to produce unique, non-generalizable results. Although this pipeline is a good starting place as a statistical and analytical framework, it does not meet the requirements of building data products.\n\nThe pipeline is a pedagogical model for teaching the workflow required for thorough statistical analyses of data, as shown in the figure below. In each phase, an analyst transforms an initial data set, augmenting or ingesting it from a variety of data sources, wrangling it into a normal form that can be computed upon, either with descriptive or inferential statistical methods, before producing a result via visualization or reporting mechanisms. These analytical procedures are usually designed to answer specific questions, or to investigate the relationship of data to some business practice for validation or decision-making.\n\nThis original workflow model has driven most early data science thought. Although it may come as a surprise, original discussions about the application of data science revolved around the creation of meaningful information visualization, primarily because this workflow is intended to produce something that allows humans to make decisions. By aggregating, describing, and modeling large data sets, humans are better able to make judgments based on patterns rather than individual data points. Data visualizations are nascent data products \u2014 they generate their value from data, then allow humans to take action based on what they learn, creating new data from those actions.\n\nHowever, this model does not scale. The human-centric and one-way design of this workflow precludes the ability to efficiently design self-adapting systems that are able to learn. Machine learning algorithms have become widely available beyond academia, and fit the definition of data products very well. These types of algorithms derive their value from data as models are fit to existing data sets, then generate new data in return by making predictions about new observations.\n\nIn order to create machine learning systems, several things are required. First, an initial data set. If this initial data set is annotated with the \u201ccorrect\u201d answers, then our system will be supervised learning. If it is not, then we will be undertaking pattern analysis in an unsupervised form. Either way, some interactive work will be required to either annotate the initial dataset, or add labels to discovered patterns. After the initial dataset has been created, some model or algorithm is required to fit to our data. Models range from instance-based k-Nearest-Neighbors algorithms to recurrent neural networks. The time required to fit models is related to the complexity of computation and the size of the initial data set.\n\nAfter a model has been fit to the initial data, it must be validated, that is, checked for its ability to accurately generalize to unseen input. A process of validation is to divide the shuffled training dataset into 12 parts. The model is fit on 10 of the parts, reinforced after training with one of the parts, and then tested on the last, unseen part. When this process is done 12 times, with each part omitted for validation once, the average of the accuracy gives a good sense of how the model will generalize (e.g. whether or not it overfits or has other biases). After validation, the model is trained on the entire training data set and stored so that it can be used to make predictions or estimations about new input data on demand. Finally, as new observations are made, the model should be adjusted through feedback in order to improve as new information becomes available.\n\nWith the machine learning life cycle in mind, we can refactor the human-driven data science pipeline into an iterative model with four stages: interaction, data, storage, and computation, as shown in the figure above. The life cycle has two phases: the build phase, where models are trained and evaluated (the top section of the life cycle, ingestion through normalization), and an operational phase where models are used in practice (the bottom section of selection through feedback). Crucially, this pipeline builds feedback into the workflow, such that automatic systems can be built to self-adapt, and where the generation of new data is required.\n\nIn the following sections, we will take a look at each phase and operation in the pipeline, and compare the data product pipeline to the data science pedagogical model.\n\nThe training phase of the data product pipeline embeds the primary efforts of the original data science pipeline. Although the workflow can (and eventually should) be automated during this phase, often the training phase can be very hands-on and exploratory. In a typical architecture, the application can be built relatively early on, as the components of the operational phase can be seen as primarily engineering efforts. However, it is in the training phase that the primarily scientific efforts are undertaken.\n\nAnand Rajaraman\u2019s semi-controversial statement that more data beats better algorithms has often been misused to imply that larger training datasets are required for more accurate machine learning models. This is partially true. Machine learning is essentially learning by example, and the greater the variety of examples, the more generalizable a model may become. However, Anand\u2019s post states that it wasn\u2019t simply more examples that provided better results, but rather more features, the inclusion of semi-related data for better decision-making on the part of the model.\n\nOften, our core data products will be based on the investigation of a specific dataset, but the inclusion of other information is essential. Thanks to the Internet, there is a lot of data available to enhance our models. Web crawlers, APIs, sensors, and user-facing applications all provide essential data that needs to be collected and stored for use downstream in the data product pipeline. Often ingestion processes can be divided into two categories: ingestion of high volumes of data rarely or the continual ingestion of data that is constantly streaming into the system.\n\nIngested data is usually provided in a form that is usable to the provider of the data, and not immediately suited to the application of the consumer without some wrangling. After long experience, however, data scientists have learned never to throw out data that may become useful in the long run. Therefore ingested data is saved to an intermediary data store: write once, read many (WORM) storage. This storage is also occasionally called a \u201cdata lake.\u201d\n\nOnce data is collected, it has to be annotated in the case of supervised learning methods. The annotation process is interactive, where a human expert provides correct answers for some data set. The annotation process can also be seen as a data ingestion. Because of the requirements for annotation and WORM storage, often the ingestion component of a data product is a full application that includes web services and multi-process systems. Critically, this component should only fetch external data and store it. It should not make corrections or transform the data in any way. Any necessary transformations are left to the next stage.\n\nData, especially data created by humans, is never perfect. D.J. Patil stresses that 80% of the work of data science is cleaning bad data; the truism \u201cgarbage in, garbage out\u201d certainly applies to predictive models. Occasionally, this is as simple as data type wrangling: data is usually ingested in a string format, and these formats must be parsed into their actual data types (dates, times, currencies, numbers, time zones, encoding, prices, or more complex structures like social networks, email, etc). Missing data must be either inferred (using a descriptive statistic like the mean, median, or mode) or simply dropped. Repeated data must be eliminated, and units need to be standardized (all temperatures to Celsius, for example).\n\nOperations that are applied to data, such as filtering, aggregation, counting, normalization (as in database normalization), or denormalization (joining two tables), all ensure data is in a form it can be computed on. These processes are inherently destructive. The original data doesn\u2019t survive the wrangling process, which is why it\u2019s critical to make copies of the data.\n\nThe biggest challenge in wrangling is often dealing with multiple, similar data sources, all of which require special handling to get to some standardized form. A typical model is to create a templateprogramming interface, where a base class includes several callable functions that expect output in the standard form. Subclasses handle specific cases, hoping to cover as much of the expected data as possible. Finally, leaf nodes handle individual data sources. With this construct in place, a simple mapping of source to template creates a robust and flexible data wrangling system, though there is no avoiding per-source wrangling efforts.\n\nThe last piece to consider in the wrangling phase, and one that extends to the normalization phase, is initial feature extraction. Once all data is more or less in a similar format, features must be generated or extracted into a usable representation. For example, textual data requires segmentation (splitting into sentences), tokenization (splitting into word forms), part of speech tagging, lexical disambiguation, feature space reduction (lemmatization or stemming), or even parsing for structural analyses. Sound files need to be transcribed, video files need to be marked, etc. Graphs need to be simplified through entity resolution. Statistical processes are often used to perform these extractions, and the wrangling phase may include smaller, internal data products that follow a similar life cycle.\n\nIn order to provide transactional or aggregate operations on many instances of data used in statistical processes, a data warehouse is required. This is especially true when the volume of data often cannot simply be kept in memory, but requires some data management system. Extract, transform, and load (ETL) methodologies operationalize wrangling methods to move data to operational storage locations. In order to train a model, many examples or instances are required, but those instances need to be in a normal form since the instance is usually described by a complex data type.\n\nNormalization in this context usually refers to relational database normalization, but similar techniques are also essential to NoSQL data warehouses. One efficient way to implement machine learning algorithms is to use graph algorithms, since graph data structures provide inherent efficiencies for iterative data processing. NoSQL graph databases, like Neo4j, require data to be stored in the property graph format. Graph normalization, therefore, is the modeling and extraction of nodes and relationships, and both vertices and edges require relevant properties added to instances. Columnar or document data stores also have their own challenges for data storage that need to be tackled in this phase.\n\nOnce data is in a normal form it is stored in a computational data store \u2014 a repository of instance data that can be explored using traditional analytical techniques. It is very popular to hook Tableau up to PostgreSQL to do data explorations, or to use SQL or Cypher to perform queries or analyses to generate hypotheses. The computational data store is the central component for all operations surrounding the data product.\n\nOnce data has been ingested, the computational data store is used to define and build predictive models. This is the \u201csexy\u201d part of data analytics and data science, and most data science literature relates to these processes. In order to build predictive models, three primary activities are required:\n\nFeature analysis is used to identify key data points and to develop dimensions upon which to perform predictions and analyses. The work of feature analysis often leads to more feature extraction, which filters back to the wrangling and normalization stages. Models are usually created on both a human hypothesis basis, as well as searching a large data space. Because of this, feature analysis is usually an iterative process that requires a clean data store and significant computational time.\n\nModels themselves also must be computed, a process called training or fitting. Training involves the optimization of a cost curve to select the model parameters for a specific space. Optimization is iterative, and computationally time consuming. Descriptive statistics, sampling, aggregations, and analysis of variance all assist in model optimization, but these computations can be time consuming. In order to reduce computational time, associated computations such as indices, relations, internal classifiers or clustering may be used to make downstream calculations more efficient. Parameterizing the model itself might also involve brute force grid searches.\n\nIn order to make this phase efficient, a data product normally employs a strategy of continuous builds: a process whereby models are constantly computed upon on a regular basis (e.g. nightly or weekly). For some very computationally intensive models like recurrent neural networks, this process might be overlapping, for others like SVMs, a variety of models with different parameterizations (different kernel functions, alphas, gammas, etc.) might be built in parallel. In order to determine the efficacy of a variety of models, cross-validation is used and the model with the highest score (accuracy, precision, recall, F1, R-squared, etc.) is then saved to disk to be used during the operational phase.\n\nCross validation is used to ensure that a model is not biased through overfitting such that it can generalize well to unseen data. In the 12-part cross validation approach mentioned previously, the final test of goodness is the average of some statistical score across all twelve parts. This score is saved, along with the final model, to be used in production.\n\nThe operational phase extends the original data science pipeline to create a workflow that is designed for the production deployment of statistical models in data products. During this phase, models that are stored in the computational data store are used to make predictions and generate new data in real time in customer facing applications. This phase can power visualizations and reporting tools that were the traditional work products of the original data science pipeline, but also attempts to incorporate feedback and interaction into a data lifecycle that will continuously grow the data product.\n\nUsually a data product is not a single model, but rather an ensemble of models that are used for a variety of tasks during interaction with the user. Models are generated on a routine basis, while data is continually ingested as more feedback is collected. Models can also be generated on different samples of data, producing a variety of accuracy scores. As the computational phase creates and stores multiple models on disk, a selection process is required to use the model in routine operation. This can be as simple as choosing the latest model or the one with the highest goodness-of-fit score (lowest mean square error, highest coefficient of determination, highest accuracy, best F1 score, etc). More complex methods use A/B testing or multi-armed bandits to select models and optimize \u201cexploration vs. exploitation\u201d in real time.\n\nAlternatively, the model selection phase might not be to operationalize some predictive model into the application, but rather to pre-compute results for immediate reuse. This is very common for recommendation systems where, instead of computing recommendations for a particular user when a user adds a product to a cart, recommendations are computed on a per-user or per-item basis and displayed immediately. Usually pre-computation is an offline process where an entire data store is updated or replaced by the new results and then swapped in for immediate use in the system.\n\nWhichever workflow is chosen for an application, consideration must be paid to the storage components of the interactive system. Because there are multiple models and multiple options for deploying models, this stage is usually where post-evaluation occurs on models, experimenting on their efficacy \u201cin the wild.\u201d Monitoring the behavior of different types of predictive models ensures that the entire system continues to perform well over time. Model build phases can be adapted as a result of such monitoring. For example, a model can be made to \u201cforget\u201d old training data that may no longer be relevant, thereby improving its utility. In order to implement selection and monitoring phases, close interaction with both the previous computational phase as well as the next phase, the API, is required.\n\nData products are implemented as software products in a variety of domains \u2014 not just on the web, but also in physical products. No matter their implementation, some interaction with the predictive or inferential capabilities of the model is required: whether it is interacting with a model that is loaded into memory to compute on demand or whether it is looking up the pre-computed predictions in a database. The glue between an end user and the model is the application programming interface, or API.\n\nWhen most people think of APIs, they think of RESTful web interfaces, which are an extremely good examples of how to employ or deploy data products. A RESTful API accepts an HTTP request and returns a structured response, usually via some serialization mechanism like JSON or XML. The simplest API for a data product is a GET request with some feature vector, whose response is a prediction or estimation, and a POST request with some indication of whether or not the prediction was accurate. This simple architecture can be used in a variety of applications, or as connective glue between the various components of larger applications. It behooves data scientists to learn how to create simple APIs like this for effective deployment of their models!\n\nA RESTful API isn\u2019t the only type of interface for a data product. A CSV dump of data, streaming data on demand, or micro-services can all power a suite of tools and are all data products. It is very popular to create simple HTML5 applications using D3 and static JSON data for visualizations and mash-ups. However the API is implemented, it serves as the connective tissue between the back-end data and the user, and is a vital component of building data products.\n\nUsers interact directly with an API or with an application that is powered by an API (e.g. a mobile app, a web site, a thermostat, a report, or even a visualization). It is through the interaction of users with the system that new data is generated: either as the user makes more requests or gives more information (e.g. an effective search tool will get more searches, which power more effective search) or as the user validates the response of the system (by purchasing a recommended product or by upvoting, tagging, or any other interaction). Although often overlooked, having a system that accepts feedback is essential to creating a self-adapting system and to take advantage of the economic gains of data products which generate more data.\n\nHumans recognize patterns effectively and on a per-instance basis can bring a wide variety of experience and information to bear on a problem solving task. For this reason, human feedback is critical for active learning \u2014 machine learning where the system rebuilds models by detecting error. The action/interaction cycle is part of what drives the engine of data generation, building more value as humans react to or utilize information from different systems. It is this final piece of the data product pipeline that makes this workflow a life cycle, rather than simply a static pipeline.\n\nThe conversation regarding what data science is has changed over the course of the past decade, moving from the purely analytical towards more visualization-related methods, and now to the creation of data products. Data products are economic engines that derive their value from data, are self-adapting, learning, and broadly applicable, and generate new data in return. Data products have engaged a new information economy revolution that has changed the way that small business, technology startups, larger organizations, and government view their data.\n\nIn this post, we\u2019ve described a revision to the original pedagogical model of the data science pipeline, and proposed a data product pipeline. The data product pipeline is iterative, with two phases: the building phase and the operational phase, and four stages: interaction, data, storage, and computation. It serves as an architecture for performing large scale data analyses in a methodical fashion that preserves experimentation and human interaction with data products, but also enables parts of the process to become automated as larger applications are built around them. We hope that this pipeline can be used as a general framework for understanding the data product lifecycle, but also as a stepping stone so that more innovative projects may be explored."
    },
    {
        "url": "https://medium.com/district-data-labs/markup-for-fast-data-science-publication-16732ff6e592",
        "title": "Markup for Fast Data Science Publication \u2013 District Data Labs \u2013",
        "text": "As data scientists, it\u2019s easy to get bogged down in the details. We\u2019re busy implementing Python and R code to extract valuable insights from data, train effective machine learning models, or put a distributed computation system together. Many of these tasks, especially those relating to data ingestion or wrangling, are time-consuming but are the bread and butter of the data scientist\u2019s daily grind. What we often forget, however, is that we must not only be data engineers, but also contributors to the data science corpus of knowledge.\n\nIf a data product derives its value from data and generates more data in return, then a data scientist derives their value from previously published works and should generate more publications in return. Indeed, one of the reasons that Machine Learning has grown ubiquitous (see the many Python-tagged questions related to ML on Stack Overflow) is thanks to meticulous blog posts and tools from scientific research (e.g. Scikit-Learn) that enable the rapid implementation of a variety of algorithms. Google in particular has driven the growth of data products by publishing systems papers about their methodologies, enabling the creation of open source tools like Hadoop and Word2Vec.\n\nBy building on a firm base for both software and for modeling, we are able to achieve greater results, faster. Exploration, discussion, criticism, and experimentation all enable us to have new ideas, write better code, and implement better systems by tapping into the collective genius of a data community. Publishing is vitally important to keeping this data science gravy train on the tracks for the foreseeable future.\n\nIn academia, the phrase \u201cpublish or perish\u201d describes the pressure to establish legitimacy through publications. Clearly, we don\u2019t want to take our rule as authors that far, but the question remains, \u201cHow can we effectively build publishing into our workflow?\u201d The answer is through markup languages \u2014 simple, streamlined markup that we can add to plain text documents that build into a publishing layout or format. For example, the following markup languages/platforms build into the accompanying publishable formats:\n\nThe great thing about markup languages is that they can be managed inline with your code workflow in the same software versioning repository. Github goes even further as to automatically render Markdown files! In this post, we\u2019ll get you started with several markup and publication styles so that you can find what best fits into your workflow and deployment methodology.\n\nMarkdown is the most ubiquitous of the markup languages we\u2019ll describe in this post, and its simplicity means that it is often chosen for a variety of domains and applications, not just publishing. Markdown, originally created by John Gruber, is a text-to-HTML processor, where lightweight syntactic elements are used instead of the more heavyweight HTML tags. Markdown is intended for folks writing for the web, not designing for the web, and in some CMS systems, it is simply the way that you write, no fancy text editor required.\n\nMarkdown has seen special growth thanks to Github, which has an extended version of Markdown, usually referred to as \u201cGithub-Flavored Markdown.\u201d This style of Markdown extends the basics of the original Markdown to include tables, syntax highlighting, and other inline formatting elements. If you create a Markdown file in Github, it is automatically rendered when viewing files on the web, and if you include a in a directory, that file is rendered below the directory contents when browsing code. Github Issues are also expected to be in Markdown, further extended with tools like checkbox lists.\n\nMarkdown is used for so many applications it is difficult to name them all. Below are a select few that might prove useful to your publishing tasks.\n\nThere are also a wide variety of editors, browser plugins, viewers, and tools available for Markdown. Both Sublime Text and Atom support Markdown and automatic preview, as well as most text editors you\u2019ll use for coding. Mou is a desktop Markdown editor for Mac OSX and iA Writer is a distraction-free writing tool for Markdown for iOS. (Please comment your favorite tools for Windows and Android). For Chrome, extensions like Markdown Here make it easy to compose emails in Gmail via Markdown or Markdown Preview to view Markdown documents directly in the browser.\n\nClearly, Markdown enjoys a broad ecosystem and diverse usage. If you\u2019re still writing HTML for anything other than templates, you\u2019re definitely doing it wrong at this point! It\u2019s also worth including Markdown rendering for your own projects if you have user submitted text (also great for text-processing).\n\nRendering Markdown can be accomplished with the Python Markdown library, usually combined with the Bleach library for sanitizing bad HTML and linkifying raw text. A simple demo of this is as follows:\n\nFirst install and using :\n\nThen create a markdown parsing function as follows:\n\nGiven a markdown file whose contents are as follows:\n\nWill produce the following HTML output:\n\nHopefully this brief example has also served as a demonstration of how Markdown and other markup languages work to render much simpler text with lightweight markup constructs into a larger publishing framework. Markdown itself is most often used for web publishing, so if you need to write HTML, then this is the choice for you!\n\nTo learn more about Markdown syntax, please see Markdown Basics.\n\niPython Notebook is an web-based, interactive environment that combines Python code execution, text (marked up with Markdown), mathematics, graphs, and media into a single document. The motivation for iPython Notebook was purely scientific: How do you demonstrate or present your results in a repeatable fashion where others can understand the work you\u2019ve done? By creating an interactive environment where code, graphics, mathematical formulas, and rich text are unified and executable, iPython Notebook gives a presentation layer to otherwise unreadable or inscrutable code. Although Markdown is a big part of iPython Notebook, it deserves a special mention because of how critical it is to the data science community.\n\niPython Notebook is interesting because it combines both the presentation layer as well as the markup layer. When run as a server, usually locally, the notebook is editable, explorable (a tree view will present multiple notebook files), and executable \u2014 any code written in Python in the notebook can be evaluated and run using an interactive kernel in the background. Math formula written in LaTeX are rendered using MathJax. To enhance the delivery and shareability of these notebooks, the NBViewer allows you to share static notebooks from a Github repository.\n\niPython Notebook comes with most scientific distributions of Python like Anaconda or Canopy, but it is also easy to install iPython with :\n\niPython itself is an enhanced interactive Python shell or REPL that extends the basic Python REPL with many advanced features, primarily allowing for a decoupled two-process model that enables the notebook. This process model essentially runs Python as a background kernel that receives execution instructions from clients and returns responses back to them.\n\nTo start an iPython notebook execute the following command:\n\nThis will start a local server at and automatically open your default browser to it. You'll start in the \"dashboard view\", which shows all of the notebooks available in the current working directory. Here you can create new notebooks and start to edit them. Notebooks are saved as files in the local directory, a format called \"Jupyter\" that is simple JSON with a specific structure for representing each cell in the notebook. The Jupyter notebook files are easily reversioned via Git and Github since they are also plain text.\n\nTo learn more about iPython Notebook, please see the iPython Notebook documentation.\n\nreStructuredText is an easy-to-read plaintext markup syntax specifically designed for use in Python docstrings or to generate Python documentation. In fact, the reStructuredText parser is a component of Docutils, an open-source text processing system that is used by Sphinx to generate intelligent and beautiful software documentation, in particular the native Python documentation.\n\nPython software has a long history of good documentation, particularly because of the idea that batteries should come included. And documentation is a very strong battery! PyPi, the Python Package Index, ensures that third party packages provide documentation, and that the documentation can be easily hosted online through Python Hosted. Because of the ease of use and ubiquity of the tools, Python programmers are known for having very consistently documented code; sometimes it\u2019s hard to tell the standard library from third party modules!\n\nIn How to Develop Quality Python Code, I mentioned that you should use Sphinx to generate documentation for your apps and libraries in a directory at the top-level. Generating reStructuredText documentation in a directory is fairly easy:\n\nThe quickstart utility will ask you many questions to configure your documentation. Aside from the project name, author, and version (which you have to type in yourself), the defaults are fine. However, I do like to change a few things:\n\nSimilar to iPython Notebook, reStructured text can render LaTeX syntax mathematical formulas. This utility will create a Makefile for you; to generate HTML documentation, simply run the following command in the directory:\n\nThe output will be built in the folder where you can open the in your browser.\n\nWhile hosting documentation on Python Hosted is a good choice, a better choice might be Read the Docs, a website that allows you to create, host, and browse documentation. One great part of Read the Docs is the stylesheet that they use; it\u2019s more readable than older ones. Additionally, Read the Docs allows you to connect a Github repository so that whenever you push new code (and new documentation), it is automatically built and updated on the website. Read the Docs can even maintain different versions of documentation for different releases.\n\nNote that even if you aren\u2019t interested in the overhead of learning reStructuredText, you should use your newly found Markdown skills to ensure that you have good documentation hosted on Read the Docs. See MkDocs for document generation in Markdown that Read the Docs will render.\n\nTo learn more about reStructuredText syntax, please see the reStructuredText Primer.\n\nWhen writing longer publications, you\u2019ll need a more expressive tool that is just as lightweight as Markdown but able to handle constructs that go beyond simple HTML, for example cross-references, chapter compilation, or multi-document build chains. Longer publications should also move beyond the web and be renderable as an eBook (ePub or Mobi formats) or for print layout, e.g. PDF. These requirements add more overhead, but simplify workflows for larger media publication.\n\nWriting for O\u2019Reilly, I discovered that I really enjoyed working in AsciiDoc \u2014 a lightweight markup syntax, very similar to Markdown, which renders to HTML or DocBook. DocBook is very important, because it can be post-processed into other presentation formats such as HTML, PDF, EPUB, DVI, MOBI, and more, making AsciiDoc an effective tool not only for web publishing but also print and book publishing. Most text editors have an AsciiDoc grammar for syntax highlighting, in particular sublime-asciidoc and Atom AsciiDoc Preview, which make writing AsciiDoc as easy as Markdown.\n\nAsciiDoctor is an AsciiDoc-specific toolchain for building books and websites from AsciiDoc. The project connects the various AsciiDoc tools and allows a simple command-line interface as well as preview tools. AsciiDoctor is primarily used for HTML and eBook formats, but at the time of this writing there is a PDF renderer, which is in beta. Another interesting project of O\u2019Reilly\u2019s is Atlas, a system for push-button publishing that manages AsciiDoc using a Git repository and wraps editorial build processes, comments, and automatic editing in a web platform. I\u2019d be remiss not to mention GitBook which provides a similar toolchain for publishing larger books, though with Markdown.\n\nTo learn more about AsciiDoc markup see AsciiDoc 101.\n\nIf you\u2019ve done any graduate work in the STEM degrees then you are probably already familiar with LaTeX to write and publish articles, reports, conference and journal papers, and books. LaTeX is not a simple markup language, to say the least, but it is effective. It is able to handle almost any publishing scenario you can throw at it, including (and in particular) rendering complex mathematical formulas correctly from a text markup language. Most data scientists still use LaTeX, using MathJax or the Daum Equation Editor, if only for the math.\n\nIf you\u2019re going to be writing PDFs or reports, I can provide two primary tips for working with LaTeX. First consider cloud-based editing with Overleaf or ShareLaTeX, which allows you to collaborate and edit LaTeX documents similarly to Google Docs. Both of these systems have many of the classes and stylesheets already so that you don\u2019t have to worry too much about the formatting, and instead just get down to writing. Additionally, they aggregate other tools like LaTeX templates and provide templates of their own for most document types.\n\nMy personal favorite workflow, however, is to use the Atom editor with the LaTeX package and the LaTeX grammar. When using Atom, you get very nice Git and Github integration \u2014 perfect for collaboration on larger documents. If you have a TeX distribution installed (and you will need to do that on your local system, no matter what), then you can automatically build your documents within Atom and view them in PDF preview.\n\nA complete tutorial for learning LaTeX can be found at Text Formatting with LaTeX.\n\nSoftware developers agree that testing and documentation is vital to the successful creation and deployment of applications. However, although Agile workflows are designed to ensure that documentation and testing are included in the software development lifecycle, too often testing and documentation is left to last, or forgotten. When managing a development project, team leads need to ensure that documentation and testing are part of the \u201cdefinition of done.\u201d\n\nIn the same way, writing is vital to the successful creation and deployment of data products, and is similarly left to last or forgotten. Through publication of our work and ideas, we open ourselves up to criticism, an effective methodology for testing ideas and discovering new ones. Similarly, by explicitly sharing our methods, we make it easier for others to build systems rapidly, and in return, write tutorials that help us better build our systems. And if we translate scientific papers into practical guides, we help to push science along as well.\n\nDon\u2019t get bogged down in the details of writing, however. Use simple, lightweight markup languages to include documentation alongside your projects. Collaborate with other authors and your team using version control systems, and use free tools to make your work widely available. All of this is possible becasue of lightweight markup languages, and the more profecient you are at including writing in your workflow, the easier it will be to share your ideas.\n\nThis post is particularly link-heavy with many references to tools and languages. For reference, here are my preferred guides for each of the Markup languages discussed:\n\nSpecial thanks to Rebecca Bilbro for editing and contributing to this post. Without her, this would certainly have been much less readable!"
    },
    {
        "url": "https://medium.com/district-data-labs/modern-methods-for-sentiment-analysis-694eaf725244",
        "title": "Modern Methods for Sentiment Analysis \u2013 District Data Labs \u2013",
        "text": "Sentiment analysis is a common application of Natural Language Processing (NLP) methodologies, particularly classification, whose goal is to extract the emotional content in text. In this way, sentiment analysis can be seen as a method to quantify qualitative data with some sentiment score. While sentiment is largely subjective, sentiment quantification has enjoyed many useful implementations, such as businesses gaining understanding about consumer reactions to a product, or detecting hateful speech in online comments.\n\nThe simplest form of sentiment analysis is to use a dictionary of good and bad words. Each word in a sentence has a score, typically +1 for positive sentiment and -1 for negative. Then, we simply add up the scores of all the words in the sentence to get a final sentiment total. Clearly, this has many limitations, the most important being that it neglects context and surrounding words. For example, in our simple model the phrase \u201cnot good\u201d may be classified as 0 sentiment, given \u201cnot\u201d has a score of -1 and \u201cgood\u201d a score of +1. A human would likely classify \u201cnot good\u201d as negative, despite the presence of \u201cgood\u201d.\n\nAnother common method is to treat a text as a \u201cbag of words\u201d. We treat each text as a 1 by vector, where is the size of our vocabulary. Each column is a word, and the value is the number of times that word appears. For example, the phrase \u201cbag of bag of words\u201d might be encoded as [2, 2, 1]. This could then be fed into a machine learning algorithm for classification, such as logistic regression or SVM, to predict sentiment on unseen data. Note that this requires data with known sentiment to train on in a supervised fashion. While this is an improvement over the previous method, it still ignores context, and the size of the data increases with the size of the vocabulary.\n\nRecently, Google developed a method called Word2Vec that captures the context of words, while at the same time reducing the size of the data. Word2Vec is actually two different methods: Continuous Bag of Words (CBOW) and Skip-gram. In the CBOW method, the goal is to predict a word given the surrounding words. Skip-gram is the converse: we want to predict a window of words given a single word (see Figure 1). Both methods use artificial neural networks as their classification algorithm. Initially, each word in the vocabulary is a random N-dimensional vector. During training, the algorithm learns the optimal vector for each word using the CBOW or Skip-gram method.\n\nFigure 1: Architecture for the CBOW and Skip-gram method, taken from Efficient Estimation of Word Representations in Vector Space. is the current word, while , , etc. are the surrounding words.\n\nThese word vectors now capture the context of surrounding words. This can be seen by using basic algebra to find word relations (i.e. \u201cking\u201d \u2014 \u201cman\u201d + \u201cwoman\u201d = \u201cqueen\u201d). These word vectors can be fed into a classification algorithm, as opposed to bag-of-words, to predict sentiment. The advantage is that we now have some word context, and our feature space is much lower (typically ~300 as opposed to ~100,000, which is the size of our vocabulary). We also had to do very little manual feature creation since the neural network was able to extract those features for us. Since text have varying length, one might take the average of all word vectors as the input to a classification algorithm to classify whole text documents.\n\nHowever, even with the above method of averaging word vectors, we are ignoring word order. As a way to summarize bodies of text of varying length, Quoc Le and Tomas Mikolov came up with the Doc2Vec method. This method is almost identical to Word2Vec, except we now generalize the method by adding a paragraph/document vector. Like Word2Vec, there are two methods: Distributed Memory (DM) and Distributed Bag of Words (DBOW). DM attempts to predict a word given its previous words and a paragraph vector. Even though the context window moves across the text, the paragraph vector does not (hence distributed memory) and allows for some word-order to be captured. DBOW predicts a random group of words in a paragraph given only its paragraph vector (see Figure 2).\n\nFigure 2: Architecture for Doc2Vec, taken from Distributed Representations of Sentences and Documents.\n\nOnce it has been trained, these paragraph vectors can be fed into a sentiment classifier without the need to aggregate words. This method is currently the state-of-the-art when it comes to sentiment classification on the IMDB movie review data set, achieving only a 7.42% error rate. Of course, none of this is useful if we cannot actually implement them. Luckily, a very-well optimized version of Word2Vec and Doc2Vec is available in gensim, a Python library.\n\nIn this section we show how one might use word vectors in a sentiment classification task. The library comes standard with the Anaconda distribution or can be installed using pip. From there you can train word vectors on your own corpus (a dataset of text documents) or import pre-trained vectors from C text or binary format:\n\nI find this especially useful when loading Google\u2019s pre-trained word vectors trained over ~100 billion words from the Google News dataset found in the \u201cPre-trained word and phrase vectors\u201d section here. Note that the file is ~3.5 GB unzipped. Using the Google word vectors we can see some interesting relationships between words:\n\nWhat\u2019s interesting is that it can find grammatical relationships, for example identifying superlatives or verb stems:\n\n\u201cbiggest\u201d - \u201cbig\u201d + \u201csmall\u201d = \u201csmallest\u201d\n\nIt\u2019s clear from the above examples that Word2Vec is able to learn non-trivial relationships between words. This is what makes them powerful for many NLP tasks, and in our case sentiment analysis. Before we move on to using them in sentiment analysis, let us first examine Word2Vec\u2019s ability to separate and cluster words. We will use three example word sets: food, sports, and weather words taken from a wonderful website called Enchanted Learning. Since these vectors are 300 dimensional, we will use Scikit-Learn\u2019s implementation of a dimensionality reduction algorithm called t-SNE in order to visualize them in 2D.\n\nFirst we have to obtain the word vectors as follows:\n\nWe can then use TSNE and matplotlib to visualize the clusters with the following code:\n\nThe result is as follows:\n\nWe can see from the above that Word2Vec does a good job of separating unrelated words, as well as clustering together like words.\n\nNow we will move on to an example in sentiment analysis with tweets gathered using emojis as search terms. We use these emojis as \u201cfuzzy\u201d labels for our data; a smiley emoji ( ) corresponds to positive sentiment, and a frowny ( ) to negative. The data consists of an even split between positive and negative with a total of ~400,000 tweets. We randomly sample positive and negative tweets to construct an 80/20, train/test, split. We then train the Word2Vec model on the train tweets. In order to prevent data leakage from the test set, we do not train Word2Vec on the test set until after our classifier has been fit on the training set. To construct inputs for our classifier, we take the average of all word vectors in a tweet. We will be using Scikit-Learn to do a lot of the machine learning.\n\nFirst we import our data and train the Word2Vec model.\n\nNext we have to build word vectors for input text in order to average the value of all word vectors in the tweet using the following function:\n\nScaling moves our data set is part of the process of standardization where we move our dataset into a gaussian distribution with a mean of zero, meaning that values above the mean will be positive, and those below the mean will be negative. Many ML models require scaled datasets to perform effectively, especially those with many features (like text classifiers).\n\nFinally we have to build our test set vectors and scale them for evaluation.\n\nNext we want to validate our classifier by calculating the prediction accuracy on test data, as well as examining its Receiver Operating Characteristic (ROC) curve. ROC curves measure the true-positive rate vs. the false-positive rate of a classifier while adjusting a parameter of the model. In our case, we adjust the cut-off threshold probability for classifying a tweet as positive or negative sentiment. Generally, the larger the Area Under the Curve (AUC), the better our model does at maximizing true positives while minimizing false positives. More on ROC curves can be found here.\n\nTo start we\u2019ll train our classifier, in this case using Stochastic Gradient Descent for Logistic Regression.\n\nWe\u2019ll then create the ROC curve for evaluation using matplotlib and the roc_curve method of Scikit-Learn\u2019s package.\n\nThe resulting curve is as follows:\n\nFigure 4: ROC Curve for a logistic classifier on our training data of tweets.\n\nWithout any type of feature creation and minimal text preprocessing we can achieve 73% test accuracy using a simple linear model provided by Scikit-Learn. Interestingly, removing punctuation actually causes the accuracy to suffer, suggesting Word2Vec can find interesting features when characters such as \u201c?\u201d and \u201c!\u201d are present. Treating these as individual words, training for longer, doing more preprocessing, and adjusting parameters in both Word2Vec and the classifier could all help in improving accuracy. I have found that using Artificial Neural Networks (ANNs) can improve the accuracy by about 5% when using word vectors. Note that Scikit-Learn does not provide an implementation of ANN classifiers so I used a custom library I created:\n\nThe resulting accuracy is 77%. As with any machine learning task, picking the right model is usually more a matter of art than science. If you\u2019d like to use my custom library you can find it on my github. Be warned, it is likely very messy and not regularly maintained! If you would like to contribute please feel free to fork the repository. It could definitely use some TLC!\n\nUsing the averages of word vectors worked fine in the case of tweets. This is because tweets are typically only a few to tens of words in length, which allows us to preserve the relevant features even when averaging. Once we go to the paragraph scale, however, we risk throwing away rich features when we ignore word order and context. In this case it is better to use Doc2Vec to create our input features. As an example we will use the IMDB movie review dataset to test the usefulness of Doc2Vec in sentiment analysis. The data consists of 25,000 positive movie reviews, 25,000 negative, and 50,000 unlabeled reviews. We first train Doc2Vec over the unlabeled reviews. The methodology then identically follows that of the Word2Vec example above, except now we will use both DM and DBOW vectors as inputs by concatenating them.\n\nNext we instantiate our two Doc2Vec models, DM and DBOW. The gensim documentation suggests training over the data multiple times and either adjusting the learning rate or randomizing the order of input at each pass. We then collect the movie review vectors learned by the models.\n\nNow we are ready to train a classifier over our review vectors. We will again use sklearn\u2019s SGDClassifier.\n\nThis model gives us a test accuracy of 0.86. We can also build a ROC curve for this classifier as follows:\n\nFigure 5: ROC Curve for a logistic classifier on our training data of IMDB movie reviews.\n\nThe original paper claimed they saw an improvement when using a 50 node neural network over a simple logistic regression classifier:\n\nInterestingly, here we see no such improvement. The test accuracy is 0.85, and we do not approach their claimed test error of 7.42%. This could be for many reasons: we did not train for enough epochs over the training/test data, their implementation of Doc2Vec/ANN is different, their hyperparameters are different, etc. It\u2019s hard to know exactly which since the paper does not go into great detail. In any case, we were able to obtain an 86% test accuracy with very little pre-processing and no feature creation/selection. No fancy convolutions or treebanks necessary!\n\nI hope you have seen not only the utility but ease of use for Word2Vec and Doc2Vec using standard tools like Python and gensim. With a very simple algorithm we can gain rich word and paragraph vectors that can be used in all kinds of NLP applications. What\u2019s even better is Google\u2019s release of their own pre-trained word vectors trained on a much larger data set than anyone else can hope to obtain. If you want to train your own vectors over large data sets there is already an implementation of Word2Vec in Apache Spark\u2019s MLlib. Happy NLP\u2019ing!"
    },
    {
        "url": "https://medium.com/district-data-labs/getting-started-with-spark-in-python-bb02cc54f1c2",
        "title": "Getting Started with Spark (in Python) \u2013 District Data Labs \u2013",
        "text": "Hadoop is the standard tool for distributed computing across really large data sets and is the reason why you see \u201cBig Data\u201d on advertisements as you walk through the airport. It has become an operating system for Big Data, providing a rich ecosystem of tools and techniques that allow you to use a large cluster of relatively cheap commodity hardware to do computing at supercomputer scale. Two ideas from Google in 2003 and 2004 made Hadoop possible: a framework for distributed storage (The Google File System), which is implemented as HDFS in Hadoop, and a framework for distributed computing (MapReduce).\n\nThese two ideas have been the prime drivers for the advent of scaling analytics, large scale machine learning, and other big data appliances for the last ten years! However, in technology terms, ten years is an incredibly long time, and there are some well-known limitations that exist, with MapReduce in particular. Notably, programming MapReduce is difficult. You have to chain Map and Reduce tasks together in multiple steps for most analytics. This has resulted in specialized systems for performing SQL-like computations or machine learning. Worse, MapReduce requires data to be serialized to disk between each step, which means that the I/O cost of a MapReduce job is high, making interactive analysis and iterative algorithms very expensive; and the thing is, almost all optimization and machine learning is iterative.\n\nTo address these problems, Hadoop has been moving to a more general resource management framework for computation, YARN (Yet Another Resource Negotiator). YARN implements the next generation of MapReduce, but also allows applications to leverage distributed resources without having to compute with MapReduce. By generalizing the management of the cluster, research has moved toward generalizations of distributed computation, expanding the ideas first imagined in MapReduce.\n\nSpark is the first fast, general purpose distributed computing paradigm resulting from this shift and is gaining popularity rapidly. Spark extends the MapReduce model to support more types of computations using a functional programming paradigm, and it can cover a wide range of workflows that previously were implemented as specialized systems built on top of Hadoop. Spark uses in-memory caching to improve performance and, therefore, is fast enough to allow for interactive analysis (as though you were sitting on the Python interpreter, interacting with the cluster). Caching also improves the performance of iterative algorithms, which makes it great for data theoretic tasks, especially machine learning.\n\nIn this post we will first discuss how to set up Spark to start easily performing analytics, either simply on your local machine or in a cluster on EC2. We then will explore Spark at an introductory level, moving towards an understanding of what Spark is and how it works (hopefully motivating further exploration). In the last two sections we will start to interact with Spark on the command line and then demo how to write a Spark application in Python and submit it to the cluster as a Spark job.\n\nSpark is pretty simple to set up and get running on your machine. All you really need to do is download one of the pre-built packages and so long as you have Java 6+ and Python 2.6+ you can simply run the Spark binary on Windows, Mac OS X, and Linux. Ensure that the program is on your or that the environment variable is set. Similarly, must also be in your .\n\nAssuming you already have Java and Python:\n\nAt this point, you\u2019ll have to figure out how to go about things depending on your operating system. Windows users, please feel free to comment about tips to set up in the comments section.\n\nGenerally, my suggestion is to do as follows (on a POSIX OS):\n\n2. Move the unzipped directory to a working application directory ( for example on Windows, or on Linux). Where you move it to doesn't really matter, so long as you have permissions and can run the binaries there. I typically install Hadoop and related tools in on my Ubuntu boxes, and will use that directory here for illustration.\n\n3. Symlink the version of Spark to a directory. This will allow you to simply download new/older versions of Spark and modify the link to manage Spark versions without having to change your path or environment variables.\n\n4. Edit your BASH profile to add Spark to your and to set the environment variable. These helpers will assist you on the command line. On Ubuntu, simply edit the or files and add the following:\n\n5. After you source your profile (or simply restart your terminal), you should now be able to run a interpreter locally. Execute the command, and you should see a result as follows:\n\nAt this point Spark is installed and ready to use on your local machine in \u201cstandalone mode.\u201d You can develop applications here and submit Spark jobs that will run in a multi-process/multi-threaded mode, or you can configure this machine as a client to a cluster (though this is not recommended as the driver plays an important role in Spark jobs and should be in the same network as the rest of the cluster). Probably the most you will do with Spark on your local machine beyond development is to use the scripts to configure an EC2 Spark cluster on Amazon's cloud.\n\nThe execution of Spark (and PySpark) can be extremely verbose, with many INFO log messages printed out to the screen. This is particularly annoying during development, as Python stack traces or the output of statements can be lost. In order to reduce the verbosity of Spark, you can configure the log4j settings in . First, create a copy of the file, removing the \".template\" extension.\n\nEdit the newly copied file and replace with at every line in the code. You log4j.properties file should look similar to:\n\nNow when you run PySpark you should get much simpler output messages! Special thanks to @genomegeek who pointed this out at a District Data Labs workshop!\n\nWhen Googling around for helpful Spark tips, I discovered a couple posts that mentioned how to configure PySpark with IPython notebook. IPython notebook is an essential tool for data scientists to present their scientific and theoretical work in an interactive fashion, integrating both text and Python code. For many data scientists, IPython notebook is their first introduction to Python and is used widely so I thought it would be worth including it in this post.\n\nMost of the instructions here are adapted from an IPython notebook: Setting up IPython with PySpark. However, we will focus on connecting your IPython shell to PySpark in standalone mode on your local computer rather than on an EC2 cluster. If you would like to work with PySpark/IPython on a cluster, feel free to check out those instructions and if you do, please comment on how it went!\n\nKeep note of where the profile has been created, and replace the appropriate paths in the following steps:\n\n2. Create a file in and add the following:\n\n3. Start up an IPython notebook with the profile we just created.\n\n4. In your notebook, you should see the variables we just created.\n\n5. At the top of your IPython notebook, make sure you add the Spark context.\n\n6. Test the Spark context by doing a simple computation using IPython.\n\nIf you get a number without errors, then your context is working correctly!\n\nEditor\u2019s Note: The above configures an IPython context for directly invoking IPython notebook with PySpark. However, you can also launch a notebook using PySpark directly as follows:\n\nEither methodology works similarly depending on your use case for PySpark and IPython. The former allows you to more easily connect to a cluster with IPython notebook, and thus, it is the method I prefer.\n\nIn my time teaching distributed computing with Hadoop, I\u2019ve discovered that a lot can be taught locally on a pseudo-distributed node or in single-node mode. However, in order to really get what\u2019s happening, a cluster is necessary. There is often a disconnect between learning these skills and the actual computing requirements when data just gets too large. If you have a little bit of money to spend learning how to use Spark in detail, I would recommend setting up a quick cluster for experimentation. Note that a cluster of 5 slaves (and 1 master) used at a rate of approximately 10 hours per week will cost you approximately $45.18 per month.\n\nA full discussion can be found at the Spark documentation: Running Spark on EC2. Be sure to read this documentation thoroughly as you\u2019ll end up sending money on an EC2 cluster if you start these steps! I\u2019ve highlighted a few key points here:\n\nNote that different utilities use different environment names, so make sure to use these for the Spark scripts.\n\nThese scripts will automatically create a local HDFS cluster for you to add data to, and there is a command that will allow you to sync code and data to the cluster. However, your best bet is to simply use S3 for data storage and create RDDs that load data using the URI.\n\nNow that we have Spark set up, let\u2019s have a bit of a discussion about what Spark is. Spark is a general purpose cluster computing framework that provides efficient in-memory computations for large data sets by distributing computation across multiple computers. If you\u2019re familiar with Hadoop, then you know that any distributed computing framework needs to solve two problems: how to distribute data and how to distribute computation. Hadoop uses HDFS to solve the distributed data problem and MapReduce as the programming paradigm that provides effective distributed computation. Similarly, Spark has a functional programming API in multiple languages that provides more operators than map and reduce, and does this via a distributed data framework called resilient distributed datasets or RDDs.\n\nRDDs are essentially a programming abstraction that represents a read-only collection of objects that are partitioned across machines. RDDs can be rebuilt from a lineage (and are therefore fault tolerant), are accessed via parallel operations, can be read from and written to distributed storages like HDFS or S3, and most importantly, can be cached in the memory of worker nodes for immediate reuse. Because RDDs can be cached in memory, Spark is extremely effective at iterative applications, where the data is being reused throughout the course of an algorithm. Most machine learning and optimization algorithms are iterative, making Spark an extremely effective tool for data science. Additionally, because Spark is so fast, it can be accessed in an interactive fashion via a command line prompt similar to the Python REPL.\n\nThe Spark library itself contains a lot of the application elements that have found their way into most Big Data applications including support for SQL-like querying of big data, machine learning and graph algorithms, and even support for live streaming data.\n\nBecause these components meet many Big Data requirements as well as the algorithmic and computational requirements of many data science tasks, Spark has been growing rapidly in popularity. Not only that, but Spark provides APIs in Scala, Java, and Python; meeting the needs for many different groups and allowing more data scientists to easily adopt Spark as their Big Data solution.\n\nProgramming Spark applications is similar to other data flow languages that had previously been implemented on Hadoop. Code is written in a driver program which is lazily evaluated, and upon an action, the driver code is distributed across the cluster to be executed by workers on their partitions of the RDD. Results are then sent back to the driver for aggregation or compilation. Essentially the driver program creates one or more RDDs, applies operations to transform the RDD, then invokes some action on the transformed RDD.\n\nThese steps are outlined as follows:\n\nWhen Spark runs a closure on a worker, any variables used in the closure are copied to that node, but are maintained within the local scope of that closure. Spark provides two types of shared variables that can be interacted with by all workers in a restricted fashion. Broadcast variables are distributed to all workers, but are read-only. Broadcast variables can be used as lookup tables or stopword lists. Accumulators are variables that workers can \u201cadd\u201d to using associative operations and are typically used as counters.\n\nSpark applications are essentially the manipulation of RDDs through transformations and actions. Future posts will go into this in greater detail, but this understanding should be enough to execute the example programs below.\n\nA brief note on the execution of Spark. Essentially, Spark applications are run as independent sets of processes, coordinated by a in a driver program. The context will connect to some cluster manager (e.g. YARN) which allocates system resources. Each worker in the cluster is managed by an executor, which is in turn managed by the . The executor manages computation as well as storage and caching on each machine.\n\nWhat is important to note is that application code is sent from the driver to the executors, and the executors specify the context and the various tasks to be run. The executors communicate back and forth with the driver for data sharing or for interaction. Drivers are key participants in Spark jobs, and therefore, they should be on the same network as the cluster. This is different from Hadoop code, where you might submit a job from anywhere to the JobTracker, which then handles the execution on the cluster.\n\nThe easiest way to start working with Spark is via the interactive command prompt. To open the PySpark terminal, simply type in on the command line.\n\nPySpark will automatically create a for you to work with, using the local Spark configuration. It is exposed to the terminal via the variable. Let's create our first RDD.\n\nThe method loads the complete works of Shakespeare into an RDD named text. If you inspect the RDD you can see that it is a MappedRDD and that the path to the file is a relative path from the current working directory (pass in a correct path to the shakespeare.txt file on your system). Let's start to transform this RDD in order to compute the \"hello world\" of distributed computing: \"word count.\"\n\nWe first imported the operator , which is a named function that can be used as a closure for addition. We'll use this function later. The first thing we have to do is split our text into words. We created a function called whose argument is some piece of text and who returns a list of the tokens (words) in that text by simply splitting on whitespace. We then created a new RDD called by transforming the RDD through the application of the operator, and passed it the closure . As you can see, is a , but the execution should have happened instantaneously. Clearly, we haven't split the entire Shakespeare data set into a list of words yet.\n\nIf you\u2019ve done the Hadoop \u201cword count\u201d using MapReduce, you\u2019ll know that the next steps are to map each word to a key value pair, where the key is the word and the value is a 1, and then use a reducer to sum the 1s for each key.\n\nInstead of using a named function, we will use an anonymous function (with the keyword in Python). This line of code will map the lambda to each element of words. Therefore, each is a word, and the word will be transformed into a tuple (word, 1) by the anonymous closure. In order to inspect the lineage so far, we can use the method to see how our is being transformed. We can then apply the action to get our word counts and then write those word counts to disk.\n\nOnce we finally invoke the action , the distributed job kicks off and you should see a lot of statements as the job runs \"across the cluster\" (or simply as multiple processes on your local machine). If you exit the interpreter, you should see a directory called \"wc\" in your current working directory.\n\nEach part file represents a partition of the final RDD that was computed by various processes on your computer and saved to disk. If you use the command on one of the part files, you should see tuples of word count pairs.\n\nNote that none of the keys are sorted as they would be in Hadoop (due to a necessary shuffle and sort phase between the Map and Reduce tasks). However, you are guaranteed that each key appears only once across all part files as you used the operator on the counts RDD. If you want, you could use the operator to ensure that all the keys are sorted before writing them to disk.\n\nWriting Spark applications is similar to working with Spark in the interactive console. The API is the same. First, you need to get access to the , which was automatically loaded for you by the application.\n\nA basic template for writing a Spark application in Python is as follows:\n\nThis template gives you a sense of what is needed in a Spark application: imports for various Python libraries, module constants, an identifying application name for debugging and for the Spark UI, closures or other custom operation functions, and finally, some main analytical methodology that is run as the driver. In our , we create the and execute main with the context as configured. This will allow us to easily import driver code into the context without execution. Note that here a Spark configuration is hard coded into the via the method, but typically you would just allow this value to be configured from the command line, so you will see this line commented out.\n\nTo close or exit the program use or .\n\nIn order to demonstrate a common use of Spark, let\u2019s take a look at a common use case where we read in a CSV file of data and compute some aggregate statistic. In this case, we\u2019re looking at the on-time flight data set from the U.S. Department of Transportation, recording all U.S. domestic flight departure and arrival times along with their departure and arrival delays for the month of April, 2014. I typically use this data set because one month is manageable for exploration, but the entire data set needs to be computed upon with a cluster. The entire app is as follows:\n\nTo run this code (presuming that you have a directory called ontime with the two CSV files in the same directory), use the command as follows:\n\nThis will create a Spark job using the localhost as the master, and look for the two CSV files in an ontime directory that is in the same directory as . The final result shows that the total delays (in minutes) for the month of April go from arriving early if you're flying out of the continental U.S. to Hawaii or Alaska to an aggregate total delay for most big airlines. Note especially that we can visualize the result using directly on the driver program, :\n\nSo what is this code doing? Let\u2019s look particularly at the function which does the work most directly related to Spark. First, we load up a CSV file into an RDD, then map the function to it. The function parses each line of text using the module and returns a tuple that represents the row. Finally we pass the action to the RDD, which brings the data from the RDD back to the driver as a Python list. In this case, is a small jump table that will allow us to join airline codes with the airline full name. We will store this jump table as a Python dictionary and then broadcast it to every node in the cluster using .\n\nNext, the function loads the much larger . After splitting the CSV rows, we map the function to the CSV row, which converts dates and times to Python dates and times, and casts floating point numbers appropriately. It also stores the row as a called for efficient ease of use.\n\nWith an RDD of objects in hand, we map an anonymous function that transforms the RDD to a series of key-value pairs where the key is the name of the airline and the value is the sum of the arrival and departure delays. Each airline has its delay summed together using the action and the operator, and this RDD is collected back to the driver (again the number airlines in the data is relatively small). Finally the delays are sorted in ascending order, then the output is printed to the console as well as visualized using .\n\nThis example is kind of long, but hopefully it illustrates the interplay of the cluster and the driver program (sending out for analytics, then bringing results back to the driver) as well as the role of Python code in a Spark application.\n\nAlthough far from a complete introduction to Spark, we hope that you have a better feel for what Spark is, and how to conduct fast, in-memory distributed computing with Python. At the very least, you should be able to get Spark up and running and start exploring data either on your local machine in stand alone mode or via Amazon EC2. You should even be able to get iPython notebook set up and configured to run Spark!\n\nSpark doesn\u2019t solve the distributed storage problem (usually Spark gets its data from HDFS), but it does provide a rich functional programming API for distributed computation. This framework is built upon the idea of resilient distributed datasets or \u201cRDDs\u201d for short. RDDs are a programming abstraction that represents a partitioned collection of objects, allowing for distributed operations to be performed upon them. RDDs are fault-tolerant (the resilient part) and, most importantly, can be stored in memory on worker nodes for immediate reuse. In memory storage provides for faster and more easily expressed iterative algorithms as well as enabling real-time interactive analyses.\n\nBecause the Spark library has an API available in Python, Scala, and Java, as well as built-in modules for machine learning, streaming data, graph algorithms, and SQL-like queries; it has rapidly become one of the most important distributed computation frameworks that exists today. When coupled with YARN, Spark serves to augment not replace existing Hadoop clusters and will be an important part of Big Data in the future, opening up new avenues of data science exploration.\n\nHopefully you\u2019ve enjoyed this post! Writing never happens in a vacuum, so here are a few helpful links that helped me write the post; ones that you might want to review to explore Spark further.\n\nThis was more of an introductory post than is typical for District Data Labs articles , but there are some data and code associated with the introduction that you can find here:\n\nLike Hadoop, Spark has some fundamental papers that I believe should be required reading for serious data scientists that need to do distributed computing on large data sets. The first is a workshop paper from HotOS (hot topics in operating systems) that describes Spark in an easily understandable fashion. The second is a more theoretical paper that describes RDDs in detail."
    },
    {
        "url": "https://medium.com/district-data-labs/creating-a-hadoop-pseudo-distributed-environment-3b59cfa7aa04",
        "title": "Creating a Hadoop Pseudo-Distributed Environment \u2013 District Data Labs \u2013",
        "text": "Hadoop developers usually test their scripts and code on a pseudo-distributed environment (also known as a single node setup), which is a virtual machine that runs all of the Hadoop daemons simultaneously on a single machine. This allows you to quickly write scripts and test them on limited data sets without having to connect to a remote cluster or pay the expense of EC2. If you\u2019re learning Hadoop, you\u2019ll probably also want to set up a pseudo-distributed environment to facilitate your understanding of the various Hadoop daemons.\n\nThese instructions will help you install a pseudo-distributed environment with Hadoop 2.5.2 on Ubuntu 14.04.\n\nThere are a couple of options that will allow you to quickly get up and running if you are not familiar with systems administration on Linux or do not wish to work through the process of installing Hadoop yourself. District Data Labs has provided a Virtual Machine Disk (VMDK) configured exactly as the instructions below, available for you to download directly. You can then use this VMDK in the virtualization software of your choice (e.g. VirtualBox or VMWare Fusion). Alternatively both Hortonworks and Cloudera supply virtual machines for quick download. Be aware that if you do use Cloudera or Hortonworks distributions, then the environment may be subtly different than the one described below.\n\nClick here to download the VMDK we have put together.\n\nIf you are using the VMDK supplied by District Data Labs, log in to the machine using the username and password as follows:\n\nIf you\u2019re brave enough to set up the environment yourself, go ahead and move to the next section!\n\nBefore you can get started installing Hadoop, you\u2019ll need to have a Linux environment configured and ready to use. These instructions assume that you can get an Ubuntu 14.04 distribution installed on the machine of your choice, either in a dual booted configuration or using a virtual machine. Using Ubuntu Server or Ubuntu Desktop is left to your preference, since you\u2019ll also need to be familiar working with the command line. Personally, I prefer to use Ubuntu Server since it\u2019s more lightweight, and SSH into it from my host operating system.\n\nMake sure your system is fully up-to-date with the required by running the following commands:\n\nIn order to secure our Hadoop services, we will make sure that Hadoop is run as a Hadoop-specific user and group. This user would be able to initiate SSH connections to other nodes in a cluster, but not have administrative access to do damage to the operating system upon which the service was running. Implementing Linux permissions also helps secure HDFS and is the start of preparing a secure computing cluster.\n\nThis tutorial is not meant for operational implementation. However, as a data scientist, these permissions may save you some headache in the long run, so it is helpful to have the permissions in place on your development environment. This will also ensure that the Hadoop installation is separate from other software applications and will help organize the maintenance of the machine.\n\nCreate the user and group, then add the user to the Hadoop group:\n\nOnce you have logged out and logged back in (or restarted the machine) you should be able to see that you\u2019ve been added to the Hadoop group by issuing the command. Note that the flag creates a system user without a home directory.\n\nSSH is required and must be installed on your system to use Hadoop (and to better manage the virtual environment, especially if you\u2019re using a headless Ubuntu). Generate some ssh keys for the Hadoop user by issuing the following commands:\n\nSimply hit enter at all the prompts to accept the defaults and to create a key that does not require a password to authenticate (this is required for Hadoop). In order to allow the key to be used to SSH into the box, copy the public key to the authorized_keys file with the following command:\n\nYou should be able to download this key and use it to SSH into the Ubuntu environment. To test the SSH key issue the following command:\n\nIf this completes successfully without asking you for a password, then you have successfully configured SSH for Hadoop. Exit the SSH window by typing . You should be returned back to the user. Exit the Hadoop user by typing again, you should now be in a terminal window that says .\n\nHadoop and most of the Hadoop ecosystem require Java to run. Hadoop requires a minimum of Oracle Java\u2122 1.6.x or greater and used to recommend particular versions of Java\u2122 to use with Hadoop. Now, Hadoop maintains a reporting of the various JDKs that work well with Hadoop. Ubuntu does not maintain an Oracle JDK in Ubuntu repositories because it is proprietary code, so instead we will install OpenJDK. For more information on supported Java\u2122 versions, see Hadoop Java Versionsand for information about installing different versions on Ubuntu, please see Installing Java on Ubuntu.\n\nDo a quick check to ensure the right version of Java\u2122 is installed:\n\nHadoop is currently built and tested on both OpenJDK and Oracle\u2019s JDK/JRE.\n\nIt has been reported for a while now that Hadoop running on Ubuntu has a conflict with IPv6, and ever since Hadoop 0.20, Ubuntu users have been disabling IPv6 on their clustered boxes. It is unclear whether or not this is still a bug in the latest versions of Hadoop, however in a single-node or pseudo-distributed environment we will have no need for IPv6, so it is best to simply disable it and not worry about any potential problems.\n\nEdit the file by executing the following lines of code:\n\nThen add the following lines to the end of the file:\n\nFor this change to take effect, reboot your computer. Once it has rebooted check the status with the following command:\n\nIf the output is 0, then IPv6 is enabled. If it is 1, then we have successfully disabled IPv6.\n\nTo get Hadoop, you\u2019ll need to download the release of your choice from one of the Apache Download Mirrors. These instructions will download the current stable version of Hadoop with YARN at the time of this writing, Hadoop 2.5.2.\n\nAfter you\u2019ve selected a mirror, type the following commands into a Terminal window, replacing with the mirror URL that you selected and that is best for your region:\n\nYou can verify the download by ensuring that the matches the md5sum which should also be available at the mirror:\n\nOf course, you can use any mechanism you wish to download Hadoop \u2014 or a browser will work just fine.\n\nAfter obtaining the compressed tarball, the next step is to unpack it. You can use an Archive Manager or simply follow the instructions that follow next. The most significant decision that you have to make is where to unpack Hadoop to.\n\nThe Linux operating system depends upon a hierarchical directory structure to function. At the root, many directories that you\u2019ve heard of have specific purposes:\n\nYou can read more about these directories in this Stack Exchange post.\n\nA good choice to move Hadoop to is the and directories.\n\nThese commands unpack Hadoop, move it to the service directory where we will keep all of our Hadoop and cluster services, and then set permissions. Finally, we create a to the version of Hadoop that we would like to use, this will make it easy to upgrade our Hadoop distribution in the future.\n\nIn order to ensure everything executes correctly, we are going to set some environment variables so that Hadoop executes in its correct context. Enter the following command on the command line to open up a text editor with the profile of the user to change the environment variables.\n\nAdd the following lines to this file:\n\nWe\u2019ll also add some convenience functionality to the student user environment. Open the student user bash profile file with the following command:\n\nAdd the following contents to that file:\n\nThese simple aliases may save you a lot of typing in the long run! Feel free to add any other helpers that you think might be useful in your development work.\n\nCheck that your environment configuration has worked by running a Hadoop command:\n\nIf that ran with no errors and displayed an output similar to the one above, then everything has been configured correctly up to this point.\n\nThe penultimate step to setting up Hadoop as a pseudo-distributed node is to edit configuration files for the Hadoop environment, the MapReduce site, the HDFS site, and the YARN site. This will mostly entail configuration file editing.\n\nEdit the hadoop-env.sh file by entering the following on the command line.\n\nThe most important part of this configuration is to change the following line:\n\nReplace the with the following:\n\nEdit the MapReduce site configuration following by copying the template then opening the file for editing:\n\nReplace the with the following:\n\nNow edit the HDFS site configuration by editing the following file:\n\nReplace the with the following:\n\nAnd update the configuration as follows:\n\nWith these files edited, Hadoop should be fully configured as a pseudo-distributed environment.\n\nThe final step before we can turn Hadoop on is to format the namenode. The namenode is in charge of HDFS, the distributed file system. The namenode on this machine is going to keep its files in the directory. We need to initialize this directory and then format the namenode to properly use it.\n\nYou should see a bunch of Java messages scrolling down the page if the namenode has executed successfully. There should be directories inside of the directory, including a directory. If that is what you see, then Hadoop should be all set up and ready to use!\n\nAt this point we can start and run our Hadoop daemons. When you formatted the namenode, you switched to being the user with the command. If you're still that user, go ahead and execute the following commands:\n\nThe daemons should start up and issue messages about where they are logging to and other important information. If you get asked about your SSH key, just type at the prompt. You can see the processes that are running via the command:\n\nIf the processes are not running, then something has gone wrong. You can also access the Hadoop cluster administration site by opening a browser and point it to http://localhost:8088. This should bring up a page with the Hadoop logo and a table of applications.\n\nTo wrap up the configuration, prepare a space on HDFS for our account to store data and to run analytical jobs on:\n\nYou can now exit from the user's shell with the command.\n\nIf you reboot your machine, the Hadoop daemons will stop running and will not automatically be restarted. If you are attempting to run a Hadoop command and you get a \u201cconnection refused\u201d message, it is likely because the daemons are not running. You can check this by issuing the command as sudo:\n\nTo restart Hadoop in the case that it shuts down, issue the following commands:\n\nThe processes should start up again as the dedicated user and you'll be back on your way!\n\nFor the most part, installing services on Hadoop (e.g. Hive, HBase, or others) will consist of the following in the environment we have set up:\n\nHive also follows this pattern. Find the Hive release you wish to download from the Apache Hive downloads page. At the time of this writing, Hive release 0.14.0 is current. Once you have selected a mirror, download the file to your directory. Then issue the following commands in the terminal to unpack it:\n\nEdit your with these environment variables by adding the following to the bottom of the :\n\nNo other configuration for Hive is required, although you can find other configuration details in including the Hive environment shell file and the Hive site configuration XML.\n\nInstalling Spark is also pretty straight forward, and we\u2019ll install it similarly to how we installed Hive. Find the Spark release you wish to download from the Apache Spark downloads page. The Spark release at the time of this writing is 1.1.0. You should choose the package type \u201cPre-built for Hadoop 2.4\u201d and the download type should be \u201cDirect Download\u201d. Then unpack it as follows:\n\nEdit your with the following environment variables at the bottom of the file:\n\nAfter you source your or restart your terminal, you should be able to run a interpreter locally. You can now use and commands to run Spark jobs.\n\nAt this point you should now have a fully configured Hadoop setup ready for development in pseudo-distributed mode on Ubuntu with HDFS, MapReduce on YARN, Hive, and Spark all ready to go as well as a simple methodology for installing other services."
    },
    {
        "url": "https://medium.com/district-data-labs/simple-csv-data-wrangling-with-python-3496aa5d0a5e",
        "title": "Simple CSV Data Wrangling with Python \u2013 District Data Labs \u2013",
        "text": "I wanted to write a quick post today about a task that most of us do routinely but often think very little about \u2014 loading CSV (comma-separated value) data into Python. This simple action has a variety of obstacles that need to be overcome due to the nature of serialization and data transfer. In fact, I\u2019m routinely surprised how often I have to jump through hoops to deal with this type of data, when it feels like it should be as easy as JSON or other serialization formats.\n\nThe basic problem is this: CSVs have inherent schemas. In fact, most of the CSVs that I work with are dumps from a database. While the database can maintain schema information alongside the data, the scheme is lost when serializing to disk. Worse, if the dump is denormalized (a join of two tables), then the relationships are also lost, making it harder to extract entities. Although a header row can give us the names of the fields in the file, it won\u2019t give us the type, and there is nothing structural about the serialization format (like there is with JSON) that we can infer the type from.\n\nThat said, I love CSVs. CSVs are a compact data format \u2014 one row, one record. CSVs can be grown to massive sizes without cause for concern. I don\u2019t flinch when reading 4 GB CSV files with Python because they can be split into multiple files, read one row at a time for memory efficiency, and multiprocessed with seeks to speed up the job. This is in stark contrast to JSON or XML, which have to be read completely from end to end in order to get the full data (JSON has to be completely loaded into memory and with XML you have to use a streaming parser like SAX).\n\nCSVs are the file format of choice for big data appliances like Hadoop for good reason. If you can get past encoding issues, extra dependencies, schema inference, and typing; CSVs are a great serialization format. In this post, I will provide you with a series of pro tips that I have discovered for using and wrangling CSV data.\n\nSpecifically, this post will cover the following:\n\nAlthough we won\u2019t cover it in this post, using these techniques you have a great start towards multiprocessing to quickly dig through a CSV file from many different positions in it at once. Hopefully this intro has made CSV sound more exciting, and so let\u2019s dive in.\n\nConsider the following data set, a listing of company funding records as reported by TechCrunch. You can download the data set here.\n\nThe first ten rows are shown below:\n\nThe CSV file has a header row, so we have the field names, but we do have a couple of data type conversions that we have to make. In particular, the needs to be transformed to a Python date object and the needs to be converted to an integer. This isn't particularly onerous, but consider that this is just a simple example, more complex conversions can be easily imagined.\n\nNote: This file was obtained from the following source SpatialKey Sample Data on October 21, 2014.\n\nPython has a built in module that handles all the ins and outs of processing CSV files, from dealing with dialects (Excel, anyone?) to quoting fields that may contain the delimiter to handling a variety of delimiters. Very simply, this is how you would read all the data from the funding CSV file:\n\nA couple of key points with the code above:\n\nThis code allows you to treat the data source as just another iterator or list in your code. In fact if you do the following:\n\nYou\u2019ll see that the function \u201creturns\u201d a generator, thanks to the statement in the function definition. This means, among other things, that the data is evaluated lazily. The file is not opened, read, or parsed until you need it. No more than one row of the file is in memory at any given time. The context manager in the function also ensures that the file handle is closed when you've finished reading the data, even in the face of an exception elsewhere in code. Note that this pattern requires the file to be open while you're reading from it. If you attempt to read from a data file that's closed, you will get an error.\n\nThis is powerful because it means that even for much larger data sets you will have efficient, portable code. Moreover, as we start looking at wrangling or munging the data from the CSV file, you\u2019ll have well encapsulated code that can handle a variety of situations. In fact, in code that has to read and parse files from a variety of sources, it is common to wrap the module in a class so that you can persist statistics about the data and provide multiple reads to the CSV file. In the above example you have to ensure that you call the function to get the data every time you want to read or do any computation, whereas a class can save some state of the data between reads. Here is an example of persisted data for our funding class:\n\nThis is a pretty simple class that keeps track of the number of rows overall as well as the number of rows for each company. All that\u2019s stored in memory is the company name and the number of funding records, but you can see how this might be extremely useful, especially if you\u2019re going to filter the data by company. No matter what, you have to make one complete read to get this data, but on all passes of the data this is stored and cached. So when we ask for the length of the data at the end, the file is completely read, and when we ask for the number of companies, we don\u2019t return to disk.\n\nThe module is excellent for reading and writing CSV files, is memory efficient (providing an iterator that reads only one line from the file into memory at a time), and comes included with Python. Why would you need anything else?\n\nThe thing is, I regularly see this in the code of my colleagues (especially those that do research):\n\nAlthough rarer, I also see things like also used as a production CSV reader.\n\nThe utilities for analysis that Pandas gives you, especially DataFrames, are extremely useful, and there is obviously a 1:1 relationship between DataFrames and CSV files. I routinely use Pandas for data analyses, quick insights, and even data wrangling of smaller files. The problem is that Pandas is not meant for production-level ingestion or wrangling systems. It is meant for data analysis that can happen completely in memory. As such, when you run this line of code, the entirety of the CSV file is loaded into memory. Likewise, Numpy arrays are also immutable data types that are completely loaded into memory. You\u2019ve just lost your memory efficiency, especially for larger data sets.\n\nAnd while this may be fine on your Macbook Pro, keep in mind that if you\u2019re writing data pipeline code, it will probably be run more routinely on a virtual cloud server such as Rackspace, AWS, or Google App Engine. Since you have a budget, it will also probably be running on small or micro servers that might have 1 GB of memory, if you\u2019re lucky. You don\u2019t want to blow it away!\n\nThen there is the issue of dependency management. The Pandas library depends on Numpy, which itself takes a lot of compilation to put together. On my Macbook Pro, pip installing Pandas into a takes about 5 minutes. This is a lot of overhead when you're not planning on using DataFrames for their potential!\n\nSo don\u2019t do it. Use the native module.\n\nText encoding is the bane of my existence in so many ways. Mismatched encoding can crash entire programs that have been continually running for a long time and passed unit tests. Even if you handle encoding, if you chose the wrong encoding scheme (usually ), when something comes in, you can get invisible errors that don't raise any exceptions. Encoding is always something you should consider when opening up a file from anywhere.\n\nIf you\u2019re lucky enough to be able to use Python 3, you can skip this section. But as mentioned previously, production ingestion and wrangling systems are usually run on micro or small servers in the cloud, usually on the system Python. So if you\u2019re like me and many other data scientists, you\u2019re using Python 2.6+ which is currently what ships with Linux servers.\n\nIf you look at the module documentation, they suggest adding three classes to encode, read, and write unicode data from a file passed to the csv module as a wrapped function. These encoders translate the native file encoding to , which the module can read because it's 8-bit safe. I won't include those classes here. Instead, you should always do the following when you import the csv module.\n\nThis is a very small dependency that can be easily fetched via and you can find this module in pretty much every one of my data product repository's files. To learn more, visit the PyPI page for the latest version: unicodecsv 0.9.4.\n\nNow that we can easily extract data from a CSV file with a memory efficient and encoding-tolerant method, we can begin to look at actually wrangling our data. CSVs don\u2019t provide a lot in terms of types or indications about how to deal with the data that is coming in as strings. As noted previously, if there is a header line, we may get information about the various field names, but that\u2019s about it. We have already parsed our rows into Python dictionaries using the built in , but this definitely depends on the header row being there. If the header row is not there, then you can pass a list of field names to the and you can get dictionaries back.\n\nDictionaries are great because you can access your data by name rather than by position, and these names may help you parse the data that\u2019s coming in. However, dictionaries aren\u2019t schemas. They are key-value pairs that are completely mutable. When using a single CSV file, you may expect to have dictionaries with all the same keys, but these keys aren\u2019t recorded anywhere in your code, making your code dependent on the structure of the CSV file in an implicit way.\n\nMoreover, dictionaries have a bit more overhead associated with them in order to make them mutable. They use more memory when instantiated and in order to provide fast lookups, they are hashed (unordered). They are great for when you don\u2019t know what data is coming in, but when you do, you can use something much better: namedtuples.\n\nRunning this code produces a result as shown below. As you can see the result is a tuple (an ordered set of values) that have names associated with them, much like the keys of a dictionary. Not only that, but the tuple is typed \u2014 it is not only a variable named \u201cFundingRecord\u201d, each row is a .\n\nThe function actually returns a subclass of tuple with the type specified by the typename, the first argument to the function. The type that is returned is immutable once constructed and is extremely lightweight because it contains no internal . It is very similar to using for memory performance and efficiency. As you can see from the above code, you would use the just like any other object, so long as you didn't modify it in place.\n\nThis leads to an obvious question, how can you parse the data into various types if you can\u2019t assign it back to the object? Well, the nice thing is that the return from is an actual type, one that you can subclass to add methods to!\n\nIf you run this code, you\u2019ll see a result as follows:\n\nWe now have a simple, self-documenting methodology for constructing schemas and parsers for CSVs in code that is not only memory efficient and fast but also encapsulates all the work we might want to do on a single record in the CSV file.\n\nKeep in mind that the class is immutable, and therefore read only!\n\nAs a side example, I tested the performance in both time and space using a DictReader vs using a NamedTuple. The tests operated on a 548 MB CSV file that contained 5,000,000 rows consisting of a tuple that met the schema (uuid, name, data, page, latitude, longitude). I loaded the entire data set into memory by appending each record (either dict or namedtuple) to a list of records and then monitored the memory usage of the Python process. The code I used is below.\n\nThe result I got on my Macbook Pro with an 2.8GHz Intel i7 processor with 16 GB of memory was as follows:\n\nThe namedtuple benchmark code is similar and is as follows:\n\nRunning this code on the same data, on the same machine gave me the following result:\n\nIn terms of memory usage, the named tuples also used significantly less memory. Here is a summary of the overall performance:\n\nUsing dictionaries is almost twice as slow, if a little more generic. Worse, using dictionaries takes almost three times the memory because of the aforementioned overhead of a mutable vs. immutable type. These small adjustments can mean the difference between performant applications and searching for new solutions.\n\nThese challenges with CSV as a default serialization format may prompt us to find a different solution, especially as we propagate data throughout our data pipelines. My suggestion is to use a binary serialization format like Avro, which stores the schema alongside compressed data. Because the file is stored with a binary seralization, it is as compact as and can be read in a memory efficient manner. Because the data is stored with its schema, it can be read from any code and immediately extracted and automatically parsed.\n\nSo let\u2019s return to our original example with the data set. In order to convert our parsed rows, we need to first define a schema. Schemas in Avro are simply JSON documents that define the field names along with their types. The schema for a FundingRecord is as follows:\n\nSave this schema in a file called . To save out our parsed CSV to an Avro serialized format, use the following code:\n\nUnfortunately, Avro doesn\u2019t currently support native objects in Python but hopefully it will soon. Otherwise we simply convert our back into a dictionary (or just use the directly) and pass that to the Avro serializer.\n\nOnce saved to disk, the savings become apparent. The original file was approximately 92K. After serializing with Avro, we have compressed our data to 64K! Not only do we have a compressed file format, but we also have the schema associated with the data. Reading the data is pretty easy now, and requires no parsing as before.\n\nAs you can see, the data in the dictionary has already been converted (at least the two integers) and no extra parsing is required. Avro will also enforce schema constraints and invariants to ensure that your wrangling is successful without things like databases for normalization.\n\nNot only that, but Avro is a standard for Apache Hadoop. If you\u2019re going to be loading this data into HDFS, it\u2019s a good idea to consider loading it as Avro sequence file formats rather than CSV formats. It can make your life a lot easier, especially if you\u2019re using Hadoop streaming.\n\nIn this post, we looked several issues that arise when wrangling CSV data in Python. First, we reviewed the basics of CSV processing in Python, taking a look at the module and how that compared to Pandas and Numpy for importing and wrangling data stored in CSV files. Next, we highlighted the importance of encoding and how to avoid unicode issues. Then we took a look at the differences between dictionaries and namedtuples, and we saw how namedtuples are better when you know what data you're ingesting. And finally, we covered how to serialize CSV data with Avro so that the data is stored not only in a compact format but with its schema as well.\n\nHopefully, what you\u2019ll take away from this post is a basic data wrangling process that transforms data serialized input into an application-specific or data model usable form. The process uses a lot of transformations to and from flat files on disk, but in batch is far faster than accessing databases directly. If you have CSVs in multiple formats, this strategy can come in very handy to normalize data into a backwards-compatible safe methodology for storing and accessing data in WORM storage.\n\nThe code files to accompany this post can be found on Github.\n\nFinally, below are some additional resources where you can continue exploring some of the stuff we\u2019ve covered.\n\nI\u2019d like to thank Tony Ojeda for his help with preparing and producing this post. This started as an iPython notebook full of tips for some colleagues and classmates, and without his help it would not be the polished post that you see in front of you now!"
    },
    {
        "url": "https://medium.com/district-data-labs/conditional-probability-with-r-5544c6886621",
        "title": "Conditional Probability with R \u2013 District Data Labs \u2013",
        "text": "In addition to regular probability, we often want to figure out how probability is affected by observing some event. For example, the NFL season is rife with possibilities. From the beginning of each season, fans start trying to figure out how likely it is that their favorite team will make the playoffs. After every game the team plays, these probabilities change based on whether they won or lost. This post won\u2019t speak to how these probabilities are updated. That\u2019s the subject for a future post on Bayesian statistics. What we will explore is the concept of conditional probability, which is the probability of seeing some event knowing that some other event has actually occurred.\n\nSome more examples of where we might encounter such conditional probabilities:\n\nThe flu season is rapidly approaching. Each of us have some probability of getting the flu, which can be naively computed as the number of cases of flu last year divided by the number of people potentially exposed to the flu virus in that same year. Let\u2019s call this probability .\n\nIf a person gets a flu vaccination, their chance of getting the flu should change. This would be denoted as , and is read as \"probability of getting the flu given you have been vaccinated.\" Because of the \"been vaccinated\" condition, this is a conditional probability.\n\nSo how do you compute a conditional probability? There is a basic equation that defines this:\n\nwhich also tells us that\n\nis often called the joint probability of A and B, and and are often called the marginal probabilities of A and B, respectively.\n\nAdapting the equations above to our flu example,\n\nThe numerator is the probability that a person gets the vaccine and the flu; the denominator is the probability that a person gets the vaccine.\n\nLet\u2019s look at a table of hypothetical frequencies for a population:\n\nSo we have:\n\nPlugging in the conditions (A, B, C, & D) from our table above:\n\nNext, we will swap out the the different conditions (A B C D) with numbers so that we can calculate an answer!\n\nPlugging in the numbers in our new table:\n\nSo this probability is the chance of getting the flu only among those who were vaccinated. We have normalized the probability of an event (getting the flu) to the conditioning event (getting vaccinated) rather than to the entire sample space.\n\nChallenge Question: According to the table above, what is the probability of getting the flu if you weren\u2019t vaccinated ? What is the probability of getting the flu in general?\n\nThere is another way of looking at conditional probability. If we don\u2019t know anything about event B, P(A) is the size of the light blue circle within the entire sample space (denoted by the rectangle).\n\nIf we know that the conditioning event B has happened, the probability of the event A now becomes the ratio of the light blue section to the light and dark blue section. We can then make our sample space of interest the space where event B occurs.\n\nWe can compare the probability of an event (A) and how it changes if we know that another event (B) has happened. How does the chance of catching flu (A) change if you\u2019re vaccinated (B)? How does a football team\u2019s chance of going to the playoffs (A) change if the quarterback is injured (B)? In both these cases, we think those chances will change.\n\nBut will the chance of the Pittsburgh Steelers beating New England Patriots (sacrilegious to some, I know) in the 4 pm game depend on the Seattle Seahawks beating the San Francisco 49ers (caveat: I\u2019m from Seattle) during the same time? We think (and hope) not.\n\nWhen knowledge of one event does not change the probability of another event happening, the two events are called statistically independent. We see a lot of things that are independent in this sense. Successive tosses of a coin are independent, or so we believe. So are successive dice rolls and slot machine plays.\n\nStatistical independence has some mathematical consequences. It implies that\n\nwhich directly implies, from the definition, that\n\nThis means that we can compute the probability of two independent events happening together by merely multiplying the individual probabilities.\n\nCaution: You\u2019ll often find probabilities of joint events like this computed as the product of the individual events. However, this is only true if the assumption of statistical independence is valid. Often times, it is not, and so you must be careful interpreting such computations.\n\nLet\u2019s do a little experiment in R. We\u2019ll toss two fair dice, just as we did in an earlier post, and see if the results of the two dice are independent. We first roll the dice 100,000 times, and then compute the joint distribution of the results of the rolls from the two dice.\n\nLet\u2019s evaluate the probability that both with and without knowledge of . If we don't observe , that probability is:\n\nIf we know that , then the conditional probability that given is:\n\nThese results are very close.\n\nNote: R makes it very easy to do conditional probability evaluations. In R, you can restrict yourself to those observations of when by specifying a Boolean condition as the index of the vector, as .\n\nIf we assumed that the results from the two dice are statistically independent, we would \n\nhave, for every pair of values i,j in 1,2,3,4,5,6:\n\nWe computed the first part earlier from .\n\nNow for the second part.\n\nWe see that and are quite close, indicating that the rolls of the two dice are probably independent.\n\nOne statistical test for testing independence of two frequency distributions (which means that for any two values of x and y, their joint probability is the product of the marginal probabilities) is the Chi-squared test. In R, this is implemented by the function .\n\nWe see that the p-value of this test is quite large, indicating that there is insufficient evidence to suggest that x and y are not independent.\n\nChallenge question: If two events cannot occur together (they are mutually exclusive) can they be independent?\n\nUnderstanding how conditional probabilities change as information is acquired is part of the central dogma of the Bayesian paradigm. That paradigm is based on Bayes\u2019 theorem, which is nothing but a theorem of conditional probabilities.\n\nThis provides the mathematical framework for understanding how A affects B if we know something about how B affects A.\n\nRearranging this formula provides a bit more insight:\n\nIn other words, how knowledge of B changes the probability of A is the same as how knowledge of A changes the probability of B, at least as a ratio. If A and B are independent, this ratio is 1.\n\nSuppose we have a test for the flu that is positive 90% of the time when tested on a flu patient , and is negative 95% of the time when tested on a healthy person . We also know that the flu is affecting about 1% of the population . You go to the doctor and test positive. What is the chance that you truly have the flu?\n\nYou can answer this question directly using Bayes\u2019 theorem, but we\u2019ll tackle this a bit differently. We\u2019ll create a hypothetical population of 100,000 people, and see if we can figure this out.\n\nIn the above code we first simulate who has the flu, given on average 1% of the population gets the flu. We then find out whom among those without the flu would test positive, based on . Recall that the when considering a conditioning event, the conditioning event is considered the sample space, and so all the laws of probability hold within that space.\n\nWe do a similar computation for the people with flu.\n\nThe question we are asking, what is the chance that you have the flu given that you tested positive, can then be directly answered as:\n\nWow! Even though the test is pretty good, the chance that we actually have the flu even if we test positive is actually pretty small. This is because the chance of actually getting the flu is pretty small in the first place. However, if we look at how much our chance of having the flu changed with a positive test, it is quite large:\n\nThat is, the knowledge that we tested positive increased our chance of truly having the flu 15-fold! A constant issue in medicine is if we should address the absolute increase in risk (1% to 15%) or the relative risk (15-fold) when deciding on best clinical practice.\n\nIf we calculate the probability using Bayes\u2019 theorem, we get a very similar result:\n\nConditional probabilities and Bayes\u2019 theorem have many everyday applications such as determining the risk of our investments, what the weather will be like this weekend, and what our medical test results mean. These concepts are central to understanding the consequences of our actions and how relationships between entities can affect outcomes. With recent increases in the amount and availability of data, understanding these concepts become essential for making informed, data-driven decisions. In this post, we reviewed how to formally look at conditional probabilities, what rules they follow, how to use those rules along with Bayes\u2019 theorem to figure out the conditional probabilities of events, and even how to \u201cflip\u201d them.\n\nBelow are some additional resources that you can use to continue to build on what we\u2019ve covered here."
    },
    {
        "url": "https://medium.com/district-data-labs/computing-a-bayesian-estimate-of-star-rating-means-651496a890ab",
        "title": "Computing a Bayesian Estimate of Star Rating Means \u2013 District Data Labs \u2013",
        "text": "Consumers rely on the collective intelligence of other consumers to protect themselves from coffee pots that break at the first sign of water, eating bad food at the wrong restaurant, and stunning flops at the theater. Although occasionally there are metrics like Rotten Tomatoes, we primarily prejudge products we would like to consume through a simple 5 star rating. This methodology is powerful, because not only does it provide a simple, easily understandable metric, but people are generally willing to reliably cast a vote by clicking a star rating without too much angst.\n\nIn aggregate, this is wonderful. Star ratings can minimize individual preference because the scale is not large enough to be too nuanced. After enough ratings, it becomes pretty clear whether or not something is a hit or a flop. The problem is, however, that many ratings are needed to make this system work because the quality of a 5 star rating depends not only on the average number of stars but also on the number of reviews.\n\nConsider the following two items:\n\nWhich item should be listed first in a list of items sorted by rating? It would seem clear that the Brew Central has more reviewers and that many of them are happy with the product. Therefore, it should probably appear before the Presto even though its average rating is lower.\n\nWe could brainstorm a couple of strategies that would give us this outcome and would help us deal with items that have an insufficient number of ratings when we come across them in the future:\n\nInstead, we need some empirical metric for the average rating that embeds the number of reviewers into the score. Most methodologies in use today use Bayesian estimation, which takes into account the fact that we don\u2019t have enough data to make an estimation via the mean and also incorporates all the data we have about other observations.\n\nIn this post, we will consider how to implement a Bayesian estimation of the mean of star reviews with a limited number of observations. The estimation is useful for recommender services and other predictive algorithms that use preference space measures like star reviews. It is also a good crash course in Bayesian estimation in general for those that are interested.\n\nBefore we get going, I would like to unashamedly cite Paul Masurel\u2019s article Of Bayesian Average and Star Ratings as the inspiration for this post. This post will review some of his work and look more specifically at the Movielens data set of star reviews. If you\u2019re interested in more math, please read his post!\n\nIn this post, we will be working with the MovieLens data set, which contains 100,000 ratings from 1,000 users on 1,700 movies. This data set is freely available for download from GroupLens.org, the official site of Social Computing Research at the University of Minnesota. A simple download is all that\u2019s required for ingestion, but some wrangling needs to occur.\n\nThe first thing we need to do is join the and the data sets into a single, denormalized data set that contains the , , ,and fields. We can use easily enough to do this for us. We won't need to do this more than once, so we can simply use an interpreter to perform our data wrangling. In the same directory as your data, type to open up the interpreter.\n\nThis block of code loads two Pandas from the the TSV (tab separated values) file which contains user and ratings information and the file which contains movie information and is pipe delimited. Using the function we can left join the two data sets on the to produce a denormalized data set that we write out to another CSV file.\n\nWe will use the resulting file for the rest of our analyses. To ensure that we can create a quality and extensible analysis of the movie ratings, create a file to add all of your Python code. Moreover, we will create a class to wrap all the work we expect to do on these ratings. In a file called ,add the following code:\n\nAfter importing all the needed libraries (the repository for this post contains a file with all of the external dependencies), this code creates a relative path to the CSV file, presuming that the CSV and the Python script are in the same directory. Our class initially does two things. First, it loads the data into a and when printed, it reports the first several rows of that file. Using the block of code, we can test our work, execute the file by typing on the command line. If everything is working you should see the first few rows from the ratings CSV.\n\nWith the logistics out of the way it\u2019s time to begin our analysis. Let\u2019s find the top 10 movies by average rating. Add the following methods to the class:\n\nWhen you execute again, it is quickly apparent that there is a problem with our current approach. Movies with high average ratings but very few reviewers are listed first. The top movies (Aiqing Wansui (1994), Star Kid (1997), Santa with Muscles (1996), and more) all have a mean rating of 5 stars but also significantly less than 10 reviewers each.\n\nIn order to get a feel for our data, let\u2019s visually compare the mean rating to the counts. Add the following code to your class and adapt the statement accordingly:\n\nWhen you plot the simple mean against the number of reviews for the entire movie ratings data set, a pattern begins to clearly emerge. The result of the plot should appear as follows:\n\nThis figure shows that the average rating of a movie is actually slightly higher than 3. There is a strong cluster of movies with approximately 100 reviewers whose simple mean rating surrounds 3.5, not 3. This contrasts with the left side of the figure, where you can clearly see that the mean is 3 when there are very few reviewers.\n\nThe figure visually calls our attention to a problem. When there are less than 10 reviewers, there are dramatic tails that ensure equally high and low reviews. If there are less than 10 reviews, the probability is much higher that a movie will have an average rating of 1.0, whereas after 10 reviews, the figure shows that it is extremely unlikely to see a movie with an average rating less than 1.5 or greater than 4.5.\n\nWe can hypothesize two things from this plot that allow us to make decisions for our specific data set. First, we become confident that a mean rating is good so long as it has between 50 and 120 reviews. Secondly, we know that the average review is between 3.0 and 3.5 when a movie has that number of reviews. Using these two hypotheses, we can come up with a rule that will allow us to rank movies according to both the mean rating the movie has received as well as the number of reviewers that have reviewed it. We can formalize these hypotheses with Bayesian estimation.\n\nBayesian estimation attempts to codify our hypotheses by allowing us to forgo computing a direct value from a limited number of observations and instead creates a probability distribution that describes the observable space more completely. We can then use this probability distribution to come up with a better estimate for our computation. That probability distribution can also be used to assign confidences or bounds that can lead to better decision making.\n\nAs you might (hopefully) remember from your statistics classes, or from this DDL post, in order to estimate probabilities you generate observations.\n\nLet\u2019s look at a simple example. Suppose we want to estimate the likelihood that someone will give a movie a thumbs up. We would initially assign each person a probability and initialize it to probability that they will give the movie a thumbs up. We would then go stand outside a theater where that movie was playing and ask people leaving the theater that saw that movie whether they would give the movie a thumbs up or a thumbs down. The result would be a collection of observations that we would use to update our , making it more accurate. The updated based on , the sequence of observations after tests, is called the posterior distribution.\n\nBayes formula tells us how to compute the probability of given :\n\nis the probability of observation and our prior, , is what we believe the distribution of the probabilities is before seeing any data. When you don't know anything about the probabilities, you can start with the assumption that there is an underlying uniform distribution where all possible values of the random variable are equally likely (other choices are possible, but that's for another day). Therefore, we are only interested in the proportionality relationship between (read \"the probability of given \") and (read \"the probability of given \"). The probability of O given X ( ) is called the likelihood and can be computed by taking the product of the probability of each observation (assuming they are independent):\n\nFor each observation, the probability of a thumbs up is and the probability of something other than a thumbs up is , so if we observe K thumbs up for N observations, we can derive the posterior probability as follows:\n\nThis is called the binomial distribution. As we collect more observations, the distribution becomes increasingly refined and, with that refinement, we can compute an interval (usually called the credible interval) within which the value probably lies.\n\nBack to our thumbs-up and down example; consider that as we\u2019re standing outside of the movie theater. As our first person comes out, we get a thumbs up! Our posterior distribution is based off of K=1, N=1, which is plotted in the first pane of the figure below. After 5 observations we have 4 thumbs up and one thumbs down, after 10 observations we have 7 thumbs up, after 25 we have 18, after 50 we have 37, and after 100 we have 72 (this is a really big theater)! The distributions at each point of the observation are plotted in the panes below:\n\nAs you can see the more observations we get, the more refined our posterior distribution becomes, and the more confident we are in the interval surrounding the value with maximum probability of the distribution. In fact, for this task, I created an observation data set that had a probability of 0.68 that the reviewer would give a thumbs up; given the vagaries of random number generation, the probability of 0.72 after 100 observations provides a posterior that includes a high probability for 0.72.\n\nNote: The code to generate these posterior distributions given some number of observations is available here.\n\nTo use Bayesian estimation to compute the posterior probability for star ratings, we must use a joint distribution. We are not estimating the distribution of some scalar value but, rather, the joint distributions of the probability estimate of whether or not the reviewer will give the movie a 1, 2, 3, 4, or 5 star rating (not just a simple thumbs up or down). In this case, the random variable is a categorical distribution because it can take some value within {1,2,3,4,5} with probabilities as follows:\n\nOur reasoning for the binomial distribution still applies, however, and we can compute the proportionality of the relationship between our probability and observations in a similar way, replacing with , , , , . Our likelihood is still the product of probabilities of each observation, and each individual observation is given by an associated probability. We can compute our posterior probability with N observations for , , , , categories as follows:\n\nThis is not a binomial distribution, but rather a multinomial distribution with a parameter that can be expressed as follows:\n\nIf we include our prior as a distribution of the exact same form in the proportionality equation (e.g. a Dirichlet distribution with parameter \u237a0) we can factorize the distribution as follows:\n\nThis is another Dirichlet distribution with another parameter, \u237a1 that can be characterized as follows:\n\nAnd this is the distribution which we can use to estimate the posterior likelihood of our joint probability given categorical observations, as we did previously using the binomial distribution.\n\nBecause we are primarily interested in the estimate of the average star rating, we can rephrase our problem as, \u201cWhat is the expected value of the average rating given a posterior in the shape of our Dirichlet distribution?\u201d \n\nThe average value of a categorical random variable is the weighted average of the values of the random variable weighted by the respected probability values. In other words, the sum of the probability of getting a star, given our observations, multiplied by that star\u2019s value for each star value from 1 to 5. So, for the categorical variable of star ratings, the average value would be:\n\nThe expected value of the average rating based on the posterior is then computed for our star ratings as follows:\n\nUsing our Dirichlet distribution we can compute the probability of a star value given our observations as the ratio of the Dirichlet parameter for that star to the sum of the Dirichlet parameters:\n\nThis probability can be plugged into our expected value with a little simplification as follows:\n\nTake a minute to digest this formula, and expand out the summations if you need to. There are only 5 things you can sum, one for each star. In his post, Paul Masurel goes into a bit more detail and summarizes the Bayesian average as follows:\n\nThis fits in very well with the context of our hypothesis: is the average rating we can expect for movie, given some minimum number of observations, . Simply eyeballing our hexagonal histogram, we could estimate to be 3.25 and to be 50. The Bayesian average would then be:\n\nReplacing for the number of reviews and and for our confidence and prior respectively. Let's update our class to reflect these new insights:\n\nIf you run the updated code, you should get the following top 10 ratings:\n\nNow that\u2019s much better! This list definitely seems like a list of the most popular movies, and we have significant counts for each movie. Note that the Bayesian estimate is pretty close to the computed mean, but they are not exactly the same. The Bayesian estimate embeds a prior and a likelihood (from observations) to the mean. This means that for movies that have less than ratings, the estimate is strongly influenced by the prior. However, for movies that have more than ratings, the observed mean has more influence than the prior, but the prior still has a little pull. As you can see, each of the estimates is slightly less than the mean, heading towards our likelihood, .\n\nIf you begin to play around with the parameters for our estimation, and you begin to note that it changes the top 10 list slightly. Actually these parameters are very important, especially as we get to the middle of our cluster! Instead of just eyeballing them, we can instead compute the distribution of the Dirichlets from our observations by using a prior that is the number of expected votes for each category.\n\nThis methodology uses a uniform prior of 2 votes for each star rating, which is represented as a list of length 5 (one for each star rating). Then we collect the counts of each vote (e.g. the number of votes for each star rating) and add the prior values to the counts, this gives us our posterior distribution. We compute the weights as the sum of the product of the number of votes and the star value, then return a mean by dividing the weighted sum by the total number of votes received.\n\nNote: Sorry about the complex Python code; all the and functions do is manipulate multiple lists together combining and mapping functions to every value as necessary. If you're unsure what is going on here, a great strategy is to print out each intermediary step to see what the counter looks like, what the votes and posterior look like, etc.\n\nThe top 10 movies if you run this code again now look as follows:\n\nClearly we have achieved the same effect as before: only movies with significant votes are ranked in the top ten. However, using this methodology, the means are significantly closer to the computed mean, which has reordered the top ten list and we don\u2019t have to guess at a prior mean and confidence, but can instead simply use a prior that casts a uniform number of votes to all ratings.\n\nCongratulations on making it through this long post! Hopefully you enjoyed the refresher of Bayesian statistics, and were able to dig into the Python code to analyze the MovieLens data set. At the very least, this post shows a data exploration and analysis technique using class-based Python and a hypothesis that a Bayesian model can be used to estimate or refine star rating predictions. Sometimes there is a disconnect in data science between writing code and doing some sort of statistical modeling, and we hoped to bridge the gap in this post.\n\nSpecifically, in this post we discussed the application of Bayesian techniques to estimate and rank preferences by 5 star rating. We discovered that using a simple mean is not enough because the number of observations contributes to the quality of a 5 star rating. Our hypothesis was that we could use a Bayesian posterior, specifically a Dirichlet posterior on our joint distribution of categorical ratings to refine our estimates given increasingly refined observations. We wrote a lot of code to view the top rated movies in the MovieLens data set according to the techniques we discussed.\n\nCertainly these techniques can be improved further from here. For example, your ranking could implement a cooling factor, where newer votes contribute more significantly to the score of the movie than older votes. This would help restrain summer blockbusters and allow classics to continue to circulate to the top over time because people will forget about the blockbuster, but perhaps continue to rate the classics!\n\nI hope you\u2019ve enjoyed the post! I mentioned a lot of links and folks throughout the post, but have compiled them here for you to review at your leisure.\n\nThe code used in this post can be found on Github at the District Data Labs account, as well as the requirements.txt file that specifies all of the required dependencies:\n\nThe data comes from GroupLens:\n\nThe inspiration for this post was Paul Masurel, and further reading can be found in his post (listed first) as well as the other posts listed below:"
    },
    {
        "url": "https://medium.com/district-data-labs/how-to-develop-quality-python-code-8209af654d79",
        "title": "How to Develop Quality Python Code \u2013 District Data Labs \u2013",
        "text": "Developing in Python is very different from developing in other languages. Python is an interpreted language like Ruby or Perl, so developers are able to use read-evaluate-print loops (REPLs) to execute Python in real-time. This feature of Python means that it can be used for rapid development and prototyping because there is no build process. Python includes many functional programming tools akin to Scala or Javascript to assist with closure based script development. But Python is also a fully scalable object-oriented language, a paradigm used to build large modular software rather than to simply execute scripts, more akin to Java or C++.\n\nPython sits in the middle of these paradigms, providing the best of many worlds. Python is used for writing quick one-off scripts, large scale web frameworks like Django, data processing with Celery, even numerical and scientific computing. Python is lightweight, is standard on many operating systems, and is effective, thereby making it the top choice for data scientists and analysts for data engineering and analytical tasks.\n\nHowever, the breadth of Python means that there is no one workflow to developing with it, and certainly there is no standard IDE or environment framework to make these decisions on your behalf. Most Python educational materials focus on the scripting aspects of the language, leaving out the important details of how to construct larger projects. This post will focus on the question of how a developer interacts with Python to build larger data applications.\n\nSo what do you need in order to successfully develop data apps with Python? Quite simply all you need is:\n\nThat\u2019s it! There are many development environments for using Python that add additional tools to your workflow including debuggers, code completion, and syntax highlighting. However, when it comes down to it, these software programs are simply wrapping the basic text editor and terminal together with some added functionality. If you must use an IDE, I would suggest the following:\n\nHowever, even when using one of these tools, you\u2019ll still probably use the basic workflow described below. Many professional Python developers are content with Sublime Text 3 for its subtly powerful features and syntax highlighting coupled with and the command line. This is what I do, and it will enable you to have truly foo development!\n\nAs your projects grow larger you will also want to include the following tools into your worklow:\n\nThere are many tools to aid in software development, but these three tools are a vital part of modern Python development, and will be discussed further in the rest of this post.\n\nAs you develop Python code, you\u2019ll inevitably start including third party packages, especially if you\u2019re doing data science and require tools like Numpy, Pandas, and others. Building these third party libraries and installing them on your system is typically done with - the python package manager. Make sure that you have installed on your system, it will save you a lot of time and effort!\n\nTo install the Python library, a simple HTTP library that allows you to very easily fetch things from the web, you would simply run the following command:\n\nUninstalling and package management, including upgrading, are also included with the command. By using you can also get a list of the third party Python packages you have installed on your system. To search for various libraries to use in your code, see the Python Package Index (PyPI).\n\nAs you start to develop more code, you\u2019ll start to find that specific versions of tools or tools that are hard to build and maintain are required for specific projects and that they conflict with versions of software in other projects. Even Python can be a problem if you develop for both Python 2 and 3 depending on your deployment environment! More worringly, Python is also a crucial aspect of many operating systems, the (small) possibility exists that you may end up breaking the system Python during development!\n\nThe solution to these problems is to use virtualenv to package a complete development environment with your code. Virtualenv allows you to create a directory that contains a project-specific version of Python, pip, and all third-party dependencies. The virtualenv can be activated and deactivated on the command line, allowing you to create a self-contained environment. Moreover, it can be used to create environments that match your production environment (typically a Linux server).\n\nVirtualenvwrapper is another library that allows you to manage multiple virtual environments and associate them with specific projects. This tool will also quickly become essential to your workflow. To install and , use the following code:\n\nThen edit your in your home directory and add the following to the end:\n\nAll your virtual environments will be stored in a hidden directory called , and your project directory is where you would store your code. We'll discuss this more in the following section. I also alias many of the virtualenv scripts to make it easier to work with, you can see my extensions at Ben's VirtualEnv Cheat Sheet.\n\nNote: Windows users may have to follow OS-specific instructions, which I would be happy to update and include in this post.\n\nCreating and executing Python code follows two different workflow patterns.\n\nGenerally speaking, developers do both. Python programs are intended to be executed on the command line via the binary, and the thing that is executed is usually an entry point to a much larger library of code that is imported. The difference between importing and execution is subtle, but as you do more Python it becomes more important.\n\nWith either of these workflows, you create your code in as modular a fashion as possible and, during the creation process, you execute it in one of the methods described above to check it\u2019s working. Most Python developers are back and forth between their terminal and the editor, and can do fine grained testing of every single line of code as they\u2019re writing it. This is the rapid prototyping aspect of Python.\n\nSo let\u2019s start with a simple example.\n\nLet\u2019s create our first Python script. You can either open your favorite editor and save the file into your workspace (the ~/Projects/myproject directory), or you can touch it and then open that file with your editor.\n\nPRO TIP: If you\u2019re using Sublime Text 3 and have the command line tool installed (See Sublime Text installation instructions), you can use the following command to open up the current directory in the editor:\n\nI use this so much that I\u2019ve aliased the command to .\n\nSo here\u2019s where you should be: You should have a text editor open and editing the file at , and you should have a terminal window open whose current working directory is . You're now ready to develop. Add the following code to foo.py:\n\nThis code is very simple. It just implements a function that accepts a path and returns an iterator so that you can access every row of a CSV file, while also converting the third item in every row to an integer.\n\nPRO TIP: The (pronounced \"shebang\") line must appear at the very beginning of an executable Python script with nothing before it. It will tell your computer that this is a Python file and execute the script correctly if run from the command line as a standalone app. This line doesn't need to appear in library modules, that is, Python code that you plan to import rather than execute.\n\nCreate some data so that we can use our function. Let\u2019s keep all of our data in a fixtures directory in our project.\n\nUsing your editor, add this data to the calories.csv file:\n\nOk, now it\u2019s time to use our code. First, let\u2019s try to execute the code in the interpreter. Open up the REPL as follows:\n\nYou should now be presented with the Python prompt ( ). Anything you type in now should be in Python, not bash. Always note the prompts in the instructions. A prompt with means type in command line instructions (bash), a prompt that says means type in Python on the REPL, and if there is no prompt, you're probably editing a file. Import your code:\n\nA lot happened here, so let\u2019s inspect it. First, when you imported the dataset function from foo, Python looked in your current working directory and found the file, and that's where it imported it from. Where you are on the command line and what your Python path is matters!\n\nWhen you import the dataset function the way we did, the module is loaded and executed all at once and provided to the interpreter\u2019s namespace. You can now use it by writing a for loop to go through every row and print the first item. Note the prompt. This means that Python is expecting an indented block. To exit the block, hit enter twice. The print results appear right in the screen, and then you're returned to the prompt.\n\nBut what if you make a change in the code, for example, capitalizing the first letter of the words in first item of each row? The changes you write in your file won\u2019t show up in the REPL. This is because Python has already loaded the code once. To get the changes, you either have to exit the REPL and restart or you have to import in a different way:\n\nNow you can reload the foo module and get your code changes:\n\nThis can get pretty unwieldy as code gets larger and more changes happen, so let\u2019s shift our development strategy over to executing Python files. Inside foo.py, add the following to the end of the file:\n\nTo execute this code, you simply type the following on the command line:\n\nThe statement means that the code will only get executed if the code is run directly, not imported. In fact, if you open up the REPL and type in , nothing will be printed to your screen. This is incredibly useful. It means that you can put test code inside your script as you're developing it without worrying that it will interfere with your project. Not only that, it documents to other developers how the code in that file should be used and provides a simple test to check to make sure that you're not creating errors.\n\nIn larger projects, you\u2019ll see that most developers put test and debugging code under so called \u201cifmain\u201d statements at the bottom of their files. You should do this too!\n\nWith this example, hopefully you have learned the workflow for developing Python programs both by executing scripts and using \u201cifmain\u201d as well as importing and reloading scripts in the REPL. Most developers use both methods interchangeably, using whatever is needed at the time.\n\nOk, so how do you write an actual Python program and move from experimenting with short snippets of code to larger programs? The first thing you have to do is organize your code into a project. Unfortunately there is really nothing to do this for you automatically, but most developers follow a well known pattern that was introduce by Zed Shaw in his book Learn Python the Hard Way.\n\nIn order to create a new project, you\u2019ll implement the \u201cPython project skeleton,\u201d a set of directories and files that belong in every single project you create. The project skeleton is very familiar to Python developers, and you\u2019ll quickly start to recognize it as you investigate the code of other Python developers (which you should be doing). The basic skeleton is implemented inside of a project directory, which are stored in your workspace as described above. The directory structure is as follows (for an example project called ):\n\nThis is a lot, but don\u2019t be intimidated. This structure implements many tools including packaging for distribution, documentation with Sphinx, testing, and more.\n\nLet\u2019s go through the pieces one by one. Project documentation is the first part, implemented as and files. The README file is a markdown document that you can add developer-specific documentation to your project. The LICENSE can be any open source license, or a Copyright statement in the case of proprietary code. Both of these files are typically generated for you if you create your project in Github. If you do create your file in Github, you should also use the Python that Github provides, which helps keep your repositories clean.\n\nThe script is a Python setuptools or distutils installation script and will allow you to configure your project for deployment. It will use the to specify the third party dependencies required to implement your project. Other developers will also use these files to create their development environments.\n\nThe directory contains the Sphinx documentation generator, Python documentation is written in restructuredText, a Markup language similar to Markdown and others. This documentation should be more extensive and should be for both users and developers. The directory will contain any executable scripts you intend to build. Data scientists also typically also have a directory in which to store data files.\n\nThe and directories are actually Python modules since they contain the file. You'll put your code in foo and your tests in tests. Once you start developing inside your foo directory, note that when you open up the REPL, you have to import everything from the 'foo' namespace. You can put import statements in your files to make things easier to import as well. You can still also execute your scripts in the foo directory using the \"ifmain\" method.\n\nYou don\u2019t have to manually create the structure above, many tools will help you build this environment. For example the Cookiecutter project will help you manage project templates and quickly build them. The spinx-quickstart command will generate your documentation directory. Github will add the and stubs. Finally, will generate the file.\n\nStarting a Python project is a ritual, however, so I will take you through my process for starting one. Light a candle, roll up your sleeves, and get a coffee. It\u2019s time.\n\nInside of your Projects directory, create a directory for your workspace (project). Let\u2019s pretend that we\u2019re building a project that will generate a social network from emails, we\u2019ll call it \u201cemailgraph.\u201d\n\nThis will create the virtual environment in ~/.virtualenvs/emailgraph and automatically activate it for you. At any time and at any place on the command line, you can issue the command and you'll be taken to your project directory (the flag specifies that this is the project directory for this virtualenv).\n\nCreate the various directories that you\u2019ll require:\n\nAnd then create the various files that are needed:\n\nYou can safely use the defaults, but make sure that you do accept the Makefile at the end to quickly and easily generate the documentation. This should create an index.rst and conf.py file in your directory.\n\nInstall nose and coverage to begin your test harness:\n\nOpen up the file with your favorite editor, and add the following initialization tests:\n\nFrom your project directory, you can now run the test suite, with coverage as follows:\n\nYou should see two tests passing along with a 100% test coverage report.\n\nOpen up the file and add the following lines:\n\nSetting up your app for deployment is the topic of another post, but this will alert other developers to the fact that you haven\u2019t gotten around to it yet.\n\nFinally, commit all the work you\u2019ve done to email graph to the repository.\n\nWith that you should have your project all setup and ready to go. Get some more coffee, it\u2019s time to start work!\n\nWith this post, hopefully you\u2019ve discovered some best practices and workflows for Python development. Structuring both your code and projects this way will help keep you organized and will also help others quickly understand what you\u2019ve built, which is critical when working on projects involving more than one person. More importantly, this project structure is the preparation for deployment and the base for larger applications and professional, production grade software. Whether you\u2019re scripting or writing apps, I hope that these workflows will be useful.\n\nIf you\u2019d like to explore further how to include professional grade tools into your Python development, check out some of the following tools:"
    },
    {
        "url": "https://medium.com/district-data-labs/what-are-the-odds-55ad70a46ccc",
        "title": "What Are the Odds? \u2013 District Data Labs \u2013",
        "text": "The probability of an event represents the likelihood of the event to occur. For example, most of us would agree that the probability of getting a heads after flipping a fair coin is 0.5 or that the probability of getting a one on rolling a fair die is 1/6. However, there are many more places where we encounter probabilities in our lives. During election season, we have pundits and polls speaking to the likelihood (probability) of winning for each candidate. Doctors will often state that a patient has low or high risk for heart attacks or breast cancer, reflecting either data or the doctor\u2019s belief of how likely a patient is to experience that outcome. Banks use customer data to figure out how likely a customer is to default on a loan \u2014 the credit risk. We see probability appear in many guises, being called likelihood, risk, odds, propensity, and other synonyms.\n\nBroadly speaking, we can arrive at probabilities for events in two ways:\n\nWe can easily demonstrate frequentist probability using R, by simulating random events. For example, let\u2019s consider the easiest example, flipping a coin. Instead of heads and tails, we\u2019ll call them 1 and 0, since the computer likes them more.\n\nWe see that, after 10,000 experiments, the relative frequency of flipping heads is 0.5, as we would expect from a fair coin. But, how quickly could we have figured this out?\n\nIt turns out that after about 1,000 experiments, we\u2019re consistently within 1% of the true value.\n\nLook at the graph below. It is similar to the graph above, but in this one, we have performed 500 coin-tossing experiments. Do you think the coin is fair? What would be your guess about the probability of flipping heads?\n\nTwo things are evident here. First, we need to experience quite a number of \n\nindependent experiments to ascertain the probability of a particular event \n\n(fortunately, for many, we have collective experience). Second, for a small number of experiments, our estimates can bounce around quite a bit. Probability allows us to figure out how much they can bounce around and with what likelihood. This leads us to the idea of probability distributions.\n\nWe\u2019ll continue the coin-tossing experiment as before. If we toss a fair coin 5 times, we expect to see 2 or 3 heads. But what if we see no heads? Or 5 heads? If we see either of these, we tend to think there\u2019s something wrong with the coin. Let\u2019s do an experiment to see how often a fair coin would result in 5 heads in 5 tosses. This is a random experiment, and so we can simulate this in R. We\u2019ll run 5000 experiments.\n\nThough small, there is still about a 3.3% chance of seeing 5 heads out of 5 tosses. Not 0!\n\nOut of 5 tosses, we can potentially see 0, 1, 2, 3, 4, or 5 heads. How often would we see each possible outcome? Before we go forward, let\u2019s establish some definitions:\n\nHere we will define our experiment as number of heads in 5 tosses of a fair coin. The sample space for this experiment is then S = {0,1,2,3,4,5}, with each of {0}, {1}, {2}, {3}, {4} and {5} being elementary events. First, let\u2019s see the probability associated with each of the elementary events.\n\nThis gives us a fair idea about how often we\u2019d see each possible outcome.\n\nThis is a popular sample experiment, and you can think about real experiments for which this would be a good model. The theoretical distribution that this experiment gives rise to is the binomial distribution. R, being a statistics and probability oriented language, can simulate various standard theoretical distributions like the binomial distribution directly, as well as elicit probabilities directly. Let\u2019s see what the binomial distribution says about the probabilities of each elementary event in our sample space.\n\nWe see that our experimental probabilities are pretty close to the binomial probabilities. Why are they not exactly the same? The binomial distribution is conceptually based on an infinite number of experiments, and we have used only 5000 experiments. The discrepancy will go down as we increase the number of experiments. Also, notice that error is higher for the rarer outcomes (0 and 5) than for the more common outcomes (2 and 3). This makes sense since we\u2019ll see fewer occurences of the rarer outcomes, and so the corresponding estimates would be worse.\n\nChallenge: Increase the number of experiments and see how many experiments you need to get closer to the binomial probabilities. For a particular error rate, see how many more experiments you need to achieve it for Prob(0) compared to Prob(3).\n\nBoth the experiment-based distribution and the binomial distribution are examples of probability distributions. The formal definition is a bit involved, but conceptually, we might consider the following definition for sample spaces like the one we\u2019ve been playing with \u2014 sample spaces that have a finite number of elements.\n\nThere are corresponding definitions for infinite sample spaces (like, for example, the heights of all people in a city, or the distance a crossbow can fire an arrow), but that is a topic for another day (or today, if you like).\n\nThere are many standard probability distributions that are used to model real-life events, including:\n\nIn general, events need not be elementary events. We can consider different kinds of events that are composed of elementary events. Since a lot of the history of Probability Theory is entangled with gambling, or, \u201cgames of chance,\u201d let\u2019s consider rolling dice. In particular, let\u2019s roll two (fair) dice, like in the game of craps. The full sample space S for this experiment has 36 elements, which we\u2019ll denote by the pair of numbers that we see from the first and second die:\n\nWe can use R to create a dice-rolling function:\n\nTo compute the probability of different kinds of events, we have several building blocks that make our lives easier than actually counting out the probabilities of elementary events. Imagine if you had to find probabilites of different winning combinations in Powerball by enumeration!\n\nBut first, some definitions:\n\nThe first building block for getting probabilities of composite events is that the probability of seeing at least one of a set of disjoint events is the sum of the individual probabilities. If we want to figure out the probability that we get a 7 on rolling two dice, we have to add up the probabilities (1/36) of the elementary events that give that outcome, so {(1,6),(2,5),\u2026,(6,1)}.\n\nThe next building block is that the probability for the entire sample space is 1. This makes sense since, in an experiment, we necessarily have to see one of the outcomes in its sample space. Formally,\n\nThis leads to the probability of complementary events. If we look at an event {A} and its complement {not A}, we see that these two events are disjoint and the composite event {A or not A} is the entire sample space. So,\n\nUsing these building blocks, we can compute the probabilities for any composite event once we know the probability distribution. For example, Prob(total is less than 5) = Prob(total is 2) + Prob(total is 3) + Prob(total is 4), and each of these can be computed from the probabilities of the corresponding elementary events. Similarly, Prob(total is not 7) = 1 \u2014 Prob(total is 7).\n\nFortunately, one thing the computer is very good at is enumeration and computation. We can easily use simulation to generate the probability distribution for the totals.\n\nWe can also find the theoretical distribution rather easily using R.\n\nThis gives the totals for all 36 elementary events as a matrix. Since each elementary event is equally likely with fair dice, each element in the matrix has probability 1/36 to occur. So to find the probability of the total being 7, we just find the number of 7\u2019s in the matrix and divide by 36.\n\nWe can also find the probability of not getting a 7 using the formula Prob(not 7) = 1-Prob(7). Using the computer, we can also verify this from the matrix above.\n\nChallenge: Find the probabilities of other events that would be of interest to a craps player or players of other games with two dice.\n\nProbability plays a central role in many events in our lives. Understanding what probability is, how it is ascertained, and how to figure out the probabilities of different events aids our ability to make sense of many phenomena in our lives, including some of the things that appear in newspaper and TV reports. How probability affects our lives is, in itself, fascinating, and there are several elegant and comprehensible books available for further reading. A few personal favorites are listed below:"
    },
    {
        "url": "https://medium.com/district-data-labs/how-to-transition-from-excel-to-r-f9304fabcbab",
        "title": "How to Transition from Excel to R \u2013 District Data Labs \u2013",
        "text": "In today\u2019s increasingly data-driven world, business people are constantly talking about how they want more powerful and flexible analytical tools, but are usually intimidated by the programming knowledge these tools require and the learning curve they must overcome just to be able to reproduce what they already know how to do in the programs they\u2019ve become accustomed to using. For most business people, the go-to tool for doing anything analytical is Microsoft Excel.\n\nIf you\u2019re an Excel user and you\u2019re scared of diving into R, you\u2019re in luck. I\u2019m here to slay those fears! With this post, I\u2019ll provide you with the resources and examples you need to get up to speed doing some of the basic things you\u2019re used to doing in Excel in R. I\u2019m going to spare you the countless hours I spent researching how to do this stuff when I first started so that you feel comfortable enough to continue using R and learning about its more sophisticated capabilities.\n\nQuick note before we do: There are usually multiple ways to do everything in R. I\u2019m going to show you the way I do it, but if you know of a better/shorter/faster/easier way to do something, please leave it in the comments!\n\nLet\u2019s start with the basics. You\u2019ll want to make sure you have downloaded and installed R. I\u2019m also using RStudio as my IDE, so you should install that as well. You\u2019ll be glad you did; it\u2019s awesome.\n\nYou\u2019ll also want to install and load the library, which not only contains the data set we want to use but will also come in handy when we get to creating charts and graphs later. We will also install and load the library to help with manipulating the data.\n\nWe are going to use the diamonds data set that comes with . The data set contains prices and other attributes of over 50,000 diamonds.\n\nOK, so let\u2019s take an initial look at the data. You can type into the R console and it will print out the data set in the console screen, but I advise against doing this. If you're an Excel user, you're used to viewing data in a tabular format. You can do that in one line of code.\n\nThis should have created a data frame object in RStudio\u2019s upper right hand pane (the one called \u201cEnvironment\u201d) and it should say \u201c53940 obs. of 10 variables\u201d right next to it. This means the table has 53,940 rows and 10 columns. Click on diamonds in that pane, and RStudio will show you the table.\n\nIt should look like this:\n\nThe first 7 columns are pretty well labeled, so we won\u2019t mess with those, but the last 3 aren\u2019t labeled very well. So let\u2019s rename columns 8, 9, and 10. We\u2019ll call them length, width, and depth respectively.\n\nYou\u2019ll notice that now we have two columns named depth. Let\u2019s rename the first one (column 5) to \u201cdepthperc.\u201d\n\nNow the data frame should look like this:\n\nOne of the most common things people do in Excel is perform calculations. For example, if we wanted to multiply length, width, and depth together; we would type in cell K2 in Excel and then copy that formula all the way down to the last row of the data set. Then you would probably name the column \"cubic\" or something like that.\n\nIn R, you can perform all 3 of these actions with a single function.\n\nHere we are using the function on the diamonds data set to multiply length, width, and depth. It's assigning the outcome of that to a new column called cubic.\n\nAnother way of doing this, which may be more approachable to someone coming from Excel, is as follows.\n\nThe data frame should now look like this:\n\nChallenge: Try creating another column called \u201ctotal\u201d where you add the columns together instead of multiplying them.\n\nThe next most common task that Excel is used for is summarizing data. These tasks range from simply calculating column totals to the more intermediate pivot tables. I\u2019ll show you how to do both in R.\n\nFirst, let\u2019s say that we want to summarize our data set and calculate the overall averages for all the numeric fields (carat, depthperc, table, price, length, width, depth, and cubic). This would be the equivalent of going to the bottom of a column in Excel, typing in , and then copying that formula over to the bottom of all the other columns you wanted to average.\n\nIn R, you would use the function.\n\nHere, we are telling R to calculate the column-wise means for column 1 and then columns 5 through 11. This will print the averages for those column numbers in the R console.\n\nLet\u2019s say you wanted to add carat to the non-numeric fields and then calculate the averages for each combination of the new group of non-numeric fields. This would take a bit of work in Excel (maybe even some pivot-tabling), but is pretty easy in R.\n\nFirst, let\u2019s round the carat values to the nearest 0.25 carat so that our numbers are not all over the place.\n\nYou\u2019re basically telling R to aggregate the diamonds data frame, take the mean of all the numeric fields, and group by the non-numeric fields. The result is a Summary data frame that looks like this:\n\nNext, we\u2019ll replicate Excel\u2019s useful pivot table functionality. Now that we have the averages for each of our numerical fields, let\u2019s choose one and see how it tends to change based on some of the non-numerical attributes. Say we wanted to analyze the difference in average prices of diamonds of different color & clarity combinations. In Excel, you might create a pivot table with color as a Row Label, clarity as a Column Label, and average price in the Values section.\n\nIn R, you can use the package to do the exact same thing. First, install and load the package.\n\nThen, we\u2019ll use the function to get our data into the same pivot table format.\n\nHere, we\u2019re taking the color, clarity, and price columns from the diamonds data frame, casting (pivoting) them out by color (rows) and clarity (columns), and calculating the average price for each combination.\n\nChallenge: Try casting/pivot-tabling using a different combination of non-numerical fields and the averages of one of the other numerical fields.\n\nAnother very common thing people do in Excel are vlookups. The scenario arises where you have two related data sets and you want to pull some values from data set B over to their appropriate place in data set A. So you type something like and Excel looks up the value in A2 in column K and returns the value in the column next to the matching value.\n\nIn R, we can do this using the function. So let's say we wanted to calculate how far above or below a diamond's price was compared to the average for their cut,color, clarity, and carat. In this case, our data set A will be the diamonds data frame and data set B will be the Summary data frame.\n\nFirst, let\u2019s change the name of the price column in the Summary data frame to avgprice. This way, we won\u2019t have two price fields when we bring it over.\n\nNext, let\u2019s merge the data sets and bring over the average price.\n\nWe merged the diamonds data frame with just the columns that we needed from the Summary data frame and the result was that it added the avgprice field to our diamonds data frame.\n\nChallenge: Merge the averages of some of the other numerical fields in the Summary table over to the diamonds data frame.\n\nBonus Challenge: Calculate what percentage over/under priced each diamond is compared to the average. Hint: Remember how to do column calculations?\n\nExcel users also periodically use conditional (IF) statements for filling in values according to whether certain conditions are met. R is also very good for doing this.\n\nLet\u2019s say we wanted to categorize diamonds into size categories such as small, medium, and large based on their carat weight.\n\nHere we\u2019ve set anything less than 0.5 carat to small, anything between 0.5 and 1 carat to medium, and anything 1 carat and above to large.\n\nThe last group of Excel common tasks we\u2019ll cover here are the creation of charts and graphs. Excel has a very \u201cdrag-and-drop\u201d method of graph creation, whereas R has a very \u201ctype out what you want\u201d method. This may be a little daunting at first, but once you get the hang of it, you\u2019ll start to find it easier to customize charts and graphs in R than having to fish around in Excel for the right menu option to make the change you want.\n\nBased on my experience in a business environment, I\u2019m going to cover the 3 most common graphs I\u2019ve seen people create in Excel \u2014 the bar/column chart, the line chart, and the scatterplot.\n\nTaking a look at our diamonds data set, let\u2019s say we want to create a chart that shows how many diamonds of each size (small/medium/large) are in our data. Here\u2019s how you would do that in R.\n\nThis produces a bar chart that looks like this:\n\nThe second type of chart we\u2019re going to create is a line chart. These are usually used when you have data that changes over some period of time and you want to see the magnitude and velocity of those changes. Since our diamonds data set doesn\u2019t have any time series data in it, we\u2019ll do something a little different. We will create a line for each color and see how the number of diamonds of that color change across clarity categories.\n\nHere\u2019s how to do that with the function.\n\nAnd here\u2019s what it looks like.\n\nIt looks like most diamonds fall into the middle clarity categories. Also, pretty interesting that there are more G color diamonds in the higher clarity categories than any other color.\n\nNow let\u2019s do a fairly simple scatter plot so you can get a sense of how to do one in R. For this, we are going to use the command again.\n\nAlternatively, you can produce the same thing with the function as well.\n\nThe resulting plot shows the relationship between the carat weight and the price of the diamonds in our data set, and we\u2019ve also set the points to be different colors according to the clarity of the diamond. The graph below shows us that the larger the diamond and the better the clarity, the more expensive it tends to be.\n\nTo create other types of charts and graphs, the ggplot2 index site is a wonderful resource that has code and visuals for different types of graphs.\n\nWell, there you have it \u2014 a guide to get almost any Excel user started in R. Two things I want to mention before I leave you to explore some more on your own.\n\nFirst, I\u2019ve found that if you want to get good at R (or anything really), the trick is to find a reason to use it every day. It doesn\u2019t matter if it\u2019s something small, just open up R Studio and go through some of the exercises you\u2019ve learned or try to stretch your knowledge and do one new thing in R each day. You\u2019ll find that after a month or so you\u2019ll be much more comfortable with it and hopefully you\u2019ll keep getting better at it and never look back!\n\nSecond, the Internet is your friend. Part of R\u2019s appeal is the community of people that use it, write about it, and ask/answer questions about it online. There are a vast number of websites just a click away that can guide you in the right direction when you\u2019re stuck on something (this blog included). All it takes is a little searching.\n\nFinally, if you found this post useful, go to the blog\u2019s home page and click the Subscribe button. We\u2019ll be cranking out a lot more great, educational data science content in the near future and I\u2019d hate for you to miss any of it.\n\nMay the learning be with you!"
    }
]