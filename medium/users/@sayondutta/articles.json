[
    {
        "url": "https://medium.com/@sayondutta/intelligent-signals-unstable-deep-learning-why-and-how-to-solve-them-295dc12a7fb0?source=user_profile---------1----------------",
        "title": "Intelligent Signals : Unstable Deep Learning. Why and How to solve them ?",
        "text": "Deep learning models are basically deep layered neural networks where we try to discover those hidden patterns/information through multiple layers of abstraction. These abstractions are the reason behind the advantage of deep neural networks over other traditional machine learning algorithms while dealing with complex and large amount of data.\n\nAs we all know at least in basic terms that it\u2019s the backpropagation which is the key behind the learning/training of these deep neural networks. It\u2019s this backpropagation which trains a deep learning model and it\u2019s the maths behind backpropagation which will tell us why it\u2019s hard to train at the same time.\n\nWe all know a deep neural network is nothing but a black box of weights and biases trained over a large data to discover the hidden patterns/information which otherwise being impossible for humans and even if possible then it\u2019s not that much scalable. When we scale down to a neuron(node in neural network) level we find each of the neurons present in different layers learn at different speeds i.e. they have different gradients.\n\nLearning or as we say training happens from later layers(right side) to early layers(left side). As a result, later layers learn well but early layers learn very less during the process. This becomes worse with increase in number of layers. And, there are also cases where earlier layers learn extremely fast and later layers face the slowdown.\n\nLearning about these difficulties and understanding them helps us to understand the reasonable solutions available and how it can be made better. Remember, it\u2019s not magic it\u2019s mathematics.\n\nInstead of giving you a definition, it\u2019s better if to create, explore and witness this problem. Follow the steps:\n\nThis phenomenon described in above steps is what we know as \u201cvanishing gradient problem\u201d.\n\nTrying the same approach as mentioned above, but when the observations are totally opposite i.e. the gradients become larger in earlier layers. This tends to \u201cexploding gradient problem\u201d.\n\nWe will get into the details of both with basic examples. But, let\u2019s think for a while that sometimes gradients vanish while sometimes they explode, these gradients which are the key behind backpropagation are actually unstable.\n\nSo, even if the later layers learn properly, still the overall training faces difficulties to classify the inputs correctly because the learning in the earlier layers have been hampered and since they carry the initial abstraction which affects the abstraction in the later layers.\n\nLet us consider an example:\n\nLet\u2019s try backpropagation on this given neural network with sigmoid as the activation function at each input,\n\ntherefore, the derivative of sigmoid function be,\n\nLet\u2019s say, initially weights were initialised from a standard normal distribution, i.e. mean = 0 and standard deviation = 1.\n\nTaking the above case and moving forward, there can be many cases where weights grow during training as result of constant increase in weights are each successive layers causing gradient explosion. As a result, learning rate increases but fails to converge to the optimum. Instead of vanishing we have a totally opposite scenario here called exploding gradient problem.\n\nLet\u2019s consider the similar multi layered neural network(with sigmoid activation function) architecture mentioned in the above section and go through a following case below and see how gradient explosion might occur.\n\nAs a result, at each successive layer an increasing factor > 1 will get multiplied to compute the gradient in that successive layer and will increase the gradients at each successive layer. For example, if the initial weight be say 100, then that increasing factor will shoot over 25 and explode the gradients in earlier layers.\n\nGoing through both the cases of vanishing and exploding gradient, we can easily conclude that we are dealing here with unstable gradient problem in our deep learning models. With increase in hidden layers we add more instability.\n\nMoreover, the toy cases considered above had only one neuron(node unit) at each layer unlike actual deployed deep learning models comprising of several layers each having lots of neurons.\n\nFollowing are the possible solutions which you can put into practise whenever you are training your deep learning models:\n\np.s. : Feel free to comment, drop any queries and suggest possible edits."
    },
    {
        "url": "https://medium.com/@sayondutta/intelligent-signals-maths-part-2-pan-digital-number-f87e7959726?source=user_profile---------2----------------",
        "title": "Intelligent Signals : Maths, Part 2 : Pan Digital Number",
        "text": "Mathematics is a world in itself and the key to enter this fascinating world is to align the real world with numbers and rest what happens is nothing different from magic. Last time we discussed some of the cool introductory facts which are effective pillars in Mathematics. Just like \u03c0 and \u03a6, there are many fascinating numbers which define different patterns in this universe. We will dive deeper into it one by one. Today our frontier is Pan Digital Number, where we will discuss what it is and will try to cover some special Pan Digital Numbers with examples.\n\nSay base is 10, so digits it should use is from 0\u20139. Similarly, if base is 8, then digits from 0\u20137 are applicable. Let us fix base 10 for future examples present in this article since covering the whole section of Pan Digital Number is out of the scope of this article but don\u2019t worry I will try my level best to fascinate you and believe me you will add one interesting knowledge in your mathematical arsenal.\n\nLet\u2019s consider 381,654,729 (a fascinating zero-less pan digital number). It is the only zero-less pan digital number where it\u2019s first n digits are divisible by n. See the explanation below:\n\nLet\u2019s consider another Pan Digital Number, and explore it\u2019s cool facts. This time we have, 9,814,072,356. It is the largest Pan Digital square number, i.e. the largest Pan Digital Number which is a perfect square. It\u2019s the square of 99,066. If check it properly, it\u2019s a Strobogrammatic Number.\n\nDon\u2019t worry we will have a separate article catering some of the fascinating facts related to Strobogrammatic Number.\n\nNext in the list, we have 10,123,457,689. It\u2019s the first Pan Digital Prime provided base is 10.\n\nAt last, we have this formula alongside and the unique thing about this formula is that it\u2019s a unique pan digital representation which approximates the famous Euler-Macheroni Constant e upto first 18 trillion trillion digits(also known as the Euler\u2019s number in the memory of Leonhard Euler and was discovered by famous Swiss mathematician Jacob Bernoulli). We will discuss about this representation later in a separate article dedicated to e.\n\nYeah, I can understand that these type of numbers are purely recreational but isn\u2019t the world of mathematics built with these fascinating numbers and patterns figured owing to the creativity of the coolest dudes on the planet i.e. mathematicians (considering I\u2019m being biased here :P).\n\nSo, today\u2019s task for you would be to choose a base (any 10,8,2, etc.) and explore some of the pan digital numbers and comment them below. Feel free to suggest any edits required."
    },
    {
        "url": "https://medium.com/@sayondutta/intelligent-signals-maths-part-1-255eb1858706?source=user_profile---------3----------------",
        "title": "Intelligent Signals : Maths, Part 1 : Introduction \u2013 Sayon Dutta \u2013",
        "text": "Universe seems to have inherited mathematical order in most of the phenomena. Believe it or not, mathematics is behind nature, science or faith. If not give some time and it will align itself to mathematics.\n\nPart 1, here I will focus more on to develop interest in Mathematics with some facts to start the journey.\n\nYou take a river for example, the ratio of the length of the stream channel to the length of the straight line distance is called sinuosity, i.e., curve and bend level of the river. Average sinuosity of all the rivers in the world is \u03c0. \u03c0 is the same constant which fits in variety of natural phenomena, whether it\u2019s calculation of the acceleration due to gravity, mass of an electron or the gentle breathing of a baby and many more.\n\n\u03c0 representing so many natural phenomena itself has no fixed pattern being an irrational number. In future, we will cover more about \u03c0.\n\nQuite next to \u03c0 in the world of mathematics aligning with many natural and artificial structures in the world is \u03a6 i.e. the Golden Ratio. First instance of my knowledge of Golden Ratio comes from Fibonacci series:\n\nWhether it\u2019s the spiral pattern in the seed pod, the angular offset between a leaf and one below it or the spiral patterns among the seeds of sunflower all follow the Golden Ratio \u03a6.\n\nEven, the design of ancient Parthenon, the golden rectangle icon of National Geographic Channel, the famous Vitruvian man figure by Leornardo da Vinci are artistic examples of Golden Ratio. We will dig deeper in future parts of this series for now lemme share some cooler stuffs which will compel you to explore Mathematics even more.\n\nNatural selection which defines the theory of evolution through billions of years of different species on earth. Breeder\u2019s Equation : \u25b3Z = h\u00b2S which predicts the evolutionary change in a trait is a mathematical pillar for this theory of natural selection.\n\nJust like natural selection solves the Breeder\u2019s equation, we have birds waterfall solving the Navier Stokes equation, branches of trees following Fibonacci series and many more other activities.\n\nCan maths help us understand brain related activities?\n\nWe will try to cover these also here at Intelligent Signals in future. With a hope that this start was enough to boost you to board this exciting journey (where we explore more about the milestones in this awesome field of mathematics), I am signing off for now.\n\nNext we have Pan Digital Number. Till then be safe."
    },
    {
        "url": "https://medium.com/@sayondutta/nuts-and-bolts-of-applying-deep-learning-by-andrew-ng-89e1cab8b602?source=user_profile---------4----------------",
        "title": "Intelligent Signals : Nuts and Bolts of Applying Deep Learning by Andrew Ng",
        "text": "This article refers to the talk given by Andrew Ng at Bay Area Deep Learning School (held on 25th and 26th September, 2016). Andrew\u2019s talk emphasised more on when and how to apply deep learning in different use cases. These days Artificial Intelligence is taking world like a storm and last half decade has witnessed deep learning becoming an integral part of the research as well as the applied arm of the AI arsenal.\n\nLet\u2019s dive into some important points which were discussed by him.\n\nFirstly, there\u2019s always a question that why Deep Learning now all of a sudden since the field isn\u2019t new and the researchers have been exploring the field for over three decades. The main reason being is the scale at the present moment. Yes, the scale of data and the scale of computation power at this time of human civilisation.\n\nIt\u2019s been over four decades of internet. Digital footprints of lots of businesses, researches, phenomenon etc. have grown over these four decades owing to betterment of storage technology and increases in computation power to process this massive amount of data. Today, as per our current state we have the power of utilise those massive amount of data to verify all the discoveries which best of the researchers have done since past three decades with heavy computation engines.\n\nWhy we need these two things, why not we implement a large neural network on small amount of data. These are the question which makes you dig deeper to realise the requirement of a technology to thrive. Think it in form of a data structure. You always tend to use those structure which are sufficient to handle that particular kind of value. e.g. you will not store a scalar value in a variable having a tensor data type.\n\nSimilarly, large networks tend to play a good role when they are able to create good representations of the input data. And these representations becomes distinct and able to comprehend a pattern/behaviour in the data having a high volume and wide variety.\n\nTraditional Machine Learning algorithms after a certain time converges over time as they are not able to absorb.\n\nCheck the bottom left part of the graph, near the origin. This is the region where the data is not defined, i.e. relative ordering of the algorithms is not well defined here. Since the size of the data is small, therefore, representations are not that distinct. At this level, better feature engineering performs better. These hand engineered features fail with the increase in data size. That\u2019s where deep neural networks comes to the picture as they are able to capture better representation owing the vast data size.\n\nPoint is clear here, you don\u2019t just fit in a deep learning architecture to any data you encounter. There\u2019s a volume and variety requirement for the data obtained. Sometimes data with small size works better with traditional Machine Learning algorithms.\n\nDeep Learning further can be segregated into different buckets based on the area of research and application:\n\nLooking across the industry, value is mostly driven by the first three buckets but the fourth bucket is where the future of Artificial Intelligence lies.\n\nTill now ML models were giving real numbers as output e.g. movie reviews(sentiment score), image classification(class object). But now apart from the numbers other type of outputs are being generated. e.g. Image Captioning(input: Image, output: text), Machine Translation(input: text, output: text), Speech Recognition(input: audio, output: text), etc.\n\nAfter all this research and exploration, End to End Deep Learning is not the solution. This is because, for End to End Deep Learning to work properly it needs to incorporate a lot of labelled data.\n\nSo when our model doesn\u2019t work we try all weird approaches possible:\n\nJumping on to the need of human level performance being commonly applied in Deep Learning. A human level accuracy becomes constant after some time converging to the highest possible to Optimal Error Rate(Bayes Error Rate i.e. lowest possible error rate for any classifier of a random outcome).\n\nThe reason behind this is that a lot of problems have theoretical limit in performance owing to the noise in the data. Therefore, human level accuracy is a good approach to improve your models by doing error analysis by incorporating human level error, training set error and validation set error to estimate bias variance effects and even getting more human labelled data.\n\nThese various efforts help us to benchmark the level of improvements with respect to each other and therefore, help us to take some crucial decision i.e. whether to invest in deep learning, or go with traditional approaches if they are crossing the threshold.\n\nAt the last, Andrew answered two most important and frequently asked questions:\n\nHe finally ended up on a positive note saying \u201cJust like electricity transformed agriculture and energy industry. Similarly, AI has the power to transform almost all sectors.\u201d\n\nYou can watch the whole talk below."
    },
    {
        "url": "https://medium.com/marax-ai/intelligent-signals-visualising-data-df9152c10b00?source=user_profile---------5----------------",
        "title": "Intelligent Signals : Visualising Data \u2013 Marax AI \u2013",
        "text": "Before going in the details of data visualisation let\u2019s start with a famous example graphic.\n\nIn the graphic above, important details shared are the size of Napoleonic troop while marching ahead, during retreat, temperature, elevation of the topography and the name of the region with year. So many information in one graphic. In this article I will walk you through the important principles which one must consider for data visualisation.\n\nAlberto Cairo and Edward Tufte, two great pioneers of the concerned field have put forward some core principles from past knowledge experiences and fallacies in visualising data which are now considered to be the fundamentals in the field of data visualisations.\n\nTufte\u2019s heuristics of dark-ink ratio, chartjunk, sparklines and lie factor and Cairo\u2019s Five Qualities of Great Visualisations (truthful, functional, beautiful, insightful, and enlightening) are well worth considering when one is trying to either report or judge visualisation.\n\nIn his book, The Functional Art, Alberto Cairo provides a tool for thinking about design trade-offs when building information graphics, and he calls this tool the Visualisation Wheel.\n\nThe trade offs that Cairo considers are as followed:\n\nLet\u2019s now discuss Edward Tufte\u2019s graphical heuristics but first of all what\u2019s a heuristic?\n\nTufte\u2019s graphical heuristics are as follows:\n\n3. Tufte also suggested items called sparklines and referred to them as data words a way to bridge the gap between text and figures. He suggested that sparklines could not only be represented directly in text, but also could be embedded in tables along with the data they describe. For example, a time series data has been provided with a sparkline giving the overview of the trend overtime. There have been lots of different uses of sparklines, such as video games, are a great example. Modern example of sparklines, called the sparktweet are the unicode block characters used to display a bar graph inside the 140 characters allotted by Twitter.\n\n4. Lie factor : Defined as the size of an effect shown in the graphic divided by the size of the effect actually in the data. It\u2019s misleading to the observers. For example, this graphic alongside from Time Magazine in 1979 is showing various barrels of oil and the price of oil over six years. Multiple queries, whether the barrels different sizes, is it the volume of the barrel that represents the growth of the cost, or the height of the barrel?\n\nNext we dive into Cairo\u2019s Five Qualities of Great Visualisations.\n\nThe first quality of a good visualisation is that it\u2019s truthful or not. There are techniques and data science and information visualisation, which we can use to shine light on to specific pieces of data which can cast distrust and provide false inference.\n\nLet\u2019s check the example of a chart put out by the national cable and telecommunications association. This association lobbies on behalf of US cable companies and provided a graphic which suggested that, after regulations were relaxed, cable companies invested four times as much in the industry. The original chart has been taken down, but Cairo provides an example of what was visualised. Firstly, the title intended to draw the reader to the conclusion, i.e. less regulation means more industry investment. Let\u2019s dive into the problems associated with this graphic, it\u2019s unclear whether the monies are adjusted for inflation, which is a big issue when providing charts of financial data. Then, the time spans for the two bars are different. One covers a span of three years, while the other covers a span of four years. The bars have the same width, but don\u2019t represent the same period of time. And finally, there seems to be a bunch of missing data from years 1997 and 1998. And the chart stops in the year 2003, but the information was tweeted in 2014. For a more honest view of the data check the provided line graph, where after regulation, there were years of sustained industry investment and moreover a slight drop after deregulation, then a massive spike in spending. Then a significant fall from the period post 2002, much of which was excluded from the original chart. The lesson to learn here is that there maybe no absolute truth but always try to use such display tools which are truthful than others.\n\nThe second quality of a visualisation to consider is whether it is functional or not. The direct labelling of graph is one way to increase comprehension along with heuristics for increasing functionality.\n\nThe third quality of visualisation is whether it is beautiful or not. This requires knowing a great deal about your audience as perceptions vary from people to people.\n\nThe fourth quality of visualisation is that it\u2019s insightful or not. The graphic shouldn\u2019t just replicate data, rather it should provide some inference for the viewer giving them an eureka moment. A common issue with most of the people is that they put unnecessary additions to the graphic causing increase in complexity and reader fatigue. But the right results hit the mind of the readers who don\u2019t have extra time to do full reviews.\n\nThe fifth and final quality of visualisation is that it\u2019s enlightening or not.\n\nI hope, these basic fundamentals were enough to provide a sound information for all newcomers in the field of data science for a proper and innovative approach towards data visualisation.\n\np.s. : This was the study and analysis done as per my general experience in the field and a recent study of this MOOC."
    }
]