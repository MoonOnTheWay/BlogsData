[
    {
        "url": "https://medium.com/artists-and-machine-intelligence/what-we-dont-see-2de8920d56be?source=---------0",
        "title": "What We Don\u2019t See \u2013 Artists and Machine Intelligence \u2013",
        "text": "Look out the window. The day is gray and damp or bright, morning, late afternoon, Fall or Friday or Christmas. We see trawlers, skiffs, shells, and tankers. Sailors go by wearing loud foul weather gear, companions on deck. A technicolor houseboat is pulled to a new mooring, while a worn highway being dismantled somewhere in the city comes down the cut in pieces day by day.\n\nTour boats, duck boats, rent-by-the-hour boats. And there\u2019s Google to your right\u2026and there\u2019s Google to your right\u2026and there\u2019s Google to your right\u2026(wave, wave, wave). Lumbering flats slide through an extended view, crossing three panes of look-out, stretching slowly. A quickness at the end reveals tugs coercing dirt mountains downstream. To the left is the city, to the sea is everything else.\n\nThe boats have names: Knot in Kansas, Scorpio, Weekender, Love Hertz, Sea Life, Tyme Pilot. Call and response foghorn blasts precede each drawbridge passage for naval carriers that fully eclipse the view and sailboats that dart quickly through and on. We see a girl craning her neck, twisting toward the sun. This lasts a long time \u2014 20 minutes or more. The relief when she rises to walk away fills the room with disparate deskside applause for a selfie so hard won.\n\nWindow washers dangle outside. A flock of ducks floats aimlessly, takes flight. Some days the light shifts quickly, even one frame to the next. The mundane persistence of this view doesn\u2019t stop us from watching for quiet narratives, from seeing what will happen.\n\nGoogle hired Andy Rogers, a photojournalist; Christopher Woon-Chen, a filmmaker; and me, a fine art photographer to teach an intelligent camera about photography. The first on-device AI of its kind, we filled it with directives \u2014 this is good, that is not. Yes, no, yes, no. Now, not then. We attempted to imbue this camera with the ability to anticipate \u201cgood\u201d shots. To recognize familiar faces and pets. To trigger when detecting smiles, gestures, good lighting, eye contact, the rule of thirds, when to start looking and for how long.\n\nWe were given a corner desk spot overlooking a canal. Watching boats float down the waterway outside our office window, each vessel took on singular interest for the brief span before, during, and after cruising through our view. Using a faux television screen, I started recording and publishing these passages. With this, Yacht TV was born \u2014 a theatrical complement to the daily task of training an AI. We were devoted watchers, waiting for serendipitous boat blips to become unintentional characters, related to each other in a frame-bound story of perpetual forward motion.\n\nThe unbroken gaze can be a luxurious looking, or a test of will. Andy Warhol performed the act of focusing a camera on a subject for the entirety of one activity in his 1964 film Sleep, depicting over five hours of Warhol\u2019s lover at rest. At an original screening in 1964, only 50 of the initial 500 moviegoers remained in the theatre at the end of the film. Others threatened violence. A few years later filmmaker Jean-Luc Godard\u2019s single shot sequence in Weekend followed a traffic jam for seven minutes. As discrete dramas played out, the camera tracked steadily along. It blurred the lines of filmic fiction, giving viewers the impression they were watching life as it unfolded in time.\n\nIn 1968, the photographer Paul Fusco took hundreds of photographs from the inside of a train looking out. In The Train, we see images of people assembling along the tracks of a funeral train holding the body of Robert F. Kennedy as it traveled south along the Eastern Seaboard from New York to Washington D.C. The images bear witness to his progression down the coast, saying goodbye by watching. People in fields, at stations, on street corners, and train trestle hillsides look on. They gather in diverse groups specific to each passing locale to witness, to watch, to see.\n\nThese 60s-era conceptual experiments in duration resurfaced in the early aughts as endurance entertainment branded \u201cSlow Television\u201d by Norway\u2019s national broadcast company, NRK. Depicting the 7-hour train journey from Bergen to Oslo in 2009, the show began as a way to commemorate 100 years of the Bergen rail line. Its surprising popularity quickly spawned many \u201cminutt for minutt\u201d offshoots, which included cruises, the 8-hour knitting of a full sweater, a fire made from wood chop to log formation, and various fixed views of wildlife.\n\nYacht TV is in the slow TV tradition, yet, rather than recording ad infinitum, it selects one discrete subject after another, yacht by yacht. Thanks to a collaboration with Google engineer Larry Lindsey and an AIY Vision kit, what began as a manual capture is now being produced by a machine that recognizes \u201cboat.\u201d Riffing on a movement that includes slow travel, film, food, radio, worship, and other savoring rituals, this is television as an activated medium, optimized for object detection.\n\nTo automate the detection of a desirable image is to consider the nature of observation itself. What happens when we are freed from the randomness of glancing and instead are given a constant archive from which we choose moments to distinguish with our thoughtful attention? This automation of the gaze, built on expert human instruction, is a kind of collaborative perception.\n\nMany years after Fusco\u2019s RFK train series, Dutch photographer Rein Jelle Terpstra contacted the photographed onlookers who had held cameras. He requested their images from that day, which were long buried in 40 years of albums, boxes, and envelopes of negatives. Some spectators made train images only, while others documented their friends and family waiting alongside the track or pointing to things just outside the frame.\n\nOn one side of this two-part series is a vehicular gaze on an unfolding event: Quick punctuations of figures from various points of view create what curator Cl\u00e9ment Ch\u00e9roux describes as a \u201clong, sad human chain that formed along the tracks.\u201d On the other side is the inverse of these myriad gazes, looking back at machinery and documenting it. When seen together these co-images create a true mirror, weaving a collective event and narrative of time, movement, and witness.\n\nTo ride and watch, to go and look, to stand and witness. The act of observation is innately human; it begins and ends in the mind. And yet, our bodies and minds are not without a history of technological augmentation, including the recent iteration of an intelligent eye. Seeing is, alongside love, a supremely human endeavor, even being used as a metaphor for being deeply understood by another. The AIY automation of Yacht TV extends our view from an office window. What else might we do with personalized, creative AI that can augment our senses?\n\nThere is a beautiful simplicity in the regular cadence of observing the everyday, allowing mental narratives to unravel against a steady view. In the words of writer Robert Walser, whose practice was heavily contingent on long walks alone in the landscape: \u201cWe don\u2019t need to see anything out of the ordinary. We already see so much.\u201d While giving lessons to an intelligent camera in the hopes of creating something that resembled instinctive perception, we simultaneously enacted our own, learning about the mental structures we had built around seeing as we instructed how to replicate them.\n\nIn his 2014 New Yorker article heralding the slow TV age, Nathan Heller writes: \u201dSlow TV is high-definition in its visual information, yet it gets its meaning from viewers\u2019 imaginative consciousness. As entertainment, it is backward; it appears to do its job by casting viewers into their own minds.\u201d As humans, we instinctively engage with the natural world and are destined to continue extending our bodies through technology. Where we amplify ourselves in the boundlessness of watching, while excusing ourselves simultaneously from the mechanics of capture, we perceive alongside AI, not as separate beings but rather as dual remote prostheses, reflecting and extending each other. As we become interconnected with technology, we are not evolving and adapting, or being replaced, dominated or destroyed. We are being seen."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/perception-engines-8a46bc598d57?source=---------1",
        "title": "Perception Engines \u2013 Artists and Machine Intelligence \u2013",
        "text": "Can neural networks create abstract objects from nothing other than collections of labelled example images? Neural networks excel at perception and categorization, so it is plausible that with the right feedback loop perception is all you need to drive a constructive creative process. Human perception is an often under-appreciated component of the creative process, so it is an interesting exercise to to devise a computational creative process that puts perception front and center. In this work, the creative process involves the production of real-world, non-virtual objects.\n\nGiven an image, a neural network can assign it to a category such as fan, baseball, or ski mask. This machine learning task is known as classification. But to teach a neural network to classify images, it must first be trained using many example images. The perception abilities of the classifier are grounded in the dataset of example images used to define a particular concept.\n\nIn this work, the only source of ground truth for any drawing is this unfiltered collection of training images. For example, here are the first few dozen training images (from over over a thousand) in the electric fan category:\n\nAbstract representational prints are then constructed which are able to elicit strong classifier responses in neural networks. From the point of view of trained neural network classifiers, images of these ink on paper prints strongly trigger the abstract concepts within the constraints of a given drawing system. This process developed is called perception engines as it uses the perception ability of trained neural networks to guide its construction process. When successful, the technique is found to generalize broadly across neural network architectures. It is also interesting to consider when these outputs do (or don\u2019t) appear meaningful to humans. Ultimately, the collection of input training images are transformed with no human intervention into an abstract visual representation of the category represented.\n\nThe first perception engine implementations were not concerned with physical embodiment. These pixel based systems were inspired by and re-purposed the techniques of adversarial examples. Adversarial examples are a body of research which probes machine learning systems with small perturbations in order to cause a classifier to fail to correctly assign the correct label.\n\nAdversarial examples are usually constrained to making small changes to existing images. However, perception engines allows arbitrary changes within the constraints of a drawing system. Adversarial techniques also often target specific neural networks. But in this work we hope to create images that generalize across all neural networks and \u2014 hopefully \u2014 humans as well. So perception engines use ensembles of trained networks with different well known architectures and also includes testing for generalization.\n\nAs the architecture of these early systems settled, the operation could be cleanly divided into three different submodules:\n\nThe perception engine architecture uses the random search of\n\nthe planning module to gradually achieve the objective through iterative refinement. When the objective is to maximize the perception of an electric fan, the system will incrementally draw or refine a proposed design\n\nfor a fan. Combining these systems is a bit like creating a computational ouija board: several neural networks simultaneously nudge and push a drawing toward the objective.\n\nThough this is effective when optimizing for digital outputs, additional\n\nwork is necessary when planning physical objects which are subject to\n\nproduction tolerances and a range of viewing conditions.\n\nAfter the proof of concept I was ready to target a physical drawing system. The Riso Printer was chosen as a first target; it employs a physical ink process similar to screen printing. This meant all outputs are subject to a number of production constraints such as limited number of ink colors (I used about 6) and unpredictable layer alignment between layers of different colors.\n\nAt this point in my development I was awarded a grant from Google\u2019s Artist and Machine Intelligence group (AMI). With their support, I was able to print a series of test prints and iteratively improve my software system to model the physical printing process. Each source of uncertainty that could cause a physical object to have variations in appearance is modeled as a distribution of possible outcomes.\n\nIt is common for Riso prints to have a small amount of mis-alignment between layers because the paper must be inserted separately for each different color. This possibility was handled by applying a small amount of jitter manually between colors.\n\nIn practice this jitter keeps the final design from being overly dependent on the relative placement of elements across different layers.\n\nThe colors of a digital image can be given exactly. But a physical object will be perceived with slightly different colors depending on the ambient lighting conditions. To allow the final print to be effective in a variety of environments, the paper and ink colors were photographed under multiple conditions and then simulated as various possibilities.\n\nThe lighting and layer adjustments were independent and could be applied concurrently.\n\nIn a physical setting, the exact location of the viewer is not known. To keep the print from being dependent on a particular viewing angle, a set of perspective transformations were also applied. These are generally added during a final refinement stage and are done in addition to the alignment and lighting adjustments.\n\nThis system typically runs for many hours on a deep learning workstation in order to generate hundreds to thousands of iterations on a single design. Once the system has produced a candidate, a set of master pages are made. Importantly, the perspective and jitter transforms are disabled to produce these masters in their canonical form. For the fan print, two layers were produced: one for the purple ink and one for black.\n\nThese masters are used to print ink versions on paper.\n\nAfter printing, a photo is used to test for generalization. This is done by querying neural networks that were not involved in the original pipeline to see if they agree the objective has been met \u2014 an analogue of a train / test split across several networks with different architectures. In this case, the electric fan image was produced with the influence of 4 trained networks, but generalizes well to 5 others.\n\nA philosophical note on creativity and intent: Using perception engines inverts the stereotypical creative relationship employed in human computer interaction. Instead of using the computer as a tool, the Drawing System module can be thought of a special tool that the neural network itself drives to make its own creative outputs. As the human artist, my main creative contribution is the design of a programming design system that allows the neural network to express itself effectively and with a distinct style. I\u2019ve designed the constraint system that defines the form, but the neural networks are the ultimate arbiter of the content.\n\nIn my initial set of perception engine objects I decided to explicitly caption each image with the intended target concept. Riffing off of Magritte\u2019s \n\nThe Treachery of Images (and not being able to pass on a pun), these first prints were called The Treachery of ImageNet.\n\nThe conceit was that many of these prints would strongly evoke their target concepts in neural networks in the same way people find Magritte\u2019s painting evocative of an actual, non-representational pipe. The name also emphasizes the ImageNet\u2019s role in establishing the somewhat arbitrary ontology of concepts used to train these networks (the canonical ILSVRC subset) which I also tried to highlight by choosing an eclectic set of labels across the series.\n\nAdditional print work is in various stages of production using the same core architecture. Currently these use the same objective and planner but vary the drawing system, such as using multiple ink layers or a more generic screen printing technique. Some more recent experiments also use very different creative objectives and more radical departures from the current types of drawing systems and embodiments. As these are completed I\u2019ll share incremental results on twitter with occasional write-ups here. Additional photos of all completed prints can be found in my online store which is also used to fund future work."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/ami-talks-summer-roundup-d93bf930d94a?source=---------2",
        "title": "AMI Talks: Summer Roundup \u2013 Artists and Machine Intelligence \u2013",
        "text": "This summer was alive with creative AI gatherings in the US and EU. I had a chance to sit down for a few conversations which you can listen to, watch, and read below.\n\nI spoke with artist Ian Cheng and curator Troy Conrad Therrien at the Guggenheim Museum in NYC about the intersection of AI, art, and esoteric cultural movements.\n\nAt the Serpentine Miracle Marathon I had a conversation with Jason Louv about designing AI-human relationships inspired by wisdom traditions. Below is a recap of the Marathon by the inimitable Victoria Sin.\n\nAt Google Design\u2019s SPAN conference I got to ask a few questions of my own to AMI Focused Research Award recipient Golan Levin and Heather Kelley of CMU and Kokoromi. Watch the uncannily animated Livestream below:\n\nBen Vickers, CTO of London\u2019s Serpentine Gallery, asked several provocative questions on behalf of the always excellent Cura magazine. Find your copy this way:\n\nTo catch inspiring conversations with thinkers and makers in realtime follow me on Twitter and Instagram ( @kenricmcdowell ) and AMI on Twitter ( @artwithMI ). Thanks for listening/watching/reading!"
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/neural-artistic-style-transfer-a-comprehensive-look-f54d8649c199?source=---------3",
        "title": "Neural Artistic Style Transfer: A Comprehensive Look",
        "text": "Spring Quarter of my freshman year, I took Stanford\u2019s CS 231n course on Convolutional Neural Networks. My final project for the course dealt with a super cool concept called neural style transfer, in which the style of a piece of artwork is transferred onto a picture. Here\u2019s a classic example \u2014 a picture of Hoover Tower at Stanford, in the style of The Starry Night:\n\nThe first published paper on neural style transfer used an optimization technique \u2014 that is, starting off with a random noise image and making it more and more desirable with every \u201ctraining\u201d iteration of the neural network. This technique has been covered in quite a few tutorials around the web. However, the technique of a subsequent paper, which is what really made neural style transfer blow up, has not been covered as thoroughly. This technique is feedforward \u2014 train a network to do the stylizations for a given painting beforehand so that it can produce stylized images instantly.\n\nIn this tutorial, we will cover both techniques, the intuitions and math behind how they work, and how the second builds off the first. We will also cover the next step: cutting-edge research as explored by me in my CS 231n project, as well as research teams at Cornell and Google. By the end, you will be able to train your own networks to produce beautiful pieces of art!\n\nIn this method, we do not use a neural network in a true sense. That is, we aren\u2019t training a network to do anything. We are simply taking advantage of backpropagation to minimize two defined loss values. The tensor which we backpropagate into is the stylized image we wish to achieve \u2014 which we call the pastiche from here on out. We also have as inputs the artwork whose style we want to transfer, known as the style image, and the picture that we want to transfer the style onto, known as the content image.\n\nThe pastiche is initialized to be random noise. It, along with the content and style images, are then passed through several layers of a network that is pretrained on image classification. We use the outputs of various intermediate layers to compute two types of losses: style loss and content loss \u2014 that is, how close is the pastiche to the style image in style, and how close is the pastiche to the content image in content. Those losses are then minimized by directly changing our pastiche image. By the end of a few iterations, the pastiche image now has the style of the style image and the content of the content image \u2014 or, said differently, it is a stylized version of the original content image.\n\nBefore we dive into the math and intuition behind the losses, let\u2019s address a concern you may have. You may be wondering why we use the outputs of intermediate layers of a pretrained image classification network to compute our style and content losses. This is because, for a network to be able to do image classification, it has to understand the image. So, between taking the image as input and outputting its guess at what it is, it\u2019s doing transformations to turn the image pixels into an internal understanding of the content of the image.\n\nWe can interpret these internal understandings as intermediate semantic representations of the initial image and use those representations to \u201ccompare\u201d the content of two images. As an example: if we pass two images of cats through an image classification network, even if the initial images look very different, after being passed through many internal layers, their representations will be very close in raw value. This is the content loss \u2014 pass both the pastiche image and the content image through some layers of an image classification network and find the Euclidean distance between the intermediate representations of those images. Here\u2019s the equation for content loss:\n\nThe summation notation makes the concept look harder than it really is. Basically, we make a list of layers at which we want to compute content loss. We pass the content and pastiches images through the network until a particular layer in the list, take the output of that layer, square the difference between each corresponding value in the output, and sum them all up. We do this for every layer in the list, and sum those up. One thing to note, though: we multiply each of the representations by some value alpha (called the content weight) before finding their differences and squaring it, whereas the original equation calls for the value to be multiplied after squaring it. I found, in practice, the former to work much better than the latter, as it produces appealing stylizations much more quickly.\n\nThe style loss is very similar, except instead of comparing the raw outputs of the style and pastiche images at various layers, we compare the Gram matrices of the outputs. A Gram matrix results from multiplying a matrix with the transpose of itself:\n\nBecause every column is multiplied with every row in the matrix, we can think of the spatial information that was contained in the original representations to have been \u201cdistributed\u201d. The Gram matrix instead contains non-localized information about the image, such as texture, shapes, and weights \u2014 style!\n\nNow that we have defined the Gram matrix as having information about style, we can find the Euclidean distance between the Gram matrices of the intermediate representations of the pastiche and style image to find how similar they are in style:\n\nSimilar to our style loss computation, we find the Euclidean distances between each corresponding pair of values in the Gram matrices computed at each layer in a predefined list of layers, multiplied by some value beta (known as the style weight).\n\nWe have the content loss \u2014 which contains information on how close the pastiche is in content to the content image \u2014 and the style loss \u2014 which contains information on how close the pastiche is in style to the style image. We can now add them together to get the total loss. We then backpropagate through the network to reduce this loss by getting a gradient on the pastiche image and iteratively changing it to make it look more and more like a stylized content image. This is all described in more rigorous detail in the original paper on the topic by Gatys et al.\n\nNow that we know how it works, let\u2019s build it. We\u2019ll be using the PyTorch library for all our code, so make sure you have it installed.\n\nNow, let\u2019s define our network. Let\u2019s create a class with an initializer that sets some important variables:\n\nThe variables which we initialize are: the style, content, and pastiches images; the layers at which we compute content and style loss, as well as the alpha and beta weights we multiply the representations by; the pretrained network that we use to get the intermediate representations (we use VGG-19); the Gram matrix computation; the loss computation we do (as MSE is the same as Euclidean distance); and the optimizer we use. We also want to make use of a GPU if we have on on our machine.\n\nNow for the network\u2019s \u201ctraining\u201d regime. We pass the images through the network one layer at a time. We check to see if it is a layer at which we do a content or style loss computation. If it is, we compute the appropriate loss at that layer. Finally, we add the content and style losses together and call backward on that loss, and take an update step. Here\u2019s how all that looks:\n\nOne more step before we can start transferring some style \u2014 we need to write up a couple of convenience functions:\n\nThe image_loader function opens an image at a path and loads it as a PyTorch variable of size imsize; the save_image function turns the pastiches image, which is a PyTorch variable, into the appropriate PIL format to save it to file.\n\nNow, to produce beautiful art. First, let\u2019s import some stuff, load our images, and put them onto the GPU if we have one. Then, let\u2019s define and call our training loop to turn our initially random pastiche into an incredible piece of art. Here\u2019s how that all looks:\n\nAnd that\u2019s it! We have used an optimization method to generate a stylized version of a content image. You can now render any image into the style of any painting \u2014 albeit slowly, as the optimization process is iterative. Here\u2019s an example that I made from our code \u2014 a dancer in the style of Figure Dans un Fauteuil by Picasso:\n\nWhy does the previous method take so much time? Why do we have to wait for an optimization loop to generate a pretty picture? Can\u2019t we tell a network, \u201cHey, learn the style of Starry Night so well that I can give you any picture and you\u2019ll turn it into a Starry Night-ified version of the picture instantly\u201d? As a matter of fact, we can!\n\nEssentially, we create an untrained Image Transformation Network which transforms the content image into its best guess at an appealing pastiche image. We then use this as the pastiche image which, along with the content and style images, is passed through the pretrained image classification network (now called the Loss Network) to compute our content and style losses. Finally, to minimize the loss, we backpropagate into the parameters of the Image Transormation Network, not directly into the pastiche image. We do this with a ton of random content image examples, thereby training the Image Transformation Network to transform any given picture into the style of some predefined artwork.\n\nWe now need to add an Image Transformation Network to our multi-stage network architecture, and make sure that the optimizer is set to optimize the parameters of the ITN. We will be using the architecture from Johnson et al. for our ITN. Replace the last 8 lines of the StyleCNN initializer with this code:\n\nAlso, since we now want our training regime to accept a content image batch as training examples, we can remove the content image as an input to the initializer. Our training regime accepts a content image batch, transforms it into pastiche images, computes losses as done above, and calls backward on the final loss to update the ITN parameters. Here\u2019s all that in code:\n\nAwesome! Let\u2019s change our save_image function to be able to save not just one image, but a batch of images:\n\nOne more order of business. We need to have available to us a dataset of content images as training examples to be passed into our network at every training iteration. We will use the Microsoft COCO dataset for this. You have to install and set up the COCO API before you can do anything with it in PyTorch. The API can be download from source here. After downloading it, install it by running setup.py.\n\nOnce you\u2019ve completed those steps, add these imports:\n\nFinally, we replace our main function with this code:\n\nWoo! Now we can train a network to transform any image into a stylized version of it, based on the style of our prepicked image. That is awesome! In case you were wondering, this technology is exactly how apps like Prisma work. Here are some example outputs from three feedforward networks trained on three different paintings:\n\nYou may have noticed that, although our feedforward implementation can produce stylized pastiches instantly, it can only do so for one given style image. Would it be possible to train a network which can take, in addition to any content image, any style image and produce a pastiche from those two images? In other words, can we make a truly arbitrary neural style transfer network?\n\nAs it so happens, a very cool finding in this problem suggested that it is possible. A few years ago, it was found that the Instance Normalization layers in the Image Transformation Networks were the only important layers to represent style. In other words, if we keep all Convolutional parameters the same and learn only new Instance Normalization parameters, we can represent completely different styles in just one network. Here\u2019s the paper with that finding. This suggests that we can take advantage of Instance Norm layers by using, in those layers, parameters that are specific to each style.\n\nThe first attempt to do this was from a team at Cornell. Their solution was using Adaptive Instance Normalization, in which an encoder-decoder architecture was used to generate the Instance Norm parameters from the style images. Here is the paper on that. This had fairly high success.\n\nThe next attempt at arbitrary style transfer was my own project, in which I use a secondary Normalization Network to transform the style image into Instance Normalization parameters to be used by the Image Transformation Network, learning the parameters in both the Normalization Network and the ITN all at once. Here is my paper on that. This method was able to demonstrate success on a small set of random styles and therefore was not truly arbitrary; however, it was theoretically successful.\n\nThe latest attempt at achieving arbitrary style transfer was from a team at Google Brain. They used a method which happens to be almost identical to the my method, as described above, except that they use a pre-trained Inception network to transform the style images into Instance Norm parameters instead of using an untrained network and training it, along with the ITN, end-to-end. This method saw wild success and produced beautiful results, achieving true arbitrary neural style transfer. Here is the awesome paper on that.\n\nWhile I was unable to fully solve the problem, I felt honored to have been able to participate in the research conversation and be on the cutting-edge of solving such an intriguing problem. It was thrilling to see Google finally solve the problem with such an elegant and beautiful solution.\n\nThere\u2019s quite a bit of literature on this problem, so some great next steps would be to skim through them. I linked the papers throughout the article, but I thought I\u2019d post them all in one place too:\n\nHope you found this exciting application of neural networks to be as fascinating and fun as I do. Happy learning!\n\nIf you liked this article, please be sure to give me a clap and follow me to see my future articles in your feed! Also, check out my personal blog and follow my Twitter for more of my musings."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/internet-archaeology-210d78311145?source=---------4",
        "title": "Internet Archaeology \u2013 Artists and Machine Intelligence \u2013",
        "text": "Computers and the Internet have vastly expanded the human capacity to generate, process, and communicate information. At the same time, these technologies have altered us in more subtle ways, changing our attention spans, aesthetic sensibilities, and even the way we conceive of our own minds. This summer, the Internet Archive offers a rare opportunity to look back at this metamorphosis through its artist-in-residency program. In a concluding gallery show at Ever Gold [Projects] in San Francisco, Internet Archive artists-in-residence Jenny Odell and Jeremiah Jenkins investigate the changes in human nature wrought by computers and the Internet. Artworks by Jenkins and Odell picture the parts of us that have been shaped by these technologies and embody the fear of their loss.\n\nOdell\u2019s Neo-Surreal presents a series of found advertisements from Byte magazine, a computer hobbyist journal published from the late 1970s into the early 1980s. The images in Neo-Surreal combine people and computers in startling and unexpected cyborg forms. These strange advertisements expose the commercial engine which transformed the way people thought about the mind after World War II. In the early 20th century, popular psychology relied overwhelmingly on Freudian ideas of internal conflict and repression, sex drive and death instinct. During World War II, scientists working on anti-aircraft systems departed from the Freudian approach to the human mind. These scientists began instead to model humans as information-processing machines. In this computational metaphor, the mind became a program running on the brain\u2019s hardware. In the 1970s, the availability of personal computing helped spread the computational metaphor for the human mind. And as computing spread, the metaphor of mind as machine came to supplement and displace the older Freudian ideas in popular culture.\n\nIn a 1984 book on Computers and the Human Spirit, anthropologist Sherry Turkle argues that the computational metaphor reflects only one side of a two-way exchange of ideas. She observes that people speak of themselves as programs and of their computers as minds with desires and intentions. Looking at Odell\u2019s collection of images from Byte magazine, I am struck by the eerie sense of d\u00e9ja vu that comes from being outflanked by a profound truth. Turkle doesn\u2019t discuss computer advertising in her book, but I can imagine no more forceful illustration of the exchange of computational metaphors than the images in Byte magazine.\n\nVisual puns in the Byte ads establish an equivalence between human and machine. One ad shows a man inside a tie working simultaneously on computers running DOS and UNIX. The text of the ad puns on operating systems as personalities and promotes a software antidote to this \u201cpersonality problem.\u201d Elsewhere, the floppy disk is a popular motif: there is a woman with a floppy disk torso and a board meeting where humanoid robot board members are served floppy disks (computers as people, data as food). The editors of Byte used the computational metaphor playfully alongside the earnest advertisements. A full-page \u201cTechnology Update,\u201d on a \u201cTouring Machine,\u201d bicycle puns self-consciously on the notion of the cyborg human as an information-processing machine. Even the most oblivious reader could hardly miss the elision of thought and data in an image of a brain with a floppy disk half-inserted.\n\nThe analogy between mind and computer helped consumers grasp the novel capabilities of the computer. At the same time, this metaphor gave voice to newfound anxieties about the human self as a machine: predictable, interchangeable, and easily manipulated. Historically, these anxieties manifested as early as 1964 in protests at UC Berkeley. Marching against a university administration which was perceived as uncaring and bureaucratic, students declared that, \u201cAt Cal, you\u2019re little more than an IBM card,\u201d and carried signs announcing: \u201cI am a UC student. Please do not fold, bend, spindle, or mutilate me.\u201d These fears have broadened and intensified in recent years as advances in machine intelligence have enabled computers to perform increasingly sophisticated cognitive tasks. The decades-old images in Neo-Surreal capture the dystopian side of the computational metaphor with surprising freshness and relevance. In one ad, the rows of identical men with computers for faces suggest that we are becoming homogenized and controlled by our devices. The image of a desktop computer giving a press conference (\u201cI\u2019ll only use my exceptional powers for the good of mankind,\u201d) celebrates an inversion of control, a world in which humans hang back while robots run the show. The prospect of job automation is palpable too in the robot board meeting.\n\nYet, in the dark absurdity of Neo-Surreal, there is also a strange element of humor. I first encountered the images in Neo-Surreal on Odell\u2019s Twitter and Facebook feeds. There, accompanied by Odell\u2019s pithy and irreverent captions, the images made me laugh out loud. In the gallery, packed into a tight grid on the wall, the images became dry and humorless. Odell\u2019s work has long embraced the surreal. Working with aerial photographs, YouTube videos, and old maps, she defamiliarizes ordinary objects and situations by stripping away their context. In other works, like her Bureau of Suspended Objects, Odell reinvests discarded objects with meaning by assembling the lost information of their production, provenance, and use. In the Bureau, the isolation and distance of the gallery focus attention and heighten awareness of banal, everyday objects. By contrast, the images in Neo-Surreal are less surprising and interesting in the gallery than on social media. Perhaps the artistic nature of the commercial illustrations makes them more at home in the gallery setting. Interleaved instead with the everyday contents of social media, the images are unexpected, uncomfortable, and comical. To laugh at Odell\u2019s images is to recognize the uncomfortable truth that the strange world they depict \u2014 of cyborg confusion, of technological promise and obsolescence \u2014 has become our own.\n\nIn another piece in the Internet Archive artist-in-residency exhibition, called Browser History, Jenkins shows a series of clay tablets impressed with web pages representing \u201ctrade, lifestyle, art, government, and other aspects of our society that are similar to the kinds of information that we have about ancient civilizations.\u201d Where Odell\u2019s images from Byte depict the present by way of the recent past, Jenkins\u2019 clay tablets look at contemporary culture and society from a distant, post-apocalyptic future. Clay tablets covered in cuneiform script are some of the earliest known records of civilization; the oldest such tablets date back to around 3000 BC. After the exhibition, the tablets in Browser History will be scattered across California: hidden in caves, buried underground, and submerged underwater. Jenkins speculates that these tablets could last for as many as 40,000 years. Clay tablets are the most primitive of recording technologies. They are low-tech and low-res, the antithesis of the colossal data centers which host the Internet Archive. To create such a time capsule is to imagine the possibility of its necessity. To view it is to inhabit a time in which the clay tablets are not simply a record of the year 2017 AD, but the only surviving record. Browser History draws viewers imperceptibly into the far future and invites them to look back 40,000 years and decipher our contemporary culture through the Internet.\n\nGazing across the gulf of years that separates me from the freshly fired clay tablets in front of my nose, I observe how little they convey of the Internet and of our contemporary culture. The unique formal qualities of the Internet are utterly lost: its hyperlink structure, movement and interaction, the color and resolution of videos and images, and everything \u201cbelow the fold.\u201d The tablets bear tantalizing traces of a depth and complexity that exceeds them. The Wikipedia page on \u201cInternet\u201d describes \u201cinter-linked hypertext documents,\u201d and contains affordances to discover \u201cWhat links here\u201d and obtain a \u201cPermanent link.\u201d In another instance of recursive self-reference (a favorite trope of computer culture since the \u201880s), Porn.com declares itself \u201cThe biggest thing to happen to porn since \u2018The Internet.\u2019\u201d The tablets in Browser History are more like screenshots than webpages. I knew as soon as I saw the tablets that they depicted different sites on the Internet. It is precisely this unstated unifying theme which will be most invisible to our distant descendants. Will our distant descendants understand that these tablets are mere representations of another medium? Or will they declare confidently that primitive humans in the year 2017 AD communicated by exchanging clay tablets stamped with words and images?\n\nClay tablets are just as inadequate for conveying the collective hopes and tragedies of our culture as they are for capturing the formal qualities of the Internet. Numbers impressed in clay without context give little sense of their meaning. My descendants will feel none of my dismay in looking at the craigslist and CareerBuilder tablets, which show rental prices in San Francisco (too high!) and salaries for teachers in the Bay Area (too low!). The tablets of Donald Trump\u2019s Twitter profile and Philando Castile\u2019s Facebook wall are the most fraught, and most heavily freighted with meaning. A meagre three tweets give no hint of the racism, misogyny, and transphobia Trump has expressed in his eight months in office, of the disgust and adoration he inspires \u2014 or of the role that his vast Twitter following played in launching him to the Presidency. The photo on Castile\u2019s wall, \u201chidden because\u2026of mature content, such as graphic violence,\u201d obscures the terrible injustice of his death, and the chilling pathos of the Facebook Live video (briefly censored by Facebook) that broadcast to millions of Americans his death at the hands of police. Browser History is eloquent in its austerity.\n\nJenkins and Odell are complementary media archaeologists, and the placement of Neo-Surreal and Browser History side-by-side in the gallery enriches both pieces. The images that Odell gathers in Neo-Surreal existed a mere 40 years ago in (almost) exactly the same form under the paintbrush or pencil of a commercial illustrator. Those originals are inaccessible today, having been forgotten or discarded long ago. Today, they exist only in the pages of Byte magazine. Like an archaeologist for the digital age, Odell brushes away the crust of words to recover pure images. The strangeness of Odell\u2019s forty-year-old images heightens the vertigo of Jenkins\u2019 40,000-year retrospective on the present. Browser History is an act of reverse archaeology, an artifact for Odell\u2019s someday-successors to discover and decipher.\n\nBoth artworks express a sense of anxiety that, in gaining mastery over our world, we are losing control of ourselves. Browser History clings protectively to Internet culture and our Internet selves, while Neo-Surreal embodies the fear that computers will automate human jobs and replace organic human consciousness. Though the technologies are new, these are ancient anxieties, experienced many times over the millennia with the invention of writing, the printing press, photography, and radio (to name a few). Freud observed that rational fear or \u201creal fear\u201d arises from ignorance. The fear of of technology is surely as real as any other, for we have not yet managed to understand the way that technology transfigures us (though many scholars and critics have tried: Mumford, McLuhan, Borgmann). Only when we grasp the pattern of technology in its entirety will we be able to work through the fear of losing ourselves in it.\n\nMagic and Loss: The Internet As Art (Virginia Heffernan, 2017)\n\nThe Second Self: Computers and the Human Spirit (Sherry Turkle, 1984)"
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/generative-machine-learning-on-the-cloud-1ccdfeb33ea2?source=---------5",
        "title": "Generative Machine Learning on the Cloud \u2013 Artists and Machine Intelligence \u2013",
        "text": "In the last year we\u2019ve witnessed rapid advancements in hardware capabilities, continued development of user-friendly machine learning libraries, and AI connectivity for the maker community. With this backdrop of increasingly user-friendly AI, I spent the summer working with Google\u2019s Artists & Machine Intelligence (AMI) program on a cloud-based tool to make generative machine learning and synthetic image generation more accessible, especially to artists and designers. This post will explain some common generative model structures as well as pitfalls and resources for people interested in coding their own.\n\nBefore I get to the project, a little about me.\n\nMy name is Emily Glanz. I graduated from the University of Iowa about a year ago with a B.S. in Electrical Engineering, and have been working on various Google teams as part of the Engineering Residency program. My experience with machine learning prior to AMI was centered around prediction and classification tasks while working on a hearing loss diagnostic tool in college.\n\nThe goal of my AMI project was to lower the barrier to entry for using Google\u2019s Cloud ML infrastructure. I wanted to make it easier to train and use generative models for creative applications. Using the cloud gives users easy access to GPUs for training without needing to set up a workstation. A concise and easy to use TensorFlow example acts as a perfect starting point for modifications and customization. Check out the project on Github: GenerativeMLonCloud.\n\nThe end-to-end system design allows a user to provide a custom dataset of images to train a Variational Autoencoder Generative Adversarial Network (VAE-GAN) model on Cloud ML. From here, their model is deployed to the cloud, where they can input an embedding to have synthetic images generated from their dataset or input an image to get an embedding vector. In addition, I created an App Engine web application to demonstrate using Cloud ML\u2019s python API to interact with the deployed model. The scope for the current tool focuses on generative images, but we hope to add examples in the future that deal with other inputs such as text or audio.\n\nTo kick off this project, I took the route taken by many jumping into neural nets: a few days spent on TensorFlow tutorials, a couple read throughs of Chris Olah\u2019s fantastic blog, and some nice time digging through the endless examples of generative neural nets on Github.\n\nAfter the Convolutional Neural Net (CNN) MNIST tutorial by TensorFlow, I was ready to dive headfirst into generative image models. I looked first at Variational Autoencoders (VAEs), then Generative Adversarial Networks (GANs), and ended up using a combo of a VAE-GAN as the final model for the image to image model.\n\nThe first step in developing the generative tool was generating handwritten numbers using a VAE which I created from the CNN tutorial. I started with a VAE as this particular network has been one of the most popular approaches to generative imagery in the past couple years. The MNIST dataset is commonly used as it is a standard benchmark for image based neural networks:\n\nThe MNIST dataset is a great black and white image dataset to get started with \u2014 TensorFlow even provides a nicely formatted version of the set with their library.\n\nA VAE is comprised of an Encoder network and a Decoder network. The Encoder takes input images and encodes them into embedding vectors that live in a latent space. The latent space has a lower-dimensionality than the input image which is why it is sometimes referred to as the \u2018bottleneck\u2019 of the network. This bottleneck forces the Encoder to learn an information-rich compression of the raw input data as it maps the image to the latent space. For example, one of the features learned by the VAE could be the amount of \u2018smile\u2019 in a face. A constraint is added to the Encoder that forces the network to create latent vectors that follow a unit gaussian distribution. The Decoder can reconstruct the given input from these embeddings. These models become generative when a randomly sampled vector from the unit gaussian (the distribution enforced in the Encoder) is passed into the Decoder; simultaneously the Decoder learns to use these embeddings to generate synthetic images from the latent space. The network is trained end-to-end: the Encoder learns the most important features of the input image, allowing the Decoder to reconstruct the input image from the latent vector representation.\n\nIn the variant I used, I have a couple convolutional layers in my encoder and a couple of convolutional transpose layers (aka \u201cdeconvolution\u201d) in my decoder (I\u2019ll get more into the detailed architecture of the VAE later). I found this tutorial a great explanation of VAEs.\n\nAt this point I took a detour into Conditional VAE (CVAE) land, and used the MNIST dataset to play with this autoencoder variation that conditions on label information to allow the user to specify which number they would like to generate. The CVAE is trained by appending the one-hot encoded vector representing the label of the input image (so if the input image is a 9, the label vector is [0,0,0,0,0,0,0,0,0,1]) to the input image and the latent space vector. Then to request a specific generated number, the user can input a random embedding sampled from the unit gaussian distribution combined with the one-hot encoded vector of the number desired.\n\nOne fun thing that can be done with a CVAE is to mix together two labels (in this case two numbers). Usually, we\u2019re only supposed to set one of the bits of the one-hot vector high, but what happens if we set two bits high? What if we asked the CVAE to generate an image with the condition [0,0,1,0,0,1,0,0,0,0]? This is essentially asking the CVAE to generate an image with label 2 and 5. In this case, the decoder tries to generate an image that matches this condition, resulting in an image that looks like a 2 combined with a 5. Beyond digits, this feature of CVAEs could be used to combine images of different labels \u2014 one such application could be used for generating synthetic faces matching specific attributes, like \u2018female, brunette, etc\u2019. This chart shows what happens when each number (0 through 9) is combined with 2:\n\nFor the above image, the number requested is the desired number (0 through 9) OR\u2019ed with the one-hot representation of 2. For example, to get a 7 combined with a 2, the embedding vector looks like: [0,0,1,0,0,0,0,1,0,0] concatenated with a random sampling from the unit gaussian!\n\nNext, it was time to add a GAN (Generative Adversarial Network) loss onto the end of the VAE to sharpen the generated output. VAE\u2019s tend to make images blurry because of the way the network is penalized while training. For VAEs, the reconstruction cost (typically a mean-squared-error (L2) loss), penalizes slightly moved edges and features with respect to the input image. Adding a GAN, which uses an adversarial loss of the Generator vs Discriminator described below, sharpens the output as this loss is more forgiving to exact reconstruction and focuses on the realism of the image features instead.\n\nA GAN is trained using adversarial learning. A GAN is comprised of two networks, a Generator and a Discriminator. The Discriminator\u2019s goal is to correctly distinguish between \u201creal\u201d and \u201cfake\u201d input (in this case, real MNIST images from generated MNIST images). The Generator\u2019s goal is to produce output that fools the Discriminator. These two networks play a game to see who can beat whom. In the case of adding a GAN loss to the VAE, the VAE supplies the generator and all we need to do is add a discriminator network. Check out this blog for a more thorough run down on GANs.\n\nFrom this point, I started developing a model for RGB images. I took the VAE-GAN architecture I had used with the MNIST digits, and beefed it up with inspiration from: this DCGAN, this training technique, and this VAE-GAN on Github. The DCGAN link is where most of the layer architecture originated for this VAE-GAN.\n\nA very simplified view of the network looks visually like this:\n\nBatch normalization was used in each of the networks. While training the VAE-GAN I encountered all the woes associated with GAN training, including mode collapse, exploding gradients, and generated noise.\n\nAnother way to explore the embedding space is using spherical linear interpolation, aka slerp. This technique, introduced in this paper and applied to VAEs and GANs in this paper allows me to traverse the space between two known embedding vectors. For example, I can take an image of a smiling women, take the picture of a not smiling man, and explore the transformation between the two in the latent space.\n\nThe top images are the reconstructions of the first input image, and the bottom images are the reconstructions of the second input image, with the images between the transitions. The second from the right column shows the result of using a non-face image, a picture of a crow, in the mix. The rightmost column shows the result of two non-face images (a dog and a cat). The autoencoder has trouble reconstructing the images of animals because the latent space has been customized for faces, specifically from those of the CelebA dataset.\n\nWhere q1 and q2 are the embedding vectors produced by the encoder from the two input images, is the angle between the two vectors, and parameter mu varies from 0 to 1.\n\nTraining generative networks can be tricky and it\u2019s worth recognizing some of the common ailments and their remedies, so let\u2019s detour into the previously mentioned woes.\n\nThis case shows that a generator does not always converge to the data distribution.\n\nAt first it appears that the VAE-GAN is starting to learn different features (like hair, face orientation, almost sunglasses at one point) but then we see the generator (in this case the decoder of the VAE portion) breaks down and only produces one example. Playing around with learning rates for the networks and batch normalization solved this problem in my specific case.\n\nThe generator first just generates images of a solid color. The generator is not successfully generating images even close to face-like. Training the generator and discriminator based on loss thresholds kept one network from getting too much stronger than the other for me.\n\nCase: Learning rate too high for VAE over discriminator network\n\nBy just altering the learning rate of the VAE, the network started to generate noisy faces. Experimenting with learning rates for the different optimizers was tricky.\n\nChoosing the appropriate embedding size, number of training steps, etc. is crucial to getting realistic output from a GAN. I found this github site to have some awesome tips of GAN training.\n\nAn old version of the model:\n\nOnce I had a generative model for images, it was time to really solidify the end-to-end system, the main goal of this project.\n\nThe dream is to allow a user (with a directory of images) to train their very own VAE-GAN model on their very own image dataset.\n\nHere is an overview of the steps required to make the user\u2019s generative model dreams a reality:\n\nTo get the tool up and running on Cloud ML, first the Cloud environment has to be set up. A Cloud Platform project has to be set up on the projects page, billing has to be setup, and then the Cloud ML engine and Compute engine APIs have to be enabled. To use the command line interface, the Cloud SDK must be installed. Follow these instructions to set up the cloud environment.\n\nFrom here, the user can begin running training jobs. I created a script that allows the user to specify an image directory and then takes care of preprocessing the images and starting the training job on Cloud ML. Other flags allow the user to further tune their training/preprocessing tasks such as center-cropping the images or which port to start their TensorBoard instance (TensorBoard: the greatest way to monitor any TensorFlow training).\n\nAnother script I created allows users to create and deploy their models they created from running training jobs on Cloud ML. Once on Cloud ML, getting generated images or image embeddings is one API call away.\n\nPlaying around in with VAEs and GANs let me generate some fun images:\n\nThe MNIST dataset and CelebA dataset are great datasets to test and develop a network \u2014 but what else could one use to autoencode and generate?\n\nHere are some of my favorite generative art projects for inspiration:\n\nThis work was supported by Google\u2019s Engineering Residency program, on my rotation with Artists and Machine Intelligence. I\u2019d like to thank Larry Lindsey and Mike Tyka, for guiding me in generative machine learning and TensorFlow, as well as the entirety of AMI for answering any questions I had and giving me fantastic insight into the world of AI. Huge shoutout to Jac de Haan and Kenric McDowell for all the support for the project as well."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/neural-nets-for-generating-music-f46dffac21c0?source=---------6",
        "title": "Neural Nets for Generating Music \u2013 Artists and Machine Intelligence \u2013",
        "text": "Dice games, Markov chains, and RNNs aren\u2019t the only ways to make algorithmic music. Some machine learning practitioners explore alternative approaches like hierarchical temporal memory, or principal components analysis. But I\u2019m focusing on neural nets because they are responsible for most of the big changes recently. (Though even within the domain of neural nets there are some directions I\u2019m leaving out that have fewer examples, such as restricted Boltzmann machines for composing 4-bar jazz licks, short variations on a single song, or hybrid RNN-RBM models, or hybrid autoencoder-LSTM models, or even neuroevolutionary strategies).\n\nThe power of RNNs wasn\u2019t common knowledge until Andrej Karpathy\u2019s viral post \u201cThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d in May 2015. Andrej showed that a relatively simple neural network called char-rnn could reliably recreate the \u201clook and feel\u201d of any text, from Shakespeare to C++. The same way that the popularity of dice games was buffeted by a resurgence of rationalism and interest in mathematics, Andrej\u2019s article came at a time when interest in neural networks was exploding, triggering a renewed interest in recurrent networks. Some of the first people to test Andrej\u2019s code applied it to symbolic music notation.\n\nSome people started with char-rnn as inspiration, but developed their own architecture specifically for working with music. Notable examples come from Daniel Johnson and Ji-Sung Kim.\n\nChristian Walder uses LSTMs in a more unusual way: starting with a pre-defined rhythm, and asking the neural net to fill in the pitches. This provides a lot of the global structure that is otherwise usually missing, but heavily constrains the possibilities.\n\nWhile all the examples so far are based on symbolic representations of music, some enthusiasts pushed char-rnn to its limits by feeding it raw audio.\n\nUnfortunately it seems that char-rnn is fundamentally limited in its capacity to abstract higher level representations of raw audio. The most inspiring results on audio turned out to be nothing more than noisy copies of the source material (some people explain this when sharing their work, see SomethingUnreal modeling his own speech). In machine learning this is related to the concept of \u201coverfitting\u201d: when a model can recreate the training data faithfully, but can\u2019t effectively generalize to anything novel that it hasn\u2019t been trained on. During training, initially first the model performs poorly on both the training and novel data, then it starts to perform better on both. But if you let it train too long, it gets worse at generalizing to novel data at the expense of recreating the training data. Researchers stop the training just before hitting that point. But overfitting is not so clearly a \u201cproblem\u201d in creative contexts, where recombination of existing material is a common strategy that is hard to distinguish from \u201cgeneralization\u201d. Some people like David Cope go so far as to say \u201call music [is] essentially inspired plagiarism\u201d (but he has also been accused of publishing pseudoscience and straight-up plagiarism).\n\nIn September 2016 DeepMind published their WaveNet research demonstrating an architecture that can build higher level abstractions of audio sample-by-sample.\n\nInstead of using a recurrent network to learn representations over time, they used a convolutional network. Convolutional networks learn combinations of filters. They\u2019re normally used for processing images, but WaveNet treats time like a spatial dimension.\n\nLooking into the background of the co-authors, there are some interesting predecessors to WaveNet.\n\nOne of my favorite things to emerge from the WaveNet research is this rough piano imitation by Sageev Oore, who was on sabbatical at Google Brain at the time.\n\nIn April 2017, Magenta built on WaveNet to create NSynth, a model for analyzing and generating monophonic instrument sounds. They created an NSynth-powered \u201cSound Maker\u201d experiment in collaboration with Google Creative Lab New York.\n\nIn February 2017 a team from Montreal lead by Yoshua Bengio published SampleRNN (with code) for sample-by-sample generation of audio using a set of recurrent networks in a hierarchical structure. This research was influenced by experiments from Ishaan Gulrajani who trained a hierarchical version of char-rnn on raw audio.\n\nBoth SampleRNN and WaveNet take an unusually long time to train (more than a week), and without optimizations (like fast-wavenet) they are many times slower than realtime for generation. To reduce the training and generation time researchers use audio at 16kHz and 8 bits.\n\nBut for companies like Google or Baidu, the primary application of audio generation is text to speech, where fast generation is essential. In March 2017 Google published their Tacotron research, which generates audio frame-by-frame using a spectral representation as an intermediate output step and a sequence of characters (text) as input.\n\nThe Tacotron demo samples are similar to WaveNet, with some small discrepancies. One limitation of generating audio frame-by-frame is that the final synthesis step relies on Griffin-Lim phase reconstruction, which is not robust for polyphonic and noisy audio, and limits this architecture to speech (and possibly monophonic instruments, though I haven\u2019t heard any examples). Baidu built on the Tacotron architecture with their Deep Voice 2 research, increasing the audio quality by adding some final stages specific to speech generation. There is a wealth of other research related to speech synthesis, but very little seems relevant to music."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/rethinking-design-tools-in-the-age-of-machine-learning-369f3f07ab6c?source=---------7",
        "title": "Rethinking Design Tools in the Age of Machine Learning",
        "text": "The creative reach of the individual is expanding.\n\nThe assortment of available tools, platforms and devices for design is growing while their costs are diminishing. You can make a film, record an album, design a city or print your own flower pot. You can do all of this on a home computer or even on your phone.\n\nNaturally, we all want to try our hands at these exciting new possibilities. We want to revel in the freedom to make things. We like the idea of being Renaissance thinkers, makers, doers.\n\nBut just because the tools have gotten cheaper, more accessible and easier to use does not mean that it\u2019s gotten any easier to create a powerful image or tell a compelling story.\n\nSaying something original is as difficult as ever. Structuring the many parts of an aesthetic experience is as difficult as ever. These things still take expertise, practice, experience.\n\nDesign tools and programming languages put a lot of power in our hands. But the power isn\u2019t really ours unless we know how to use it. And not just use it, but do something of our own, have an original idea within a given medium.\n\nDesign tools shouldn\u2019t help you to feel like you\u2019re making something. They should help you to actually make something of your own.\n\nI would like to suggest that machine learning can help us to simplify design tools without limiting their expressivity, without taking creative control away from the designer.\n\nThis may seem totally counter-intuitive. When we think of machine learning or artificial intelligence, we think of automation.\n\nIn a design context, we would probably imagine something like this:\n\nwhich in all cases would return this image:\n\nThere is no getting around the fact that producing an original design involves making a lot of decisions. Decisions take time.\n\nAs a result, design tools have tended towards two opposite extremes:\n\nOn one end of the spectrum, we have the \u201cone-size-fits-all\u201d approach, which can generally be found in consumer-level design tools \u2014 tools that simplify design processes by forcing users into one of a handful of pre-ordained templates.\n\nOn the other end of the spectrum, we have the \u201ckitchen sink\u201d approach, utilized by professional design tools \u2014 tools that provide an overwhelming number of low-level features that come with steep learning curves and often do not coincide with the user\u2019s way of thinking.\n\nAt first, it would appear that machine learning offers a slightly more sophisticated version of the \u201cone-size-fits-all\u201d approach \u2014 a way of simplifying design processes by shifting some of the decision-making responsibilities away from the designer.\n\nI\u2019ll admit that machine learning can be used this way and is likely to be, especially in its early years. But it also offers much richer possibilities.\n\nThough we can\u2019t really change the number of decisions involved in a design process, we can change what\u2019s involved in making those decisions.\n\nI\u2019d like to talk about a few ways that machine learning can transform how we interact with design software and make decisions through it.\n\nThey are: emergent feature sets, design through exploration, design by description, process organization and conversational interfaces.\n\nI think these ideas have the potential to streamline design processes without taking creative control from the designer.\n\nBut, perhaps even more exciting is that these mechanisms will enable designers to give their entire focus to the design work itself rather than on learning how to map their ideas to the ways in which a particular tool has been organized.\n\nIn other words, the designer will lead the tool, and not the other way around.\n\nWhen a designer sits down to produce a design, maybe they have an exact picture in their mind of the end product and maybe they don\u2019t.\n\nIn either case, they need to find their way to that end product by uncovering a sequence of component tool actions that transform the blank canvas into the final product.\n\nThis reminds me of a famous quote:\n\nI like this quote because it frames artistic and design processes as a kind of search.\n\nA block of marble has certain concrete boundaries and within these boundaries, an infinite number of possible sculptures exist simultaneously. The artist\u2019s job is to discover the needle in the haystack \u2014 a particular combination of properties that meet a specific set of requirements.\n\nThis is much like a chemist searching for a new molecule or a chef looking for a new flavor combination.\n\nThe search-spaces may be entirely different for these problems, but there is a definite similarity of process in that every design problem relates to a particular set of interrelated properties and constraints.\n\nLet\u2019s look at some of the considerations that might be involved in designing a household object such as a wine glass.\n\nIf we make the glass taller, we probably need to widen the base to prevent it from tipping over too easily. Here, we vary two properties with respect to one constraint.\n\nWhen we first encounter the problem, we may not have an internal sense of the constraint: \u201cAt what point does this property ratio cause the glass to tip over?\u201d\n\nWe gain expertise by experimenting within the search space, learning the relation of properties to one another and to an initially unknown set of constraints imposed by the physical world.\n\nLet\u2019s imagine this search-space as a very large map in which each possible end-state is represented by a unique set of coordinates.\n\nIn this map, each feature of the software acts as a road that takes us a certain distance in a particular direction.\n\nA low-level feature (within a professional design tool) would be equivalent to a local road in the sense that it moves us only a short distance within the map.\n\nIf we want to make the glass larger, we could apply a sequence of low-level commands.\n\nOr, we could distill this sequence into a single high-level feature, like those offered by consumer design tools. This feature would act more like a highway.\n\nThe nice thing about highways is that they take us great distances with a relatively small number of component actions.\n\nThe problem, however, is that highways only have exit ramps at commonly-visited destinations. To reach more obscure destinations, the driver must take local roads, which requires a greater number of component actions.\n\nYet, in most consumer design tools, we are not even given the option to take local roads.\n\nPerhaps a slightly different destination (just a short distance from the exit ramp) would have been more to our liking, but we have no way of reaching it. Furthermore, we may not even be aware of this alternate destination or the effect it would have upon our overall design goals.\n\nSo, while high-level tools have the benefit of moving us through the search-space more quickly, they also reduce expressiveness or the ability to move anywhere within the space.\n\nAs a result, many users will stop at more easily reachable destinations, leaving vast regions of the map unexplored.\n\nA determined user may find a winding path to their destination.\n\nBut if the exact properties of that destination are not fully articulated ahead of time in the user\u2019s mind, it is unlikely that he or she will navigate there organically.\n\nSo, while a high-level tool may make only small regions of the map completely inaccessible, its fragmentation of the search-space makes much larger regions practically inaccessible.\n\nIn this sense, consumer-level design tools do not extend the human reach, they force our creative explorations into narrow passageways.\n\nIf we want to maintain creative freedom, we seemingly need to either stick to low-level operations or generate a very large number of high-level features that would cover a wider range of possible use cases but would also sacrifice the succinctness of the tool\u2019s vocabulary.\n\nIdeally, a new highway would be constructed for us whenever we leave home so that we could arrive at any possible destination with a small number of actions.\n\nBut this is not possible within a pre-built high-level feature set.\n\nMachine learning allows us to extrapolate a great deal of information about users and what they wish to achieve through the observation of their behaviors.\n\nRather than trying to anticipate the designer\u2019s needs through a pre-built high-level feature set, we can instead create tools that learn from the designer\u2019s engagement with the software.\n\nOver the last few years, a type of machine learning system called \u201crecurrent neural networks\u201d have been shown to be particularly adept at learning sequential patterns. These systems have been applied to tasks such as predicting the next characters in a string of text or the next notes in a piece of music.\n\nRather than constructing design tools around an immutable set of pre-built high-level features, a recurrent neural network can be employed to discover commonly-used sequences of low-level features and dynamically synthesize purpose-built features related to the designer\u2019s current activity.\n\nThe behavioral patterns used in the automated production of custom high-level features can be mined from individual designers or across many designers.\n\nSomewhat like recommendation systems that suggest music or movies based on the similarities of users\u2019 tastes, the discovery of patterns across numerous designers can be employed to suggest relevant features to an individual based on the workflows he or she tends to utilize.\n\nThis will allow toolmakers to better address the diversity of designers and their varied ways of digesting information, making decisions and interacting with software.\n\nIt will enable toolmakers to meet designers where they are rather than asking them to adapt to a singular pre-ordained mentality or workflow offered by a more conventional, static feature set.\n\nBy extrapolating behavioral patterns across many users, toolmakers can better understand the implicit relationships between the features offered within their systems.\n\nThis will provide toolmakers with important insights for how to improve their software.\n\nBy adopting this methodology, the toolmaker\u2019s role would shift away from the overall curation of high-level functionality and towards the creation of more granular interface elements.\n\nThis movement from preset rule systems and interfaces to implicit, intelligently-generated ones means that toolmakers would be relinquishing some control of certain aspects of the software.\n\nDoing so, however, would enable designers to address tasks that have not been explicitly anticipated by the toolmaker.\n\nEveryone has an innate sense of aesthetics and design \u2014 a feel for what is pleasing or useful.\n\nBut many of us lack the vocabulary, methodology or confidence to apply these intuitions towards actual creative output in a design field with which we have no prior experience.\n\nDesign tools shouldn\u2019t only help us to execute a design in a domain we already understand, they should also help us to build expertise in new design domains.\n\nIf we were to stop a random sample of people on the street, give them a blank piece of paper and ask them to design their ideal living room, many people would not know where to start.\n\nBut, if we instead gave them to access to Pinterest and asked them to design a living room by picking and choosing elements they like, many people would have a much easier time with the task.\n\nThis \u201cknow it when I see it\u201d sensibility can form a powerful mechanism for driving our interactions with design tools.\n\nThe user doesn\u2019t have to memorize the behavior attached to some obscure function name within a complex menu system. Instead, they can see the behavior applied to a copy of the scene and decide whether they like it.\n\nEarlier, we looked at a two-dimensional visualization of a two-dimensional search-space. Though limited in scope, this visualization offers a straightforward yet fine-grained mechanism for design. The user simply needs to point to a location to arrive at any possible design within the boundaries of the search-space.\n\nThis spatial organization also enables the user to build a clear mental model for the effect of a given translation within the search-space.\n\nOf course, very few real-world design problems are comprised by only two axes of variability. But, through the use of what is called a \u201cdimensionality reduction\u201d machine learning system, it is possible to produce a low-dimensional map of a high-dimensional feature-space.\n\nIn the animation above, I have presented a set of images of leaf silhouettes to a dimensionality reduction system.\n\nAs the training process unfolds, the algorithm reconfigures the position of each leaf in a two-dimensional map in order to find an arrangement in which similar leafs are positioned near one another.\n\nThis ultimately creates a continuous two-dimensional map that captures the full range of variability that was expressed by the original leaf images.\n\nOnce trained, this system can reconstruct the image associated with any two-dimensional coordinates within the map\u2019s boundaries.\n\nRemarkably, this can be done with coordinates for which no training sample was provided, thereby offering a simple mechanism for quickly constructing novel variations of a design.\n\nWe are free to visit any position on the map. We can explore the full range of possible entities that exist within the conceptual domain of leaves. We have equal access to all points and can freely jump between them. We can, for instance, visit the coordinates that represent the halfway point between a maple and oak leaf.\n\nIf we started with a fixed idea of what we wanted to design, we can use this process to arrive at it. But, if we want to explore a bit further, see what else might be possible, this approach allows us to move outward from our solution and try new things.\n\nAlternatively, this map view could be temporarily superimposed over the project view. This would allow the user to explore possible variations of an element without losing a sense of the context in which it is embedded.\n\nThis sort of exploratory interface makes it possible to change elements of a design without having to redo them from scratch.\n\nFor example, let\u2019s say we\u2019ve created the above drawing of an oak leaf using Bezier paths and later decide that we want something that looks more like a maple leaf.\n\nTo do so in traditional design software, we need to map this transformation to the logic of a Bezier path.\n\nAs designers, we\u2019re so accustomed to this workflow that it seems completely natural. But, in truth, the knowledge of how to manipulate a Bezier path is only tangentially related to the problem at hand.\n\nTransforming one shape into the other is likely to undo much of our original effort in creating the first shape.\n\nThis adds a high cost to exploration. It shouldn\u2019t be this way. To get to our neighbor\u2019s house, we shouldn\u2019t need to travel across town.\n\nA variation control surface allows designers to be guided by their design intuitions rather than being limited by the proclivities of a particular tool\u2019s way of abstracting the path to a given destination.\n\nThrough this approach, we are not reducing the designer\u2019s control, we are simply removing the auxiliary demands and conceptual remapping imposed by an earlier generation of tools.\n\nThis frees designers to focus on building their expertise in the design decisions themselves rather than on the technical mechanisms through which those decisions are fulfilled.\n\nThe kinds of interfaces we\u2019ve discussed so far are very much like maps in the conventional sense of the word.\n\nLike any other map, we can add textual labels \u2014 street signs if you will. This allows us to navigate the design space verbally with commands like: \u201ctake me to a maple leaf.\u201d Once there, we could say something like: \u201ctake me a bit closer to an oak leaf.\u201d\n\nThis is really powerful on its own. But, it turns out that we can take this idea even further\u2026\n\nIn 2013, Tomas Mikolov and others released a series of papers describing a set of techniques for producing low-dimensional maps that represent the conceptual relationships between words.\n\nMuch like the maps we\u2019ve been discussing, words that are closely aligned in their real-world usage will also be located near one another in the word embedding map.\n\nMore incredibly, though, Mikolov and his collaborators discovered that it is possible to apply conceptually meaningful algebraic transformations to word vectors.\n\nIn other words, they discovered that you can perform algebra on real-world concepts.\n\nFor example, they showed that the result of the word-vector expression:\n\nis closer to \u201cParis\u201d than to any other word vector.\n\n\u201cKing \u2014 Man + Woman\u201d results in a vector very close to \u201cQueen\u201d.\n\nThis fascinating mechanism provides a completely new way of thinking about design tools. It allows us to operate on visual concepts visually or linguistically without the use of auxiliary abstractions and control systems.\n\nFor example, if we were looking for an aesthetic that was like Picasso\u2019s but not from the height of his (analytic) Cubist period, we could say something like:\n\nWe could do the same with auditory information or in any other medium.\n\nIn the last few years, similar techniques including \u201cStyle Transfer\u201d and \u201cNeural Doodle\u201d have extended these mechanisms even further.\n\nThese techniques have been implemented in photo-sharing apps \u2014 not as features within more extensive design tools, but as a kind of novelty image filter, somewhat like Instagram or Photoshop filters.\n\nAs the Photoshop \u201cfilter bubble\u201d of the 1990s proved, the novelty presentation of this functionality quickly turns to kitsch and does little to re-conceptualize or extend design processes in a meaningful way.\n\nBut as individual components of a larger and more wholistic design framework, these techniques provide a powerful mechanism for operating on media without leaving their native vocabulary, without mapping them onto an abstraction.\n\nThey allow us to explore and compose ideas through direct manipulation of the concept space in which these ideas reside.\n\nBut, as transformative as techniques may be, I think there\u2019s still something missing.\n\nEvery designer knows that the hardest thing about design isn\u2019t what\u2019s involved in making an individual decision. The hard part is reconciling many component decisions to one another in order to produce a cohesive whole.\n\nAs designers, we have to move back and forth between many component decisions while keeping the whole in mind. Sometimes these component decisions conflict with one another.\n\nLike a Rubik\u2019s cube, we can\u2019t simply solve one side of the cube and then move onto the next. This would cause us to undo some of our earlier work. We must solve all sides simultaneously.\n\nThis can be a very complex process and learning to navigate it is at the heart of what it means to become a designer.\n\nWhile the machine learning techniques we\u2019ve discussed can help to streamline these component decisions, they do not fully address this most difficult aspect of design.\n\nTo help designers build this kind of expertise, let\u2019s explore two more concepts\u2026\n\nSimple expressions that communicate an individual command or point of information are more easily understood by machine learning systems than complex, multifaceted statements. However, one of the most difficult processes in designing something is thinking through how to break down a complex, dynamic system into discrete parts.\n\nOne of the most useful things design tools could do would be to help the designer through this process.\n\nTools could help the designer to deliver concise statements by creating interfaces and workflows that lead the user through a series of simple exercises or decision points that each address a single facet of a much larger and more complex task.\n\nAn excellent example of this approach is 20Q, an electronic version of the game Twenty Questions.\n\nLike the original road trip game, 20Q asks the user to think of an object or famous person and then poses a series of multiple choice questions in order to discover what the user has in mind.\n\nThe first question posed in this process is always: \u201cIs it classified as Animal, Vegetable, Mineral or Concept?\u201d\n\nSubsequent questions try to uncover further distinctions that extend from the information that the user has already provided.\n\nFor example, if the answer to the first question were \u201cAnimal,\u201d then the next question posed might be \u201cIs it a mammal?\u201d\n\nIf instead the first answer were \u201cVegetable,\u201d the next question might be \u201cIs it usually green?\u201d\n\nEach of these subsequent questions can be answered with: Yes, No, Sometimes or Irrelevant.\n\n20Q guesses the correct person, place or thing 80% of the time after twenty questions and 98% of the time after twenty five questions.\n\nThis system uses a kind of machine learning algorithm called a Learning Decision Tree to determine the sequence of questions that will lead to the correct answer in the smallest number of steps possible.\n\nUsing the data generated by previous users\u2019 interactions with the system, the algorithm learns the relative value of each question in removing as many incorrect options as possible so that it can present the most important questions to the user first.\n\nFor example, if it were already known that the user had a famous person in mind, it would likely be more valuable for the next question to be whether the person is living than whether the person has written a book because only a small portion of all historic figures are alive today but many famous people have authored a book of one kind or another.\n\nThough none of these questions individually encapsulates the entirety of what the user has in mind, a relatively small number of well-chosen questions can uncover the correct answer with surprising speed.\n\nIn addition to aiding the system\u2019s comprehension of the user\u2019s expressions, this process can benefit the user directly in his or her ability to communicate ideas more clearly and purposefully.\n\nAt its core, this process can be seen as a mechanism for discovering an optimal path through a large number of interrelated decisions.\n\nEach question and answer interaction serves as a translation vector through concept-space, moving the user a bit closer to his or her intended output while also probing the user to think about and articulate each facet of the idea.\n\nThis mechanism can also be extended to a design interface, allowing the user to hone in on a desired form by answering a series of questions about it. Drawing on natural modes of interaction, these questions could be answered either verbally\u2026\n\n\u2026Or gesturally, preventing the user from needing to learn a complex menu system in order to access the tool\u2019s capabilities:\n\nBuilding on recent advances in machine learning, it is increasingly possible for the machine to answer the user\u2019s complex, contextual questions about the properties of a design:\n\nFor example, the user could pose factual questions that would help him or her to evaluate the design\u2019s suitability for some intended use:\n\nThis dialogue would imitate the form of human conversation, but would benefit from the machine\u2019s omniscient knowledge of the design\u2019s properties.\n\nThis could also be tied to the machine\u2019s ability to model real-world constraints such as material, physical or chemical ones:\n\nBy embedding this capability within a realtime interaction, an architect, for example, could save a great deal of time by being able to quickly eliminate nascent ideas that are unlikely to yield fruitful results.\n\nAside from \u201creal world constraints,\u201d the user\u2019s meaning in a given interaction may not always be clear \u2014 either because of the machine\u2019s knowledge limits or because of a lack of clarity in the user\u2019s statement:\n\nRather than going with a \u201cbest guess,\u201d the machine could offer clarifying questions and alternatives:\n\nThis conversational approach would therefore help to clarify the user\u2019s intent as well as build the machine\u2019s knowledge base.\n\nA conversational approach also presents a natural mechanism for preserving the user\u2019s iterative process in a manner that is far more accessible for review and reflection than an \u201cAction History.\u201d\n\nBy unrolling the interface into a linear, traversable \u201cnews-feed,\u201d the user is able to inspect each stage in his or her thinking and easily return to earlier iterations, branching off in a new direction while still preserving each other version of the design.\n\nOver the last few years, I\u2019ve been working to actualize some of these ideas in software.\n\nI\u2019ve created a combined programming language and design tool called Foil which aims to bring many of the concepts we\u2019ve discussed to life and is intended for users along the full spectrum of design experience, from novice to expert.\n\nFoil tailors itself to the designer\u2019s needs as it learns from their interaction and supports designers in growing and developing expertise.\n\nDepending on the user, Foil can be a consumer design tool, a professional design tool, and a platform for the creation of emergent interface elements and design widgets which users will ultimately be able to share with each other.\n\nI\u2019ll be releasing an alpha version of Foil very soon.\n\nThese ideas have been greatly shaped by my work at NYU\u2019s ITP, a graduate program investigating the intersection of the arts and technology.\n\nIn this interdisciplinary context, it is clear to me that we are at the beginning of an immense cultural convergence and that the tools and vocabularies of once disparate media are ever more relevant to and interoperable with one another.\n\nTo that end, Rune Madsen and I have started a research group focused on the intersection of machine learning and design.\n\nWe are also currently co-teaching a class called Rethinking Production Tools, which asks students to investigate and develop new paradigms in tool making.\n\nThe technical renaissance in machine learning over the last few years has led to incredible new possibilities. But the real work in actualizing these possibilities is only just beginning.\n\nThis work requires an interdisciplinary approach and I have been extremely fortunate to be able to collaborate with Rune, the faculty and the students at ITP in this exciting period of transformation.\n\nA lot of people seem to be worried that artificial intelligence will take our jobs and render us useless.\n\nI see a different possibility for the future, a more optimistic one, in which we are still relevant. In fact, in this future, we are all the more powerful, we are all the more human.\n\nIn this future, we are not competing with objects, we are using them to extend our reach as we have always done.\n\nBut to get there, we need to remind ourselves what tools are for.\n\nTools are not meant to make our lives easier. Not really.\n\nThey are meant to give us leverage so that we can push harder."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/ai-poetry-hits-the-road-eb685dfc1544?source=---------8",
        "title": "AI Poetry Hits the Road \u2013 Artists and Machine Intelligence \u2013",
        "text": "I\u2019ve just returned from Ross Goodwin\u2019s AI-assisted stab at the American literary road trip, a project called Wordcar that put AI on the highway to generate 200,000 words of machine poetry. It\u2019s a classic trope with a 21st century twist. But in our moment of tender and anxious global ecological crisis, the free-wheeling ride into the unknown mythologized by Jack Kerouac, Ken Kesey, and Hunter S. Thompson takes on a sinister shade. Those authors set out in search of freedom, masculinity, enlightenment, hedonism \u2014 20th century values currently under renovation. These days, hitting the road in a gas-guzzler in search of anything other than a job feels irresponsible or at least unnecessary.\n\nWe are where we are. Many aspects of life and the open road have been inexorably transformed by the cannibalistic junkspace [1] of techno-capitalism. The mutation currently on display comes from the revived field of Artificial Intelligence. Because of breakthroughs in neural-net architecture and GPU vector processing, what is called Deep Learning has taken center stage in the field of AI, which increasingly goes by the less-narratively-burdened handle Machine Learning. The through line from Kerouac to cutting-edge RNN-LSTMs (Recurrent Neural Net Long Short Term Memory) starts with an amphetamine Beat and dips into self-absorbed spiritual utopianism and Gonzo paranoia before it settles in the Bay Area, where dropouts, acid-heads and home-brew hackers laid compost for the home computing revolution [2] and by extension key components in the techno-capitalist Stack [3].\n\nIt\u2019s an essential part of our era\u2019s hypocritical hypocrisy [4] that we question the ethics of any given act of consumption vis-a-vis the ecological and extinction crises while still consuming. This impotent self-awareness coated my perception as we set out from Bushwick, Brooklyn in a rented Dodge minivan and Cadillac sedan. As we pulled up behind a vintage Ford, I said to Ross and his sister Beth,\u201dIt must have been nice to be a Boomer. Cars were weird. Gas was cheap. You didn\u2019t have to feel guilty.\u201d Our engine idled as I cast about for someone else to blame.\n\nWhat besides the knowingness of our hypocrisy distinguished this trip from the cluster of mid-century journeys historicized by white guys from the west coast? I was the resident person of color and our party had gender parity \u2014 incremental progress, perhaps. Yet all of us hailed from the coastal, generally liberal, urban centers where tech thrives, and the left coast maintained strong representation. Photographer Christiana Caro and I work for Google; Beth, Ross, and I all grew up in the SF Bay Area. Tech was at the center of the journey, as a synchronic key, as the \u201cliterary\u201d engine, and as Ross put it, a substitute for mind-altering substances.\n\nAs with many aestheticized adventures in our era of cultural recycling, it was through tech that we marked our contemporaneity. Ross works with generative systems that produce text, specifically AIs that write poetry. My team at Google (Artists + Machine Intelligence) is a band of passionate twenty-percenters that have aided Ross with technical advice, financial support, and professional development as part of our mission to support an emerging form of art made with AI.\n\nOur automotive AI assemblage was inspired by an absurdist art exercise: write with a car. When we\u2019ve talked about writing, Ross has mentioned David Foster Wallace, Jorge Luis Borges, Ursula K LeGuin. On this trip he cited On The Road, The Electric Kool-Aid Acid Test, and Fear and Loathing in Las Vegas as influences and Beth recounted his early enthusiasm for these books. I found it hard to imagine him synthesizing these influences in his previous career as a speechwriter for Barack Obama, Timothy Geithner, and John Kerry. Political speech is way too constrained for Ross. Our drive from NYC to NOLA was a better channel for his automated graphomania.\n\nThese literary precedents all couple the road with one or more psychoactive substances. We rambled through New Jersey, Delaware, Maryland, without a suitcase of meth or a punch bowl of LSD, but we did have a neural net and a surveillance camera, and the babbling of Wordcar\u2019s simulated brain was an uncanny approximation of the stimulated screeds of yesterday\u2019s eschatologists. I won\u2019t over-promise \u2014 it was more Dada than Brautigan, and that may be the state of the art, for now.\n\nWhile the initiating impulse came from the written word, it was through the image that the word became. The eye was an Axis M3007 surveillance camera mounted magnetically to the trunk of the Cadillac. This is the standard model for home or business surveillance, a favorite tool of casino pit bosses who use them to see in four directions at once. Its industrial design is neutral in the loaded way utilitarian objects express blankness: an off-white square frames a transparent bulb, which wraps a black robotic eye; the person on the other end of the signal is camouflaged by the normest of cores.\n\nRoss customized his M3007 to rotate and \u201clook around\u201d by feeding its orientation controls Perlin noise. Ross\u2019s script instructed the camera to capture an image every twenty seconds. This image was first textualized in a most literal way: as ASCII art rendering a grayscale image with characters. Then an image recognition net described the image in a sentence, which fed a free-associating, text-generating neural net (in mathematical terms, a ~36,000 dimensional model of the linguistic space of nearly 200 hand-picked books, prodded to produce a string of statistically likely characters following the initial description).\n\nWhat did the neural net see? What did it talk about? It talked about what it knew. It knew the time. It knew where it was (in the way any computer does, via GPS) and it knew what was around it. To avoid anthropomorphizing, I\u2019ll be specific: it knew locations and businesses (like the Biloxi Hard Rock Cafe) that were proximate because they were exposed by the Foursquare API, which is to say by the priorities of the techno-capitalist producer and subject. These locations were often gas stations and fast-food restaurants. There are many on the American road trip.\n\nRoss\u2019s sister Beth is a food writer and at times the conversation turned to food deserts and the business structures that keep them in place. Distribution networks owned by fast-food conglomerates have an edge on small businesses that can\u2019t afford to send fresh produce out to exurban or rural areas; the roadside stand has come a long way since its first documented incarnation in upstate NY. The output of this network is the de facto diet of developed-world poverty. Foursquare in the Lower East Side, the Mission, or Silver Lake paints a very different picture. But this patchwork of chain convenience stores and fast food franchises was what Ross\u2019s Wordcar showed us. To be fair, these weren\u2019t the only features of the landscape to surface. There were bridges, rivers, and parks. From the perspective of the AI at the heart of Wordcar, however, they formed a substrate seen incidentally through accrued layers of gas and synthetic food distribution.\n\nWhy did Ross choose to show us this slice of American life in semi-sensical LSTM poetry gathered via API and surveillance camera? We weren\u2019t just roaming the concrete corridor connecting Yankees and Southerners. We had an objective: a stop in Biloxi, Mississippi to meet Josh Sniffen of Not From Concentrate Systems, a brilliant fabricator of gaming PCs, who embedded GPUs in vintage 8x10 cameras for Ross\u2019s upcoming show at the Rubber Factory in Manhattan. We saw a Jeep he constructed \u201cfrom scratch\u201d, his YouTube broadcasting setup, gaming PCs he\u2019d built, Ross\u2019s 8x10 and 11x14 cameras (from 1890 and 1905 respectively.) The work he did was beautiful.\n\nJosh invited us into his home and grilled delicious sous-vide steaks for everyone. Where Ross\u2019s Bushwick living room drips with receipt-scrolls of AI poetry and runs a Google screensaver on Ubuntu Linux, Josh\u2019s home decor includes a posted list of of family rules and an informal garage shrine to the Virgin Mary. While our film crew captured footage of Josh\u2019s studio, I waited in a lawn chair. Josh\u2019s children rode tricycles. Humid air came off the bayou.\n\nRoss and Josh had never met in person but they got along, diving into the obsessive tech-speak that engineers and hardware hackers fling. They both love manufactured systems. They both have complex relationships with mainstream American culture and religion. They come from different ends of various axes: North/South, hardware/software, a lapsed Jew and a devout Catholic. On a global scale these differences are minimal. But in contemporary American political discourse they are often framed as insurmountable. The intensity of their shared interest brought Josh and Ross close enough to experience each other\u2019s difference. Their meeting wasn\u2019t a site of ideological conflict (like, say, Twitter). But it was clear that the cultural space between them wasn\u2019t simple, that traversing it would take time, and that it ultimately wasn\u2019t necessary in order to have a productive relationship around an art and fabrication project.\n\nWe ended the trip in New Orleans. By then we\u2019d been through ten mostly Southern states. As in many areas of the US, our route was dotted with industrial infrastructure unused and in decay. There were power plants, factories, railroads, mines. These were scenic and the filmmakers we traveled with turned their lenses on the ruins as backdrops. They hoped to highlight one of the most pressing concerns around AI: the changes that automation will bring to the economy and the predicted loss of jobs on a massive scale. Automation has already transformed mining and manufacturing. But AI that can predict, AI that can diagnose, AI that can write\u2026 these threaten blue and white collar jobs equally. As it often does, automation speculation led to discussion of Universal Basic Income, the idea that the state should provide for every citizen\u2019s basic needs. Under neoliberal (or neofascist) techno-capitalism this is unthinkable. But it wasn\u2019t so long ago that jobs were created by the Works Progress Administration, during another time of economic instability.\n\nIn fact, the morning after our first day on the road, we learned about a document created by WPA laborers (writers and historians, or what we might now call creatives and content producers). As we ate breakfast in a 10,000+ square foot mansion in Goldsboro, North Carolina, Laurie Sneed (the aunt of Ross\u2019s fiancee Lily) shared with us a collapsing edition of The American Guide Series a written history of places traversed and annexed by interstates. Think of it as an archaic, proto-GPS-indexed feed of quirky and boring stories about small towns dotting highways in the 1930s. It\u2019s the sort of entertainment that might strike us as quaint or musty. But in our ambient Anthropocene anxiety it\u2019s almost soothing to read this excerpt describing the roadside grave of a circus clown, paved over even 80 years ago:\n\n \n\n At 25m., embedded in the cement pavement of the highway, is a Tombstone \u00ae broken during the War between the States by the wheels of a gun carriage. Inscribed \u201cGone But Not Forgotten,\u201d it marks the grave of a circus clown who died near here in the 1840s.\n\n \n\nThe clown and the old book beg questions: Who will look back on the half-absurd techno-engagements of Ross Goodwin and his ilk in 10, 50 or 100 years? How will their basic needs be met? How will the mechanisms that meet them frame their understanding of the Wordcar project or any literary road trip? Are we crude psychonauts prefiguring mainstream mind-manufacture? Are we hypocritical hypocrites on a dirty freeway? Are we everyday artists like the people to come? How are we etching our names in the land?"
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/ami-residency-part-2-realtime-control-of-sequence-generation-with-recurrent-neural-network-88448dde3500?source=---------9",
        "title": "AMI Residency Part 2 : Realtime control of sequence generation with Recurrent Neural Network\u2026",
        "text": "Recurrent Neural Networks (RNNs) \u2014in particular, a recurrent architecture called Long Short-Term Memory (LSTM, initially proposed in 1997) \u2014 are very popular and successful in modelling and generating sequences. I\u2019m not going to talk in detail about how they work, as there are quite a few great tutorials on the subject, e.g.\n\nBut to give a very brief explanation, an RNN can learn to predict (amongst other things):\n\nWhich can be read as \u201cThe probability of x at time t+1, given the values of x at time 1, time 2, all the way up until time t\u201d (NB. the vertical bar means \u2018given\u2019 or \u2018conditioned on\u2019 or \u2018assuming we know this to be true\u2019 etc.). In other words, the RNN learns (and outputs) a probability distribution for the next item, for a given sequence of items. (This may make RNNs sound a bit like Markov Chains or Hidden Markov Models etc. But they are quite different under the hood and can model much more complex sequences. Some nice experiments here. In fact, RNNs are theoretically Turing complete and hence are now being used in Neural Turing Machines or Differentiable Neural Computers to learn algorithms from data, not just functions).\n\nSo RNNs have been very successful in generating sequences in domains such as music (Eck2002, Sturm2015), images (Gregor2015), handwriting (Graves2013), speech (Wu2016), choreography (Friis2016) etc. and became very popular in 2015 with Karpathy\u2019s blog post and easy to use open-source implementation char-rnn, and subsequent implementations like torch-rnn.\n\nOverall, the generative process can be summarised as:\n\nCurrently, a lot of the applications which work with such systems are not real-time, let alone interactive, let alone expressively interactive. I.e. you tell the system to generate a sequence, e.g. some music or text, and after waiting some time (ranging from a few seconds to a few minutes) you get back a sequence, e.g. a few seconds or minutes of music, or a few words or sentences or paragraphs of text.\n\nOne thing that I\u2019m interested in, is how can I, a human user, interact with the generative process, and steer or control it while it\u2019s generating. I want to be in control, and respond to the generative process in real-time. In this particular case, I want to be able to control the style of the output as it\u2019s being generated. I want to be able to play the system like a piano, or at least, \u2018conduct\u2019 it.\n\nThere are many ways one could approach this problem. I wrote about one possible method using an agent (e.g. I tried an agent driven by MCTS, I\u2019m also looking at Reinforcement Learning, which Karpathy also has a great post on :). Another method is to use something colloquially referred to as \u2018priming\u2019 \u2014 which I have also tried and will write about in another post.\n\nIn this particular project I tried using an ensemble of models trained on different data-sets.\n\nAn ensemble basically means a bunch of models. Usually one would train a bunch of different models, maybe different architectures for each model, maybe even different learning algorithms altogether. The idea being that when the time comes to make a prediction, all of the models are fed the new input, and they are all asked to make a prediction. Then through a kind of \u2018voting\u2019 process, the outcome of all of the models are averaged. E.g. if we were working on an image recognition system, and we trained 10 models, each using different architectures and methods, then if 8 of those models predicted that a particular image was of a cat, and the other two models predicted that it was of a dog, then we can be a bit more confident that the image is indeed that of a cat.\n\nIn this case, I trained a bunch of models, each on an entirely different data-set, and looked into mixing their predictions. This also ties in nicely with calculating a joint distribution given a marginal distribution of styles.\n\nOne of my end goals is to apply this interactive control to the audio-visual domain (images, music, sound etc.), but I decided to start with character based text (i.e. the models produce text character-by-character. Karpathy\u2019s post explains this really well). I chose this for a number of reasons:\n\nI trained one LSTM RNN model per data-set. i.e. For every \u2018style\u2019 there is a corresponding data-set (e.g. Trump, Love Song lyrics, Jane Austen etc.), and a corresponding model trained on that data-set.\n\nI used the relatively easy and high level machine learning framework Keras. The trainer is loosely based on this example, though I changed the architecture a bit, details in the paper.\n\nConceptually, what the system does is very simple:\n\nEach model learns and then outputs:\n\nThis is the output of the ith model at time t and is the probability distribution for the next character, conditioned on the previous characters and the parameters of that model. That last statement \u2018[conditioned] on the parameters of the model\u2019 might seem obvious and implicitly suggested, but when we have multiple models, each trained on a different data-set, thinking of the probability like this enables us to think of it as a conditional probability, conditioned on a particular style (i.e. data-set), so then we can calculate a joint probability distribution via\n\n(Here the denominator is just a normalising factor). The system then samples a character from this distribution to predict the next character. This new character is printed on screen, and fed back into each of the models so they\u2019re all in sync, and at the next frame the whole process is repeated. (Actually I don\u2019t feed the new character into all 24 models, only the ones which have a mixture weight greater than 5%. This is just an optimisation to not waste compute power on unused models. When a model becomes active for the first time, I do feed it the full history of characters \u2014 up to a max history of 80 characters \u2014 to make sure that it is making predictions on the correct text)."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/tensorflow-and-art-d5bc2cc7622a",
        "title": "TensorFlow in the (Art) Classroom \u2013 Artists and Machine Intelligence \u2013",
        "text": "TensorFlow is a great way for students to get hands on with machine learning, but provisioning, managing and tracking instances can be cumbersome and classrooms sharing an instance are quickly limited by CPU and memory constraints.\n\nOur Google Cloud Platform friends recently launched a tool that should prove useful to educators teaching machine learning for creative applications: TensorFlow instance management with JupyterHub.\n\nGoogle Container Engine allows admins to quickly provision Docker containers running unique TensorFlow environments for each student. With the included example, students can generate art with the DeepDream algorithm.\n\nIf you\u2019re interested in teaching creative ML, head over to the solution guide for step by step instructions. Faculty and students can apply for a Google Cloud Platform education grant \u2014 so don\u2019t miss out on the opportunity for free computation."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/mami-lectures-part-4-d3af3ceaaa2c",
        "title": "MAMI Lectures, Part 4 \u2013 Artists and Machine Intelligence \u2013",
        "text": "This is it. The last post in this series. Stay tuned for\u2026 next year?\n\nBlaise Aguera y Arcas gave us a whirlwind tour of the history of brain mapping (including beautiful new visualizations of neurons) and the magic formula behind neural-net-based image generation.\n\nFernanda Viegas showed us how visualizations of complex systems can help us understand AI and weather, and otherwise work with non-human entities.\n\nValorie Salimpoor presented her research on why music gives us chills, including neural images of anticipation and reward, and specific musical techniques (composers: take note!)\n\nMore music! Elisabeth Margulis discussed the power of repetition in music, and the heightened emotional response that comes with familiarity.\n\nWe finished off the day with demos of machine generated art.\n\nSheldon Brown took us deep into the uncanny valley of symbiotic human-machine imagination, where familiar landscapes behave in strange ways.\n\nRoss Goodwin showed us how he co-writes with machines, generating (and reading) an only-recently-possible poetry.\n\nAnd Ian Cheng walked us through simulated worlds he creates with simple rules, inspired by, among other things, Jaynesian theory of mind.\n\nWe capped the day with stimulating panel discussion and insights from Sageev Oore, Caroline Pantofaru, Timothy Morton, and Martin Wattenberg, hosted by Greg Corrado.\n\nThanks for staying with us. This is just the beginning. For more, follow us on Medium and Twitter."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/ami-residency-part-1-exploring-word-space-andprojecting-meaning-onto-noise-98af7252f749",
        "title": "AMI Residency Part 1 : Exploring (word) space, projecting meaning onto noise, learnt vs human bias.",
        "text": "It first picks 2 to 4 completely random words from a vocabulary of 100K words (actually it\u2019s 53K words, reasons explained in the src . Note that it\u2019s estimated that educated native English speakers have a vocabulary of around 20K-30K words). The bot plots these words in a high dimensional latent space (using the famous word2vec model trained by Mikolov et al on 100 billion words of Google news ). It then performs random arithmetic operations (addition or subtraction) on these vectors. This results in a new location in the high dimensional space. The bot then returns the closest words.\n\nI.e. \u2018human\u2019 - \u2018god\u2019 = \u2018animal\u2019 means that the bot has randomly picked the words \u2018human\u2019 and \u2018god\u2019, and randomly decided to perform a subtraction. It subtracts the vector for \u2018god\u2019 from the vector for \u2018human\u2019, and finds and tweets the closest word to that point, in this case \u2018animal\u2019 (actually it tweets the top five closest words, here I just hand-picked some of my favourite results).\n\nAbove you can see some fully genuine, untampered results. But I should point out that there are hundreds (if not thousands?) of results, and I cherry-picked a few of my favourites. (I haven\u2019t actually thoroughly looked through them all, there might be much more interesting ones).\n\nInitially I was curating and trying to impose rules on what words the bot should pick from, so that the results would be more \u2018sensible\u2019 and interesting. But in doing so, I was actually limiting the ability of the bot to find more \u2018creative\u2019 (and arguably more interesting, or unexpected) results. So I removed any constraints that I had imposed, and let the bot explore the space a lot more freely. It now produces things which are a bit more nonsensical, and sometimes a lot harder to make sense of.\n\nAnd in fact this is what this project ended up being about.\n\nIt\u2019s not about what the model tells us, but what we look for and see in the outcome.\n\nTons of examples can be found on twitter. Below are a few I selected. Some of the first few examples are probably quite easy to interpret.\n\nThis is an interesting one. It could be interpreted as: \u201cif we don\u2019t have / believe in god, we will descend to the level of primitive animals\u201d or alternatively: \u201cwhat sets humans apart from other animals, is that we were created in the image of god\u201d. Or maybe: \u201chumans are just animals, that have invented religions and beliefs in god\u201d etc.\n\nThere are probably many other ways of interpreting this, and I\u2019d be curious to hear some other ideas. But the truth is, I don\u2019t think it means any of those things. Because there is no one behind this, saying it, to give it any meaning. It\u2019s just noise, shaped by a filter, and then we project whatever we want onto it. It\u2019s just a starting point for us to shape into what we desire, consciously or unconsciously.\n\nSome might disagree and say that the model has learnt from the massive corpus of text that it has trained on, and that this artifact produced by the model carries the meanings embedded in the corpus. This is of course true to some degree, and can be verified with the examples given earlier, such as king-man+woman=queen, or walking-walked+swam=swimming. Surely it\u2019s not a coincidence that the model is returning such meaningful results in those cases?\n\nIt does seem like the model has learnt something. But when we start to push the boundaries of the model, we have to resist the temptation of jumping to conclusions as to what the model has learnt vs what is just \u2018semi-random\u2019 results, with our brain completing the rest of the picture. I\u2019m not suggesting that there is a cut-off point as to when the model stops making sense and starts generating random results. It\u2019s more of a spectrum. The more we sway away from what the model is \u2018comfortable\u2019 with (i.e. has seen in abundance during training, has learnt and is able to generalise), the more noise is injected into the output, and potentially the more fertile the output for our biased interpretations.\n\nI will expand on this in more detail a bit later. But first some more examples.\n\nI particularly like this one. I interpret it as \u201cwithout the need for a god, nature is just the laws of physics\u201d. But it\u2019s also very plausible that the word \u2018dynamics\u2019 just happens to be close to \u2018nature\u2019 and \u2018god\u2019, along with a bunch of other words that I don\u2019t find that interesting or relevant. But you might. (NB I find \u2018nature\u2019 - \u2018god\u2019 = \u2018engaging\u2019 also quite interesting).\n\nI couldn\u2019t believe this one when I saw it. It almost needs no explanation. \u201cbots on twitter become memes\u201d. Too good to be true.\n\nThis is a powerful one. I interpret it as \u201cSex without love is just intercourse\u201d, or \u201cprostitution is sex without love\u201d, or \u201crape involves sex and hate (as the opposite of love)\u201d. These results are very interesting. But again, it should not be assumed that the model is learning this particular interpretation from the training data. In most likeliness, all of these words are somewhere in the vicinity of \u2018sex\u2019 and/or \u2018love\u2019, since they are all related words. And yes perhaps these words do lie in a particular direction of \u2018love\u2019 or \u2018sex\u2019. But there is a difference between a bunch of words being laid out in space, and the sentence \u201csex without love is intercourse or prostitution\u2026\u201d. The latter is my interpretation of the spatial layout.\n\nI have to push my creativity to be able to make sense of this one. I ask myself \u201cIf we think of philosophy as the act of thinking, and being logical or critical, then perhaps this sentence says that police and governments are authorities that don\u2019t think, and are not logical?\u201d. Or in other words \u201cwhat kinds of authorities do not think, and are illogical? Police and governments\u201d.\n\nThis one pushes the limits of my creativity even further. But I can still find meaning if I try hard. E.g. Let\u2019s assume that a beard traditionally and stereotypically signifies wisdom. Imagine a beard, that is not justified \u2014 i.e. it pretends to signify wisdom, but actually it doesn\u2019t. In fact, this particular beard also replaces space (which I liberally assume to represent the \u2018universe\u2019, \u2018knowledge\u2019, \u2018science\u2019) with doctrine. Where might we find such a beard, pretending to be wise, but replacing science with doctrine? In theology of course, e.g. a preacher.\n\nOf course this is me trying quite hard to fit a square peg into a round hole, trying to make sense of this \u2018semi-random\u2019 sentence which the model has spat out. I wouldn\u2019t be surprised if somebody was able to interpret this sentence to mean the exact opposite to how I chose to interpret it."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/mami-lectures-part-3-be4ff649becf",
        "title": "MAMI Lectures, Part 3 \u2013 Artists and Machine Intelligence \u2013",
        "text": "This is the third of several posts in which we\u2019ll share lectures from the Music, Art and Machine Intelligence conference that took place in San Francisco, CA on June 1, 2016. Previously: Part 1. Part 2.\n\nDid you think that was it? That we couldn\u2019t get more creative with MI?\n\nBelow, you\u2019ll see a MI-augmented drumstick prosthetic by Gil Weinberg of Georgia Tech, robotic brushes by Columbia\u2019s Hod Lipson, Memo Akten\u2019s MI-based lighting control and gestural interface for music, and Magenta\u2019s MI-generated compositions on a simulated Moog synth (presented at MAMI by Adam Roberts, but depicted here in its Moogfest incarnation)."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/a-journey-through-multiple-dimensions-and-transformations-in-space-the-final-frontier-d8435d81ca51",
        "title": "A journey through multiple dimensions and transformations in SPACE",
        "text": "These are eigenfaces, a very simple demonstration of this. Going into detail as to how they\u2019re generated is beyond the scope of my talk right now, but I\u2019ll spend just a couple of minutes to give a very rough overview for the technically curious.\n\nWhat does that mean?\n\n(This bit can be skipped if a. you\u2019re already comfortable with PCA or b. you don\u2019t really care. If this section isn\u2019t clear, don\u2019t worry about it as afterwards I\u2019ll summarise the conceptual significance, which is the bit that really matters).\n\nImagine we have a bunch of 3D data, and we plot them in 3D space, we get something that resembles a point cloud.\n\nNow this point cloud might be perfectly spherical, but it\u2019s more likely to be kind of elongated and blobby like in this image. The point cloud in this image is quite elliptical and is oriented in a particular direction. The directions in which the data point cloud is elongated in are called the principle components (or eigenvectors). And Principle Component Analysis (PCA) is a method of finding these directions in which the data is most \u2018elongated\u2019. Then we can define these directions as new axes, and project our data into that new axis system. I.e. transform it.\n\nBut here\u2019s an important detail: PCA finds the directions of elongations (eigenvectors), and how big each \u2018elongation\u2019 is (eigenvalue) along that direction. We can then choose to omit any directions (eigenvectors) where the elongation (eigenvalue) isn\u2019t that significant i.e. If it\u2019s quite flat in a particular directions.\n\nE.g. Imagine we plot a bunch of data in 3D space, and it turns out to be (almost) flat like a piece of cardboard, but tilted at an angle. If we can calculate that angle, we can transform our coordinate system, and reduce it to 2D. That\u2019s exactly what we can do with PCA: reduce dimensions by transforming the data to a new axis system, one which potentially represents the data more optimally.\n\nAnd as always, this works in any number of dimensions. If we have 100D data in 100D space, it might also have elongations. In fact, because it\u2019s in high dimensions, it will probably have many elongations in many different directions and dimensions. PCA will find all of these elongations. In fact PCA will return the same number of elongations as there are original dimensions. I.e. For a 100D space, PCA will return a new set of 100 directions (axes). But these 100 axes will be rotated to fit our data more optimally. Most importantly, because we know how much elongation there is on each direction (the eigenvalue corresponding to the eigenvector) we can sort the axes by elongation amount. The first axis will have the most elongation (pointing in the direction of most variance), second axis will have the second most elongation (pointing in the direction of second most variance), etc. and the last (100th) axis will have the least amount of elongation. This means that we can choose an arbitrary cutoff point (for amount of variance), and just ignore the axes (dimensions) beyond that cut off point. The same way that we can transform 3D data that is \u2018flat\u2019 into 2D (by finding the most \u2018important\u2019 set of 2D axes), we can transform 100D data into say 20D data \u2014 by finding the most \u2018important\u2019 set of 20D axes.\n\nIf this wasn\u2019t very clear, it doesn\u2019t matter. Understanding what exactly PCA does isn\u2019t the purpose of my talk. This bit was only for those who were interested and might have already seen this before. The important thing is to understand the implications of this which I will explain next.\n\nLet\u2019s take our dataset of face images (which we assume to be 32 x 32 pixels, so that it ties in with our previous discussion). Remember that every single 32 x 32 BW image is a single point in 1024D pixel space. If we plot all of our face images, we get a point cloud in 1024D. We can run PCA on this 1024D dataset and choose an arbitary number of dimensions (i.e. axes) to reduce it to.\n\nE.g. If we were to choose the top 24 dimensions, we might get something like this\n\nEach one of these \u2018face\u2019 images, is an eigenvector of this dataset, i.e. the \u2018directions\u2019 in which our dataset point cloud is most elongated in 1024D. These are the new axes which represent our data set more optimally, in a more compact manner.\n\nWhat does it even mean for \u2018an image to be an axis\u2019? Well, remember that in our 1024D space each point is an image. So each of these images here, is also a point in 1024D space. It\u2019s a vector. And eigenface image 1 is our new axis 1, eigenface image 2 is our new axis 2, eigenface image 3 is our new axis 3\u2026 eigenface image 24 is our new axis 24 etc.\n\nAnd this is conceptually really significant. Because first we discussed a 1024D pixel space. In that space, each axis (i.e. feature) corresponds to a pixel in a 32 x 32 grid \u2014 i.e. the features of the space are pixels.\n\nNow (after PCA / Eigenfaces) we have a new coordinate system (i.e. new axes, which are somehow rotated in space) to fit our particular dataset better. These new axes constitute a 24D latent space \u2014 I call it latent space because it\u2019s features (i.e. axes) are not directly observable. And these latent features are how much an input image resembles the eigenfaces. I.e. these are what the axes of this new latent space represent.\n\nI\u2019ll give an example to try and make this a bit clearer.\n\nThis image of Hedy Lamarr has a pixel representation, a 1024D vector of pixel values. How do we transform it from 1024D pixel space to 24D latent space? How do we find its representation in this latent space? i.e. the 24D vector of latent features? How do we encode it?\n\nWith this particular latent space (i.e. the one we constructed via eigenfaces and PCA), it\u2019s very simple.\n\nWe take the image (cropped and resized to 32 x 32, so it\u2019s a 1024D vector) and dot product it with the first eigenface (which is also a 1024D vector), that will give us a number, how much the image \u2018resembles\u2019 the first eigenface. That\u2019s the value of our first latent feature. i.e. if we were to plot this 24D representation as a point in 24D latent space, that\u2019s the distance we would go along the first of the 24 axes. We then dot product the image with the second eigenface, that number will give us the second latent feature, i.e. the distance to go along the second axes. Etc. All the way to the 24th eigenface and the last latent feature (i.e. axis).\n\nIf you\u2019re not familiar with dot products etc and this bit wasn\u2019t clear, it doesn\u2019t matter. The most crucial thing here is:\n\nIt might turn out that this image of Lamarr is 24% 1st eigenface, 12% 2nd eigenface, -31% 3rd eigenface, \u2026, 17% 24th eigenface etc. Or in a more compact syntax: [0.24, 0.12, -0.31, \u2026 0.17]. That\u2019s only 24 numbers! We would call each of these 24 numbers, the latent features of this image (in this particular latent space), and the vector (i.e. list) of 24 numbers is a representation of this image in (this particular) latent space.\n\nIf we have a representation of an image in this 24D latent space, i.e. a vector of 24 latent features, how can we reconstruct the original image? I.e. transform from 24D latent space back to 1024D pixel space? I.e. decode it?\n\nRemember that the latent features in this space are simply how much an image resembles each eigenface. So we simply multiply each of the 24x eigenfaces with the value of the corresponding latent feature, and add them up. I.e. for each pixel, we do:\n\nThat\u2019s it. The resulting 1024D vector is the pixel representation.\n\nThis is a huge compression of information.\n\nIf I want to send you a picture of a face, I don\u2019t need to send you all of the pixels of the image, i.e. a pixel representation, i.e. a vector of 1024 pixel features. I can just transform my image from 1024D pixel space into 24D latent space. I encode it. And then I can send you just the 24 numbers, the latent representation, a vector of 24 latent features. Of course you need a way of decoding those 24 numbers, transforming from latent space back to pixel space. If you already have these eigenfaces handy, then you can easily transform back to pixel space as I described before.\n\nBut if you don\u2019t have the eigenfaces, then I\u2019d need to send them to you first, and that would be very inefficient for just one picture.\n\nBut if you don\u2019t have the eigenfaces, and I want to send you a million 32 x 32 face images, sending pixel representations for all images would take up 1GB (1,000,000 images * 1,024 pixels per image, assuming 1 byte per pixel). Alternatively I could send you the pixel representations of the eigenfaces first which would be 24KB (24 images * 1,024 pixels per image). Then I could send the latent representations for each of the million faces which would be roughly 24MB (1,000,000 images * 24 latent features per image, assuming 1 byte per latent feature). A massive compression.\n\nThere is a catch associated with this. This is a lossy compression. Very lossy. If we take an image (e.g. Hedy Lamarr) and encode it, i.e. transform from 1024D pixel space to 24D latent space, we will end up with a 24D vector of latent features, a representation in 24D latent space. If we decode that, i.e. transform it from latent space back into 1024D pixel space, we will end up with an image again. We could call this a reconstructed image. But the reconstructed image will not necessarily be identical to the original input image (e.g. Lamarr). The \u2018difference\u2019 between the original input image and the reconstructed image is the error (of this encoding-decoding). There are many different ways of measuring this \u2018difference\u2019, and it depends on the domain. For an image like this, we could simply take the difference between all of the pixels and add them up (L1 Norm) or we could measure the euclidean distance in 1024D space (L2 Norm) etc. (For a more complicated, probabilistic model it\u2019s more common to look at the \u2018difference\u2019 between probability distributions, e.g. using something like KL divergence).\n\nThere are two main reasons for this error:\n\nLet\u2019s also remember that this 24D representation, the vector of 24 latent features, are coordinates in a 24D space. So each 32 x 32 pixel face image can be thought of as a point in 24D latent space (in addition to being a point in 1024D pixel space). Now what happens when we perform geometric operations in this 24D space? If we average two points (i.e. two latent representations of face images)?\n\nThe method I just described was using PCA and eigenfaces. PCA is a method dating back to 1901, and was applied to faces in 1987. It\u2019s quite old, really not state of the art at all. Also PCA is a linear dimensionality reduction technique. I.e. the new (latent) features are linear combinations of the original features (e.g. pixels).\n\nwhere all Kx_y are constants. PCA\u2019s job is to find those constants.\n\nSo PCA won\u2019t find complicated, intricate manifolds (i.e. crumpled pieces of paper, or intricate mountain ridges), or even slightly curved manifolds (like the surface of bowl). It will only find completely \u2018flat\u2019 manifolds (like a flat piece of cardboard). So even though finding midpoints of multiple image representations in this new latent space will be more interesting than doing it in the 1024D space, the results will still be linear combinations and not terribly exciting. However\u2026\n\nI only showed and spent so much time on PCA / Eigenfaces because they\u2019re relatively easier to visualise and understand what\u2019s going on under the hood (compared to the \u2018black-box\u2019 of neural networks).\n\nThere are many other, totally different methods which essentially do what we want here, which is to\u2026\n\nIn the next sections I\u201dll talk about a few other methods which are considerably more complicated under the hood, so I won\u2019t go into so much detail on how they work. I\u2019ll focus mainly on the end result and how they work on a conceptual level.\n\nBut first I want to underline a few things:"
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/mami-lectures-part-2-5379d3be26bc",
        "title": "MAMI Lectures, Part 2 \u2013 Artists and Machine Intelligence \u2013",
        "text": "In our last installment, we explored two conjoined activities: generating with neural nets and investigating their inner states. The lectures featured below introduce further modes of understanding machine learning.\n\nRebecca Fiebrink of Goldsmith\u2019s treated us to some Mego-worthy industrial noise, controlled by her highly playable Wekinator software.\n\nHannah Davis walked us through her TransProse project, which translates literature into music through MI-based emotion mapping.\n\nMichael Tyka presented an outline of art history, observing the accelerated nature of kitsch and the absorption of novelty by art viewers.\n\nChris Olah demystified neural nets so we could understand them as simple high-dimensional manipulations of geometry.\n\nArtist Tivon Rice capped the session off with drone photogrammetry of urban architecture, coupled with neural-storyteller text generated from these images and trained on corpora of city planning submissions and public responses."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/kaoru-okumura-butoh-x-deepdream-3e378b7e618d",
        "title": "Kaoru Okumura: Butoh x DeepDream \u2013 Artists and Machine Intelligence \u2013",
        "text": "Artists & Machine Intelligence is proud to present this teaser video featuring our upcoming collaboration with Butoh dancer Kaoru Okumura.\n\nOkumura\u2019s piece with AMI will premiere at the Seattle Butoh Festival on July 16th. In the video and in her festival performance, Okumura dances through morphing DeepDream projections by Michael Tyka. Her piece explores the contrast between Butoh\u2019s inherent downward motion and DeepDream\u2019s \u201cobsessive expansion\u201d. The two systems diverge but find harmony in the alien.\n\nSeattle Butoh Festival details and tickets are available here."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/mami-lectures-part-1-2a685ef07e0b",
        "title": "MAMI Lectures, Part 1 \u2013 Artists and Machine Intelligence \u2013",
        "text": "This is the first of several posts in which we\u2019ll share lectures from the Music, Art and Machine Intelligence conference that took place in San Francisco, CA on June 1, 2016.\n\nThe discovery of the generative capability of neural nets, at least in the case of DeepDream, was largely driven by the desire to know just what\u2019s going on in there. Start with tensors or with transmitters \u2014 when you\u2019re probing a deeply-networked structure the boundaries between exploration and creation are blurry, and very generative. The first six lectures of the MAMI conference explore the creative side of investigation and the investigative side of creation.\n\nOther new techniques are showcased by Aaron Courville of University of Montreal. Specifically, Variational Auto-Encoders (VAE) and Generative Adversarial Networks (GAN).\n\nNYU\u2019s Emily Denton explains adversarial techniques for generating, among other things, bedrooms.\n\nMario Klingemann, an independent artist currently in residence with Google\u2019s Cultural Institute, shows his surreal generated collages.\n\nAnd Jason Yosinki takes an alternative approach to surrealism, showing us how computers see lions in fields of static where none exist, and generally prodding around in unseen regions of image recognition systems."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/submit-an-ami-article-598634cbd631",
        "title": "Submit an AMI article \u2013 Artists and Machine Intelligence \u2013",
        "text": "Google\u2019s Artists and Machine Intelligence (AMI) program brings together artists and engineers to realize projects using Machine Intelligence.\n\nJoin the discussion \u2014 submit an original written piece that helps illustrate the ways Machine Learning is inspiring a completely new creative process.\n\nTo submit, draft your work in Medium and share an unpublished link with artwithmi@google.com.\n\nThanks for furthering the exploration."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/music-art-machine-intelligence-2016-conference-proceedings-ea376a4e2576",
        "title": "Music, Art & Machine Intelligence 2016 Conference Proceedings",
        "text": "While \u201cislands\u201d of research do exist, interdisciplinary efforts are bolstering an image of creative research and researched creativity. Generative art (whether genetic, rule-based, or hallucinated) fills the humid equatorial regions. The great northern tundra is home to ever more efficient and ingenious MI techniques, while nomadic neuroscientists and non-denominational wanderers traverse the plains between.\n\nOn June 1st, Google\u2019s AMI and Magenta groups jointly hosted a conference on MI/ML and creative practice, called Music, Art & Machine Intelligence. The roughly 80 attendees and 29 presenters represented a broad range of perspectives on music, art, and machine intelligence, as well as neuroscience and philosophy. Each presentation lasted only ten minutes, but the variety of disciplines, the art and music demos, and regular breaks in the San Francisco sun and air kept brains and bodies stimulated.\n\nLook in the knapsack of any member of these tribes and you\u2019ll see a squirming plethora of generated entities. Google\u2019s Rahul Sukthankar alone was found holding three varieties. He presented neural-net-generated fonts, mathematical proofs, and less fantastically, image compression. Mario Klingemann, an independent artist currently in residence with Google\u2019s Cultural Institute, showed surreal generated collages. His rule-based systems jabber endlessly, while he plucks the tastiest fruits from his strange garden. Jason Yosinski took an alternative approach to surrealism, showing us how computers see lions in fields of static where none exist, and generally prodding around in unseen regions of image recognition systems.\n\nNYU\u2019s Emily Denton presented adversarial techniques for generating, among other things, bedrooms. Other new techniques were showcased: Aaron Courville of University of Montreal discussed Variational Auto-Encoders (VAE) and Generative Adversarial Networks (GAN), and Google\u2019s Samy Bengio presented a new approach to training neural networks called Reinforced Maximum Likelihood.\n\nA cluster of talks on new generative tools took things to the next level of interactivity. Rebecca Fiebrink of Goldsmiths treated us to some Mego-worthy industrial noise, controlled by her highly playable Wekinator software, a combination of Leap motion tracking and on-the-fly perceptron manipulation that enables access to the subtleties of embodied knowledge acoustic musicians take for granted. Hannah Davis of NYU crossed the already-crossed streams with her TransProse project, which translates literature into music through emotion mapping. By reading the emotional temperature of a text with NLP, Davis created timelines that could be composed to algorithmically. Her early experiments sounded very mid-20th c. and atonal, while later explorations became impressionistic.\n\nMike Tyka and Chris Olah were on-hand to situate us art-historically and geometrically, respectively. Tyka observed the accelerated nature of kitsch and the absorption of novelty by art viewers. Olah demystified neural nets so we could understand them as simple high-dimensional manipulations of geometry and not slime-breathing multi-eyed dog-monsters.\n\nArtist Tivon Rice capped the session off with exquisite corpses of another nature, namely drone photogrammetry of buildings under construction in Seattle, a hotbed of urban change like many cities in the US right now. His project with AMI pairs neural-storyteller text generated from these images and trained on corpora of city planning submissions and public responses. His show is currently on view at Threshold Gallery at Mithun Architecture in Seattle.\n\nIn the segment titled Creating with Machines, Gil Weinberg of Georgia Tech showed us his regime for training robots to listen to and generate music. Musical augmentation is just around the corner; Weinberg is working on a drum-centric prosthetic arm that can follow (and fill) along using MI. We saw examples ranging from a simple swing ride pattern to black metal-ready 20Hz snare blasts, bringing restoration of human ability into super-human territory.\n\nColumbia\u2019s Hod Lipson showed us physical works painted by a robotic brush. The artist in question reproduces existing works and generates new ones using MI. The (human) artist Ian Cheng brought a whole host of entities into the mix, some of whom were controlled by voices from the bicameral beyond. His video pieces are real-time simulations of small scales societies, or, in one case, a sponge-y animal-vegetable hybrid. These simulations produce unexpected and unpredictable behavior, in beautifully emergent and entrancing ways.\n\nResident AMI artist Memo Akten presented MI-based lighting control that responds to the motions of dancers. He also showed a gestural interface for music, intended to provide the feeling of performing classical piano without stressful years of training at the hands of a brutal Russian master. Expect to see more from Memo as he completes his residency with the AMI team.\n\nIf you missed it at Moogfest, you could have caught it at MAMI \u2014 Magenta\u2019s Adam Roberts showed off the group\u2019s latest music sequence generation. Roberts played a musical phrase on an MI-enabled illustration of a MiniMoog synth, which improvised on the theme. It may not have been \u2018Trane but it was well-trained and reliably melodic.\n\nThen the aforementioned neuroscience nomads showed up to blow our minds. Google\u2019s Blaise Aguera y Arcas brought it all back to the brain, with Ramon y Cajal\u2019s early neural images in their eerily circuitous formations, Alan Turing\u2019s MI-prescience and the interrelated nature of perception and creation. Valorie Salimpoor showed us how music gives us chills, and how composers manipulate our cognitive rewards system and physiological response to recognizing patterns. Elizabeth Margulis showed us the musical and emotional power of repetition, with a hypnotic example. Musical tension and release really mess with dopamine. How long can you take the krautrock?\n\nAMI artists Sheldon Brown and Ross Goodwin landed us squarely in the uncanny, with Brown\u2019s Shepard-tone enhanced, spiraling spatial installations wreaking havoc on our perceptual grounding, and Goodwin\u2019s LSTM-generated poetry and interactive-MI-writing systems destabilizing our linguistic grasp on reality.\n\nThe day ended on a collective philosophical note in a panel discussion with Sageev Oore (St. Mary\u2019s Nova Scotia), Timothy Morton (Rice University), and Google\u2019s Carolina Pantofaru, Martin Wattenberg, Blaise Aguera y Arcas and Douglas Eck.\n\nThe field of MI-enhanced creativity is wild, and in many ways, unexplored. It was clear at the MAMI conference that a multidisciplinary approach is not only fruitful and necessary but also entertaining and thought-provoking. Perception and creation are indeed two ends of one kaleidoscope, and the multi-sensory ways of knowing that art and music provide are essential in deepening our investigations of creativity, technology, and humanity."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/alt-ai-4f6e706c7aab",
        "title": "Alt-AI \u2013 Artists and Machine Intelligence \u2013",
        "text": "I recently attended #alt-ai, a mini conference on Art and MI organized by Gene Kogan, Lauren Gardner, and folks at the School for Poetic Computation (sfpc) in New York City. The event took place in a building that was previously occupied by Bell Labs and was the location of 9 evenings almost 50 years ago. The building later became the Westbeth Artist community (home to many influential and successful artists over the years) and is now home to sfpc.\n\nAll the #alt-ai talks can be watched in full here: http://livestream.com/internetsociety/alt-ai/\n\nThe first day started with a gallery opening (about 30 pieces, many shown on Openframe.io) and 4 speakers.\n\nGene Kogan gave an intro and a bird\u2019s eye view of the sudden explosion of interest in this field over the last year. He gave examples of DeepDream, Style transfer, DCGAN and also spoke about his art piece, the Cubist Mirror (a screen + camera + realtime style transfer). Gene is using a recently published algorithm that accomplishes style transfer using a forward-only network and is about 1000x faster than other methods, making realtime possible.\n\nGolan Levin premiered his new project with Kyle McDonald, Aman Tiwari and others: http://www.terrapattern.com/ After training a CNN on satellite imagery, they select an arbitrary map tile and search Google maps for similar tiles using the embedding vector (so it goes beyond the training classes). This works really well and allows you to find all sorts of interesting geographical features: E.g., \u201cShow me all the locations in NYC of: tennis courts, swimming pools, ship docks, gas tanks etc.\u201d\n\nCassie Tarakajian presented some VR visualizations of a CNN, based on something like this: http://scs.ryerson.ca/~aharley/vis/conv/, but in a VR environment. Given that the tensors representing each layer in a convnet are usually 3 dimensional (x,y,filter), a 3D visualization makes a lot of sense. At the front of the VR helmet is a LeapMotion sensor so you can use your hands inside of the VR environment.\n\nShe also had a live demo in the gallery:\n\nHannah Davis presented a very cool music generator based on emotion extracted from text. It looks for keywords in a book and maps them onto about 10d vector space. Notes are then picked in reference to the emotional arc of the prose, with rules such as \u201cmore dissonance on high emotion, more major key on happy, etc.\u201d More info here: http://www.musicfromtext.com/about.html\n\nThe evening ended with a live performance by Jason Levine using extempore, which allows you to write continuously executed code in realtime. Jason used a library of audio samples, organized using tSNE (based on an idea by Kyle McDonald) in a 2D space. He then used the live coding environment to create periodic paths through that space to pick which samples to play, generating rhythms and music.\n\nRebecca Fiebrink spoke about her latest project which allows interactive machine learning to map arbitrary controllers to arbitrary behavior (e.g., game controllers to a synthesizer). This system makes it really easy for artists to create new instruments without needing to learn to code. You just plug in the input and output devices, choose a behavior (say a particular synthesizer setting), and then move the controller in a way that you want to correspond to the chosen sound. Repeating this process maps the controller phase space to the output phase space in an incremental and intuitive manner. In-between states naturally interpolate, though sometimes combine to give unexpected effects. If desired, these can be embraced and refined, if not they can be overwritten by further training. Pretty cool, and quite practical.\n\nHeather Dewey-Hagborg is an artist who\u2019s perhaps best known work to date is Stranger Visions in which she collected various artifacts which carry the DNA of their owners, such as chewing gum and cigarette butts. She extracts and PCRs DNA from these and uses it to reconstruct the likely facial features of the individual who left the item. The reconstructed faces are then 3d printed and presented with the item. The reconstruction process involves quite a bit of machine learning during matching of known faces to known DNA profiles.\n\nHeather\u2019s talk delved deep into the many fallacies of this approach, the biases in the training sets concerning our preconceived notions of race, gender and other \u201caxes\u201d onto which facial features are mapped. Proprietary technology like this is currently used by law enforcement agencies and it is not hard to see how the limitations and biases of this technology play hand in hand with the already huge existing prejudices. Her key phrase, \u201calgorithms are political,\u201d stuck with me and the generalization to many other areas of machine learning for social prediction and profiling are as straightforward as they are concerning.\n\nBrian Whitman, currently chief scientist at Spotify and cofounder of Echo Nest (currently the basis for most music recommendation algorithms), spoke about different aspects of recommendations systems as well as automatic music generation. He pointed out how the advent of visual generative methods last year followed closely on the heels of progress in image classification. Once again, the duality between perception and creation pops up. He showcased progress in music classification and asserted that generative music is around the corner, positing that within five years we\u2019ll be tuning into robot generative music stations that cater to our personalized tastes. Unsurprisingly, this is something Spotify is already working on.\n\nAllison Parrish presented her work on text manipulation using word2vec. She uses the organized semantic space of word embeddings to change and erode text, by replacing words with other words nearby in the embedding place. She also presented a neat method for taking the vector sequence of a sentence and applying a jpeg-like compression to it. Truncating down to the most important \u201cfrequencies\u201d and then converting back from embedding vectors to words yields a sort of a lossy conversion in which slight artifacts are introduced, depending on the compression factor. Its interesting to think of a sentence as a path in embedding space and how slight alteration of paths leads to almost-right phrases. A similar thing can happen to a patient\u2019s speech center after brain surgery. Right after the operation they experience a similar erosion of the word-choice precision. When asked where they are, they will say \u201cschool\u201d/\u201dinstitute\u201d/\u201doffice\u201d/\u2026 rather than \u201chospital\u201d. If the injuries are minor, the brain quickly repairs the damage and after a few weeks precision returns.\n\nMario Klingemann presented a series of tools he\u2019s been developing to sort and organize vast amounts of book illustrations from the British library. Once organized, he is able to find recurring themes and even cases of plagiarism or modifications applied to drawings used in other books. He uses the image data to create interesting artworks. One method involves marking universal \u201cconnection points\u201d for each item, which can then be used to automatically generate intricate collages of these components. Depending on the recursive rules set he applies, a variety of fascinating effects are achieved:\n\nSuch a curated dataset is also perfect for training a neural network. Mario showed some examples of training a convnet on large, intricately decorated initials from old books. Once trained, techniques such as class visualization or deep dreaming can be used to create new recombinant images:\n\nLynn Cherny presented wonderfully whimsical work on text and images. The first project was a JavaScript RNN-based generator for British town names (based on Andrej Karpathy\u2019s recurrentjs). These were combined with a map that automatically populates with made-up places. Another project was an adaptation of Daniel Shiffman\u2019s demo processing code that evolves drawings using interactive feedback from the user. She adapted the code to use pieces of castles instead of primitives and used the genetic algorithm to evolve interesting castles. The last project was a cocktail generator. Instead of using an RNN or Word2Vec however, Lynn actually painstakingly encoded substitution rules to make strange but realistic sounding cocktail recipes from real ones. Some of the rules involved changing sweet ingredients to positive nouns and bitter ingredients to negative nouns. Other rules replace words based on phonetics. Either way the results were fantastic involving ingredients such as \u201ca dash of nightmare\u201d or \u201c1/4 cup of white pretentiousness\u201d. Here\u2019s one full recipe:\n\nKathryn Hume\u2019s talk, entitled Work of Art in the age of algorithmic reproduction, wrapped up the conference with a philosophical take on style transfer and how meaning and style are intertwined. Gatys, et al discovered that it is possible to factor out style from content in an artificial neural net. Inherently, our own visual systems have learned to do the same. By definition, content recognition has to be invariant to all information that is irrelevant to the semantics of the task. This discarded information can then be regarded as \u201cstyle\u201d, orthogonal to content so to speak. She muses that our ability to create and enjoy art may be a corollary of the inference capabilities of our own visual systems. This raises some interesting questions: given a number of style-transformed images, \u201cWhat\u2019s the essence that\u2019s preserved between different version? What is the \u2018minimal viable\u2019 Mona Lisa?\u201d\n\nAlt-Ai featured a pretty fabulous gallery of various machine learning-based experiments. A few selected pieces:\n\nA full list of gallery projects with pictures and descriptions can be found here."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-part-ii-dc585af054cb",
        "title": "Adventures in Narrated Reality, Part II \u2013 Artists and Machine Intelligence \u2013",
        "text": "To call the film above surreal would be a dramatic understatement. Watching it for the first time, I almost couldn\u2019t believe what I was seeing \u2014 actors taking something without any objective meaning, and breathing semantic life into it with their emotion, inflection, and movement.\n\nAfter further consideration, I realized that actors do this all the time. Take any obscure line of Shakespearean dialogue and consider that 99.5% of the audience who hears that line in 2016 would not understand its meaning if they read it in on paper. However, in a play, they do understand it based on its context and the actor\u2019s delivery.\n\nAs Modern English speakers, when we watch Shakespeare, we rely on actors to imbue the dialogue with meaning. And that\u2019s exactly what happened in Sunspring, because the script itself has no objective meaning.\n\nOn watching the film, many of my friends did not realize that the action descriptions as well as the dialogue were computer generated. After examining the output from the computer, the production team made an effort to choose only action descriptions that realistically could be filmed, although the sequences themselves remained bizarre and surreal. The actors and production team\u2019s interpretations and realizations of the computer\u2019s descriptions was a fascinating case of human-machine collaboration.\n\nFor example, here is the stage direction that led to Middleditch\u2019s character vomiting an eyeball early in the film:\n\nAnd here\u2019s the final description that resulted in the strange action sequence at the end of the film, containing the shot that\u2019s my personal favorite, wherein Middleditch\u2019s character breaks the fourth wall and pulls on the camera itself, followed by a change in camera angle that reveals that he is, in fact, holding nothing at all:\n\nThe machine dictated that Middleditch\u2019s character should pull the camera. However, the reveal that he\u2019s holding nothing was a brilliant human interpretation, informed by the production team\u2019s many years of combined experience and education in the art of filmmaking. That cycle of generation and interpretation is a fascinating dialogue that informs my current understanding of this machine\u2019s capacity to augment our creativity.\n\nAs I wrote in Part I, I believe that augmenting human creativity is the core utility of these machines. However, their use can also inform our understanding of the semantic mechanisms, or lack thereof, embedded in the words we read every day. We typically consider the job of imbuing words with meaning to be that of the writer. However, when confronted with text that lacks objective meaning, the reader assumes that role. In a certain way, the reader becomes the writer.\n\nThat\u2019s what I love most about Sunspring \u2014 that so many versions of it exist. First, there\u2019s the version I nervously watched emerge from my Nvidia Jetson TK1 computer early on the first morning of the contest, after receiving our team\u2019s prompts and using them to seed my LSTM generator. Second, there\u2019s the version that Oscar Sharp, the director, and the rest of the production crew created by cutting down that initial version, which was far longer than the contest\u2019s 5-minute limit would allow, as well as the version they created in their heads when they gave direction to the actors. Third, there\u2019s the version that Thomas Middleditch and our other superb actors created, when they imbued objectively meaningless lines with context using inflection, emotion, and movement. And finally, there are the countless versions that every viewer creates by projecting their own experiences and ideas onto what they\u2019re seeing, and joyously struggling to make sense of something that objectively does not make any sense at all.\n\nFrom the experience of screening this film, I have begun to gather that while some people may not want to read prose or poetry that makes no sense, many of them will watch and enjoy a film that makes no sense (after all, look at any David Lynch project), and will certainly listen to music with lyrics that make no objective sense.\n\nA lot of lyrics, across every musical genre and with countless artists, have no single, correct interpretation. I could list a whole lot of examples, but I\u2019d rather just point to a single particularly salient one: My Back Pages by Bob Dylan. That song is brilliant because you can hear whatever you want in it and project your own experiences onto it, because the lyrics make no sense, even if the song\u2019s Wikipedia page would have you believe otherwise.\n\nIn Sunspring, the song you hear toward the end, when the surreal action sequence begins, uses lyrics generated by an LSTM I trained on about 25,000 folk songs. I worked with Tiger Darrow and Andrew Orkin, two talented musicians who chose lines that would work well with their music, and composed an impressively catchy song in just a few hours.\n\nHere\u2019s a sample of what that lyrics model\u2019s raw output looks like:\n\nFor me, the most rewarding part of watching professional musicians play with lyrics from the machine was watching their transition, in just a few hours, from questioning the machine\u2019s utility entirely to asking if they could use the generated lyrics in their work outside the 48-hour film competition. Later, after someone on the crew asked Tiger if using such a machine would be considered cheating, she made an excellent comparison: \u201cNo,\u201d she said, \u201cIt\u2019s like using a rhyming dictionary,\u201d which is a tool that lyricists use all the time. (At the time, I hesitated to mention that rhyming is a fundamentally algorithmic process, even if many of us don\u2019t usually think of it that way.)\n\nAnd it\u2019s easy to imagine how such a tool might work in practice: the lyricist writes a few lines, maybe gets a bit stuck, then uses the machine to generate possibilities for the next line until she finds one she likes, edits it a bit (if necessary), and continues writing. Autocomplete for lyricists could turn writing song lyrics into a conversational process, with a writing partner made of metal and electricity, perhaps fine tuned with bias toward the topics and references that its human collaborator prefers."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/selection-of-resources-to-learn-artificial-intelligence-machine-learning-statistical-inference-23bc56ba655",
        "title": "Selection of resources to learn Artificial Intelligence / Machine Learning / Statistical Inference\u2026",
        "text": "These are probably enough (combined with some tutorials) if you just want to be able to play with and tweak existing ML/DL code and algorithms.\n\nMachine Learning by Andrew Ng @ Coursera\n\nhttps://www.coursera.org/learn/machine-learning\n\nFantastic introductory course and foundation for ML. Covers basics of ML from linear and logistic regression to artificial neural networks. Gives great insight into concepts and techniques with minimal maths. Requires basic knowledge of linear algebra and differential calculus. Note: doesn\u2019t cover specifities of current deep learning (e.g. convolutional neural networks, recurrent neural networks etc.), so is mainly a great foundation for more advanced studies. Andrew Ng was co-founder of Google Brain and now chief scientist at Baidu research. He is great at giving intuition.\n\nDeep Learning by Google @ Udacity\n\nhttps://www.udacity.com/course/deep-learning--ud730\n\nBrief introduction to DL for those who are familiar with ML. This is a very short course, I think I went through the whole thing in under 2 hours. It\u2019s almost a reading of the tensorflow tutorials (https://www.tensorflow.org/versions/master/tutorials/index.html ). It gives a top level summary of basic DL techniques. Assumes you\u2019re comfortable with ML and related concepts. So at least Andrew Ng\u2019s coursera (or equivalent knowledge) is a must. Don\u2019t expect to be a DL wizard after this, but at least you might know what a CNN or RNN is. If you\u2019re going to look at any of the advanced ML courses below, watch this DL course after them.\n\nThese will help you understand what\u2019s actually going on, perhaps even understand some DL papers (I say \u2018some\u2019 DL papers because others are just insanely theoretical and dense).\n\nCS188 Introduction to Artificial Intelligence by Pieter Abbeel @ Berkeley\n\n(some videos have audio issues, so below are a bunch of playlists from different years, I had to pick and choose from different playlists depending on audio problems).\n\nhttps://www.youtube.com/channel/UCDZUttQj8ytfASQIcvsLYgg (Spring 2015)\n\nhttps://www.youtube.com/channel/UCB4_W1V-KfwpTLxH9jG1_iA (Spring 2014)\n\nhttps://www.youtube.com/channel/UCshmLD2MsyqAKBx8ctivb5Q (Fall 2013)\n\nhttps://www.youtube.com/user/CS188Spring2013 (Spring 2013)\n\nThis is a fantastic introduction to AI in general, not specifically ML and introduces many different fundamental areas of AI and ML. Spreads the net very wide, so if all you\u2019re interested in is playing convolutional neural networks to make things like Deepdream, then 90% of this course won\u2019t be relevant. The first half is more agent-based AI starting with CSPs, decision trees, MDPs etc, and in that respect it is a bit unique compared to the other courses on this list. Then goes into various different classic ML topics. It is an introduction, so requires no prior knowledge of AI or ML, but it does go into maths, so requires decent understanding of the usual probability, linear algebra, calculus etc. Doesn\u2019t cover DL but a great foundation for a lot of AI and ML, especially if you want to get more into agent-based AI such as RL and Monte Carlo Tree Search (MCTS).\n\nCS540 Machine Learning by Nando de Freitas @ UBC 2013\n\nhttps://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6\n\nThis covers many classic ML and SI etc from start all the way to neural networks. Doesn\u2019t require prior knowledge of ML, so can be considered comprehensive introduction. It\u2019s way more thorough and detailed than Andrew Ng\u2019s Coursera and goes heavy into maths. Bear in mind it\u2019s a post-graduate CS course so it\u2019s quite advanced. Again spreads the net quite wide, but not as wide as CS188, instead goes deeper into some areas. Only brief intro to DL but comprehensive foundation in ML and SI. Nando is ace. Also prof at Oxford and works for Deepmind.\n\nCS340 Machine Learning by Nando de Freitas @ UBC 2012\n\nhttps://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf\n\nSimilar to above, but undergraduate version. I haven\u2019t actually watched these so I don\u2019t know how they differ from CS540. Probably bit simpler.\n\nDeep Learning by Nando de Freitas @ Oxford 2015\n\nhttps://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu\n\nSimilar to CS540 but more about DL. Definitely requires more understanding of statistics and multivariate differential calculus, and prior knowledge in ML/SI (Andrew Ng\u2019s coursera may be enough, but I really recommend Nando\u2019s CS540 or Pieter\u2019s CS188). Even knowledge of information theory would be useful. Great guest lectures by Alex Graves on generative RNNs and Karol Gregor on VAEs.\n\nCS229 Machine Learning by Andrew Ng @ Stanford 2008\n\nhttps://www.youtube.com/view_play_list?p=A89DCFA6ADACE599\n\nAnother very comprehensive introduction to ML/SI. Nothing like his Coursera, way more theoretical and covers lots more topics, and much more thorough. Kind of like a mashup of Pieter Abbeel\u2019s CS188 AI Course and Nando de Freitas\u2019s CS540 ML Course. This course is more detailed in some areas, and less detailed in others (e.g. AFAIR goes deeper into MDPs and RL than Abbeel\u2019s CS188, but doesn\u2019t cover bayes nets). They all provide slightly different perspectives and insights. Also doesn\u2019t cover DL, just a really solid comprehensive foundation for ML and SI.\n\nNeural Networks for Machine Learning by Geoffrey Hinton @ Coursera\n\nhttps://www.coursera.org/course/neuralnets\n\nGoes deep into some areas of DL and rather advanced. Hinton is one of the titans of DL and there is a lot of insight in here, but I found it a bit all over the place and I wasn\u2019t a huge fan of it. I.e. I don\u2019t think it\u2019s very useful as a linear educationalresource and requies prior knowledge of ML, SI and DL. If you first learn these topics elsewhere (e.g. videos above) and then come back to this course then you can find great insight. Otherwise if you dive straight into this you will get lost.\n\nComputational Neuroscience by Rajesh Rao & Adrienne Fairhall @ Coursera\n\nhttps://www.coursera.org/course/compneuro\n\nNot directly related to DL but fascinating nevertheless. Starts quite fun but gets rather heavy, especially Adrienne\u2019s sections. Rajesh takes things quite slow and re-iterates everything, but I think Adrienne is used to dealing with comp-neuroscience postgrad students and flies through the slides. Expect to pause the video on every slide while you try to digest what\u2019s on the screen. Requires decent understanding of the usual suspects, linear algebra, differential calculus, probability and statistical analysis, including things like PCA etc."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/deepdream-art-and-machine-learning-symposium-2016-recap-396d1ecf87e3",
        "title": "DeepDream Art and Machine Learning Symposium 2016 recap",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/can-artificial-intelligence-be-used-to-help-artists-improve-c5c6ff056c56",
        "title": "Can artificial intelligence be used to help artists improve?",
        "text": "Can artificial intelligence be used to help artists improve?\n\nLast August, researchers published an algorithm (Gatys et al) that enables the style of one image to be recomposed with the content of another. Many in the computational arts community were immediately excited and jumped in to try it. Notably, artists Gene Kogan and Kyle McDonald shared extensive restyling experiments with recognizable styles by established artists. The results were quite impressive.\n\nThe remarkable ability to perform a creative task that takes humans a lifetime to master has already spawned popular takes announcing the end of human art, and decrying the fact that artificial intelligence is going to put artists out of business.\n\nIts not quite so simple. There is a long history of procedural, generative and indeterminate art works, where the artist \u2018outsources\u2019 some of creation to a system, another person, or even to chance. I would argue that many formalist experiments are investigating the same thing; what does it mean to move some of the agency away from the hand of the artist?\n\nA lot of research focuses on the promise of collaboration between human and machine. Tandem, a piece by Harshit Agrawal and Arnav Kapur of the MIT Media Lab, appeared in last weekend\u2019s Alt-AI exhibition. Not only does the user draw in collaboration, they also get to choose the \u2018character\u2019 of their digital partner.\n\nFlow Machines, a project led by Fran\u00e7ois Pachet in Paris, is a workflow for the manipulation of computationally modeled style specifically with music and text. The idea is taken further, proposing that easy meta-manipulation of vast amounts of stylistic materials in the canon will allow the creator to stay in a flow state and iterate quickly to expand their own style.\n\nThis sounds incredibly fun and exciting to try, but there are some ambiguities as how this will help artists better their work. Its not readily clear how to address such questions as the value of physically embodying the motions needed to generate art or music, and how to amplify the strength of one\u2019s inner vision to conceive of it in the first place.\n\nI decided to try using the style transfer algorithm to help me develop my own style. Rather than remixing the work of others, I was interested to see what happened when I remixed myself. Can I use style transfer technology not to recreate preexisting styles but to develop a new one?\n\nMy background is in media art but I also paint sometimes. I should also add that I was born in Hawaii and am the daughter of a florist, which explains the tendency to paint oversize tropical flowers every time I sit down. Figurative art interests me because of the fact that its often disregarded as uninteresting and naive in contemporary art circles. However, there are ways that it can be very personally revealing in a way that abstract or conceptual pieces can\u2019t.\n\nLike others who draw and paint, I\u2019m antsy to be better at it. I\u2019d especially like to embark on more complex and richer textures. In a way that feels sacrilege to the craft and physical ritual of painting, I thought, what if I could just take a shortcut to developing my own style?\n\nI decided to draw the same portrait of a woman surrounded by flowers, in an arrangement that is similar to earlier paintings of mine. This time I tried to sketch the same portrait five times, but with five different qualities. To keep an admittedly subjective creative experiment on the methodical side, I wanted the five versions to exhibit a scale of expressive qualities, from timid and naive, to more gratuitous and haphazard."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/harold-cohen-rip-578bffb553f4",
        "title": "Harold Cohen RIP \u2013 Artists and Machine Intelligence \u2013",
        "text": "Harold Cohen (born May 1st, 1928 in Britain) studied painting at the Slade School of Fine Arts and taught there for several years before moving to San Diego in 1968. Serialist composers like Iannis Xenakis and Gottfried Michael Koenig had begun to experiment with using computers to generate music but the intersection of artificial intelligence and art was largely unexplored. Around this time Cohen began working on AARON, a computer program that can independently create original abstract paintings. Initially the software controlled a plotter, and later a robotic gantry applying paint to real canvas. Each work produced by AARON is unique.\n\nMany have questioned whether what AARON produces can or should be considered art \u2014 and if so, whose art it is. Despite this debate, works have ended up in private collections around the world as well as public institutions like the Tate Gallery.\n\nThese questions about the nature of art and agency were at the core of Cohen\u2019s work. In his 1995 essay \u201cthe further exploits of AARON, Painter\u201d he wrote:\n\nIf what AARON is making is not art, what is it exactly, and in what ways, other than its origin, does it differ from the \u201creal thing?\u201d If it is not thinking, what exactly is it doing?\n\nCohen\u2019s work pioneered the field of computational creativity \u2014 an area of research which has recently gained renewed attention through advances in neural net technology. His work will continue to influence and inspire a generation of young programmers interested in the still nascent field of machine intelligence and art."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/a-digital-god-for-a-digital-culture-resonate-2016-15ea413432d1",
        "title": "A digital god for a digital culture. Resonate 2016 \u2013 Artists and Machine Intelligence \u2013",
        "text": "This is my favorite image of the presentation. I found it on iStock and it was actually called \u201cDoes everyone understand?\u201d.\n\nEarlier when talking about Big Data and Robot Readable Worlds, I mentioned the driving force for AI was building machines that could understand data, and produce an executive summary.\n\nI realize \u2018Understand\u2019 is a loaded word. A complex concept, and there most definitely isn't a cross-discipline universally agreed definition of what it means or how it works. However in Artificial Intelligence \u2014 or looking at human understanding through the lens of AI (which is how the field of AI was born) \u2014 there is a rough agreement / direction on what it is. And that involves data compression.\n\nAn oversimplification is: finding a more compact way of representing information is \u2018learning\u2019, i.e. finding regularities and patterns in data. Learning a compression (or model) that allows explanations of the data and future predictions is \u2018understanding\u2019.\n\nAnd the more that you can compress, and predict, the more you've learnt and understood.\n\nWhenever we\u2019re presented with something new \u2014 a new image, a new sound, a new object, a new concept \u2014 we look at it through the lens of what we already know, and we try to strip it apart into components that are already familiar to us, and store it in relation to those familiar components,\n\nImagine these sentences purely as sequences of lines and shapes (The second line is a Google translation of the first, which I believe is incorrect, but I prepared this slide late last night and didn't have time to correct it).\n\nIf I had to memorize each of these sentences, to replicate elsewhere \u2014 i.e. write down without looking \u2014 the first one would be a lot easier for me. Because I can compress that information really well. The most important thing for me to remember is the concept of \u2018horse\u2019. Once I have that, \u2018fence\u2019 is more probable for me in that context (e.g. compared to the word \u2018fortnight\u2019) so I can compress it well. As soon as I store \u2018horse\u2019 and \u2018fence\u2019 concepts, the concept of \u2018over\u2019 is very probable in that context too, so can be stored with very few bits. And once I have \u2018horse\u2019, \u2018fence\u2019 and \u2018over\u2019; \u2018jump\u2019 is almost implied because it\u2019s so probable, so I can store it with very very few bits. Once I've stored those concepts, I already know how to spell the words. H-o-r-s-e for horse etc. I already know how to draw each letter. And I actually know how to draw each letter as sequences of primitive shapes such as straight lines, curves, intersections etc which I learnt as a baby. So all extremely compressible. And when I look at the sentence, the same process happens in reverse. First the receptive fields in my retinal ganglion cells apply filters to the incoming light signals and pick up all of the edges and most fundamental shapes and orientations. Further along the visual cortex those different edge and basic shape detections are cross-correlated and I recognize letters. As I recognize letters, the brain predicts other most likely phenomena. E.g. when I see H-O-R-S, it\u2019s highly likely that that\u2019s HORSE and will be followed by an E. Collections of letters trigger words, words trigger concepts etc.\n\nWhereas in the second sentence \u2014 I don\u2019t speak Japanese \u2014 so I would have to memorize it as a sequence of shapes, lines, curves etc. That\u2019s a lot of information that I can\u2019t compress very well. Still I\u2019d try to compress it, perhaps as: a 2x3 grid with a beard; followed by a vertical line; then a kind of pound sign without a top; a 7 with a curved bottom; sideways capital H; 30 degree rotated smiley face with one eye etc. So there\u2019s still a bit of compression that I can do, but it\u2019s far from optimal.\n\nPsychologists and philosophers might frown at this definition of learning and understanding. Especially anti-computationalists. But what I just said shouldn't be seen as a comprehensive explanation of what learning and understanding is, especially in humans, or even as a defence of functionalism or computationalism. It\u2019s a hypothesis, of a testable model of learning and understanding in computational systems, as seen through the lens of Information Theory.\n\nBut J\u00fcrgen Schmidhuber (pronounced \u201cYou Again Scmidhuuboo) \u2014 one of the rather important figures in artificial intelligence and machine learning \u2014 takes it a bit further and applies this concept as a general intrinsic driving force for all intelligent agents.\n\nHe believes that this is the underlying principle for unsupervised learning and the path to general artificial intelligence, human-level intelligence. That curiosity and creativity are fueled by our intrinsic desire to develop better compressors. Within our capacity \u2014 as deemed fit by our evolutionary survival strategy \u2014 the more we are able to compress and predict, the better we have understood the world, and thus will be more successful in dealing with it.\n\nAs we receive information from the environment via our senses, our compressor is constantly comparing the new information to predictions it\u2019s making. If predictions match the observations, this means our compressor is doing well and no new information needs to be stored. The subjective beauty of the new information is proportional to how well we can compress it (i.e. how many bits we are saving with our compression \u2014if it\u2019s very complex but very familiar then that\u2019s a high compression). We find it beautiful because that is the intrinsic reward of our intrinsic motivation system, to try to maximize compression and acknowledge familiarity.\n\nHowever if the predictions of our compressor do not match the observations, our compressor /predictor has failed. That is good thing, an opportunity to learn something new about our environment. Our curiosity drive is encouraged to find such observations, to find new information that our compressor cannot initially compress.\n\nIf we store the new incompressible information as is, i.e. without learning to compress it, that is not ideal. Because we haven\u2019t actually learnt anything, or improved our compressor. However if we are able to find new regularities in the new information, that means we have improved our compressor. What was incompressible, has now become compressible. That is subjectively interesting. The amount we improve our compressor by, is defined as how subjectively interesting we find that new information. Or in other words, subjective interestingness of information is the first derivative of its subjective beauty, and rewarded as such by our intrinsic motivation system.\n\nSchmidhuber has written many papers and articles (1, 2, 3) on this with mathematical formalizations on how it can be applied to define and drive attention, curiosity, creativity, beauty, poetry, art, science and even humour. Here he is explaining a joke. Please imagine that it\u2019s read with a strong German accent.\n\nImportant to note how he distinguishes a random punch-line (one that is totally unrelated to the rest of the joke, and thus we will not be able to learn to compress i.e. relate to the joke) to a punch-line that seems initially unrelated (i.e. we fail to predict, and doesn't match our current compressor), but then once we \u2018get-it\u2019, we recognize how it relates and are able to compress. We have improved our compressor. And that\u2019s why it\u2019s funny. He must be so much fun to hang out with.\n\nTo summarize, I'm an old fashioned artist. I paint landscapes\u2026"
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/adventures-in-narrated-reality-6516ff395ba3",
        "title": "Adventures in Narrated Reality \u2013 Artists and Machine Intelligence \u2013",
        "text": "At this point, since this is my very first Medium post, perhaps I should introduce myself: my name is Ross Goodwin, I\u2019m a graduate student at NYU ITP in my final semester, and computational creative writing is my personal obsession.\n\nBefore I began my studies at ITP, I was a political ghostwriter. I graduated from MIT in 2009 with a B.S. degree in Economics, and during my undergraduate years I had worked on Barack Obama\u2019s 2008 Presidential campaign. At the time, I wanted to be a political speechwriter, and my first job after graduation was a Presidential Writer position at the White House. In this role, I wrote Presidential Proclamations, which are statements of national days, weeks, and months of things\u2014everything from Thanksgiving and African American History Month to lesser known observances like Safe Boating Week. It was a very strange job, but I thoroughly enjoyed it.\n\nI left the White House in 2010 for a position at the U.S. Department of the Treasury, where I worked for two years, mostly putting together briefing binders for then-Secretary Timothy Geithner and Deputy Secretary Neal Wolin in the Department\u2019s front office. I didn\u2019t get many speechwriting opportunities, and pursuing a future in the financial world did not appeal to me, so I left to work as a freelance ghostwriter.\n\nThis was a rather dark time in my life, as I rapidly found myself writing for a variety of unsavory clients and causes in order to pay my rent every month. In completing these assignments, I began to integrate algorithms into my writing process to improve my productivity. (At the time, I didn\u2019t think about these techniques as algorithmic, but it\u2019s obvious in retrospect.) For example, if I had to write 12 letters, I\u2019d write them in a spreadsheet with a paragraph in each cell. Each letter would exist in a column, and I would write across the rows\u2014first I\u2019d write all the first paragraphs as one group, then all the second paragraphs, then all the thirds, and so on. If I had to write a similar group of letters the next day for the same client, I would use an Excel macro to randomly shuffle the cells, then edit the paragraphs for cohesion and turn the results in as an entirely new batch of letters.\n\nWriting this way, I found I could complete an 8-hour day of work in about 2 hours. I used the rest of my time to work on a novel that\u2019s still not finished (but that\u2019s a story for another time). With help from some friends, I turned the technique into a game we called The Diagonalization Argument after Georg Cantor\u2019s 1891 mathematical proof of the same name.\n\nIn early 2014, a client asked me to write reviews of all the guides available online to learn the Python programming language. One guide stood out above all others, in the sheer number of times I saw users reference it on various online forums and in the countless glowing reviews it had earned across the Internet: Learn Python the Hard Way by Zed Shaw\n\nSo, to make my reviews better, I decided I might as well try to learn Python. My past attempts at learning to code had failed due to lack of commitment, lack of interest, or lack of a good project to get started. But this time was different somehow\u2014Zed\u2019s guide worked for me, and just like that I found myself completely and hopelessly addicted to programming.\n\nAs a writer, I gravitated immediately to the broad and expanding world of natural language processing and generation. My first few projects were simple poetry generators. And once I moved to New York City and started ITP, I discovered a local community of likeminded individuals leveraging computation to produce and enhance textual work. I hosted a Code Poetry Slam in November 2014 and began attending Todd Anderson\u2019s monthly WordHack events at Babycastles.\n\nIn early 2015, I developed and launched word.camera, a web app and set of physical devices that use the Clarifai API to tag images with nouns, ConceptNet to find related words, and a template system to string the results together into descriptive (though often bizarre) prose poems related to the captured photographs. The project was about redefining the photographic experience, and it earned more attention than I expected [1,2,3]. In November, I was invited to exhibit this work at IDFA DocLab in Amsterdam.\n\nAt that point, it became obvious that word.camera (or some extension thereof) would become my ITP thesis project. And while searching for ways to improve its output, I began to experiment with training my own neural networks rather than using those others had trained via APIs."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/what-is-ami-96cd9ff49dde",
        "title": "What is AMI? \u2013 Artists and Machine Intelligence \u2013",
        "text": "What do art and technology have to do with each other? What is machine intelligence, and what does \u201cmachine intelligence art\u201d look, sound and feel like? What are the emerging relationships between humans and machines; what does it mean to be human; and what can we learn about intelligence, human or otherwise, through art? How should we think about our future?\n\nThese are some of the questions we\u2019ve been asking ourselves, especially in the wake of a series of breakthroughs in neural network-generated imagery beginning in the summer of 2015. Artists and Machine Intelligence is a long-term collaboration we\u2019ve begun to explore these questions through art. It brings together artists and thinkers, researchers, engineers, and, of course, the machine intelligences these researchers and engineers are building.\n\nWe\u2019ll also try to think our way through the big questions in a series of essays. The first essay, Art in the Age of Machine Intelligence, introduces many of the themes we hope to develop over time in more detail."
    },
    {
        "url": "https://medium.com/artists-and-machine-intelligence/what-is-ami-ccd936394a83",
        "title": "Art in the Age of Machine Intelligence \u2013 Artists and Machine Intelligence \u2013",
        "text": "Art has always existed in a complex, symbiotic and continually evolving relationship with the technological capabilities of a culture. Those capabilities constrain the art that is produced, and inform the way art is perceived and understood by its audience.\n\nLike the invention of applied pigments, the printing press, photography, and computers, we believe machine intelligence is an innovation that will profoundly affect art. As with these earlier innovations, it will ultimately transform society in ways that are hard to imagine from today\u2019s vantage point; in the nearer term, it will expand our understanding of both external reality and our perceptual and cognitive processes.\n\nAs with earlier technologies, some artists will embrace machine intelligence as a new medium or a partner, while others will continue using today\u2019s media and modes of production. In the future, even the act of rejecting it may be a conscious statement, just as photorealistic painting is a statement today. Any artistic gesture toward machine intelligence \u2014 whether negative, positive, both, or neither \u2014 seems likelier to withstand the test of time if it\u2019s historically grounded and technically well informed.\n\nWalter Benjamin illustrated this point mordantly in his 1931 essay, Little History of Photography, citing an 1839 critique of the newly announced French daguerreotype technology in the Leipziger Stadtanzeiger (a \u201cchauvinist rag\u201d):\n\nThis sense of affront over the impingement of technology on what had been considered a defining human faculty has obvious parallels with much of today\u2019s commentary on machine intelligence. It\u2019s a reminder that what Rosi Braidotti has called \u201cmoral panic about the disruption of centuries-old beliefs about human \u2018nature\u2019\u201d is nothing new. [1]\n\nBenjamin goes on to comment:\n\nWhile these \u201ctheoreticians\u201d remained stuck in their thinking, practitioners were not standing still. Many professionals who had been making their living painting miniature portraits enacted a very successful shift to studio photography; and with those who brought together technical mastery and a good eye, art photography was born, over the following decades unfolding a range of artistic possibilities latent in the new technology that had been inaccessible to painters: micro-, macro- and telephotography, frozen moments of gesture and microexpression, slow motion, time lapse, negatives and other manipulations of the film, and on and on.\n\nArtists who stuck to their paintbrushes also began to realize new possibilities in their work, arguably in direct response to photography. David Hockney interprets cubism from this perspective:\n\nOf course, the ongoing relationship between painting and photography is by no means mutually exclusive; the language of wholesale embrace on the one hand versus response or critique on the other is inadequate. Hockney\u2019s \u201cjoiners\u201d explored rich artistic possibilities in the combination of photography with \u201ca painter\u2019s hand and eye\u201d via collage in the 1980s, and his more recent video pieces from Woldgate Woods do something similar with montage.\n\nHockney was also responsible, in his 2001 collaboration with physicist Charles Falco, for reigniting interest in the role optical instruments \u2014 mirrors, lenses, and perhaps something like a camera lucida \u2014 played in the sudden emergence of visual realism in early Renaissance art. It has been clear for a long time that visual effects like the anamorphic skull across the bottom of Hans Holbein\u2019s 1553 painting The Ambassadors could not have been rendered without clever optical tricks involving tracing from mirrors or lenses \u2014 effectively, paintbrush-assisted photography. [3] Had something like the Daguerre-Ni\u00e9pce photochemical process existed in their time, it seems likely that artists like van Eyck and Holbein would have experimented with it, either in addition to, in combination with, or even instead of paint.\n\nSo, the old European masters fetishized by the Leipziger Stadtanzeiger were not reproducing \u201cman\u2019s God-given features without the help of any machine\u201d, but were in fact using the state of the art. They were playing with the same new optical technologies that allowed Galileo to discover the moons of Jupiter, and van Leeuwenhoek to make the first observations of microorganisms.\n\nUnderstanding the ingenuity of the Renaissance artists as users and developers of technology should only increase our regard for them and our appreciation of their work. It should not come as a surprise, as in their own time they were not \u201cOld Masters\u201d canonized in the historical wings of national art museums, but intellectual and cultural innovators. To imagine that optics somehow constituted \u201ccheating\u201d in Renaissance painting is both a failure of the imagination and the application of a historically inappropriate value system. Yet even today, some commentators and theoreticians \u2014 typically not themselves working artists \u2014 remain wedded to what Benjamin called \u201cthe philistine notion of \u2018art\u2019\u201d, as pointed out in an article in The Observer from 2000 in response to the Hockney-Falco thesis:\n\nThere is a pungent irony here. Scientific inquiry has, step by step, revealed to us a universe much more vast and complex than the mythologies of our ancestors, while the parallel development of technology has extended our creative potential to allow us to make works (whether we call them \u201cart\u201d, \u201cdesign\u201d, \u201ctechnology\u201d, \u201centertainment\u201d, or something else) that would indeed appear miraculous to a previous generation. Where we encounter the word \u201cblasphemy\u201d, we may often read \u201cprogress\u201d, and can expect miracles around the corner. [5]\n\nOne would like to believe that, after being discredited so many times and over so many centuries, the \u201cantitechnological concept of art\u201d would be relegated to a fundamentalist fringe. However, if history has anything to teach us in this regard, it\u2019s that this particular debate is always ready to resurface. Perhaps this is because it impinges, consciously or not, on much larger issues of human identity, status and authority. We resist epistemological shock. Faced with a new technical development in art it\u2019s easier for us to quietly move the goalposts after a suitable period of outrage, re-inscribing what it means for something to be called fine art, what counts as skill or creativity, what is natural and what is artifice, and what it means for us to be privileged as uniquely human, all while keeping our categorical value system \u2014 and our human apartness from the technology \u2014 fixed.\n\nMore radical thinking that questions the categories and the value systems themselves comes from writers like Donna Haraway and Joanna Zylinska. Haraway, originally a primatologist, has done a great deal to blur the conceptual border between humans and other animals [6]; the same line of thinking led her to question human exceptionalism with respect to machines and human-machine hybrids. This may seem like speculative philosophy best left to science fiction, but in many respects it already applies. Zylinska, in her 2002 edited collection The Cyborg Experiments: The Extensions of the Body in the Media Age, interviewed the Australian performance artist Stelarc, whose views on the relationship between humanity and technology set a useful frame of reference:\n\nAs Zylinska and her coauthor Sarah Kember elaborate in their book Life after New Media, one should not conclude that anything goes, that the direction of our development is predetermined, or that technology is somehow inherently utopian. Many of us working actively on machine intelligence are, for example, co-signatories of an open letter calling for a worldwide ban on autonomous machine intelligence-enabled weapons systems, which do pose very real dangers. Sherry Turkle has written convincingly [8] about the subtler, but in their way equally disturbing failures of empathy, self-control and communication that can arise when we project emotion onto machines that have none, or use our technology to mediate our interpersonal relationships to the exclusion of direct human contact. It\u2019s clear that, as individuals and as a society, we don\u2019t always make good choices; so far we\u2019ve muddled through, with plenty of (hopefully instructive, so far survivable) missteps along the way. However, Zylinska and Kember point out,\n\nPerhaps it\u2019s unsurprising that these perspectives have often been explored by feminist philosophers, while replicants and terminators come from the decidedly more masculine (and speculative) universes of Philip K. Dick, Ridley Scott and James Cameron. On the most banal level, the masculine narratives tend to emphasize hierarchy, competition, and winner-takes-all domination, while these feminist narratives tend to point out the collaborative, interconnected and non-zero sum; more tellingly, they point out that we are already far into and part of the cyborg future, deeply entangled with technology in every way, not organic innocents subject to a technological onslaught from without at some future date.\n\nThis point of view invites us to rethink art as something generated by (and consumed by) hybrid beings; the technologies involved in artistic production are not so much \u201cother\u201d as they are \u201cpart of\u201d. As the media philosopher Vil\u00e9m Flusser put it, \u201ctools [\u2026] are extensions of human organs: extended teeth, fingers, hands, arms, legs.\u201d [10] Preindustrial tools, like paintbrushes or pickaxes, extend the biomechanics of the human body, while more sophisticated machines extend prosthetically into the realms of information and thought. Hence, \u201cAll apparatuses (not just computers) are [\u2026] \u2018artificial intelligences\u2019, the camera included [\u2026]\u201d. [11]\n\nThat the camera extends and is modeled after the eye is self-evident. Does this make the eye a tool, or the camera an organ \u2014 and is the distinction meaningful? Flusser\u2019s characterization of the camera as a form of intelligence might have raised eyebrows in the 20th century, since, surrounded by cameras, many people had long since reinscribed the boundaries of intelligence more narrowly around the brain \u2014 perhaps, as we\u2019ve seen, in order to safeguard the category of the uniquely human. Calling the brain the seat of intelligence, and the eye therefore a mere peripheral, is a flawed strategy, though. We\u2019re not brains in biological vats. Even if we were to adopt a neurocentric attitude, modern neuroscientists typically refer to the retina as an \u201coutpost of the brain\u201d [12], as it\u2019s largely made out of neurons and performs a great deal of information processing before sending encoded visual signals along the optic nerve.\n\nDo cameras also process information nontrivially? It\u2019s remarkable that Flusser was so explicit in describing the camera as having a \u201cprogram\u201d and \u201csoftware\u201d when he was writing his philosophy of photography in 1983, given that the first real digital camera was not made until 1988. Maybe it took a philosopher\u2019s squint to notice the \u201cprogramming\u201d inherent in the grinding and configuration of lenses, the creation of a frame and field of view, the timing of the shutter, the details of chemical emulsions and film processing. Maybe, also, Flusser was writing about programming in a wider, more sociological sense.\n\nBe this as it may, for today\u2019s cameras, this is no longer a metaphor. The camera in your phone is indeed powered by software, amounting at a minimum to millions of lines of code. Much of this code performs support functions peripheral to the actual imaging, but some of it makes explicit the nonlinear summing-up of photons into color components that used to be physically computed by the film emulsion. Other code does things like removing noise in near-constant areas, sharpening edges, and filling in defective pixels with plausible surrounding color, not unlike the way our retinas hallucinate away the blood vessels at the back of the eye that would otherwise mar our visual field. The images we see can only be \u201cbeautiful\u201d or \u201creal-looking\u201d because they have been heavily processed, either by neural machinery or by code (in which case, both), operating below our threshold of consciousness. In the case of the software, this processing relies on norms and aesthetic judgments on the part of software engineers, so they are also unacknowledged collaborators in the image-making. [13] There\u2019s no such thing as a natural image; perhaps, too, there\u2019s nothing especially artificial about the camera.\n\nThe flexibility of code allows us to make cameras that do much more than producing images that can pass for natural. Researchers like those at MIT Media Lab\u2019s Camera Culture group have developed software-enabled nontraditional cameras (many of which still use ordinary hardware) that can sense depth, see around corners, or see through skin; Abe Davis and collaborators have even developed a computational camera that can \u201csee\u201d sound, by decoding the tiny vibrations of houseplant leaves and potato chip bags. So, Flusser was perhaps even more right than he realized in asserting that cameras follow programs, and that their software has progressively become more important than their hardware. Cameras are \u201cthinking machines\u201d.\n\nIt follows that when a photographer is at work nowadays, she does so as a hybrid artist, thinking, manipulating and encoding information with neurons in both the brain and the retina, working with muscles, motors, transistors, and millions of lines of code. Photographers are cyborgs.\n\nWhat new kinds of art become possible when we begin to play with technology analogous not only to the eye, but also to the brain? This is the question that launched the Artists and Machine Intelligence program. The timing is not accidental. Over the past several years, approaches to machine intelligence based on approximating the brain\u2019s architecture have started to yield impressive practical results \u2014 this is the explosion in so-called \u201cdeep learning\u201d or, more accurately, the renaissance of artificial neural networks. In the summer of 2015, we also began to see some surprising experiments hinting at the creative and artistic possibilities latent in these models.\n\nTo understand the lineage of this body of work will involve going back to the origins of computing, neuroscience, machine learning and artificial intelligence. For now, we\u2019ll briefly introduce the two specific technologies used in our first gallery event, Deep Dream (in partnership with Gray Area Foundation for the Arts in San Francisco). These are \u201cInceptionism\u201d or \u201cDeep Dreaming\u201d, first developed by Alex Mordvintsev at Google\u2019s Zurich office, and \u201cstyle transfer\u201d, first developed by Leon Gatys and collaborators in the Bethge Lab at the Centre for Integrative Neuroscience in T\u00fcbingen. [14] (In later essays we\u2019ll explain how these work.) It\u2019s fitting and likely a sign of things to come that one of these developments came from a computer scientist working on a neurally inspired algorithm for image classification, while the other came from a grad student in neuroscience working on computational models of the brain. We\u2019re witnessing a time of convergences: not just across disciplines, but between brains and computers; between scientists trying to understand and technologists trying to make; and between academia and industry. We don\u2019t believe the convergence will yield a monoculture, but a vibrant hybridity.\n\nThese are early days. The art realizable with the current generation of machine intelligence might generously be called a kind of neural daguerreotype. More varied and higher-order artistic possibilities will emerge not only through further development of the technology, but through longer term collaborations involving a wider range of artists and intents. This first show at the Gray Area is small in scale and narrow in scope; it stays close to the early image-making processes that first inspired AMI. We believe the magic in the pieces is something akin to that of Robert Cornelius\u2019s tentative self portrait in 1839.\n\nAs machine intelligence develops, we imagine that some artists who work with it will draw the same critique leveled at early photographers. An unsubtle critic might accuse them of \u201ccheating\u201d, or claim that the art produced with these technologies is not \u201creal art\u201d. A subtler (but still antitechnological) critic might dismiss machine intelligence art wholesale as kitsch. As with art in any medium, some of it undoubtedly will be kitsch \u2014 we have already seen examples \u2014 but some will be beautiful, provocative, frightening, enthralling, unsettling, revelatory, and everything else that good art can be.\n\nDiscoveries will be made. If previous cycles of new technology in art are any guide, then early works have a relatively high likelihood of enduring and being significant in retrospect, since they are by definition exploring new ground, not retreading the familiar. Systematically experimenting with what neural-like systems can generate gives us a new tool to investigate nature, culture, ideas, perception, and the workings of our own minds.\n\nOur interest in exploring the possibilities of machine intelligence in art could easily be justified on these grounds alone. But we feel that the stakes are much higher, for several reasons. One is that machine intelligence is such a profoundly transformational technology; it is about creating the very stuff of thought and mind. The questions of authenticity, reproducibility, legitimacy, purpose and identity that Walter Benjamin, Vil\u00e9m Flusser, Donna Haraway and others have raised in the context of earlier technologies shift from metaphorical to literal; they become increasingly consequential. In the era where so many of us have become \u201cinformation workers\u201d (just as I am, in writing this piece), the issues raised by MI aren\u2019t mere \u201ctheory\u201d to be endlessly rehearsed by critics and journalists. We need to make decisions, personally and societally. A feedback loop needs to be closed at places like Google, where our work as engineers and researchers will have a real effect on how the technology is developed and deployed.\n\nThis requires that we apply ourselves rigorously and imaginatively across disciplines. The work can\u2019t be done by technophobic humanists, any more than it can be done by inhuman technologists. Luckily, we are neither of the above. Both categories are stereotypes, if occasionally self-fulfilling ones, perpetuated by an unhelpful cultural narrative: the philistines again, claiming that artists are elves, and technical people dwarves, when of course the reality is that we are all (at least) human. There\u2019s no shortage today of artists and intellectuals who, like Alberti, Holbein or Hockney, are eager to work with and influence the development of new technologies. There\u2019s also no shortage of engineers and scientists who are thoughtful and eager to engage with artists and other humanists. And of course the binary is false; there are people who are simultaneously serious artists and scientists or engineers. We are lucky to have several such among our group of collaborators."
    }
]