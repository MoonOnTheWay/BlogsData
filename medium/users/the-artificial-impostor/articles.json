[
    {
        "url": "https://medium.com/the-artificial-impostor/pytorch-0-4-0-release-1-0-preview-86ca50441b0b?source=---------0",
        "title": "PyTorch 0.4.0 Release & 1.0 Preview \u2013 The Artificial Impostor \u2013",
        "text": "This is an experimental series in which I briefly introduce the interesting data science stuffs I read, watched, or listened to during the week. Please give this post some claps if you\u2019d like this series to be continued.\n\nI\u2019ve been busy with other stuffs this week, so this issue will only cover the new Pytorch 0.4.0 and the roadmap to the production ready 1.0 version.\n\nA perhaps incomplete list of important changes with a brief summary for each one of them:\n\nThe code samples at the end of the migration guide are a good way to check if you\u2019ve understood the above changes correctly.\n\nSimilarly, a maybe incomplete list of new features:\n\nProbably one of the most important takeaways:\n\nBasically Facebook is merging Caffe2 and PyTorch to provide both a framework that works for both research and production settings, as hinted earlier in April:\n\nSo the gist of the solution is adding a just-in-time (JIT) compiler to export your model to run on a Caffe2-based C++-only runtime. This compiler has two modes:\n\nThe naming is still subject to change. The 1.0 version is expected to be released this summer."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/tpu-listing-embeddings-pyception-1b3b014c1012?source=---------1",
        "title": "TPU, Listing Embeddings, Pyception \u2013 The Artificial Impostor \u2013",
        "text": "This is an experimental series in which I briefly introduce the interesting data science stuffs I read, watched, or listened to during the week. Please give this post some claps if you\u2019d like this series to be continued.\n\nIt\u2019s been a while since Google made TPU available on their cloud platform in beta in February. I\u2019m curious if there are already some people sharing their experience using TPU on the Internet. So I did some Googling\u2026\n\nThe article above then led me to\u2026\n\nSo it seems the released TPU (TPUv2) is a bit more cost effective then . However, the fact that TPU supports only mixed precision training may become an issue sometimes.\n\nThe downside is that there are a lot of hoops to jump through to be able to use TPU, according to the Paperspace blog post. And some of them are quite intimidating. Also, only Tensorflow supports TPU so far.\n\nThis new post by RiseML showed that TPU might be even more cost-effective than we thought, and the top-1 accuracy (on the validation set) is bit better coming from TPU than from GPU.\n\nThis post by Airbnb describes how they embeds every listing on their platform to improve similar listing recommendations and later real-time search personalization. Its is well-written and easy to read. The model evaluation parts are particularly interesting. The methodology should be applicable to other similarity problems, too.\n\n(I\u2019m not affiliated with the channel.) I\u2019ve found the links posted in this channel relevant and informative:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/weekly-reading-list-1-9d436aa94b6d?source=---------2",
        "title": "Chryons, Racist Machines, \u201cGender Gap\u201d, Getting Values from ML",
        "text": "This is an experimental series in which I briefly introduce the interesting data science stuffs I read, watched, or listened to during the week. Please give this post some claps if you\u2019d like this series to be continued.\n\nThis visualization uses data from the Third Eye Project (which captures chyrons via OCR). Chyrons between August 25, 2017, and January 21, 2018 were processed and transformed answer three questions:\n\nIt\u2019s really impressive. I especially appreciate how they have written the \u201cData and methodology\u201d section.\n\nSlides from a talk given by Renee M. P. Teate, creator of Becoming a Data Scientist blog and podcast, give an overview of the bias that could be hidden inside machine learning algorithms.\n\nThe presenter then discussed typical types of machine learning models (regression, classification, and clustering) and how things can become problematic. I find the \u201cCrime Forecasting Using Spatio-Temporal Pattern with Ensemble Learning\u201d example very intriguing for its subtleness.\n\nBasically, predictive model development involves a lot of decision making, and all these decision are made by human, thus the potential bias injection. The presenter gave quite a few examples of this kind of flawed process, including:\n\nSo a machine CAN indeed be racist or sexist. What can we do about it? I recommend reading the slides for answers (page 86\u201390). They are definitely worth your time.\n\nAn excellent book on this topic \u2014 Weapons of Math Destruction:\n\n\uff08The three key ingredients of weapons of math destruction: 1.Opacity 2. Scale 3.Damage.\uff09\n\n(For some reason I wasn\u2019t able to embed the link) A great piece from the Washington Post on the huge gaps of male and female populations in India and China.Not exactly a data science article, but I just love its interactive charts. Here are some sneak peeks:\n\nThis article argues that the main problem why machine learning haven\u2019t made as much impact in the business world as we\u2019d anticipated is not the model under-performing, but the difficulty of deployment.\n\nIt describes an \u201cAI project manager\u201d they built to predict red flags of ongoing projects. And they found the biggest requirements for this product were:\n\nFinally they propose a new machine learning paradigm, with key steps described in this paper, and supported by open-source tools."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/visualizing-air-quality-data-2ec16268711e?source=---------3",
        "title": "Visualizing Air Quality Data \u2013 The Artificial Impostor \u2013",
        "text": "I wrote a post on visualizing PM 2.5 concentration before. With KDD Cup 2018(KDD Cup of Fresh Air) coming up, I figure it is a good time to write an update version.\n\nThis will be a miscellaneous collection of visualization, external resources, and random thoughts(I wouldn\u2019t call them analysis), based on the KDD Cup dataset. Although the competition rule did not seem to explicitly forbid public sharing of its dataset, I\u2019d prefer to play it safe and ask you to download the dataset from its website if you want to run the code locally.\n\nThe dataset includes hourly observations from 35 stations in Beijing and 13 stations in London (and also some stations in London that are not included in the prediction set) spanning from Jan. 2017 to Mar. 2018. New data from Apr. is available via an official API.\n\nTake stations in Beijing as an example:\n\nI added some jitter using , so we can see the density of missing points more clearly. There are quite some missing data points across the board, with some station missing out an entire segment(like zhiwuyuan station in the last row).\n\nIt can be helpful to also include valid data points in the plot:\n\nThis is to make sure all data points are accounted for. Through an earlier version of this plot I was able to find that some segments like early Jul. 2017 and May. 2017 are missing for all stations. Therefore we have to put the timestamps in ourselves (this is important especially if you want to do sequence modeling later):\n\nThe missing data can make forecasting difficult because you don\u2019t have a reliable source of history. Ways to alleviate this problem includes cross-referencing data points from different stations, but it won\u2019t help when the entire segment is missing, in which case yearly correlation can not be used either.\n\nThis package dygraphs for R is extremely easy to use and create beautiful interactive charts. A huge chunk of the above code is dividing the chart into 6 regions according to the US EPA AQI standard. I was not able to follow the color designation exactly because the background shading must be light enough so the foreground is visible.\n\nThe center line is the mean concentration of the day, the shading covers 5 to 95 percentile of the day. The maximum 95 percentile is clipped to 500 as there are some outliers (way more than 500 ug/m\u00b3).\n\nI probably should have explicitly stated the unit of concentration in the chart.\n\nThis part is largely inspired by this repo (TheMoods/AirChina) to use the Python package folium to visualize geographical information.\n\nI\u2019m sure we can combine several snapshots and make an animation in leaflet.js (folium is a Python wrapper of leaflet), but I couldn\u2019t find a easy way to do it in folium. This brings us to the next section.\n\nThe background map is not essential in the animation (you only need to see it once), so why not just ditch it? Based on this idea, I used gganimate to visualize the dispersion of PM 2.5 particles:\n\nThe mechanism of gganimate is really simple. It creates one PNG file per frame, and combine them together to make a GIF file or an MP4 file using ffmpeg (the commented out part).\n\nWe can see some regional patterns going on in the video. So taking information from nearby stations can be helpful when forecasting.\n\nBe aware that the low frame rates might make some video player not able to play the resulting MP4 files properly. One of the solution is to tell ffmepg to artificially increase the frame rate, but I find this solution too slow and the larger size of the output unacceptable. Just find a video player that supports lower frame rates (obviously Youtube supports them at least).\n\nThe above is a very good overview on how to forecast PM 2.5 concentration, along with an awesome visualization. It tells us that one of the most important factors is the wind, whose forecast is part of the bigger weather forecasting problem. So this competition might actually boil down to an weather forecasting problem.\n\nIn The Signal and The Noise, Nate Silver dedicated an entire chapter to weather forecasting, referring it as one of the rare success stories of forecasting. Indeed, the forecasting and visualization of weather is widely available and recognized. And the forecasting of pollutants based on weather is also well-studied.\n\nAFAIK, most of these forecasts are using the large-scale simulation (ensemble forecasting) technique. It seems to me the main challenge of this KDD Cup is to match that technique with much less data (we don\u2019t have pollutant readings for nearby areas) and much less computing resources (no supercomputer cheese). We are allowed to use public external data, though. So you can grab the forecasts of wind speeds and directions, and based you forecast on them. But the model will be hard to validate because we don\u2019t have the forecast data in the past. All in all, it seems to be a very challenging problem to be tackled.\n\nPlease refer to the following Github repo for code used in this post:\n\nAnd here are some rendered pages:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/tensorflow-fashion-mnist-with-dataset-api-cce1e3cc8cd4?source=---------4",
        "title": "[Tensorflow] Fashion-MNIST with Dataset API \u2013 The Artificial Impostor \u2013",
        "text": "Fashion-MNIST intends to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It addresses the problem of MNIST being too easy for modern neural networks, along with some other issues.\n\nWe\u2019re going to continue using the models from Part 2(GRU) and Part 3(TCN), but replace MNIST with Fashion-MNIST using the Dataset API.\n\nPreviously we were looping through the MNIST data batches via class method provided by and feed the data to the graph via parameter in . By switching to the Dataset API, we get:\n\nThe two fundamental abstractions of the Dataset API are:\n\nThey are roughly analogous to and in PyTorch. Though batching, shuffling, parallelism configuration are done in DataLoader in PyTorch instead of in Dataset.\n\nThese are the main resources I used when researching for this post:\n\nInterestingly, in \u201cHow to use Dataset in Tensorflow\u201d, the author did not cover Feedable Iterator because he did not think it is useful. However, I found it is quite useful in our situation where we need to evaluate the validation set once every few hundred steps. So this post could be used to fill in the missing part of that post.\n\nNow comes the real deal. As always, the code is hosted on Google Colab:\n\nWe use the CSV files from Kaggle Dataset. To download it to the Google Colab environment, I used to download from a Google Cloud Storage bucket I created (you have to create your own to run). If you don\u2019t have Google Cloud access, I suggest uploading from your local filesystem.\n\nBecause this is a small dataset, we can safely read everything into memory:\n\nAnd choose 10,000 images randomly as the validation set (note that the test set also has 10,000 images):\n\nFirst of all, we group all dataset-related definition into one scope:\n\nSo they are displayed nicely as one block in Tensorboard:\n\nNext we use to define configurable batch sizes (or you can use fixed batch sizes as in the comment):\n\nThen we directly create the dataset from the Pandas data frames (the back-end Numpy arrays, to be precise):\n\nAgain, this is because this dataset is very small. For medium-size datasets, you might want to use to create datasets with Numpy arrays. For bigger datasets, you\u2019ll have to use or .\n\nThe training set is shuffled randomly at each iteration/step. Set the buffer size to be larger or equal to the size of the dataset to make sure it is completely shuffled.\n\nWe use to make the dataset repeat indefinitely. We\u2019ll control how many iterations/steps we need outside of the graph. To make it iterate only for N epochs, use and use to detect the depletion of data.\n\nWe use function to transform the imported tensors. The first transformation to reshape the feature from (batch_size, length) to (batch_size, length, 1). The second transformation performs (one-hot encoding) on the target labels. Note that both transformation were Tensorflow functions (starts with ), as recommended by the official documentation. If you want to do transformation that depends on third-party libraries (e.g. OpenCV), you need to use to wrap the call.\n\nIt is pretty much the same as in the documentation:\n\nTo use this iterator in a session, we need to initialize the base iterators first. This also set the batch sizes for each iterator:\n\nThen tell Tensorflow which iterator you want to use when training or testing:\n\nThis final step connect the dataset to the rest of the graph:\n\nAnd we\u2019re done! The model is ready to be trained.\n\nHere\u2019s a trick to track a metric in both training and validating stages in one plot \u2014 Creating two instances that write to two different sub-folders:\n\nAnd use the same metric name for both writers:\n\n(I keep an exponential moving average of the training loss in the graph. It is a leftover from my experiments with Estimator API. Spoiler: I don\u2019t like that API.) The latter part shows you how to add values outside of the graph to the Tensorboard.\n\nThen you\u2019ll have both curve in one plot:\n\nWe can also compare curves from different runs. For example, we can see that permuted sequential Fashion-MNIST is harder from the following plot:\n\nSample results(accuracies) of the CudnnGRU models taken from the notebook:\n\nSample results(accuracies) of the TCN models taken from the notebook:\n\nGenerally TCN still performs better than GRU. But bear in mind that these models are not really tuned, so there might be some rooms for improvement. As suggested by the submitted benchmarks in the project README, adding dropouts to the GRU is likely to help with the accuracy. You can also explore more benchmarks with scikit-learn models here:\n\nThank you very much for reading. This is the last part of this series and the end of my Tensorflow crash course. There are still some missing pieces of the puzzle, e.g. higher level training APIs other than Keras. I played with Estimator and Experiment APIs a bit and found them really restricting. I\u2019d rather write my own training process. For more layer abstractions and data manipulation helpers, TensorLayer seems to be a good Tensorflow medium-level library that is quite popular. I recommend you to quickly browse through their official examples to see if it fits your needs."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7?source=---------5",
        "title": "[Tensorflow] Implementing Temporal Convolutional Networks",
        "text": "In this post, we\u2019ll learn how to write models with customized building blocks by implementing TCNs using tf.layers APIs.\n\nThe authors released the source code in PyTorch, which is well-written and easy to incorporate into your own projects. You can skip all the Tensorflow parts below and use their implementation instead if you just want to use TCNs with PyTorch.\n\nThe term \u201c Temporal Convolutional Networks \u201d (TCNs) is a vague term that could represent a wide range of network architectures. In this post it is pointed specifically to one family of architectures proposed in the paper An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling :\n\nThe most important component of TCNs is dilated causal convolution. \u201cCausal\u201d simply means a filter at time step t can only see inputs that are no later than t. Dilated convolution is well explained in this blog post. The point of using dilated convolution is to achieve larger receptive field with fewer parameters and fewer layers. (I also mentioned dilated causal convolution in the writeup of the Instacart competition).\n\nA residual block stacks two dilated causal convolution layers together, and the results from the final convolution are added back to the inputs to obtain the outputs of the block. If the width(number of channels) of the inputs and the width(number of filters) of the second dilated causal convolution layers differs, we\u2019ll have to apply an 1D convolution to the inputs before the adding the convolution outputs to match the widths.\n\nWhat TCNs do is simply stacking a number of residual blocks together to get the receptive field that we desire. If the receptive field is larger or equal to the maximum length of any sequences, the results of a TCN will be semantically equivalent to the results of a RNN.\n\nIt\u2019s important to know how to calculate the receptive field because you\u2019ll need it to determine how many layers of residual blocks you need in the model.\n\nHere we denote the number of previous time steps(history) the ith a dilated causal convolution layer can see as .\n\nFor layer 0 (a imagined convolution as the base case), F(0) = 1, as a causal convolution can always see its current time steps it\u2019s at.\n\nFor layer 1, F(1) = F(0) + 2 * [kernel_size(n)-1] * dilation(n). It can see what the previous layer can see plus the position of the last kernel minus the position of the first. We can verify this using Figure 1 \u2014 F(1) = 1 + (3\u20131) * 1 = 3.\n\nYou should be able to see the pattern now. Generally, F(n) = F(n-1) + [kernel_size(n)-1] * dilation(n), where n means we\u2019re at the nth dilated causal convolution layer since the input layer. Since every residual block has two identical dilated causal convolutions (same kernel sizes and dilations), we could simplifies the formula to F\u2019(n) = F\u2019(n-1) + 2 * [kernel_size(n)-1] * dilation(n), but n now means we are at the nth residual block.\n\nIf the kernel size is fixed, and the dilation of each residual block increases exponentially by 2, i.e. dilation(n) = 2^(n-1), we can expand the formula as F\u2019(n) = 1 + 2 * (kernel_size-1) * (1 + 2 + 2\u00b2 + \u2026 + 2^(n-1)) = . Verify using Figure 1c \u2014 1+2*(3\u20131)*(2\u00b9-1)=5. You could verify the result with more residual blocks yourself.\n\nSo there it is, with a fixed kernel size and exponentially increasing dilations, TCN with n residual blocks will have a receptive field of \n\n at the final block. It most likely won\u2019t match your maximum sequence length exactly, so you\u2019ll have to decide to add one more block to make it larger than the maximum length, or sacrifice some of the older history.\n\nAs before, the notebook with the source code use in the post is uploaded to Google Colab:\n\nWe\u2019re going to use the module to provide high-level abstraction for the implemented TCNs. The base layer class is the foundation of all other layers in the module. The official documentation recommends descendants to this class implements the following three methods:\n\nWhen in doubt, try to read the source code of a built-in layer and imitate what it does in those methods.\n\nIt\u2019s quite simple to implement this since already supports dilation through the parameter. What we need to do is to pad the start of the sequence with (kernel_size-1) * dilation zeros ourselves, and pass (basically means no padding) to the parent . The padding will make the first output element only able to see the first input element (and the padding zeros).\n\nBecause of the restriction from other layers, only support channels_last data format, i.e. input shape is always (batch_size, length, channels). It use to pad the input tensor. Most of the lines are just capturing the initialization parameters of .\n\nBesides dilated causal convolution, we still need weight normalization, dropout, and the optional 1x1 Conv to complete the residual block.\n\nI did not find an easy way to implement weight normalization in Tensorflow, so I replaced it with (layer normalization). They won\u2019t be the same, but should have similar effects in stabilizing training. The layer normalization implementation basically assumes the channels are located at the last dimension of the input tensor, so the whole stack needs to use channels_last data format.\n\nIn the dropout section, we randomly drop out some of the channels across all time steps (a.k.a spatial dropout). layer has a parameter that does exactly that. By setting the to (batch_size, 1, channels), we select some channels for each example and set the dropout mask. Then the mask is broadcast to all time steps. (Check the notebook for a simple example.)\n\nIn the following implementation is set to (1, 1, channels) to allow dynamic batch sizes. This will slow down convergence. If you want dynamic batch sizes with different masks for each example, you\u2019ll have to override the method to generate dynamically.\n\nFinally, the 1x1 convolution can easily be achieved with a layer (it creates a projection at the last dimension):\n\nThe naming of the class follows the PyTorch implementation. Two dropout layers was created instead of one (same applies to layer normalization) simply to make Tensorboard create a cleaner graph visualization:\n\nAll that is left to do is to stack residual blocks together and create dilations exponentially:\n\nNote we can name each block manually with the parameter, which will be shown in the Tensorboard:\n\nI haven\u2019t figured out how to properly write unit tests against Tensorflow layers, but it should be a hard requirement if you want to use this implementation on real-world datasets.\n\nSome differences comparing to the previous RNN models:\n\nWe set kernel size to be 8 and number of stacked blocks to be 6, so the receptive field will be 1 + 2 * (8\u20131) * (2\u2076-1) = 883, a bit larger than the maximum sequence length 784.\n\nYou can see in the notebook that a TCN with ~ 36K parameters converged faster and had better test accuracy than RNN from the previous notebook.\n\nWe\u2019ve been using the test set in the training process to pick the final model, which is a very bad practice. It makes the results from the two notebooks so far somewhat unreliable. In the next and probably the final part of this series, we\u2019ll learn how to import the Fashion-MNIST dataset and create a proper validation set to evaluate our models."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5?source=---------6",
        "title": "[Tensorflow] Building RNN Models to Solve Sequential MNIST",
        "text": "In this post, we\u2019re going to lay some groundwork for the custom model which will be covered in the next post by familiarizing ourselves with using RNN models in Tensorflow to deal with the sequential MNIST problem. The basic framework of the code used in this post is based on the following two notebooks:\n\nI\u2019ve put the source code for this post in a notebook hosted on Google Colaboratory, which kindly provides a free GPU runtime for the public to use \uff08I kept getting disconnected to the runtime when running the notebook. So some of the model training was not completed. You can copy the notebook and run it yourself.\uff09:\n\nThe notebook should have done most of the talking. The following sections of this post will discuss some parts of the notebook in more detail, and also provide some additional information that was left out in the notebook.\n\nEvery example from the MNIST dataset is a 28x28 image. We are going to apply recurrent neural network on it in two ways:\n\nThe pixel-by-pixel case is a lot harder because a decent model has to keep a very long-term memory.\n\nWe\u2019re going to build four models (two models for each case):\n\nWe\u2019re jumping directly to the second model, which is different from the first model in the following ways:\n\nI\u2019m going to discuss some of them in the following sections.\n\nThis Tensorflow LSTM benchmark is very comprehensive:\n\nTensorflow has a nice wrapper that does variational dropout for you:\n\nThat\u2019s probably the main reason why you sometimes want to use LSTMBlockCell instead of CudnnLSTM. For sequential MNIST the problem of overfitting is relatively low, so we did not use any dropouts in the notebook.\n\nI feel the difference between and is somewhat vague in the documentation. These two discussion threads (stackoverflow and github) cleared things up a bit for me. The main difference seems to be that supports dynamic maximum sequence length in batch level, while doesn\u2019t. From what I\u2019ve read, there seems to be little reason not to always use .\n\nYou simply supply the whole batch of input data as a tensor to instead of slicing them into a list of tensor (sequences). This is easier to write and read than :\n\nIn the first model, you have to define the weight and the bias for the linear (output) layer manually:\n\nAnd calculate the output logits by doing a matrix multiplication and an addition:\n\nAlbeit very good for educational purpose, you probably don\u2019t want to do it every time you need a linear layer. The abstraction provided by provides similar experience to layer in PyTorch:\n\nYou can also use the shortcut function like I just did with :\n\nRMSProp speeds up the convergence, and gradient clipping helps dealing with the exploding gradient problem of RNNs.\n\nThe row-by-row only involves 28 time steps, and is fairly easy to solve with a wide range of hyper-parameters (initialization methods, number of hidden units, learning rate, etc.). The pixel-by-pixel MNIST with 784 time steps is a lot harder to crack. Unfortunately I could not find a set of hyper-parameters for a LSTM model that could guarantee converge. Instead, I\u2019ve found GRU models much easier to tune and succeed to reach 90%+ test accuracy in multiple cases.\n\nPyTorch uses CuDNN implementations of RNNs by default, and that\u2019s what makes it faster. We could also utilize those implementations in Tensorflow via :\n\nRNN classes from the module doesn\u2019t have a parameter, so the input shape is always (length, batch_size, channels). Moreover, if you want to get the most speed, let run through the whole sequence in a single command (as the code above did) instead of feeding it step-by-step. It seems to work similarly to , meaning the maximum length is allow to differ between batches.\n\nGrouping variables and operations using tf.variable_scope brought us this modularized graph in Tensorboard:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-1-5f0ebb253ad4?source=---------7",
        "title": "[Tensorflow] Core Concepts and Common Confusions \u2013 The Artificial Impostor \u2013",
        "text": "This post will be written from my personal perspective, so the readers should already have some basic idea about what deep learning is, and preferably are familiar with PyTorch.\n\nI\u2019ve been considering picking up Tensorflow for a while, and have finally decided to do it. The main appeal of learning Tensorflow includes (compared to using PyTorch exclusively. Keras as a high-level library is not really comparable.):\n\nTensorflow started with only the kernel and low-level APIs, but has accumulated a lot of modules and become a behemoth. Official documentation recommends using Estimators and Datasets, but I personally chose to start from Layers APIs and low-level APIs to have the kind of access similar to ones in PyTorch, and work my way up to Estimators and Datasets. There will be more posts coming up on these topics.\n\nI find the complexity of its stack one of Tensorflow\u2019s main turn-offs. The vast of amount of modules to choose from can also overwhelms the beginners. It\u2019ll become a lot easier when you finally find a set of modules you can work with most comfortably.\n\nThis is the one of the most important concept for those who come from PyTorch. Pytorch create a dynamic computational graph on the fly to do automatic differentiation. Tensorflow, on the other hand, requires you to define the graph first. I\n\nFor Tensorflow, it\u2019s like building a systems of pipes first(a graph), pumping water into it and receiving the processed water in the other end (session.run). Pytorch allows you to pump water into the system while you are building it, so it\u2019s easier to find any sub-units that malfunctioned (debugging).\n\nThere are some upsides to static graphs, despite the obvious downsides. Because it is static, Tensorflow can infer some parameters like input sizes for you when compiling the graph. The trained model will also be more portable to other platforms.\n\nUsually you only need one graph. Tensorflow implicitly defines a default graph for you, but I prefer to explicitly define it and group all graph definition in a context:\n\nThis method is basically the whole point of creating a session. It starts the data flow in the graph (pumps water into a piping system). The two most important parameters are fetches(outputs) and feeds(inputs).\n\nBy passing a list of nodes (can be operations, tensors, or variables) as fetches, you tell Session.run you want the data to flow to the given nodes. Session.run will close off all the subgraphs that are not required to reach those nodes, hence saves execution time.\n\nBy passing a map from values to tensors as a dictionary of feeds, you tell Session.run to fill the tensors with the given values when running the graph. This is how you input information to the graph. (You can also use variables instead of tensors in feeds, although it\u2019s not very common.)\n\nIn the following example from the official tutorial, y is passed as the sole element of fetches, and values to placeholder x is passed in a dictionary:\n\nIn PyTorch, a variable is part of the automatic differentiation module and a wrapper around a tensor. It represents a node in the computation graph, and stores its parent in the graph and optionally its gradients. So basically all tensors in the graph are variables.\n\nA variable in Tensorflow is also a wrapper around a tensor, but has a different meaning. A variable contains a tensor that is persistent and changeable across different Session.runs. So they are usually the ones that are updated in back-propagations (e.g. the weights of a model), and also any other states we want to keep between different runs. Also, all variables need to be initialized (usually through an operation) before they can be used.\n\nVariables does not persist after a session being closed. You have to remember saving those variables before closing the session. (The official documentation mentioned modifications to variables are visible across multiple sessions, but that seems only apply to concurrent session running on multiple workers.)\n\nTo save variables/weights in Tensorflow usually involves serializing all variable into a file. While PyTorch relies on the state_dict method to extract Parameters (a subclass of Variable) and persistent buffers to be serialized.\n\nSaving models in Tensorflow involves defining a Saver in the graph definition and invoking the save method in a session:\n\nThere\u2019s also a SavedModel class that not only saves variables, but also the graph and the metadata of the graph for you. It is useful when you want to export your model.\n\nPyTorch names the parameters in a quite Pythonic way:\n\nTensorflow, however, uses namespaces to organize tensors/variables and operations. Tensorboard group operations according namespaces they belong to, and generate a nice visual representation of the graph for you:\n\nAn example of tensor name is . is the actual name of the tensor. The suffix is the endpoint used to give the tensors returned from an operation unique identifiers, i.e. is the first tensor, is the second and so on.\n\nSome medium or high level APIs like tf.layers will handles some of the scope naming for you. But I\u2019ve found it not smart enough sometime, so I had to do it manually. And it brings me to the question \u2014 which one to use, or ?\n\nThis stackoverflow answer gave a brilliant explanation to the differences between these two, as illustrated in the following graph:\n\nTurns out there is only one difference \u2014 affects , while doesn\u2019t. also has a parameter , which allow you to reuse the same variable (with the same name in the same namespace) in different part of the code without having to pass a reference to that variable around.\n\nHere\u2019s an example of mixing and from the official documentation:\n\nIMO, usually you\u2019d want to use unless there is a need to put operations and variables in different levels of namespaces.\n\nThat\u2019s pretty much it! In this post we have covered 4 topics that I\u2019ve found most confusing to beginners:\n\nNow we can go on and write some actual Tensorflow code. In the next two or three posts I\u2019ll share some of the exercises I set for myself, using low-to-medium level APIs. After that maybe I\u2019ll do a piece on how to integrate higher level APIs like Estimators and Datasets, so we can comfortably move between different levels of APIs to suit our requirements."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/review-kaggle-toxic-comment-classification-challenge-part-1-934447339309?source=---------8",
        "title": "[Review] Kaggle Toxic Comment Classification Challenge \u2014 Part 1",
        "text": "Public kernel blends performed well in this competition (i.e. did not over-fit the public leaderboard too much). I expected it to overfit, but still selected one final submission that used the best public blend of blends to play it safe. Fortunately it paid off and gave me a 0.0001 boost in AUC on private leaderboard:\n\nI tried a few ideas after building up my PyTorch pipeline but did not find any innovative approach that looks promising. Text normalization is the only strategy I had found to give solid improvements, but it is very time consuming. The final result (105th place~3%) was quite fitting IMO given the time I spent on this competition(not a lot).\n\nToxic comment classification challenge features a multi-label text classification problem with a highly imbalanced dataset. The test set used originally was revealed to be already public on the Internet, so a new dataset was released mid-competition, and the evaluation metric was c hanged from Log Loss to AUC .\n\nIn this post, I\u2019ll review some of the techniques used and shared by top competitors. I do not have enough time to test every one of them myself. Part 2 of this series will be implementing a top 5 solution by my own, if I ever find time to do it.\n\nI tired to attribute techniques to all appropriate sources, but I\u2019m sure I\u2019ve missed some sources here and there. Not all techniques are covered because of the vast amount of contents shared by generous Kagglers. I might come back and edit this list in the near future.\n\nI\u2019ve already tried these two techniques and trained a couple of models for each.\n\nHead-tail truncating (keeping 250 tokens at head, 50 tokens at tail) helped only a bit for bi-GRU, but not for QRNN. It basically had no effect on my final ensemble.\n\nFor pseudo-labelling(PL), I used the test-set predictions from my best ensemble as suggested in [1], and they improved the final ensemble a little (see table 1). I\u2019d assume that adding more model trained with PL will further boost the final AUC. However, the problem of this approach is the leakage it produces. The ensemble model had seen the the all the validation data, and that information leaked into its test set predictions. So the local CV will be distorted and not comparable to those trained without PL. Nonetheless, this technique does create the best single model, so it\u2019ll be quite useful for production deployment.\n\nI think the more conservative way of doing PL is to repeat the train-predict-train(with PL) process, so the model is trained twice for every fold. But that\u2019ll definitely takes more time."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/march-madness-predictions-using-pymc3-e64574497f47?source=---------9",
        "title": "March Madness Predictions using PyMC3 \u2013 The Artificial Impostor \u2013",
        "text": "This post describes my journey from exploring the model from Predicting March Madness Winners with Bayesian Statistics in PYMC3! by Barnes Analytics to developing a much simpler linear model.\n\nTaken from the official documentation:\n\nPyMC3 is Python-native, so I personally find it easier to use Stan. It is based on Theano, whose development has unfortunately stopped. The work to replace Theano seems to be ongoing. There have also been some new competitors closing in, e.g. Edward(based on Tensorflow) and Pyro(based on PyTorch).\n\nAs mentioned in the beginning of the post, this model is heavily based on the post by Barnes Analytics. The model seems to originate from the work of Baio and Blangiardo (in predicting footbal/soccer results), and implemented by Daniel Weitzenfeld. There is also an example in the official PyMC3 documentation that uses the same model to predict Rugby results.\n\nThe model decompose everything that influences the results of a game into five factors:\n\nAnd it use a Possion regression (belongs to the generalized linear regression family) to model the relationship between these factors. It can be easily formalized by the following formulae from Daniel Weitzenfeld\u2019s post:\n\nI\u2019m not going to spend time explaining how to do data preprocessing here. Please check the Barnes Analytics post or this Kaggle Kernel. I\u2019ve made some changes to the Barnes Analytics code. Besides some small code tweaks, there are two bigger changes:\n\nOriginally the model can only fit data from only one season (in the following case, the 2017 season):\n\n(The operations starts with \u201ctt.\u201d are Theano tensor operations. ) Note the model implements a sum-to-zero constraint by subtracting the global mean. Otherwise we can shift all offensive power by z , all defensive power by -z and still get the same model.\n\nThe simplest way to fit more than one season in one run is to assign separate offensive and defensive powers to each team for each season, and assume there are no correlations between seasons (You can also simply loop through each season and fit one model per season under this assumption.):\n\nIf we want to model the correlations between seasons, one way to do it is to base the score/power of one team in this season on the last season. For example, we can make a team\u2019s score in season 2 the sum of its score in season 1 and a white noise. There\u2019s a GaussianRandomWalk class in PyMC3 that does exactly this. However, I somehow could not make GaussianRandomWalk to work with multiple series (teams). So instead I manually defined scores season by season and linked them together. The results from this model are not better than assuming independence between seasons, so I\u2019ll skip the code for this one.\n\nThe code inside the loop takes a set of samples from the posterior distributions, calculates the corresponding thetas, sample from the resulting Poisson distributions, and see if the home team has a higher score than the away team. The code is repeated sample_size times and the winning ratio of the home team is our predicted winning probability.\n\nThe code can be vectorized to utilize the optimized code of Numpy and Theano:\n\nThe vectorized function also returns the 5, 50, and 95 percentiles of the simulated home_score and away_score for later model diagnostics.\n\nThe speedup (tested on a single season model, but should make no difference anyway):\n\nIn fact, if we take the percentile calculation away, the vectorized function can achieve a 1000x speedup (~2.6ms) instead of a 100x speedup.\n\nTake this Kaggle kernel/notebook as an example. The model was fitted to the results of regular season matches:\n\nNow we use the tournaments in March to test our predictions:\n\n(Correction: it\u2019s really a 90% confidence interval instead of 95% in the screenshot) Only around 85% of the time the actual score fell into the 90% confidence interval, so the model is far from ideal.\n\nAs for accuracy, overall accuracy is 67.54% (the percentage where the model predicted the winner correctly), and for 2017 season the accuracy is 67.16%. The overall and 2017 logloss is 0.5388 and 0.5654, respectively.\n\nOne of my favorite things to do is to take a problem and try to find the simplest model that can solve the problem to a reasonable degree. For this prediction problem, I want to assign only one score per team, use the difference of the scores between two teams to predict the difference of their final points in a game. So there are down to only three factors influencing the results of a match:\n\nThe relationship between the difference of the final points and the factors is entirely linear, i.e. . I know, it seems to incredibly over-simplify things, but does it?\n\n(Although the differences of the final score are discrete, using a Gaussian distribution to approximate it should not be a big problem) Extremely simple code. There are two random variables set as the standard deviations, the (latent) variables for (season, team), and one random variable for the home court advantage.\n\nTake this Kaggle kernel/notebook as an example:\n\nBecause this is a linear model, the results are very interpretable. Home team get an advantage of around 3.4 points in average, which is not a lot, but already requires two field goals to counter. The standard deviation of team scores is around 8.6, and the one of point differences is around 10.35. They are both quite large, which means the uncertainty of the predictions are high.\n\nAs for accuracy, overall accuracy is 69.78% (the percentage where the model predicted the winner correctly), and for 2017 season the accuracy is 70.15%. The overall and 2017 logloss is 0.5007 and 0.5349, respectively.\n\nSurprisingly, the second model performs better than the first model. In additional to the 2%+ improvement in accuracy, the better logloss values also indicate a better quality in probability predictions.\n\nAdmittedly, 70% accuracy is still nothing to be proud of, but we can build more sophisticated models based on it by adding more features or data. For example, we can add an adjustment term to account for the fact that some teams play better against better teams, while only mediocre against equal or worse teams. It\u2019s also common sense that basket ball cannot be truly linear. Even if team A beats team B by 10 points and team B beats team C by 10 points, team A beating team C by 20 points is definitely not a sure thing (It\u2019d probably often be less than that). How to better model that dynamics is for you to find out.\n\nThe first model (offensive and defensive decomposition):\n\nThe second model (single score per team):\n\nThis visualization tool written by Mark McClure is really good. The quality of your prediction is shown in colors, and hovering over a match will show the probability you predicted:\n\nYou have to follow Kaggle\u2019s submission format, though. Check it here and here."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/note-talent-vs-luck-the-role-of-randomness-in-success-and-failure-edc97896f0c2",
        "title": "Talent vs Luck: the role of randomness in success and failure",
        "text": "There are some discussions on the Internet about this paper which captured my attention recently:\n\nIt turns out there are already at least two write-up by well-known media outlets:\n\nThere are also two Hacker News threads on this topic: [1], [2].\n\nBasically the authors, by running simulations based on a model, argued in the paper that luck may play a bigger role in an individual\u2019s success than we usually think. The key observation is that while talents are normally distributed (which arguably is a big assumption), the wealth distribution in the real world typically follows a power law. The authors claimed that their model successfully explains this gap between the two distributions, and then provided some directions to \u201cimprove meritocracy\u201d based on some simulation results.\n\nI\u2019ll admit that this kind of arguments is pretty much what I\u2019d like to hear. They usually lead to solutions like \u201cLet\u2019s tax the rich more to give the unlucky ones more chances to success, since the rich probably don\u2019t deserve most of their wealth anyway\u201d, etc., etc. But honestly, as a layman, I\u2019m not very convinced by this paper. Evidences provided by this paper to support the model seems flimsy to me. The only connection between the real-world and the model is that the results from simulation follows the 80:20 rule (and also roughly a power law). Even if we ignore the lack of evidence, a model that fits the data well does not necessarily generalize well. In my opinion, the paper need to try harder on justifying the model.\n\nThe paper chose to leave out some model details, but similar results is not very hard to reproduce. I tried to simplify the model by removing the square world, and the results are still roughly in line with the ones in the paper, as briefly explained below.\n\nThe model proposed in the paper randomly put people and \u201cevents\u201d in a square world. These \u201cevents\u201d move randomly at each time step. When an event intercepts the position of a person, the capital of the person can be doubled or halved depending on the type of the event and the person\u2019s talent.\n\nThe paper did not specify the coordinate system, and how a event would act if if it moves beyond the boundary of the square world. So the actual probabilities at any time step of an event intercepting a person is unknown.\n\n20180316 Update: For those who are interested in trying the original model in a square world, the paper stated they used NetLogo to conduct the agent-based experiments.\n\nI removed the square world and simply assigned a global probability of event happening at any time step. This gives an absolute level playing field for every person, and it\u2019s not so in a square world. Once the people and events are placed at time zero in a square world, there are gonna be some regions where the number of \u201clucky\u201d events is higher than the \u201cunlucky\u201d one, and people in this region of the world will tend to be a bit luckier than the others (because of the nature of random walks). It helps model some inequality of opportunities.\n\nWe\u2019ll run the simulation in R. First set up the parameters:\n\nLet\u2019s see the result of a single run:\n\nIn this run, the top 20% rich hold 80.19711% of the wealth. The 80:20 rule is not automatically true. I had to tune (probability of an event occurring) to match it. Turns out quite robustly produces results that follows this rule. That means a person will in average encounter 6 events in his working life.\n\n10.3% of people did better than when they started (having more than 10 units of capital at the end). For those with talent > 0.7, 13.89% did better. So talent still plays a role, but not much.\n\nIn this simplified model, the talented people have a larger chance of becoming insanely rich than in the square world. The simulation run shown above is one of the more unfortunate ones (for the talented). The reason is because in the simplified model, every people get the exactly same probability of good event, so there are fewer chances where an ordinarily talented get incredibly lucky.\n\nAs you can see from the plot, the capital distribution definitely not emulate the real-world wealth distribution every well. This is because of the artificial \u201cdouble/half\u201d capital fluctuation rule. If we further complicate the model to make it match the real-world data better, whether the underlying dynamic will be the same still remains to be seen.\n\n20180403 Update: This post did a very impressive job reviewing the paper and playing with more models based on the original one. I recommend anyone who find my post interesting to read it:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/analyzing-tweets-with-r-92ff2ef990c6",
        "title": "Analyzing Tweets with R \u2013 The Artificial Impostor \u2013",
        "text": "NLP(Natural-language processing) is hard, partly because human is hard to understand. We need good tools to help us analyze texts. Even if the texts are eventually fed into a black box model, doing exploratory analysis is very likely to help you get a better model.\n\nI\u2019ve heard great things about a R package tidytext and recently decided to give it a try. The package authors also wrote a book about it and kindly released it online:\n\nAs the name suggests, tidytext aims to provide text processing capability in the tidyverse ecosystem. I was more familiar with data.table and its way of data manipulation, and the way tidyverse handles data had always seemed tedious to me. But after working through the book, I\u2019ve found the syntax of tidyverse very elegant and intuitive. I love it! All you need is some good examples to help you learn the ropes.\n\nChapter 7 of the book provides a case study comparing tweet archives of the two authors. Since twitter only allows downloading the user\u2019s own archive, it is hard for a reader without friends (i.e. me) to follow. So I found a way to download tweets of public figures and I\u2019d like to share with you how to do it. This post also presents an example comparing tweets from Donald Trump and Barack Obama. The work flow is exactly the same as in the book.\n\nWarning: The content of this post may seem very elementary to professionals.\n\nFirst of all, follow the instruction of this article to obtain your own API key and access token, and install package:\n\nYou need these four variables:\n\nThe main access point for this post is . It downloads at most 3200 recent tweets of a public twitter user. The default parameter seems to remove a lot of false positives, so we\u2019ll instead do it manually later.\n\nNow we have tweets from @realDonaldTrump, @BarackObama and @POTUS44 as List objects. We\u2019ll now convert them to data frames:\n\nNow we plot the time distribution of the tweets:\n\nYou could remove to have compare the absolute amount of tweets instead of relative:\n\n(The lack of activity of @realDonaldTrump is from the 3200-tweet restriction) We can see that as a president, Donald Trump tweets a lot more than Barack Obama did.\n\nFrom this point we\u2019ll enter the world of tidyverse:\n\n(It\u2019s worth noting that the pattern used in are for matching separators, not tokens.) Now we have the data in one-token-per-row tidy text format. We can use it to do some counting:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/review-kaggle-corporaci%C3%B3n-favorita-grocery-sales-forecasting-part-ii-680cca7f9bc5",
        "title": "[Review] Kaggle Corporaci\u00f3n Favorita Grocery Sales Forecasting \u2014 Part II",
        "text": "The following table contains the results which were obtained from the \u201cLate Submission\u201d feature of Kaggle. The numbers in the model columns are the ensemble weights and the actual number of trained models involved (in the parenthesis). The \u201ccomp\u201d under in v12 LGB column means the predictions from the v12 LGB models (3-model average) were used only for (store, item) combinations with no sales in recent 56 days. The local CV scores are not comparable between v13 and v14, and does not include the scores for (store, item) combination covered by v12 LGB models.\n\nThe local CV scores of v13 models seems to be consistent with leaderboard scores, so I\u2019d say my validation is good enough. (The CV scores of v14 are only for reference. They should not be relied upon.)\n\nThe final row represents the ensemble I\u2019d mostly likely to have picked in the competition, to hedge the risk of v14 blind training being a huge mistake. Its private score of .513 means I probably wouldn\u2019t get top 3 results even if I was given more time and didn\u2019t place that bad bet. So there\u2019s that.\n\nThe training time of my models became too long for v13 and v14 because of the 56-day filter, so I only trained 2 models with different seeds for each hyper-parameter setting . I actually included dozens of models and around 10 types of model variations in the final submission. We might be able to get even lower score if we use stronger bagging and include more variations (some of them I\u2019ll describe in the following sections), but I am not interested in using more computing resources to find out.\n\nMany of the the features and model structures were inspired by:\n\nThere are three types of features in my models:\n\nInteger features 4 to 14 are converted to vectors by entity embedding.\n\nThe alignment of year 1 and year 2 is tricky. To predict the sales at time t, we want the sales at time t-1, all other year 2 features at time t, and year 1 features at time t-364. So every year 2 sales features are shifted 1 day to the left. It\u2019s important to remember that when calculating correlation coefficients and shift the year 2 sales series back.\n\nThe derived features are calculated on the fly in a Dataset method. The other two types of features are written to numpy memmap files on the disk and will be read by the Dataset instance when needed (this saves a tremendous amount of memory).\n\nNot all features are always used. Sometimes some features are dropped when training a model to increase overall variations. A dropout of 0.25 is also applied along the embedding dimension.\n\nThe float series are normalized by subtracting their (series-wise) means, and then divided by their corresponding constant numbers, which are the standard deviations of all the residuals (after subtracting means) for that type of series, e.g. year 2 (store, item) sales, in the training data. The normalized values are clipped by (-3, 3) to reduce the influence of outliers.\n\nFor example, this is the distribution of the log-transformed float series 1:\n\nAfter normalization, it becomes:\n\nFor model structure 3 and 4, LSTM, GRU, SRU[3], QRNN[4] are all available. I\u2019ve found SRU and QRNN can obtain quite good validation loss with less training time, so they are heavily used when doing feature selection. The training time reduction is not as much as the papers reported, though. It might has something to do with my code not optimized enough.\n\nThe model 3(a) with scheduled sampling actually has the best local CV scores, but it takes very long to train, and decay schedule is very hard to tune. Hence I did not include it in the post-competition models.\n\nAs mentioned in Part I, I use ReduceLROnPlateau to schedule learning rate. For model structure 1 and 4(CNN) Adam optimizer is used. RMSProp optimizer is used for the rest.\n\nI tried Yellowfin after reading its paper[7] and thought it was really promising. However its behavior was really strange. I had to do a lot of hand-tuning to make it almost on par with RMSProp, so it was eventually dropped. Recently I tried the official PTB example with both its PyTorch and Tensorflow implementation, and found that the Yellowfin optimizer still underperformed compared to Adam. Not sure what the problem was (I used Python 2.7, Tensorflow 1.1 and PyTorch 0.2.0 as specified in the READMEs.).\n\nI found out that the same seed does not generated the same PyTorch model mid-competition, and spent quite some time trying to find out why. I became pretty sure that the non-deterministic behavior came from customized weight calculation for perishables. (I explicitly create a weight vector and multiply it with the loss vector before doing back-propagation, and keep the weight vector and the product vector to track the learning curve.) Tensorflow community has this discussion about the non-deterministic mean and sum reduction. I think PyTorch should have the similar problem. It really make sense because parallel summation needs to ensure the exact same workload split and reduce order to guarantee the same result (the problem of float point precision).\n\nI used the almost the same model structure on other dataset without non-uniform sample weight and the trained PyTorch models were perfectly reproducible.\n\nThere are some really bizarre series that are almost impossible to predict, for example:\n\nThe reason of the abrupt might be items out of supply or being removed from the shelf. We can only guess.\n\nI have not started preparing the code that is ready to be published yet\u2026.\n\nThere are some experimental code blocks that need to be removed. The model ensemble script currently involves hand-picking models, which should be automated instead. There are some works to be done.\n\nI\u2019ll update this post with a link to the Github repo, or write a Part III for that. We\u2019ll see then.\n\nUpdate on 2017-02-11: The incomplete solution on Github:"
    },
    {
        "url": "https://medium.com/the-artificial-impostor/sgd-implementation-in-pytorch-4115bcb9f02c",
        "title": "SGD implementation in PyTorch \u2013 The Artificial Impostor \u2013",
        "text": "PyTorch documentation has a note section for optimizer that says:\n\nThis is the formula the paper used (g being the gradient; v the velocity; p the parameters/weights; rho the momentum coefficient; lr the learning rate):\n\nAnd this is the formula PyTorch used:\n\nThe only difference is where the learning rate is applied. In the paper the learning rate is applied when calculating new velocity, and in PyTorch the learning rate is applied when calculating new parameters/weights.\n\nIt may or may not have observable impacts on train and validation loss, but being aware of the difference can help guide the tuning schedule toward the right direction.\n\nFor example, if we\u2019re using schedule that reduce the learning rate once the validation score plateaus. If we misunderstood the PyTroch SGD implementation to be the one in the paper, we\u2019d expect gradients to have much less influence in later velocity updates; in other words, we\u2019d expect the momentum to increase. But in reality the momentum did not change. Instead, we were just getting smaller changes in parameters in each iteration.\n\nIf we substitute in the PyTorch formula, we get and . If we set we\u2019d get the same formula in the paper. So what PyTorch does is actually adjusting the momentum coefficient relative to the learning rate, so momentum stays invariant to changes in learning rate.\n\nIt\u2019s not hard to modify the SGD implementation in PyTorch and make it consistent with the paper (If that\u2019s what you want).\n\nIf we take a look at the source code, we\u2019d find it quite easy to read:\n\nLine 23 and 25 get the gradients. Line 30, 33, and 35 update the velocity. Line 39 updates the parameters.\n\nSo if we just tweak line 19 and apply the learning rate directly on the gradients (line 27) we\u2019d have implemented the formula in the paper:\n\nThe small difference in implementation might not be a big deal, but can cause you some confusion when tuning if you have not understood it correctly. Moreover, tuning algorithms that are based on the alternative formula may not work as expected in PyTorch. For example, in YellowFin paper[2] this is used, where learning rate and momentum coefficient are decoupled:\n\nYou\u2019ll need to be careful when implementing those algorithms in PyTorch. Otherwise a huge amount of time is likely to be wasted on debugging."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3",
        "title": "Feature Importance Measures for Tree Models \u2014 Part I",
        "text": "This post is inspired by a Kaggle kernel and its discussions [1]. I\u2019d like to do a brief review of common algorithms to measure feature importance with tree-based models. We can interpret the results to check intuition(no surprisingly important features), do feature selection, and guide the direction of feature engineering.\n\nHere\u2019s the list of measures we\u2019re going to cover with their associated models:\n\nNote that measure 2 and 3 are theoretically applicable to all tree-based models.\n\nAccording to [1], MDI counts the times a feature is used to split a node, weighted by the number of samples it splits:\n\nHowever, Gilles Louppe gave a different version in [4]. Instead of counting splits, the actual decrease in node impurity is summed and averaged across all trees. (weighted by the number of samples it splits).\n\nIn R package , the implementation seems to be consistent with what Gilles Louppe described [5] (another popular package, , also seems to be doing the same) [6]:\n\nThis is IMO most interesting measure, because it is based on experiments on out-of-bag(OOB) samples, via destroying the predictive power of a feature without changing its marginal distribution. Because scikit-learn doesn\u2019t implement this measure, people who only use Python may not even know it exists.\n\nHere\u2019s permutation importance described in the Element of Statistical Learning:\n\nFor other tree models without bagging mechanism (hence no OOB), we can create a separate validation set (apart from the test set) and use it to evaluate the decrease in accuracy.\n\nThis algorithm gave me an impression that it should be model-agnostic (can be applied on any classifier/regressors), but I\u2019ve not seen literatures discussing its theoretical and empirical implications on other models. The idea to use it on neural networks was briefly mentioned on the Internet. And the same source claimed the algorithm works well on SVM models [8].\n\nBoruta is the name of an R package that implements a novel feature selection algorithm. It randomly permutes variables like Permutation Importance does, but performs on all variables at the same time and concatenates the shuffled features with the original ones. The concatenated result is used to fit the model.\n\nDaniel Homola, who also wrote the Python version of Boruta(BorutaPy), gave an wonderful overview of the Boruta algorithm in his blog post [7]:\n\nThe shuffled features (a.k.a. shadow features) are basically noises with identical marginal distribution w.r.t the original feature. We count the times a variable performs better than the \u201cbest\u201d noise and calculate the confidence towards it being better than noise (the p-value) or not. Features which are confidently better are marked \u201cconfirmed\u201d, and those which are confidently on par with noises are marked \u201crejected\u201d. Then we remove those marked features and repeat the process until all features are marked or a certain number of iteration is reached.\n\nAlthough Boruta is a feature selection algorithm, we can use the order of confirmation/rejection as a way to rank the importance of features.\n\nFor Kagglers, this part should be familiar due to the extreme popularity of XGBoost and LightGBM. Both packages implement more of the same measures (XGBoost has one more):\n\nFirst measure is split-based and is very similar with the one given by [1] for Gini Importance. But it doesn\u2019t take the number of samples into account.\n\nThe second measure is gain-based. It\u2019s basically the same as the Gini Importance implemented in R packages and in scikit-learn with Gini impurity replaced by the objective used by the gradient boosting model.\n\nThe final measure, implemented exclusively in XGBoost, is counting the number of samples affected by the splits based on a feature.\n\nThe default measure of both XGBoost and LightGBM is the split-based one. I think this measure will be problematic if there are one or two feature with strong signals and a few features with weak signals. The model will exploit the strong features in the first few trees and use the rest of the features to improve on the residuals. The strong features will look not as important as they actually are. While setting lower learning rate and early stopping should alleviate the problem, also checking gain-based measure may be a good idea.\n\nNote that these measures are purely calculated using training data, so there\u2019s a chance that a split creates no improvement on the objective in the holdout set. This problem is more severe than in the random forest since gradient boosting models are more prone to over-fitting. It\u2019s also one of the reason why I think Permutation Importance is worth exploring.\n\nAs usual, I will demonstrate some results of these measures on actual datasets in the next part."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/docker-nvidia-gpu-nvidia-docker-808b23e1657",
        "title": "Docker + NVIDIA GPU = nvidia-docker \u2013 The Artificial Impostor \u2013",
        "text": "Update on 2018-02-10: nvidia-docker 2.0 has been released and 1.0 has been deprecated. Check the wiki for more info.\n\nDocker is a wonderful technology. In my opinion, every reasonably Python(what I am most familiar with) project that requires teamwork should have at least one Dockerfile, preferably with application-level configuration files(e.g. Docker Compose files). Life before Docker was full of the infamous \u201cIt works on my machine\u201c problems. I get to experience that nightmare again recently. When I asked someone from another team what packages are needed to run their code, he handed my a dump of output from his machine\u2026\n\nDocker virtualizes CPU natively. CPU resource should automatically available to you inside the container. You can even allocate CPU resource with parameters (e.g. ). Not so easy for GPU. GPU usually requires specialized(often proprietary) drivers to run inside the container.\n\nFor NVIDIA GPUs, one the early solutions is to fully install the driver inside the container. The problem of this solution is that the driver version inside the container must exactly match the driver version on the host machine. That means whenever I upgrade the driver on the host machine, I must rebuild every Docker images that uses GPU (not to mention it\u2019s not really straightforward to install driver inside containers). I gave up that solution very quickly and settled with miniconda to manage my deep learning packages. It caused some regression, since previously I had mostly switched from virtualenvwrapper to Docker containers for managing Python development environment.\n\nNVIDIA has been developing another solution since late 2015. Recently I\u2019ve noticed open-sourced deep learning implementations are starting to come with docker images, and a PaaS provider seems to build the entire service around GPU-enabled docker images. It seems to me that the new solution has become production-ready, so it\u2019s a good time to give it a try."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/kaggle-instacart-competition-b7177c3324dd",
        "title": "[Review] Kaggle Instacart Competition \u2013 The Artificial Impostor \u2013",
        "text": "Now we come back to what brought me to this competition \u2014 figuring out how to learn the features automatically from neural networks, without having to hand-craft features myself.\n\nSean Vasquez did a fantastic job with his solution which relies purely on learnt features. The code was written superbly. However, it requires 64GB of RAM to run and I have only 16GB\u2026 Therefore I spent some time modifying my code to be closer to Sean\u2019s and you can find it on Github (WIP) :\n\nAs mentioned in the last section, the data is split by users and save as a single pickle file (basket.preprocessing.prepare_users):\n\nUSER_CHUNK is set to 1000. Opening data/users/ in the file manager (accidentally) might freeze the GUI if instead we store the files in the same folder. (Modern file systems support hundreds of thousands of files in the same folder, but some of the tools are not comfortable with that.)\n\nThe features are dynamically assembled and split into batches with a fixed size (basket.models.rnn_product.data_loader). The InstacartDataLoader class is based on PyTorch\u2019s DataLoader. The custom dataset of PyTorch is not applicable because we want to sample by user and have non-standard return values. One of the disadvantage is that rows from the same user is highly likely to be in the same batch, but I think this is a necessary trade-off.\n\nThe model combines 2-layer LSTM and 3-layer causal CNN with dilated convolution. Sean used 1-layer LSTM and 6-layer CNN with dilated convolution, but I find this structure more effective in my setting (only considers products appeared in the last 10 orders). In fact, I didn\u2019t find causal CNN provide any noticeable gain in performance. This part of the training (basket.models.rnn_product.model) should run smoothly with no less than 8GB of RAM.\n\nBTW, I found spotlight a really interesting project and it helps me understand how causal CNN (which is new to me) can be implemented in PyTorch:\n\nThe state of the final fully connected layer is extracted and feed into a LightGBM model. Currently I have only implemented one neural network model, and the performance of LightGBM model isn\u2019t much different from directly apply sigmoid function on the last layer. Perhaps LightGBM will perform much better with states from multiple models. But I\u2019ll have to reduce the size of the last layers or the states won\u2019t fit into memory (16GB RAM can handle ~ 80 features).\n\nThere\u2019s a lot more to be done. The current result is far from competitive. I might need to implement more ideas from Sean\u2019s solution. And Colin Morris also provided some interesting insights:\n\nHowever, I\u2019m a bit burnt out on this one and need to take a break. Hopefully I\u2019ll come back later and push this solution to at least around 0.4050 private score.\n\nI\u2019ve added a Bernoulli mixture model for product-level features. It\u2019s basically a LSTM + FC model structure, but uses the last FC layer to simultaneously create multiple predictions and weights to these predictions (each use half of the layer output). And the final prediction is a weighted sum of these predictions.\n\nI\u2019ve tried to combine it with the previous model and feed it to GBM meta model, but there\u2019s no significant improvement to the public nor the private score. Maybe I should try add some tabular features to the meta model."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/learning-notes-from-planet-competition-b764e2e458f5",
        "title": "Learning Notes from Planet Competition \u2013 The Artificial Impostor \u2013",
        "text": "Full name of the competition: Planet: Understanding the Amazon from Space \u2014 Use satellite data to track the human footprint in the Amazon rain forest\n\nI started participating in this competition pretty late, essentially not until the last week. As a result I gave up using tiff images (instead using the pre-processed jpeg images), and focused on practicing building a image classification pipeline.\n\nIt was quite frustrating that my result keeps falling short of beating the best public scripts (I did get on par, and even a little better with the scripts in the end). The biggest problem I cannot figure out is when I use higher resolution (256x256 and 224x224) images, the increase in training time does not accompany with a increase in accuracy. In the end, my best submission comes from a model based on 128x128 images. It\u2019s weird because the pre-trained model I used (VGG16 and Resnet50) should work better with 224x224 images. My guess is that I didn\u2019t manage the learning rate well. I\u2019ll update the post if later I somehow figured it out.\n\nThe two highlights of the learning experiences are:\n\nI plan to update this post periodically until I finished reviewing this competition.\n\nUpdate on 2018/01/23: I\u2019ve found the cause of under-performance a while ago. It was a stupid bug which led to only a small portion of the parameters were updated at each step. Using JPEG files is actually enough to achieve top-level performance. I had also done some more experiments on this dataset and planed to write a independent post on the results, but I never find the time to write it."
    },
    {
        "url": "https://medium.com/the-artificial-impostor/cxx11-is-not-defined-problem-in-mro-3-4-e51f1d27da15",
        "title": "\u201cCXX11 is not defined\u201d problem in MRO 3.4 \u2013 The Artificial Impostor \u2013",
        "text": "Recently I tried to set up the R development environment on a fresh Linux system, every thing went well until I hit this error installing xgboost:\n\nAfter some Googling I found this Github issue. It appears to be a MRO-specific problem, and mjmg provided a working solution.\n\nThe gist is, replacing every \u201cCXX1X\u201d with \u201cCXX11\u201d in R compiler configuration files (System wide: Per-user: ) and fill in some values:"
    }
]