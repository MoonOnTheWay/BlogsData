[
    {
        "url": "https://medium.com/tensorflow/colab-an-easy-way-to-learn-and-use-tensorflow-d74d1686e309?source=---------0",
        "title": "Colab: An easy way to learn and use TensorFlow \u2013 TensorFlow \u2013",
        "text": "Colaboratory is a hosted Jupyter notebook environment that is free to use and requires no setup. You may have already seen it in Machine Learning Crash Course, tensorflow.org\u2019s eager execution tutorial, or on various research articles (like this one). We wanted to offer 5 tips for using it:\n\nWhen you create a new notebook on colab.research.google.com, TensorFlow is already pre-installed and optimized for the hardware being used. Just , and start coding.\n\n2. Setup your libraries and data dependencies in code cells\n\nCreating a cell with or works as you\u2019d expect. It also makes it easy for others to reproduce your setup.\n\nTo get in your training data, you can follow these tutorials for popular data sources: BigQuery, Drive, Sheets, or Google Cloud Storage. You also have access to the shell with , so , , etc. might also help.\n\n3. Use it with Github\n\nIf you have a nice .ipynb on Github, it\u2019s easy to create a one-click link for your readers to start playing with it. Just add your Github path to colab.research.google.com/github/ . For example, colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb will load this ipynb stored on Github.\n\nYou can also easily save a copy of your Colab notebook to Github by using File > Save a copy to Github\u2026\n\nColab notebooks are just like Google Docs and Sheets. They are stored in Google Drive and can be shared, edited, and commented on collaboratively. Just click the Share button in the top right of any notebook that you\u2019ve created.\n\nBy default, Colab notebooks run on CPU. You can switch your notebook to run with GPU by going to Runtime > Change runtime type, and then selecting GPU. You can also have a Colab notebook use your local machine\u2019s hardware by following these instructions.\n\nFor more tips, see our welcome notebook, read our FAQ, or find useful code snippets while using Colab (Help > Search code snippets..).\n\nThanks, and we hope you enjoy using TensorFlow and Colab!"
    },
    {
        "url": "https://medium.com/tensorflow/introducing-swift-for-tensorflow-b75722c58df0?source=---------1",
        "title": "Introducing Swift For TensorFlow \u2013 TensorFlow \u2013",
        "text": "Posted by the Swift for TensorFlow team at Google\n\nAt the TensorFlow Developer Summit in March, we announced and demo\u2019d the Swift for TensorFlow project. Now, we\u2019re excited to launch Swift for TensorFlow as an open-source project on GitHub!\n\nSwift for TensorFlow provides a new programming model that combines the performance of graphs with the flexibility and expressivity of Eager execution, with a strong focus on improved usability at every level of the stack. This is not just a TensorFlow API wrapper written in Swift \u2014 we added compiler and language enhancements to Swift to provide a first-class user experience for machine learning developers.\n\nOur approach is a new and different way to use TensorFlow, opening new design opportunities and new avenues for solving existing problems. Though the project is in early development, we\u2019ve decided to open-source it and move our design discussions to a public mailing list so anyone interested in the project can get involved.\n\nWe\u2019ve written some detailed documents to outline our approach and explain how things work, all accessible from our project README. A good place to start is the Swift for TensorFlow Design Overview, which explains the major components of the project and how they fit together.\n\nAfter that, we have a few documents which dive deeper into important areas of the project. A cornerstone of our design is an algorithm we call Graph Program Extraction, which allows you to write in an eager execution-style programming model while retaining all of the benefits of graphs. Our design also includes support for advanced automatic differentiation built directly into Swift. We also have a deep dive on Python integration with Swift, which allows you to use arbitrary Python APIs directly from Swift code.\n\nFinally, our Graph Program Extraction approach imposed several technical constraints on our implementation, which led us to choosing Swift as a host language. The Why *Swift* for TensorFlow? deep dive explains the tradeoffs in more detail, along with the decision process that led to this choice.\n\nIt is a bit too early to rewrite your deep learning models using Swift for TensorFlow, but if you\u2019re interested in ML, languages and compilers, there are many ways you can get involved and contribute. We have pre-built packages for macOS and Linux that you can try now, along with a sample MNIST model. We also have instructions for building from source. At this phase of the project, there are many known issues \u2014 if you run into trouble, please reach out to us on our mailing list.\n\nWe are excited about building a beautiful new interface to TensorFlow that users will love, and we would really like to hear what you think about the project!"
    },
    {
        "url": "https://medium.com/tensorflow/predicting-the-price-of-wine-with-the-keras-functional-api-and-tensorflow-a95d1c2c1b03?source=---------2",
        "title": "Predicting the price of wine with the Keras Functional API and TensorFlow",
        "text": "Can you put a dollar value on \u201celegant, fine tannins,\u201d \u201cripe aromas of cassis,\u201d or \u201cdense and toasty\u201d? It turns out a machine learning model can. In this post I\u2019ll explain how I built a wide and deep network using Keras (tf.keras) to predict the price of wine from its description. For those of you new to Keras, it\u2019s the higher level TensorFlow API for building ML models. And if you\u2019d like to skip right to the code, it\u2019s available on GitHub here. You can also run the model directly in the browser with zero setup using Colab here.\n\nShout-out to Francois, Josh, and Yufeng for their help and input on this post.\n\nI\u2019ve been building a lot of Keras models recently (here are some examples) using the Sequential model API, but I wanted to try out the Functional API. The Sequential API is the best way to get started with Keras \u2014 it lets you easily define models as a stack of layers. The Functional API allows for more flexibility, and is best suited for models with multiple inputs or combined models. A good use case for the Functional API is implementing a wide and deep network in Keras. There\u2019s a lot of great resources on wide and deep so I won\u2019t focus on the specifics, but if you\u2019re interested in learning more I recommend this post.\n\nAnd before you jump to solve your ML problem with a wide and deep network, it\u2019s best to make sure it\u2019s well suited for what you\u2019re trying to predict. If you\u2019ve got a prediction task where there\u2019s a relatively direct relationship between inputs and outputs, a wide model will probably suffice. Wide models are models with sparse feature vectors, or vectors with mostly zero values. Multi-layer deep networks, on the other hand, have been known to do well on tasks like image or speech recognition, where there may be unexpected relationships between inputs and outputs. If you\u2019ve got a prediction task that could benefit from both of these models (recommendation models or models with text inputs are good examples), wide & deep might be a good fit. In this case, I tried a wide and deep model each separately, then combined them, and found accuracy to be best with wide & deep together. Let\u2019s dive in.\n\nWe\u2019ll use this wine dataset from Kaggle to see:\n\nCan we predict the price of a bottle of wine from its description and variety?\n\nThis problem is well suited for wide & deep learning because it involves text input and there isn\u2019t an obvious correlation between a wine\u2019s description and its price. We can\u2019t definitively say that wines with the word \u201cfruity\u201d in the description are more expensive, or that wines with \u201csoft tannins\u201d are cheaper. In addition, there are multiple ways to represent text when we feed it into our model, and both can lead to different types of insights. There are both wide representations (bags of words) and deep ones (embeddings), and combining the two can allow us to extract more meaning from text. This dataset has lots of different feature possibilities but we\u2019ll use only the description and variety to keep things relatively simple. Here\u2019s a sample input and prediction from this dataset:\n\nTo begin, here are all the imports we\u2019ll need to build this model:\n\nSince the output (prediction) of our model is a number for price, we\u2019ll feed the price value directly to our model for training and evaluation. The full code for this model is available on GitHub. Here I\u2019ll highlight the key points.\n\nFirst, let\u2019s download the data and convert it to a Pandas data frame:\n\nNext we\u2019ll split it into a training and testing set and extract the features and labels:\n\nTo create a wide representation of our text descriptions we\u2019ll use a bag of words model. More on that here, but for a quick recap: a bag of words models looks for the presence of words in each input to our model. You can think of each input as a bag of Scrabble tiles, where each tile contains a word instead of a letter. The model doesn\u2019t take into account the order of words in a description, just the presence or absence of a word.\n\nInstead of looking at every word found in every description in our dataset, we\u2019ll limit our bag of words to the top 12,000 words in our dataset (don\u2019t worry, there\u2019s a built-in Keras utility for creating this vocabulary). This is considered \u201cwide\u201d because the input to our model for each description will be a 12k element wide vector with 1s and 0s indicating the presence of words from our vocabulary in a particular description.\n\nKeras has some handy utilities for text preprocessing that we\u2019ll use to convert the text descriptions into a bag of words. With a bag of words model we\u2019ll typically want to only include a subset of the total words found in our dataset in the vocabulary. In this example I used 12,000 words, but this is a hyperparameter you can tune (try a few values and see what works best on your dataset). We can use the Keras class to create our bag of words vocabulary:\n\nThen we\u2019ll use the function to convert each description to a bag of words vector:\n\nIn the original Kaggle dataset there are 632 total varietals of wine. To make it easier for our models to extract patterns, I did a bit of preprocessing to keep only the top 40 varietals (around 65% of the original dataset, or 96k total examples). We\u2019ll use a Keras utility to convert each of these varieties to integer representation, and then we\u2019ll create 40-element wide one-hot vectors for each input to indicate the variety:\n\nNow we\u2019re ready to build the wide model.\n\nKeras has two APIs for building models: the Sequential API and the Functional API. The Functional API gives us a bit more flexibility in how we define our layers, and lets us combine multiple feature inputs into one layer. It also makes it easy to combine our wide and deep models into one when we\u2019re ready. With the Functional API, we can define our wide model in just a few lines of code. First, we\u2019ll define our input layer as a 12k element vector (for each word in our vocabulary). We\u2019ll then connect this to our Dense output layer to generate price predictions:\n\nThen we\u2019ll compile the model so it\u2019s ready to use:\n\nIf we were using the wide model on its own, this is where we\u2019d run training with and evaluation with . Since we\u2019re going to combine it with our deep model later on we can hold off on training until the two models are combined. Time to build our deep model!\n\nTo create a deep representation of the wine\u2019s description we\u2019ll represent it as an embedding. There are lots of resources on word embeddings, but the short version is that they provide a way to map word to vectors so that similar words are closer together in vector space.\n\nTo convert our text descriptions to an embedding layer, we\u2019ll first need to convert each description to a vector of integers corresponding to each word in our vocabulary. We can do that with the handy Keras method:\n\nNow that we\u2019ve got integerized description vectors, we need to make sure they\u2019re all the same length to feed them into our model. Keras has a handy method for that too. We\u2019ll use to add zeros to each description vector so that they\u2019re all the same length (I used 170 as the max length so that no descriptions were cut short):\n\nWith our descriptions converted to vectors that are all the same length, we\u2019re ready to create our embedding layer and feed it into a deep model.\n\nThere are two ways to create an embedding layer \u2014 we can use weights from pre-trained embeddings (there are many open source word embeddings) or we can learn the embeddings from our vocabulary. It\u2019s best to experiment with both and see which one performs better on your dataset. Here we\u2019ll use learned embeddings.\n\nFirst, we\u2019ll define the shape of our inputs to the deep model. Then we\u2019ll feed it into the Embedding layer. Here I\u2019m using an Embedding layer with 8 dimensions (you can experiment with tweaking the dimensionality of your embedding layer). The output of the Embedding layer will be a three dimensional vector with shape: [batch size, sequence length (170 in this example), embedding dimension (8 in this example)]. In order to connect our Embedding layer to the Dense, fully connected output layer we need to flatten it first:\n\nOnce the embedding layer is flattened it\u2019s ready to feed into the model and compile it:\n\nOnce we\u2019ve defined both of our models, combining them is easy. We simply need to create a layer that concatenates the outputs from each model, then merge them into a fully connected Dense layer, and finally define a combined model that combines the input and output from each one. Obviously since each model is predicting the same thing (price), the output or labels from each one will be the same. Also note that since the output of our model is a numerical value we don\u2019t need to do any preprocessing \u2014 it\u2019s already in the right format:\n\nWith that, it\u2019s time to run training and evaluation. You can experiment with the number of training epochs and batch size that works best for your dataset:\n\nTime for the most important part \u2014 seeing how our model performs on data it hasn\u2019t seen before. To do this, we can call on our trained model, passing it our test dataset (in a future post I\u2019ll cover how to get predictions from plain text input):\n\nThen we\u2019ll compare predictions to the actual values for the first 15 wines from our test dataset:\n\nHow did the model do? Let\u2019s take a look at the three examples from our test set:\n\nPretty well! It turns out there is some relationship between a wine\u2019s description and its price. We may not be able to see it instinctively, but our ML model can.\n\nWe covered a lot of material here but there are always more layers \ud83d\ude09. In a future post, I\u2019ll cover how to train this model in the cloud. Also, a trained model isn\u2019t the end of the road. If you\u2019re training a model chances are you probably want to build an app that makes predictions on it. In another post I\u2019ll cover serving this model in production and building an app to make predictions against it: enter a wine description, predict the price.\n\nWant to build your own wide + deep model in Keras? Check out the full code from this model on GitHub and dive into the Keras Functional API docs. Let me know if you have any feedback in the comments or on Twitter @SRobTweets. Cheers! \ud83e\udd42"
    },
    {
        "url": "https://medium.com/tensorflow/upcoming-tensorflow-events-in-april-and-beyond-33115450225?source=---------3",
        "title": "Upcoming TensorFlow events in April and beyond. \u2013 TensorFlow \u2013",
        "text": "Do you want to learn more about TensorFlow, chat with members of the team, or meet other developers? Join us at any of these upcoming events! Here are some of the places the TensorFlow team will be in April and beyond.\n\nWhere: New York, New York, NY\n\nGetting up and running with TensorFlow: [Speaker, Amy Unruh]\n\nAmy Unruh walks you through the process of building a complete machine learning pipeline, covering ingest, exploration, training, evaluation, deployment, and prediction.\n\nTensorFlow Lite: How to accelerate your Android and iOS app with AI [Speaker, Kaz Sato]\n\nTensorFlow Lite \u2014 TensorFlow\u2019s lightweight solution for Android, iOS, and embedded devices \u2014 enables on-device machine learning inference with low latency and a small binary size. TensorFlow Lite also supports hardware acceleration with the Android Neural Networks API and Apple Core ML. Kazunori Sato walks you through using TensorFlow Lite, helping you overcome the challenges for bringing the latest AI technology to production mobile apps and embedded systems.\n\nA friendly introduction to Deep Learning, taught at the beginner level. We\u2019ll work through introductory exercises across several domains \u2014 including computer vision, natural language processing, and structured data classification. We\u2019ll introduce TensorFlow, explore the latest APIs, discuss best practices, and point you to recommended educational resources you can use to learn more. Note from Josh: all the code will be online for folks who can\u2019t make it IRL, publishing shortly after Google I/O later this month.\n\nI/O brings together developers from around the globe for talks, hands-on learning with Google experts, and a first look at Google\u2019s latest developer products. Note: Sorry, we\u2019re no longer accepting applications for Google I/O 2018 tickets. However, you can join remotely via I/O Extended. Of course, all the talks will be livestreamed and on YouTube after as well.\n\nIntroducing AIY: Do it yourself Artificial Intelligence [Speakers: Dushyantsinh Jadeja, Bill Luan, Sebastian Trzcinski-Clement]\n\nOpportunities, challenges, and strategies to develop AI for everyone [Speakers: Daphne Luong, John Platt, Rajen Sheth, Fernanda Viegas]\n\nA friendly introduction to Deep Learning, taught at the beginner level. We\u2019ll work through introductory exercises across several domains \u2014 including computer vision, natural language processing, and structured data classification. We\u2019ll introduce TensorFlow, explore the latest APIs, discuss best practices, and point you to recommended educational resources you can use to learn more. Note from Josh: all the code will be online for folks who can\u2019t make it IRL.\n\nA friendly introduction to Deep Learning, taught at the beginner level. We\u2019ll work through introductory exercises across several domains \u2014 including computer vision, natural language processing, and structured data classification. We\u2019ll introduce TensorFlow, explore the latest APIs, discuss best practices, and point you to recommended educational resources you can use to learn more. Note from Josh: all the code will be online for folks who can\u2019t make it IRL."
    },
    {
        "url": "https://medium.com/tensorflow/speed-up-tensorflow-inference-on-gpus-with-tensorrt-13b49f3db3fa?source=---------4",
        "title": "Speed up TensorFlow Inference on GPUs with TensorRT",
        "text": "TensorFlow remains the most popular deep learning framework today, with tens of thousands of users worldwide. NVIDIA\u00ae TensorRT\u2122 is a deep learning platform that optimizes neural network models and speeds up for inference across GPU-accelerated platforms running in the datacenter, embedded and automotive devices. We are excited about the integration of TensorFlow with TensorRT, which seems a natural fit, particularly as NVIDIA provides platforms well-suited to accelerate TensorFlow. This enables TensorFlow users with extremely high inference performance plus a near transparent workflow when using TensorRT.\n\nTensorRT performs several important transformations and optimizations to the neural network graph (Fig 2). First, layers with unused output are eliminated to avoid unnecessary computation. Next, where possible convolution, bias, and ReLU layers are fused to form a single layer. Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective output. Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters. Note that these graph optimizations do not change the underlying computation in the graph: instead, they look to restructure the graph to perform the operations much faster and more efficiently.\n\nFigure 2 (a): An example convolutional neural network with multiple convolutional and activation layers. (b) TensorRT\u2019s vertical and horizontal layer fusion and layer elimination optimizations simplify the GoogLeNet Inception module graph, reducing computation and memory overhead.\n\nIf you were already using TensorRT with TensorFlow models, you knew that applying TensorRT optimizations used to require exporting the trained TensorFlow graph. You also needed to manually import certain unsupported TensorFlow layers, and then run the complete graph in TensorRT. You should not need to do that for most cases any more. In the new workflow, you use a simple API to apply powerful FP16 and INT8 optimizations using TensorRT from within TensorFlow. Existing TensorFlow programs require only a couple of new lines of code to apply these optimizations.\n\nTensorRT sped up TensorFlow inference by 8x for low latency runs of the ResNet-50 benchmark. These performance improvements cost only a few lines of additional code and work with the TensorFlow 1.7 release and later. In this article we will describe the new workflow and APIs to help you get started with it.\n\nAdding TensorRT to the TensorFlow inference workflow involves an additional step, shown in Figure 3. In this step (highlighted in green), TensorRT builds an optimized inference graph from a frozen TensorFlow graph.\n\nFigure 3: Workflow Diagram when using TensorRT within TensorFlow\n\nTo accomplish this, TensorRT takes the frozen TensorFlow graph and parses it to select sub-graphs that it can optimize. It then applies optimizations to the subgraphs and replaces them with TensorRT nodes in the original TensorFlow graph leaving the remaining graph unchanged. During inference, TensorFlow executes the complete graph calling TensorRT to run the TensorRT optimized nodes. With this approach, developers can continue to use the flexible TensorFlow feature set with the optimizations of TensorRT.\n\nLet\u2019s look at an example of a graph with three segments, A, B, and C. TensorRT optimizes Segment B, then replaces it with a single node. During inference, TensorFlow executes A, calls TensorRT to execute B, and then TensorFlow executes C. From a user\u2019s perspective, you continue to work in TensorFlow as earlier.\n\nTensorRT optimizes the largest sub-graphs possible in the TensorFlow graph. The more compute in the subgraph, the greater benefit obtained from TensorRT. You want most of the graph optimized and replaced with the fewest number of TensorRT nodes for best performance. Based on the operations in your graph, it\u2019s possible that the final graph might have more than one TensorRT nodes. With the TensorFlow API, you can specify the minimum number of the nodes in a sub-graph for it to be converted to a TensorRT node. Any sub-graph with less than the specified set number of nodes will not be converted to TensorRT engines even if it is compatible with TensorRT. This can be useful for models containing small compatible sub-graphs separated by incompatible nodes, in turn leading to tiny TensorRT engines.\n\nLet\u2019s look at how to implement the workflow in more detail.\n\nThe new TensorFlow API enables straightforward implementation of TensorRT optimizations with a couple of lines of new code. First, specify the fraction of available GPU memory that TensorFlow is allowed to use, the remaining memory being available for TensorRT engines. This can be done with the new parameter of the function. This parameter needs to be set the first time the TensorFlow-TensorRT process starts. For example, setting to 0.67 allocates 67% of GPU memory for TensorFlow and the remaining third for TensorRT engines.\n\nThe next step is letting TensorRT analyze the TensorFlow graph, apply optimizations, and replace subgraphs with TensorRT nodes. You apply TensorRT optimizations to the frozen graph with the new function. This function uses a frozen TensorFlow graph as input, then returns an optimized graph with TensorRT nodes, as shown in the following code snippet:\n\nLet\u2019s look at the function\u2019s parameters:\n\n: list of strings with names of output nodes e.g.[\u201c \u201d]\n\n: integer (default = 3), control min number of nodes in a sub-graph for TensorRT engine to be created\n\nThe and parameters should be used together to split GPU memory available between TensorFlow and TensorRT to get providing best overall application performance.To maximize inference performance, you might want to give TensorRT slightly more memory than what it needs, giving TensorFlow the rest. For example, if you set the parameter to ( 12\u20134 ) / 12 = 0.67, then setting max_workspace_size_bytes parameter to 4000000000 for a 12GB GPU allocates ~4GB for the TensorRT engines. Again, finding the most optimum memory split is application dependent and might require some iteration.\n\nTensorBoard enables us to visualize the changes to the ResNet-50 node graph once TensorRT optimizations are applied in TensorBoard. Figure 4 shows that TensorRT optimizes almost the complete graph, replacing it with a single node titled \u201cmy_trt_op0\u201d (highlighted in red). Depending on the layers and operations in your model, TensorRT nodes replace portions of your model due to optimizations. The box titled \u201cconv1\u201d isn\u2019t actually a convolution layer; it\u2019s really a transpose operation from NHWC to NCHW.\n\nFigure 4. (a) ResNet-50 graph in TensorBoard (b) ResNet-50 after TensorRT optimizations have been applied and the sub-graph replaced with a TensorRT node.\n\nUsing half-precision (also called FP16) arithmetic reduces memory usage of the neural network compared with FP32 or FP64. FP16 enables deployment of larger networks while taking less time than FP32 or FP64. NVIDIA\u2019s Volta architecture incorporates hardware matrix math accelerators known as Tensor Cores. Tensor Cores provide a 4x4x4 matrix processing array which performs the operation D = A * B + C, where A, B, C and D are 4\u00d74 matrices. Figure 5 shows how this works. The matrix multiply inputs A and B are FP16 matrices, while the accumulation matrices C and D may be FP16 or FP32 matrices.\n\nTensorRT automatically uses hardware Tensor Cores when detected for inference when using FP16 math. Tensor Cores offer peak performance about an order of magnitude faster on the NVIDIA Tesla V100 than double-precision (FP64) while throughput improves up to 4 times faster than single-precision (FP32). Just use \u201cFP16\u201d as value for the parameter in the function to enable half precision, as shown below. is a helper function that reads the frozen network from the protobuf file and returns a of the network.\n\nFigure 6 shows ResNet-50 performing 8 times faster under 7 ms latency with the TensorFlow-TensorRT integration using NVIDIA Volta Tensor Cores versus running TensorFlow only on the same hardware.\n\nPerforming inference using INT8 precision further improves computation speed and places lower requirements on bandwidth. The reduced dynamic range makes it challenging to represent weights and activations of neural networks.\n\nTensorRT provides capabilities to take models trained in single (FP32) and half (FP16) precision and convert them for deployment with INT8 quantizations while minimizing accuracy loss. Converting models for deployment with INT8 requires calibrating the trained FP32 model before applying the TensorRT optimizations described earlier. The workflow changes to incorporate a calibration step prior to creating the TensorRT optimized inference graph, as shown in Figure 7:\n\nFirst use the function, setting the parameter set to \u201cINT8\u201d to calibrate the model. The output of this function is a frozen TensorFlow graph ready for calibration.\n\nNow run the calibration graph with calibration data. TensorRT uses the distribution of node data to quantize weights for the nodes. It\u2019s imperative you use calibration data closely reflecting the distribution of the problem dataset in production. We suggest checking for error accumulation during inference when first using models calibrated with INT8. The parameter can help tune the optimized graph to minimize quantization-errors. Using , you can change the minimum number of nodes in the optimized INT8 engines to change the final optimized graph to fine tune result accuracy.\n\nAfter executing the graph on calibration data, apply TensorRT optimizations to the calibration graph with the function. This function also replaces the TensorFlow subgraph with a TensorRT node optimized for INT8. The output of the function is a frozen TensorFlow graph that can be used for inference as usual.\n\nAll it takes are these two commands to enable INT8 precision inference with your TensorFlow model.\n\nIf you want to check out the examples shown here, check out code required to run these examples at https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz\n\nWe expect that integrating TensorRT with TensorFlow will yield the highest performance possible when using NVIDIA GPUs while maintaining the ease and flexibility of TensorFlow. NVIDIA continues to work closely with the Google TensorFlow team to further enhance these integration capabilities. Developers will automatically benefit from updates as TensorRT supports more networks, without needing to change existing code.\n\nFind instructions on how to get started today at: https://www.tensorflow.org/install/install_linux\n\nIn the near future, we expect the standard pip install process to work as well. Stay tuned!\n\nWe believe you\u2019ll see substantial benefits to integrating TensorRT with TensorFlow when using GPUs. You can find more information on TensorFlow at https://www.tensorflow.org/.\n\nAdditional information on TensorRT can be found on NVIDIA\u2019s TensorRT page at https://developer.nvidia.com/tensorrt."
    },
    {
        "url": "https://medium.com/tensorflow/my-collaboration-with-painting-robots-and-a-graffiti-artist-409ef5ee6b01?source=---------5",
        "title": "My Collaboration with Painting Robots and a Graffiti Artist",
        "text": "I am an AI Artist and it is an interesting time for my art.\n\nSpecifically, I build creative painting robot systems. These robots are programmed to collaborate with me using a variety of artificial intelligence, deep learning, and feedback loops. We have been working together for almost 15 years. In this time we have created over a thousand canvases, each artwork painted one brush stroke at a time.\n\nThough I have been sharing my work this entire time, the art world does not yet fully considered it to be art. My robots have been called over-engineered printers and little more than complex photoshop filters. Fellow artist have told me that they did not know whether to be impressed or disgusted by my machines. But in the last couple of years, things have been gradually changing. People are beginning to realize just how creative AI has become.\n\nAn interesting moment in the recognition of this new genre occurred just recently. Famed New York Art Critic Jerry Saltz reviewed several AI generated images. He roasted just about everything he looked at which included some important work by AICAN, Mario Klingemann, and Google Deep Dream. When he got to one of my paintings, he looked at it and said that it \u201cdoesn\u2019t look like a computer made it.\u201d before concluding \u201cThat doesn\u2019t make it any good.\u201d\n\nI would have preferred a kinder review, but loved it nonetheless. The fact that Jerry Saltz even took the time to look at AI art was an important moment for those of us in the genre. As I have mentioned, most of the art world does not even consider our work to be art. At least now some accept it as bad art. That\u2019s progress.\n\nA good analogy that I often turn to is that the AI genre is probably where graffiti art was right before the turn of the century. Street art was some of the most interesting art out there, but was completely ignored by the art world. In a similar manner, it is obvious to me that today\u2019s AI artists are the Avant-garde . It should therefore only be a matter of time before the public realizes it.\n\nBeing both a part of the new AI Art movement and a fan of Street Art, I am exciting to reveal a collaboration that I have been working on with Bristol based graffiti artist 3D (aka Robert Del Naja of Massive Attack). 3D has been spray painting walls, canvases, and just about anything he can get his hands on since the early 80\u2019s. As far as his street cred, BANKSY is quoted to have said that he \u201ccopied 3D from Massive Attack.\u201d Beyond painting, 3D\u2019s work with Massive Attack often explores new media with innovative and experimental interactive performances. It was our interest in each others work that brought us together to see if we could combine AI, graffiti, and interactive performance art.\n\nOur collaboration began about six months ago as we brainstormed ideas. Work in earnest began a couple of months ago when we began experimenting with some of those ideas by applying CNNs, GANs, and many of my own artificial intelligent algorithms to his artwork. I have long been working at teaching my own painting robots to imitate my own artistic process with computationally creative code. 3D and I are now exploring if we can capture parts of his artistic process.\n\nExecution started simply enough with looking at the patterns behind 3D\u2019s paintings. We started creating mash-ups in an implementation of Gatys, Ecker, and Bethge\u2019s A Neural Algorithm for Artistic Style, commonly called Style Transfer. Style Transfer is of course a popular convolutional neural network (CNN) that takes two input images and combines the contours of one with the colors and texture of the other. A breakdown of the CNN can be seen in Gatys\u2019 graphic below.\n\nIn their following example, you can see a photo of the \u201cNeckarfront\u201d in Tubingen rendered in the style of Van Gogh and Munch.\n\nYou can learn more about how to implement this algorithm either directly from their paper or from one of the many implementations on Git-Hub. Two TensorFlow projects that I found useful for getting me started were log0\u2019s tutorial and Google Magenta\u2019s Jupyter Notebook.\n\nWhile the intent of Style Transfer is to combine separate content and style images, 3D and I experimented with using his artwork as both the content and style. We created the following grid where seven of his paintings were combined with themselves. It was interesting to see what worked and what didn\u2019t. Furthermore, it was interesting to see what about each painting\u2019s imagery became dominant as they were combined with one another.\n\nAs cool as these looked, we were both left underwhelmed by the symbolic and emotional aspects of the mash-ups. We felt the art needed to be meaningful. All that was really being combined was color and texture, not symbolism or context. So we thought about it some more and 3D came up with the idea of trying to use the CNNs to paint portraits of historical figures that made significant contributions to printmaking. Couple of people came to mind as we bounced ideas back and forth before 3D suggested Martin Luther. At first I thought he was talking about Martin Luther King Jr, which left me confused. But then when I realized he was talking about the the author of The 95 Theses and it made more sense. Not sure if 3D realized I was confused, but I think I played it off well and he didn\u2019t suspect anything. We tried applying CNNs to Martin Luther\u2019s famous historic portrait and got the following results.\n\nIt was nothing all that great, but I made a couple of paintings from it to test things. I also tried having my robots paint a couple of other new media figures like Mark Zuckerberg.\n\nThings still were not gelling though. Good paintings, but nothing great. Then 3D and I decided to experiment with some different approaches.\n\nI showed him some faces being created by a Generative Adversarial Network (GAN) based on the often cited work lead by Ian Goodfellow. For anyone interested in making their own GAN, many TensorFlow implementations of GANs exist. The lesson that I learned to make one from was Udacity\u2019s DLND Face Generation project.\n\nWhile the most recent GANs are capable of producing remarkably high resolution faces, I was not as interested in those. I showed 3D how a neat part of the face generation occurred near the beginning. I was fascinated by the part just as faces first began to emerge from nothing. I also showed him the grid of faces (left) that I have come to recognize as a common visualization of GANs.\n\nWe got to talking about how as a polyptych, the grid recalled a common Warhol trope of repeating images except that there was something different. Warhol was all about mass produced art and how repeated images looked interesting next to one another. But these images were even cooler, because it was a new kind of mass production. These faces were mass produced imagery made from neural networks where each image was unique.\n\nI started having my GANs generate tens of thousands of faces. But I didn\u2019t want the faces in too much detail. I like how they looked before they resolved into clear images. It reminded me of how my own imagination worked when I tried to picture something in my mind. My imagination is foggy and nondescript. So I implemented the Viola-Jones face detection algorithm with OpenCV to stop the GAN as soon as faces were beginning to be recognized. From there I sent the nondescript faces into a Style Transfer with several of 3D\u2019s paintings to see which would best render them.\n\n3D\u2019s Beirut (Column 2) was the most interesting, so I chose that one and put it into the artificially creative process that I have been developing over the past fifteen years. A simplified outline of this process can be seen in the graphic below.\n\nAs just described, my robots would begin by having the GAN imagine faces. I then ran the Viola-Jones face detection algorithm on the GAN images until it began detected faces. This would stop the GAN right as the general outlines of faces emerged. Then I applied Style Transfer on the faces to render them in the style of 3D\u2019s Beirut. With this image in its memory, my robots started painting. The brushstroke geometry was taken out of my historic database that contains the strokes of hundreds of paintings, including Picassos, Van Goghs, and my own work. Feedback loops refined the image as the robot tried to paint the faces on 11\"x14\" canvases. All told, dozens of AI algorithms, multiple deep learning neural networks, and feedback loops at all levels started pumping out face after face after face."
    },
    {
        "url": "https://medium.com/tensorflow/building-an-iris-classifier-with-eager-execution-13c00a32adb0?source=---------6",
        "title": "Building an Iris classifier with eager execution \u2013 TensorFlow \u2013",
        "text": "One of the new additions to TensorFlow in the last months has been the eager execution, an additional low-level interface promising to make development a lot simpler and easier to debug. Whenever eager execution is enabled, operations are executed immediately, instead of having to go through a separate execution step. In a nutshell, this means that writing TF code can be (potentially) as simple as writing pure NumPy code!\n\nBeing a long-requested feature, I was looking forward to test it. After playing around for a while with the new interface, I can say the new workflow feels much simpler and intuitive, all the methods are integrated almost perfectly with the rest of TF and, despite being in an experimental stage, most of the functionalities are already stably implemented.\n\nAt the same time, eager execution is a further addition in a framework that is already complex for many newcomers. Which is the reason for this post: a guided tutorial for navigating into this new interface by showing all the steps needed to build a simple classification model from scratch on the Iris dataset. At the end of this article, you will have implemented a fully working model in eager execution from which you can continue experimenting.\n\nThis post is also intended as a self-contained introduction to the eager execution: if you are new to low-level programming in TensorFlow, you will learn how you can implement your own algorithms and run them using this new interface. But if you already consider yourself an expert in TF, this post will also highlight the differences when enabling eager execution as opposed to the standard graph construction.\n\nEverybody is ready? Let\u2019s go!\n\nNote: if you want to experiment with eager execution, I also wrote a Jupyter notebook to serve as an extended companion to this post. And if you don\u2019t have a working TensorFlow installation? Remember you can run the notebook using the (completely free) Google Colaboratory service in the cloud!\n\nNowadays TF includes several high-level APIs, such as Estimators and Keras. But many times, you\u2019d also like to explore and run some low-level code, or dig deeper in the framework. At that point, you may like to write or run something simple to begin with.\n\nTo see how the eager execution works, let\u2019s start by adding and multiplying some numbers, and see how the code would look like without eager execution:\n\nIf you (like me) have been here since the early days of Theano, it is hard to remember just how strange this code feels if you are not used to it: why do you even need an external object just to print a number? And why can\u2019t you find the value of b anywhere? The reason is that, internally, TensorFlow is building a computational graph describing all the operations, which is then compiled and optimized behind the scenes \u2014 optimization that, in the initial development of TF, was supposed to happen only after the full specification of the model.\n\nNow compare the same example, but after enabling the eager execution:\n\nThis is pretty much NumPy code! In programming terms, the default API for TF is declarative: execution only happens when we request the output of a specific node, and it only returns that particular result. The new eager execution, instead, is imperative: execution follows definition immediately, with all of its benefits, immediate visual debugging for one. Let\u2019s see what this means for the optimization of our models \u2014 but first, let\u2019s take a small detour to see how you can install and enable eager execution.\n\nEager execution has been included as an experimental feature since TF v1.5, and for the purposes of this post I will use the latest v1.7rc0 release (mentioning where you might need to change the code for backward compatibility).\n\nIf you want to play with the latest version, the recommended way is to set up a separate virtual environment (you can find detailed instructions on the notebook and on the official guide) and install the nightly build of TF from the console:\n\nEager execution can be enabled with a single line of code:\n\nImportantly, you need to run the command at the beginning of the program, otherwise the command will throw an error and refuse to execute. The reason is that, by enabling eager execution, you are changing the default inner mechanisms for TF described before, meaning that several low-level commands will be available only in one of the two modalities (graph construction or eager execution) as described in the documentation.\n\nIn the rest of this tutorial, we are going to build a classification model (with eager execution enabled) for the Iris dataset, a simple classification dataset which you should already have encountered when following the starting guide for TF. The task is to classify a set of Irises in three different classes, depending on four numerical features describing their geometrical shape:\n\nFor the purpose of this article I am simply loading the dataset\u2019s version from the scikit-learn library, normalizing its inputs, and splitting it in two for training and testing. If you are curious, here is the full code for loading the dataset:\n\nWe will proceed in five steps. The first part is just to get some familiarity with the basic eager objects such as variables and functions, and how they interoperate with NumPy arrays. Then, we will learn how to build the classification model (part 2), optimize it (part 3), load the data in TF (part 4), and debug the overall process (part 5).\n\nThe basic operation provided by eager execution is automatic differentiation: given some function operating on tensors, we can compute the gradient with respect to one or more of these tensors automatically (and, it goes without saying, efficiently).\n\nIf you are costructing a TF graph, functions and gradients are defined as nodes inside the computational graph. With the eager execution enabled, instead, both of them are standard Python functions. For example, here is some code defining a simple squaring function and computing its gradients:\n\nInstead of having to build the graph, all operations are internally saved inside a \u201ctape\u201d, transparently to the user. The is where the magic happens: you can pass any Python function (having as input at least one tensor) as argument, and it will return another Python function for computing the gradient with respect to any of its arguments.\n\nYou can also chain the calls to get higher-order derivatives:\n\nA fundamental building block of TF are variables: stateful arrays of numbers that compose our models and that we need to optimize in the training phase. Eager has a custom implementation of variables:\n\nAs you can see, variables in eager are fully interoperable with NumPy arrays, and their value is automatically initialized as soon as they are requested (while with graph construction you would need to explicitly call an initialization routine). Using , you can also automatically compute the gradients of a function with respect to all variables it uses. For example, this is equivalent to the gradient computation above, but using variables and implicit gradients:\n\nNow that we have all the building blocks in hand, we can move to a higher level of abstraction, starting with the definition of our neural network.\n\nIn theory, we already have everything we need for optimizing a model: we can instantiate a bunch of variables (the parameters of our models), and encapsulate the logic of the model itself inside a function. For example, this is a one-dimensional logistic regression model:\n\nHowever, as soon as we start adding more operations (e.g., new hidden layers, dropout, \u2026) this approach does not scale. A simple way to build more complicated models is to use layers, which should be quite familiar to anyone using the high-level interface of TF:\n\nIn the above example, we are building a model with a single hidden layer and a dropout operation in the middle: the definition of variables is taken care of inside each layer, and we can define our model by chaining together the layers inside a simple (once more, Python) function. The model is quite simple to interpret, even if you are not used to TF, and we can implement the dropout logic by just passing another Boolean flag to the function, adding to the readability.\n\nSince building networks in this fashion is a common workflow, TF also provides a further level of abstraction with the object, a custom class that encapsulates the model\u2019s logic, providing a few advanced tools for debugging and checkpointing the training:\n\nA major benefit of networks is that they encapsulate all the logic concerning the variables of the model. To understand it, let\u2019s play around with our model a little bit:\n\nLine 2 is just the initialization of the model. The fifth line shows the lazy nature of variables inside a model: even though eager execution has a dynamic workflow, variables are only initialized when a model is run for the first time, because prior to that eager has no indication on the shape of its input tensors. Predictions can be obtained by simply calling the object with some data (line 8): after this, variables are correctly initialized, as shown in line 11.\n\nSince we are going to use our model to do some classification, we need to define a proper cost function to train it. Similarly to before, we can use the standard components of TF, by encapsulating them in a Python function:\n\nFor the optimization, we can define our custom logic or (more probably) use one of the many optimizers already defined inside TF. At this point, you will probably guess what our code will look like:\n\nThe only syntactic sugar here is using an anonymous function with no arguments to call the minimization routine: this is needed because the optimizers were designed to work for the graph construction, and have been customized to also support the new eager interface. Another possibility is to explicitly compute the gradients of our model, and use the function of the optimizer: if you are interested I describe this alternative way in the notebook.\n\nIf you have kept up-to-date with TF, you will know that the preferred way to feed data to our models is though the high-level Dataset API, which can be used to load data from multiple sources and connects with all the high-level APIs. A nice feature of eager execution is that you can encapsulate datasets inside a Pythonesque iterator, and use the iterator to cycle over it as you would any other iterator.\n\nLet\u2019s loop over the training portion of the Iris dataset we loaded before using an iterator:\n\nIn the above code we are loading the data inside the object (line 2), and then using the iterator to loop over mini-batches of 32 elements, computing the average number of positive labels for each batch, e.g.:\n\nThe last thing we need for our task are some tools for debugging and visualizing the results. Because in the standard graph construction the execution is masked underneath the session\u2019s dynamics, debugging has always been a difficult point for newcomers to TF, and the main reason why the TensorBoard was developed. In eager execution, however, everything runs dynamically and you can use any tool in your Python\u2019s toolbox for it: from Matplotlib\u2019s plots to the console\u2019s outputs, you name it.\n\nEager has also a few utilities to simplify the debugging. For example, metrics can be used to compute and accumulate values across each epoch:\n\nThe code above loops through a single epoch, and for each batch it computes the accuracy of the model. As we have not trained it yet, the accuracy will probably hoover around 33%, or random chance.\n\nAnd if you are fond of the good ol\u2019 TensorBoard, you can save values for visualization on the dashboard using the experimental implementation of the summaries:\n\nThe syntax here is slightly more involved: after creating a writer, we need to set it as the default one and choose an interval for saving the values on disks (based on the global step of TF, which will increase every-time we make an optimization step).\n\nThis was only a cursory look at some debugging tools with the eager execution enabled: check out the notebook for a description of other techniques for visualization and debugging.\n\nWe are finally ready to put it all together! We are going to train our network for a few epochs, using a metric to compute the accuracy and store it in a NumPy array, and a summary to keep track of the loss by saving it on the disk. At the end we use Matplotlib to plot the accuracy across all epochs.\n\nThe network easily reaches 100% accuracy, this being a simple benchmark:\n\nAnd if you open up the TensorBoard, you can check the loss\u2019 progress:\n\nYou have reached the end of my tutorial on the eager execution. By now, I hope you are convinced that using eager allows for extremely fast prototyping while making the debugging much simpler. Once again, you can check the Jupyter notebook if you want more details on all the methods we used in this tutorial, which was only intended as a concise introduction to the interface.\n\nThere are many things we have not seen here, such as how to enable GPU support, or how to use control flow operations inside your model. Nonetheless, I hope this is enough boilerplate code to kick-start you on your journey to the low-level programming beauty of TF!"
    },
    {
        "url": "https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245?source=---------7",
        "title": "Introducing TensorFlow Probability \u2013 TensorFlow \u2013",
        "text": "At the 2018 TensorFlow Developer Summit, we announced TensorFlow Probability: a probabilistic programming toolbox for machine learning researchers and practitioners to quickly and reliably build sophisticated models that leverage state-of-the-art hardware. You should use TensorFlow Probability if:\n\nTensorFlow Probability gives you the tools to solve these problems. In addition, it inherits the strengths of TensorFlow such as automatic differentiation and the ability to scale performance across a variety of platforms: CPUs, GPUs, and TPUs.\n\nOur stack of probabilistic ML tools provides modular abstractions for probabilistic reasoning and statistical analysis in the TensorFlow ecosystem.\n\nLayer 0: TensorFlow. Numerical operations. In particular, the LinearOperator class enables matrix-free implementations that can exploit special structure (diagonal, low-rank, etc.) for efficient computation. It is built and maintained by the TensorFlow Probability team and is now part of in core TF.\n\nThe TensorFlow Probability team is committed to supporting users and contributors with cutting-edge features, continuous code updates, and bug fixes. We\u2019ll continue to add end-to-end examples and tutorials.\n\nA linear mixed effects model is a simple approach for modeling structured relationships in data. Also known as a hierarchical linear model, it shares statistical strength across groups of data points in order to improve inferences about any individual one.\n\nAs demonstration, consider the InstEval data set from the popular lme4 package in R, which consists of university courses and their evaluation ratings. Using TensorFlow Probability, we specify the model as an Edward2 probabilistic program ( ), which extends Edward. The program below reifies the model in terms of its generative process.\n\nThe model takes as input a features dictionary of \u201cservice\u201d, \u201cstudents\u201d, and \u201cinstructors\u201d; they are vectors where each element describes an individual course. The model regresses on these inputs, posits latent random variables, and returns a distribution over the courses\u2019 evaluation ratings. TensorFlow session runs on this output will return a generation of the ratings.\n\nCheck out the \u201dLinear Mixed Effects Models\u201d tutorial for details on how we train the model using the tfp.mcmc.HamiltonianMonteCarlo algorithm, and how we explore and interpret the model using posterior predictions.\n\nA Copula is a multivariate probability distribution for which the marginal probability distribution of each variable is uniform. To build a copula using TFP intrinsics, one can use Bijectors and TransformedDistribution. These abstractions enable easy creation of complex distributions, for example:\n\nThe \u201cGaussian Copula\u201d creates a few custom Bijectors and then shows how to easily build several different copulas. For more background on distributions, see \u201cUnderstanding TensorFlow Distributions Shapes.\u201d It describes how to manage shapes for sampling, batch training, and modeling events.\n\nA variational autoencoder is a machine learning model which uses one learned system to represent data in some low-dimensional space and a second learned system to restore the low-dimensional representation to what would have otherwise been the input. Because TF supports automatic differentiation, black-box variational inference is a breeze! Example:\n\nTo see more details, check out our variational autoencoder example!\n\nA Bayesian neural network is a neural network with a prior distribution over its weights and biases. It provides improved uncertainty about its predictions via these priors. A Bayesian neural network can also be interpreted as an infinite ensemble of neural networks: the probability assigned to each neural network configuration is according to the prior.\n\nAs demonstration, consider the CIFAR-10 dataset which has features (images of shape 32 x 32 x 3) and labels (values from 0 to 9). To fit the neural network, we\u2019ll use variational inference, which is a suite of methods to approximate the neural network\u2019s posterior distribution over weights and biases. Namely, we use the recently published Flipout estimator in the TensorFlow Probabilistic Layers module ( ).\n\nThe model object composes neural net layers on an input tensor, and it performs stochastic forward passes with respect to probabilistic convolutional layer and probabilistic densely-connected layer. The function returns an output tensor with shape given by the batch size and 10 values. Each row of this tensor represents the logits (unconstrained probability values) that each data point belongs to one of the 10 classes.\n\nFor training, we build the loss function, which comprises two terms: the expected negative log-likelihood and the KL divergence. We approximate the expected negative log-likelihood via Monte carlo. The KL divergence is added via regularizer terms which are arguments to the layers.\n\ncan also be used with eager execution using the tf.keras.Model class.\n\nTo get started with probabilistic machine learning in TensorFlow, just run:\n\nFor all the code and details, check out github.com/tensorflow/probability. We\u2019re excited to collaborate with you via GitHub, whether you\u2019re a user or contributor!"
    },
    {
        "url": "https://medium.com/tensorflow/mit-6-s191-introduction-to-deep-learning-24994d705aca?source=---------8",
        "title": "MIT 6.S191: Introduction to Deep Learning \u2013 TensorFlow \u2013",
        "text": "Recurrent neural networks (RNNs) are extensively used in sequence modeling and prediction tasks, on everything from stock trends, to natural language processing, to medical signals like EKGs. Check out our course lecture on deep sequence modeling for some background on RNNs and their applications.\n\nRNNs are well suited for music generation, as they can capture temporal dependencies in time-series data like music. In this first lab, students work through encoding a dataset of music files, defining a RNN model in TensorFlow, and sampling from the model to generate new music that has never been heard before.\n\nThe dataset is a set of pop song snippets that are encoded into vector format to feed into the RNN model. Once the data is processed, the next step is to define and train a RNN model using this dataset of pop song snippets.\n\nThe model is based off a single long short-term memory (LSTM) cell, where the state vector tracks the temporal dependencies between consecutive notes. At each time step, a sequence of previous notes is fed into the cell, and the final output of the last unit in our LSTM is fed into a fully connected layer. Thus, we can output a probability distribution over the next note at time step t given the notes at all previous time steps. We visualize this process in the diagram below:\n\nfoundationalWe provided students with a guide to building the RNN model and defining the appropriate computation graph. Again, we\u2019ve designed these labs to be accessible to everyone who\u2019s interested, regardless of their prior experience with TensorFlow, so they\u2019re guided for a reason!\n\nThe lab first goes through setting the relevant hyperparameters, defining placeholder variables, and initializing the weights for the RNN model. Students then worked to define their own function that takes in the corresponding input variables and defines a computation graph.\n\nThe lab allows students to experiment with various loss functions, optimization schemes, and even accuracy metrics:\n\nThe fun doesn\u2019t end with building and training an RNN! After all, this lab is about music generation \u2014 what\u2019s left is to use this RNN to actually create new music.\n\nThe lab guides students through feeding the trained model a seed (after all, it can\u2019t predict any new notes without something to start with!), and then iteratively predicting each successive note using the trained RNN. This amounts to randomly sampling from the probability distribution over the next note that\u2019s outputted by the RNN at each time step, and then using these samples to generate a new song.\n\nAs before, we gave students a guided structure for doing this, but defining the sampling was all on them.\n\nTo provide a sampling (pun intended) of generated songs, we went ahead and trained a model, then sampled from it to generate new songs. Listen in on an example generated with a trained model:\n\nAs one can probably tell, there\u2019s a lot of room for improvement here. We wanted students to play around with the skeleton we provided, by tuning hyperparameters, placing priors over the distributions, and augmenting the dataset to generate even sweeter sounding music."
    },
    {
        "url": "https://medium.com/tensorflow/a-gentle-introduction-to-tensorflow-js-dba2e5257702?source=---------9",
        "title": "A Gentle Introduction to TensorFlow.js \u2013 TensorFlow \u2013",
        "text": "Tensorflow.js is a library built on deeplearn.js to create deep learning modules directly on the browser. Using that you can create CNNs, RNNs , etc \u2026 on the browser and train these modules using the client\u2019s GPU processing power. Hence, a server GPU is not needed to train the NN. This tutorial starts by explaining the basic building blocks of TensorFlow.js and the operations on them. Then, we describe how to create some complicated models.\n\nI created this simple demo with the code in Github. Also if you want to play with the code I created an interactive coding session on Observable.\n\nIf you are familiar with deep learning platforms like TensorFlow you should be able to recognize that tensors are n dimensional arrays that are consumed by operators. Hence they represent the building block for any deep learning application. Let us create a scalar tensor\n\nThis created a scalar tensor. We also can convert arrays to tensors\n\nThis creates a constant tensor of the array . In other words we converted the one dimensional array to a tensor by a applying the tensor function. We can use to retrieve the of the tensor.\n\nThis has the shape . We can also create a tensor with specific size. For instance, here we create a tensor of zeros with shape .\n\nIn order to use tensors we need to create operations on them. Let us say we want to find the square of a tensor\n\nThe value of will be . TensorFlow.js also allows chaining operations. For example, to evaluate the 2nd power of a tensor we use\n\nThe tensor will have value .\n\nUsually we generate lots of intermediate tensors. For instance, in the previous example after evaluating we don\u2019t need the value of . In order to do that we call\n\nNote that we can no longer use the tensor in later operations. Now, it might be a little inconvenient to do that for every tensor. Actually, not disposing tensors will be an overhead for the memory. TensorFlow.js offers a special operator to dispose intermediary tensors automatically\n\nNotice that the value of the tensor will be disposed since we don\u2019t need it after we evaluate the value of .\n\nNow that we are done with manipulating tensors using operations. We are ready to create complicated models. First note that since TensorFlow.js uses automatic differentiation using computational graphs we don\u2019t need to evaluate the gradient since it will be evaluated automatically for us. We just need to create the layers, optimizer and compile the model. Let us create a sequential model\n\nNow we can add different layers for the model. Let us add the first convolutional layer with input\n\nHere we created a layer that takes input of size . The input will be a gray image of size . Then we apply kernels of size and stride equals to initialized with . After that, we apply an activation function which basically takes the negative values in the tensor and replaces them with zeros. Now we can add this convlayer to the model\n\nNow what is nice about Tensorflow.js we don\u2019t need to specify the input size for the next layer as it will be evaluated automatically after we compile the model. We can also add max-pooling, dense layers , so on. Here is a simple model\n\nWe can apply a tensor for any layer to inspect the output tensor. But here is a catch the input needs to be of shape where represents the number of dataset elements we apply to the model at a time. Here is an example of how to evaluate a convolutional layer\n\nAfter inspecting the shape of the tensor we see it has shape . This is evaluated using the formula\n\nWhich will result in in our case. Returning to our model we realize that we used which basically convert the input from the shape to the shape . This is important because in the dense layers we cannot apply arrays. Finally, we used the dense layer with output units which represents the number of classes we need in our recognition system. Actually this model is used for the recognizing hand-written digits in the so called MNIST dataset.\n\nAfter creating the model we need a way to optimize the parameters. There are different approaches to do that like SGD and Adam optimizer. For instance, we can create an optimizer using\n\nThis will create an Adam optimizer using the specified learning rate. Now, we are ready to compile the model (attaching the model with the optimizer)\n\nHere we created model that uses Adam to optimize the loss function that evaluates a cross entropy of the predicted output and the true label.\n\nAfter compiling the model we are ready to train the model on a dataset. We need to use the function for that\n\nNote that we are feeding to the fit function a batch of training set. The second variable for the function represents the true labels of the model. Lastly, we have the configuration parameters like the and . Note that represents how many times we iterate over the current batch NOT the whole dataset. Hence we can for example wrap that code inside a loop that iterates over all the batches of the training set.\n\nNote that we used the special keyword which basically blocks and waits for the function to finish executing the code. It is like running a another thread and the main thread is waiting for the fitting function to finish execution.\n\nUsually the given labels are numbers which represents the class. For instance, suppose we have two classes an orange class and an apple class. Then we will give the orange class label and the apple class label . But, our network accepts a tensor of size . Hence we need to use what we call one hot encoding\n\nHence we converted the tensor off labels into a tensor of shape .\n\nIn order to inspect the performance of our model we need to know the loss and the accuracy. In order to do that we need to fetch the results of the model fitting using the history module\n\nNote that we are evaluating the loss and accuracy of the that was an input to the function.\n\nSuppose that we are done with training a model and it gives good loss and accuracy. It is time to predict the results of unseen data element. Suppose we are given an image that is in our browser or we took directly from our webcam, then we can use our trained model to predict its class. First, we need to convert the image into tensor\n\nHere we created a and retrieved from it and then we converted to a tensor. Now the tensor will have size but the model takes 4-dimensional vectors. Hence we need to add an extra dimension for the tensor using\n\nHence the output tensor will have size since we have added a dimension at index . Now for prediction we simply use\n\nThe function will return the value of the last layer in our network usually a activation function.\n\nIn the previous sections we had to train our model from scratch. However, this is an expensive operation since it requires more training iterations. Hence, we use a pretrained model called mobilenet. It is a light-wight CNN that is optimized to run in mobile applications. Mobilenet was trained on ImageNet classes. Basically we have precomputed activations trained on different classes.\n\nTo load the model we use the following\n\nWe can use inputs , outputs to inspect the structure of the model\n\nHence we need images of size and the output will be a tensor of size which holds the probability of each class in the ImageNet dataset.\n\nFor the sake of simplicity we will take an array of zeros and try to predict the class number out of classes\n\nAfter running the code I get class = 21 which represents a kite o:\n\nNow we need to inspect the contents of the model. To do that we can get the models layers and names\n\nWe see that we have layers which would be so expensive to train again on another dataset. Hence, the basic trick is to use this model just to evaluate the activations (we will not retrain) but we will create dense layers that we can train on another number of classes.\n\nFor instance, suppose we need a model to differentiate between carrots and cucumbers. We will use mobilene tmodel to calculate the activation up to some layer we choose. Then we use dense layers with output size to predict the correct class. Hence the model will be in some sense \u2018freezed\u2019 and we just train the dense layers.\n\nFirst, we need to get rid of the dense layers of the model. We choose to extract a random layer let us say number with name\n\nNow let us update our model to have this layer is an output\n\nFinally, we create the trainable model but we need to know the last layer output shape\n\nWe see that the shape Now we can input this to our dense layers\n\nAs you can see we created a dense layer with neurons and the output layer with size .\n\nAnd we can use the previous sections to train the last model using a certain optimizer."
    },
    {
        "url": "https://medium.com/tensorflow/introducing-tensorflow-model-analysis-scaleable-sliced-and-full-pass-metrics-5cde7baf0b7b",
        "title": "Introducing TensorFlow Model Analysis: Scaleable, Sliced, and Full-Pass Metrics",
        "text": "Today we\u2019ve launched TensorFlow Model Analysis (TFMA), an open-source library that combines the power of TensorFlow and Apache Beam to compute and visualize evaluation metrics. Before deploying any machine learning (ML) model, ML developers need to evaluate it to ensure that it meets specific quality thresholds and behaves as expected for all relevant slices of data. Additionally, this computation should seamlessly scale from a small dataset that fits into memory to large, distributed computation. In this post we will give an overview of TFMA and how developers can use it to address all of the aforementioned challenges.\n\nBefore we jump into how TFMA works, we will first look into how it differs from the evaluation metrics that can already be computed and observed in TensorBoard.\n\nTensorFlow metrics that are visualized in TensorBoard are computed during a training run and show how a metric changes against global training steps (across all training workers). This can answer questions such as \u201cIs my model converging?\u201d.\n\nTFMA exports a SavedModel containing the eval graph and additional metadata to compute metrics, which means it computes metrics once using the exported model. It is important to evaluate performance on the final model because that\u2019s the model that will be deployed.\n\nTensorBoard is commonly used to inspect the training progress of a single model. It can also be used to visualize metrics for more than one model, with performance for each plotted against their global training steps as they are training.\n\nTFMA also allows developers to visualize model metrics over time in a time series graph. The difference between TensorBoard and TFMA lies within the horizontal axis. TensorBoard visualizes streaming metrics of multiple models over global training steps, whereas TFMA visualizes metrics computed for a single model over multiple versions of the exported SavedModel.\n\nMost model evaluation results look at aggregate metrics. A model may have an acceptable AUC over the entire eval dataset, but underperform on specific slices. In general, a model with good performance \u201con average\u201d may exhibit failure modes that are not apparent by looking at an aggregate metric.\n\nSlicing metrics allows us to analyze the performance of a model on a more granular level. This functionality enables developers to identify slices where examples may be mislabeled, or where the model over- or under-predicts. For example, TFMA could be used to analyze whether a model that predicts the generosity of a taxi tip works equally well for riders that take the taxi during day hours vs night hours (if sliced by the feature hour).\n\nTensorFlow metrics that are visualized in TensorBoard are commonly computed on a mini-batch basis during training. They are called \u201cstreaming metrics\u201d and are approximations based on those observed mini-batches.\n\nTFMA uses Apache Beam to do a full pass over the specified evaluation dataset. This not only allows more accurate calculation of metrics, but also scales up to massive evaluation datasets, since Beam pipelines can be run using distributed processing back-ends.\n\nNote that TFMA computes the same TensorFlow metrics that are computed by the TensorFlow eval worker, just more accurately by doing a full pass over the specified dataset. TFMA can also be configured to compute additional metrics that were not defined in the model.\n\nFurthermore, if evaluation datasets are sliced to compute metrics for specific segments, each of those segments may only contain a small number of examples. To compute accurate metrics, a deterministic full pass over those examples is important.\n\nWhen designing the API for TFMA, we paid particular attention to the developer workflow and to minimize the amount of information needed to enable this additional functionality. As a result, TFMA simply requires developers to export a separate evaluation graph from the trainer into its own SavedModel. TFMA uses the graph in this SavedModel to compute (sliced) metrics and provides visualization tools to analyze those metrics.\n\nIf developers use TensorFlow Estimators, they are already familiar with exporting a SavedModel for TensorFlow Serving, using the export_savedmodel method. TFMA provides an analogous export_eval_savedmodel method that exports a SavedModel containing the eval metrics and all additional information needed to compute them. The eval_input_fn may also contain analysis-only features, i.e. features that were not trained on but that are used for slicing the eval metrics.\n\nOnce the evaluation SavedModel has been exported, developers can either immediately run TFMA, which will compute the metrics that have been configured in the Estimator, or additionally configure slices they want to have computed. The code example below shows how to configure a feature for slicing.\n\nTFMA uses Apache Beam to evaluate the model using the entire dataset. We use the Beam SDK to define a data-parallel processing pipeline which can be run using several distributed processing back-ends (or runners). On a local machine (or in a notebook), a DirectRunner can be used to run this pipeline locally. One of the major benefits of using the Beam SDK is that the same data processing pipeline can be scaled up on different backends, e.g. in the cloud with the DataflowRunner.\n\nThe render_slicing_metrics call loads the slicing browser component in the Jupyter notebook (see Figure 4) and allows developers to inspect the results of the TFMA eval run.\n\nThe animation in Figure 4 demonstrates a workflow of how to identify a slice with the lowest AUC by first selecting \u201cMetrics Histogram\u201d for visualization, filtering to show only slices with 100 or more examples, selecting the metric \u201cauc\u201d, sorting the table by \u201cauc\u201d and selecting the row with the lowest AUC. In this example it is the slice with the feature value 2 for trip_start_hour (using the model and data from our end-to-end example). Once an interesting slice has been identified, developers can investigate whether this is expected or needs to be mitigated.\n\nWe\u2019ve open-sourced TFMA and published it on GitHub at github.com/tensorflow/model-analysis under the Apache 2.0 License. This release includes everything needed for exporting an evaluation SavedModel, computing sliced metrics using Apache Beam, and visualizing them in a Jupyter notebook.\n\nWe\u2019ve also published an extensive end-to-end example, showcasing how developers can use TensorFlow Transform, TensorFlow Estimators, TensorFlow Model Analysis, and TensorFlow Serving together.\n\nTo help developers get started, we suggest reading and trying out the end-to-end example on GitHub.\n\nAs mentioned in the introduction, developers can use TFMA for evaluating model quality against thresholds, analyzing model behavior on different slices of data, and identifying failure modes that need to be addressed either via data cleaning or improved modeling.\n\nWith this release we make TFMA available to the TensorFlow community to analyze TensorFlow models and improve the quality of models that are deployed. We will soon add the capability of computing metrics that cannot be computed in a TensorFlow graph, e.g. ranking metrics.\n\nIn addition to being able to run TFMA locally (with the DirectRunner) and on Google Cloud (with the DataflowRunner), the Apache Flink and Apache Beam communities are nearing completion of a Flink Runner. Follow the corresponding JIRA ticket, Apache Beam blog, or mailing lists to get notifications about availability of the Flink Runner. The community has also started work on the Spark Runner which will be available in a few months.\n\nAt Google, we use Tensorflow Model Analysis as part of a larger ML platform called TensorFlow Extended (TFX). In continuously running TFX pipelines we use it for automated validation of TensorFlow models and interactive model analysis workflows (e.g. using the slicing browser component shown in Figure 4.) As announced at the TensorFlow Dev Summit, we will be open sourcing more of TFX in the future. Stay tuned to the TensorFlow Blog for an upcoming TFX post, and keep in touch by following TensorFlow on Twitter."
    },
    {
        "url": "https://medium.com/tensorflow/introducing-tensorflow-hub-a-library-for-reusable-machine-learning-modules-in-tensorflow-cdee41fa18f9",
        "title": "Introducing TensorFlow Hub: A Library for Reusable Machine Learning Modules in TensorFlow",
        "text": "One of the things that\u2019s so fundamental in software development that it\u2019s easy to overlook is the idea of a repository of shared code. As programmers, libraries immediately make us more effective. In a sense, they change the problem solving process of programming. When using a library, we often think of programming in terms of building blocks \u2014 or modules \u2014 that can be tied together.\n\nHow might a library look for a machine learning developer? Of course, in addition to sharing code, we\u2019d also like to share pretrained models. Sharing a pretrained model makes it possible for a developer to customize it for their domain, without having access to the computing resources or the data used to train the model originally on hand. For example, NASNet took thousands of GPU-hours to train. By sharing the learned weights, a model developer can make it easier for others to reuse and build upon their work.\n\nIt\u2019s the idea of a library for machine learning developers that inspired TensorFlow Hub, and today we\u2019re happy to share it with the community. TensorFlow Hub is a platform to publish, discover, and reuse parts of machine learning modules in TensorFlow. By a module, we mean a self-contained piece of a TensorFlow graph, along with its weights, that can be reused across other, similar tasks. By reusing a module, a developer can train a model using a smaller dataset, improve generalization, or simply speed up training. Let\u2019s look at a couple examples to make this concrete.\n\nAs a first example, let\u2019s look at a technique you can use to train an image classifier, starting from only a small amount of training data. Modern image recognition models have millions of parameters, and of course, training one from scratch requires a large amount of labeled data and computing power. Using a technique called Image Retraining, you can train a model using a much smaller amount of data, and much less computing time. Here\u2019s how this looks in TensorFlow Hub.\n\nThe basic idea is to reuse an existing image recognition module to extract features from your images, and then train a new classifier on top of these. As you can see above, TensorFlow Hub modules can be instantiated from a URL (or, from a filesystem path) while a TensorFlow graph is being constructed. There are variety of modules on TensorFlow Hub for you to choose from, including various flavors of NASNet, MobileNet (including its recent V2), Inception, ResNet, and others. To use a module, you import TensorFlow Hub, then copy/paste the module\u2019s URL into your code.\n\nEach module has a defined interface that allows it to be used in a replaceable way, with little or no knowledge of its internals. In this case, this module has a method that you can use to retrieve the expected image size. As a developer, you need only provide a batch of images in the correct shape, and call the module on them to retrieve the feature representation. This module takes care of preprocessing your images for you, so you can go directly from a batch of images to a feature representation in a single step. From here, you can learn a linear model, or other type of classifier, on top of these.\n\nIn this case, notice the module we\u2019re using is hosted by Google, and is versioned (so you can rely on the module not changing as you work on your experiments). Modules can be applied like an ordinary Python function to build part of the graph. Once exported to disk, modules are self-contained, and can be used by others without access to the code and data used to create and train it (though of course you can publish those, too).\n\nLet\u2019s take a look at a second example. Imagine you\u2019d like to train a model to classify movie reviews as positive or negative, starting with only a small amount of training data (say, on the order of several hundred positive and negative movie reviews). Since you have a limited number of examples, you decide to leverage a dataset of word embeddings, previously trained on a much larger corpus. Here\u2019s how this looks using a TensorFlow Hub.\n\nAs before, we start by selecting a module. TensorFlow Hub has a variety of text modules for you to explore, including Neural network language models in a variety of languages (EN, JP, DE, and ES), as well as Word2vec trained on Wikipedia, and NNLM embeddings trained on Google News.\n\nIn this case, we\u2019ll use a module for word embeddings. The code above downloads a module, uses it to preprocess a sentence, then retrieves the embeddings for each token. This means you can go directly from a sentence in your dataset to a format suitable for a classifier in a single step. The module takes care of tokenizing the sentence, and other logic like handling out-of-vocabulary words. Both the preprocessing logic and the embeddings are encapsulated in a module, making it easier to experiment with various datasets of word embeddings, or different preprocessing strategies, without having to substantially change your code.\n\nIf you\u2019d like to try this out, use this tutorial to take it for a spin, and to learn how TensorFlow Hub modules work with TensorFlow Estimators.\n\nWe\u2019ve also shared a TensorFlow Hub module for something new! Below is an example using the Universal Sentence Encoder. It\u2019s a sentence-level embedding module trained on a wide variety of datasets (in other words, \u201cuniversal\u201d). Some of the things it\u2019s good at are semantic similarity, custom text classification, and clustering.\n\nAs in image retraining, relatively little labeled data is required to adapt the module to your own task. Let\u2019s try it on a restaurant reviews, for example.\n\nCheck out the this tutorial to learn more.\n\nTensorFlow Hub is about more than image and text classification. On the website, you\u2019ll also find a couple modules for Progressive GAN and Google Landmarks Deep Local Features.\n\nThere are a couple of important considerations when using TensorFlow Hub modules. First, remember that modules contain runnable code. Always use modules from a trusted source. Second, as in all of Machine Learning, fairness is an important consideration. Both of the examples we\u2019ve shown above leverage large pre-trained datasets. When reusing such a dataset, it\u2019s important to be mindful of what data it contains (and whether there are any existing biases there), and how these might impact the product you are building, and its users.\n\nWe hope you find TensorFlow Hub useful in your projects! To get started, head to tensorflow.org/hub. If you run into any bugs, you can file an issue on GitHub. To stay in touch, you can star the GitHub project, and follow TensorFlow on Twitter. Thanks for reading!"
    },
    {
        "url": "https://medium.com/tensorflow/using-tensorflow-lite-on-android-9bbc9cb7d69d",
        "title": "Using TensorFlow Lite on Android \u2013 TensorFlow \u2013",
        "text": "TensorFlow Lite is TensorFlow\u2019s lightweight solution for mobile and embedded devices. It lets you run machine-learned models on mobile devices with low latency, so you can take advantage of them to do classification, regression or anything else you might want without necessarily incurring a round trip to a server.\n\nIt\u2019s presently supported on Android and iOS via a C++ API, as well as having a Java Wrapper for Android Developers. Additionally, on Android Devices that support it, the interpreter can also use the Android Neural Networks API for hardware acceleration, otherwise it will default to the CPU for execution. In this article I\u2019ll focus on how you use it in an Android app.\n\nTensorFlow Lite is comprised of a runtime on which you can run pre-existing models, and a suite of tools that you can use to prepare your models for use on mobile and embedded devices.\n\nIt\u2019s not yet designed for training models. Instead, you train a model on a higher powered machine, and then convert that model to the .TFLITE format, from which it is loaded into a mobile interpreter.\n\nTensorFlow Lite is presently in developer preview, so it may not support all operations in all TensorFlow models. Despite this, it does work with common Image Classification models including Inception and MobileNets. In this article you\u2019ll look at running a MobileNet model on Android. The app will look at the camera feed and use the trained MobileNet to classify the dominant images in the picture.\n\nFor example, in this image I pointed the camera at my favorite coffee mug, and saw that it was primarily classified as a \u2018cup\u2019, and given its shape it\u2019s easy to understand why! It\u2019s also interesting that it has a large, wide, handle which you can see is very teapot-like!\n\nSo how does this work? It\u2019s using a MobileNet model, which is designed and optimized for a number of image scenarios on mobile, including Object Detection, Classification, Facial Attribute detection and Landmark recognition.\n\nThere are a number of variants of MobileNet, with trained models for TensorFlow Lite hosted at this site. You\u2019ll notice that each one is a zip file containing two files \u2014 a labels.txt file that contains the labels that the model is trained for and a .tflite file that contains a version of the model that is ready to be used with TensorFlow Lite. If you want to follow along and build an Android app that uses MobileNets you\u2019ll need to download a model from this site. You\u2019ll see that in a moment.\n\nYou can learn more about TensorFlow Lite in this Video:\n\nTo build an Android App that uses TensorFlow Lite, the first thing you\u2019ll need to do is add the tensorflow-lite libraries to your app. This can be done by adding the following line to your build.gradle file\u2019s dependencies section:\n\nOnce you\u2019ve done this you can import a TensorFlow Lite interpreter. An Interpreter loads a model and allows you to run it, by providing it with a set of inputs. TensorFlow Lite will then execute the model and write the outputs, it\u2019s really as simple as that.\n\nTo use it you create an instance of an Interpreter, and then load it with a MappedByteBuffer.\n\nThere\u2019s a helper function for this in the TensorFlow Lite sample on GitHub. Just ensure that getModelPath() returns a string that points to a file in your assets folder, and the model should load.\n\nThen, to classify an image, all you need to do is call the run method on the Interpeter, passing it the image data and the labels array, and it will do the rest:\n\nGoing into detail on how to grab the image from the camera, and to prepare it for tflite is beyond the scope of this post, but there\u2019s a full sample on how to do it in the tensorflow github. By stepping through this sample you can see how it grabs from the gamera, prepares the data for classification, and handles the output by mapping the weighted output priority list from the model to the labels array.\n\nYou can learn more about building an Android app that uses TensorFlow Lite in this video:\n\nGetting and Running the Android Sample\n\nTo run the sample, make sure you have the full TensorFlow source. You can get it using\n\nOnce you\u2019ve done that, you can open the TensorFlow sample project from the /tensorflow/contrib/lite/java/demo folder in Android Studio:\n\nThe demo file does not include any models, and it expects the mobilenet_quant_v1_224.tflite file, so be sure to download the model from this site. Unzip it and put it in the assets folder.\n\nYou should now be able to run the app.\n\nNote that the app potentially supports both Inception and the Quantized MobileNet. It defaults to the latter, so you need to make sure you have the model present, or the app will fail! The code for capturing data from the camera and converting it into a byte buffer for loading into the model can be found in the ImageClassifier.java file.\n\nThe core of the functionality can be found in the classifyFrame() method in the Camera2BasicFragment.java file:\n\nHere you can see the bitmap is loaded and sized to the appropriate size for the classifier. The classifyFrame() method will then return text containing a list of the top 3 classes that match the image along with their weights.\n\nTensorFlow Lite is still evolving, and you can keep track of it at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite"
    },
    {
        "url": "https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db",
        "title": "Introducing TensorFlow.js: Machine Learning in Javascript",
        "text": "We\u2019re excited to introduce TensorFlow.js, an open-source library you can use to define, train, and run machine learning models entirely in the browser, using Javascript and a high-level layers API. If you\u2019re a Javascript developer who\u2019s new to ML, TensorFlow.js is a great way to begin learning. Or, if you\u2019re a ML developer who\u2019s new to Javascript, read on to learn more about new opportunities for in-browser ML. In this post, we\u2019ll give you a quick overview of TensorFlow.js, and getting started resources you can use to try it out.\n\nRunning machine learning programs entirely client-side in the browser unlocks new opportunities, like interactive ML! If you\u2019re watching the livestream for the TensorFlow Developer Summit, during the TensorFlow.js talk you\u2019ll find a demo where @dsmilkov and @nsthorat train a model to control a PAC-MAN game using computer vision and a webcam, entirely in the browser. You can try it out yourself, too, with the link below \u2014 and find the source in the examples folder.\n\nIf you\u2019d like to try another game, give the Emoji Scavenger Hunt a whirl \u2014 this time, from a browser on your mobile phone.\n\nML running in the browser means that from a user\u2019s perspective, there\u2019s no need to install any libraries or drivers. Just open a webpage, and your program is ready to run. In addition, it\u2019s ready to run with GPU acceleration. TensorFlow.js automatically supports WebGL, and will accelerate your code behind the scenes when a GPU is available. Users may also open your webpage from a mobile device, in which case your model can take advantage of sensor data, say from a gyroscope or accelerometer. Finally, all data stays on the client, making TensorFlow.js useful for low-latency inference, as well as for privacy preserving applications.\n\nIf you\u2019re developing with TensorFlow.js, here are three workflows you can consider.\n\nIf you like, you can head directly to the samples or tutorials to get started. These show how-to export a model defined in Python for inference in the browser, as well as how to define and train models entirely in Javascript. As a quick preview, here\u2019s a snippet of code that defines a neural network to classify flowers, much like on the getting started guide on TensorFlow.org. Here, we\u2019ll define a model using a stack of layers.\n\nThe layers API we\u2019re using here supports all of the Keras layers found in the examples directory (including Dense, CNN, LSTM, and so on). We can then train our model using the same Keras-compatible API with a method call:\n\nThe model is now ready to use to make predictions:\n\nTensorFlow.js also includes a low-level API (previously deeplearn.js) and support for Eager execution. You can learn more about these by watching the talk at the TensorFlow Developer Summit.\n\nAn overview of TensorFlow.js APIs. TensorFlow.js is powered by WebGL and provides a high-level layers API for defining models, and a low-level API for linear algebra and automatic differentiation. TensorFlow.js supports importing TensorFlow SavedModels and Keras models.\n\nGood question! TensorFlow.js, an ecosystem of JavaScript tools for machine learning, is the successor to deeplearn.js which is now called TensorFlow.js Core. TensorFlow.js also includes a Layers API, which is a higher level library for building machine learning models that uses Core, as well as tools for automatically porting TensorFlow SavedModels and Keras hdf5 models. For answers to more questions like this, check out the FAQ.\n\nTo learn more about TensorFlow.js, visit the project homepage, check out the tutorials, and try the examples. You can also watch the talk from the 2018 TensorFlow Developer Summit, and follow TensorFlow on Twitter.\n\nThanks for reading, and we\u2019re excited to see what you\u2019ll create with TensorFlow.js! If you like, you can follow @dsmilkov, @nsthorat, and @sqcai from the TensorFlow.js team on Twitter for updates."
    },
    {
        "url": "https://medium.com/tensorflow/highlights-from-tensorflow-developer-summit-2018-cd86615714b2",
        "title": "Highlights from the TensorFlow Developer Summit, 2018",
        "text": "Posted by Sandeep Gupta, Product Manager for TensorFlow, on behalf of the TensorFlow team.\n\nToday, we\u2019re holding the second TensorFlow Developer Summit at the Computer History Museum in Mountain View, CA! The event brings together over 500 TensorFlow users in-person and thousands tuning into the livestream at TensorFlow events around the world. The day is filled with new product announcements along with technical talks from the TensorFlow team and guest speakers.\n\nMachine learning is solving challenging problems that impact everyone around the world. Problems that we thought were impossible or too complex to solve are now possible with this technology. Using TensorFlow, we\u2019ve already seen great advancements in many different fields. For example:\n\nWe\u2019re excited to see these amazing uses of TensorFlow and are committed to making it accessible to more developers. This is why we\u2019re pleased to announce new updates to TensorFlow that will help improve the developer experience!\n\nResearchers and developers want a simpler way of using TensorFlow. We\u2019re integrating a more intuitive programming model for Python developers called eager execution that removes the distinction between the construction and execution of computational graphs. You can develop with eager execution and then use the same code to generate the equivalent graph for training at scale using the Estimator high-level API. We\u2019re also announcing a new method for running Estimator models on multiple GPUs on a single machine. This allows developers to quickly scale their models with minimal code changes.\n\nAs machine learning models become more abundant and complex, we want to make it easier for developers to share, reuse, and debug them. To help developers share and reuse models, we\u2019re announcing TensorFlow Hub, a library built to foster the publication and discovery of modules (self-contained pieces of TensorFlow graph) that can be reused across similar tasks. Modules contain weights that have been pre-trained on large datasets, and may be retrained and used in your own applications. By reusing a module, a developer can train a model using a smaller dataset, improve generalization, or simply speed up training. To make debugging models easier, we\u2019re also releasing a new interactive graphical debugger plug-in as part of the TensorBoard visualization tool that helps you inspect and step through internal nodes of a computation graph in real-time.\n\nModel training is only one part of the machine learning process and developers need a solution that works end-to-end to build real-world ML systems. Towards this end, we\u2019re announcing the roadmap for TensorFlow Extended (TFX) along with the launch of TensorFlow Model Analysis, an open-source library that combines the power of TensorFlow and Apache Beam to compute and visualize evaluation metrics. The components of TFX that have been released thus far (including TensorFlow Model Analysis, TensorFlow Transform, Estimators, and TensorFlow Serving) are well integrated and let developers prepare data, train, validate, and deploy TensorFlow models in production.\n\nAlong with making TensorFlow easier to use, we\u2019re announcing that developers can use TensorFlow in new languages. TensorFlow.js is a new ML framework for JavaScript developers. Machine learning in the browser using TensorFlow.js opens exciting new possibilities, including interactive ML and support for scenarios where all data remains client-side. It can be used to build and train modules entirely in the browser, as well as import TensorFlow and Keras models trained offline for inference using WebGL acceleration. The Emoji Scavenger Hunt game is a fun example of an application built using TensorFlow.js.\n\nWe also have some exciting news for Swift programmers: TensorFlow for Swift will be open sourced this April. TensorFlow for Swift is not your typical language binding for TensorFlow. It integrates first-class compiler and language support, providing the full power of graphs with the usability of eager execution. The project is still in development, with more updates coming soon!\n\nWe\u2019re also sharing the latest updates to TensorFlow Lite, TensorFlow\u2019s lightweight, cross-platform solution for deploying trained ML models on mobile and other edge devices. In addition to existing support for Android and iOS, we\u2019re announcing support for Raspberry Pi, increased support for ops/models (including custom ops), and describing how developers can easily use TensorFlow Lite in their own apps. The TensorFlow Lite core interpreter is now only 75KB in size (vs 1.1 MB for TensorFlow) and we\u2019re seeing speedups of up to 3x when running quantized image classification models on TensorFlow Lite vs. TensorFlow.\n\nFor hardware support, TensorFlow now has integration with NVIDIA\u2019s TensorRT. TensorRT is a library that optimizes deep learning models for inference and creates a runtime for deployment on GPUs in production environments. It brings a number of optimizations to TensorFlow and automatically selects platform specific kernels to maximize throughput and minimizes latency during inference on GPUs.\n\nFor users who run TensorFlow on CPUs, our partnership with Intel has delivered integration with a highly optimized Intel MKL-DNN open source library for deep learning. When using Intel MKL-DNN, we observed up to 3x inference speedup on various Intel CPU platforms.\n\nThe list of platforms that run TensorFlow has grown to include Cloud TPUs, which were released in beta last month. The Google Cloud TPU team has already delivered a strong 1.6X performance increase in ResNet-50 performance since launch. These improvements will be available to TensorFlow users with the 1.8 release soon.\n\nMany data analysis problems are solved using statistical and probabilistic methods. Beyond deep learning and neural network models, TensorFlow now provides state-of-the-art methods for Bayesian analysis via the TensorFlow Probability API. This library contains building blocks like probability distributions, sampling methods, and new metrics and losses. Many other classical ML methods also have increased support. As an example, boosted decision trees can be easily trained and deployed using pre-made high-level classes.\n\nMachine learning and TensorFlow have already helped solve challenging problems in many different fields. Another area where we see TensorFlow having a big impact is in genomics, which is why we\u2019re releasing Nucleus, a library for reading, writing, and filtering common genomics file formats for use in TensorFlow. This, along with DeepVariant, an open-source TensorFlow based tool for genome variant discovery, will help spur new research and advances in genomics.\n\nThese updates to TensorFlow aim to benefit and grow the community of users and contributors \u2014 the thousands of people who play a part in making TensorFlow one of the most popular ML frameworks in the world. To continue to engage with the community and stay up-to-date with TensorFlow, we\u2019ve launched the new official TensorFlow blog and the TensorFlow YouTube channel.\n\nWe\u2019re also making it easier for our community to collaborate by launching new mailing lists and Special Interest Groups designed to support open-source work on specific projects. To see how you can be a part of the community, visit the TensorFlow Community page and as always, you can follow TensorFlow on Twitter for the latest news.\n\nWe\u2019re incredibly thankful to everyone who has helped make TensorFlow a successful ML framework in the past two years. Thanks for attending, thanks for watching, and remember to use #MadeWithTensorFlow to share how you are solving impactful and challenging problems with machine learning and TensorFlow!"
    },
    {
        "url": "https://medium.com/tensorflow/livestream-the-tensorflow-developer-summit-2018-2cb445660c7f",
        "title": "Livestream the TensorFlow Developer Summit, 2018 \u2013 TensorFlow \u2013",
        "text": "We\u2019re hosting the second annual TensorFlow Developer Summit at the Computer History Museum in Mountain View, California on March 30th, 2018! If you like, you can tune in to the livestream on the TensorFlow YouTube channel. Of course, all the talks will be uploaded to the YouTube channel after the event, in case you can\u2019t watch them live.\n\nDuring the TensorFlow Developer Summit, we\u2019ll be sharing a variety of announcements and improvements to TensorFlow, including:\n\nWe\u2019ll be publishing a few more articles on this blog as well the day of the summit, as well as a recap of all the major announcements. Beginning next week, you can expect to find articles from the TensorFlow community here as well.\n\nYou can follow the announcements throughout the day on twitter.com/tensorflow, and join the conversation on social using #TFDevSummit. Thanks everyone!"
    },
    {
        "url": "https://medium.com/tensorflow/classifying-text-with-tensorflow-estimators-a99603033fbe",
        "title": "Classifying text with TensorFlow Estimators \u2013 TensorFlow \u2013",
        "text": "Welcome to Part 4 of a blog series that introduces TensorFlow Datasets and Estimators. You don\u2019t need to read all of the previous material, but take a look if you want to refresh any of the following concepts. Part 1 focused on pre-made Estimators, Part 2 discussed feature columns, and Part 3 how to create custom Estimators.\n\nHere in Part 4, we will build on top of all the above to tackle a different family of problems in Natural Language Processing (NLP). In particular, this article demonstrates how to solve a text classification task using custom TensorFlow estimators, embeddings, and the tf.layers module. Along the way, we\u2019ll learn about word2vec and transfer learning as a technique to bootstrap model performance when labeled data is a scarce resource.\n\nWe will show you relevant code snippets. Here\u2019s the complete Jupyter Notebook that you can run locally or on Google Colaboratory. The plain source file is also available here. Note that the code was written to demonstrate how Estimators work functionally and was not optimized for maximum performance.\n\nThe dataset we will be using is the IMDB Large Movie Review Dataset, which consists of 25,000 highly polar movie reviews for training, and 25,000 for testing. We will use this dataset to train a binary classification model, able to predict whether a review is positive or negative.\n\nFor illustration, here\u2019s a piece of a negative review (with 2 stars) in the dataset:\n\nKeras provides a convenient handler for importing the dataset which is also available as a serialized numpy array file to download here. For text classification, it is standard to limit the size of the vocabulary to prevent the dataset from becoming too sparse and high dimensional, causing potential overfitting. For this reason, each review consists of a series of word indexes that go from 4 (the most frequent word in the dataset: the) to 4999, which corresponds to orange. Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown (also known as out-of-vocabulary or OOV) tokens. These indexes have been obtained by pre-processing the text data in a pipeline that cleans, normalizes and tokenizes each sentence first and then builds a dictionary indexing each of the tokens by frequency.\n\nAfter we\u2019ve loaded the data in memory we pad each of the sentences with 0 to a fixed size (here: 200) so that we have two 2-dimensional 25000\u00d7200 arrays for training and testing respectively.\n\nThe Estimator framework uses input functions to split the data pipeline from the model itself. Several helper methods are available to create them, whether your data is in a file, or in a , whether it fits in memory or not. In our case, we can use for both the train and test sets.\n\nWe shuffle the training data and do not predefine the number of epochs we want to train, while we only need one epoch of the test data for evaluation. We also add an additional key that captures the length of the original, unpadded sequence, which we will use later.\n\nDatasets can work with out-of-memory sources (not needed in this case) by streaming them record by record, and the method uses a to continuously sample from fixed sized set without loading the entire thing into memory.\n\nIt\u2019s good practice to start any machine learning project trying basic baselines. The simpler the better as having a simple and robust baseline is key to understanding exactly how much we are gaining in terms of performance by adding extra complexity. It may very well be the case that a simple solution is good enough for our requirements.\n\nWith that in mind, let us start by trying out one of the simplest models for text classification. That would be a sparse linear model that gives a weight to each token and adds up all of the results, regardless of the order. As this model does not care about the order of words in a sentence, we normally refer to it as a Bag-of-Words approach. Let\u2019s see how we can implement this model using an .\n\nWe start out by defining the feature column that is used as input to our classifier. As we have seen in Part 2, is the right choice for this pre-processed text input. If we were feeding raw text tokens other could do a lot of the pre-processing for us. We can now use the pre-made .\n\nFinally, we create a simple function that trains the classifier and additionally creates a precision-recall curve. As we do not aim to maximize performance in this blog post, we only train our models for 25,000 steps.\n\nOne of the benefits of choosing a simple model is that it is much more interpretable. The more complex a model, the harder it is to inspect and the more it tends to work like a black box. In this example, we can load the weights from our model\u2019s last checkpoint and take a look at what tokens correspond to the biggest weights in absolute value. The results look like what we would expect.\n\nAs we can see, tokens with the most positive weight such as \u2018refreshing\u2019 are clearly associated with positive sentiment, while tokens that have a large negative weight unarguably evoke negative emotions. A simple but powerful modification that one can do to improve this model is weighting the tokens by their tf-idf scores.\n\nThe next step of complexity we can add are word embeddings. Embeddings are a dense low-dimensional representation of sparse high-dimensional data. This allows our model to learn a more meaningful representation of each token, rather than just an index. While an individual dimension is not meaningful, the low-dimensional space \u2014 when learned from a large enough corpus \u2014 has been shown to capture relations such as tense, plural, gender, thematic relatedness, and many more. We can add word embeddings by converting our existing feature column into an . The representation seen by the model is the mean of the embeddings for each token (see the argument in the docs). We can plug in the embedded features into a pre-canned .\n\nA note for the keen observer: an is just an efficient way of applying a fully connected layer to the sparse binary feature vector of tokens, which is multiplied by a constant depending of the chosen combiner. A direct consequence of this is that it wouldn\u2019t make sense to use an directly in a because two consecutive linear layers without non-linearities in between add no prediction power to the model, unless of course the embeddings are pre-trained.\n\nWe can use TensorBoard to visualize our 50-dimensional word vectors projected into R\u00b3 using t-SNE. We expect similar words to be close to each other. This can be a useful way to inspect our model weights and find unexpected behaviors.\n\nAt this point one possible approach would be to go deeper, further adding more fully connected layers and playing around with layer sizes and training functions. However, by doing that we would add extra complexity and ignore important structure in our sentences. Words do not live in a vacuum and meaning is compositional, formed by words and its neighbors.\n\nConvolutions are one way to take advantage of this structure, similar to how we can model salient clusters of pixels for image classification. The intuition is that certain sequences of words, or n-grams, usually have the same meaning regardless of their overall position in the sentence. Introducing a structural prior via the convolution operation allows us to model the interaction between neighboring words and consequently gives us a better way to represent such meaning.\n\nThe following image shows how a filter matrix F of shape d\u00d7m slides across each 3-gram window of tokens to build a new feature map. Afterwards a pooling layer is usually applied to combine adjacent results.\n\nLet us look at the full model architecture. The use of dropout layers is a regularization technique that makes the model less likely to overfit.\n\nAs seen in previous blog posts, the framework provides a high-level API for training machine learning models, defining , and operations, handling checkpointing, loading, initializing, serving, building the graph and the session out of the box. There is a small family of pre-made estimators, like the ones we used earlier, but it\u2019s most likely that you will need to build your own.\n\nWriting a custom estimator means writing a that returns an . The first step will be mapping the features into our embedding layer:\n\nThen we use to process each output sequentially.\n\nFinally, we will use a to simplify the writing of our last part of the . The head already knows how to compute predictions, loss, train_op, metrics and export outputs, and can be reused across models. This is also used in the pre-made estimators and provides us with the benefit of a uniform evaluation function across all of our models. We will use , which is a head for single label binary classification that uses as the loss function under the hood.\n\nRunning this model is just as easy as before:\n\nUsing the API and the same model , we can also create a classifier that uses a Long Short-Term Memory (LSTM) cell instead of convolutions. Recurrent models such as this are some of the most successful building blocks for NLP applications. An LSTM processes the entire document sequentially, recursing over the sequence with its cell while storing the current state of the sequence in its memory.\n\nOne of the drawbacks of recurrent models compared to CNNs is that, because of the nature of recursion, models turn out deeper and more complex, which usually produces slower training time and worse convergence. LSTMs (and RNNs in general) can suffer convergence issues like vanishing or exploding gradients, that said, with sufficient tuning they can obtain state-of-the-art results for many problems. As a rule of thumb CNNs are good at feature extraction, while RNNs excel at tasks that depend on the meaning of the whole sentence, like question answering or machine translation.\n\nEach cell processes one token embedding at a time and updates its internal state based on a differentiable computation that depends on both the embedding vector x at time t\u200b and the previous state h at time t\u22121\u200b. In order to get a better understanding of how LSTMs work, you can refer to Chris Olah\u2019s blog post.\n\nThe complete LSTM model can be expressed by the following simple flowchart:\n\nIn the beginning of this post, we padded all documents up to 200 tokens, which is necessary to build a proper tensor. However, when a document contains fewer than 200 words, we don\u2019t want the LSTM to continue processing padding tokens as it does not add information and degrades performance. For this reason, we additionally want to provide our network with the length of the original sequence before it was padded. Internally, the model then copies the last state through to the sequence\u2019s end. We can do this by using the feature in our input functions. We can now use the same logic as above and simply replace the convolutional, pooling, and flatten layers with our LSTM cell.\n\nMost of the models that we have shown before rely on word embeddings as a first layer. So far, we have initialized this embedding layer randomly. However, much previous work has shown that using embeddings pre-trained on a large unlabeled corpus as initialization is beneficial, particularly when training on only a small number of labeled examples. The most popular pre-trained embedding is word2vec. Leveraging knowledge from unlabeled data via pre-trained embeddings is an instance of transfer learning.\n\nTo this end, we will show you how to use them in an . We will use the pre-trained vectors from another popular model, GloVe.\n\nAfter loading the vectors into memory from a file we store them as a using the same indexes as our vocabulary. The created array is of shape . At every row index, it contains the 50-dimensional vector representing the word at the same index in our vocabulary.\n\nFinally, we can use a custom initializer function and pass it in the object to our , without any modifications.\n\nNow we can launch TensorBoard and see how the different models we\u2019ve trained compare against each other in terms of training time and performance.\n\nWe can visualize many metrics collected while training and testing, including the loss function values of each model at each training step, and the precision-recall curves. This is of course most useful to select which model works best for our use-case as well as how to choose classification thresholds."
    },
    {
        "url": "https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9",
        "title": "Fitting larger networks into memory. \u2013 TensorFlow \u2013",
        "text": "TLDR; we release the python/Tensorflow package openai/gradient-checkpointing, that lets you fit 10x larger neural nets into memory at the cost of an additional 20% computation time.\n\nGPU memory is often the limiting factor for modern neural network architectures. Memory requirement to train a neural network increases linearly with both network depth and batch-size. You want to go deeper for standard reasons, but also to increase the batch-size to make use of second order methods like KFAC. Such methods need fewer examples to learn compared to mini-batch SGD.\n\nToday, we release a python/Tensorflow package, openai/gradient-checkpointing, that extends the technique in \u201cTraining Deep Nets with Sublinear Memory Cost\u201d, Tianqi Chen et al, to rewrite your TensorFlow model to use less memory. It gives equivalent memory saving for simple feed-forward networks, but it also lets you save memory for general neural networks, such as multi-tower architecture. The package is joint work by Yaroslav Bulatov and Tim Salimans.\n\nApplying it to TensorFlow official CIFAR10 resnet example produces the following memory and execution times for batch size = 1280.\n\nWhile regular backprop scales linearly, this method scales as square root of depth. The difference is more apparent when we try it out for deeper networks.\n\nExtrapolating memory requirement of standard approach gives 60GB of memory to run this iteration, meanwhile memory saving gradients accomplishes it in 6GB of RAM.\n\nComputation overhead is 1 extra forward pass regardless of depth. In experiments this translated to 20% increase in wall-clock time on GTX1080, and 30% increase in wall-clock time on V100 GPU.\n\nTo understand memory requirements of general computation, computer scientists use the concept of the \u201cpebble game\u201d, introduced in \u201cComplete Register Allocation Problems\u201d by Sethi in 1975. Consider the following computation\n\nYou can visualize this computation as a computation graph:\n\nIn order to compute each value, you need to have its dependencies loaded into memory. This is represented by placing \u201cpebbles\u201d on the children of the node. Once all children of a node have pebbles on them, the node is ready for execution. Computing its value is represented by placing a pebble on it:\n\nOnce the value is not needed anymore, you can remove the pebble from the node and re-use it for later computations:\n\nThe goal is to follow the rules of the game to place a pebble on the target node. The number of pebbles needed corresponds to peak memory requirement.\n\nEven for this simple graph, various strategies give different memory requirements. For instance, we can compute the leaves first. That gives following execution strategy, requiring 4 pebbles.\n\nNote that we are computing x4 before it\u2019s needed and keeping its value for a while. We could reduce memory requirement by deferring computation of x4 to a later step.\n\nThis gives us strategy requiring 3 pebbles instead of original 4.\n\nA more pronounced difference happens in a computational graph below:\n\nIf squares correspond to slow operation like matmul and circles correspond to fast operation like random_uniform, computing things as soon as they are ready will require memory proportional to the length of the chain:\n\nAs an alternative, you can defer computations until they are needed, which lets you accommodate arbitrary chain length with 3 units of memory:\n\nComputing values \u201cas soon as possible\u201d (the first strategy) is the default execution strategy used by TensorFlow. You can instruct TensorFlow to compute values \u201cas late as possible\u201d by adding control dependencies. A tool to do this automatically for TensorFlow graphs is linearize.py\n\nFinding the minimum number of pebbles required for general graphs is hard, even approximately, and even if you forbid placing a pebble on any node after it\u2019s been removed (one-shot pebbling). This is shown in \u201cInapproximability of Treewidth, One-Shot Pebbling, and Related Layout Problems.\u201d\n\nFor a simple feed-forward neural network with n layers, gradient computation graph can be pictured below:\n\nSee earlier post \u201cBackprop and systolic arrays\u201d for explanation of where this graph comes from.\n\nUsing pebbling analogy we can visualize the default strategy to compute backprop in this graph.\n\nAt the peak, the algorithm stores all activations, which means O(n) memory requirement for network of depth n. In this case this means 7 units of memory. Unlike the previous example, you can\u2019t save memory by adjusting the order of execution.\n\nYou could instead save memory by forgetting nodes as they are consumed and recomputing them later. This strategy, pictured below, needs 4 units of memory to compute the target.\n\nMore generally, this \u201cmemory-poor\u201d strategy needs O(1) memory but requires O(n\u00b2) computation steps.\n\nA compromise is to save some intermediate results. These saved nodes are called \u201ccheckpoints\u201d in openai/gradient-checkpointing, and can be either selected automatically or provided manually. For the example above, intermediate strategy could be to use the circled node below as a checkpoint\n\nUsing this checkpoint yields a strategy that needs 5 units of memory and has runtime somewhere between memory-poor and default strategies.\n\nFor a chain of length n, generalization of this strategy is to place checkpoints every sqrt(n) steps. This is the most memory efficient strategy if you require that any node is computed at most twice. The memory requirement is O(sqrt(n)), and compute requirement is an additional forward pass. This is the strategy we adopt in our package.\n\nBelow is the summary of memory and compute requirements of these 3 strategies.\n\nIf you monitor memory usage during the evaluation of the gradient in the sqrt(n) strategy, you will see a characteristic zigzag graph like one below.\n\nThe first half of the graph corresponds to the first forward pass and saving the initial checkpoints. Spikes represent recomputation of forgotten activations from each checkpoint. Getting graph of within-step memory usage can be done with yaroslavvb/mem_util package.\n\nWhat happens if you have a more general architecture? IE, suppose you have a resnet architecture like below:\n\nNote that circular nodes are bad candidates for checkpoints. If you save just the circular nodes, your execution time is as bad as the \u201cmemory-poor\u201d strategy for the chain, with O(n\u00b2) computation steps:\n\nFor deep networks we\u2019d like time complexity to stay at O(n). To achieve this, we could take the square nodes as checkpoint candidates and apply the same sqrt(n) selection strategy on this set.\n\nWhat makes square nodes special is that knowing the value of each square node removes the need to recompute any nodes before that node. In graph terminology, these nodes are \u201cgraph separators\u201d \u2014 removing one separates the graph into disjoint subgraphs, \u201cbefore\u201d subgraph and \u201cafter\u201d subgraph.\n\nGraph separators of size 1 are called \u201carticulation points\u201d. This is the class of nodes considered by our package as candidates for checkpoints.\n\nSuppose you have a multi-tower architecture below:\n\nUnlike the previous example, there\u2019s no single node that can serve as a checkpoint candidate. To get any memory saving we must consider nodes in sets of two:\n\nWe currently have not automated the selection of such sets of nodes. To get the sqrt(n) benefit in this kind of architecture with our package, you will need to use a manual checkpoint selection strategy. To use separators pictures in the graph above, you would call the package with the following nodes specified as checkpoints: a, b, c, d.\n\nThis \u201csqrt(n)\u201d strategy is useful in practice, but it\u2019s an arbitrary point on the computation/memory trade-off curve, and a better algorithm would minimize computation time subject to memory budget.\n\nAlso, this algorithm only applies recomputation during backward pass. Architectures with significant branching like openai/pixel-cnn, run out of memory during forward pass. You can apply similar checkpointing idea to pick nodes to recompute during forward pass.\n\nFor instance in the architecture with skip connections we can forget the first node after the middle node is computed, and then compute it again when it\u2019s needed again for the last node.\n\nAn ideal algorithm would choose the nodes to recompute in backward pass or the forward pass to give the smallest runtime subject to memory budget.\n\nThe default strategy is \u201cmemory-rich\u201d: it saves everything that will be needed later, and computes gradient in O(n) memory and O(n) time.\n\nThe \u201cmemory-poor\u201d strategy is to forget each node as soon as it\u2019s consumed and recompute when it\u2019s needed at a later time. It can compute gradient of network of depth n in O(1) memory and O(n\u00b2) time. Example of implementing this strategy in tensorflow is in http://github.com/yaroslavvb/chain_constant_memory/\n\nYou can interpolate between \u201csave everything\u201d and \u201csave nothing\u201d above by saving some nodes in an optimal way. This is analyzed in detail in Chapter 12 of \u201cEvaluating derivatives: principles and techniques of algorithmic differentiation\u201d, Griewank A., Walther A, 2nd ed. More recently this technique was applied in Memory-Efficient Backpropagation Through Time.\n\nThe idea of the approach is to use dynamic programming to find the most optimal computation schedule.\n\nBuilding block of DP solution is the algorithm which keeps the first activation in memory and computes the target backprop as fast as possible with memory budget M.\n\nTo see how it breaks into smaller parts, suppose the set of nodes checkpointed by this algorithm contained node i. Then this algorithm could be decomposed into parts Left/Right as follows:\n\n2. Given Ai, Bn, compute Bi with budget M-M0. M0 is memory cost of first activation which needs to be subtracted since \u201cLeft\u201d is keeping the first activation in memory.\n\nIf the memory budget is too small to save any nodes, then there\u2019s only choice of strategy \u2014 the O(n\u00b2) memory poor strategy. This is the base case for divide and conquer.\n\nYou can go over all choices of \u2018i\u2019 as potential nodes to generate initial split, and call the algorithm recursively on individual splits.\n\nTensorFlow lets you you obtain computation times of individual nodes from the same timeline structure as used by the mem_util package, so you can use this information to obtain an optimal schedule based on empirical values of execution times.\n\nDynamic programming algorithm above is formulated for chain graphs, how could it be extended to general computation graphs?\n\nOur checkpoints have to be graph separators in order to reuse recomputation. The algorithm above works on chains because in a chain, every node is a separator. There\u2019s an extension of the dynamic programming approach above to work on trees by using the same divide-and-conquer idea: saving any node in a tree-structured computation graph splits the graph into \u201cbefore\u201d and \u201cafter\u201d parts. The extra complication is that you need to know the order in which to compute the children of each node. If the degree is small enough, a simple exact approach is to try all orders. A heuristic approach is to assume that the most memory-hungry child runs first. An efficient exact strategy was developed in the context of sparse linear systems in \u201cAn Application of Generalized Tree Pebbling to Sparse Matrix Factorization\u201d by J. Liu.\n\nFor a general computation graph, things are harder. However, note that typical neural network architectures are \u201cpretty close to a tree\u201d. For a multi-tower architecture above, we can merge nodes from parallel paths into sets of nodes, such that resulting graph is a tree.\n\nNow that we have a tree, every node in this representation is a checkpoint candidate and we can apply divide-and-conquer approach over this representation.\n\nIn a general graph we use a similar technique known as tree decomposition that works to successively merge sets of nodes until the result is a tree. In tree decomposition, every merged node, also known as bag is a separator of the graph.\n\nFor this representation to help with memory saving, merged nodes have to be small. In the worst case you may keep merging nodes and won\u2019t get a tree until everything is merged into a single bag.\n\nThis doesn\u2019t happen with neural network examples above which are \u201cpretty close to a tree.\u201d To make this precise, our example graphs have tree decompositions where where largest bag has size k some small k. The number k is the treewidth of a graph.\n\nIf the treewidth is small, separators are small, and you can recursively split your search for optimal solution into tractable sub-problems.\n\nMany hard problems on graphs can be solved in polynomial time once tree-width is bounded. This includes the problem of determining the optimal tree decomposition, and the problem of finding optimal pebbling strategy.\n\nMore generally such problems are known as fixed-parameter tractable, or FPT, where the fixed parameter is treewidth. Courcelle\u2019s Theorem gives a precise characterization of a set of problems which are FPT for treewidth. A generic algorithm for solving a problem on graphs given its tree decomposition is given in section 5.3.1.2 of Miriam Heinz \u201cTree-Decomposition: Graph Minor Theory and Algorithmic Implications\u201d\n\nYou can combine these features to get a net with larger, but still bounded, treewidth. IE, if you have a neural network which uses k towers, skip connections in each tower going back k steps and k global statistics, it will have O(k\u00b2) treewidth.\n\nFinding optimal tree decomposition for a bounded treewidth graph has a polynomial time algorithm discovered by Bodlaender. Unfortunately this algorithm is not practical, similar to other Galactic Algorithms.\n\nIn practice, heuristics find tree-decompositions that are good enough. One particular heuristic is a \u201cmin-fill decomposition\u201d which requires a single pass over the graph, and works well for sparsely connected graphs.\n\nI\u2019ve prototyped min-fill heuristic tree decomposition here. The code gives results you see above for toy networks as well as reasonable results for real computational graphs.\n\nIt produces the following tree structure:\n\nA more advanced algorithm might combine these ideas to find an optimal execution strategy for a general neural network without any input from the user."
    },
    {
        "url": "https://medium.com/tensorflow/interactive-supervision-with-tensorboard-9a101c91d3f7",
        "title": "Interactive supervision with TensorBoard \u2013 TensorFlow \u2013",
        "text": "The TensorBoard projector features t-distributed Stochastic Neighborhood Embedding (t-SNE) for visualizing high-dimensional datasets, since it is a well-balanced dimensionality reduction algorithm that requires no labels yet reveals latent structure in many types of data. What happens when t-SNE can use partial labeling to recreate pairwise similarities in a lower dimensional embedding?\n\nIBM Research AI implemented semi-supervision in TensorBoard t-SNE and contributed components required for interactive supervision to demonstrate cognitive-assisted labeling. A metadata editor, distance metric/space selection, neighborhood function selection, and t-SNE perturbation were added to TensorBoard in addition to semi-supervision for t-SNE. These components function in concert to apply a partial labeling that informs semi-supervised t-SNE to clarify the embedding and progressively ease the labeling burden.\n\nAvailable sample class labels can be used to calculate the Bayesian priors of Leland et al. [1], which can be applied to high-dimensional similarities to promote greater attraction between same-label pairs. The attractive and repulsive forces in weighted t-SNE are balanced with a connection scalar according to Yang et al. [1], but we normalize the gradient size by dividing with the sum of prior probabilities and leaving the repulsion normalization unaffected.\n\nThe general effect is, predictably, that same-label samples form tighter and combined clusters, which effectively clears space in the embedding that highlights outliers and unlabeled points. This may incrementally reduce the user difficulty in applying labels to a dataset, as the embedding progressively becomes organized into compact clusters. t-SNE is extremely useful in providing an initial view of the data structure, but then supervision can be injected into its objective and iterative gradient descent can compose a user perspective of the data.\n\nImposing additional constraints by supervising t-SNE could make it harder to escape local optima, which is required e.g. to join two separated same-label clusters, especially when the Barnes-Hut approximation localizes attractive forces. Also, labeling becomes harder when same-label clusters collapse, so a method is required to kick the embedding out of its local optimum.\n\nWe propose random walks for points to perturb t-SNE, by iteratively applying independent offsets within small hyperspheres to a user-specified extent. The perturb function can be applied at any time, which can help to reduce sprite occlusion so that selections can be refined or to join separated same-label clusters.\n\nMetadata in TensorBoard provide information on tensors, such as a class label for each sample. Now it is possible to edit existing metadata in TensorBoard, which effectively allows for labels to be applied to selected samples. The Projector switches into a metadata context when the user starts labeling, which shows a label histogram that helps to quickly identify and apply a desired label.\n\nPreviously, only cosine and Euclidean metrics in the high-dimensional input space were available to select neighborhoods. These distance metrics have been expanded to include use in the PCA and t-SNE embedding spaces, which is required for multi-sample labeling in the semi-supervised setting.\n\nGeodesic neighborhood selection is proposed to grab smaller clusters based on discontinuities disregarded by k-nearest neighbor selection. Geodesic neighborhoods are calculated in a greedy approximate manner and normally provides good multi-sample labeling prospects.\n\nHow many interactions are required to obtain a sufficient labeling for an image dataset like EMNIST Letters (26 classes) or CIFAR-100 (100 classes)?\n\nLabeling datasets is normally a very time-consuming, unenviable task, but one that usually cannot be escaped. Labeling facilitates the use of supervised machine learning, but why not use machine learning to facilitate minimum supervision labeling? Of course, transfer learning, zero-shot or one-shot learning could be used to circumvent the need for labels all together, but these rely on assumptions that will typically not hold for most real-world data.\n\nProvided labels can also be explicitly used to train a feature extractor and classifier that is able to make increasingly confident label recommendations. Recognize however how t-SNE can present an initial view to the user that is amenable to clustering, and that the single global objective function is harnessed to help solve the minimum supervision problem in an elegant and self-contained manner, adhering to the philosophy of simplicity.\n\nEMNIST Letters is a 26-class dataset with 411,302 samples for which a 85.15% accuracy is achieved with an OPIUM-based classifier [3], though we use only about 2000 stratified samples for the labeling exercise. This is a good dataset to demonstrate labeling on, as the sample images are small, familiar and easily distinguishable by the human eye. The bottleneck thus becomes the labeling system, and the challenge is to learn as much from every human click/keypress so as to require the least number of interactions to obtain a decent labeled sample size for every class.\n\nThe above image shows a snippet of a longer labeling session, sped up by 4x. It turns out that a lot of interactions are required and that labeling really is a painful task! However, it is clear that the clarification provided by semi-supervised t-SNE in conjunction with geodesic neighborhood selection definitely increases the labels/interaction efficiency. It often joins disparate samples into its membership cluster upon being labeled, so it clears up the embedding and it becomes easier to notice and handle unlabeled samples.\n\nThe SETI Institute commandeers a formidable radio telescope, called the Allen Telescope Array, which listens to the night sky in the hopes of detecting ET signals. Unfortunately, most signals come from human-made sources and are unwanted interference that have to be filtered out. There are however natural categories of RFI appearing in millions of captured signal events, and it would be much easier to take out the noise if it can be accurately classified.\n\nWe represent signals as small square images that are depictions of spectrograms, or a time-vs-frequency plot that can explain the frequency content and possible nature of the signal. So now if we can visualize signals, we can use TensorBoard interactive labeling to good effect as sample similarity can easily be seen which makes it easy to delineate good clusters.\n\nSome 14 million archived measurements have been processed with spectral feature extraction followed by autoencoding to generate a balanced sample of 2000 measurements possessing a good diversity of signal activity. In the above video we inspect these samples with TensorBoard and progressively label geodesic clusters with user-defined terms.\n\nRemaining unlabeled samples can be explored as possible anomalies which may require follow-up measurements. You will notice some strange looking signals in the latter part of the video.\n\nNote the utility provided by semi-supervised t-SNE in assisting the labeling process:\n\nFrom the above demonstrations it is thus conceivable that the labeling process can be simplified by harnessing a global weighted objective that is solved iteratively with gradient descent. The obvious limitation here is that points have to move through the embedding and with the Barnes-Hut approximation it becomes very difficult for separated same-label clusters to agglomerate for a perfect clustering to be obtained. Future work may consider alternative approaches to make better use of labeling to elegantly obtain the best clusters.\n\n[2] Zhirong Yang, Jaakko Peltonen, and Samuel Kaski. \u201cOptimization equivalence of divergences improves neighbor embedding\u201d. International Conference on Machine Learning. 2014.\n\n[3] Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr\u00e9 van Schaik. \u201cEMNIST: an extension of MNIST to handwritten letters.\u201d arXiv preprint arXiv:1702.05373 (2017)."
    }
]