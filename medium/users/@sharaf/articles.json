[
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-27-learning-to-explore-with-meta-policy-gradient-ed19c81c7e7e?source=user_profile---------1----------------",
        "title": "A Paper A Day: #26 Learning to Explore with Meta-Policy Gradient",
        "text": "This is a short review for the recently published work on learning exploration by Tianbing Xu, Qiang Liu, Liang Zhao, Wei Xu, Jian Peng.\n\nThe paper proposes a new algorithm for learning to do exploration in off-policy reinforcement learning algorithms. The family of off-policy RL algorithms includes the Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG) algorithms. Although the experiments conducted only includes results for DDPG.\n\nInstead of using a simple heuristic for doing exploration like epsilon-greedy or using independent Gaussian noise, we will use an additional separate policy to select exploration data. This new separate policy will be trained using on-policy gradient methods (e.g. PPO or TRPO \u2014 see below). The reward for training this policy will be the relevant improvement in the performance of the exploitation policy network.\n\nExperimental results show that using this extra exploration policy helps in faster convergence of DDPG leading to higher rewards and better sample complexity on six RL environments with continuous action spaces.\n\nAuthors highlight the difference between two different types of policy gradient reinforcement learning approaches:\n\nThe proposed work focuses only on exploration for off-policy algorithms.\n\nThe proposed approach is shown in the figure above. The essence of the approach is in lines 5\u20138. An extra policy for exploration is maintained, we sample exploration trajectories from this extra exploration policy (line 5), we pretend as if we used these trajectories to update the exploitation policy (line 6) and measure the relative improvement which is used as a reward signal (line 7). The reward signal is used to updated the exploration policy in a REINFORCE like fashion (line 8). The exploration trajectories collected on line 5 server another purpose, they are also augmented in the replay buffer and used to update the exploitation policy as well.\n\nExperiment where conduced on six continuous action RL enviroments: InvertedDoublePendulum-v1, InvertedPendulum-v1, Hopper-v1, Pendulum-v0, HalfCheetah-v1, Reacher-v1. The results are shown on the figure below. Meta-Policy Gradient outperforms DDPG on five out of the six environments.\n\nIt\u2019s not clear how exactly the number of steps (sample complexity) is counted. In particular it\u2019s not mentioned whether these steps also count the extra interactions the exploration policy uses to get the estimate for the quality of the generated exploration trajectories. For a fare comparison with DDPG the total number of interactions with the environment should be fixed. It\u2019ll also be nice to see results on discrete actions spaces and compare to Q-learning algorithms like DQN."
    },
    {
        "url": "https://medium.com/@sharaf/learning-algorithms-for-active-learning-776a8ca62908?source=user_profile---------2----------------",
        "title": "Learning Algorithms for Active Learning \u2013 Amr Sharaf \u2013",
        "text": "This is a discussion for the \u201cLearning Algorithms for Active Learning\u201d paper by Philip Bachman, Alessandro Sordoni, and Adam Trischler.\n\nThe paper proposes an end-to-end approach for learning an active learning heuristic using meta-learning. In a nutshell, the model uses the accuracy on a held-out evaluation dataset as a reward signal for learning active learning heuristics. The model jointly learns the following components:\n\nThe model is trained end-to-end using a policy gradient RL algorithm for tuning the active learning heuristic, and back propagation for learning the classifier and input representation.\n\nThe proposed architecture borrows heavily from the Matching Networks architecture by Vinyals et al. Experiments show the effective of the proposed approach on an image classification task for the Omniglot dataset, and for collaborative filtering on the MovieLens dataset.\n\nIn active learning, a model selects which instances to label so as to maximize some combination of task performance and data efficiency.\n\nIn an abstract level, the goal of meta-learning is to swap-out a hard coded procedure that outputs models and replace that with a trainable model for performing the same procedure.\n\nIn many cases, we might have historic labeled data that has been collected before, but we still want to label more samples, either to make the model better, or to support predictions for new categories / classes.\n\nThis is the case for example in collaborative filtering, where we might have a recommendation system for recommending movies for users. In this setting, it makes sense to leverage the rating provided by users for the system to learn an active learning strategy to collect ratings from new users that have just joined the system and haven\u2019t interacted with the system at all.\n\nThe model metalearns by attempting to actively learn on tasks sampled from a distribution over tasks, using supervised feedback to improve its expected performance on new tasks drawn from a similar distribution. The model solves each task by adaptively selecting items for the labeled support set used by a Matching Network (Vinyals et al., 2016) to classify test items.\n\nDuring training, for computational efficiency, the model maximizes the following objective function:\n\nwhere R \u0303 is a prediction reward for item in the support set, and R is the prediction reward for item in the evaluation set defined as:\n\nThe parameters of the model are optimized using a combination of backpropagation and policy gradients. The gradient of the objective function can be written as:\n\nPolicy gradient using Generalized Advantage Estimation is used for learning the model parameters.\n\nThe model comprises multiple modules: context-free and context-sensitive encoding, controller, selection, reading, fast prediction, and slow prediction.\n\nThe context-free encoder associates each item with an embedding independent of the context in which the item was presented. For the Omniglot dataset, this is computed using a convnet, for the MovieLens data, a lookup table is used to embed every movie ID into a fixed size movie embedding. The embedding is learned by factorizing the rating matrix.\n\nThe context-sensitive encoder over all context-free embeddings for the support set, and then add a linear function of the concatenated forwards and backwards states.\n\nThis module concatenates the embedding and label for the item indicated by the selection module, and linearly transforms them before passing them to the controller.\n\nAt each time step, the controller receives an input from the reading module which encodes the most recently read item/label pair. Additional inputs could take advantage of task-specific information. The control module performs a standard LSTM update.\n\nAt each time step, the selection module places a distribution over all unlabeled items. It then samples the index of an item to label from the distribution, and feeds it to the reading module.\n\nOmniglot of Lake et al. is a MNIST-like scribbles dataset with 1623 characters with 20 examples each. The large number of classes (characters) with relatively few data per class (20), makes this an ideal data set for testing small-scale one-shot classification.\n\nThe MovieLens dataset contains approximately 20M ratings on 27K movies by 138K users. The ratings are on an ordinal 10-point scale, from 0.5 to 5 with intervals of 0.5."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-25-hierarchical-attention-networks-for-document-classification-dd76ba88f176?source=user_profile---------3----------------",
        "title": "A Paper A Day: #25 Hierarchical Attention Networks for Document Classification",
        "text": "Today we discuss a paper by Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy for document classification using hierarchical attention networks.\n\nThe paper proposes a hierarchical attention network for document classification. The model has two distinctive characteristics:\n\nExperiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\nThe overall architecture of the Hierarchical Attention Network (HAN) is shown in the figure below. It consists of several parts: a word sequence encoder, a word-level attention layer, a sentence encoder and a sentence-level attention layer.\n\nAssume that a document has L sentences Si and each sentence contains Ti words. The proposed model projects the raw document into a vector representation, on which a classifier is built to perform document classification.\n\nWord Encoder: Given a sentence with words Wit , t \u2208 [0, T], the words are embedded to vectors through an embedding matrix We. A bidirectional GRU is used to get annotations of words by summarizing information from both directions for words, and therefore incorporate the contextual information in the annotation.\n\nWord Attention: Not all words contribute equally to the representation of the sentence meaning. Hence, an attention mechanism is used to extract such words that are important to the meaning of the sentence and the representation of those informative words is then aggregated to form a sentence vector.\n\nSentence Attention: to reward sentences that are clues to correctly classify a document, again an attention mechanism is used and a sentence level context vector is used to measure the importance of a sentence.\n\nThe effectiveness of the model is evaluated on six large scale document classification data sets. These data sets can be categorized into two types of document classification tasks: sentiment estimation and topic classification. 80% of the data is used for training, 10% for validation, and the remaining 10% for test.\n\nExperimental results demonstrate that the model performs significantly better than previous methods. Visualization of these attention layers illustrates that our model is effective in picking out important words and sentences."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-24-attention-is-all-you-need-26eb2da90a91?source=user_profile---------4----------------",
        "title": "A Paper A Day: #24 Attention Is All You Need \u2013 Amr Sharaf \u2013",
        "text": "Today we discuss Google brain\u2019s paper by Ashish Vaswani, Noam Shazeer, Llion Jones, Niki Parmar, Jakob Uszkoreit, Aidan N. Gomez, and \u0141ukasz Kaiser for training sequence-2-sequence models based solely on attention mechanisms.\n\nStandard sequence-2-sequence models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. This paper proposes a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. The model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, the model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. Experiments show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\nThe Transformer follows the overall architecture for a standard encoder-decoder model, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n\nThe Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of the figure below, respectively.\n\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. A residual connection is employed around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d-model = 512.\n\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, residual connections are employed around each of the sub-layers, followed by layer normalization. The self-attention sub-layer in the decoder stack is also modified to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n\nPositional Encoding: Since the model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, \u201cpositional encodings\u201d are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d-model as the embeddings, so that the two can be summed. There are many choices of positional encodings, in this work, sine and cosine functions of different frequencies are used.\n\nMulti-Head Attention: Instead of performing a single attention function with d-model-dimensional keys, values and queries, authors found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dq, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values the attention function is then performed in parallel, yielding dv-dimensional output values. These are concatenated, resulting in the final values.\n\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, the model achieves a new state of the art. In the former task the best model outperforms even all previously reported ensembles."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-23-cross-lingual-transfer-of-semantic-role-labeling-models-e55094a46f4e?source=user_profile---------5----------------",
        "title": "A Paper A Day: #23 Cross-lingual Transfer of Semantic Role Labeling Models",
        "text": "Today we discuss a paper by Mikhail Kozhevnikov and Ivan Titov for cross-lingual transfer of semantic role labeling models.\n\nThe paper addresses the problem of transferring a Semantic Role Labeling (SRL) model from one language to another using a shared feature representation. This approach is then evaluated on three language pairs, demonstrating competitive performance as compared to a state-of-the-art unsupervised SRL system and a cross-lingual annotation projection baseline. Experiments also consider the contribution of different aspects of the feature representation to the performance of the model.\n\nA number of approaches to the construction of semantic role labeling models for new languages have been proposed:\n\nLearning a cross-lingual model requires the engineering of language agnostic shared feature representations. McDonald et al. (2011) successfully apply this idea to the transfer of dependency parsers, using part-of-speech tags as the shared representation of words. A later extension of Ta\u0308ckstro\u0308m et al. (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance.\n\nIn the case of SRL, authors hypothesize that a shared representation that is purely syntactic is likely to be insufficient, since structures with different semantics may be realized by the same syntactic construct, for example \u201cin August\u201d vs \u201cin Britain\u201d. However with the help of recently introduced cross-lingual word representations, such as the cross-lingual clustering or cross-lingual distributed word representations of Klementiev et al. (2012), we may be able to transfer models of shallow semantics in a similar fashion.\n\nIn this work, For each word, the following set of feature representations is used: its part-of-speech tag, cross-lingual cluster ID, word identity (glossed, when evaluating on the target language) and its dependency relation to its parent. Features associated with an argument word include the attributes of the predicate word, the argument word, its parent, siblings and children, and the words directly preceding and following it. Also included are the sequences of part-of-speech tags and dependency relations on the path between the predicate and the argument.\n\nWhile annotation projection approaches require sentence and word-aligned parallel data and crucially depend on the accuracy of the syntactic parsing and SRL on the source side of the parallel corpus, cross-lingual model transfer can be performed using only a bilingual dictionary.\n\nThe authors considered the cross-lingual model transfer approach as applied to the task of semantic role labeling and observed that for closely related languages it performs comparably to annotation projection approaches. It allows one to quickly construct an SRL model for a new language without manual annotation or language-specific heuristics, provided an accurate model is available for one of the related languages along with a certain amount of parallel data for the two languages."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-22-massively-multilingual-word-embeddings-ced1c68045d2?source=user_profile---------6----------------",
        "title": "A Paper A Day: #22 Massively Multilingual Word Embeddings",
        "text": "Today we discuss a paper by Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A. Smith for learning massively multilingual word embeddings.\n\nThe paper introduces new methods for estimating and evaluating embeddings of words in more than fifty languages in a single shared embedding space. These estimation methods use dictionaries and monolingual data; they do not require parallel data. The paper also presents a new evaluation method, multiQVEC-CCA, that is shown to correlate better than previous ones with two downstream tasks: text categorization and parsing.\n\nShared representation of words across languages offers intriguing possibilities. For example, in machine translation, translating a word never seen in parallel data may be overcome by seeking its vector-space neighbors, provided the embeddings are learned from both plentiful monolingual corpora and more limited parallel data. A second opportunity comes from transfer learning, in which models trained in one language can be deployed in other languages. While previous work has used hand-engineered features that are cross-linguistically stable, automatically learned embeddings offer the promise of better generalization at lower cost.\n\nThe paper proposes two dictionary-based methods \u2014 multiCluster and multiCCA \u2014 for estimating multilingual embeddings which only require monolingual data and pairwise parallel dictionaries, and uses them to train embeddings in 59 languages for which these resources are available. Parallel corpora are not required but can be used when available.\n\nIn this approach, the problem is decomposed into two simpler subproblems: E = E embed \u25e6 E cluster, where E cluster : L \u00d7 V \u2192 C deterministically maps words to multilingual clusters C, and E embed : C \u2192 Rd assigns a vector to each cluster. A bilingual dictionary is used to find clusters of translationally equivalent words, then distributional similarities of the clusters are used in monolingual corpora from all languages in L to estimate an embedding for each cluster.\n\nmultiCCA extends the bilingual embeddings of Faruqui and Dyer (2014). First, they use monolingual corpora to train mono-lingual embeddings for each language independently, capturing semantic similarity within each language separately. Then, using a bilingual dictionary, they use canonical correlation analysis (CCA) to estimate linear projections from the ranges of the monolingual embeddings yielding a bilingual embedding.\n\nIn this work, a simple extension is used to construct multilingual embeddings for more languages. The vector space of the initial (monolingual) English embeddings is selected to serve as the multilingual vector space (since English typically offers the largest corpora and wide availability of bilingual dictionaries). Projections from the monolingual embeddings of the other languages are estimated into the English space.\n\nThe QVEC evaluation metric (Tsvetkov et al., 2015) is adapted to evaluating multilingual embeddings (multiQVEC). In addition, a new evaluation method multiQVEC-CCA was developed, which addresses a theoretical shortcoming of multiQVEC. Compared to other intrinsic metrics used in the literature, both multiQVEC and multiQVEC-CCA achieve better correlations with extrinsic tasks."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-21-memory-networks-b2fb86929a91?source=user_profile---------7----------------",
        "title": "A Paper A Day: #21 Memory Networks \u2013 Amr Sharaf \u2013",
        "text": "Today we discuss the memory networks paper by Jason Weston, Sumit Chopra, and Antoine Bordes.\n\nThe paper presents a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component. The long-term memory can be read and written to, with the goal of using it for prediction. Experiments evaluate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base.\n\nThe central idea is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.\n\nA memory network consists of a memory m and four (potentially learned) components I, G, O and R as follows:\n\nGiven an input x (e.g., an input character, word or sentence depending on the granularity chosen, an image or an audio signal) the flow of the model is as follows:\n\nThis process is applied at both train and test time, if there is a distinction between such phases, that is, memories are also stored at test time, but the model parameters of I, G, O and R are not updated.\n\nOne possible instantiation of the four components I, G, O, and R for a QA task could be:\n\nI component: Component I can make use of standard pre-processing, e.g., parsing, coreference and entity resolution for text inputs. It could also encode the input into an internal feature representation, e.g., convert from text to a sparse or dense feature vector.\n\nG component: The simplest form of G is to store I(x) in a \u201cslot\u201d in the memory: m(x; H(x)) = I(x)\n\nwhere H(.) is a function selecting the slot. That is, G updates the index H(x) of m, but all other parts of the memory remain untouched.\n\nO and R components: The O component is typically responsible for reading from memory and performing inference, e.g., calculating what are the relevant memories to perform a good response. The R component then produces the final response given O. For example in a question answering setup O finds relevant memories, and then R produces the actual wording of the answer, e.g., R could be an RNN that is conditioned on the output of O.\n\nIn the basic QA architecture, the I module takes an input text in the form of a sentence. The text is stored in the next available memory slot in its original form. The G module is thus only used to store this new memory, so old memories are not updated.\n\nThe core of inference lies in the O and R modules. The O module produces output features by finding k supporting memories given x.\n\nFinally, R needs to produce a textual response r. To perform true sentence generation, one can instead employ an RNN for this task. In experiments, the authors also consider an easy to evaluate compromise approach where textual responses are limited to be a single word.\n\nAn example task is given in the figure below:\n\nIn order to answer the question x = \u201cWhere is the milk now?\u201d, the O module first scores all memories, i.e., all previously seen sentences, against x to retrieve the most relevant fact, mo1 = \u201cJoe left the milk\u201d in this case. Then, it would search the memory again to find the second relevant fact given [x, mo1], that is mo2 = \u201cJoe travelled to the office\u201d (the last place Joe went before dropping the milk). Finally, the R module would score words given [x, mo1 , mo2] to output r = \u201coffice\u201d.\n\nTraining is done in a fully supervised setting where we are given desired inputs and responses, and the supporting sentences are labeled as such in the training data (but not in the test data, where we are given only the inputs). Training is then performed with a margin ranking loss and stochastic gradient descent (SGD).\n\nExperiments investigate memory network models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. Evaluation is done on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, experiments show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-20-improving-hypernymy-detection-with-an-integrated-path-based-and-distributional-71f7bd07d73b?source=user_profile---------8----------------",
        "title": "A Paper A Day: #20 Improving Hypernymy Detection with an Integrated Path-based and Distributional\u2026",
        "text": "Today we discuss a paper by Vered Shwartz, Yoav Goldberg, and Ido Dagan for hypernymy detection with an integrated path-based and distributional methods.\n\nThe task of hypernymy detection is defined as follows: given a term-pair (x, y), determine whether y is a hypernym of x, based on their occurrences in a large corpus. For example, in the sentence: \u201cTom Cruise is an actor\u201d, we want to detect that \u201cactor (y)\u201d is a hypernym of \u201cTom Cruise (x)\u201d.\n\nFor a couple of decades, this task has been addressed by two types of approaches:\n\nContributions in this paper are multi-fold:\n\nThe figure below shows the proposed architecture for the hypernym detection task, named HypeNet. Each term-pair is represented by several paths. Each path is a sequence of edges, and each edge consists of four components: lemma, POS, dependency label and dependency direction. Each edge vector is fed in sequence into the LSTM, resulting in a path embedding vector. The averaged path vector becomes the term-pair\u2019s feature vector, used for classification. The dashed vectors refer to the integrated network.\n\nNeural networks typically require a large amount of training data, whereas the existing hypernymy datasets are relatively small. Therefore, authors followed the common methodology of creating a dataset using distant supervision from knowledge resources. Aiming to create a larger dataset, hypernymy relations were extracted from several resources: WordNet, DBPedia, Wiki-data, and Yago.\n\nHypeNET was compared with several state-of-the- art methods for hypernymy detection: path-based methods (snow and PATTY\u2019s generalized paths), and distributional methods (unsupervised and supervised approaches).\n\nResults demonstrated the superior performance of HypeNet due to the increase in recall as a result of generalizing semantically-similar paths, in contrast to prior methods, which either make no generalizations or over-generalize paths.\n\nExtending the network by integrating distributional signals yields an improvement of additional 14 F1 points, and demonstrate that the path-based and the distributional approaches are indeed complementary.\n\nThe code is available on GitHub."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-19-a-multichannel-convolutional-neural-network-for-cross-language-dialog-state-bb00f5328163?source=user_profile---------9----------------",
        "title": "A Paper A Day: #19 A Multi-Channel Convolutional Neural Network For Cross-Language Dialog State\u2026",
        "text": "I\u2019m posting two summaries today to make up for the writeup I missed a couple of days ago as I was catching a paper deadline.\n\nThis is a discussion for a paper on cross-language dialog state tracking by Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi Yamagami and Noriaki Horii.\n\nThe goal in dialog state tracking is to transfer human utterances into a slot-value representation (dialog state) that is easy for computers to process and track the information that appeared in the dialog. To provide a common testbed for this task, a series of Dialog State Tracking Challenges (DSTC) was initiated. This paper presents a system description for a model used in the fifth DSTC challenge.\n\nThe fifth Dialog State Tracking Challenge (DSTC5) introduces a new cross-language dialog state tracking scenario, where the participants are asked to build their trackers based on the English training corpus, while evaluating them with the unlabeled Chinese corpus.\n\nAlthough the computer-generated translations for both English and Chinese corpus are provided in the dataset, these translations contain errors and careless use of them can easily hurt the performance of the built trackers.\n\nTo address this problem, a multichannel Convolutional Neural Network (CNN) architecture was presented, in which English and Chinese language are treated as different input channels of one single CNN model.\n\nThe dialog state in this challenge is defined by an ontology which contains 5 topic branches with different slot sets. These topic-slot combinations indicate the most important information mentioned in that topic, for example the \u2018CUISINE\u2019 slot under the topic of \u2018FOOD\u2019 refers to the cuisine types, while the \u2018STATION\u2019 slot for the topic of \u2018TRANSPORTATION\u2019 refers to the train stations. In total there are 30 such topic-slot combinations, and all possible values for each topic-slot are given as a list in the ontology. The main task of DSTC5 is to predict the proper value(s) for each slot given the current utterance and its topic, with all dialog history prior to that turn.\n\nThe architecture is inspired by the multichannel convolutional neural networks commonly used in image processing. Instead of RGB channels used for color images, each input channel is applied to each different language source. In this model, the input of each channel is a two dimensional matrix, each row of which is the embedding vector of the corresponding word:\n\nIn the evaluation of DSTC5, the authors found that such multichannel architecture can effectively improve the robustness against translation errors. This multichannel model is found to be robust against the translation errors and outperformed any of the single channel models."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-18-cross-lingual-spoken-language-understanding-from-unaligned-data-using-2d668eb7320b?source=user_profile---------10----------------",
        "title": "A Paper A Day: #18 Cross-Lingual Spoken Language Understanding from Unaligned Data using\u2026",
        "text": "Today we discuss a paper by Fabrice Lefevre, Franc\u0327ois Mairesse, and Steve Young for cross-lingual SLU from unaligned data using discriminative classifiers and machine translation models.\n\nThe paper proposes to circumvent the costly operation of developing a statistical SLU module for a new language by bootstrapping an existing module from a source language to a target language, assuming the availability of semantically-annotated utterances in the source language. The paper proposes to combine an SMT system with a data-driven SLU method to bootstrap an SLU component in the target language, while maintaining a high semantic accuracy.\n\nFour different ways were proposed for SLU using SMT:\n\nExperiments were performed in which English SLU and SMT components are combined to bootstrap a French SLU module from a large set of semantically annotated English utterances (i.e., source language = English, target language = French).\n\nResults show that training on automatically translated data produced the best performance for predicting the utterance\u2019s dialogue act type, however individual slot/value pairs are best predicted by training STCs on the source language and using them to decode translated utterances. Overall, results show that good performance can be achieved by training the SMT system on a relatively small parallel corpus."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-17-accurate-large-minibatch-sgd-training-imagenet-in-1-hour-c8d82b7afa74?source=user_profile---------11----------------",
        "title": "A Paper A Day: #17 Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
        "text": "Today we discuss the recently published paper by Facebook AI Research (FAIR) for accurate, large mini-batch training of ImageNet. The authors list include: Priya Goyal, Piotr Dolla \u0301r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.\n\nTraining state-of-the-art deep learning models typically requires training on web scale data using large deep architectures. This leads to long experiment cycles that impede research and development progress.\n\nThe paper uses a distributed synchronous SGD optimization technique to leverage parallelism in the training process and improve the runtime efficiency. However, for the distributed SGD algorithm to be effective, the per-worker load must be large, leading to a non-trivial growth in the mini-batch size required for training.\n\nThe authors show that training an ImageNet model using large mini-batches lead to optimization challenges that if not addressed properly will lead to a decrease in the model accuracy. More importantly, they propose two simple techniques that enable training ImageNet using large mini-batches, while still maintaining classification performance at par with the models trained using smaller batch sizes.\n\nThe goal in this paper is to use large minibatches in place of small minibatches while maintaining training and generalization accuracy. Empirical experiment shows that the following learning rate scaling rule is surprisingly effective for a broad range of minibatch sizes:\n\nNo formal theoretical justification was provided, but extensive empirical experiments show the effectiveness of this linear update rule for selecting the learning rate. An informal discussion was also provided (see section 2.1). In addition, authors refer to the theoretical justification from the recent review paper by Bottou et al. [4] (section 4.2) showing that with the linear scaling rule, solvers follow the same training curve when having seen the same number of examples.\n\nFor large minibatches (e.g., 8k) the linear scaling rule breaks down when the network is changing rapidly, which commonly occurs in early stages of training. Authors found that this issue can be alleviated by a properly designed warmup, namely, a strategy of using less aggressive learning rates at the start of training. Two kinds of warmup techniques were proposed: constant warmup, and gradual warmup."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-16-dual-learning-for-machine-translation-d2e8b2bf2721?source=user_profile---------12----------------",
        "title": "A Paper A Day: #16 Dual Learning for Machine Translation",
        "text": "Today we discuss a paper by Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma for machine translation using dual learning.\n\nThe paper presents a new approach for using monolingual data to improve machine translation models. The approach is based on the interactive reinforcement learning scenario described below:\n\nThe idea is very simple, we learn two language models for the source and target languages from the monolingual data, and we already have access to the translation models, we collect two kinds of reward signals: \u201clanguage model reward\u201d measuring the fluency of the generated sentences, and \u201ccommunication reward\u201d assessing the quality of the machine translation system. Using these rewards, we use any policy gradient method (for example REINFORCE) to update the model parameters. The detailed algorithm is shown below.\n\nExperiments were conducted on English-French and French-English datasets. Results are shown in the table below. Experiments show that dual-NMT works very well on English\u2194French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-15-cross-lingual-models-of-word-embeddings-an-empirical-comparison-6d133dc02380?source=user_profile---------13----------------",
        "title": "A Paper A Day: #15 Cross-lingual Models of Word Embeddings: An Empirical Comparison",
        "text": "Today we discuss a paper by Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and Dan Roth. The paper presents an empirical comparison of different cross-lingual models of word embeddings.\n\nThe paper includes an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typologically different language pairs. The evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on down- stream semantic and syntactic applications.\n\nThe paper evaluates four different approaches for inducing cross-lingual embeddings, each requiring a different form of cross-lingual supervision:\n\nResults show that models which require expensive cross-lingual knowledge (word alignments or sentence alignments) almost always perform better on semantic tasks, but cheaply supervised models (such as context agnostic translation dictionary) often prove competitive on certain syntactic tasks.\n\nThe quality of the induced cross-lingual word embeddings was measured in terms of their performance, when used as features in the following tasks:\n\nThe authors summarize their main findings and conclusions as follows:\n\nInstructions and code to reproduce the experiments available here."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-14-a-copy-augmented-sequence-to-sequence-architecture-gives-good-performance-on-44727e880044?source=user_profile---------14----------------",
        "title": "A Paper A Day: #14 A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on\u2026",
        "text": "Today we discuss a paper by Mihail Eric and Christopher D. Manning for task-oriented dialogue using sequence-to-sequence architectures.\n\nTask-oriented dialogue focuses on conversational agents that participate in user-initiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse.\n\nThe paper presents a class of neural models for task-oriented dialogue that is able to outperform other more intricately designed neural architectures on a number of metrics. The main idea is to combine a standard sequence-to-sequence model with an attention based copy mechanism that allows the model to extract and use relevant entities in its response, without requiring intermediate supervision of dialogue state or belief tracker modules.\n\nThe appeal of the proposed model comes from the simplicity and effectiveness of framing system response generation as a sequence-to-sequence mapping with a soft copy mechanism over relevant context. Unlike the task-oriented dialogue agents of Wen et. al (2016b), this architecture does not explicitly model belief states or KB slot-value trackers, and preserves full end-to-end-trainability.\n\nThe proposed architecture is a standard seq2seq model combined with an attention based copy mechanism. An effective task-oriented dialogue system must have powerful language modeling capabilities and be able to pick up on relevant entities of an underlying knowledge base. To this end, the authors augment the attention encoder-decoder model with an attention based copy mechanism in the style of (Jia and Liang, 2016). During decoding, the combined attention scores of the encoder hidden states are used as additional input for the decoder, and a new logits vector is computed after the augmentation, thus the model either predicts a token from the output vocabulary or softly copies a token from the encoder input context based on the logit scores.\n\nThe proposed model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on the DSTC2 dataset."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-13-batch-learning-from-logged-bandit-feedback-through-counterfactual-risk-fcdc78ce20f1?source=user_profile---------15----------------",
        "title": "A Paper A Day: #13 Batch Learning from Logged Bandit Feedback through Counterfactual Risk\u2026",
        "text": "Today we discuss a paper by Adith Swaminathan and Thorsten Joachims for batch learning from logged bandit feedback.\n\nThis paper develops a learning principle and an efficient algorithm for batch learning from logged bandit feedback. In the bandit feedback setting, an algorithm makes a prediction (e.g., ad ranking) for a given input (e.g., query) and observes bandit feedback (e.g., user clicks on presented ads) for only this prediction, but no other.\n\nIn many systems, we may have access to offline / historic log data, collected from a deployed exploration model and the objective is to build a better target model by taking advantage of the observed bandit feedback.\n\nThis problem is hard for at least two reasons. First, the logged data is biased, since it was collected by an exploration model and the goal is to learn a better target model. Second, we have partial information, since we only observe the feedback for the predictions we observed, and not for other outputs we could have possibly predicted but they were not displayed to the user.\n\nIn order to solve this problem, we need to deal with the counterfactual question of what would have been the observed bandit feedback if the target model had control over the logged predictions. This is known as the counterfactual learning problem. This paper addresses the counterfactual nature of the learning problem through propensity scoring and proves generalization error bounds that account for the variance of the propensity-weighted empirical risk estimator.\n\nIn analogy to the Structural Risk Minimization principle of Vapnik and Tscherwonenkis (1979), these bounds give rise to the Counterfactual Risk Minimization (CRM) principle. CRM is a general principle for learning from logged bandit data, and experiments in this paper show how it can be used effectively to solve a multi-labeling structured prediction problem.\n\nThe table below identifies the main differences between supervised learning and bandit learning. The main difference being that in bandit learning, we suffer from dataset bias and partial observability, and it\u2019s important to account for stochastic policies in the hypothesis class, since actions exploration is fundamental in this setting. In addition, for practical bandit learning from log data we need a data-dependent regularizer that guarantees robust learning and decrease the variance in the risk estimator.\n\nFor designing machine learning methods for batch learning from bandit feedback, theoretical analysis shows that a learning algorithm should jointly optimize the empirical risk estimate R^(h) as well as its empirical standard deviation, where the latter serves as a data-dependent regularizer for reducing the variance of the risk estimator. The authors call this learning principle the Counterfactual Risk Minimization (CRM) Principle.\n\nThe main takeaways are summarized in the paper\u2019s conclusion as follows:"
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-12-doubly-robust-policy-evaluation-and-learning-7e6a09665d7f?source=user_profile---------16----------------",
        "title": "A Paper A Day: #12 Doubly Robust Policy Evaluation and Learning",
        "text": "Today we discuss a paper by Miroslav Dudik, John Langford, and Lihong Li for doubly robust policy evaluation and learning.\n\nIn this paper, we are dealing with contextual bandits where in each round we observe a vector of contexts x \u2208 X, we choose an action (or arm) a \u2208 A, and we observe a reward r \u2208 [0,1] for the given action, but we don\u2019t observe rewards for any other arms. Contexts are chosen IID from an unknown distribution D(x) and the rewards depend on the actions and contexts according to unknown distribution D(r|a, x).\n\nWe have also done some offline learning according to an exploration policy \u03bc, which has generated a history of zk = (x1, a1, r1, . . . , xk, ak, rk). Given input data zn, we want to do policy evaluation on a stationary policy v. Stationary means that the policy\u2019s choice of action only depends on the context, not on the previous history. Policy evaluation means we wish to estimate the expected reward of the policy.\n\nThe paper presents different approaches for off-policy evaluation. These approaches are discussed below.\n\nThe direct Method (DM) and relies on having an estimate r\u02c6(x, a) of the expected reward given a context and action. The estimate of the reward distribution will have to come from the provided exploration history, and once we have that then we can estimate V(v) as follows:\n\nThe problem with this approach is that the estimation of the rewards is based on the exploration policy,\n\nwhich may not align well with the policy we\u2019re evaluating. If they are highly dissimilar, then the estimation may be poor for the arms that v is likely to play.\n\nIPS relies on estimates \u03bc\u02c6k(a|xk) which is an estimate of how likely the exploration policy is to choose action a given context xk and the history up to time k. Then the estimate of V(v) is:\n\nThe advantage of this approach is that it\u2019s usually easier to provide estimates of the exploration policy, \u03bc. The disadvantage is that the variance of V\u02c6IPS can potentially be very large if \u03bc is very small.\n\nWe would like to combine the two approaches to get an estimator that has good theoretical guarantees. This will use both the estimates of the rewards and the estimates of the exploration policy. This is called the doubly robust estimate, which is defined as:\n\nThe paper also proves bounds on the error and variance of the three different estimation techniques. In addition, since we have an approach for evaluating any policy offline, the paper shows how to use policy optimization techniques to learn a better policy from historic log data. We will discuss the policy optimization problem in a separate episode."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-11-pointer-networks-59f7af1a611c?source=user_profile---------17----------------",
        "title": "A Paper A Day: #11 Pointer Networks \u2013 Amr Sharaf \u2013",
        "text": "Today we discuss the pointer networks paper by Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n\nPointer networks are sequence-to-sequence models where the output is discrete tokens corresponding to positions in an input sequence. The main difference between pointer networks and standard seq2seq models is two-fold:\n\nPointer networks are suitable for problems like sorting, word ordering, or computational linguistic problems such as convex hulls and traveling sales person problems. One common characteristic for all these problems is that the size of the target dictionary varies depending on the input length.\n\npointer network solves the problem of variable size output dictionaries using a mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output.\n\nThe authors summarize the main contributions in this paper as follows:\n\nThe figure above shows the pointer networks architecture. A traditional sequence-to-sequence model uses a softmax distribution over a fixed sized output dictionary to compute p(Ci|C1, . . . , Ci\u22121, P). Thus it cannot be used for problems where the size of the output dictionary is equal to the length of the input sequence. To solve this problem pointer networks model p(Ci|C1, . . . , Ci\u22121, P) using an attention mechanism as follows:\n\nwhere softmax normalizes the vector ui (of length n) to be an output distribution over the dictionary of inputs, and v, W1, and W2 are learnable parameters of the output model. In pointer networks, we do not blend the encoder state ej to propagate extra information to the decoder, but instead, use uij as pointers to the input elements.\n\nExperiments show that Ptr-Nets can be used to learn solutions to three different combinatorial optimization problems. Ptr-nets work on variable sized inputs (yielding variable sized output dictionaries), something the baseline models (sequence-to-sequence with or without attention) cannot do directly. Even more impressively, they outperform the baselines on fixed input size problems \u2014 to which both models can be applied."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-10-latent-anaphora-resolution-for-cross-lingual-pronoun-prediction-2d720d523a59?source=user_profile---------18----------------",
        "title": "A Paper A Day: #10 Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction",
        "text": "Today we discuss another paper for anaphora resolution by Christian Hardmeier, Jo\u0308rg Tiedemann, and Joakim Nivre. This paper addresses the task of predicting the correct French translations of third-person subject pronouns in English discourse. The paper presents an approach based on neural networks that models anaphoric links as latent variables and show that its performance is competitive with that of a system with separate anaphora resolution while not requiring any coreference-annotated training data. This demonstrates that the information contained in parallel bitexts can successfully be used to acquire knowledge about pronominal anaphora in an unsupervised way.\n\nThe task considered in this paper is much simpler than the general anaphora resolution task discussed in yesterday\u2019s paper. The task considered in this paper is: we are given an English discourse containing a pronoun along with its French translation and word alignments between the two languages, which were computed automatically using a standard SMT pipeline with GIZA++ (Och and Ney, 2003). The task is limited to the four English third-person subject pronouns he, she, it and they. The output of the classifier is a multinomial distribution over six classes: the four French subject pronouns il, elle, ils and elles, corresponding to masculine and feminine singular and plural, respectively; the impersonal pronoun ce/c\u2019, which occurs in some very frequent constructions such as c\u2019est (it is); and a sixth class OTHER, which indicates that none of these pronouns was used.\n\nThe authors experimented with two different test sets. The TED data set consists of around 2.6 million tokens of lecture subtitles released in the WIT3 corpus (Cettolo et al., 2012). The WIT3 training data yields 71,052 examples, which were randomly partitioned into a training set of 63,228 examples and a test set of 7,824 examples. The official WIT3 development and test sets were not used in these experiments.\n\nThe news-commentary data set is version 6 of the parallel news-commentary corpus released as a part of the WMT 2011 training data1. It contains around 2.8 mil- lion tokens of news text and yields 31,017 data points, which were randomly split into 27,900 training examples and 3,117 test instances.\n\nIn all experiments, BART was used to predict anaphoric links for pronouns. The model used with BART is a maximum entropy ranker trained on the ACE02-npaper corpus. In order to obtain a probability distribution over antecedent candidates rather than one-best predictions or coreference sets, the ranking component with which BART resolves pronouns was modified to normalize and output the scores assigned by the ranker to all candidates instead of picking the highest-scoring candidate.\n\nIn order to create a simple, but reasonable baseline for this task, a maximum entropy (ME) classifier was trained with the MegaM software package (Hal Daum\u00e9 III \u2014 2007) . The baseline results show an overall higher accuracy for the TED data than for the news- commentary data. While the precision is above 50% in all categories and considerably higher in some, recall varies widely.\n\nA simple multi-class maximum entropy classifier, while making correct predictions for much of the data set, has a significant bias towards making majority class decisions, relying more on prior assumptions about the frequency distribution of the classes than on antecedent features when handling examples of less frequent classes. In order to create a system that can be trained to rely more explicitly on antecedent information, a neural network classifier was used for this task. The introduction of a hidden layer should enable the classifier to learn abstract concepts such as gender and number that are useful across multiple output categories, so that the performance of sparsely represented classes can benefit from the training examples of the more frequent classes.\n\nThe overall structure of the network is shown in the figure below. As inputs, the network takes the same features that were available to the baseline ME classifier, based on the source pronoun (P) with three words of context to its left (L1 to L3) and three words to its right (R1 to R3) as well as the words aligned to the syntactic head words of all possible antecedent candidates as found by BART (A).\n\nConsidering the figure above, we note that the bilingual setting of the classification task adds some information not available to the monolingual anaphora resolver that can be helpful when determining the correct antecedent for a given pronoun. Knowing the gender of the translation of a pronoun limits the set of possible antecedents to those whose translation is morphologically compatible with the target language pronoun. We can exploit this fact to learn how to resolve anaphoric pronouns without requiring data with manually annotated anaphoric links.\n\nTo achieve this, the neural network was extended with a component to predict the probability of each antecedent candidate to be the correct antecedent. The extended network is identical to the previous version except for the upper left part dealing with anaphoric link features. The only difference between the two networks is the fact that anaphora resolution is now performed by a part of the neural network itself instead of being done by an external module and provided to the classifier as an input.\n\nThis paper introduced cross-lingual pronoun prediction as an independent natural language processing task. Even though it is not an end-to-end task, pronoun prediction is interesting for several reasons. In this work, it was shown that pronoun prediction can be effectively modeled in a neural network architecture with relatively simple features. More importantly, the authors have demonstrated that the task can be exploited to train a classifier with a latent representation of anaphoric links. With parallel text as its only supervision this classifier achieves a level of performance that is similar to, if not better than, that of a classifier using a regular anaphora resolution system trained with manually annotated data."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-9-anaphora-resolution-with-pointer-networks-96a00be3e38c?source=user_profile---------19----------------",
        "title": "A Paper A Day: #9 Anaphora Resolution With Pointer Networks",
        "text": "Today we discuss a paper by Changki Leea, Sangkeun Jungb, and Cheon-Eum Parka for anaphora resolution with pointer networks. The paper was published in Pattern Recognition Letters, were it was received in March 2016, and appeared online few days ago in May 2017.\n\nWords or phrases that point to other words or phrases are anaphors. The term anaphora denotes the actual act of pointing. Any time a given expression (word, phrase, or some combination of words) points to some linguistic expression (its antecedent), it is an anaphor. The paper\u2019s introduction highlights the importance of anaphora resolution:\n\nThis paper proposes a Ptr-Net model based anaphora resolution. The main contributions are as follows:\n\nThe figure below shows an example of the encoding and the decoding process of anaphora resolution with Ptr-Net. Input tokens from the first anaphora to the <EOS> symbol are fed to the en- coding layer, and the corresponding mention chains are generated from the decoding layer.\n\nThe ETRI anaphora resolution dataset was used to test the proposed model. This dataset consists of 721 quiz-type question documents, which contain 583 mention entities. The dataset is divided into 537, 73 and 111 documents for the training, development (dev) and test (test) set, respectively. State-of-the art results were reported using the ptr-network architecture."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-8-logarithmic-time-one-against-some-3470504f1010?source=user_profile---------20----------------",
        "title": "A Paper A Day: #8 Logarithmic Time One-Against-Some",
        "text": "Today we discuss a paper by Hal Daume \u0301 III, Nikos Karampatziakis, John Langford, and Paul Mineiro for extreme multi-class classification in logarithmic time.\n\nThe paper proposes and approach for multi-class prediction when the number of classes K is extremely large. The proposed approach is an online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes.\n\nThe introduction summarizes the main contributions for this work as follows:\n\nThe recall tree algorithm uses an OAA-like structure to make a final prediction, but instead of scoring every class, only a small subset of O(logK) classes are scored. This procedure is called \u201cone-against-some\u201d (OAS). How can we efficiently determine what classes should be scored? For this, a dynamically built tree is constructed to efficiently whittle down the set of candidate classes. The goal of the tree is to maximize the recall of the candidate set, hence the name \u201cRecall Tree.\u201d\n\nThe figure below depicts the inference procedure for the Recall Tree: an example is routed through a tree until termination, and then the set of eligible classes compete to predict the label.\n\nThe training procedure is a little bit more involved. Each node in the tree maintains a set of statistics. First, each node n maintains a router, denoted fn, that maps an example to either a left or right child. This router is implemented as a binary classifier. Second, each node maintains a histogram of the labels of all training examples that have been routed to, or through, that node. This histogram is used in two ways:\n\nThis is a crucial issue with trees because a child node sees fewer data than its parent. Therefore we do not simply rely on the empirical recall (i.e. the observed fraction of labels that fall into the most frequent F labels at this node) of a node since such estimate can have considerable variance at deep nodes. Instead, a lower bound of the true recall is used. This lower bound is computed via an empirical Bernstein inequality.\n\nExperiments were conducted on several large scale multi-class classification datasets. Results show that the recall tree algorithm can compete with one-against-all in both space and predictive power while offering exponential improvements in speed when the number of classes is large."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-7-learning-as-search-optimization-20650e533cfd?source=user_profile---------21----------------",
        "title": "A Paper A Day: #7 Learning as Search Optimization \u2013 Amr Sharaf \u2013",
        "text": "Today we discuss a paper by Hal Daum\u00e9 III and Daniel Marcu for structured prediction using approximate large margin methods.\n\nStructured prediction is a machine learning task in which the goal is to jointly predict the values of a collection of variables that interact. Classic examples are part of speech tagging of text (labels of adjacent words correlate), machine translation (the output is an English sentence) and image parsing (neighboring superpixels have related labels).\n\nStructured prediction problems are typically learned using extensions of classification algorithms. One possible extension for such classification algorithms is to use a tractable probabilistic graphical model, for example, a linear-chain conditional random field (CRF). Another possible extension is to use a variant of large margin methods, for example, Structured Support Vector Machines (SVM). Under such tractability assumptions, the model parameters can be estimated efficiently either using maximum likelihood optimization (for CRF), or by solving a quadratic optimization problem (for Struct-SVM), however, under this setting, we\u2019re solving an approximate problem using an exact method rather than solving the original problem we care about. In many cases, such approximations are not possible, and may lead to poor performance.\n\nThis paper embraces the difficulty of structured prediction problems and treat the structured output problem in terms of approximate search. The paper presents a framework for learning structured prediction models using search optimization. The paper also presents two parameter update techniques for learning, together with convergence theorems and bounds.\n\nIn this section, we describe how to phrase the structured prediction learning problem in terms of search. Most AI texts contain a definition of the search problem and a general search algorithm; this paper works with that from Russell and Norvig (1995). A search problem is a structure containing four fields: states (the world of exploration), operators (transitions in the world), goal states (a subset of states of interest) and path cost (computes the cost of a path). The paper defines a general framework for search as follows:\n\nUnder this formulation, the goal of learning, is to produce an enqueue function that places good hypotheses high on the queue and bad hypotheses low on the queue. The model proposed by this paper assumes that the enqueue function is based on two components: a path component g and a heuristic component h, and that the score of a node will be given by g + h. This formulation includes A* search when h is an admissible heuristic, heuristic search when h is inadmissible, best-first search when h is identically zero, and any variety of beam search when a queue is cut off at a particular point at each iteration. The model assumes that h is given and that g is a linear function of features of the input x and the path to and including the current node, n: g = w\u2019 \u03a6(x, n), where \u03a6(\u00b7, \u00b7) is the vector of features.\n\nThe main question now is how to learn the parameter weights w effectively. First the Learning as Search Optimization algorithms modifies the search procedure in two ways:\n\nTwo different updates for the weight vector are proposed: perceptron updates, and large margin updates. Theoretical justification is provided for both techniques.\n\nLaSO was applied to two tasks: a simple syntactic chunking task for which exact search is possible (to allow for com- parison to exact learning and decoding methods) and a joint tagging / chunking task for which exact search is intractable. experiments shows that LaSO can outperform exact models at smaller computational cost."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-6-unsupervised-domain-adaptation-by-backpropagation-c004f0d9d9f3?source=user_profile---------22----------------",
        "title": "A Paper a Day: #6 Unsupervised Domain Adaptation by Backpropagation",
        "text": "Today we discuss another paper for unsupervised domain-adaptation. We discuss a paper from ICML 2015 by Yaroslav Ganin and Victor Lempitsky for unsupervised domain adaptation by backpropagation.\n\nFirst, we discuss why domain-adaptation is an important problem in its own right. The motivation behind domain-adaptation is well presented in few sentences in the abstract for today\u2019s paper:\n\nIn many cases, we don\u2019t even have the data to train a decent machine learning model. For example, consider a virtual assistant system like Alexa, Cortana, or Google Home, yesterday\u2019s paper describes a use-case where domain adaptation is necessary:\n\nToday\u2019s paper proposes a new approach to domain adaptation in neural networks that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target domain data is necessary).\n\nIn any domain adaptation problem, we have to learn features that are:\n\nFor a neural machine learning model, the first set of features can be learned by using a suitable network architecture and optimizing a suitable loss function. Today\u2019s paper describes an approach for the second set of features (features invariant with respect to the shift between domains) using a standard adversarial training together with a reverse gradient layer. The resulting augmented architecture can be trained end-to-end using standard back- propagation.\n\nThe figure below shows the proposed architecture for domain adaptation. The architecture includes a deep feature extractor (green) and a deep label predictor (blue), which together form a standard feed-forward architecture. Unsupervised domain adaptation is achieved by adding a domain classifier (red) connected to the feature extractor via a gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation based training. Otherwise, the training proceeds in a standard way and minimizes the label prediction loss (for source examples) and the domain classification loss (for all samples). Gradient reversal ensures that the feature distributions over the two domains are made similar (as indistinguishable as possible for the domain classifier), thus resulting in the domain-invariant features."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-5-adversarial-adaptation-of-synthetic-or-stale-data-4719673093ca?source=user_profile---------23----------------",
        "title": "A Paper a Day: #5 Adversarial Adaptation of Synthetic or Stale Data",
        "text": "Today we discuss another paper for domain adaptation by Young-Bum Kim, Karl Stratos, and Dongchan Kim. in Episode #3, we discussed a domain adaptation technique based on data selection. This paper however, uses an adversarial training approach for domain adaptation.\n\nThe paper focuses on two different types of domain adaptation problems: transferring from synthetic data to live user data (a deployment shift), and transferring from stale data to current data (a temporal shift). Both cause a distribution mismatch between training and evaluation, leading to a model that overfits the flawed training data and performs poorly on the test data.\n\nThis paper uses and builds on several recent advances in neural domain adaptation such as adversarial training and domain separation network, proposing a new effective adversarial training scheme. In both supervised and unsupervised adaptation scenarios.\n\nThe proposed model is based on an extension to the architecture proposed by Ganin et al. (2016) for adversarial training methods for unsupervised domain adaptation. They partition the model parameters into two parts: one inducing domain specific (or private) features and the other domain invariant (or shared) features. The domain invariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic.\n\nThe paper proposes approaches for both supervised and non-supervised adaptation. The architecture makes heave use of BiLSTMs for encoding feature relationships. The architecture has three BiLSTM encoders\n\nThe main idea is to define suitable loss functions for each of these encoder to learn the source-specific, target-specific, and domain-invariant features. These loss function include:\n\nI\u2019ll leave out the details for each of these loss functions, but the name enough should be self explanatory. The model is trained jointly used the above five loss functions, and can be extended to handle the supervised domain adaptation case.\n\nExperiments consider two possible domain adaptation (DA) scenarios:\n\nIn both cases, the approach yields clear improvement over strong baselines."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-4-embedding-methods-for-fine-grained-entity-type-classification-406fd8461?source=user_profile---------24----------------",
        "title": "A Paper A Day: #4 Embedding Methods for Fine Grained Entity Type Classification",
        "text": "Today we look into a paper by Dani Yogatama, Dan Gillick, and Nevena Lazic for fine grained entity type classification.\n\nEntity type classification is the task of assigning type labels (e.g. person, location, organization) to mentions of entities in documents. These types are useful for downstream applications such as co-reference resolution, knowledge base construction, and question answering.\n\nThe task of fine grained entity type classification is concerned with labelling such types on a much larger set of fine grained labels. Fine grained labels are typically sub-types of the coarse grained ones, thus, fine-grained labels form a hierarchical like structure.\n\nWhile the output of interest for the fine-grained entity classification task is highly structured (since labels form a hierarchical graph), this paper treats the problem as a multi-labeling task, where every input can have one or more valid entity label, and then a separate post-processing step is performed for removing labels that doesn\u2019t form a path along the hierarchy.\n\nDatasets for fine-grained classification are typically small in size, and thus, training strong classification models for fine-grained classification is always a challenge.\n\nOne approach for addressing this challenge is to leverage similarity between the predicted labels. For example, if the model is pretty sure that the entity can be tagged as an artist, it\u2019s pretty clear that the same entity can be tagged as a person as well.\n\nThe main question is how to capture this correlation between the predicted labels effectively. The proposed approach in this paper is to learn an embedding for each label and each input features such that labels which frequently co-occur are close in the embedding space.\n\nTo leverage the relationships among the fine grained labels, the model learns an embedding space for both labels as well as features. The idea is that we\u2019d expect an entity tagged as an artist to also be tagged as a person, and by selecting a suitable objective function, we can encourage the embedding for both \u201cartist\u201d and \u201cperson\u201d to be similar.\n\nThus, the selection of a suitable objective function is crucial. We don\u2019t want the set of true labels for a particular entity to compete with one another, what we need is that the model assigns \u201cgood\u201d scores for correct labels, and \u201cbad\u201d scores for all the other labels. The authors selected a ranking loss function, with a formulation very similar to an SVM-like maximum margin objective. In contrast to cross-entropy loss, this objective function maximizes the margin between the score for all the possible correct predictions, and the score for all the negative labels.\n\nThe proposed architecture is so simple. A set of features is computed for every entity. This set includes features like the role of the entity, words within a context window, a general topic assigned for this entity (etc., Table 1 includes a list of all the used features). For the labels, the model represents each input label as a one-hot encoding vector. Next, the model maps both the features and the labels to a joint embedding space, where two learnable matrices are used to compute a joint embedding in the same space. Using these embeddings, the dot product between the feature embedding vector and the label embedding vector is used as our scoring function . The system is trained end-to-end using stochastic (sub-)-gradient descent using the ranking loss function as a loss function.\n\nThe model is evaluated on two publicly available datasets: Google Fine Types (GFT) and the FIGER dataset. The model is shown to exploit the correlation between the entity labels, and out-performs the state-of-the-art on this task as of 2015."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-3-domain-adaptation-via-pseudo-in-domain-data-selection-8ab5905e1a15?source=user_profile---------25----------------",
        "title": "A Paper A Day: #3 Domain Adaptation via Pseudo In-Domain Data Selection",
        "text": "Today I summarize a paper by Amittai Axelrod, Xiaodong He, and Jianfeng Gao for domain adaptation by data selection. While the approach is so simple, it has proven to be very effective in domain adaptation particularly for machine translation.\n\nThe idea is so simple, our goal is to adapt a model trained on one domain to work better on data from a different domain. For a concrete example, consider the task of fine-tuning a machine translation model trained on news to translate text from the legal domain. We\u2019ll refer to the data from the news domain as out-of-domain data, and the data from the legal domain as in-domain data.\n\nThe paper proposes a simple, yet effective approach for doing domain adaptation using data selection. The idea is to select the most similar training instances from the out-of-domain dataset and to use only this subset for training the machine translation model.\n\nThe main question is how to compute this similarity metric. The paper proposes several approaches for computing the similarity scores. The simplest of which is to learn a language model from both domains, and use the learned language models to compute a cross-entropy score for every sentence in the out-of-domain data.\n\nExperiments were conducted on the International Workshop on Spoken Language Translation (IWSLT) Chinese-to-English DIALOG task, consisting of transcriptions of conversational speech in a travel setting. Two corpora are needed for the adaptation task. The in-domain data consisted of the IWSLT corpus of 30,000 sentences in Chinese and English, and the out-of-domain corpus was 12 million parallel sentences comprising a variety of publicly available datasets, web data, and private translation texts. The simplest data selection method, using only the source-side cross-entropy, was able to outperform the out-of-domain model when selecting 150k out of 12 million sentences. Other similarity metrics using both the source-side and the target side cross-entropy measures boosted performance by 1.8 BLEU while using less than 1% of the available training data.\n\nThe code is available online on Github."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-2-sequence-to-sequence-learning-as-beam-search-optimization-92424b490350?source=user_profile---------26----------------",
        "title": "A Paper A Day: #2 Sequence-to-Sequence Learning as Beam-Search Optimization",
        "text": "Today I\u2019ll summarize a paper by Sam Wiseman and Alexander Rush for seq2seq learning using beam search optimization.\n\nThe dominant approach for training seq2seq models is based on learning a conditional language model with training optimizing for the likelihood of the next target word conditioning on the input sequence and the ground-truth history of target words. Usually the objective function the model learns is a cross entropy loss function, minimizing the KL-divergence between the true distribution of words and the distribution of words generated by our learned model.\n\nWhile this approach has proven to be effective for language models, the objective function doesn\u2019t really align with the metric we care about at test time. For example, in machine translation, we care about generating fully formed sequences for a target language given a source sentence. Thus, we should be optimizing for a \u201csequence level\u201d score (e.g. BLEU) rather than using a per-word level objective function.\n\nAnother issue is the exposure bias between training and test time, where at training time we feed-in the ground truth annotations, while at test time the model is exposed to its own predictions by feeding-in predicted words rather than ground-truth words.\n\nThe authors also highlight the importance of controlling for the label bias, where a local objective function like cross-entropy will normalize the probability scores guaranteeing that successors of incorrect histories receive the same mass as do the successors of the true history.\n\nThe question now is how can we design an effective approach for solving the above problems: training by scoring sequences rather than words, exposing the model to its own predictions, and controlling for the labeling bias.\n\nThe authors offer a possible solution for the three problems. The main idea is to assign a score to any possible target sequence, and use a training procedure inspired by the learning as search optimization (LaSO) framework of Daume \u0301 III and Marcu (2005). Furthermore, the framework uses an efficient algorithm to backpropagate through beam-search during the seq2seq training procedure, thus the standard model architecture for seq2seq training can still be maintained.\n\nI\u2019ll leave out the details for how to do LaSO for a future post (maybe tomorrow?) but the main idea for doing beam search training can be summarized as follows. The first trick is to predict a ranking score for a sequence rather than predicting the probability of the next word. This can simply be done by using a recurrent LM architecture and leaving out the softmax layer. This helps in controlling for the labeling bias, and by scoring sequences rather than words, this provides a solution for the mismatch in training / testing time.\n\nThe tricky part is how to do this in a tractable way. Ideally we would train by comparing the gold sequence to the highest-scoring complete sequence. However, because finding the argmax sequence according to this model is intractable, the authors adopt a LaSO-like scheme to train, where a loss function is defined to penalizes the gold sequence falling off the beam during training.\n\nExperiments span three different tasks: word ordering, syntactic parsing, and machine translation. The results are compared to a highly tuned seq2seq system with attention (Luong et al., 2015). The version with beam search optimization shows significant improvements on all three tasks, and particular improvements on tasks that require difficult search.\n\nThe source code is available on Github."
    },
    {
        "url": "https://medium.com/@sharaf/a-paper-a-day-1-convolutional-sequence-to-sequence-learning-b7dbcb59fc54?source=user_profile---------27----------------",
        "title": "A Paper A Day: #1 Convolutional Sequence to Sequence Learning",
        "text": "Starting today, I\u2019ll be posting a short summary for a research paper every day \u2014 hopefully! I hope this will be useful for people interested in machine learning, reinforcement learning, and natural language processing, but I wanted to do this for other reasons as well. Basically I think this will be a motivation for me to read more, learn more, as well as improve my writing and analytical skills.\n\nI\u2019ll start by discussing the recent paper by the Facebook AI research (FAIR) team for convolutional sequence to sequence learning. Here is the main take-aways I got by reading this paper:\n\nSource code is available on Github."
    }
]