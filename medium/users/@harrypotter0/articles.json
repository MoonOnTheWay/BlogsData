[
    {
        "url": "https://codeburst.io/machine-learning-part-5-d2b4b80ccbf4?source=user_profile---------1----------------",
        "title": "Machine Learning Part-5 \u2013",
        "text": "This post is part of a series covering the exercises from Andrew Ng\u2019s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.\n\nTo attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn\u2019t work well because classification is not actually a linear function.\n\nThe classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then x(i) may be some features of a piece of email, and y may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, y\u2208{0,1}. 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols \u201c-\u201d and \u201c+.\u201d Given x(i), the corresponding y(i) is also called the label for the training example.\n\nWe could approach the classification problem ignoring the fact that y is discrete-valued, and use our old linear regression algorithm to try to predict y given x. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn\u2019t make sense for h\u03b8(x) to take values larger than 1 or smaller than 0 when we know that y \u2208 {0, 1}. To fix this, let\u2019s change the form for our hypotheses h\u03b8(x) to satisfy 0\u2264h\u03b8(x)\u22641. This is accomplished by plugging \u03b8Tx into the Logistic Function.\n\nOur new form uses the \u201cSigmoid Function,\u201d also called the \u201cLogistic Function\u201d:\n\nThe following image shows us what the sigmoid function looks like:\n\nThe function g(z), shown here, maps any real number to the (0, 1) interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.\n\nh\u03b8(x) will give us the probability that our output is 1. For example, h\u03b8(x)=0.7 gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).\n\nIn order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:\n\nThe way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:\n\nSo if our input to g is \u03b8TX, then that means:\n\nFrom these statements we can now say:\n\nThe decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.\n\nIn this case, our decision boundary is a straight vertical line placed on the graph where x1=5, and everything to the left of that denotes y = 1, while everything to the right denotes y = 0.\n\nAgain, the input to the sigmoid function g(z) (e.g. \u03b8TX) doesn\u2019t need to be linear, and could be a function that describes a circle (e.g. z=\u03b80+\u03b81x21+\u03b82x22) or any shape to fit our data.\n\nWe cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.\n\nInstead, our cost function for logistic regression looks like:\n\nWhen y = 1, we get the following plot for J(\u03b8) vs h\u03b8(x):\n\nSimilarly, when y = 0, we get the following plot for J(\u03b8) vs h\u03b8(x):\n\nCost(h\u03b8(x),y)=0 if h\u03b8(x)=yCost(h\u03b8(x),y)\u2192\u221e if y=0andh\u03b8(x)\u21921Cost(h\u03b8(x),y)\u2192\u221e if y=1andh\u03b8(x)\u21920\n\nIf our correct answer \u2018y\u2019 is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.\n\nIf our correct answer \u2018y\u2019 is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.\n\nNote that writing the cost function in this way guarantees that J(\u03b8) is convex for logistic regression.\n\nNote: [6:53 \u2014 the gradient descent equation should have a 1/m factor]\n\nWe can compress our cost function\u2019s two conditional cases into one case:\n\nNotice that when y is equal to 1, then the second term (1\u2212y)log(1\u2212h\u03b8(x)) will be zero and will not affect the result. If y is equal to 0, then the first term \u2212ylog(h\u03b8(x)) will be zero and will not affect the result.\n\nWe can fully write out our entire cost function as follows:\n\nRemember that the general form of gradient descent is:\n\nWe can work out the derivative part using calculus to get:\n\nNotice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta.\n\nNote: [7:35 \u2014 \u2018100\u2019 should be 100 instead. The value provided should be an integer and not a character string.]\n\n\u201cConjugate gradient\u201d, \u201cBFGS\u201d, and \u201cL-BFGS\u201d are more sophisticated, faster ways to optimize \u03b8 that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they\u2019re already tested and highly optimized. Octave provides them.\n\nWe first need to provide a function that evaluates the following two functions for a given input value \u03b8:\n\nWe can write a single function that returns both of these:\n\nThen we can use octave\u2019s \u201cfminunc()\u201d optimization algorithm along with the \u201coptimset()\u201d function that creates an object containing the options we want to send to \u201cfminunc()\u201d. (Note: the value for MaxIter should be an integer, not a character string \u2014 errata in the video at 7:30)\n\nWe give to the function \u201cfminunc()\u201d our cost function, our initial vector of theta values, and the \u201coptions\u201d object that we created beforehand.\n\nNow we will approach the classification of data when we have more than two categories. Instead of y = {0,1} we will expand our definition so that y = {0,1\u2026n}.\n\nSince y = {0,1\u2026n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that \u2018y\u2019 is a member of one of our classes.\n\nWe are basically choosing one class and then lumping all the others into a single second class. We do this repeatedly, applying binary logistic regression to each case, and then use the hypothesis that returned the highest value as our prediction.\n\nThe following image shows how one could classify 3 classes:\n\nTrain a logistic regression classifier h\u03b8(x) for each class to predict the probability that y = i .\n\nTo make a prediction on a new x, pick the class that maximizes h\u03b8(x)\n\nConsider the problem of predicting y from x \u2208 R. The leftmost figure below shows the result of fitting a y = \u03b80+\u03b81x to a dataset. We see that the data doesn\u2019t really lie on straight line, and so the fit is not very good.\n\nInstead, if we had added an extra feature x2 , and fit y=\u03b80+\u03b81x+\u03b82x2 , then we obtain a slightly better fit to the data (See middle figure). Naively, it might seem that the more features we add, the better. However, there is also a danger in adding too many features: The rightmost figure is the result of fitting a 5th order polynomial y=\u22115j=0\u03b8jxj. We see that even though the fitted curve passes through the data perfectly, we would not expect this to be a very good predictor of, say, housing prices (y) for different living areas (x). Without formally defining what these terms mean, we\u2019ll say the figure on the left shows an instance of underfitting \u2014 in which the data clearly shows structure not captured by the model \u2014 and the figure on the right is an example of overfitting.\n\nUnderfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. At the other extreme, overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.\n\nThis terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:\n\nNote: [5:18 \u2014 There is a typo. It should be \u2211nj=1\u03b82j instead of \u2211ni=1\u03b82j]\n\nIf we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.\n\nSay we wanted to make the following function more quadratic:\n\nWe\u2019ll want to eliminate the influence of \u03b83x3 and \u03b84x4 . Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our cost function:\n\nWe\u2019ve added two extra terms at the end to inflate the cost of \u03b83 and \u03b84. Now, in order for the cost function to get close to zero, we will have to reduce the values of \u03b83 and \u03b84 to near zero. This will in turn greatly reduce the values of \u03b83x3 and \u03b84x4 in our hypothesis function. As a result, we see that the new hypothesis (depicted by the pink curve) looks like a quadratic function but fits the data better due to the extra small terms \u03b83x3 and \u03b84x4.\n\nWe could also regularize all of our theta parameters in a single summation as:\n\nThe \u03bb, or lambda, is the regularization parameter. It determines how much the costs of our theta parameters are inflated.\n\nUsing the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting. Hence, what would happen if \u03bb=0 or is too small ?\n\nNote: [8:43 \u2014 It is said that X is non-invertible if m \u2264 n. The correct statement should be that X is non-invertible if m < n, and may be non-invertible if m = n.\n\nWe can apply regularization to both linear regression and logistic regression. We will approach linear regression first.\n\nWe will modify our gradient descent function to separate out \u03b80 from the rest of the parameters because we do not want to penalize \u03b80.\n\nThe term \u03bbm\u03b8j performs our regularization. With some manipulation our update rule can also be represented as:\n\nThe first term in the above equation, 1\u2212\u03b1\u03bbm will always be less than 1. Intuitively you can see it as reducing the value of \u03b8j by some amount on every update. Notice that the second term is now exactly the same as it was before.\n\nNow let\u2019s approach regularization using the alternate method of the non-iterative normal equation.\n\nTo add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:\n\nL is a matrix with 0 at the top left and 1\u2019s down the diagonal, with 0\u2019s everywhere else. It should have dimension (n+1)\u00d7(n+1). Intuitively, this is the identity matrix (though we are not including x0), multiplied with a single real number \u03bb.\n\nRecall that if m < n, then XTX is non-invertible. However, when we add the term \u03bb\u22c5L, then XTX + \u03bb\u22c5L becomes invertible.\n\nWe can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pink line, is less likely to overfit than the non-regularized function represented by the blue line:\n\nRecall that our cost function for logistic regression was:\n\nWe can regularize this equation by adding a term to the end:\n\nThe second sum, \u2211nj=1\u03b82j means to explicitly exclude the bias term, \u03b80. I.e. the \u03b8 vector is indexed from 0 to n (holding n+1 values, \u03b80 through \u03b8n), and this sum explicitly skips \u03b80, by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations:\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science . You can follow me and my code on Github"
    },
    {
        "url": "https://codeburst.io/machine-learning-part-4-6f285d68cf48?source=user_profile---------2----------------",
        "title": "Machine Learning Part-4 \u2013",
        "text": "This post is part of a series covering the exercises from Andrew Ng\u2019s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.\n\nLinear regression with multiple variables is also known as \u201cmultivariate linear regression\u201d.\n\nWe now introduce notation for equations where we can have any number of input variables.\n\nx(i)jx(i)mn=value of feature j in the ith training example=the input (features) of the ith training example=the number of training examples=the number of features\n\nThe multivariable form of the hypothesis function accommodating these multiple features is as follows:\n\nIn order to develop intuition about this function, we can think about \u03b80 as the basic price of a house, \u03b81 as the price per square meter, \u03b82 as the price per floor, etc. x1 will be the number of square meters in the house, x2 the number of floors, etc.\n\nUsing the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:\n\nThis is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.\n\nRemark: Note that for convenience reasons in this course we assume x(i)0=1 for (i\u22081,\u2026,m). This allows us to do matrix operations with theta and x. Hence making the two vectors \u2018\u03b8\u2019 and x(i) match each other element-wise (that is, have the same number of elements: n+1).]\n\nThe gradient descent equation itself is generally the same form; we just have to repeat it for our \u2019n\u2019 features:\n\nThe following image compares gradient descent with one variable to gradient descent with multiple variables:\n\nNote: [6:20 \u2014 The average size of a house is 1000 but 100 is accidentally written instead]\n\nWe can speed up gradient descent by having each of our input values in roughly the same range. This is because \u03b8 will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n\nThe way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:\n\nThese aren\u2019t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.\n\nTwo techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n\nWhere \u03bci is the average of all the values for feature (i) and si is the range of values (max \u2014 min), or si is the standard deviation.\n\nNote that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range \u2014 the programming exercises use standard deviation.\n\nFor example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, xi:=price\u221210001900.\n\nNote: [5:20 \u2014 the x -axis label in the right graph should be \u03b8 rather than No. of iterations ]\n\nDebugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, J(\u03b8) over the number of iterations of gradient descent. If J(\u03b8) ever increases, then you probably need to decrease \u03b1.\n\nAutomatic convergence test. Declare convergence if J(\u03b8) decreases by less than E in one iteration, where E is some small value such as 10\u22123. However in practice it\u2019s difficult to choose this threshold value.\n\nIt has been proven that if learning rate \u03b1 is sufficiently small, then J(\u03b8) will decrease on every iteration.\n\nIf \u03b1 is too small: slow convergence.\n\nIf \u03b1 is too large: may not decrease on every iteration and thus may not converge.\n\nWe can improve our features and the form of our hypothesis function in a couple different ways.\n\nWe can combine multiple features into one. For example, we can combine x1 and x2 into a new feature x3 by taking x1\u22c5x2.\n\nOur hypothesis function need not be linear (a straight line) if that does not fit the data well.\n\nWe can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n\nFor example, if our hypothesis function is h\u03b8(x)=\u03b80+\u03b81x1 then we can create additional features based on x1, to get the quadratic function h\u03b8(x)=\u03b80+\u03b81x1+\u03b82x21 or the cubic function h\u03b8(x)=\u03b80+\u03b81x1+\u03b82x21+\u03b83x31\n\nIn the cubic version, we have created new features x2 and x3 where x2=x21 and x3=x31.\n\nTo make it a square root function, we could do: h\u03b8(x)=\u03b80+\u03b81x1+\u03b82x1\u2212\u2212\u221a\n\nOne important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.\n\neg. if x1 has range 1\u20131000 then range of x21 becomes 1\u20131000000 and that of x31 becomes 1\u20131000000000\n\nNote: [8:00 to 8:44 \u2014 The design matrix X (in the bottom right side of the slide) given in the example should have elements x with subscript 1 and superscripts varying from 1 to m because for all m training sets there are only 2 features x0 and x1. 12:56 \u2014 The X matrix is m by (n+1) and NOT n by n. ]\n\nGradient descent gives one way of minimizing J. Let\u2019s discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the \u201cNormal Equation\u201d method, we will minimize J by explicitly taking its derivatives with respect to the \u03b8j \u2019s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n\nThere is no need to do feature scaling with the normal equation.\n\nThe following is a comparison of gradient descent and the normal equation:\n\nGradient DescentNormal EquationNeed to choose alphaNo need to choose alphaNeeds many iterationsNo need to iterateO (kn2)O (n3), need to calculate inverse of XTXWorks well when n is largeSlow if n is very large\n\nWith the normal equation, computing the inversion has complexity O(n3). So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\n\nWhen implementing the normal equation in octave we want to use the \u2018pinv\u2019 function rather than \u2018inv.\u2019 The \u2018pinv\u2019 function will give you a value of \u03b8 even if XTX is not invertible.\n\nIf XTX is noninvertible, the common causes might be having :\n\nSolutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science . You can follow me and my code on Github"
    },
    {
        "url": "https://codeburst.io/machine-learning-part-3-fb2f26c92c0b?source=user_profile---------3----------------",
        "title": "Machine Learning Part-3 \u2013",
        "text": "This post is part of a series covering the exercises from Andrew Ng\u2019s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.\n\nThe above matrix has four rows and three columns, so it is a 4 x 3 matrix.\n\nA vector is a matrix with one column and many rows:\n\nSo vectors are a subset of matrices. The above vector is a 4 x 1 matrix.\n\nAddition and subtraction are element-wise, so you simply add or subtract each corresponding element:\n\nTo add or subtract two matrices, their dimensions must be the same.\n\nIn scalar multiplication, we simply multiply every element by the scalar value:\n\nIn scalar division, we simply divide every element by the scalar value:\n\nWe map the column of the vector onto each row of the matrix, multiplying each element and summing the result.\n\nThe result is a vector. The number of columns of the matrix must equal the number of rows of the vector.\n\nAn m x n matrix multiplied by an n x 1 vector results in an m x 1 vector.\n\nWe multiply two matrices by breaking it into several vector multiplications and concatenating the result.\n\nAn m x n matrix multiplied by an n x o matrix results in an m x o matrix. In the above example, a 3 x 2 matrix times a 2 x 2 matrix resulted in a 3 x 2 matrix.\n\nThe identity matrix, when multiplied by any matrix of the same dimensions, results in the original matrix. It\u2019s just like multiplying numbers by 1. The identity matrix simply has 1\u2019s on the diagonal (upper left to lower right diagonal) and 0\u2019s elsewhere.\n\nWhen multiplying the identity matrix after some matrix (A\u2217I), the square identity matrix\u2019s dimension should match the other matrix\u2019s columns. When multiplying the identity matrix before some other matrix (I\u2217A), the square identity matrix\u2019s dimension should match the other matrix\u2019s rows.\n\nThe inverse of a matrix A is denoted A\u22121. Multiplying by the inverse results in the identity matrix.\n\nA non square matrix does not have an inverse matrix. We can compute inverses of matrices in octave with the pinv(A) function and in Matlab with the inv(A) function. Matrices that don\u2019t have an inverse are singular or degenerate.\n\nThe transposition of a matrix is like rotating the matrix 90\u00b0 in clockwise direction and then reversing it. We can compute transposition of matrices in matlab with the transpose(A) function or A\u2019:\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science . You can follow me and my code on Github"
    },
    {
        "url": "https://codeburst.io/recurrent-neural-network-4ca9fd4f242?source=user_profile---------4----------------",
        "title": "Recurrent Neural Network \u2013",
        "text": "This is a pure numpy implementation of word generation using an RNN\n\nWe\u2019re going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we\u2019ll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n\nFeedforward networks are great for learning a pattern between a set of inputs and outputs.\n\nBut what if the ordering of the data matters?\n\nAlphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist).\n\nWe feed the hidden state from the previous time step back into the the network at the next time step.\n\nSo instead of the data flow operation happening like this\n\nit happens like this\n\nwait. Why not this?\n\nHidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n\nIt basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n\nThe total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence\n\nThe network need a big txt file as an input.\n\nThe content of the file will be used to train the network.\n\nI use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like.\n\nNeural networks operate on vectors (a vector is an array of float) So we need a way to encode and decode a char as a vector.\n\nWe\u2019ll count the number of unique chars (vocab_size). That will be the size of the vector. The vector contains only zero exept for the position of the char wherae the value is 1.\n\nThe dictionary defined above allosw us to create a vector of size 61 instead of 256.\n\n Here and exemple of the char \u2018a\u2019\n\n The vector contains only zeros, except at position char_to_ix[\u2018a\u2019] where we put a 1.\n\nThe neural network is made of 3 layers:\n\nAll layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer. The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n\nTo centralise values that matter for the training (hyper parameters) we also define the sequence lenght and the learning rate\n\nThe model parameters are adjusted during the trainning.\n\nYou\u2019ll see in the next section how theses parameters are used to create a sentence.\n\nThe loss is a key concept in all neural networks training. It is a value that describe how good is our model.\n\n The smaller the loss, the better our model is.\n\n (A good model is a model where the predicted output is close to the training output)\n\nDuring the training phase we want to minimize the loss.\n\nThe loss function calculates the loss but also the gradients (see backward pass):\n\nThis function take as input:\n\nThe forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n\nxs[t] is the vector that encode the char at position t ps[t] is the probabilities for next char\n\nor is dirty pseudo code for each char\n\nThe naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters. This is possible but would be time consuming. There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.\n\n Gradients are calculated in the oposite order of the forward pass, using simple technics.\n\nThe loss for one datapoint\n\nHow should the computed scores inside f change tto decrease the loss? We\u2019ll need to derive a gradient to figure that out.\n\nSince all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n\nOur first gradient of our loss. We\u2019ll backpropagate this via chain rule\n\nThe chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n\nThis last part of the code is the main trainning loop:\n\nWe create two array of char from the data file, the targets one is shifted compare to the inputs one.\n\nFor each char in the input array, the target array give the char that follows.\n\nThis is a type of gradient descent strategy\n\nThe easiest technics to update the parmeters of the model is this:\n\nAdagrad is a more efficient technique where the step_size are getting smaller during the training.\n\nIt use a memory variable that grow over time:\n\nand use it to calculate the step_size:\n\nSmooth_loss doesn\u2019t play any role in the training. It is just a low pass filtered version of the loss:\n\nIt is a way to average the loss on over the last iterations to better track the progress\n\nHere the code of the main loop that does both trainning and generating text from times to times:\n\nFollow me as I write about Machine Learning , Data Science and Blockchain . You can follow me and my code on Github"
    },
    {
        "url": "https://codeburst.io/generating-text-using-an-lstm-network-no-libraries-2dff88a3968?source=user_profile---------5----------------",
        "title": "Generating Text using an LSTM Network (No libraries)",
        "text": "We\u2019ll train an LSTM network built in pure numpy to generate Eminem lyrics. LSTMs are a fairly simple extension to neural networks, and they\u2019re behind a lot of the amazing achievements deep learning has made in the past few years.\n\nRecurrent nets are cool, they\u2019re useful for learning sequences of data. Input. Hidden state. Output.\n\nIt has a weight matrix that connects input to hidden state. But also a weight matrix that connects hidden state to hidden state at previous time step.\n\nSo we could even think of it as the same feedforward network connecting to itself overtime (unrolled) since passing in not just input in next training iteration but input + previous hidden state\n\nIf we want to predict the last word in the sentence \u201cThe grass is green\u201d, that\u2019s totally doable.\n\nBut if we want to predict the last word in the sentence \u201cI am French (2000 words later) i speak fluent French\u201d. We need to be able to remember long range dependencies. RNN\u2019s are bad at this. They forget the long term past easily.\n\nThis is called the \u201cVanishing Gradient Problem\u201d. The Gradient exponentially decays as its backpropagated\n\nThere are two factors that affect the magnitude of gradients \u2014 the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then exploding might happen.\n\nBut there exists a solution! Enter the LSTM Cell.\n\nWe\u2019ve placed no constraints on how our model updates, so its knowledge can change pretty chaotically: at one frame it thinks the characters are in the US, at the next frame it sees the characters eating sushi and thinks they\u2019re in Japan, and at the next frame it sees polar bears and thinks they\u2019re on Hydra Island.\n\nThis chaos means information quickly transforms and vanishes, and it\u2019s difficult for the model to keep a long-term memory. So what you\u2019d like is for the network to learn how to update its beliefs (scenes without Bob shouldn\u2019t change Bob-related information, scenes with Alice should focus on gathering details about her), in a way that its knowledge of the world evolves more gently.\n\nIt replaces the normal RNN cell and uses an input, forget, and output gate. As well as a cell state\n\nThese gates each have their own set of weight values. The whole thing is differentiable (meaning we compute gradients and update the weights using them) so we can backprop through it\n\nWe want our model to be able to know what to forget, what to remember. So when new a input comes in, the model first forgets any long-term information it decides it no longer needs. Then it learns which parts of the new input are worth using, and saves them into its long-term memory.\n\nAnd instead of using the full long-term memory all the time, it learns which parts to focus on instead.\n\nBasically, we need mechanisms for forgetting, remembering, and attention. That\u2019s what the LSTM cell provides us.\n\nWhereas a vanilla RNN uses one equation to update its hidden state/memory:\n\nWhich piece of long term memory to remember and forget? We\u2019ll use new input and working memory to learn remember gate. Which part of new data should we use and save? Update working memory using attention vector.\n\nThe most popular application right now is actually in natural language processing which involves sequential data such as words, sentences, sound spectrogram, etc. So applications with translation, sentiment analysis, text generation, etc.\n\nIn other less obvious areas there\u2019s also applications of lstm. Such as for image classification (feeding each picture\u2019s pixel in row by row). And even for deepmind\u2019s deep Q Learning agents.\n\nFollow me as I write about Machine Learning , Data Science and Blockchain . You can follow me and my code on Github"
    },
    {
        "url": "https://medium.com/@harrypotter0/how-does-monero-work-17f18ea37652?source=user_profile---------6----------------",
        "title": "Monero \u2014 An Anonymous CryptoCurrency \u2013 Akash Kandpal \u2013",
        "text": "Who controls what money? Who owns which wallet???\n\nOnce you have selected the other inputs, you need to finish creating the RingCT ring signature.\n\nYou sign it so it appears in such a way that all these inputs appear to be the real one used. This signature includes several other important elements\n\nThe key image is critically important. It\u2019s a one-way reference to the real input (the red one). This key image is given to the network as proof that the signature was created appropriately. The network verifies that this image has not been used before (to prevent double spends) and that it isn\u2019t a made-up number (to prevent people from spending money they never had). The network can verify this information without knowing which input is the real one.\n\nNext is the pedersen commitment. This is used to prevent other people from knowing how much is actually being spent. You can use this commitment to commit to spending a certain value that you have the authority to spend, but other people no longer know what this value is.\n\nThis pedersen commitment is the critical component of ring confidential transactions, or RingCT. It hides the actual value a by adding a random number x. The commitment value is calculated for the set of inputs and outputs in the transaction, and it is broadcast to the network.\n\nIt all comes together to form the RingCT ring signature. This results in an unknown amount of Monero being spent. The commitment public key is what is used by the network to verify the commitment.\n\nSo how do outputs get used over time? Let\u2019s compare Bitcoin and Monero to find out.\n\nIn the picture above, I have created a theoretical history for the output you control. All of the blocks highlighted red are ones where the output appears. If this was for Bitcoin, you would be able to easily tell that this output was transferred from user A to B to C, etc. However with Monero, this is not so simple.\n\nThere are three reasons for an output to show up in a block:\n\nSince there is no way to differentiate between case #2 and case #3, outside observers have no idea if an output is actually being spent, even though it appears on the blockchain several times. Since every transaction includes multiple decoys, it\u2019s more likely than not that the output is not actually spent despite appearing in a certain block.\n\nAnd with that, we have completed the discussion on ring signatures and RingCT!\n\nA CryptoNote blockchain reveals only amounts and one-time addresses that cannot be linked to any other transactions made by the sender or recipient, rendering blockchain analysis useless to anyone seeking to identify individuals transacting in these currencies.\n\nAs we went over, a Monero transaction has an ambiguous output origin, an unknown amount in a commitment, and an unknown receiver. For every transaction on the network, all of the information stored on the blockchain is obfuscated.\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science . You can follow me and my code on Github"
    },
    {
        "url": "https://codeburst.io/machine-learning-day-1-d35749ef05c2?source=user_profile---------7----------------",
        "title": "Machine Learning Part-1 \u2013",
        "text": "This post is part of a series covering the exercises from Andrew Ng\u2019s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.\n\nTwo definitions of Machine Learning are offered. Arthur Samuel described it as: \u201cthe field of study that gives computers the ability to learn without being explicitly programmed.\u201d This is an older, informal definition.\n\nTom Mitchell provides a more modern definition: \u201cA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\u201d\n\nE = the experience of playing many games of checkers\n\nP = the probability that the program will win the next game.\n\nIn general, any machine learning problem can be assigned to one of two broad classifications:\n\nIn supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\n\nSupervised learning problems are categorized into \u201cregression\u201d and \u201cclassification\u201d problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.\n\nGiven data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.\n\nWe could turn this example into a classification problem by instead making our output about whether the house \u201csells for more or less than the asking price.\u201d Here we are classifying the houses based on price into two discrete categories.\n\n(a) Regression \u2014 Given a picture of a person, we have to predict their age on the basis of the given picture\n\n(b) Classification \u2014 Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.\n\nUnsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don\u2019t necessarily know the effect of the variables.\n\nWe can derive this structure by clustering the data based on relationships among the variables in the data.\n\nWith unsupervised learning there is no feedback based on the prediction results.\n\nClustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.\n\nNon-clustering: The \u201cCocktail Party Algorithm\u201d, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).\n\nTo establish notation for future use, we\u2019ll use x(i) to denote the \u201cinput\u201d variables (living area in this example), also called input features, and y(i) to denote the \u201coutput\u201d or target variable that we are trying to predict (price). A pair (x(i),y(i)) is called a training example, and the dataset that we\u2019ll be using to learn \u2014 a list of m training examples (x(i),y(i));i=1,\u2026,m \u2014 is called a training set. Note that the superscript \u201c(i)\u201d in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = \u211d.\n\nTo describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X \u2192 Y so that h(x) is a \u201cgood\u201d predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this:\n\nWhen the target variable that we\u2019re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.\n\nWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x\u2019s and the actual output y\u2019s.\n\nTo break it apart, it is 12 x\u00af where x\u00af is the mean of the squares of h\u03b8(xi)\u2212yi , or the difference between the predicted value and the actual value.\n\nThis function is otherwise called the \u201cSquared error function\u201d, or \u201cMean squared error\u201d. The mean is halved (12)as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the 12 term. The following image summarizes what the cost function does:\n\nIf we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by h\u03b8(x)) which passes through these scattered data points.\n\nOur objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of J(\u03b80,\u03b81) will be 0. The following example shows the ideal situation where we have a cost function of 0.\n\nWhen \u03b81=1, we get a slope of 1 which goes through every single data point in our model. Conversely, when \u03b81=0.5, we see the vertical distance from our fit to the data points increase.\n\nThis increases our cost function to 0.58. Plotting several other points yields to the following graph:\n\nThus as a goal, we should try to minimize the cost function. In this case, \u03b81=1 is our global minimum.\n\nA contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below.\n\nTaking any color and going along the \u2018circle\u2019, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for J(\u03b80,\u03b81) and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when \u03b80 = 800 and \u03b81= -0.15. Taking another h(x) and plotting its contour plot, one gets the following graphs:\n\nWhen \u03b80 = 360 and \u03b81 = 0, the value of J(\u03b80,\u03b81) in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data.\n\nThe graph above minimizes the cost function as much as possible and consequently, the result of \u03b81 and \u03b80 tend to be around 0.12 and 250 respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most \u2018circle\u2019.\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science . You can follow me and my code on Github"
    },
    {
        "url": "https://codeburst.io/machine-learning-day-1-60bd231d0660?source=user_profile---------8----------------",
        "title": "Machine Learning Part-2 \u2013",
        "text": "This post is part of a series covering the exercises from Andrew Ng\u2019s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.\n\nSo we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That\u2019s where gradient descent comes in.\n\nImagine that we graph our hypothesis function based on its fields \u03b80 and \u03b81 (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters.\n\nWe put \u03b80 on the x axis and \u03b81 on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.\n\nWe will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph.\n\nThe way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter \u03b1, which is called the learning rate.\n\nFor example, the distance between each \u2018star\u2019 in the graph above represents a step determined by our parameter \u03b1. A smaller \u03b1 would result in a smaller step and a larger \u03b1 results in a larger step. The direction in which the step is taken is determined by the partial derivative of J(\u03b80,\u03b81). Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places.\n\nAt each iteration j, one should simultaneously update the parameters \u03b81,\u03b82,\u2026,\u03b8n. Updating a specific parameter prior to calculating another one on the j(th) iteration would yield to a wrong implementation.\n\nIn this video we explored the scenario where we used one parameter \u03b81 and plotted its cost function to implement a gradient descent. Our formula for a single parameter was :\n\nRegardless of the slope\u2019s sign for dd\u03b81J(\u03b81), \u03b81 eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of \u03b81 increases and when it is positive, the value of \u03b81 decreases.\n\nOn a side note, we should adjust our parameter \u03b1 to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.\n\nThe intuition behind the convergence is that dd\u03b81J(\u03b81) approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:\n\nWhen specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to :\n\nwhere m is the size of the training set, \u03b80 a constant that will be changing simultaneously with \u03b81 and xi,yiare values of the given training set (data).\n\nNote that we have separated out the two cases for \u03b8j into separate equations for \u03b80 and \u03b81; and that for \u03b81 we are multiplying xi at the end due to the derivative. The following is a derivation of \u2202\u2202\u03b8jJ(\u03b8) for a single example :\n\nThe point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.\n\nSo, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called batch gradient descent. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate \u03b1 is not too large) to the global minimum. Indeed, J is a convex quadratic function. Here is an example of gradient descent as it is run to minimize a quadratic function.\n\nThe ellipses shown above are the contours of a quadratic function. Also shown is the trajectory taken by gradient descent, which was initialized at (48,30). The x\u2019s in the figure (joined by straight lines) mark the successive values of \u03b8 that gradient descent went through as it converged to its minimum.\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science . You can follow me and my code on Github"
    },
    {
        "url": "https://codeburst.io/getting-started-with-maths-for-data-science-part1b-d1203447e4bc?source=user_profile---------9----------------",
        "title": "Getting Started with Data Science using Python \u2014 Part1b",
        "text": "We will build a Support Vector Machine that will find the optimal hyperplane that maximizes the margin between two toy data classes using gradient descent.\n\nIt\u2019s a supervised machine learning algorithm which can be used for both classification or regression problems. But it\u2019s usually used for classification. Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n\nSupport vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set, they are what help us build our SVM.\n\nGeometry tells us that a hyperplane is a subspace of one dimension less than its ambient space. For instance, a hyperplane of an n-dimensional space is a flat subset with dimension n \u2212 1. By its nature, it separates the space into two half spaces.\n\nSometimes our data is linearly seperable. That means for N classes with M features, we can learn a mapping that is a linear combination. (like y = mx + b). Or even a multidimensional hyperplane (y = x + z + b + q). No matter how many dimensions/features a set of classes have, we can represent the mapping using a linear function.\n\nBut sometimes its not. Like if there was a quadratic mapping. Luckily for us SVMs can can efficiently perform a non-linear classification using what is called the kernel trick. I\u2019ll talk about that lter on in the course.\n\nWe\u2019ll use the Hinge loss. This is a loss function used for training classifiers. The hinge loss is used for \u201cmaximum-margin\u201d classification, most notably for support vector machines (SVMs).\n\nc is the loss function, x the sample, y is the true label, f(x) the predicted label.\n\nAs you can see, our objective of a SVM consists of two terms. The first term is a regularizer, the heart of the SVM, the second term the loss. The regularizer balances between margin maximization and loss. We want to find the decision surface that is maximally far away from any data points.\n\nHow do we minimize our loss/optimize for our objective (i.e learn)?\n\nWe have to derive our objective function to get the gradients! Gradient descent ftw. As we have two terms, we will derive them seperately using the sum rule in differentiation.\n\nThis means, if we have a misclassified sample, we update the weight vector w using the gradients of both terms, else if classified correctly,we just update w by the gradient of the regularizer.\n\nincluding the learning rate \u03b7 and the regularizer \u03bb The learning rate is the length of the steps the algorithm makes down the gradient on the error curve.\n\nThe regularizer controls the trade off between the achieving a low training error and a low testing error that is the ability to generalize your classifier to unseen data. As a regulizing parameter we choose 1/epochs, so this parameter will decrease, as the number of epochs increases.\n\nHere is the Source Code !!\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science and don\u2019t waste your time by writing rubbish,irrelevant or long answers . Have a nice day !!!"
    },
    {
        "url": "https://codeburst.io/getting-started-with-maths-for-data-science-part1-2ef73067984?source=user_profile---------10----------------",
        "title": "Getting Started with Data Science using Python \u2014 Part1a",
        "text": "This week\u2019s coding challenge is to implement gradient descent to find the line of best fit that predicts the relationship between 2 variables from a kaggle dataset.\n\nThat\u2019s the least relevant thing but \u201cCS:GO Dataset\u201d has been chosen.\n\nFrom file \u2018ADRvsRating.csv\u2019 has been chosen the variables:\n\nRead the file, save the data as matrix, save some util variables and and give it a lookup\n\nWe can see that there is a linear relationship between the two variables, let\u2019s find the best fitting line with **GRADIENT DESCENT**\n\nDon\u2019t forget that at the end we will be able to **predict** the *Rating* of a player given the *ADR*\n\nLet\u2019s try to draw a line that fits through all the data points, but obviously the data is too spaced to draw a line that fits every points, so **we are going to find the BEST fitting line**\n\nMathematically, the line will be:\n\nIn other words **we are going to find the optimal \u2018m\u2019 and \u2018b\u2019 values that defines the BEST fitting line**\n\nThen, the prediction will be as easy as given any new \u2018x\u2019 value plug it into our equation and compute the \u2018y\u2019 (the prediction!)\n\nOur error metric can be understood as a **measure of closeness**\n\n- If the line doesn\u2019t fit at all => Error will be BIG\n\n- If the line fits well => Error will be SMALL\n\nOf course, we want to **reduce** the **error**, because doing that we are closer to the **BEST fitting line**\n\nThere are severals ways of measure the error, we will be using SSE that corresponds to that formula:\n\nWhat we need to compute the SSE:\n\n1. The current \u2018m\u2019 and \u2018b\u2019 to know which is our current fitting line\n\n2. The real data, that corresponds to \u2018target\u2019 on the equation of the image\n\nWhat we will return:\n\n*Note: Error will be used to **measure** the **quality** of the fitting line but **IT IS NOT INVOLVED** in the **optimization process** done by Gradient Descent.*\n\nThe 2nd point implies that we need a direction, a way to descend the error valley\n\nThe way to go: Compute the partial derivatives respect to \u2018m\u2019 and \u2018b\u2019. Get the direction that decreases the gradient (towards zero error) and update \u2018m\u2019 and \u2018b\u2019 accordingly\n\nWhat we need to compute the Gradient Descent:\n\nWhat will do:\n\nWhat we will return:\n\nExample of use:\n\nDoing just 2 steps of GD from line:\n\nLet\u2019s do a little function to make that repetitive process easier\n\nNow, we only have to do many Gradient Descent steps and choose the last \u2018m\u2019 and \u2018b\u2019, because they will be:\n\nThe \u2018m\u2019 and \u2018b\u2019 that defines the BEST FITTING line\n\nNow it\u2019s YOUR turn, define the hyperparams of the process:\n\nFeel free to experiment with different values\n\nA lot better than before :)\n\nBut hey! Don\u2019t go yet, we can still do some faaancy and DOPE visualizations\n\nNote: All the code beyond here is out of the challenge, so you can just skip it and see the visualizations\n\n2. An Introduction to Gradient Descent and Linear Regression \u2014 MATT NEDRICH\n\nHere is the Jupyter Notebook !!\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science and don\u2019t waste your time by writing rubbish,irrelevant or long answers . Have a nice day !!!"
    },
    {
        "url": "https://medium.com/@harrypotter0/an-introduction-to-data-analysis-with-pandas-27ecbce2853?source=user_profile---------11----------------",
        "title": "An Introduction to Data analysis with Pandas \u2013 Akash Kandpal \u2013",
        "text": "pandas probably is the most popular library for data analysis in Python programming language. This library is a high-level abstraction over low-level NumPy which is written in pure C. I use pandas on a daily basis and really enjoy it because of its eloquent syntax and rich functionality. Hope this note will help you dive into data analysis realm with Python and pandas.\n\nIn order to master pandas you have to start from scratch with two main data structures: DataFrame and Series. If you don\u2019t understand them well you won\u2019t understand pandas.\n\nSeries is an object which is similar to Python built-in list data structure but differs from it because it has associated label with each element or so-called index. This distinctive feature makes it look like associated array or dictionary (hashmap representation).\n\nTake a look at the output above and you will see that index is leftward and values are to the right. If index is not provided explicitly, then pandas creates RangeIndex starting from 0 to N-1, where N is a total number of elements. Moreover, each Series object has data type (dtype), in our case data type is int64.\n\nSeries has attributes to extract its values and index:\n\nYou can retrieve elements by their index number:\n\nIt is easy to retrieve several elements by their indexes or make group assignment:\n\nFiltering and math operations are easy as well:\n\nBecause Series is very similar to dictionary, where key is an index and value is an element, we can do this:\n\nAlso Series object and its index have name attributes, so you can label them:\n\nIndex can be changed \u201con fly\u201d by assigning list to index attribute:\n\nBut bear in mind that the length of the list should be equal to the number of elements inside Series and also labels have to be unique.\n\nSimply said, DataFrame is a table. It has rows and columns. Each column in a DataFrame is a Series object, rows consist of elements inside Series.\n\nDataFrame can be constructed using built-in Python dicts:\n\nIn order to make sure that each column is a Series object let\u2019s do this:\n\nDataFrame object has 2 indexes: column index and row index. If you do not provide row index explicitly, pandas will create RangeIndex from 0 to N-1, where N is a number of rows inside DataFrame.\n\nOur table/DataFrame has 4 elements from 0 to 3.\n\nThere are numerous ways to provide row index explicitly, for example you can provide index when creating a DataFrame or do it \u201con the fly\u201d during runtime:\n\nBesides creating an index I have also named it as Country Code. Series object will also have the same index as DataFrame has:\n\nRow access using index can be performed in several ways:\n\nSelection of particular rows and columns can be performed this way:\n\n.loc takes 2 arguments: index list and column list, slicing operation is supported as well:\n\nBy the way, columns can be accessed using attribute or Python dict notation, for example df.population and df[\u2018population\u2019] are the same operations.\n\nIf you want to reset index, no problem:\n\nBy default when you manipulate a DataFrame, pandas will return a new instance (the old one will not be affected).\n\nDon\u2019t like new column? OK, let\u2019s delete it:\n\nIf you are too lazy to type drop\u2026 you can use del df[\u2018density\u2019]\n\nIn our renaming example make sure that you dropped the index otherwise invoking rename method will have no effect.\n\npandas supports many popular file formats including CSV, XML, HTML, Excel, SQL, JSON many more (check out official docs).\n\nBut usually (in my practice) I use CSV files. For example, if you want to save our previous DataFrame run this:\n\nto_csv method takes many arguments (for example, separator character), if you want to know more make sure to check official documentation.\n\nWe have saved our DataFrame, but what about reading data? No problem:\n\nNamed argument sep points to a separator character in CSV file called filename.csv. There are many different ways to construct DataFrame from external sources, for example using read_sql method pandas can perform SQL query and store results inside a new DataFrame instance.\n\nGrouping is probably one of the most popular methods in data analysis. If you want to group data in pandas you have to use .groupby method. In order to demonstrate aggregates and grouping in pandas I decided to choose popular Titanic dataset which you can download using this link.\n\nLet\u2019s load data and see how it goes:\n\nLet\u2019s calculate how many passengers (women and men) survived and how many did not, we will use .groupby as stated above.\n\nNow let\u2019s analyze the same data by cabin class:\n\nTerm \u201cpivot table\u201d is known for those who are pretty familiar with tools like Microsoft Excel or other spreadsheet instruments. In order to pivot a table in pandas you have to use .pivot_table method. I will demonstrate how to use it on our Titanic dataset. Let\u2019s assume that we have to calculate how many women and men were in a particular cabin class.\n\nNow DataFrame index will be Sex column values, new columns are represented by PClass values and aggregation function is count which calculate the number of passengers by Name column.\n\nPandas was created to analyze time series data. In order to illustrate how easy it is, I prepared sample dataset with Apple stock prices (5 year period). You can download it here.\n\nWe have created a DataFrame with DatetimeIndex by Date column and then sort it. If datetime column is different from ISO8601 format, then you have to use built-in pandas function pandas.to_datetime.\n\nBut what about specific time period?\n\nDo you want to know a mean of closing price by weeks? No prob.\n\nResampling is a very powerful tool when it comes to time series analysis. If you want to know more about resampling go ahead and dive into official docs.\n\nAt the very beginning of this post I said that pandas is built on numpy, when it comes to visualization pandas uses library called matplotlib. Let\u2019s see how Apple stock prices change over time on a graph:\n\nValues of X-axis are represented by index values of DataFrame (by default if you do not provide explicitly), Y-axis is a closing price. Take a look at 2014, there is a drop because of 7 to 1 split held by Apple.\n\nThat\u2019s it for the introduction to pandas. Hope you like it, if so please leave a comment below."
    },
    {
        "url": "https://codeburst.io/getting-started-with-data-analysis-in-python-2aaa4dbedd46?source=user_profile---------12----------------",
        "title": "Getting Started With Data Analysis in Python \u2013",
        "text": "Pandas is a Python package aimed to provide fast and flexible data structures designed to make working with data easy and intuitive. Do you want to load an csv file and easily manipulate the data in it? Do you want to replace missing values on your data or ignore them all together? Do want a quick statistic summary of your data? Well, pandas got you covered.\n\nFrom simple operations like the above to complex data filtering and slicing, pandas provides a set of tools to make working with data simple and efficient. Pandas aims to be the most powerful and flexible open source data analysis / manipulation tool available in any language.\n\nLoading data with pandas is quite easy. The library provides methods to load data from Excel files(xls, xlsx), csv, json, pickle, sql and others. For this example we will be using a mock data generated with mockaroo.\n\nThis operation will return a pandas.DataFrame object, a table like data structure that will make it easier for us to manipulate or data set and extract information. From now on df will be the representation of our DataFrame.\n\nPandas provides some methods to visualize the data we are working on.\n\nUsed to visualize the first few rows on our DataFrame, the default value is 5.\n\nSimilar to df.head(), will return the last few rows on our DataFrame.\n\nEvery pandas DataFrame has an immutable ndarray implementing an ordered, sliceable set. The basic object storing axis labels for all pandas objects. This works as an index for the table.\n\nColumn labels to use for identifying, filter and selecting data. Will default to np.arange(n) if no column labels are provided. When using methods like read_csv() or read_excel() the first line will be used as columns, unless explicitly told otherwise. when reading data from a database, the table\u2019s columns will be used as columns for the DataFrame.\n\nDescribe shows a quick statistic summary of your data, on the numeric columns, in our case, only in the id column.\n\nSelecting a single column, which yields a Series, equivalent to df.first_name. Its possible to use methods like df.head() and df.tail() in a partial DataFrame.\n\nPandas also support python dict like syntax for accessing columns.\n\nSelecting via [], which slices the rows.\n\nThe df.loc property takes to arguments, the first is the indexes of the slices, since was left open, i.e, df.loc[ : ], there isn\u2019t any row slicing. The second argument is the names of the columns we desire to slice.\n\nWe could combine our last 2 examples in one, like this:\n\nFor getting fast access to a single value\n\nPandas has support to Boolean indexing, with that we can filter our data based on the value of a column\n\nWe can also filter using multiple values, using the builtin function df.isin()\n\nWorking with missing or incomplete data can be trick, but pandas makes it easy.\n\nPandas assigns missing values with a numpy.NaN value, we can use this information to remove the rows or columns with missing data, or replace the missing values to another of out choosing.\n\nWe can use pandas df.dropna() to remove incomplete data from our DataFrame.\n\nThere\u2019s a couple of arguments to look when using df.dropna(). First there\u2019s the argument how, witch can receive to values:\n\nThe argument how=\u2019any\u2019 is the default and will drop any row(or column) with any missing data, the second, how=\u2019all\u2019, will drop any row or column where all values are missing, this can be useful to trim rows or columns from malformed data, like excel files with headers and footers.\n\nThe last thing I want to share on this topic is the argument thresh, witch will drop any rows or columns with a n number of missing values.\n\nComparing this last example with the one where we removed the columns we can see that the column ip_address was preserved, since it didn\u2019t met the threshold necessary to be dropped.\n\nPandas is a really powerful and fun library for data manipulation / analysis, with easy syntax and fast operations. This article is just the tip of the iceberg, is possible to do much more explore the rest of the tools that pandas provides, and I encourage you guys to try it and share your experiences.\n\nFollow me as I write about Algorithms ,Competitive Programming , Python , Web Development,Machine Learning ,Deep Learning and Data Science and don\u2019t waste your time by writing rubbish,irrelevant or long answers . Have a nice day !!!"
    },
    {
        "url": "https://medium.com/@harrypotter0/twitter-bot-under-50-lines-using-javascript-29ca588a3102?source=user_profile---------13----------------",
        "title": "Twitter bot under 50 lines using Javascript \u2013 Akash Kandpal \u2013",
        "text": "My latest project started with the decision to repurpose one of my testing repos as documentation for how to use the npm package. I assume that you have installed node and npm installed on your system if not kindly refer to this .\n\nThis article is meant as a reference for me and anyone else that\u2019s interested in Twitter bots in JavaScript.\n\n2. Move into the repository and type . This installs some code to the subdirectory, which you don't need to worry about. (It's Twit, the library that lets us talk to Twitter.\n\n3. At this point you need to register a Twitter account and also get its \u201capp info\u201d.\n\nSo create a Twitter account for whatever account you want to tweet this stuff. Twitter doesn\u2019t allow you to register multiple twitter accounts on the same email address. I recommend you create a brand new email address (perhaps using Gmail) for the Twitter account. Once you register the account to that email address, wait for the confirmation email. Then go here and log in as the Twitter account for your bot:\n\nOnce you\u2019re there, fill in the required fields: name, description, website. None of it really matters at all to your actual app, it\u2019s just for Twitter\u2019s information. Do the captcha and submit.\n\nNext you\u2019ll see a screen with a \u201cDetails\u201d tab. Click on the \u201cSettings\u201d tab and under \u201cApplication Type\u201d choose \u201cRead and Write\u201d, then hit the update button at the bottom."
    },
    {
        "url": "https://medium.com/@harrypotter0/list-of-blogs-articles-for-web-developer-to-follow-e4d234c3914b?source=user_profile---------14----------------",
        "title": "List of Blogs /Articles for Web Developer to follow",
        "text": "I have started my journey of Web Development in March,2017 . Now here I thought to collect all list of good blog posts , tutorials to keep us updated with the new tricks , habits (style of coding) of top Web Developers . Here goes the list \u2026.\n\nThis list is not complete . I would like you guys to complete the list by sending me more useful resources below in comments :)"
    }
]