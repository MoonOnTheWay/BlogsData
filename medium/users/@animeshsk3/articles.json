[
    {
        "url": "https://buzzrobot.com/energy-preserving-neural-networks-c26629b9553b?source=user_profile---------1----------------",
        "title": "Energy Preserving Neural Networks \u2013",
        "text": "In the current deep learning era, there is a neural network architecture available for possibly every problem; for instance, ResNet / CNN for computer vision problems, RNN with attention for Language / Speech problems and so on. The Deep Learning models have surpassed almost all the traditional Machine Learning techniques and are the current SOTA for various problems that were deemed to be impossible to achieve for a computer.\n\nHowever, if we take a deep look at the primary computations performed by a neural network, we can obtain better insights about the intricate details of its working. One prominent phenomenon that I observed is that the norm of the data that we process through the network changes to a certain extent. In order to address this problem, the use of batch-normalization layer is common. But it doesn\u2019t fully solve that problem as it assumes the data to come from a Gaussian distribution (since we only normalize across mean and variance in BN layer).\n\nAnother way to visualise a neural network is to consider it as a series of computations (neural computations) over an input data signal. Basically, what happens is that across the neural computations, the energy of the signal doesn\u2019t remain constant; it might drastically increase (use of high growth non-linear activation) or it might drop down too based on the computations. My thought on this is that if the network computations are supposed to resemble the real operations performed by the brain, then the primary rule they must follow should be of Energy-Conservation. I.e. the computations should preserve the norm of the input data."
    },
    {
        "url": "https://buzzrobot.com/mathematical-optimization-simplicity-is-all-you-need-d0cea72b8d5a?source=user_profile---------2----------------",
        "title": "Mathematical Optimization: \u2018simplicity is all you need\u2019",
        "text": "Much of adam\u2019s complexity is what accounts for it\u2019s performance. I perceive that this complexity arises from the fact that the cost dimension is considered separate from the parameter dimensions. This is where I like to quote the concept of spacetime (not space-time) from the Einstein\u2019s theory of General Relativity. \u201cSpace and time are not separate, but one\u201d. Following this principle, I discovered that using the cost dimension in the update equation can hugely affect the optimization. I hypothesise that \u2018the cost dimension and the parameter dimensions are not separate and should be used together while making updates during optimization\u2019.\n\nWith this thought, I furthered my study and found the Newton-Raphson equation for root finding.\n\nNote that the dy_dx appears in the denominator as opposed to the original gradient descent algorithm. According to the Newton-Raphson method, the derivative term is inversely proportional to the update value. Perhaps this is the reason why gradient descent is so heavily dependant on the alpha to balance out this inverse relationship.\n\nIn order to check it\u2019s validity, I tried running it on a one dimensional function. And, what\u2019s a better function than the square function to test it.\n\nRunning the Newton-Raphson algorithm on this function produced a very smooth optimization curve. It was obtained by running the algorithm for 50 iterations and starting with x = 1000.\n\nThe cost dropped to zero (close to ) in less than 10 iterations. This showed a lot of promise and convinced me of the presence of some meaning to my original hypothesis.\n\nThe question that arises is: Can we use Newton-Raphson algorithm directly in Machine Learning? Well, the answer is No. But it definitely gets very close.\n\nIn case of Machine Learning, the objective functions are defined as positive real valued functions, because of which, the minimum value of it could only be 0 (or greater) and the function wouldn\u2019t cut the parameter axes. It could only smoothly touch it. So, then that\u2019s perfect for using Newton-Raphson, right? Finding the root is equivalent to minimising it. Isn\u2019t it? Well, unfortunately it cannot be guaranteed that a root exists and assuming that it does is just too optimistic which I learnt the hard way.\n\nI studied the behaviour of Adam closely to find out what makes it so good. And, I found out that the momentum term in the numerator and rms term in the denominator is kind of emulating the inverse derivative behaviour. This (and a series of trial and error equation finding) lead me to the following update equation.\n\nPlease note that the (dy_dx)\u00b2 is just the square of the derivative term and not the second order derivative of y wrt. x.\n\nUsing this equation on the square function produced the following optimization curve:\n\nQuite smooth right! When it comes to optimization equations, the study cannot be complete without testing it on the exponential function.\n\nHonestly, I was a little worried, because the derivative of exp(x) is itself. So, is the equation stable enough on the unshaken, undeviating (differentiation proof): exp(x)?\n\nWell, yes, the equation is stable on exponentiation also. In the next section, I will explain why it is stable."
    },
    {
        "url": "https://medium.com/@animeshsk3/adam-momentum-y-aka-cost-terms-1938aade20b9?source=user_profile---------3----------------",
        "title": "adam - momentum + y (aka. cost) terms \u2013 Animesh Karnewar \u2013",
        "text": "With reference to my previous blog, I had earlier posted it as an update equation for mathematical optimization. It was the Newton-Raphson method for finding roots of an equation. I thought this method mostly applies for minimization in machine learning as cost is always defined as a positive real valued function. But it was pointed out to me that the update equation of newton-raphson method, which is\n\nis unstable at local minima (where dy_dx = 0) since it makes the update burst to infinity.\n\nI kept playing with this equation more since it gave me the smoothest curve for what I had been trying to do here, and, I was firm on the idea that the update equation must include the cost term (the Einstein\u2019s space-time analogy: The dimension of cost is connected with all the parameter dimensions).\n\nlink to code here. The work is found in the last section of the notebook: \u201cEquation 2: modified from newton-raphson method\u201d.\n\nTo relate this update equation with the title: if we consider the update portion of the equation as a separate function,\n\nIt is quite similar to adam optimization update, since there is a squared gradient term in the denominator and the gradient term in the numerator. This equation doesn\u2019t use momentum (running averages) and, the learning_rate is replaced by cost (y).\n\nNow, in Adam, the alpha is initially manually set and is utilised as per the decay equation:\n\nWith the equation that I have mentioned, the hypothesis is that this decay is kind of estimating the cost term itself.\n\nLink to the 3d curve of the g(x, y) function is here.\n\nPlease let me know what you think about this hypothesis and what it\u2019s implications are. I would be thankful and highly grateful if you could point me to some more relevant research so that I can move forward with this."
    },
    {
        "url": "https://buzzrobot.com/implementing-a-capsule-network-bd944d6ce515?source=user_profile---------4----------------",
        "title": "Implementing a Capsule Network . . . \u2013",
        "text": "The first step that I feel should be towards building this architecture is to go through the paper and completely understand the concept of it. There are many articles that have popped throughout the web that attempt to explain it, but I observed that the paper itself available here is very well drafted and presents a very insightful explanation.\n\nOne more resource, that everyone would agree, for understanding the paper is Aurelian\u2019s video on YouTube here. This explanatory video has received tremendous accolades for its simplicity and the explanatory power. In fact, the authors themselves have praised it. I would highly recommend the readers to check it out if haven\u2019t already.\n\nThe code has been made available on my repository here. I have attempted to document this jupyter notebook as much as possible, but if there are any more improvements related to the documentation or otherwise, feel free to open a PR.\n\nSince, the framework used is TensorFlow, I\u2019ll skip over the usual errands like creating placeholders, one hot encoding labels, defining optimizer, init op, etc. that we have to take care of while writing TF graphs and primarily focus on the important / challenging aspects of the implementation.\n\nThe first layer is a simple convolutional layer with a filter size of (9 x 9) with a stride of 1 and \u201cvalid\u201d padding. This can be directly implemented using the TensorFlow\u2019s layers api here.\n\nThe second layer known as the PrimaryCaps layer is again very simple to implement. It is same as the first layer (with a stride of 2) with a small difference in the activation function (block nonlinearity) for the conv layer. We have to use the squash function as defined in the paper.\n\nThe epsilon is used to make the computations stable, because even a single zero in the vector can infest the computations with nan values.\n\nThis layer is the crux of the entire algorithm, i.e. the dynamic routing algorithm between the capsules in adjacent layers. The implementational details are as follows:\n\nThe output of primaryCaps layer need to be reshaped into capsule form. So, the (6 x 6 x 256) volume needs to be reshaped as (1152 x 8 x 1) which corresponds to a flat layer of 8 dimensional capsules. It is from these capsules that we later generate the final output for the digits of size (10 x 16). Note that in the actual code, there will be one more dimension for the batch size at the beginning of these tensor shapes.\n\nThere need to be a (1152 x 10 x 16 x 8) dimensional trainable tensor of matrices for generating the ui|j prediction vectors. The 1152 is the number of capsules in the previous layer and 10 is the number of digits capsules (one for each digit). The capsules in the previous layer are 8 dimensional while the digits caps are 16 dimensional, hence the last two dimensions.\n\nUsing the TensorFlow\u2019s matMul operation, we can obtain all the matrix multiplications at once. Which would result in (1152 x 10 x 16 x 1) number of prediction vectors. Basically, every capsule in the previous gives it\u2019s 10 predictions for the outputs of the digit capsules.\n\nWhat we do next is the most novel, interesting and a bit challenging part: We need to attend over this (1152 x 10) volume of 16 dimensional vectors much like the content based attention used in typical seq2seq models. Note that there would be (1152 x 10) attention weights (initially all set to 0 so that softmax is an equal distribution) and we tf.reduce_sum this in the 1152 dimension to obtain the required predictions of size (10 x 16). But wait! this is just the first round. We need to repeat this procedure some n number of times. The paper has recommended the use of n = 3.\n\nTo go over the loop, this time around, we have to update the attention weights such that they capture the agreement between the last two layers better than the last time (which was equal distribution for the 1st iteration). To do that, we simply obtain scalar products of the digit caps output with the predictions and add those scalar products to the attention weights. And continue this process n times. This can be implemented using the TensorFlow\u2019s while_loop construct mentioned here. The code for this is available in the aforementioned repository. Viola! we have implemented the dynamic routing algorithm.\n\nThe main loss function used for optimization is defined as:\n\nwhere Tk is the vector denoting if the digit is present in the image or not and vk is the array of the output vectors. It\u2019s implementation is pretty straightforward and doesn\u2019t pose any challenges.\n\nThe paper has also included the reconstruction module which takes the correct digit vector (16 dimensional) to regenerate the images it was derived from using three fully connected layers. The reconstruction loss is added with a very small lambda scalar multiplier to the main loss.\n\nUpon training for 10 epochs, I obtained the following output from the reconstruction module:"
    },
    {
        "url": "https://medium.com/@animeshsk3/ranik-optimizer-mathematical-optimization-for-machine-learning-using-insights-from-physics-561e648d4a82?source=user_profile---------5----------------",
        "title": "\u2018Mathematical Optimization for Machine Learning\u2019 using insights from Physics",
        "text": "In this section, I mention a brief background of the concept of optimization and the basics of physics. Please note that I do not possess the knowledge of the intricate details involved in physics and because of which the insights used for deriving the new equation may not be fully accurate from the standpoint of physics. However, for us i.e. Machine Learning folks, this definitely provides a good direction to move forward since the experimental results show abundant promise.\n\nThe concept of Mathematical Optimization forms a distinct field of study and is quite vast. Studying and covering all the possible algorithms used in this field is out of scope for this article. The wikipedia page for this summarizes many of the algorithms.\n\nIn fact, while studying Machine Learning, we majorly concentrate on the technique called Gradient Descent. Various modifications have been suggested till now for optimizing the parameter update. The current standard variant of the original gradient descent that we all use is the Adam optimizer (Kingma and Ba, 2015).\n\nTo describe the process in short, there is a function J(w) which is commonly known by the names: objective function or loss function or cost function and others. The objective is to update the parameters w such that the function has a minimum value for the updated parameters. Original Gradient descent proposes the following as the update rule:\n\nwhere the alpha is the manually set learning_rate and the term gradient is the derivative of the objective function with respect to the parameter w.\n\nThe update equations for the Adam optimization are:\n\nThe learning rate is the most important hyperparameter that if not tuned properly can cause the algorithm to diverge instead of converging. So, this was also one of the motivations for developing the equation (i.e to remove hyperparameters from the update equations)."
    },
    {
        "url": "https://medium.com/mlreview/aann-absolute-artificial-neural-network-ae8f1a65fa67?source=user_profile---------6----------------",
        "title": "Absolute ANN: A simplified approach for structuring the learnt representations",
        "text": "Since structurally, the AANN is similar to an FCNN which has been around for about decades now. So, what makes the AANN different from the FCNN? There are three key distinctions between them: first is the way it is trained, second the cost function (mathematical objective function) that it optimises and finally the activation function it uses. Let\u2019s look at how the network is trained:\n\nI firstly worked with two separate neural networks for the encoder and the decoder part and made the technique work for it. Upon experimenting further, I discovered that we can also use the same encoder network for the decoder part as well (Tying the weights of an autoencoder). So, conceptually, it can be visualised as using a fully connected NN in the reverse direction. This makes the structure exactly same as that of an FCNN. On a single neuron level, it can be imagined as a network of the bidirectional neurons depicted below.\n\nThis is the link to the code for this technique (The repo contains some more ideas that I have in mind). In the following sections, I will step through the modifications that I made to the AE architecture in order to reach the AANN. The video is not a replacement for this article (or vice versa). There are certain aspects that I touch here while the others are covered in the video. In this article, I\u2019ll also share some of the details of the experimentation that I did in order to make this architecture work.\n\nThe explanation of the AANN technique is presented using the MNIST dataset, which is regarded as the \u2018Drosophila\u2019 of Deep Learning by Geoffrey Hinton. Here is the link to the video in which I explain the technique from a Neural Network\u2019s standpoint (without mentioning the GANs or AEs).\n\nBasically, the last layer ensures that the representation vectors are just as long as there are labels. And the vectors tend to be closer to the label axis due to the cost defined using the cosines of the angles made by the vector with the label axes. Thus, in the last but one layer, the representations belong to a more complicated (even tangled) and high dimensional space (depends on the no. of hidden neurons used in that layer. I used 512 neurons); but, in the last layer the representations belong to a very structured m-dimensional space. Due to the cost function, what happens is that the network tries to separate the vectors and make them as close as possible to the label axis. The trick here is that I am only clustering based on the angles, thus the magnitude of the vectors allow them to encode the mapping information on the corresponding axes. As an example, let\u2019s say there are only four digits \u2014 0, 1, 2 and 3. So the representation vectors of 3 would look like: [0, 0, 0, 10.33] or [0, 1e-6, 0.001, 99.63] and so on.. That is, all the information of the digit 3 is encoded in a positive (will explain why positive? in the next section) real number range in the specific position for that digit in the representation.\n\nThe input n-dimensional feature vector is passed through the neural network consisting of hidden layers, constructed from the bidirectional neurons, to obtain an m-dimensional representation vector; where m corresponds to the number of labels. The obtained representation vector is then converted into a unit vector, which corresponds to the cosines of the angles made by the representation vector with the coordinate axes. Finally, the forward cost is computed as the mean absolute difference between the unit representation vector Y\u2019 and the one-hot-encoded-label vector Y.\n\nThus the network now not only needs to be able to classify the MNIST digits, but also be able to generate them back given a digit query vector.\n\nSo, after performing the forward and the backward passes, we obtain the forward and the backward costs and finally to define the main cost function for optimization, it is just the sum of the two.\n\nFor the reverse pass, note that I feed in the original representation vector and not the directional cosines normalised (unit) vector back in the network. By feeding backwards, I mean the network computations are done using the transpose of the weight matrices and a different set of bias vectors for the reverse direction. The cost computed is again just the mean absolute difference between the original image (input vector) and the reconstructed image (the backward generated vector). There is no trick here, it is same as the decoder part of an Autoencoder.\n\nThe network obtains a forward accuracy of 99.86% on the train set (containing 39900 images) and 97.43% on the dev set (containing 2100 images). These results are alright! I mean on MNIST dataset, you can do that easily. But where the network shines is in generating the digits back from the same set of weight values. Take a look:\n\nI feel that there should be some metric that allows us to calculate a score for how well the network could generate the images in the backward direction. So, only looking at the accuracy should not be it.\n\nThere are some more key observations here, but first take a look at this video of the digits visualisation that I made:\n\nThe first graph is for the vectors that are fed into the network with the linear interpolation of values in the the range [0\u201380]. The second graph is the activations generated on the last but one layer in the forward direction and the third one is the actual digit generated by the network.\n\nNote how simple it is to feed in representation vectors into the network in order to generate the digits. (No sampling from a random distribution required). Next is that the generated digits transform from one form to another (while being what they are) smoothly. So, the network has learnt a differentiable function from the representations to the images, which means that these are not simple input output mappings (which is the case for a simple forward fully connected network). Also, notice the representations at the last but one layer (middle graph of the video). They correspond to the typical representations that we obtain using a traditional AE (for feature extraction).\n\nThis concludes the explanation of the AANN technique. I would like to mention a few more points about the activation function used for this network and would also like to make some final comments regarding the future scope of it.\n\nWell, the answer is both yes and no. The cost function definition was quite lucid given all the already done work on the AEs. I had the direction-magnitude trick for penalizing the cost function of an AE in mind for a long time. However, making this cost function work was the difficult part.\n\nThis architecture didn\u2019t work directly for the first time (in fact, I tried at least 25 different models before I found the above explained model). I realised very soon that it is the activation function used in the network that is keeping the network from adjusting itself to minimise both the costs. As it turns out, using the Absolute valued function as the activation function for the network allows it to optimize this hybrid objective function. (I mentioned above positive real number ranges. This is the reason why the activations are always positive. We are using the absolute function as the activation function). And, hence the name: \u201cAbsolute Artificial Neural Network\u201d. This was the difficult part, as I mentioned, because, it is not one of the standard activation functions used for the Neural Networks. I thought of this function since I was trying to create a symmetric ReLU.\n\nThese are some of the observations that I made while trying out the available activation functions and that is how I finally concluded with the use of abs function. (a) Upon using the ReLU, i.e. Rectified Linear-Unit, function as the activation function for this architecture, all the activations shoot to nan in the forward direction leading to proliferation of nan in the reverse direction as well (gradients exploding). If the Linear activation function is used, the network performs poorly in the forward direction, leading to very high classification error rates, while, the network converges to the point that it outputs the same structure as shown in (b), for every possible representation vector. On activating the hidden neurons with a ReLU in the forward direction and with an Abs in the reverse direction, the network kills all the activations, i.e. outputs the zero vector for every input, in the forward direction. In the backward direction, the network converges to the structure shown in (c). Upon using the Abs function in the forward direction and the ReLU in the backward direction, the network this time kills all the activations in the backward direction as visualized in (d). The (e) in above figure is the output achieved by using the Sigmoid activation function in the network. The result obtained is very similar to the result of using Linear activation function, as in (b).\n\nI would like to especially highlight the case where we use ReLU in the forward direction and Abs in the backward direction. This is what lead me to the use of absolute function everywhere. Firstly, by using ReLU forward and linear backward generated some grey coloured images, which I knew are caused by negative values. So, I thought how about I use the abs function to visualize what is getting generated in the backward direction. When I did this, the network converged to a point (forward cost decreased and backward cost increased) that the network outputted zero vectors for all the inputs in the forward direction. In fact, this convergence was so strong that when I tried to train the network only in the forward direction with ReLU, the weights didn\u2019t move. All the gradients vanished. This is something that I am still trying to understand why such a phenomenon occurs. Anyway, this lead me to try Abs in the forward direction as well, and that\u2019s it. It worked!"
    }
]