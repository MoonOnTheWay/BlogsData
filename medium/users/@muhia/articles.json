[
    {
        "url": "https://hackernoon.com/8-deep-learning-best-practices-i-learned-about-in-2017-700f32409512?source=user_profile---------1----------------",
        "title": "8 Deep Learning Best Practices I Learned About in 2017",
        "text": "Transfer learning by fine-tuning VGG-16 and ResNext50. (computer vision and image classification) For image classification work, you can get a lot of mileage by fine-tuning, for your specific problem, a neural network architecture that has done well on a more general challenge. An example is the residual network ResNext50, a 50-layer convolutional neural network. It was trained on the 1000 categories of the ImageNet challenge, and because it performed very well, the features it was able to extract from the image data are general enough to be reused. To get it to work for my problem domain, what I needed to do was replace the last layer, which outputs a 1000-dimensional vector of ImageNet predictions, with a layer which outputs a 2-dimensional vector. The two output classes are specified in the folder called in the code snippet above. For the Spider vs. Scorpion challenge, I had the following: Notice that the two contents of the folder are themselves folders, each containing 290 images. An example diagram of the fine-tuning procedure is shown here, which retrains a 10-dimensional final layer: The learning rate is probably the most important hyper-parameter to tune for training deep neural networks. The way it is typically done, in a non-adaptive setting (i.e. not using Adam, AdaDelta or their variants), is by a DL practitioner/researcher running, in parallel, multiple experiments, each with a small delta difference between learning rates. This takes a terribly long time if you have a large dataset and is quite error prone, if you are inexperienced in the ways of building intuition with stochastic matrices. However, in 2015, Leslie N. Smith, of the US Naval Research Laboratory, found a way to automatically search for the optimal learning rate by starting from a very small value, running a few mini-batches through the network, and tweaking the learning rate while tracking the change to the loss, until the loss starts decreasing. Two blog posts by fellow fast.ai students explaining the cyclical learning rates method are here and here. In fastai, you take advantage of learning rate annealing by running on your learner object, and to identify the point which coincides with the optimal learning rate. A screenshot:\n\nAnother method for speeding up stochastic gradient descent involves gradually decreasing the learning rate as training progresses. What this helps with is noticing when changes to the learning rate coincide with improvements in the loss. As you get closer to the optimal weights, you want to take smaller steps, because if you take large steps you might skip the optimal region of the error surface. If the relationship between the learning rate and loss is unstable, i.e. if a small change to the learning rate results in a large change in the loss, then we\u2019re not in a stable region (image 2 above). Then the strategy becomes to periodically increase the learning rate. The \u2018period\u2019 here is a number which determines how many times to increase the learning rate. This is the cyclical learning rate schedule. In fastai, this is set using and params to . In image 2 above, the learning rate is reset 3 times. It usually takes a much longer time to find the optimal loss when using the normal learning rate schedule, where a developer waits until all the epochs are complete before manually trying again with a different learning rate.\n\nData augmentation (computer vision and image classification \u2014 for now) Data augmentation is a simple method for increasing the amount of training and test data you have. For images, this depends on the learning problem at hand, and therefore on the number of symmetries in the images in the dataset. An example is the Spiders vs. Scorpions toy challenge. Many pictures in this dataset could be reflected vertically and still show the animal without weird distortions. This is called . Example:\n\nTest-time augmentation (computer vision and image classification \u2014 for now) We can also use data augmentation at inference time (or test time, hence the name). At inference time, all you\u2019re doing is making predictions. You could do it with the individual images in the test set, but the process becomes more robust if randomly generate a few augmentations of each image in the test set that is accessed. In fastai, 4 random augmentations per test image are used in the predictions, and the average of those predictions is used as the prediction for that image. A way to get a world-class sentiment analysis framework, without using word vectors, is to take the entire training data set that you intend to analyze and build a deep recurrent neural network language model from it. Save the model\u2019s encoder when the model has high accuracy and use the embeddings you get from the encoder to build a sentiment analysis model. This works better than the embedding matrices one gets from word vectors because RNNs can keep track of long range dependencies better than word vectors. The hidden state in a deep recurrent neural network can grow to an unwieldy size if it isn\u2019t reset after back-propagating some time-steps. For example, in a character-level RNN, if you have a million characters, then you also have a million hidden state vectors, each with their history. To adjust the gradients of the neural network, we then need to perform 1 million computations of the chain rule per character, per batch. This will consume too much memory. So, to lower the memory requirements, we set a maximum number of characters to back-propagate to. Since each loop in a recurrent neural network is known as a time-step, the task of limiting the number of layers through which back-propagation retains the history of the hidden state is called back-propagation through time. The value of this number determines the time and memory requirements of the model\u2019s computation, but it improves the model\u2019s ability to handle long sentences or sequences of actions. When doing deep learning on structured data-sets, it helps to distinguish columns that contain continuous data, such as price information in an online store, from columns that contain categorical data, e.g. dates and pick-up locations. One can then convert the one-hot encoding process for these categorical columns into a look-up table that points to an fully-connected embedding layer of a neural network. Your neural network therefore gets an opportunity to learn things about those categorical variables/columns that would have been ignored had the categorical nature of the columns been ignored. It can learn cyclical events, such as which days of the week have the most sales, what happens just before public holidays and just after, for multi-year data-sets. The end-result of this is a very effective method for predicting optimal pricing for products and collaborative filtering. This should be a standard data analysis and prediction method for all companies that have tabular data. Which is all of them. All companies should be using this. This method was applied in the Rossmann Store Sales Kaggle competition by Guo and Berkhahn, which got them to third place even if they only used deep learning with minimal feature engineering. They outlined their method in this paper.\n\nThe deep learning sub-field of AI is getting easier and easier to get into, as libraries get better. It feels like researchers and practitioners have hill-climbed their way, with bigger and bigger strides, enabled by hard work compiling large data sets and capable GPUs, to the grand achievement of releasing, in the open, a set of tools that promise to upend the course of human history. To my mind, the greatest potential lies in education and medicine, especially rejuvenation biotechnology. Even before we use deep learning to create artificial general intelligence, with the right policy, insight, motivation and global coordination, we\u2019ll be smarter, richer, and should expect to live much, much longer, healthier lives by the end of this century because of these tools."
    },
    {
        "url": "https://medium.com/@muhia/teaching-to-learn-a-deep-learning-case-study-b4688d1c5ffc?source=user_profile---------2----------------",
        "title": "Teaching To Learn: A Deep Learning Case Study \u2013 Brian Muhia \u2013",
        "text": "Wherein I selfishly teach a subject to learn it better and find my blind spots.\n\nI\u2019ve been taking a recent online deep learning course, Practical Deep Learning For Coders 1, due to fast.ai, a company that wants to make neural nets uncool again. Here\u2019s the first part of my story in n* parts.\n\nThe instructors, Jeremy Howard and Rachel Thomas, have a unique teaching philosophy that I deeply appreciate as a programmer. You only understand something well if you can explain it to someone else in such a way that they can build what you explained. Better yet, if you can explain it to a computer by actually writing the code. So, three weeks after I started taking this course, I decided to apply the \u201cteach to learn\u201d philosophy.\n\nOn Saturday, Sept. 23rd, I gave a presentation at the Nairobi Women in Machine Learning and Data Science meetup, which was hosted at the Moringa School. Since the purpose of the talk was to practice what I\u2019d just learned, I decided to demonstrate an interesting technique for prototyping a deep neural network, using an Excel spreadsheet. This is in lesson 2 of the first fast.ai course, in which we\u2019re encouraged (enthusiastically) to prototype things quickly using Excel spreadsheets, to get a better understanding of the algorithmic details, without the added interface of numpy, Keras and Tensorflow/Theano code that are typically used in neural net explainers.\n\nNot that those are problematic, it\u2019s just that it\u2019s very helpful to your intuition, to see explanations of ideas that seem abstract, and are very numerical, in many different kinds of media. While not exactly an explorable explanation, the spreadsheet I used to describe the details of a simple four-layer neural net is an excellent example of such a medium.\n\nI also took the opportunity to show some code, while explaining how I\u2019m participating in the challenges offered in the class. One of these, and what I showed, was part of my entry to the Dogs vs. Cats Redux challenge on Kaggle. The code for all of this is here, courtesy of fast.ai. This is a really cool example of the power of convolutional networks, which uses the former world-beating VGG16 neural net model to classify 2,000 images as either cats or dogs. Huddled together around a laptop, I walked people through the different code snippets that comprise a model that fine-tunes VGG16 to achieve 95% accuracy in the classification challenge.\n\nAll in all, I think that the talk fulfilled its stated purpose, for me, which was to explain something while keeping a look out for blind spots in my understanding. Here\u2019s what I noticed from the experience.\n\nOne thing I\u2019d skimmed when studying was the importance of the softmax function, which forces a neural network layer\u2019s output values to range between 0 and 1, thus becoming a probability distribution.\n\nAnother was the fact that the probability you get from the neural net after this operation is applied isn\u2019t a true probability distribution in the statistical sense, but a special kind that\u2019s only applicable to that model\u2019s training data.\n\nNext lesson: diagrams are important, even when playing with different media for explanation. This really helped when demonstrating the IPython notebook from the course, which had lots of neat visualisation techniques typically used in data science, including confusion matrices and animated graphs. These can be found in the notebook here.\n\nWhile no new results were shown except what people can run by simply cloning a github repository, I think that all 5 people who attended the meetup enjoyed the experience of seeing a well-known, surprisingly accurate visual recognition technique in action, and seeing the different hilarious ways it can fail to recognize a few errant examples.\n\nThanks to Kathleen Siminyu and Muthoni Wanyoike for reading a draft of this."
    },
    {
        "url": "https://medium.com/@muhia/what-michael-and-julia-arent-saying-e8bcb24768ac?source=user_profile---------3----------------",
        "title": "What Michael and Julia aren\u2019t saying \u2013 Brian Muhia \u2013",
        "text": "In a fascinating debate, researcher Michael Nielsen and CFAR\u2019s Julia Galef have been talking about the relationship between the accuracy of one\u2019s beliefs and the creative process in Science. Quoting Michael, who quotes Julia:\n\nA good example of this method is what I call the \u201cDrexler method\u201d, after K. Eric Drexler\u2019s article on How To Learn About Everything, which I recommend you quickly read, then come back, if you haven\u2019t.\n\nNotice the application of this technique. When you\u2019re studying something new and unusual (to you), you\u2019re more likely to understand the new points of view more clearly if you immerse yourself in the ideas for a while. Give your brain time to integrate these new concepts in the knowledge graph you already have.\n\nThis suggests a corollary to all of this: When you\u2019re learning, you should expect a brief moment of overconfidence after spending some effort that\u2019s rewarding in the sense that you feel like you\u2019ve learnt something new. This overconfidence is a risk, in that it might make your beliefs about other things inaccurate in the short term, for instance, due to the availability bias, but I think that it is a fundamental part of the learning process. Through some conscientious trial-and-error, your beliefs may converge to the true state of the world, from the point of view of your existing worldview + some understanding of the new ideas.\n\nYes, people who learn (or learn about) many different fields tend to collect many unusual points of view. In Michael\u2019s example, Feynman and Weber look, to my mind, like people who were rather good at learning.\n\nAnd this is what I don\u2019t see Michael and Julia saying explicitly, though I suspect they know this. To form accurate beliefs, you have to spend some effort learning. The newer the field, or the stranger the subject, to your point of view, I think the following is true:\n\nThere are many levels of skill in the art of learning, and what I see as the rationality community\u2019s biggest challenge, as well as its raison d\u2019etre, is to collect these methods."
    },
    {
        "url": "https://medium.com/@muhia/ahmed-kabil-on-feynman-on-the-connection-machine-56213b7b38c?source=user_profile---------4----------------",
        "title": "Ahmed Kabil on \u201cFeynman on The Connection Machine\u201d \u2013 Brian Muhia \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@muhia/some-2016-books-1d7b1dc5eaec?source=user_profile---------5----------------",
        "title": "(Some) 2016 Books \u2013 Brian Muhia \u2013",
        "text": "[Epistemic status: a list of books, most finished, that I\u2019ve come into contact with since March 2016. The purpose here, other than signaling, is to provide pointers to others who have similar tastes.]\n\nThis is the first in a (possibly quarterly) series. As a promiscuous reader who\u2019s developed a pathological reliance on lags in public transport to get his reading done, I haven\u2019t finished most of the books I\u2019ve opened. So here are some of the books I read in 2016. I particularly recommend those in bold. Impressions are mine, in italics.\n\nSeven Brief Lessons on Physics by Carlo Rovelli\n\nDr. Rovelli wrote this book in such a beautiful way that it changed how I talked for a few weeks. My only regret is reverting to my old ways of communicating the ideas I encounter from day to day. A gripping must-read for those who want to imagine the beauty of modern physics, while following the lead of a person who intuitively understands it.\n\nAn Astronaut\u2019s Guide to Life On Earth by Chris Hadfield\n\nProbably my favourite autobiography, next to that of Benjamin Franklin.\n\nFor a human, Colonel Chris Hadfield has accomplished amazing feats. Read Amazon\u2019s description of his life:\n\nHe has spent decades training as an astronaut and has logged nearly 4,000 hours in space. During this time he has broken into a Space Station with a Swiss army knife, disposed of a live snake while piloting a plane, been temporarily blinded while clinging to the exterior of an orbiting spacecraft, and become a YouTube sensation with his performance of David Bowie\u2019s \u2018Space Oddity\u2019 in space. The secret to Chris Hadfield\u2019s success \u2014 and survival \u2014 is an unconventional philosophy he learned at NASA: prepare for the worst \u2014 and enjoy every moment of it.\n\nDoing Good Better by William MacAskill\n\nAs one of the co-founders of the Effective Altruism movement, alongside Peter Singer, William does a great job of pointing at the reasons why one should dedicate a significant amount of planning and deliberation to the problem of how to make the most of their time when working to make a difference. A lot of the strategies that you might need to make a difference need don\u2019t come automatically, and there are many brilliant and good people willing to help if one acknowledges the massive room for improvement. Some of those people have formed a growing, global community. Many arguments were familiar to me, and I mostly used this as a source of anecdotes to use when discussing EA with people who haven\u2019t been exposed yet.\n\nImpossibility: The Limits of Science and the Science of Limits by John D. Barrow\n\nThis is a book about the continuing human struggle to understand just what it is, where it is, we are in this colossal void. On the journey, we\u2019ve found places where we can\u2019t look further. Where we have to work around our questions. Where even our definitions of what science is, have to change.\n\nMany times, the question of the origin of life has challenged us to grow up and look at the world with fresh eyes. This book is a chronicle of world-class biology research, with many open-ended questions left unanswered, and very interesting conclusions. This is one of those topics where every one is at the edge of our knowledge, but anybody who takes even half a step forward has the power to \u201crefactor\u201d most people\u2019s assumptions about the history of life on the planet. Highly recommended.\n\nAmazon: In The Vital Question, Nick Lane radically reframes evolutionary history, putting forward a cogent solution to conundrums that have troubled scientists for decades. The answer, he argues, lies in energy: how all life on Earth lives off a voltage with the strength of a bolt of lightning. In unravelling these scientific enigmas, making sense of life\u2019s quirks, Lane\u2019s explanation provides a solution to life\u2019s vital questions: why are we as we are, and why are we here at all?\n\nSuperforecasting: The Art and Science of Prediction by Philip E. Tetlock and Dan Gardner\n\nThis is a part history of Tetlock\u2019s research on human judgment of future events, and an explanation of observations made in his most recent large-scale experiment: the Good Judgment Project. I was familiar with his work, having read part of his earlier book \u201cExpert Political Judgment\u201d, and participated in the early versions of the GJ project, which is why I haven\u2019t finished the book. Some really interesting things about this book were the clustering of certain skillsets or habits into two groups (question-asking vs. superforecasting), very roughly mapping onto the hedgehog vs. fox dichotomy from Isaiah Berlin. Second was the public discussion of the research surrounding the ideas themselves, particularly, a disagreement between Tetlock and Daniel Kahneman on the former\u2019s cause for optimism on the expected reliability of superforecasters, and the latter\u2019s pessimism. There is a 5-part masterclass on superforecasting hosted by John Brockman and the other folks at Edge.org.\n\nThe Beginning of Infinity: Explanations that Transform The World by David Deutsch\n\nI\u2019ve just started this one, but I know I\u2019m going to love it. David here focuses on explaining what it is that makes for good explanations for the most fundamental phenomena in our universe. Why are some explanations so much better than others, is there a way to narrow them down that\u2019s more intuitively helpful to people than Occam\u2019s Razor? What are our limitations here?\n\nThis is very similar in spirit to Barrow\u2019s Impossibility. In fact, Deutsch referenced it early in the first chapter if I remember correctly, after which I slurped it up and started skimming.\n\nNote: About quarter of the way through, I listened to an Ezra Klein podcast episode featuring Patrick Collison, who said he loved it too, so I\u2019ll just add this here to spread the meme, even if I haven\u2019t finished it. I\u2019m really excited to see this one through.\n\nThe Age Of Em: Work, Love and Life when Robots Rule the Earth by Robin Hanson\n\nStatus: Unfinished. Waiting for a hardcover that I can focus on\n\nThis is a meaty book, drawing some deep inferences about the future of intelligence on the planet, from standard economic ideas. There\u2019s definitely something interesting here, especially how many ideas from standard economics, sociology, mechanism design, physics, ethics and philosophy you end up absorbing as you follow Hanson on this fantastic, weird journey through the lives of our possible descendants. To this mind, it seems like one to keep for a while and keep going back to, which is why I\u2019ve ordered a hardcover.\n\nI loved this book\u2019s mostly successful attempt at describing the inside-view of not one, but two, kinds of \u201cother\u201d minds. I can\u2019t resist the urge to spoil the whole story, but expect to be uncomfortable about attributing what you consider to be intelligent, strategic behaviour, to a machine. Please read this sci-fi novel.\n\nIf a dystopia is a world where the worst kinds of people manage to acquire horrifying power over others, then not only are we living in a world that will become that dystopia, but Morgan shows what it feels like to be both kinds of people: a normal person in that world, and all of the corrupt controllers of power in it. Some good world-building, sci-fi action.\n\nAmazon: In the twenty-fifth century, humankind has spread throughout the galaxy, monitored by the watchful eye of the U.N. While divisions in race, religion, and class still exist, advances in technology have redefined life itself. Now, assuming one can afford the expensive procedure, a person\u2019s consciousness can be stored in a cortical stack at the base of the brain and easily downloaded into a new body (or \u201csleeve\u201d) making death nothing more than a minor blip on a screen."
    }
]