[
    {
        "url": "https://heartbeat.fritz.ai/how-smartphones-manage-to-handle-huge-neural-networks-269debcb243d?source=user_profile---------1----------------",
        "title": "How smartphones manage to handle huge Neural Networks",
        "text": "Computers have large hard drives and powerful CPUs and GPUs. Smartphones don\u2019t. To compensate, they need tricks to run Deep Learning applications efficiently.\n\nDeep Learning is an incredibly versatile and powerful technology, but running neural networks can be pretty demanding in terms of computational power, energy consumption, and disk space. This is usually not a problem for cloud applications, running on servers with large hard drives and multiple GPUs.\n\nUnfortunately, running a neural network on a mobile device isn\u2019t that easy. In fact, even though smartphones are becoming more and more powerful, they still have limited computational power, battery life and available disk space, especially for apps which we like to keep as light as possible. Doing so allows for faster downloads, smaller updates, and longer battery life, all of which users appreciate.\n\nIn order to perform image classification, portrait mode photography, text prediction, and dozens of other tasks, smartphones need to use tricks to run neural networks fast, accurately, and without using too much disk space.\n\nIn this article, we\u2019ll see a few of the most powerful techniques to enable neural networks to run in real time on mobile phones.\n\nBasically, we are interested in three metrics: the accuracy of the model, its speed, and the amount of space it occupies on the phone. As there\u2019s no such thing as a free lunch, we\u2019ll have to make compromises.\n\nFor most techniques, we\u2019ll keep an eye on our metrics and look for what we call a saturation point. This is the moment where the gain in one metric stops justifying the loss in the others. By keeping the optimized values before the saturation point, we can try to have the best of both worlds.\n\nWith this approach in mind, let\u2019s get to it!\n\nFully connected layers are one of the most common components of neural networks, and they work wonders. However, as each neuron is connected to all neurons in the previous layer, they require storing and updating numerous parameters. This is detrimental to speed and disk space.\n\nConvolutional layers are layers that take advantage of local coherence in the input (often images). Each neuron is no longer connected to all neurons in the previous layer. This helps cut down on the number of connections/weights while retaining high accuracy.\n\nUsing few or no fully connected layers reduces the size of the model, while keeping high accuracy. This improves both the speed and the disk usage.\n\nIn the configuration above, a fully connected layer with 1024 inputs and 512 outputs would have around 500k parameters. A convolutional layer with the same characteristics and 32 feature maps would only have around 50k parameters. That\u2019s a 10X improvement!\n\nThis step represents a pretty straightforward tradeoff between model complexity and speed. Having many channels in convolutional layers allows the network to extract relevant information, but at a cost. Removing a few feature maps is an easy way to save space and make the model faster.\n\nWe can do the exact same thing with the receptive field of the convolution operations. By reducing the kernel size, the convolution is less aware of local patterns, but involves fewer parameters.\n\nIn both cases, the number of maps/kernel size is chosen by finding the saturation point so that the accuracy doesn\u2019t drop too much.\n\nFor a fixed number of layers and a fixed number of pooling operations, a neural network can behave quite differently. This comes from the fact that the representation of the data, as well as the computational load, depend on where the pooling operation is done:\n\nIn a trained neural network, some weights strongly contribute to the activation of a neuron, while others barely influence the result. Nonetheless, we still do some computation for these weak weights.\n\nPruning is the process of completely removing the connections with the smallest magnitude so that we can skip the calculations. This lowers the accuracy but makes the network both lighter and faster. We need to find the saturation point so that we remove as many connections as possible, without harming the accuracy too much.\n\nTo save the network on disk, we need to record the value of every single weight in the network. This means saving one floating point number for each parameter, and this represents a lot of space taken on the disk. For reference, in C a float occupies 4 bytes i.e 32 bits. A network with parameters in the hundreds of millions (such as GoogLe-Net, or VGG-16) can easily take hundreds of megabytes, which is unacceptable on a mobile device.\n\nTo keep the network footprint as small as possible, one way is to lower the resolution of the weights by quantizing them. In this process, we change the representation of the number so that it can no longer take any value, but is rather constrained to a subset of values. This enables us to only store the quantized values once, and then reference them for the weights of the network.\n\nAgain, we\u2019ll determine how many values to use by finding the saturation point. More values means more accuracy, but also a larger representation. For example, by using 256 quantized values, each weight can be referenced using only 1 byte i.e 8 bits. Compared to before (32 bits), we have divided the size by 4!\n\nWe have already fiddled with the weights a lot, but we can improve the network even more! This trick relies on the fact that the weights are not evenly distributed. Once quantized, we don\u2019t have the same number of weights bearing each quantized value. This means that some references will come up more often than others in our model representation, and we can take advantage of that!\n\nHuffman coding is the perfect solution for this problem. It works by attributing the keys with the smallest footprint to the most used values, and the keys with the largest footprint to the least used values. This helps reduce the size of the model on the device, and the best part is that there\u2019s no loss in accuracy.\n\nThis simple trick allows us to shrink the space taken by the neural network even further, usually by around 30%.\n\nNote: The quantization and encoding can be different for each layer in the network, giving more flexibility.\n\nWith the tricks we\u2019ve used, we\u2019ve been pretty rough on our neural network. We\u2019ve removed weak connections (pruning) and even changed some weights (quantization). While this makes the network super light and blazing fast, the accuracy isn\u2019t what it used to be.\n\nTo fix that, we need to iteratively retrain the network at each step. This simply means that after pruning or quantizing the weights, we train the network again so that it can adapt to the change and repeat this process until the weights stop changing too much.\n\nAlthough smartphones don\u2019t have the disk space, computing power, or battery life of good old fashioned desktop computers, they are still very good targets for Deep Learning applications. With a handful of tricks, and at the cost of a few percentage points of accuracy, it\u2019s now possible to run powerful neural networks on these versatile handheld devices. This opens the door to thousands of exciting applications.\n\nIf you\u2019re still curious, take a look at some of the best mobile-oriented neural networks, like SqueezeNet or MobileNets."
    },
    {
        "url": "https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42?source=user_profile---------2----------------",
        "title": "Memorizing is not learning! \u2014 6 tricks to prevent overfitting in machine learning.",
        "text": "Overfitting may be the most frustrating issue of Machine Learning. In this article, we\u2019re going to see what it is, how to spot it, and most importantly how to prevent it from happening.\n\nThe word overfitting refers to a model that models the training data too well. Instead of learning the genral distribution of the data, the model learns the expected output for every data point.\n\nThis is the same a memorizing the answers to a maths quizz instead of knowing the formulas. Because of this, the model cannot generalize. Everything is all good as long as you are in familiar territory, but as soon as you step outside, you\u2019re lost.\n\nThe tricky part is that, at first glance, it may seem that your model is performing well because it has a very small error on the training data. However, as soon as you ask it to predict new data points, it will fail.\n\nAs stated above, overfitting is characterized by the inability of the model to generalize. To test this ability, a simple method consists in splitting the dataset into two parts: the training set and the test set. When selecting models, you might want to split the dataset in three, I explain why here.\n\nWith this split we can check the performance of the model on each set to gain insight on how the training process is going, and spot overfitting when it happens. This table shows the different cases.\n\nNote: for this technique to work, you need to make sure both parts are representative of your data. A good practice is to shuffle the order of the dataset before splitting.\n\nOverfitting can be pretty discouraging because it raises your hopes just before brutally crushing them. Fortunately, there are a few tricks to prevent it from happening.\n\nFirst, we can try to look at the components of our system to find solutions. This means changing data we are using, or which model.\n\nYou model can only store so much information. This means that the more training data you feed it, the less likely it is to overfit. The reason is that, as you add more data, the model becomes unable to overfit all the samples, and is forced to generalize to make progress.\n\nCollecting more examples should be the first step in every data science task, as more data will result in an increased accuracy of the model, while reducing the chance of overfitting.\n\nCollecting more data is a tedious and expensive process. If you can\u2019t do it, you should try to make your data appear as if it was more diverse. To do that, use data augmentation techniques so that each time a sample is processed by the model, it\u2019s slightly different from the previous time. This will make it harder for the model to learn parameters for each sample.\n\nAnother good practice is to add noise:\n\nNote: In both cases, you need to make sure that the magnitude of the noise is not too great. Otherwise, you could end up respectively drowning the information of the input in the noise, or make the output incorrect. Both will hinder the training process.\n\nIf, even with all the data you now have, your model still manages to overfit your training dataset, it may be that the model is too powerful. You could then try to reduce the complexity of the model.\n\nAs stated previously, a model can only overfit that much data. By progressively reducing its complexity \u2014 # of estimators in a random forest, # of parameters in a neural network etc. \u2014 you can make the model simple enough that it doesn\u2019t overfit, but complex enough to learn from your data. To do that, it\u2019s convenient to look at the error on both datasets depending on the model complexity.\n\nThis also has the advantage of making the model lighter, train faster and run faster."
    },
    {
        "url": "https://hackernoon.com/stop-feeding-garbage-to-your-model-the-6-biggest-mistakes-with-datasets-and-how-to-avoid-them-3cb7532ad3b7?source=user_profile---------3----------------",
        "title": "Stop Feeding Garbage To Your Model! \u2014 The 6 biggest mistakes with datasets and how to avoid them.",
        "text": "If you haven\u2019t heard it already, let me tell you a truth that you should, as a data scientist, always keep in a corner of your head:\n\nMany people make the mistake of trying to compensate for their ugly dataset by improving their model. This is the equivalent of buying a supercar because your old car doesn\u2019t perform well with cheap gasoline. It makes much more sense to refine the oil instead of upgrading the car. In this article, I will explain how you can easily improve your results by enhancing your dataset.\n\nNote: I will take the task of image classification as an example, but these tips can be applied to all sorts of datasets.\n\nIf your dataset is too small, your model doesn\u2019t have enough examples to find discriminative features that will be used to generalize. It will then overfit your data, resulting in a low training error but a high test error.\n\nSolution #1: gather more data. You can try to find more from the same source as your original dataset, or from another source if the images are quite similar or if you absolutely want to generalize.\n\nCaveats: This is usually not an easy thing to do, at least without investing time and money. Also, you might want to do an analysis to determine how much additional data you need. Compare your results with different dataset sizes, and try to extrapolate.\n\nSolution #2: augment your data by creating multiple copies of the same image with slight variations. This technique works wonders and it produces tons of additional images at a really low cost. You can try to crop, rotate, translate or scale your image. You can add noise, blur it, change its colors or obstruct parts of it. In all cases, you need to make sure the data is still representing the same class.\n\nThis can be extremely powerful, as stacking these effects gives exponentially numerous samples for your dataset. Note that this is still usually inferior to collecting more raw data.\n\nCaveats: all augmentations techniques might not be usable for your problem. For example, if you want to classify Lemons and Limes, don\u2019t play with the hue, as it would make sense that color is important for the classification.\n\nIt\u2019s an easy one, but take time to go through your dataset if possible, and verify the label of each sample. This might take a while, but having counter-examples in your dataset will be detrimental to the learning process.\n\nAlso, choose the right level of granularity for your classes. Depending on the problem, you might need more or less classes. For example, you can classify the image of a kitten with a global classifier to determine it\u2019s an animal, then run it through an animal classifier to determine it\u2019s a kitten. A huge model could do both, but it would be much harder.\n\nAs said in the introduction, low quality data will only lead to low quality results.\n\nYou might have samples in your dataset f your dataset that are too far from what you want to use. These might be more confusing for the model than helpful.\n\nSolution: remove the worst images. This is a lengthy process, but will improve your results.\n\nAnother common issue is when your dataset is made of data that doesn\u2019t match the real world application. For instance if the images are taken from completely different sources.\n\nSolution: think about the long term application of your technology, and which means will be used to acquire data in production. If possible, try to find/build a dataset with the same tools.\n\nIf the number of sample per class isn\u2019t roughly the same for all classes, the model might have a tendency to favor the dominant class, as it results in a lower error. We say that the model is biased because the class distribution is skewed. This is a serious issue, and also why you need to take a look at precision, recall or confusion matrixes.\n\nSolution #1: gather more samples of the underrepresented classes. However, this is often costly in time and money, or simply not feasible.\n\nSolution #2: over/under-sample your data. This means that you remove some samples from the over-represented classes, and/or duplicate samples from the under-represented classes. Better than duplication, use data augmentation as seen previously.\n\nIf your data doesn\u2019t have a specific format, or if the values don\u2019t lie in the certain range, your model might have trouble dealing with it. You will have better results with image that are in aspect ratio and pixel values.\n\nSolution #1: Crop or stretch the data so that it has the same aspect or format as the other samples.\n\nSolution #2: normalize the data so that every sample has its data in the same value range.\n\nOnce your dataset has been cleaned, augmented and properly labelled, you need to split it. Many people split it the following way: 80% for training, and 20% for testing, which allow you to easily spot overfitting. However, if you are trying multiple models on the same testing set, something else happens. By picking the model giving the best test accuracy, you are in fact overfitting the testing set. This happens because you are manually selecting a model not for its intrinsic value, but for its performance on a specific set of data.\n\nSolution: split the dataset in three: training, validation and testing. This shields your testing set from being overfitted by the choice of the model. The selection process becomes:\n\nNote: Once you have chosen your model for production, don\u2019t forget to train it on the whole dataset! The more data the better!\n\nI hope by now you are convinced that you must pay attention to your dataset before even thinking about your model. You now know the biggest mistakes of working with data, how to avoid the pitfalls, plus tips and tricks on how to build killer datasets! In case of doubt, remember:\n\n\ud83c\udf89 You\u2019ve reached the end! I hope you enjoyed this article. If you did, please like it, share it, subscribe to the newsletter, send me pizzas, follow me on medium, or do whatever you feel like doing! \ud83c\udf89"
    },
    {
        "url": "https://hackernoon.com/using-q-learning-to-teach-a-robot-how-to-walk-a-i-odyssey-part-3-5285237cc3b1?source=user_profile---------4----------------",
        "title": "Using Q-Learning to teach a robot how to walk \u2014 A.I. Odyssey part. 3",
        "text": "In this episode: Q-Values, Reinforcement learning, and more.\n\nMake sure to check out part. 1 and part. 2 too!\n\nToday, we\u2019re gonna learn how to create a virtual agent that discovers how to interact with the world. The technique we\u2019re going to use is called Q-Learning, and it\u2019s super cool.\n\nLet\u2019s take a look at our agent!\n\nThis is Henry, he\u2019s a young virtual robot who has a dream: travel the world. The problem is he doesn\u2019t know much about the world. In fact, he doesn\u2019t event know how to move! He only knows his GPS location, the position of its feet, and if they are on the ground.\n\nThese elements can be split into two parts: the state, and the goal. The state of our agent is the ensemble of the informations relative to his body, while the goal is what the agent wants to increase. In our case, the agent want to use its feet to change its position.\n\nFortunately, Henry is capable of performing actions. Even though he cannot perceive much of the world, he know what he is capable of doing, and that depends on his state. For example, when both his feet are on the ground, he can lift one. And when one foot is in the air, he can move it forward or backward.\n\nTo reach his goal, the agent follows a set of instructions called a policy. These instructions can be learnt by the agent or written down manually by humans.\n\nLet\u2019s write down a simple policy for our agent to follow.\n\nThis looks promising, although the robot is loosing a lot of time lifting and putting down his feet. It\u2019s not really efficient.\n\nIn fact, we rarely write good policies manually. This is either because we don\u2019t know any (e.g. complex task), or because they are often not robust (if the agent somehow starts with the left foot up, the policy above would immediately fail as the agent cannot execute the desired action).\n\nTo help our agent fullfil his dream, we will use a reinforcement learning technique called Q-Learning to help the robot learn a robust and efficient policy. This technique consists in attributing a number, or Q-Value, to the event of performing a certain action when the agent is in a certain state. This value represents the progress made with this action.\n\nThis value is determined by a reward function, indicating whether the action had positive or negative impact on reaching the goal. In our case, if Henry moves away from where he was, it\u2019s good, if he\u2019s going back, it\u2019s bad.\n\nWith the knowledge of all Q-Values, the agent could, at each step, pick the action which would bring the best reward depending on his state. This would be his policy.\n\nThe issue here is that Henry doesn\u2019t have a clue of what the Q-Values are! This means he is not able to pick an action that will bring him closer to his goal!\n\nFortunately, we have a way of making him discover the Q-Values by himself.\n\nTo learn the Q-Values of its State-Action pairs, Henry must try these actions by himself. The problem is that there might be billions of possible combinations, and the program cannot try them all. We want Henry to try as many combinations as possible, but focus on the best ones.\n\nTo achieve that, we are going to use the Epsilon-Greedy algorithm to train the robot. It works like this: at each step, the robot has a probability epsilon of performing a random available action, otherwise it picks the best action according to the Q-Values it knows. The result of the action is used to update the Q-Values.\n\nA simple way to update the Q-Values would be to replace the value the robot has in memory, by the value he has just experienced by making the action. However, this brings a really problematic issue: the robot cannot see more than one step in the future. This makes the robot blind to any future reward.\n\nThe solution to this problem is given by the Bellman equation, which proposes a way to account for future rewards.\n\nFirst, instead of replacing the old value by the new value, the old value fades away at a certain rate (alpha, the learning rate). This enables the robot to take into account noise. Some actions might work sometimes, and sometimes not. With this progressive evolution of the Q-Value, one faulty reward doesn\u2019t mess up the whole system.\n\nAlso, the new value is calculated using not only the immediate reward, but also the expected maximal value. This value consists of the best possible reward we expect to receive from the actions available. This has a dramatic impact on the effectiveness of the learning process. With this, the rewards are propagated back in time.\n\nBy playing on the value epsilon, we are facing the dilemma of Exploration vs Exploitation. An agent with a high epsilon will mostly try random actions (exploration), while an agent with a low epsilon will rarely try new actions (exploitation). We need to find a sweet spot that will enable our robot to try many new things, without wasting too much time on unpromising lead.\n\nHere, the agent that focused more on exploration discovered a very effective technique to move. Contrarily, the yellow robot doesn\u2019t take full advantage of the extension of his feet because it didn\u2019t spend enough time trying random actions. Note that a balance must be found, as spending too much time on exploration prevents the agent from learning complex policies.\n\nLook at that! Henry\u2019s travelling the world at an astonishing pace! He managed to find by itself a better way to move than the simple policy we gave him, plus he can recover from small errors, as from any state he only has to follow his trusted Q-Values to guide him!\n\nYou can play with the code here:\n\n\ud83c\udf89 You\u2019ve reached the end! I hope you enjoyed this article. If you did, please like it, share it, subscribe to the newsletter, send me pizzas, follow me on medium, or do whatever you feel like doing! \ud83c\udf89"
    },
    {
        "url": "https://hackernoon.com/the-implications-of-adversarial-examples-deep-learning-bits-3-4086108287c7?source=user_profile---------5----------------",
        "title": "Adversarial Examples and their implications - Deep Learning bits #3",
        "text": "In this article, we are going to talk about adversarial examples and discuss their implications for deep learning and security. They must not be confused with adversarial training, which is a framework for training neural networks, as used in Generative Adversarial Networks. Adversarial examples are handcrafted inputs that cause a neural network to predict a wrong class with high confidence. Usually, neural network errors occur when the image* is either of poor quality (bad cropping, ambiguous, etc.) or contains multiple classes (car in the background etc.). This is not the case for adversarial examples, which look like ordinary images. *In this post, we will focus on images as they provide interesting visual support, but keep in mind that this can be applied to other inputs such as sound.\n\nWhile the first two mistakes are understandable, the third image definitely looks like a temple, and we can think that any properly trained neural network should be able to make the correct prediction. What\u2019s going on here then? The specificity of adversarial examples is that they do not occur in natural data, they are crafted. An adversarial attack against a neural network is a process in which someone slightly modifies an image so that it fools the network. The goal is to minimize the perturbations to the original image, while obtaining a high confidence for the target class.\n\nHow is this done? The generation of adversarial examples is a vast topic, and new techniques are being discovered to create faster, more robust perturbations with minimal image distortion. We will not dwell on how these are generated to rather focus on their implications. But a general principle and simple method is to take the original image, run it through the neural network, and use the backpropagation algorithm to find out how the pixels of the image should be modified to reach the target class. Is this really a big deal? The first thing that we usually think of when we see adversarial examples is that they are unacceptable. As humans would classify them correctly without breaking a sweat, we intuitively expect any good model to do so. This reveals our intrinsic expectations for a neural network: we want human or super-human performance. \u201cIf a model fails to classify this as a Temple, then it\u2019s necessarily bad\u201d \u2014 Or is it? Let\u2019s step back for a minute and think about what this means. On a given task \u2014 e.g. identify road signs in a self driving vehicle \u2014 we wouldn\u2019t replace the human by a computer unless it is at least as good as the human. Something we often forget is that having a model that is better than a human does not imply any requirement on the failure cases. In the end, if the human has an accuracy of 96%, and the neural network of 98%, does it really matter that the exemples the machine missed are considered easy? The answer to this question is yes\u2026 aaaand no. Even though it\u2019s frustrating and counter-intuitive to see state-of-the-art models fail on what looks like trivial examples, this doesn\u2019t represent a fundamental issue. What we care about is how powerful and reliable the model is. We have to accept that our brain and deep learning do not work in the same way and therefore don\u2019t yield the same results.\n\nWhat does matter, though, is that adversarial attacks represent a security threat to AI-based systems. How can we maliciously exploit adversarial examples? Many kinds of Deep Learning powered systems could severely suffer from adversarial attacks if someone got their hands on the underlying model. Here are some examples. Create bots that don\u2019t get flagged by google\u2019s \u201cI\u2019m not a robot\u201d system That\u2019s for the virtual world. However, implementing such attacks on real life objects is significantly harder because of all the transformations involved when taking a picture of an object, but it is still possible. Robust Adversarial Example in the wild by OpenAI \u2014 The red bar indicates the most probable class for the image. Here the cat is classified as a desktop computer With this in mind you could imagine: Stealing the identity of someone by wearing special glasses What can we do against that? There are a few things we can do to mitigate this issue. First we can think of keeping the model private. However, this is solution has two big flaws. First, secure systems should ideally be built following Kerckhoffs-Shannon principle: \u201cone ought to design systems under the assumption that the enemy will immediately gain full familiarity with them\u201d. This means we shouldn\u2019t rely on the privacy of the model because one day or another, it will be leaked*. Then, some papers have been published on universal/model-independant adversarial attacks, that could potentially work for a specific task no matter which model is used. *Note: That is the reason why all databases are encrypted. When you think about it, there is no \u201cneed\u201d to encrypt your database if you think it will never be hacked."
    },
    {
        "url": "https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df?source=user_profile---------6----------------",
        "title": "Latent space visualization \u2014 Deep Learning bits #2 \u2013",
        "text": "Last time, we have seen what autoencoders are, and how they work. Today, we will see how they can help us visualize the data in some very cool ways. For that, we will work on images, using the Convolutional Autoencoder architecture (CAE).\n\nAn autoencoder is made of two components, here\u2019s a quick reminder. The encoder brings the data from a high dimensional input to a bottleneck layer, where the number of neurons is the smallest. Then, the decoder takes this encoded input and converts it back to the original input shape \u2014 in our case an image. The latent space is the space in which the data lies in the bottleneck layer.\n\nThe latent space contains a compressed representation of the image, which is the only information the decoder is allowed to use to try to reconstruct the input as faithfully as possible. To perform well, the network has to learn to extract the most relevant features in the bottleneck.\n\nLet\u2019s see what we can do!\n\nWe\u2019ll change from the datasets of last time. Instead of looking at my eyes or blue squares, we will work on probably the most famous for computer vision: the MNIST dataset of handwritten digits. I usually prefer to work with less conventional datasets just for diversity, but MNIST is really convenient for what we will do today.\n\nNote: Although MNIST visualizations are pretty common on the internet, the images in this post are 100% generated from the code, so you can use these techniques with your own models.\n\nTo understand what kind of features the encoder is capable of extracting from the inputs, we can first look at reconstructed of images. If this sounds familiar, it\u2019s normal, we already did that last time. However, this step is necessary because it sets the baseline for our expectations of the model.\n\nNote: For this post, the bottleneck layer has only 32 units, which is some really, really brutal dimensionality reduction. If it was an image, it wouldn\u2019t even be 6x6 pixels.\n\nWe can see that the autoencoder successfully reconstructs the digits. The reconstruction is blurry because the input is compressed at the bottleneck layer. The reason we need to take a look at validation samples is to be sure we are not overfitting the training set.\n\nThe first thing we want to do when working with a dataset is to visualize the data in a meaningful way. In our case, the image (or pixel) space has 784 dimensions (28*28*1), and we clearly cannot plot that. The challenge is to squeeze all this dimensionality into something we can grasp, in 2D or 3D.\n\nHere comes t-SNE, an algorithm that maps a high dimensional space to a 2D or 3D space, while trying to keep the distance between the points the same. We will use this technique to plot embeddings of our dataset, first directly from the image space, and then from the smaller latent space.\n\nNote: t-SNE is better for visualization than it\u2019s cousins PCA and ICA.\n\nLet\u2019s start by plotting the t-SNE embedding of our dataset (from image space) and see what it looks like.\n\nWe can already see that some numbers are roughly clustered together. That\u2019s because the dataset is really simple*, and we can use simple heuristics on pixels to classify the samples. Look how there\u2019s no cluster for the digits 8, 5, 7 and 3, that\u2019s because they are all made of the same pixels, and only minor changes differentiates them.\n\n*On more complex data, such as RGB images, the only clusters would be of images of the same general color.\n\nWe know that the latent space contains a simpler representation of our images than the pixel space, so we can hope that t-SNE will give us an interesting 2-D projection of the latent space.\n\nAlthough not perfect, the projection shows denser clusters. This shows that in the latent space, the same digits are close to one another. We can see that the digits 8, 7, 5 and 3 are now easier to distinguish, and appear in small clusters.\n\nNow that we know what level of detail the model is capable of extracting, we can probe the structure of the latent space. To do that, we will compare how interpolation looks in the image space, versus latent space.\n\nWe start off by taking two images from the dataset, and linearly interpolate between them. Effectively, this blends the images in a kind of ghostly way.\n\nThe reason for this messy transition is the structure of the pixel space itself. It\u2019s simply not possible to go smoothly from one image to another in the image space. This is the reason why blending the image of an empty glass and the image of an full glass will not give the image of a half-full glass.\n\nNow, let\u2019s do the same in the latent space. We take the same start and end images and feed them to the encoder to obtain their latent space representation. We then interpolate between the two latent vectors, and feed these to the decoder.\n\nThe result is much more convincing. Instead of having a fading overlay of the two digits, we clearly see the shape slowly transform from one to the other. This shows how well the latent space understands the structure of the images.\n\nBonus: here\u2019s a few animations of the interpolation in both spaces\n\nOn richer datasets, and with better model, we can get incredible visuals.\n\nWe can also do arithmetics in the latent space. This means that instead of interpolating, we can add or subtract latent space representations.\n\nFor example with faces, man with glasses - man without glasses + woman without glasses = woman with glasses. This technique gives mind-blowing results.\n\nNote: I\u2019ve put a function for that in the code, but it looks terrible on MNIST.\n\nIn this post, we have seen several techniques to visualize the learned features embedded in the latent space of an autoencoder neural network. These visualizations help understand what the network is learning. From there, we can exploit the latent space for clustering, compression, and many other applications.\n\nYou can play with the code over there:\n\nThanks for reading this post, stay tuned for more !"
    },
    {
        "url": "https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694?source=user_profile---------7----------------",
        "title": "Autoencoders \u2014 Deep Learning bits #1 \u2013",
        "text": "Neural networks exist in all shapes and sizes, and are often characterized by their input and output data type. For instance, image classifiers are built with Convolutional Neural Networks. They take images as inputs, and output a probability distribution of the classes.\n\nAutoencoders (AE) are a family of neural networks for which the input is the same as the output*. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation.\n\n*We\u2019ll see how using altered versions of the input can be even more interesting\n\nA really popular use for autoencoders is to apply them to images. The trick is to replace fully connected layers by convolutional layers. These, along with pooling layers, convert the input from wide and thin (let\u2019s say 100 x 100 px with 3 channels \u2014 RGB) to narrow and thick. This helps the network extract visual features from the images, and therefore obtain a much more accurate latent space representation. The reconstruction process uses upsampling and convolutions.\n\nThe reconstruction of the input image is often blurry and of lower quality. This is a consequence of the compression during which we have lost some information.\n\nConvolutional autoencoders can be useful for reconstruction. They can, for example, learn to remove noise from picture, or reconstruct missing parts.\n\nTo do so, we don\u2019t use the same image as input and output, but rather a noisy version as input and the clean version as output. With this process, the networks learns to fill in the gaps in the image.\n\nLet\u2019s see what a CAE can do to replace part of an image of an eye. Let\u2019s say there\u2019s a crosshair and we want to remove it. We can manually create the dataset, which is extremely convenient.\n\nIn this example, the CAE will learn to map from an image of circles and squares to the same image, but with the circles colored in red, and the squares in blue.\n\nThe CAE does pretty well on colorizing the right parts of the image. It has understood that circles are red and squares are blue. The purple color comes from a blend of blue and red where the networks hesitates between a circle and a square.\n\nThe examples above are just proofs of concept to show what a convolutional autoencoder can do.\n\nMore exciting application include full image colorization, latent space clustering, or generating higher resolution images. The latter is obtained by using the low resolution as input and the high resolution as output.\n\nIn this post, we have seen how we can use autoencoder neural networks to compress, reconstruct and clean data. Obtaining images as output is something really thrilling, and really fun to play with.\n\nNote: there\u2019s a modified version of AEs called Variational Autoencoders, which are used for image generation, but I keep that for later.\n\nYou can play with the code over there:\n\nThanks for reading this post, stay tuned for more !"
    },
    {
        "url": "https://hackernoon.com/talk-to-you-computer-with-you-eyes-and-deep-learning-a-i-odyssey-part-2-7d3405ab8be1?source=user_profile---------8----------------",
        "title": "Use your eyes and Deep Learning to command your computer \u2014 A.I. Odyssey part. 2",
        "text": "The goal of this project is to use our eyes to trigger actions on our computer. This is a very general problem so we need to specify what we want to achieve.\n\nWe could, for instance, detect when the eyes look towards a specific corner and then work from that. That\u2019s however quite limited and not really flexible, plus it would require us to hard-code corner combinations. Instead, we are going to use Recurrent Neural Networks to learn to identify complete eye movements.\n\nWe won\u2019t work with external datasets, we\u2019ll make our own. This has the advantage of using the same source and processing for both training the model and making the predictions.\n\nWithout doubt, the most effective way to extract information from our eyes would be to use a dedicated closeup camera. With such hardware, we could directly track the center of the pupils and do all kinds of fancy stuff.\n\nI didn\u2019t want to use an external camera so I decided to use to the good old 720p webcam from my laptop.\n\nBefore we jump directly to the technical aspects, let\u2019s review the steps of the process. Here\u2019s the pipeline I came up with:\n\nWe\u2019ll go through these steps to see how we can make this work.\n\nLet\u2019s get to it !\n\nStraight from the webcam, we start by downsampling the image and converting it to grayscale (color channels are extremely redundant). This will make next steps much faster, and will help our model run in real time.\n\nFor the detection part, we\u2019ll use HAAR Cascades as they are extremely fast. With some tuning, we can get some pretty good results, but trying to detect the eyes directly leads to many false positives. To get rid of these, we don\u2019t try to find the eyes in the image, but rather the face in the image, and then the eyes in the face.\n\nOnce we have the bounding boxes of both eyes we can extract the images from the initial full-sized webcam snapshot, so that we don\u2019t lose any information.\n\nOnce we have found both eyes, we need to process them for our dataset. To do that we can simply reshape both to a fixed size \u2014 square, 24px \u2014 and use histogram normalization to get rid of the shadows.\n\nWe could then use the normalized pictures directly as input, but we have the opportunity here to do a little more work that helps a lot. Instead of using the eye images, we compute the difference between the eyes in the current and previous frame. This is a very efficient way to encode motion, which is all we need in the end.\n\n**Note that for all diagrams except the GIF below, I will use eye pictures to represent eye differences, because differences look awful on screen.**\n\nNow that we have processed both eyes, we have the choice to treat them separately as two representatives of the same class, or use them together as if they were a single image*. I chose the latter because, even though the eyes are supposed to follow the exact same motion, having both inputs will make the model more robust.\n\n*What we are going to do is a bit more clever than simply stitching the images together, though.\n\nI have recorded 50 samples for two separate motions (one that looks like a \u201cgamma\u201d, the other looks like a \u201cZ\u201d). I have tried to vary the position, scale, and speed of the samples to help the model generalize. I have also added 50 examples of \u201cidle\u201d, which contain roughly generic pattern-free eye motions as well as still frames.\n\nUnfortunately, 150 samples is tiny for such a task, so we need to augment the dataset with new samples.\n\nThe first thing we can do is fix an arbitrary sequence length \u2014 100 frames. From there, we can slow down shorter samples and speed up longer ones. That\u2019s possible because speed does not define the motion.\n\nAlso, because sequences shorter that 100 frames should be detected at any time in the 100 frames window, we can add padded examples.\n\nWith these techniques, we can augment our dataset to be around 1000\u20132000 examples.\n\nLet\u2019s take a step back for a minute, and try to understand our data. We have recorded some samples with corresponding labels. Each of these samples is a series of two 24px wide square images.\n\nNote that we have one dataset for each eye.\n\nNow that we have a dataset, we need to build the right model to learn and generalize from this data. We could write its specifications as follows:\n\nSuch a complicated system requires using a powerful artificial intelligence model \u2014 neural networks. Let\u2019s see how we can build one that meets our need. Neural networks layers are like LEGOs, we simply have to choose the right bricks and put them at the right place.\n\nTo extract information from the images, we are going to need convolutional layers. These are particularly good at processing images to squeeze out visual features. (Psst! We already saw that in part. 1)\n\nWe need to treat each eye separately, and then merge the features through a fully connected layer. The resulting convolutional neural network (CNN) will learn to extract relevant knowledge from pairs of eyes.\n\nNow that we have a simple representation of our images, we need something to process them sequentially. For that, we are going to use a recurrent layer \u2014 namely Long Short Term Memory cells. The LSTM updates its state using both the extracted features at the current time step and its own previous state.\n\nFinally, when we have processed the whole sequence of images, the state of the LSTM is then fed to a softmax classifier to predict the probability of each motion.\n\nBehold our final neural network, which takes as input a sequence of image pairs, and outputs the probability of each motion. What is crucial here is that this we build the model in one single piece, and therefore it can be trained end-to-end via backpropagation."
    },
    {
        "url": "https://hackernoon.com/finding-the-genre-of-a-song-with-deep-learning-da8f59a61194?source=user_profile---------9----------------",
        "title": "Finding the genre of a song with Deep Learning \u2014 A.I. Odyssey part. 1",
        "text": "The average library is estimated to have about 7,160 songs. If it takes 3 seconds to classify a song (either by listening or because you already know), a quick back-of-the-envelope calculation gives around 6 hours to classify them all.\n\nIf you add the time it takes to manually label the song, this can easily go up to 10+ hours of manual work. No one wants to do that.\n\nIn this post, we\u2019ll see how we can use Deep Learning to help us in this labour-intensive task.\n\nHere\u2019s a general overview of what we will do:\n\nFirst of all, we\u2019re going to need a dataset. For that I have started with my own iTunes library \u2014 which is already labelled due to my slightly obsessive passion for order. Although it is not as diverse, complete or even as big as other datasets we could find, it is a good start. Note that I have only used 2,000 songs as it already represents a lot of data.\n\nThe first observation is that there are too many genres and subgenres, or to put it differently, genres with too few examples. This needs to be corrected, either by removing the examples from the dataset, or by assigning them to a broader genre. We don\u2019t really need this Concertos genre, Classical will do the trick.\n\nOnce we have a decent number of genres, with enough songs each, we can start to extract the important information from the data. A song is nothing but a very, very long series of values. The classic sampling frequency is 44100Hz \u2014 there are 44100 values stored for every second of audio, and twice as much for stereo.\n\nThis means that a 3 minute long stereo song contains 7,938,000 samples. That\u2019s a lot of information, and we need to reduce this to a more manageable level if we want to do anything with it. We can start by discarding the stereo channel as it contains highly redundant information.\n\nWe will use Fourier\u2019s Transform to convert our audio data to the frequency domain. This allows for a much more simple and compact representation of the data, which we will export as a spectrogram. This process will give us a PNG file containing the evolution of all the frequencies of our song through time.\n\nThe 44100Hz sampling rate we talked about earlier allows us to reconstruct frequencies up to 22050Hz \u2014 see Nyquist-Shannon sampling theorem \u2014 but now that the frequencies are extracted, we can use a much lower resolution. Here, we\u2019ll use 50 pixel per second (20ms per pixel), which is more than enough to be sure to use all the information we need.\n\nNB: If you know a genre characterized by ~20ms frequency variations, you got me.\n\nHere\u2019s what our song looks like after the process (12.8s sample shown here).\n\nTime is on the x axis, and frequency on the y axis. The highest frequencies are at the top and the lowest at the bottom. The scaled amplitude of the frequency is shown in greyscale, with white being the maximum and black the minimum.\n\nI have used use a spectrogram with 128 frequency levels, because it contains all the relevant information of the song \u2014 we can easily distinguish different notes/frequencies.\n\nThe next thing we have to do is to deal with the length of the songs. There are two approaches for this problem. The first one would be to use a recurrent neural network with wich we would feed each column of the image in order. Instead, I have chosen to exploit even further the fact that humans are able to classify songs with short extracts.\n\nWe can create fixed length slices of the spectrogram, and consider them as independent samples representing the genre. We can use square slices for convenience, which means that we will cut down the spectrogram into 128x128 pixel slices. This represents 2.56s worth of data in each slice.\n\nAt this point, we could use data augmentation to expand the dataset even more (we won\u2019t here because we aready have a lot of data). We could for instance add random noise to the images, or slightly stretch them horizontally and then crop them.\n\nHowever, we have to make sure that we do not break the patterns of the data. We can\u2019t rotate the images, nor flip them horizontally because sounds are not symmetrical.\n\nE.g, see those white fading lines? These are decaying sounds which cannot be reversed.\n\nAfter we have sliced all our songs into square spectral images, we have a dataset containing tens of thousands of samples for each genre. We can now train a Deep Convolutional Neural Network to classify these samples. For this purpose, I have used Tensorflow\u2019s wrapper TFLearn.\n\nWith 2,000 songs split between 6 genres \u2014 Hardcore, Dubstep, Electro, Classical, Soundtrack and Rap, and using more than 12,000 128x128 spectrogram slices in total, the model reached 90% accuracy on the validation set. This is pretty good, especially considering that we are processing the songs tiny bits at a time. Note that this is not the final accuracy we\u2019ll have on classifying whole songs (it will be even better). We\u2019re only talking slices here.\n\nSo far, we have converted our songs from stereo to mono and created a spectrogram, which we sliced into small bits. We then used these slices to train a deep neural network. We can now use the model to classify a new song that we have never seen.\n\nWe start off by generating the spectrogram the same way we did with the training data. Because of the slicing, we cannot predict the class of the song in one go. We have to slice the new song, and then put together the predicted classes for all the slices.\n\nTo do that, we will use a voting system. Each sample of the track will \u201cvote\u201d for a genre, and we choose the genre with the most votes. This will increase our accuracy as we\u2019ll get rid of many classifications errors with this ensemble learning-esque method.\n\nWith this pipeline, we can now classify the unlabelled songs from our library. We could simply run the voting system on all the songs for which we need a genre, and take the word of the classifier. This would give good results but we might want to improve our voting system.\n\nThe last layer of the classifier we have built is a softmax layer. This means that it doesn\u2019t really output the detected genre, but rather the probabilities of each. This is what we call the classification confidence.\n\nWe can use this to improve our voting system. For instance, we could reject votes from slices with low confidence. If there is no clear winner, we reject the vote.\n\nSimilarly, we could leave unlabelled the songs for which no genre received more than a certain fraction -70%?- of the votes. This way, we will avoid mislabeling songs, which we can still label later by hand.\n\nIn this post, we have seen how we could extract important information from redundant and high dimensional data structure \u2014 tracks. We have taken advantage of short patterns in the data which allowed us to classify 2.56 second long extracts. Finally, we have used our model to fill in the blanks in a digital library.\n\nYou can play with the code here:\n\nIf you want to go further on audio classification, there are other approaches which yield impressive results, such as Shazam\u2019s fingerprinting technique or dilated convolutions.\n\nThanks for reading this post, stay tuned for more !"
    }
]