[
    {
        "url": "https://towardsdatascience.com/the-data-science-thought-process-df386ee7930a?source=user_profile---------1----------------",
        "title": "The Data Science Thought Process \u2013",
        "text": "In my last post on preparing your data science resume and project portfolio, I was discussing about showcasing the thought process that one has, because besides the maths & statistics background, that to me is very important in assessing if one is suitable to be trained further or accepted into a data scientist role.\n\nIn a certain angle, data scientist most valuable skill is to use data to provide business value. It could be providing relevant insights to solve business challenges or reach business objectives or using machine learning models for better business decision making etc. To sum it all up, data scientist is a solution provider using organization\u2019s data as raw materials. And to be an effective solution provider, the thought process, seeking and bringing different resources in is very important, in my opinion (of course there is the execution part but that might be another discussion).\n\nSo I thought I will provide some tips on strengthening that thought process so that aspiring and current data scientist can provide more value to their employers and lead to better rewards (hopefully).\n\nI am an avid reader (or at least I think I am). Technical papers, especially those found in arxiv, are definitely part of the reading diet but I also read books on varied subjects such as sociology, psychology, economics, leaderships, biographies etc. I find that reading widely and drawing relations between the different knowledge and fields, helps to strengthen the thought process.\n\nThrough reading widely, some of the knowledge gained can be used in projects, such as understand certain nuances found in the data during EDA. It also helps to create certain hypothesis (which needs to be tested further), that can help in structuring a workable business strategy.\n\nReading widely, helps to spark off new ideas (INNOVATION!) on solving business challenges as well, putting ideas from different books together to create new synergies!\n\nThe main idea is to gain and relate the knowledge through reading and bring them into the thought process.\n\nI like to read up on Bloomberg Business Week, The Economist or similar periodicals. These are very good places to understand what is going on in the different industries, such as pharmaceutical, telcos, technology, banking and for countries, understanding the political and economics climate. It can be a story on how the major trends are exerting changes in the pharmaceutical industry for instances changes in the patent duration or how FRS 39/Basel III is affecting the banking industry.\n\nAll these help the data scientist, especially if they are working in the related industry, to be prepared in how they should work on the data and machine learning models, taking into account the major trends affecting the industry.\n\nHow does networking help in strengthening the thought process? Well, it is to gather from many people, how they solved their employer\u2019s business challenges. Understand what were the technical, data and organization-specific challenges when tackling the project and also if possible, understand how these challenges are tackled.\n\nAlso try to understand how they conduct their Exploratory Data Analysis and incorporate their best practices into your own.\n\nVery seldom, one can work on a project without any hiccups. So it is best to \u201clearn\u201d from other people\u2019s experience, be prepared for it or even take precautions.\n\nAgain the main idea here is to learn and have a broader view of how other people tackle their challenges and because you never know when you might come across something similar and you can adapt the solution to your own projects.\n\nThe broad idea is to learn from as many people, blogs (Medium, of course!), periodicals, books and websites on how to tackle different challenges. Working on a data science project involves many areas, components and teams thus challenges can come from anywhere, it is most important to be prepared for it so that we can solve them as they come and be able to continuously provide value through the organization\u2019s data.\n\nI wish all readers all the best in the data science journey! Keep learning!\n\nDo visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://towardsdatascience.com/preparing-your-data-science-resume-portfolio-22af6bada8b9?source=user_profile---------2----------------",
        "title": "Preparing Your Data Science Resume & Portfolio \u2013",
        "text": "In the past few years, I have met up with a lot of employers and conducted interviews for training program. Through the conversations and interviews and seeing the end results, I thought I will share more on how to prepare for your resume and even the interviews for a data science role. Most of the tips are for people who want to enter into the data science profession with a \u201cgreen\u201d background. I cannot promise results but I hope it can help those who are passionate about data science. The tips given are really for those who are passionate in it as it requires a lot of effort.\n\nThe title explains itself but let me take the opportunity to explain it further. Often I was asked \u201cCan I use my school project as part of the project portfolio?\u201d Now here are some considerations I have if one were to showcase their school projects.\n\nFirstly, most of these school projects are guided (hopefully) and work in teams. Its very challenging to differentiate which part of the project is done by who. What I can only infer is that, based on the results of the project, whether the team is functional or dysfunctional.\n\nThe best project to showcase is done outside of the curriculum, during one\u2019s free time because it shows that the person is passionate in data science and willing to spend their free time on it. I can also attribute whatever that is done in the project to the interviewee. But that is after I have asked a few more questions on the project to ascertain it.\n\nDo have keywords (such as the machine learning models used, the model training process etc) used to explain the project but be prepared to explain those keywords especially when it is a technical interview. I tend to ask interviewees to explain those keywords and the explanation has to be at a level that the layman can understand. Well, if one cannot explain it to the layman, it means one still does not understand it completely right?\n\nIf one is to work in the data science or even AI, being able to work in a team matters tremendously, regardless of being in a leadership or team player role. So it is important to showcase any team projects and also the impact achieved, preferably quantify the impact so the interviewer can build a good mental impression. The impact will give me some information on whether the interviewee can work in a team or not. I also tend to ask the interviewee to share more about their experience in at least one project, so as to ascertain if he/she can work in a team.\n\nI like a good grasp on the level of mathematics and statistics that the interviewee has. It can be inferred from the module grades, projects and tools that they used.\n\nModule grades does help to ascertain the level. I usually look at the whole portfolio of mathematical modules that were taken as a whole to ascertain the level of maths and stats background. I do give chance for those that have mediocre grades but I will definitely ask the interviewee why the mediocrity. I do ask the interviewee about what they like and do not like about maths and stats to determine if he/she can work with the mathematics required in data science and AI.\n\nThe projects and tools does help to infer the maths background. This is seen through the machine learning models they have used, how they implement it and why they implement it in a particular manner. I may ask what were the challenges they faced during these projects, the reason a particular solution was chosen and as much as possible relate to the mathematics behind.\n\nWhere possible, do showcase any codes written, especially if the codes are written for data science projects. Otherwise, other languages are welcomed, not necessary must be those common languages used in data science (R, Python, Scala).\n\nMake sure it is well-documented. Well-documented meaning there is a good description on what the code is doing, why the codes need to be written in such a way, why the code is implemented etc. The main objective is for the interviewer to understand the thought process interviewee have gone through in writing codes and deriving insights from the project. Documentation is very important in order for data science to be reproducible, interpretive and accountable. Showcasing the thought process is a very important consideration for the interviewer to determine how much autonomy can be given to the new hire to get meaningful results from the project.\n\nThus a well-documented code is very important, as it can indicate the level of knowledge, skills and thinking of the potential hire.\n\nThe above points are what I have gathered interviewing for training program and talking to numerous employers. I hope the points shared will help you to construct a more \u201cattractive\u201d portfolio and resume to your potential employers and also be well-prepared for possible interview questions to come.\n\nI would also like to recommend the following article, \u201cHow to Construct a Data Science Portfolio from Scratch\u201d.\n\nHave fun in your data science learning journey and do visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://medium.com/@koolanalytics/gradient-descent-simply-explained-1d2baa65c757?source=user_profile---------3----------------",
        "title": "Gradient Descent: Simply Explained? \u2013 Koo Ping Shung \u2013",
        "text": "I am often asked these two questions and that is \u201cCan you please explain gradient descent?\u201d and \u201cHow does gradient descent figure in Machine Learning?\u201d.\n\nThrough the years of training I have given, I thought I will share in this blog post what is gradient descent and why it is the \u201cessential ingredient\u201d when it comes to Machine Learning.\n\nNow I am making a very important assumption here and that is the reader understand Calculus, especially differentiation up to an order of 2.\n\nFor those learning or has learnt about Machine Learning, you know that each Machine Learning model has a cost function. To explain it very briefly, it is a way to determine how well the machine learning model has performed given the different values of each parameters.\n\nFor example, the linear regression model, the parameters will be the two coefficients, Beta 1 and Beta 2.\n\nThe cost function will be the sum of least square methods.\n\nSince the cost function is a function of the parameters Beta 1 and Beta 2, we can plot out the cost function with each value of Beta. (i.e. Given the value of each coefficient, we can refer to the cost function to know how well the machine learning model has performed. )\n\nGiven that there are many parameters, you can imagine that once the cost function is determined, the contours of a multi-dimension plane is laid out. (similar to a mountainous region in a 3 dimensions).\n\nNOTE: This is where there is a lot of confusion. Basically, I have seen many people got confused by the horizontal axis. They thought that the horizontal axis is actually Xs, or the independent variables which is not true because the Xs will remain the same throughout the training phase. Always remember during the training phase, we are focused on selecting the \u2018best\u2019 value for the parameters (i.e. the coefficients).\n\nWhen we are training the model, we are trying to the find the values of the coefficients (the Betas, for the case of linear regression) that will give us the lowest cost. In other words, for the case of linear regression, we are finding the value of the coefficients that will reduce the cost to the minimum a.k.a the lowest point in the mountainous region.\n\nLet us look into the training phase of the model now.\n\nSo now imagine we put an agent into this multi-dimension plane (remember the mountainous region), the starting position is randomly given (i.e. randomly assigned a value for each coefficient). This randomly assigned starting position is known as \u201cInitialization\u201d in the machine learning world, and it is a whole research area altogether.\n\nThis agent can only see one thing and that is the gradient at the point it is standing, a.k.a rate of change of cost given a unit change in coefficient. This intuition of the gradient is gotten from the first order differentiation in Calculus. That explains the \u201cGradient\u201d of the Gradient Descent.\n\nIf you studied any materials on gradient descent, you will come across another technical term known as the Learning Rate. The learning rate actually refers to how large a step the agent takes when traveling in the \u201cmountainous region\u201d, meaning how large a change in the parameters we are taking. So if the gradient is steep at the standing point and you take a large step, you will see a large decrease in the cost.\n\nAlternatively, if the gradient is small (gradient is close to zero), then even if a large step is taken, given that the gradient is small, the change in the cost will be small as well.\n\nPutting It All Together\n\nThus in gradient descent, at each point the agent is in, the agent only knows the GRADIENT (for each parameter) and the width of the STEP to take. With the gradient and the step taken into account, the current value of each parameters will be updated. With the new values of the parameters, the gradients are re-calculated again and together with the step, the new value of the parameters is calculated. This keeps on repeating until we get to convergence (which we will discuss in a while). Given many repeated steps, the agent will slowly DESCENT to the lowest point in the mountainous region.\n\nNow you may ask why the agent will move to the lowest point and not do an ascent. That I will leave it to the readers to find out more but let me provide some direction for research. It has something to do with the fact that cost functions are convex function and how the value of the parameters are updated.\n\nOnce the agent, after many steps, realize the cost does not improve by a lot and it is stuck very near a particular point (minima), technically this is known as convergence. The value of the parameters at that very last step is known as the \u2018best\u2019 set of parameters (in the case of the linear regression model, we have the \u2018best\u2019 value for both Betas). And we have a trained model.\n\nIn conclusion, gradient descent is a way for us to calculate the best set of values for the parameters of concern.\n\nThe steps are as follows:\n\n1 \u2014 Given the gradient, calculate the change in parameter with respect to the size of step taken.\n\n2 \u2014 With the new value of parameter, calculate the new gradient.\n\nAnd the sequence of steps will stop once we hit convergence. I hope this helps with your understanding of gradient descent.\n\nSo let me welcome you to the world of gradient descent and if you are adventurous enough (which I hope you do after the blog post), do check out this blog post by Sebastian Ruder that talks about other variants of gradient descent algorithms.\n\nI hope this blog post has been useful. Have fun in your Data Science learning journey and do visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9?source=user_profile---------4----------------",
        "title": "Accuracy, Precision, Recall or F1? \u2013",
        "text": "Often when I talk to organizations that are looking to implement data science into their processes, they often ask the question, \u201cHow do I get the most accurate model?\u201d. And I asked further, \u201cWhat business challenge are you trying to solve using the model?\u201d and I will get the puzzling look because the question that I posed does not really answer their question. I will then need to explain why I asked the question before we start exploring if Accuracy is the be-all and end-all model metric that we shall choose our \u201cbest\u201d model from.\n\nSo I thought I will explain in this blog post that Accuracy need not necessary be the one-and-only model metrics data scientists chase and include simple explanation of other metrics as well.\n\nFirstly, let us look at the following confusion matrix. What is the accuracy for the model?\n\nVery easily, you will notice that the accuracy for this model is very very high, at 99.9%!! Wow! You have hit the jackpot and holy grail (*scream and run around the room, pumping the fist in the air several times*)!\n\nBut\u2026.(well you know this is coming right?) what if I mentioned that the positive over here is actually someone who is sick and carrying a virus that can spread very quickly? Or the positive here represent a fraud case? Or the positive here represents terrorist that the model says its a non-terrorist? Well you get the idea. The costs of having a mis-classified actual positive (or false negative) is very high here in these three circumstances that I posed.\n\nOK, so now you realized that accuracy is not the be-all and end-all model metric to use when selecting the best model\u2026now what?\n\nLet me introduce two new metrics (if you have not heard about it and if you do, perhaps just humor me a bit and continue reading? :D )\n\nSo if you look at Wikipedia, you will see that the the formula for calculating Precision and Recall is as follows:\n\nLet me put it here for further explanation.\n\nLet me put in the confusion matrix and its parts here.\n\nGreat! Now let us look at Precision first.\n\nWhat do you notice for the denominator? The denominator is actually the Total Predicted Positive! So the formula becomes\n\nImmediately, you can see that Precision talks about how precise/accurate your model is out of those predicted positive, how many of them are actual positive.\n\nPrecision is a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.\n\nSo let us apply the same logic for Recall. Recall how Recall is calculated.\n\nThere you go! So Recall actually calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). Applying the same understanding, we know that Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.\n\nFor instance, in fraud detection or sick patient detection. If a fraudulent transaction (Actual Positive) is predicted as non-fraudulent (Predicted Negative), the consequence can be very bad for the bank.\n\nSimilarly, in sick patient detection. If a sick patient (Actual Positive) goes through the test and predicted as not sick (Predicted Negative). The cost associated with False Negative will be extremely high if the sickness is contagious.\n\nNow if you read a lot of other literature on Precision and Recall, you cannot avoid the other measure, F1 which is a function of Precision and Recall. Looking at Wikipedia, the formula is as follows:\n\nF1 Score is needed when you want to seek a balance between Precision and Recall. Right\u2026so what is the difference between F1 Score and Accuracy then? We have previously seen that accuracy can be largely contributed by a large number of True Negatives which in most business circumstances, we do not focus on much whereas False Negative and False Positive usually has business costs (tangible & intangible) thus F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives).\n\nI hope the explanation will help those starting out on Data Science and working on Classification problems, that Accuracy will not always be the metric to select the best model from.\n\nI wish all readers a FUN Data Science learning journey and do visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://towardsdatascience.com/data-scientist-from-good-to-great-5e6aed447b69?source=user_profile---------5----------------",
        "title": "Data Scientist: From Good to Great \u2013",
        "text": "Many of you who are starting out on Data Science and looking for materials on what to study, probably have come across Drew Conway\u2019s Venn Diagram on the meaning of Data Science. I have a different version that is adapted from him and have written it in my post here.\n\nAfter much thought and looking at the post from Andrew Ng with regards to setting up his Data Science team in Baidu Research (when he was still with them), I asked myself the question how can a GOOD data scientist move to being a GREAT data scientist.\n\nOne thing that came to my mind is that for anyone working in Data Science, they cannot run away from being part of a team. They have to learn how to be a good leader and a good follower as well, in order to play a good supporting role, especially when the data scientist has to communicate to both data engineers and business users. Everyone plays a part in making the team effective and efficient, the leader cannot be effective without the cooperation from the team members and team members cannot be effective without the leader to give the direction, stay focus, managing the timelines and motivating the team members.\n\nBesides that, to be a great data scientist, in my opinion, communication skills is very important. Being able to communicate the relevant insights and in a manner that is digestible by management requires much thought to be put in. For instance, how should the presentation be structured so that the insights are easily understood.\n\nThe great data scientist would need to learn how people learn, what kind of communication medium is effective in bringing across the messages/insights, so that they are easily understood and can be used to make better decisions.\n\nLooking at the above, being a good leader, a good follower and a good communicator, they all need a common \u201cingredient\u201d and that is having empathy, being able to put oneself in other people shoes and think from their perspectives.\n\nHaving empathy allows one to understand which behavior are likely to be chosen, which perception are likely to be taken if the \u2018story\u2019 is presented in a certain way. This allows the data scientist to anticipate the possible outcomes and be prepared for it and also come up with successful presentation.\n\nFor instance, if a data scientist have to prepare an ad-hoc analysis, he/she should be able to anticipate the questions that the audience is likely to ask, and prepare the figures accordingly so that they are handy when the questions is asked. Being able to answer these questions from the audience, can increase the credibility of the data scientist.\n\nSo to be a good data scientist, I would recommend the person to be trained and have knowledge in the following:\n\nAnd to move on from good to great data scientist, the fourth skill that is needed would be\n\nTo conclude, the data scientist should not only have the \u2018hard\u2019 skills, knowledge and skills that can be picked up through books and other mediums but also need to have the \u2018soft\u2019 skills that can only be picked up through experience and practice.\n\nGood news is both of them can be picked up in parallel. So go forth and pick it up! Start working on team projects and grab more opportunities to do presentation!\n\nI hope the blog has been useful! Have fun in your data science learning journey and do visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://towardsdatascience.com/building-data-science-capabilities-in-organizations-354705c1f868?source=user_profile---------6----------------",
        "title": "Building Data Science Capabilities in Organizations",
        "text": "During personal discussions or networking events, I am often asked by business owners, department/function heads the question \u201cI believed in the power of data science, but how shall I start?\u201d. So this blog post is my attempt to answer the question.\n\nWhat to Keep in Mind\n\nIts very important to always remember, while building data science capabilities and working through the roadmap, that \u201cValue have to always move ahead of the costs\u201d. I have seen efforts that was not able to create a sustainable momentum in building data science capabilities because the costs (mostly infrastructure) ran way ahead of the value. With costs running way ahead, many will be pressured to show results and without proper planning on the projects to work on, the effort was not sustained.\n\nYes to start, please hire an experienced(!!!) data scientist. Why an experienced data scientist you might say and not those who has title of a \u201cdata scientist\u201d? The experienced data scientist should have the expertise to understand the data quickly and determine if there are any \u201clow hanging fruits\u201d that can be plucked with tools that the organization has accessed to easily, like Excel or open source. These \u201clow hanging fruits\u201d, together with the tools is to provide immediate (well about 1\u20132 months wait, depending on the data quality) value to the organization. These projects are to be used to get buy-in from other parts of the organization.\n\nSometimes hiring a data scientist maybe a high risk maneuver given that it is a permanent position (data scientists are in high demand so please do not even consider trying to hire on a contractual basis). An alternative will be to hire a consultant who has done data science projects. The consultant can sieve through the available data and determine if there are sufficient \u201clow hanging fruits\u201d.\n\nI have seen organizations that hire people who has completed a Masters or bootcamp and expect them to know how to work on their existing data. Most of these \u201cfresh\u201d trainees requires mentors to guide them further so that they know how to sieve through the data for insights. Experience really counts a lot in data science!\n\nGiven that sufficient value has been proven from existing data, the next step is to work on TWO paths: (1) data governance & management (2)infrastructure\n\nHaving proved that data is of value, it is time to set up processes to manage it, ensure that the data is of higher quality, so as to reduce the time period between extracting data to having data at the right quality to be used. This will allow data to be turned into insights for decisions quickly, pushing the value envelope further.\n\nBased on the first few projects (aka \u201clow hanging fruits), the organization can also now look at what further data can be captured (at a reasonable costs) so as to improve their insights.\n\nHaving created more buy-in from management, the organization can now work on the infrastructure. Building the infrastructure generally requires a much larger budget because of the need to integrate with existing systems and also storing of data. But since we have the \u201clow hanging fruits\u201d to show for, it will now be easier to ask for a budget to build and management will have more confidence that the budget will be used to create more value for the organization.\n\nI\u2019ve seen in a lot of situations, organization went ahead to purchase \u201cBig Data\u201d technology without proper plans on how to use them or even worst, whether there is a need to use them. In the end, the momentum to build data science capabilities was not sustained, because of various reasons value created (if they are created in the first place) was not enough to cover the infrastructure costs and these organizations are stuck with the \u2018white elephant\u2019. And the conclusion from such failed attempt was management do not believe in data science anymore (who can blame them) which to me is very sad, because the organization has lost the chance to be competitive.\n\nSo remember what I said, \u201cValue have to always move ahead of the costs.\u201d\n\nWith Better Infrastructure & Data, Comes Greater Value\n\nSetting up the infrastructure and data governance processes might take some time, like 6 to 12 months. During this time, the organization should continue to find more data science projects to create value for the organization. With better infrastructure and data quality, the value/time spent ratio will be increased. This increase will then lead to another chance to put in more resources to build better infrastructure, larger teams and collect more data.\n\nWith the value running way ahead of the costs, and ensuring that it stays that way, it will create a virtuous cycle and in due time, the data science capabilities will be built up and stay with the organization.\n\nThis is of course just a very simple description on how to build data science capabilities. There will be other considerations as well given the different domain and such. But at the end of the day, the most important message that I want to bring across is that \u201cValue have to always move ahead of the costs\u201d. otherwise the effort is not sustainable and organizations may just lose the competitive edge that is necessary to survive in this dynamic and harsh environment.\n\nI hope the blog has been useful! Have fun in your data science learning journey and do visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://towardsdatascience.com/data-science-in-start-ups-c3cb13286dc4?source=user_profile---------7----------------",
        "title": "Data Science in Start-ups? \u2013",
        "text": "Through my discussion with many people on data science and artificial intelligence, I often hear people saying, \u201cStart-ups do not need data science. Let\u2019s focus on capturing users by building the features that our users wants.\u201d, or something to that effect. Data science seldom made it to the list of priorities for most founders when they are working on their start-ups.\n\nMost of the discussion revolve around the following reasons for not adopting data science; Data science is portrayed as expensive (mega-infrastructure!), takes up too much time, its very challenging (need an expertise to work on and expertise are very rare) and data science can only work when there are HUGE amounts of data.\n\nI hold a different opinion to that. My opinion is yes, start-ups do not need the \u201csophisticated\u201d machine learning initially but it is at the right time to consider and prepare for data science capabilities in the organization.\n\nStart-ups usually work with a \u201ccleaner sheet of paper\u201d (compared to large enterprises) thus it is at the right opportunity to discuss what are the data to collect, the quality of data to collect, which stage of the business process should the data be collected etc. By having such discussions early on, data collection can be worked into the business processes easily before the different business processes extend and get more complex in the organization, becoming a \u201cbig bowl of spaghetti\u201d. It is easier to fix the car while it is at a slow speed compared to fixing a giant car(big enterprises) moving at a fast speed.\n\nFor example, most start-ups are interested to ramp up popular features quickly they need a more data-driven approach to determine popularity (i.e. conducting A/B testing). Resources such as data (which part in the business process should the data collection be done, granularity of the data to be collected) and infrastructure (which database should we use) can be discussed up front to allow start-ups to do quick analysis of the data captured, either decide if a feature is popular or decisively move on to other worthy pursuits when the analysis showed otherwise.\n\nSecondly, it takes time to collect data. Good quality data do not magically appear. It requires planning, from data collection, data quality to data storage and retrieval. Collecting data at the right quality level can cut down a lot of data preparation work that is required before any analysis. Time is an essential ingredient to collect enough data.\n\nWith data collected very early on, start-ups can learn about the impact of their strategy and conserve on resources (resources are precious in start-ups right?) if the impact is not going to be positive or great.\n\nThirdly, by starting your data collection early on, the start-up would be storing one of the critical resources that is needed to build artificial intelligence capabilities, if the start-up move through several rounds of funding. Though this might change if we see further development in AlphaGo Zero.\n\nThis misconception is likely to be brought about by the \u201cBig Data\u201d term that was used extensively to create urgency among companies to adopt data science.\n\nIf start-ups are to tap onto their data for value immediately, the first thing to do is to setup the reporting process or perhaps establishing an operation and strategy dashboard. Decide on the concerned metrics, based on current business strategy, for each of the dashboard.\n\nFor operation dashboard, the start-up can have the metrics refreshed on a more regular basis as compared to the strategic dashboard. The key here is to have everyone in the start-up understand currently, how are the operations doing; are we at the stipulated service level for our customers, is there a drop in user experience in critical areas etc. As such, the start-up can move the limited resources to the right area to sustain operations at the right service level.\n\nFor strategic dashboard, it is more for the start-up to understand if their current business model is working, if the business strategy (like capturing users, extending usage of existing users etc) is working or not.\n\nThese two dashboards do not need huge amounts of data since the data captured can be processed immediately for insights (for higher frequency of refresh). It can help start-ups to manage their operations and strategy quickly and effectively, ensuring limited resources are used in areas that has the largest positive impact.\n\nData science need not be expensive. A start-up should not commit a lot funds into tools without having a good long-term usage plan. My suggestion is plan out the data science use cases that the start-up wants to work on and research on the tools that are available, then see if it makes sense to go for open source or enterprise tools. Only commit to purchase tools when it makes absolute business sense, when the value produced by these tools exceeds the costs of tools. I strongly believe that infrastructure should grow together with the value produced by usage of data science in the start-up. Immediate purchase of enterprise tools without a good plan for it is likely to result in a huge waste of resources that are scarce in the start-up environment.\n\nAs previously mentioned, the types of analysis or machine learning done at the initial stages of start-ups need not be complicated, so start-ups could perhaps offer the opportunities to carry out the analysis or machine learning to interns, giving them relevant experience that can greatly benefit their career in the long run. This creates a win-win situation in that the start-up gets workable use cases and understand the value of data science at a low cost, which may include a nice surprise discovery of data science talents along the way. The interns gets to practice what they have learned in their undergrad studies and see the strengths and weaknesses of their current set of skills. Perhaps to ensure, that the win-win situation creates the largest impact, having a mentor to guide the intern(s) will be beneficial. More importantly is the mentor needs to have practical experience and have worked on data science projects before.\n\nMost of the data scientists I have met, are always on the lookout for interesting challenges to work on, assuming they are adequately paid. In other words, what attracts data scientists is never salary alone but also the kind of challenges provided. So if the start-up can provide good challenges and an environment that is supportive of it, they can attract their fair share of data scientists.\n\nVCs and angel investors may want to hire a data scientist (permanent role or consultative basis) to work on the data science projects provided or identified by the VCs\u2019 and investor\u2019s portfolio of start-ups.\n\nStart-up should start thinking about building data science capabilities as early as possible. The greatest benefits to start early is the amount of data collected since they do not appear with a snap of the finger. Planning early allows the start-up to collect good quality data, iterate quickly and move up the data science learning curve much earlier than their competition.\n\nInfrastructure should grow together with the amount of value derived from the use cases. Or more importantly, the costs to implement use cases moves in tandem with business value. This would create a sustainable momentum of adopting data science in start-ups.\n\nStart-up do not need huge amounts of data at the start. They can start gaining insights from whatever data that they have captured and use these insights to conserve resources and focus on more critical areas.\n\nI hope the blog has been useful! Have fun in your data science learning journey and do visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://towardsdatascience.com/moving-into-data-science-as-a-career-domain-expertise-3e36cafad1e7?source=user_profile---------8----------------",
        "title": "Moving into Data Science as a Career (Domain Expertise)",
        "text": "Following the few posts that discuss what are the knowledge that someone who is interested to pursue a data science career(overall, mathematics, data & IT management), I shall now discuss more on what are the business knowledge (domain knowledge) that the data scientist should have some knowledge of.\n\nThese days, every business or organization have processes. It is critical that these processes are always kept to be efficient and effective, especially those that are on the customer-facing side.\n\nIt is important that the data scientist understand these processes in general and how they work. Why is this important? Reason is data collection and model implementation are usually added into the business process as a company mature in data science. The data scientist needs to have a good understanding of how these business process works so as to be able to recommend when a certain data element can be captured for better quality and secondly be able to recommend where in the business process, the model should be placed so that the data elements that the model needs is provided to the model and the model is generating \u201cdecisions\u201d at the right stage of the process.\n\nFor instance, most of the credit scorecards requires credit bureau data. Thus in the credit applications process, the models have to be placed at a stage where applicant\u2019s data that are needed by the model, especially the applicant\u2019s credit bureau data, are available for the model. The model should also be generating the credit score before the decision stage since it is a critical decision factor (i.e. in deciding whether to grant credit).\n\nSince this involves the implementation of the models, it is critical that the data scientist have a good understanding of business process so as to make credible recommendations for data collection and model implementation.\n\nOften times, the insights from the data scientist needs to be turned into business strategies. For instance, insights from a marketing campaign response model can be used to determine which customer characteristics are likely to respond to a marketing campaign and from there devise a reasonable campaign that can reach out to these groups of customers.\n\nThus being able to provide \u201cactionable\u201d insights is critical and essential for a data scientist that wants to provide value to their organization. Having a good understanding of strategic management, helps the data scientist to understand what kinds of insights will be highly valued by the company, what kind of insights is actionable (perhaps given the resources available in the organization), be able to think of the next steps after presenting the actionable insights. Being able to think strategically helps the data scientist to continuously provide value through the provision of insights that can be acted upon. Being able to continuously provide actionable and valuable insight help to build up credibility since one would have a higher tendency to listen to a data scientist that gives useful insights rather than insights that cannot be acted upon (i.e. hot air and feasible actions).\n\nBusiness model is how a organization is serving a chosen market, where the competitive advantage is over other similar competitors and the revenue model would state down how the organization continue to extract value/profits from the business model.\n\nFor the data scientist to add value to their organization, it is essential that they know what is the business and revenue model of their organization, both present and future. Having some understanding, allows the data scientist to prioritize which business objectives is important and be able to provide insights that support important business objectives. This ties back with being able to provide relevant insights for strategy formulation and execution so that the organization can continue to operate, serve and extract profit from the chosen market.\n\nWith an understanding of strategic management, business model and revenue model, the data scientist can understand the amount of value each project provides thus be able to provide the relevant insights that can be acted upon and because the insights are adapted from business and revenue model, it allows the company to continue extracting value from their data, creating a sustainable momentum in pushing for more data science or analytics in organizations.\n\nA lot of people who is starting out on data science do not realize that data scientist are change agents as well, because of the insights that we provide, changes are necessary and let\u2019s face it, humans do not like change but change is necessary if the business is to survive in a dynamic environment, more dynamic than a few decades ago.\n\nData scientist being change agents, need to understand how to create sustainable change (i.e. does not revert back to old habits) through the process of providing insights. Data scientist cannot just create tremendous amount of information/insights and then just dump it into the organization. Sometimes there needs to be a measured approach to releasing the insights and information so that changes can be made and be effective.\n\nFor those that are interested in change management, I find the process designed by John Kotter as one of the best out there. You can read the Wikipedia\u2019s article on Change Management here.\n\nSubsequently, the business knowledge that the data scientist would need to have would be related to the domain that the project/analysis is in. For instance, if the data scientist is working in a risk management department, it will need to understand the specific business definitions, regulations (especially banking, healthcare, pharmaceutical, aviation), accounting policies & international standards (GAAP or IFRS), process etc. This is the part that is more specific to the organization the data scientist is deployed in.\n\nOne thing that I noticed in the hiring practices is the huge preference for employees with domain-relevant knowledge. This may severely limit the supply of data science talents the organization have accessed to. Looking at the landscape and labor force, employers would have a better chance of getting more value form data science by looking for those that are mathematically strong, being able to convert business objectives to mathematical models. Based on my observation, this is a much more difficult skill to find or train, as compared to programming and domain knowledge.\n\nWith this I conclude, what in my opinion and observations, are the key skills and knowledge that newcomers to data science should know, learn and understand.\n\nAs technology changes, the data scientist job will evolve and the knowledge and skills might have to be updated accordingly, so keep learning!!\n\nDo visit my other blog posts and LinkedIn profile for more data science nuggets."
    },
    {
        "url": "https://towardsdatascience.com/moving-into-data-science-as-a-career-data-it-management-8de5360cb06c?source=user_profile---------9----------------",
        "title": "Moving into Data Science as a Career (Data & IT Management)",
        "text": "Previously, I have given an overall view of the skills and knowledge needed by a data scientist and also did a discussion on the mathematics that the data scientist should know. Continuing, I shall now touch on the Data and IT management knowledge that a data scientist should know.\n\nAs data is the lifeblood of data science, they need to be carefully managed like all other strategic assets in a company. Thus it is important that the data scientist have knowledge in data governance and management, so as to ensure that data is of the highest quality possible. This is equivalent to cooking where the chef would as much as possible keep their ingredients fresh so as to be able to cook the tastiest dishes possible.\n\nSo what is data governance? As defined in Wikipedia, it is a set of processes that formally manages data so that data can be trusted and together with accountability, adverse data events, such as missing data, poor data quality, data leakage can be reduced to the minimum.\n\nThe data scientist need to be mindful and should even feedback on the data governance processes to ensure data is of the highest quality and of similar priority, ensure data privacy and security, making sure that only those that need to access the data can access the data.\n\nTo give readers that are new to data science a flavor of what data governance is. Take for instance, the access of data by new employee. Data governance would spell out the process that needs to be followed to give access of data to the new employee. For instance, what is the information required to make a decision on granting the access rights, what kind of access to grant, how long is the access required, who has the authority to grant the access, after the decision is made who effect the decision etc. This is, of course, just the tip of the iceberg called \u201cdata governance\u201d.\n\nAlong the way, as the organization progresses up the learning curve, the data scientist would also need to propose data collection strategy, the right granularity level of data, the technology & infrastructure to support the strategy and also associated data governance process to better manage the new data collected.\n\nDon\u2019t think I need to stress the importance of data quality. So what are the data quality dimensions that the data scientist should pay attention to and be able to propose possible maintenance solutions? They are the following:\n\nAccuracy \u2014 To what extent, does it reflect the reality?\n\nCompleteness \u2014 Have we gotten all the possible data?\n\nTimeliness \u2014 Is the data available when I need it?\n\nValidity \u2014 Does it conformed to the defined format?\n\nConsistency \u2014 Is the format the same across different tables?\n\nAlong the learning journey, the data scientist is bound to come across more aspects of data quality. The data scientist can build up his own list along the way. More importantly is the data scientist can and should feedback on the kinds of metrics that can be created and used to measure data quality. Besides the metrics to measure data quality, being able to set the right alert level (for instance, to sound an alert when % of missing value crosses 1.0) is important to ensure \u201cdirty\u201d data does not seep in.\n\nData quality is just one aspect of data management. Other aspects of data management could be for instance, automation, validation, ETL processes, back-up, access rights etc.\n\nThe data scientist need not build the whole enterprise data warehouse but it would be good for them to have some idea how to structure it because the structure can affect the data quality of the data especially the timeliness and uniqueness aspect. When building the EDW, the data scientist can add value by bearing in mind the need for business continuity in extreme times and also keep in mind the ETL (Extract, Transform & Load) processes.\n\nIn my university days, I took an \u201cIntroduction to Computers\u201d module and the knowledge gained has helped me tremendously in my work, in understanding how computers work and how computing takes place in the computer. Things like memory bus, cache, memory, hard-disk, CPU etc, was taught in that module. It gave me a lot of appreciation on the hardware side of computing and how it can limit/enhance processing. The knowledge serves as good foundation for me to understand how technology works behind the scene as well. So I must say a good understanding of computer architecture gives the data scientist the ability to propose feasible solution in capturing and maintaining data, implementing models and algorithms in IT systems.\n\nBesides understanding how computer works, it would be great for the data scientist to have some appreciation of how IT architecture are planned, designed and built in the organization.\n\nReason for that is that the data scientist may need to implement the final/chosen models into the enterprise IT architecture and having a good understanding helps data scientist to propose feasible ideas in data collection strategy or model implementation and also during the training of models, be able to take note of the constraints and possibilities of the IT architecture when it comes to embedding these models into the architecture, (i.e. good integration of the model into the existing IT systems and business processes, ensuring a smooth flow of data and numbers). For instance, some legacy systems cannot take into account composite variables (i.e. X1X2) thus the data scientist would not be able to build a machine learning models that has composite variables but rather only can us simple features (X1 and/or X2).\n\nGiven that most data scientist would need to tap on computers\u2019 immense computation capabilities, the data scientist cannot escape from coding unless he/she wants to restrict career opportunities to only corporate environments that use a lot of point&click software. Moreover, being proficient in coding allows the data scientist more flexibility in setting the models\u2019 hyper-parameters, loss functions, allowing more \u201ccreativity\u201d into the models built.\n\nI can understand why people can be averse to programming because its like trying to talk to a foreigner in their native language which we have minimal familiarity with and hope the foreigner can get the full picture what we are trying to convey. My first programming language was Java and I sucked at it when I took a module on it in my undergrad days. As I move on in my career, I realized that I would be limiting my opportunities if I do not deal with programming and decided to pick it up again. Thank goodness, the next \u2018programming\u2019 language that I picked up was SAS and because of the controlled environment, it was a more pleasant experience picking it up. It gave me the confidence to pursue further and moving on to R and Python (open source).\n\nLearning programming has gotten easier given that many IDEs out there have made it easier (through suggestion & color codings) and the tremendous amount of resources available such as YouTube, Open Course Ware, blogs, StackOverflow etc. One can pick up programming on their own or if they need structure in their learning, approach programming bootcamps, which are the rage these days.\n\nIf I may propose another angle of learning about programming, I found that programming is like solving a logical puzzle. Solving bugs requires one to think logically and have a good understanding about how the language works behind the scene. So being able to resolve a bug (although the process is supremely and absolutely frustrating), actually does provide a sense of achievement. To me at least, it feels like after struggling to solve a puzzle for a long while, the minute the solution is obvious, a barrel of feel-fantastic just opened up (and a sense of achievement too).\n\nMoreover, there are many ways to reach the end or objective through programming. Being proficient enough to code such that the code can run efficiently, also gives one a sense of achievement too. For anyone starting on data science, I would strongly encourage one to explore programming and the language that data scientist commonly used these days are R and Python, but please do not ask me which one to go for.\n\nTo a data scientist, I believed in getting the concepts correct first and then figure out the tools to use later, because with the right concepts, one has the \u201cright\u201d training wheels fitted to pick up the language efficiently. All these articles about which tool is better are mere click bait articles that the only people gaining from it are the writers of the articles (paid in the most expensive currency called Time).\n\nAs a data scientist, we have to keep ourselves updated (as much as possible) on the latest tools that are available, both enterprise and open source tools. Its because organizations would rely on the data scientist to propose the right tools to be used in each data science projects.\n\nTo give an analogy, say you are given a toolbox with all kinds of tools inside. Looking at the challenge at hand, you would only use tools (hammer, pliers, screwdrivers) that you are familiar with and know that it is effective in resolving the challenge. We know that the more tools that we are familiar with, the likely the challenge can be resolved and resolved easily.\n\nSimilarly, the data scientist should try to be familiar with all different kinds of tools, both software and hardware and have some idea how it works, what are the pros and cons of using it, what kind of situations would it be effective, what are the maintenance costs, possible integration issues with the current set of tools etc. Having a good idea what the tools out there are gives the data scientist that added value that is important for the organization to tap onto their data for more insights.\n\nThe data scientist need not read in-depth (up to documentation level) to be able to propose the tools but rather, understand at the most fundamental level how it works and its pros and cons. If there are time and resources to experiment with it, then please do as well so that the data scientist can have a good idea the implementation challenges.\n\nSome suggested tools that data scientist should be familiar with are visualization & dash-boarding tools, machine learning tools, data warehousing & process tools and computation tools.\n\nFor data science to work, the data scientist have to take great care of the data, ensuring that the data is trustworthy and at the quality that meaningful insights can be achieved. This can be achieved if the data scientist also participated in the IT infrastructure building and maintenance. Being able to propose feasible solution helps in building up the credibility of the data scientist, making the data science team a close-knit team with high internal trust, something that is important given that data science requires team efforts in organizations.\n\nThese are the Data & IT management knowledge that I think the data scientist should know minimally. For the next blog post, I will be discussing on the domain expertise that a data scientist should know to be effective.\n\nHave fun in your data science learning journey and do visit my other blog posts and LinkedIn profile for data science nuggets."
    },
    {
        "url": "https://towardsdatascience.com/moving-into-data-science-as-a-career-mathematical-models-e13f30690b00?source=user_profile---------10----------------",
        "title": "Moving into Data Science as a Career (Mathematical Models)",
        "text": "I have written an overall view of the kinds of skills and knowledge that is needed for one to be a data scientist. In this post, I am going to write in more details, one of the areas in my Venn diagram and that is mathematical models.\n\nIf you look at data science, we are actually using mathematical models to model (and hopefully through the model to explain some of the things that we have seen) business circumstances, environment etc and through these model, we can get more insights such as the outcomes of our decision undertaken, what should we do next or how shall we do it to improve the odds. So mathematical models are important, selecting the right one to answer the business question can bring tremendous value to the organization.\n\nYes, first and foremost, like most come-back Kung-Fu movies (where the protagonist got defeated by a big bad boss, when he is down and out, found a brilliant sensei to teach him Kung-Fu, LEARN KUNG-FU, then defeat the big bad boss after, THE END) that you have seen, the sensei would always start from the basic.\n\nLinear algebra & calculus would be considered the most basic. This is especially true given the \u201cDeep Learning\u201d environment that we are in. Deep learning requires us to understand linear algebra & calculus, to understand how it works, for example forward propagation, backward propagation, parameters setting etc. Having a good foundation helps us to understand how these models work, what assumptions are made and how the parameters are derived. Back in my uni days, I studied Linear Algebra and Calculus but did not see the relevance to my work until now. Sure wished I had spent more time on it.\n\nSo what should the potential data scientist learn?\n\nFor linear algebra, there are matrix operations (plus, minus, times, divide), scalar product, dot product, eigen-vectors and eigenvalues.\n\nFor calculus, the data scientist need to understand various differentiation (to second-order derivative), integration, partial differentiation. While going through some of the materials, they do touch on mathematical series such as Taylor series. If you are interested to learn more about mathematical series, Wikipedia is pretty comprehensive. Check the link.\n\nCalculus and linear algebra are used greatly when we look at designing the loss function, regularization and learning rate of the machine learning/statistical models.\n\nWell, how can one run away from statistics when doing analysis and it needs no further introduction. From experience, understanding of statistics is needed when we intend to do experiments and testing such as in marketing, we have the A/B testing. We generally want to understand if there are any statistical difference between two samples, or after certain \u201ctreatment\u201d, did it create a statistically significant effect.\n\nSo the areas in statistics are simple statistics like measurement of centrality, distributions and different probability distributions (Weibull, Poisson etc), Baye\u2019s Theorem (there\u2019s a strong emphasis on it when it comes to learning about Artificial Intelligence later), hypothesis testing etc.\n\nIn my undergraduate years, I studied econometric, which is the closest to a machine learning/statistical model. In that study, I came across linear and logistic regression. The module covers very heavily in the assumptions of the regression models namely, heteroscedasticity, autocorrelation, E(e) = 0 and multi-collinearity. Why these assumptions are important is because in training the model, we seek to achieve what is called BLUE (Best Linear Unbiased Estimates) parameters, namely the coefficients including the intercept.\n\nBut when I moved on to machine learning models, for a course on regression models, there is no emphasis on these assumptions anymore, instead there is a heavy emphasis on setting up the loss function, the rationale behind regularization, gradient descent and learning rate.\n\nComing back, learning about machine learning models is a must for any data scientist given that they would need to propose the machine learning models that can help to provide insights to the organization. The data scientist would need to convert the business objectives given and turn them into machine learning models for answers and insights.\n\nThere are generally two types of machine learning models, supervised & unsupervised learning models.\n\nAssuming you have two sets of data. Set A has the behavior data in Period 1 and outcomes in Period 2. Set B only has the behavior data in Period 3 (or 2) but do not have any outcomes in Period 4 (or 3).\n\nUsing Set A, you are going to train a model that just by looking at the behavior, be able to \u201cpredict\u201d (or give a probability) the outcome. With the model trained out, you will \u201cscore\u201d the behavior data and try to \u201cpredict\u201d (or have a probability) on which outcome is likely.\n\nModels that can be used are called supervised learning models. Its supervised because the outcomes from Set A \u201csupervised\u201d the model to come up with good predictors.\n\nSo you might have guessed, for unsupervised learning models, there is no \u201coutcomes\u201d on the Set A data and it is usually not used with a Set B data. In fact, the unsupervised learning models is just trying to find out patterns that are inside Set A, patterns that are discerned by the model\u2019s training algorithm.\n\nHaving a good understanding of supervised and unsupervised model, the data scientist would need to know in each business objectives given, which machine learning models to use, how to use them, in what sequence to use them so as to achieve the business objectives given. A lot of the training programmes that I have seen usually used a single model chosen to achieve business objectives, for instance, in creating a email marketing response model, either a logistic regression or decision tree or support vector machine is chosen to build it. This has created a blind spot that its going to be one machine learning model for each business objectives which need not necessary be the case.\n\nThe ability to recommend which models to used and structure out the modelling objectives based on business objectives comes with experience. So for any potential data scientists, do start working on it.\n\nIn machine learning, each machine learning model that you come across have many \u201cknobs\u201d and \u201cswitches\u201d for you to tune or flip during the training of your model. These \u201cknobs\u201d and \u201cswitches\u201d are known as hyper-parameters. Data scientist with a good background in mathematics would have a high comprehension as to how to turn these \u201cknobs\u201d and \u201cswitches\u201d to get the \u201cbest\u201d models. In fact, if they have a good background, they may come up with their own loss function and set up their own stochastic gradient descent method, the two key components of training the different machine learning models (mainly supervised).\n\nMost of the time, we can train several different models (given the objectives and the hyper-parameters), we would then need to understand how model selection metrics are calculated and what kind of models do they favor.\n\nChoosing the best model need not always be based on accuracy alone, because in real life, the costs of predicting positive wrongly can be very different from predicting the negative wrongly. For instance, in an epidemic, a test that can reduce false negatives is much needed than a test that can be highly accurate because it gets a lot of true negatives.\n\nWe all work in an environment where decisions are needed to be made constantly. Being able to deploy mathematical models to help make better decision is what operation research is about. What are some of the examples of operations research? They are optimization, game theory, forecasting, queuing theory, simulation, graph theory etc. Of course, operation research also includes statistical/machine learning models to help model the business environment so that a reasonable decision can be made. This is a mathematical field altogether and requires a lot of study that is non-statistical.\n\nI believe the data scientist should be able to use these models, with parameters supported by data so that \u201cbetter\u201d decisions can be made, helping the business organization to achieve their business objectives.\n\nAt the end of the day, the data scientist should be well-versed in mathematics and statistics to give him/her the best foundation to build their data science careers. My opinion is that a data scientist most essential skills is the mathematics knowledge, being able to convert the business objectives or challenges into mathematical models and using these models as part of the basis to make the best possible decision.\n\nThere will be others who argue that programming is an essential skill which I do not deny but I see that it is essential because we are now using computers to crunch the large amount of data that we have. Imagine without having the mathematical knowledge, to understand how to model the environment, how useful would the programming skill be to the data scientist then?\n\nI hope this gives anyone who is considering a career in data science, to understand what are the mathematics that they need to know in order to be a data scientist. In my next post, I will touch on the Data & IT management and Domain Knowledge.\n\nSide Note: I figure since some of you are reading this, you might be interested to know what does Andrew Ng think are the mathematics that is needed to be strong in AI and Machine Learning. Here is the link.\n\nHave fun in your data science learning journey and do visit my other blog posts and LinkedIn profile."
    },
    {
        "url": "https://medium.com/@koolanalytics/moving-into-data-science-as-a-career-10b5f400a28e?source=user_profile---------11----------------",
        "title": "Moving into Data Science as a Career \u2013 Koo Ping Shung \u2013",
        "text": "During meetups or when I am speaking to groups in Singapore, I am always asked the question, \u201cGiven my XXX (to list the common ones, they are Computer Science, Statistics, Engineering, Economics) background, how can I get started on Data Science? How do I build up my skill set and knowledge so that so I can embark on Data Science as a career?\u201d\n\nSo I decide to write several posts here that can help individuals to keep tab on their Data Science skills/knowledge inventory.\n\nFrom the macro view, I usually show the following Venn diagram to help with understanding on the skills/knowledge that is needed.\n\nThere are a lot of Venn diagrams out there that describe what Data Science is, here is a list of them.\n\nThe thoughts behind the Venn diagram I have above is to help people understand the skills and knowledge that are needed, to guide people on becoming a data scientist. Thus you may find it \u201ccleaner\u201d compared to other Venn diagram that you have seen.\n\nThere are three components of the Venn diagram and they are\n\nLet me explain why these components. Being a data scientist, we have to advise on a few areas in the IT and Data Infrastructure, areas such as how to handle missing values, can data be captured at a more granular level, how to improve on data quality, how to implement the scorecard into existing systems etc. With a good understanding of the Data & IT Infrastructure, we can then proposed constructive suggestion on managing data and using the models that we have built. Through practical suggestion, data science can continue to add value and flourish in an organization.\n\nMathematical models would need no explanation on how essential it is for data scientist to know about it. But I would also like to point out there is a need to consider computation complexity and not a one way street into \u201chighest accuracy\u201d ville. More on what is required in another post.\n\nSo what about domain expertise? Well, previously I put the circle as business expertise rather but as the experience accumulates, I notice that NGOs and Charities are beginning to tap onto their existing data to make the donations or causes go longer. Thus I decided to change it to \u201cdomain expertise\u201d instead, to correctly reflect the current environment with regards to data science.\n\nGenerally, when we decide to build any models, data scientist should think about stakeholder\u2019s reaction to it. For instance, if we build a model that segment students and provide resources to students that are likely to succeed after the segmentation, this would create an uproar among students, especially those classified as \u201cpoor\u201d. Thus we would like to structure the business/organization objectives and models in a way that really meets the business objectives without bringing \u201cdamages\u201d to other aspect of business. And that requires good knowledge of business expertise such as business models, processes & operations.\n\nAnother example would be, if we are required to build a recommender system, accuracy would never be the sole consideration in selecting the best model for the tasks. As a data scientist, we would also have to determine the computation complexity of the chosen model as well.\n\nFor anyone that wants to learn Data Science, there are two main areas they can start on and that is \u201cData & IT Management\u201d and \u201cMathematical Models\u201d.\n\nIn the next few posts, I will be writing how one can get started on these two domains and also what the knowledge & skills one can pick up in the \u201cDomain Expertise\u201d part. I will write about the areas that a Data Scientist should know. I believe such format can help people with different background to understand where they can start their journey/learning in Data Science."
    }
]