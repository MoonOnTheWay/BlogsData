[
    {
        "url": "https://medium.com/@deepnarainsingh/vanishing-gradients-while-training-neural-network-42d6d5d83cdf?source=user_profile---------1----------------",
        "title": "Vanishing Gradients while training Neural Network \u2013 deepnarainsingh \u2013",
        "text": "One of the difficulty which is faced while training deep neural network with gradient based method is the vanishing gradient problem. Back propagation algorithm is used to train the neural net and the fundamental problem which we face in this technique is the vanishing gradient and some time exploding gradients and this causes increase in training time and accuracy suffers. This problem is caused by certain activation functions and it becomes difficult to tune the parameters of different layers in neural network and as the number of layers in neural network increases this problem become more significant.\n\nLet\u2019s explain it with an example of the sigmoid activation function.\n\nSigmoid function takes any input and squeezes its value between (0,1). By seeing the graph we can interpret that the gradient near the asymptotes is tending towards zero. This makes the training of network extremely slow as the update in the network parameters is so small . We can represent this situation as saturated neuron or dead neuron where the network is no more learning.\n\nLet\u2019s go into detail and take derivative of sigmoid function and plot it.\n\nThe maximum value of the function is .25 and asymptotic value is touching zero as the value of x increases on positive or negative side. The range of the derivative of sigmoid is between (0,.25) .\n\nConsider a neural net with input x1 , a hidden layer with single neuron and a output layer. Error will be the difference of actual value and predicted value.\n\nTaking the derivative of error with respect to the weight and by the chain rule we will get.\n\nSo if we see this term we are multiplying by the derivative of sigmoid two times and the value of derivative of sigmoid lies between (0,.25). The product of number multipled together when it\u2019s value lies between 0 and 1 gives a smaller value. This is the case for one hidden layer and imagine if its a deep neural network then the product of derivative will reduce the gradient very fast and hence we will face the problem of vanishing gradient.\n\nWe have to select the activation function where in we can avoid the situation when we face saturated or dead neurons. One solution is to use relu functions and it\u2019s different versions which avoid the vanishing gradient problem. More on this in next post."
    },
    {
        "url": "https://medium.com/@deepnarainsingh/k-nearest-neighbors-implementation-from-scratch-8c1a5dad078c?source=user_profile---------2----------------",
        "title": "K Nearest Neighbors Implementation from scratch \u2013 deepnarainsingh \u2013",
        "text": "Implementing a machine learning algorithm on your own can give better understanding about how the thing works under the hood. This also helps in understanding the mathematical implementations and gives a better overview of the application of different concepts.\n\nIn this post I will be talking about implementation of k Nearest Neighbor classifier which is one of the simplest but very effective algorithm in Machine Learning. kNN can classify a new point by examining the class of its nearest neighbors. eg: If there is an area where majority of people residing are from different economic classes like high income , middle income and low income group. So given a new data point this algorithm can classify the person lies in which of the income group based on its nearest neighbors. Under the hood this algorithm uses distance metrics which can be euclidean distance , cosine similarity , taxi cab or Manhattan distance etc.\n\nHere is the link of implementation. Any feedback is appreciated :"
    },
    {
        "url": "https://medium.com/@deepnarainsingh/real-time-mapping-of-geolocation-s-in-san-francisco-for-uber-price-surge-b0059f3e10a0?source=user_profile---------3----------------",
        "title": "Real Time mapping of geolocation\u2019s in San Francisco for UBER Price Surge.",
        "text": "I had to build a Data Engineering Product for my current semester at Galvanize MS in Data Science Program. I did lot of brainstorming on different ideas and finally decided to build a product around the price surge behavior of UBER Cabs. The price surge is one feature which always interested me.\n\nSo I decided to build two features for my product one was to map the locations in real time for price surge in San Francisco and other was to do a batch analysis or historical analysis of the price surge behavior.\n\nThis can be useful for the Uber Drivers who can track the location where price surge is happening and can drive in those area to earn more.It will also help the users who can identify the patterns and schedule their travel.\n\nI used the big data technology stacks to build this infrastructure. Some main components are Kafka for the message processing. Spark Streaming for processing the real time data. Spark Streaming was suitable for my use case as it processes the data in micro batches and data from UBER API refreshes after regular intervals. Processing and transformation was done in spark streaming and then the data was saved in Hbase. A web framework using Apache Flask was built and Google Maps API was used to show the real time location where surge is happening. Once real time processing is done then the key is updated and the data is only used for the historical Analysis of the surge behavior.\n\nI love building system from scratch and to see it live and running gives immense satisfaction.It was a great learning experience this semester .\n\nBelow is the link of my work . Any feedback is appreciated."
    }
]