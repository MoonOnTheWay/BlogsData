[
    {
        "url": "https://towardsdatascience.com/drawing-inferences-from-a-b-tests-on-proportions-frequentist-vs-bayesian-approach-c85590382b14?source=user_profile---------1----------------",
        "title": "Drawing Inferences from A/B Tests on Proportions: Frequentist vs. Bayesian Approach",
        "text": "Drawing inferences from A/B tests is an integral job to many data scientists. Often, we hear about the frequentist (classical) approach, where we specify the alpha and beta rates and see if we can reject the null hypothesis in favor of the alternate hypothesis. On the other hand, Bayesian inference uses Bayes\u2019 Theorem to update the probability for a hypothesis to be true, as more evidence becomes available.\n\nIn this blog post, we are going to use R to follow the example in [1] and extend it with a sensitivity analysis to observe the impact of tweaking the priors on the findings. [1] has a great discussion on the advantages and disadvantages of Frequentist vs. Bayesian that I\u2019d recommend reading. My main takeaways are that:\n\nThe objective of the experiment is to check if a coin is biased, suppose the person who conducts the experiment (let\u2019s call him the researcher) is not the same person who performs the analysis of results (let\u2019s call him the analyst).\n\nThe researcher has two ways to stop the experiment (stopping rules):\n\nThe researcher reports HHHHHT and his stopping rule to the analyst. However, the analyst forgot what is the stopping rule.\n\nThe Frequentist analyst sets up the hypothesis:\n\nUnder stopping rule (1), the number of heads is under the Binomial distribution. More formally, Observed Heads ~ Bin(6,0.5)\n\nTherefore, we fail to reject the null hypothesis at the 0.05 significance level.\n\nUnder stopping rule (2), the number of flips required until heads appear is under the Geometric distribution.\n\nP(It takes at least 5 fails before the 1st head) = 0.0313\n\nTherefore, we reject the null hypothesis at 0.05 significance level. Notice how the same data leads to opposite conclusions.\n\nWe want to estimate theta, which is defined as the true probability that the coin would come up heads. We use a beta distribution to represent the conjugate prior. In order not to lose the focus of the case study, we have introduced the beta distribution in the appendix.\n\nAs the prior distribution, let\u2019s say the prior is under the Beta distribution (3,3), which suggests a fairly flat distribution around 0.5. This suggests that the analyst believes that the coin is fair, but uses (3,3) as an indication of his uncertainty. We will study the impact of changing these two parameters in the Sensitivity Analysis section. For now, let\u2019s go with:\n\nDuring the experiment, we have 6 flips, of which 1 is heads. Let\u2019s fill in the following table:\n\nP(theta > 0.5 | data) = 0.89, i.e., 0.89 is the area under the red curve, to the right of 0.5. In the next section, we investigate the impact of changing the shape of the prior distribution on posterior probabilities.\n\nHow would changing the prior distribution from Beta(3,3) have an impact on the posterior probability that theta > 0.5? In this section, we are going to change the variance and the expected value of the distribution as part of the sensitivity analysis.\n\n(1) Changing the variance \u2014 When we inject a stronger prior with a lower variance that the coin is fair, the posterior probability is reduced from 0.89 to 0.84.\n\n(2) Changing the mean \u2014 Similarly, and as expected, if we inject the prior that the coin is biased towards the tails, when the experiment is biased towards heads, we are less confident that the coin is biased towards heads.\n\nGiven mean and variance, I needed to compute alpha and beta. Thankfully, we have this stackoverflow post that help us do that:\n\nFor simplicity, we are rounding off alpha and beta to the nearest integer. Hence variance might be a little different.\n\nIn conclusion, we have demonstrated the Bayesian perspective on A/B testing on small samples. We saw that the stopping rule is critical in establishing the p-value in the frequentist approach, and the stopping rule is not considered in the Bayesian approach. The Bayesian approach also gives an probability that a hypothesis is true, given the prior and experiment results. Lastly, we also observed how the posterior probability is affected by the mean and variance of the prior distribution.\n\nThe beta distribution is a family of continuous probability distributions defined on the interval [0,1] parametrized by two positive shape parameters, denoted by \u03b1 and \u03b2. There are three reasons why the beta distribution is great for Bayesian inferences:\n\nFrom the above equation, we see that \u03b1 and b control the shape of the distribution, and indeed, they are known as shape parameters. Let\u2019s plug in some values into R and observe the difference in shapes. The expected value is computed by \u03b1 / (\u03b1+\u03b2).\n\nNotice how the mean of all four distributions is the same at 0.5, and different distributions could be specified. This is what we meant by a large range of beliefs could be specified using the beta distribution.\n\n[1] Jeremy Orloff, and Jonathan Bloom. 18.05 Introduction to Probability and Statistics. Spring 2014. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA."
    },
    {
        "url": "https://towardsdatascience.com/conversation-datasets-and-learning-character-styles-from-movie-dialogues-8f6bb949ca75?source=user_profile---------2----------------",
        "title": "Conversation Datasets and Learning Character Styles from Movie Dialogues",
        "text": "As Artificial Intelligence continues to push its boundaries on cognition, it takes on a challenge that we humans do so naturally \u2014 to understand and respond using natural language. Human conversations are incredibly rich in content. The foundations of the information carried across is laid upon the words at face value, tempered by the prosodic features like tone, pitch and volume, the power difference between the two speakers, in addition to the emotions and attitudinal disposition hinted through facial expression, eye contact, body language and even the time delay of the response. This rich, turn-by-turn content makes conversations particularly interesting as compared to monologues.\n\nThe complex, multi-modal nature of dialogues requires a multi-disciplinary analysis approach \u2014 linguists, psychologists and machine learning researchers come together and draw on existing research on conversation analysis, emotions analysis, and natural language processing. To do that well, reproducible research is necessary and publicly available data is sought after. Outside of the academic, publicly available data is the starting point for data scientists and machine learning practitioners to build applied machine learning systems.\n\n[1] has done an extensive review of the availability of conversations datasets suitable for building dialogue systems. For each dataset, the authors have made detailed annotations about its size, its source and whether the conversation is:\n\nI would highly recommend this review to anyone who wishes to find conversations or dialogue datasets, the link to this review is in the references section.\n\nFor the rest of the blog post, let\u2019s focus on an application where a film corpus is used to learn character speaking style. I think films are interesting in that the character style of speaking is usually consistent across the scenes, unlike in natural spontaneous conversations where the style of the speaker has a degree of mimicry towards the more powerful speaker to build rapport.\n\n[2] has built a system that automatically generates dialogue based on film characters, using hundreds of film scripts from the Internet Movie Script Database website, and the authors have released the corpus data (https://nlds.soe.ucsc.edu/software). The authors used external tools to extract distinctive features from the transcripts. I have chosen a select few tools which may be of interest:\n\n\u00b7 Linguistic Inquiry Word Count (LIWC) [3] (http://www.liwc.net/tryonline.php): a very popular feature extraction tool amongst researchers. Here, the authors extracted the LIWC category such as anger words from the tool.\n\nAfter extracting these features, the features are fed into the \u201cPersonage\u201d architecture as described in [4]. The details of implementation are complex and it out of scope for this post, consisting of multiple modules that select the syntax, aggregate the sentence, insert pragmatic markers and making choices about the lexical structure.\n\nWe conclude with an illustration of the differences in character styles. The authors include a table for parallel comparisons between generated dialogues \u2014 differences in character styles can be clearly perceived.\n\nLastly, some short clips and quotes to gain some context about the speaking style of the characters.\n\n\u201cFortune and glory, kid. Fortune and glory.\u201d \u201cI think it\u2019s time to ask yourself; what do you believe in?\u201d \u201c\u2026Indiana Jones. I always knew someday you\u2019d come walking back through my door. I never doubted that. Something made it inevitable.\u201d \u201cProfessor of Archaeology, expert on the occult, and how does one say it\u2026 obtainer of rare antiquities.\u201d \u201cThrow me the idol; I\u2019ll throw you the whip!\u201d\n\n\u201cYou don\u2019t **** with another man\u2019s vehicle. It\u2019s just against the rules.\u201d \u201cSo you\u2019re gonna go out there, drink your drink, say \u201cGoodnight, I\u2019ve had a very lovely evening\u201d, \u201cgo home, jerk off. And that\u2019s all you\u2019re gonna do.\u201d \u201cOh man, I just shot Marvin in the face.\u201d \u201cChill out, man, I told you it was an accident. We probably went over a bump or something.\u201d \u201cWhy the **** didn\u2019t you tell us there was someone in the bathroom? Slipped your mind? You forgot to mention someone\u2019s in the bathroom with a goddamn handcannon?!\u201d\n\n[1] I.V. Serban, R. Lowe, P. Henderson, L. Charlin, J. Pineau, A Survey of Available Corpora for Building Data-Driven Dialogue Systems, (2015). https://arxiv.org/pdf/1512.05742.pdf (accessed February 22, 2018).\n\n[2] M.A. Walker, G.I. Lin, J.E. Sawyer, An Annotated Corpus of Film Dialogue for Learning and Characterizing Character Style, in: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), 2012: pp. 1373\u20131378. http://lrec.elra.info/proceedings/lrec2012/pdf/1114_Paper.pdf (accessed December 11, 2017)."
    },
    {
        "url": "https://towardsdatascience.com/exploring-emotion-combinations-using-word2vec-54c8be43fa6?source=user_profile---------3----------------",
        "title": "Exploring emotion combinations using word2vec \u2013",
        "text": "To explore the additive nature of emotions, word2vec is a great candidate model. As illustrated in the previous posts, we saw that distributed representation models such as word2vec can solve the following analogies of varying complexity and suggest the italics words.\n\nEquivalently, the above word-pair analogies can be expressed as equations:\n\nIn the following sections, we are demonstrating the additive properties of word vectors to find the italics word.\n\nTo achieve this, we use the pre-trained word2vec vectors from here. With gensim, we can use these vectors with ease.\n\nUsing the above code, the most similar word for the sum of two emotions can be extracted from word2vec, compute the cosine similarity between the suggested word and human suggestion. This similarity measure ranges from -1 (complete opposite) to 1 (identical meaning), and lastly, checks if the suggested emotion from a human is within the top 10 suggested words of word2vec.\n\nThe rest of the post is organized into 2 studies, each testing the agreement between word2vec and a specific set of suggestions. In each study, we would first present the results, followed by the discussion."
    },
    {
        "url": "https://towardsdatascience.com/distributed-representation-of-anything-14e290daf975?source=user_profile---------4----------------",
        "title": "Distributed representation of anything \u2013",
        "text": "In this review, we explore various distributed representations of anything we find on the Internet \u2014 words, paragraphs, people, photographs. These representations can be used for a variety of purposes as illustrated below. We try to select subjects that seem disparate, instead of providing a comprehensive review of all applications of distributed representation.\n\nThis is where the story begins: the idea of representing some qualitative concept (e.g. words) in a quantitative manner. If we look up a word in a dictionary, we get its definition in terms other qualitative words, helpful for humans but doesn\u2019t really help the computer (unless we do additional post-processing, e.g. feeding the word vectors of the definition words into another neural network). In this previous post, we\u2019ve introduced the idea of word vectors \u2014 a numeric representation of the qualitative word.\n\nIn summary, current NLP practices often substitute the word into a fixed-length numeric vector so that words of similar meanings have similar numeric vectors. It is worth re-highlighting the training concept that in training the numeric vector of a word (let\u2019s call it the center word), the vector of the center word is optimized to predict the surrounding context words. This training concept is extended into novel applications as we will see below.\n\nAn interesting extension of the word2vec is the distributed representation of paragraphs, just as how a fixed-length vector could represent a word, a separate fixed-length vector could represent an entire paragraph.\n\nSimply summing word-vectors across the paragraph is a reasonable approach: \u201cas the word vectors are trained to predict the surround words in the sentence, the vectors are representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer, so the sum of two word vectors is related to the product of the two context distributions. [1]\u201d Since the summation of word-vectors is commutative \u2014 order of summation doesn\u2019t matter \u2014 this approach doesn\u2019t preserve word order. Below, we review two alternative ways to train paragraph vectors.\n\n[2] proposed two ways to train paragraph vectors, the similarity across these two methods is that both representations are learned to predict the words from the paragraph.\n\nThe first method is the PV-DM (paragraph vector: distributed memory). This samples a fixed-length (say, 3) from the context window. Each of the 3-words is represented by a 7-dimension vector. The paragraph vector is also represented by a 7-dimension vector. The 4 (3+1) vectors are concatenated (into a 28-dimensional vector) or averaged (into a 7-dimensional vector) to be used as input to predict for the next word. The concatenation of word vectors in a small context window takes into consideration the word order.\n\nThe second method is the PV-DBOW (paragraph vector: distributed bag of words). This randomly samples 4 words from the paragraph, and only uses the paragraph vector as input.\n\nNow that we have the paragraph vectors, we could perform clustering using these high-dimensional vectors. Paragraph embedding, whether trained using the above two methods or simple summation, enables text articles (e.g. medical notes [3]) to be clustered.\n\n[4] investigate training the paragraph vectors using Wikipedia articles: one paragraph vector represents one Wiki article. By training the word vectors jointly with the paragraph vector, the authors show that to find the Japanese equivalence of \u201cLady Gaga\u201d can be achieved by vector operations:\n\nThe mixed-use of word vectors and paragraph vectors is powerful: it can explain the difference between two articles in one word, or explain the difference between two words in one article. We can, for example, find the word vector that approximates the difference between paragraph vectors of \u201cDonald Trump\u201d and \u201cBarack Obama\u201d.\n\nExciting, isn\u2019t it? There\u2019s more, Stitch Fix has shown that we can do these operations to pictures.\n\nThere is an existing great post from the authors, so please visit it for more details regarding this fascinating work. In summary, if someone likes the pregnant version of the apparel, we could add pregnant to the current apparel and retrieve a similar-style pregnant version.\n\nThis article reviews how the notion of the surrounding words of the subject defines the subject can be useful in representing words, paragraphs, people and even pictures. Math operations can be performed on these vectors to yield insights and/or retrieve information.\n\nHave I missed any other interesting applications? Please let me know in the comments below!"
    },
    {
        "url": "https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b?source=user_profile---------5----------------",
        "title": "Understanding how Convolutional Neural Network (CNN) perform text classification with word\u2026",
        "text": "CNN has been successful in various text classification tasks. In [1], the author showed that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks \u2014 improving upon the state of the art on 4 out of 7 tasks.\n\nHowever, when learning to apply CNN on word embeddings, keeping track of the dimensions of the matrices can be confusing. The aim of this short post is to simply to keep track of these dimensions and understand how CNN works for text classification. We would use a one-layer CNN on a 7-word sentence, with word embeddings of dimension 5 \u2014 a toy example to aid the understanding of CNN. All examples are from [2].\n\nThe example is \u201cI like this movie very much!\u201d, there are 6 words here and the exclamation mark is treated like a word \u2014 some researchers do this differently and disregard the exclamation mark \u2014 in total there are 7 words in the sentence. The authors chose 5 to be the dimension of the word vectors. We let s denote the length of sentence and d denote the dimension of the word vector, hence we now have a sentence matrix of the shape s x d, or 7 x 5.\n\nOne of the desirable properties of CNN is that it preserves 2D spatial orientation in computer vision. Texts, like pictures, have an orientation. Instead of 2-dimensional, texts have a one-dimensional structure where words sequence matter. We also recall that all words in the example are each replaced by a 5-dimensional word vector, hence we fix one dimension of the filter to match the word vectors (5) and vary the region size, h. Region size refers to the number of rows \u2014 representing word \u2014 of the sentence matrix that would be filtered.\n\nIn the figure, #filters are the illustrations of the filters, not what has been filtered out from the sentence matrix by the filter, the next paragraph would make this distinction clearer. Here, the authors chose to use 6 filters \u2014 2 complementary filters to consider (2,3,4) words.\n\nFor this section, we step-through on how CNN perform convolutions / filtering. I have filled in some numbers in the sentence matrix and the filter matrix for clarity.\n\nFirst, the two-word filter, represented by the 2 x 5 yellow matrix w, overlays across the word vectors of \u201cI\u201d and \u201clike\u201d. Next, it performs an element-wise product for all its 2 x 5 elements, and then sum them up and obtain one number (0.6 x 0.2 + 0.5 x 0.1 + \u2026 + 0.1 x 0.1 = 0.51). 0.51 is recorded as the first element of the output sequence, o, for this filter. Then, the filter moves down 1 word and overlays across the word vectors of \u2018like\u2019 and \u2018this\u2019 and perform the same operation to get 0.53. Therefore, o will have the shape of (s\u2013h+1 x 1), in this case (7\u20132+1 x 1)\n\nTo obtain the feature map, c, we add a bias term (a scalar, i.e., shape 1\u00d71) and apply an activation function (e.g. ReLU). This gives us c, with the same shape as o (s\u2013h+1 x 1).\n\nNotice that the dimensionality of c is dependent both s and h, in other words, it will vary across sentences of different lengths and filters of different region sizes. To tackle this problem, the authors employ the 1-max pooling function and extract the largest number from each c vector.\n\nAfter 1-max pooling, we are certain to have a fixed-length vector of 6 elements ( = number of filters = number of filters per region size (2) x number of region size considered (3)). This fixed length vector can then be fed into a softmax (fully-connected) layer to perform the classification. The error from the classification is then back-propagated back into the following parameters as part of learning:\n\nThis short post clarifies the workings of the CNN on word embeddings by focussing on the dimensionality of matrices in each intermediate step."
    },
    {
        "url": "https://chatbotsmagazine.com/introduction-to-word-embeddings-55734fd7068a?source=user_profile---------6----------------",
        "title": "Introduction to Word Embeddings \u2013",
        "text": "Word embeddings are commonly used in many Natural Language Processing (NLP) tasks because they are found to be useful representations of words and often lead to better performance in the various tasks performed. Given its widespread use, this post seeks to introduce the concept of word embeddings to the prospective NLP practitioner.\n\nWord embeddings allow words to be represented by a series of numbers \u2014 which we would refer to as real-valued vectors from now on. For example, the following phrase can be represented by a series of vectors, each vector having a dimension of 2.\n\nI found it intriguing that a qualitative and abstract idea such as a word could be represented by a numeric vector of a fixed dimension. A common question is what do these numbers represent and how do they get decided? Wouldn\u2019t it be nice if one dimension corresponds to a degree of happiness, another corresponds to a degree of formality and the algorithm places the words in the hyperspace according to these interpretable dimensions? Unfortunately, the dimensions are not optimized to represent concepts. There have been, however, studies that use dimension reduction techniques to reveal clusters of words with similar meanings.\n\nBroadly speaking, the vector of each word is optimized to be able to predict the surrounding words.\n\nIn computational linguistics, this is known as distributional similarity. This idea is used to train the two popular word embeddings that the community is using \u2014 word2vec [1] and glove [2]. In the training process, the algorithm repeatedly adjusts the values of each word vector such that it is best predicting its surround context words. If you would like to learn more details, I highly recommend this lecture from Stanford University and in fact, the entire series.\n\nFor these two sets of word embeddings, a large corpus \u2014 with billions of tokens \u2014 is used to train to convergence. Once trained, these word vectors are now dense distributed representations of the words. Dense in the sense that it combats the problem of sparsity, to be discussed in the next paragraph. Distributed because the meaning \u2014 formally semantic content \u2014 of the word is spread across the number vector.\n\nBefore neural network architecture became popular in NLP, a common approach is to use Support Vector Machines (SVM) to tackle text classification problems. A typical approach to handle the input variable is to use an n-gram approach and count the number of occurrence of each uni-, bi- and tri-gram. A unigram is the occurrence of a word and a bigram is the occurrence of a two-word phrase. The major drawback of such an approach is the curse of dimensionality, also known as the problem of sparsity. The English vocabulary is very large, let\u2019s take 20k as an example. Although \u2018frog\u2019 and \u2018toad\u2019 have similar meaning, because of the unigram representation, this pair is as different as \u2018frog\u2019 and \u2018hotel\u2019. This is obviously undesirable and word vectors tackle this problem because similar words have similar vectors.\n\nBeing memory intensive is another problem of the n-gram approach. If the unigram representation requires 20k columns to count each occurrence, a bi-gram and trigram counters could take exponentially more columns to count the occurrences. Researchers investigate ways to represent multiple words, [3] investigates the extension of word representations to phrase representations; [4] further extends the idea to represent paragraphs. Because of its success, dense distributed embedding has been an active area of research, extending itself into other inter-disciplinary domains. For example, in the medical domain to represent medical concepts [5], notes, visits [6] and patients [7].\n\nThrough optimizing the word vectors to best predict its context words, clusters of similar words and relationships between words are formed.\n\nAs previously mentioned, dimension reduction techniques can be applied to project the multi-dimensional vectors into 2D space to visualize the clusters and relationship between words.\n\nWords that are close in meaning are clustered near to one another, as illustrated in the glove\u2019s web site, the nearest neighbor of frog is frogs, toads, litoria. This implies that it is alright for a classifier to not see the word litoria and only frog during training, and the classifier would not be thrown off when it sees litoria during testing because the two word vectors are similar.\n\nAs illustrated on the glove\u2019s website, word embeddings learn relationships \u2014 formally linear substructures \u2014 between words. Vector differences between a pair of words can be added to another word vector to find the analogous word. For example, \u201cman\u201d \u2014 \u201cwoman\u201d + \u201cqueen\u201d \u2248 \u201cking\u201d.\n\nThis post introduces word embeddings in natural language processing. It briefly discusses the process through which the values of word vectors get adjusted, the motivations of having word vectors and the desirable properties associated with word vectors."
    }
]