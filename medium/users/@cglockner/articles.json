[
    {
        "url": "https://towardsdatascience.com/bonsai-ai-using-simulink-for-deep-reinforcement-learning-32dc11dfdc5a?source=user_profile---------1----------------",
        "title": "Bonsai AI: Using Simulink for Deep Reinforcement Learning",
        "text": "This is the second post in our Simulation and Deep Reinforcement Learning (DRL) series. In our first post, we covered the benefits of simulations as training environments for DRL. Now, we\u2019ll focus on how to to make simulations + DRL work.\n\nIn the example below, we will train a Bonsai BRAIN using a Simulink model. The goal is to teach the BRAIN (an AI model built in the Bonsai Platform) how to tune a wind turbine and maximize the energy output of it by keeping it turned into the wind at an optimal angle.\n\nSimulink provides a great training environment for DRL as it allows 3rd parties like Bonsai to integrate and control simulation models from the outside. This ability is one of the basic requirements for simulation platforms to be feasible for Deep Reinforcement Learning using Bonsai AI. More requirements can be found here.\n\nThis Simulink Wind Turbine model is provided by The MathWorks. For this scenario, it represents a simple control problem that can be solved by applying reinforcement learning.\n\nFirst, we need to identify a control point within the model so Bonsai can take over inputs and outputs. We\u2019re doing this by inserting a Bonsai block into the model, replacing the existing control block.\n\nOnce training has completed, you can use the trained Bonsai BRAIN to get predictions.\n\nSimulators are a crucial tool for reinforcement learning. Enterprises can use simulation models that reflect real-world business processes or physical realities and optimize them with Bonsai\u2019s reinforcement learning technology. Typically, there are no changes needed to the simulation model. If you\u2019ve missed our first post on how simulations can be used for training, please find it on our blog.\n\nBonsai can help you apply deep reinforcement learning technology and build intelligent control into your own industrial systems using Simulink as the training environment. If you are using Simulink and you want to try out Bonsai AI, join our beta program and get started here."
    },
    {
        "url": "https://towardsdatascience.com/simulators-the-key-training-environment-for-applied-deep-reinforcement-learning-9a54353f494f?source=user_profile---------2----------------",
        "title": "Simulators: The Key Training Environment for Applied Deep Reinforcement Learning",
        "text": "Deep reinforcement learning (DRL) is one of the most exciting fields in AI right now. It\u2019s still early days, but there are obvious and underserved markets to which this technology can be applied today: enterprises that want to automate or optimize the efficiency of industrial systems and processes (including manufacturing, energy, HVAC, robotics, and supply chain systems).\n\nBut there is a key element for building applied DRL: simulation environments. In this blog, we\u2019ll tell you what simulators can do, why you need them, and how you can use the Bonsai Platform + simulators to solve real business problems.\n\nLet\u2019s start with defining the term simulation as it\u2019s quite an abstract concept. Simulations can range from flight simulators to simulations of electrical and mechanical components or models of entire cities.\n\n\u201cSimulation is the imitation of the operation of a real-world process or system over time.\u201d\n\nEssentially, there is some kind of system that has a number of inputs, applies some mathematical functions to these inputs, and delivers back an output in the form of data that can be visual (like a robotics simulator) or just pure data (like the energy simulator, EnergyPlus).\n\nSimulations have been used by computer scientists for quite some time, going back to the late 1950s. During the last 20 years, increased computing power and vast amounts of data have allowed simulations to dramatically increase in fidelity and value. Many leading industrial simulations match physical realities or business processes almost identically.\n\nA huge influence has been the evolution of the digital gaming industry. Gamers wanted a more immersive experience, requiring high fidelity graphics and more realistic behaviors of items within the virtual worlds. Gaming middleware companies developed and delivered powerful 3D and 2D physics engines over the past 30 years.\n\nBy utilizing some of these software products and a variety of mathematical libraries, enterprises are able to simulate complex systems with a large number of components that allow subject matter experts (SME) to test and evaluate systems prior to building them in the real world. Use cases include digital twins, robotics, tuning small and large industrial machines, electrical and physical systems of many kinds, and optimizing business processes like supply chains.\n\nWhile there exist a large number of custom and very specialized simulations based on a single model, there are also a number of simulator platforms which are able to run and simulate a basically infinite number of models. Examples are MATLAB Simulink (engineering and manufacturing), ANSYS (engineering), AnyLogic (supply chain), Gazebo (robotics), TRNSYS (energy), and many others.\n\n\u201cAn area of machine learning concerned with how software agents ought to take actions in an environment to maximize a cumulative reward\u201d.\n\nIn other words, RL trains an agent to learn a policy for how to act by trying a large number of actions in a given environment, optimizing for a defined reward function.\n\nDeep reinforcement learning (DRL) follows the same method, using a deep neural network to represent the policy.\n\nReinforcement learning requires a very high volume of \u201ctrial and error\u201d episodes \u2014 or interactions with an environment \u2014 to learn a good policy. Therefore simulators are required to achieve results in a cost-effective and timely way.\n\nJust imagine trying to teach a robot to walk by watching a real, physical robot try and fall 100,000 times before it could successfully and consistently walk. Or training an AI to play the boardgame GO by actually playing a human competitor for hundreds of thousands of games. Simulators allow these episodes to happen in a digital world, training an AI to reach its full potential while saving time and money.\n\nSome simulations model environments in which an agent can take continuousactions that impact the state of the environment; other simulations model settings where a discrete input creates a different output. Both of these types of simulations can be used for reinforcement learning.\n\nBonsai is an artificial intelligence platform that allows enterprises to program control into industrial systems, and the only commercially available product for programming control of industrial systems using deep reinforcement learning.\n\nUsing the Bonsai Platform, enterprises can build a BRAIN (an AI model), connect the simulator of their choice, and train the BRAIN in that environment to learn a desired behavior.\n\nTo learn more about building a simulation and applying DRL to your enterprise, head to our Getting Started page."
    },
    {
        "url": "https://medium.com/@cglockner/why-i-joined-bonsai-the-ai-revolution-98e2c476668d?source=user_profile---------3----------------",
        "title": "Why I joined Bonsai + the AI revolution \u2013 Cyrill Glockner \u2013",
        "text": "After working for almost 20 years on software platforms and tools for companies like Microsoft, Intel and others, I\u2019ve just last week started at Bonsai in the product management team. Since taking the job, a lot of people have asked me how I chose Bonsai out of all the different artificial intelligence companies, so I decided to share my story here.\n\nDuring summer 2016, I participated in a three-day Artificial Intelligence workshop organized by the Aspen Institute\u2019s Socrates chapter. Led by the incredible Neil Jacobstein, from Singularity University, our group of around 30 participants from different backgrounds worked through a comprehensive set of curated materials and engaged in deep discussions on the subject at hand. This was not meant to be a technical deep dive but rather a starting point for developing a perspective of the impacts of Artificial Intelligence and automation on societies. At the last day of the gathering, we all were convinced that we\u2019ve entered a time that will come with dramatic changes touching every aspect of human society and everyday life.\n\nThis anticipated shift made me recognize that I want to be part of upcoming AI revolution and make it my career in the foreseeable future. It also turns out, AI is a substantial field presenting a large number of opportunities for a Product person like me. While it has been around since the 50s, we recently experienced a large improvement in its ability to deliver results that have never been achieved before, which led to a large number of start-ups and increased investments by established enterprises. This has also presented a significant amount of opportunities for somebody looking to get into this space.\n\nThroughout my career, I\u2019ve been passionate about building platforms that allow both individual programmers and enterprises to develop applications that can address a broad range of uses cases. Applying this to the AI world, I decided to follow that passion and look for artificial intelligence companies building AI platforms focused on enabling users to build smarter applications by adding the latest Deep Learning (DL) technologies into their offerings.\n\nWhen evaluating platforms in general, they have to meet a number of key guiding principles:\n\nWhen assessing the AI Platform landscape it basically came down to three different approaches that I could choose from. There are Cloud Providers which are established large scale companies that offer Machine Learning as a Service (MLaaS). Familiar household names include Google, Amazon, Microsoft, IBM and others. Secondly there are Start-Ups using a simple abstraction approach of trying to make using existing DL networks easier by creating some sort of a wrapper that hides complexity. And then there are those visionary companies that have decided it is worth the risk to take a fundamentally different approach to solving the problem.\n\nAfter doing a significant amount of research and playing around with some of the existing tools and technologies like TensorFlow, Keras etc., I eventually found Bonsai.\n\nFalling into the 3rd category from above, Bonsai uniquely provides a complete platform that allows companies and developers to build AI into their applications without requiring in-depth knowledge of the latest Deep Learning technologies. I found this approach to be extremely compelling compared to how the rest of the industry is addressing the problem. The large cloud providers are investing heavily in core AI frameworks and research but are less willing to implement substantially different ways to solve customer problems, while most of the startups are also only making incremental progress in delivering against the above stated principles.\n\nThings got really exciting when I started to learn more about the individuals who founded Bonsai. Mark Hammond, the CEO, has this ability to clearly articulate what problems the company is solving and how it is doing it. While this should be a given, it\u2019s a rare find in an extremely challenging space that requires sophisticated solutions. After more discussions with the leadership team, especially with the co-founder and Head of Product, Keen Browne, I was convinced to have found the right place for me. We immediately found common ground on how we saw the platform space, and how to build products focusing on solving customer problems, as well as how we should work across teams in an environment that is dynamic, fun and rewarding.\n\nI\u2019m super excited to have joined a company whose founders vision is centered around how to create a platform for enterprises and individual developers that can learn to solve a huge number of problems using concepts that everybody understands. Done right, we can enable our customers to stay out of the weeds of low-level machine learning libraries and instead focus solely on solving real business problems.\n\nTo read more about what we are up to at Bonsai, you can read more about our AI Platform here, or if you prefer you can jump straight into the docs.\n\nAlso, we\u2019re hiring \u2014 please come and check out our open positions."
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-and-poisonous-mushrooms-4377ea4c9b80?source=user_profile---------4----------------",
        "title": "Deep Learning and Poisonous Mushrooms \u2013",
        "text": "Using a Deep Learning Network to solve a common problem while knowing very little about mushrooms or machine learning.\n\nMachine Learning is such a fascinating topic, it allows machines to learn how to accomplish tasks that historically needed a human being. While machine learning used to require the most powerful supercomputers just a few years ago, the arrival of cloud computing, cheaper CPUs, and much better algorithms allowed Machine Learning to become accessible more broadly. Today, high-school students can use Machine Learning on a regular home computer with the power to deliver better image classification results than human beings. But first things first, here\u2019s why I\u2019m excited about Machine Learning, specifically Deep Learning.\n\nGiven the recent press coverage about the advancements in Artificial Intelligence especially on Deep Learning Networks, I decided to learn about some of the core concepts that allowed machines to beat humans in Jeopardy, Go, and poker or power self-driving cars and lots of other applications that typically require a human brain. I think of Deep Learning Networks as extremely large and sophisticated layered trial and error systems that can learn by themselves when given large data sets and a significant amount of computing resources.\n\nI turned to Google Search to identify the technology that powered AlphaGo, the Artificial Intelligence that beat the world\u2019s best Go player. Google used a Deep Learning Network System called DeepMind. Shortly after winning against the best human player, DeepMind switched to an Open Source Deep Learning library called TensorFlow. Not having heard much about TensorFlow, I continued to look for a manageable entry point, that didn\u2019t require a Ph.D., to understand the basics of TensorFlow and some examples that I could replicate on my laptop at home. And I found a large library of YouTube videos, GitHub projects, blog posts and similar.\n\nI found these two resources most helpful for learning about TensorFlow:\n\n1) Youtube videos by Siraj Raval, who wants to inspire and educate developers on Artifical Intelligence so they can build games, music, chatbots, create art and lots of other cool things using YouTube videos and Udacity. I watched his video on YouTube that supposedly teaches you to \u201cBuild a TensorFlow Image Classifier in 5 Min\u201d. https://youtu.be/QfNvhPx5Px8.\n\nThe video is hands-on, fast paced and quite challenging to follow but after watching it several times, I was convinced that I could follow the example and was inspired to create my own image classifier that was not based on Darth Vader and Panda bears.\n\n2) Google Developer Codelabs is providing an example for image classification called \u201cTensorFlow for Poets\u201d which allows you to identify the genus of a given flower. This example uses several, like daisies, sunflowers, dandelions, tulips, and roses. It provides a detailed step-by-step list that is fairly simple to follow as long as everything goes by plan. https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0\n\nYou can check out the links for complete instructions.\n\nHOW THE MUSHROOMS CAME INTO PLAY\n\nWhat\u2019s a better use case for an image classifier than matching images against Darth Vader that can be implemented based on available image libraries? I turned to mushrooms. Whenever we\u2019re seeing mushroom while on a hike somewhere in the woods, I usually do not know if the spotted mushroom is edible or not. A few years ago my partner and I thought it would be great to develop a mobile application that could tell if the subject is okay for consumption or not. Just point your camera at the mushroom and the app will tell you if it\u2019s edible or not. To be clear, I have no plans to build out a web service that would is needed for such an application, but image classification based on TensorFlow is able to provide the most complicated part of the solution today and one could build the mushroom identifier app powered by a Deep Learning Network relatively easily.\n\nTHE PROJECT AND THE RESULTS\n\nThis is what I did:\n\nI used Google image search to download 600 pictures of edible mushrooms and the same number of poisonous ones and fed them into my TensorFlow-based Deep Learning Network. After roughly 30 minutes of training, accuracy and loss function values pointed into the right direction so I knew the network would be able to deliver results.\n\nThis graph shows the accuracy level of the Deep Learning Network. The number of training steps on the x-axes and accuracy on the y-axes. Reaching ~97% is quite impressive.\n\nCross entropy on the y-axes is used as the loss function, in this case. It basically shows how efficient the network learns. Smaller numbers are better. The number of training steps on the x-axes.\n\nThese are the two most impressive results. There were other less decisive results which I believe are based on the somewhat unscientific researched image library, as the accuracy of the network is pretty high. It would be an interesting project to clean up the image database and use a larger dataset of edible and poisonous mushrooms.\n\nI\u2019m amazed by the simplicity and low bar of technical knowledge that is required to get a functioning Deep Learning Network up and running. This groundbreaking technology is now accessible to developers and hobbyists worldwide, no Computer Science degree required for using it. We are witnessing an important shift, powerful Machine Learning technology is transitioning out of the labs of a few scientists into the hands of thousands of developers who will deliver hundreds of thousands AI applications with an unimaginable impact on our lives. Toolkits like TensorFlow and others will make this happen."
    },
    {
        "url": "https://medium.com/@cglockner/how-to-build-an-alexa-skill-without-being-an-engineer-or-the-ambition-to-ever-become-one-a28ecc0c62b7?source=user_profile---------5----------------",
        "title": "How to build an Alexa Skill without being an engineer (or the ambition to ever become one)",
        "text": "This is a step by step guide to how I built an Alexa Skill from scratch\u2013and what I learned along the way. Skills are extending Alexa\u2019s capabilities and are provided by 3rd parties, very similar to apps for the iPhone and android devices. After enabling the usual suspects of skills and services, think Nest, amazon music, Pandora, Lyft, I identified another use case that would address a question that comes up in our house all the time.\n\nMy partner, Leila, would often ask me \u201cCan we go sailing tomorrow?\u201d Typically, I would go the NOAA website, search for the desired location, check the wind and weather forecast and then tell her something like, \u201cNo, there is too much wind,\u201d or \u201cYeah, wind looks good, let\u2019s plan for it.\u201d With the arrival of Alexa in our lives, Leila thought it would be cool, if Alexa could answer her question, so I decided to build a skill that would exactly do that: Wind Checker.\n\nCoincidentally, I just finished a python class on codeacademy with the goal to play around with Tensorflow for getting a better understanding of the nuts and bolts of deep learning networks. I was quite happy when I learned that I could utilize my newly learned skills using a python based wrapper to develop the skill.\n\nThese are the activities that lead to publication of Wind Checker start to finish:\n\n1) Review the Alexa Skills Kit (ASK)\n\nI went to the Amazon developer website, read through most of the documentation and watched videos to understand the general architecture of skills and how intents and slots work.\n\n2) Define the Skill\n\n I then determined what the skill is supposed to do and how it interacts with the user. This is crucial as voice interfaces are so different from web and app UI. Will it be conversational or more transactional? What if Alexa doesn\u2019t receive all needed information or some of it is not correct or in the wrong format?\n\n3) Start Building\n\n I learned the most while writing code and building simple interactions between Alexa and my first demo skills. I do have some prior coding experience and used a Unix based shell 20 years ago which helped me to set up my Mac for software development, though this is not a requirement for getting started with Alexa. None of the technologies and tools I\u2019ve used are difficult to learn and there are plenty of instructions available on the Internet.\n\n \n\n Here\u2019s the list of tools I needed to install:\n\na. Pip. Packet manager needed for installing more tools.\n\nb. Flask Ask. Python wrapper for ASK (Alexa Skills Kit).\n\nc. Homebrew (optional). If you want to use python3.\n\nd. Virtualenv. Creates virtual environments for each of your projects. Confusing at first but very valuable when deploying your final work.\n\ne. Atom. My editor of choice.\n\nf. Ngrok. Crucial for simplifying local development. Ngrok points ASK requests to your local machine using https.\n\ng. PYOWM. Easy to use python wrapper for OpenWeatherMap API access. This is where I get the wind data from.\n\nh. Zappa. Awesome tool to deploy your skill and related python modules to Amazon Lambda.\n\nIt\u2019s okay to think, \u201cWhat the heck are ngrok and zappa?\u201d when reading through this list. I was surprised how fast one can become familiar with these tools and comfortable using them. Whenever I got stuck, I tried to break down the problem in smaller manageable parts and work through them one by one. This approach helped me make progress while learning the needed python syntax.\n\n4) Focus on Development\n\n Flask ASK and the ASK (Alexa Skills Kit) are the areas that I spent most time on. The actual amount of python code itself is very small, which makes these projects doable in a short amount of time and without the need to fully understand all the details. Accessing weather data and making sense out of it took some time, as I needed to understand the API provided by OpenWeatherMap. Once I was able to configure a skill on the Alexa Dev Portal, and have it interact with my local python script using ngrok, I felt that I got the basic foundation done and could start to add more functionality piece by piece.\n\n5) Prepare for Publishing and Certification\n\n Before publishing the skill, make sure to do test the skill for potential error cases. In my case, I needed to catch missing data, invalid dates from the past or dates too far out in the future etc. Certification by amazon takes 2\u20133 days and is pretty straight forward. They test your example interactions and make sure skills are not infringing on somebody else\u2019s trademarks. I hope amazon will improve publishers ability to modify skill descriptions and other metadata without the need to re-certify in the future. The marketplace is not on par with what iTunes and google play offer.\n\n6) Watch and learn\n\n 63 Unique Users have activated the skill within the last 4 days. No reviews have been written yet but I\u2019m hopeful to learn what my audience wants Wind Checker to become.\n\nMy intention for building the skill were based on curiosity of what it would take doing it and to create a fun way for Leila and others to check if wind conditions allow for going out on the water.\n\nFor the future, I have a list of features that I would like to add to Wind Checker:\n\n- Users can specify time of day e.g. afternoon or 2pm\n\n- Skill has access to NOAA marine weather including severe weather warnings\n\n- Provide personalized recommendations based on sailing skills\n\n- Wind Checker learns from user feedback to improve recommendations\n\nI also believe there is room for 3rd parties to supply tools that add BI and a/b testing features to ASK. It seems that amazon has its hands full to add more core platform features like notifications and identity do Alexa right now and leaves space for other companies to fill in the blanks.\n\nVoice will play a much larger role for accessing services and apps in the future and this is just the beginning. Amazon, Microsoft, Google, Apple and others are just getting started in building out their eco-systems and that\u2019s the most exciting time to engage with Alexa & Co.\n\nPlease reach out to me or leave feedback, I\u2019m happy to share more details but would also like to learn about your experience while developing skills for Alexa."
    },
    {
        "url": "https://towardsdatascience.com/ai-and-ethics-7d860c7f352d?source=user_profile---------6----------------",
        "title": "Ethical AI Index \u2013",
        "text": "Can we teach robots to make us happy?\n\nIn August 2015, a growing number of smart, tech savvy people were sharing their concerns about the dangers of Artificial Intelligence (AI) systems. Stephen Hawking warned that \u201cAI could bring us all sorts of things we didn\u2019t like \u2014 autonomous weapons, economic disruption and machines that developed a will of their own, in conflict with humanity.\u201d\n\nThis started a necessary and welcomed debate highlighting an issue that historically didn\u2019t get the needed visibility and attention of larger parts of our society outside of the research community. The arrival of Automation and Robotics has shown that we\u2019re trying to mitigate the impact of new technologies long after they\u2019ve disrupted many industries and changed the lives of millions of people. Given the much larger potential of AI systems to impact humanity, we need to explore the social discourse on how humans and AI systems can exist together to the benefit of everybody.\n\nInstead of questioning the technical feasibility of a general AI system, I assume it\u2019s going be developed at some point in time and rather suggest an interdisciplinary approach that merges the concepts of normative ethics with the latest deep learning networks to address the potentially negative impacts of AI. Normative ethics is essentially a set of concepts that allow us to decide whether an action is right or wrong, morally speaking. Meaning, how can we make sure the AI is doing the right thing?\n\n1) The robot can only act if the result is positive\n\nIt\u2019s quite straight forward. Take the well-known and broadly discussed theory Immanuel Kant\u2019s Categorical Imperative and connect it to an AI system. Simplified, Kant came up with an ethical guide for human actions, that defines an action as ethical if it could become a general law, that would allow all other humans the exact same behavior. While this seems to be a very smart and simple approach, it\u2019s not feasible to live our lives following it as we don\u2019t know the impact of our actions if we let them become general laws. AI can help with that because, in reverse, we can program AI systems in a way where they could run every action through the categorical imperative formula and only act if the result is positive.\n\n2) The robot can only act if I\u2019m smiling\n\nAnother approach could be to tie actions of an AI system to the human happiness index. This would follow a more utilitarian approach as it basically defines an action as ethical if it increases somebody\u2019s happiness (and unethical if not). The AI must learn what increases human happiness and only act under the condition to make us happier. Doesn\u2019t that sound great?\n\nNot all decisions have to be that black and white, though. To address different needs for different scenarios, we need an ethical index that represents a number on a scale from 1\u2013100. Let\u2019s call it the Ethical Artificial Intelligence Index (EAII). So, 1 would be the least ethical while 100 would be a perfect score. For example, a self-driving car should only make decisions that are based on a perfect or near perfect ethical score because human lives are at risk, while for an AI that makes investment decisions, something within the 70+ range would be acceptable.\n\nApplying the EAII scale would be based on moral standards that we as a society have defined over decades but also leaves room for improvement (e.g. increasing the level of an ethically acceptable investment decision to 80+ over time). As a starting point, AI applications could follow the EAII scale on a voluntary basis. This is important as we need to remain in an environment that doesn\u2019t stifle innovation of AI systems but rather create one that helps them to make better decisions.\n\nLet\u2019s use the investment decision as an example. Imagine a bank is using an AI System to decide where to invest someone\u2019s retirement funds. It decides to only follow recommended actions by the AI that have an EAII number above 70. By using these parameters, results of the deep learning network are restricted by boundaries that are not just maximizing for profit alone as they are required to meet an ethical standard as well. It also receives feedback from decisions that have not reached the defined EAII and will use these to learn for future recommendations.\n\nWe currently are not able to establish cause and effect for a larger range of our (or the AI system\u2019s) actions, but I\u2019m confident we can identify narrow use cases that provide enough data to get started and, over time, expand to broader scenarios.\n\nNow more than ever, the AI community and other enthusiasts need to work across disciplines and engage deeper for a future that has the potential to create more just and ethical societies while making sure AI systems support our moral standards.\n\nAbout the author: Cyrill Glockner is a technology enthusiast with a deep passion for AI & Philosophy and their practical application in everyday life. He spent almost 20 years as a Business Development and Product executive for companies like Microsoft, Intel and Verizon."
    }
]