[
    {
        "url": "https://buzzrobot.com/hiding-images-using-ai-deep-steganography-b7726bd58b06?source=user_profile---------1----------------",
        "title": "Hiding Images using AI \u2014 Deep Steganography \u2013",
        "text": "Deep Learning is giving us some very new kinds of things. From areas like Style Transfer, to Unsupervised Translation , it is constantly pushing the boundaries of computers. Interestingly, we have not yet reached an upper bound , and new papers with great results seem to come up very often. This post discusses one such new paper \u2014 Deep Steganography.\n\nSteganography is the process of hiding some type of data into other data. An example would be to hide an Image inside another Image. The key difference between cryptography and steganography is that in steganography, the Image looks unchanged, and therefore will not be scrutinised or analysed by middlemen.\n\nFigure 1.0 shows a general steganography framework. It consists of 2 inputs, a Secret Image , and a Cover image. The Secret Image the image you want to hide. The Cover image is the image that should \u2018cover\u2019 the secret image. These two inputs are passed through some Hiding Algorithm to generate the Output Image. The output should look exactly like the cover image, but upon using a Revealing Algorithm, it will generate the secret image.\n\nThus, to an unsuspecting eye, the output will look like an ordinary image, but it would also contain a secret image.\n\nCurrent methods that hide images in other images already exist, but there are a few problems associated with these.\n\nConvolutional Neural Networks have shown to learn structures that correspond to logical features. These features increase their level of abstraction as we go deeper into the network. Using a ConvNet will solve all the problems mentioned above. Firstly, the convnet will have a good idea about the patterns of natural images, and will be able to make decisions on which areas are redundant, and more pixels can be hidden there. By saving space on redundant areas, the amount of hidden information can be increased. Because the architecture and the weights can be randomised, the exact way in which the network will hide the information cannot be known to anybody who doesnt have the weights.\n\nThe entire network architecture is surprisingly similar to Auto Encoders. In general, auto-encoders are made to reproduce the input after a series of transformations. By doing this, they learn about the features of the input distribution.\n\nIn this case, the architecture is slightly different. Instead of merely reproducing images, the architecture has to hide an image , as well as reproduce an other image."
    },
    {
        "url": "https://buzzrobot.com/4-ways-to-easily-fool-your-deep-neural-net-dca49463bd0?source=user_profile---------2----------------",
        "title": "4 Ways to Easily Fool your Deep Neural Net \u2013",
        "text": "The previous paper finds tiny perturbations in the entire image to make the whole network misclassify. The authors of this paper take it further. They argue that modifying the entire image is not required.\n\nInstead, they modify only a small portion of the image such that the modified image is predicted as the wrong class.\n\nFormally, for a given input x , the probability of x belonging to class t is f_x(). The task at hand is denoted in Equation 2.0.\n\nwhere, adv is the adversarial class to optimise for.\n\nHere, e(x) is the more interesting term. It is the adversarial data (similar to the one in the previous paper) that is added to the input. However, in this case, this e() has the following constraint:\n\nThis just means that the number of elements in the vector x has to be less than L , which is a tuneable parameter. (The ||0 means 0th norm, which is the number of non zero elements in a vector). The maximum value of the elements produced by e() is constrained , similar to the previous paper.\n\nThe previous paper used backprop in order to optimize for the right values of the adversarial input. In my opinion, giving access to the model\u2019s gradients is unfair , as its essentially possible to exactly know how the model \u2018thinks\u2019. Therefore optimizing for the adversarial inputs becomes easy.\n\nIn this paper, the authors decided to not use those. Instead they used Differential Evolution. It is a method that takes some samples, generates \u2018children\u2019 from samples. Then from those children, it only keeps the ones that are better than the parent samples. This method then goes on, and new children are generated.\n\nThis does not give any information about gradients, and finding the right values for the adversarial input can be done without having any knowledge of how the model works (it will even work if the model is not differentiable , unlike the previous method)\n\nThis are the results reported by the authors:\n\nFor CIFAR 10, the value of L was kept to 1, which means only one pixel was allowed to be modified. And the adversarial was able to really fool the classifier to predict very different classes."
    },
    {
        "url": "https://buzzrobot.com/machine-translation-without-the-data-21846fecc4c0?source=user_profile---------3----------------",
        "title": "Machine Translation Without the Data \u2013",
        "text": "If an auto-encoder is taught to reconstruct the input exactly the way it was fed to it, it may simply learn to do nothing at all. In this case, the outputs will be perfectly reconstructed, but we won\u2019t have any useful features in the bottleneck layer. To remedy this, de-noising auto-econders are used. First, the actual input is corrupted slightly by adding some noise to it. Then the network is made to reconstruct the original image (not the noisy version). This way, the network is forced to learn useful features of the image by learning what the noise is (and what the really useful features are).\n\nSimply put, the space in which the input ( now transformed by the encoder) lies in the bottleneck layer is known as the latent space.\n\nAuto-encoders are a broad class of neural networks that are used on unsupervised tasks. The idea is that they are made to re-create the same input that they have been fed. The key is that the network has a layer in the middle, called the bottleneck layer. This layer is supposed to capture all the interesting information about the input and throw away the useless information.\n\nIn the most abstract sense, the authors found out how to learn a latent space that is common between both the languages.\n\nThe authors of the paper figured out how to convert this task into an unsupervised task. In this task, the only data required would be two arbitrary corpora of each of the two languages, e.g. any fiction novel in English vs. any fiction novel in Spanish. Note that the two novels do not have to be the same.\n\nAs mentioned briefly above, the biggest problem with using neural networks in machine translation is that it requires a dataset of sentence pairs in both languages. It is available for widely spoken languages like English and French, but will not be available for other pairs. If the language pair data was available, this problem would be a supervised task.\n\nThis article loosely follows the structure of the paper. I added my own bits to explain the material to simplify it.\n\nOne of the tasks where deep networks excel is machine translation. They are currently the state-of-the-art in this task, and feasible enough that even Google Translate now uses them . In Machine translation, sentence-level parallel data is required to train the model, i.e. for every sentence in the source language there needs to be the translated language in the target language. It is not hard to imagine why this could be a problem. It is hard to get a large amount of data (so that the power of deep learning can be used) for some language pairs.\n\nDeep Learning is being aggressively used in day-to-day tasks. It especially excels in areas where there is a degree of \u2018humanness\u2019 involved, e.g. image recognition. Probably the most useful feature of Deep Networks, unlike other Machine Learning algorithms, is that their performance increases as it gets more data. So if it is possible to get more data, a performance increase can be expected.\n\nThe latent space captures the features of the data (in our case, the data is sentences). So if it was possible to learn a space that would produce the same features when language A was fed to it as when language B is fed to it, it would be possible to have a translation between them. Since the model has learned the right \u2018features\u2019, encoding from language A\u2019s encoder, and decoding using language B\u2019s decoder would effectively be asking it to do a translation.\n\nAs you may have guessed , the authors utilised denoising auto-encoders to learn a feature space. They also figured out how to make the auto encoder learn a common latent space (they call it an aligned latent space)in order to perform unsupervised machine translation.\n\nThe authors used a Denoising encoder to learn the features in an unsupervised manner. The loss defined by them is:\n\nl is the language(for this setup , there will be 2 possible languages) . x is the input. C(x) is just the result after adding noise to x. We will get to noise creating function C shortly. e() is the encoder, and d() is the decoder. The term at the end , with the \u0394(x hat ,x) is the sum of cross entropy errors at the token level. Since we have an input sequence , and we get an output sequence , we need to make sure that every token is in the right order. Therefore such a loss is used. It can be thought of a multi label classification , where the ith token in the input is compared with the ith token in the output. A token is a single unit which cannot be broken further. In our case, it is a single word.\n\nSo, Equation 1.0 is the loss that will make the network minimze the difference between the output of it(when given a noisy input), and the original , untouched sentence.\n\nThe \ud835\udd3c is the symbol for expectation. In this context, it means , the distribution of the inputs need to come from the language l, and the average of the loss is taken. It is just a mathematical formality , and the actual loss during implementation (sum of cross entropy) will be implemented as usual.\n\nThe ~ in particular , means \u201ccomes from a probability distribution of\u201d.\n\nI wont go into details here, but you can read about this notation in detail in Chapter 8.1 in the Deep Learning Book.\n\nUnlike images , where its just possible to add floating point numbers to pixels to add noise, adding noise to language needs to be different. Therefore, the authors developed their own system to create noise. They denote their noise function as C() . It takes in the input sentence, and outputs the noisy version of that sentence.\n\nThere are two different ways to add noise.\n\nFirst, it is possible to simply drop a word from the input with a probability of P_wd.\n\nSecondly, each word can shift from its original position with this constraint\n\nHere, \u03c3 means the shifted location of the ith token. So , Equation 2.0 means : \u201ca token can shift from its position at most k tokens to the left or to the right\u201d\n\nThe authors used a k value of 3 , and a P_wd value of .1\n\nIn order to learn to translate between two languages , there should be some process to map an input sentence(in language A) to an output sentence (in language B). The authors call this cross domain training. First, an input sentence (x) is sampled. Then the translated output(y) is produced by using the model(M()) from the previous iteration. Putting it together we have y = M(x). After that, y is corrupted using the same noise function C() described above ,giving C(y). The encoder of language A is made to encode this corrupted version, and the decoder of Language B is made to decode the output from Language A\u2019s encoder, and recreate a clean version of C(y) . The models are trained using the same sum of cross entropy error like in Equation 1.0.\n\nSo far , there has been no mention of how to learn the common latent space. The cross domain training mentioned above may somewhat help learn a space that is similar, but a stronger constraint to push the models to learn a similar latent space is required.\n\nThe authors used Adversarial Training. They used another model(called Discriminator) that takes the output of each of the encoders, and predict which language that encoded sentence belongs to. Then , the gradients from the discriminator are taken , and the encoder is also trained to fool the discriminator. This is conceptually no different than a standard GAN (Generative Adversarial Network). The Discriminator takes in the feature vector of each time step(because RNNs are used), and predicts which language it came from.\n\nThe 3 different losses(autoencoder loss, translation loss , and discriminator loss) mentioned above are added together , and all the model weights are updated in one step.\n\nSince this was a sequence to sequence problem , the authors used an LSTM network, with attention, i.e. there are two LSTM based autocoders , one for each language.\n\nAt a high level, there are 3 main steps to training this architecture. It follows an iterative training procedure. The training loop would look somewhat like this:\n\nNote that even though step 2 and 3 are listed separately, the weights are updated for both of them together.\n\nAs mentioned above, the model uses its own translation from the previous iteration to improve on its translation capabilities. Therefore, before the training loop begins, it is important to have some form of translation capability already. The authors used FastText , to learn word level bilingual dictionary. Note that this method is very naiive and required only to give the model a starting point.\n\nThe whole framework is given in the flowchart below"
    },
    {
        "url": "https://buzzrobot.com/using-t-sne-to-visualise-how-your-deep-model-thinks-4ba6da0c63a0?source=user_profile---------4----------------",
        "title": "Using T-SNE to Visualise how your Deep Model thinks",
        "text": "Deep Learning has given us a new way to think about things. Partially, the reason is that they can be made arbitrarily big, allowing them to have immense capacity, and they can be regularised, preventing overfitting. However, it is not very easy to understand how these models work. In this post, I will show a simple technique that allows somebody to somewhat see what the model is doing.\n\nI will go over the individual components, how to use them together, and at the end of the post, I will link to the source code to get started.\n\nNeural networks operate on vectors, which is a list of real numbers. E.g. A convolutional neural network passes the input through a series of convolutions, then are eventually passed through fully connected layers. At this point , the model has an idea of the high level features of the input , e.g. the \u2018eyes\u2019 , \u2018dogs\u2019 , \u2018faces\u2019 , etc.\n\nUnfortunately, it is not directly possible to interpret these fully connected layers. It is possible to visualise the convolution activations, but its harder to understand whats going on in the dense layers.\n\nThere are usually many hundreds of neurons in the fully connected layers. E.g. the VGG16 architecture contains 4096 neurons after the convolutions. These 4096 values can be considered as features mentioned above. If only there was a way to visualise these numbers!\n\nDimensionality reduction is a way to to reduce high dimensional features into lower dimensions, while trying to preserve the characteristics of the data. E.g. similar images will be closer together, and dissimilar images will be far away.\n\nI wont go into the mathematical details, but T-SNE is one such dimensionality reduction technique. It tries to keep the structure of the high dimensional data, and also reduce the dimensions. For visualizing these dimensions, we will reduce the dimensions to 3, so that they can be plotted in a 3d plot. You can read about T-SNE here.\n\nHere\u2019s an example of T-SNE. Note that in this case, the pixels were directly used as features , i.e. no neural network was used.\n\nNotice how it was able to separate it so well. There is a clear separation between the digits, and similar digits are clustered together.\n\nMNIST is a trivial task in 2017. How can we do the same thing for more \u2018real\u2019 images? The answer is Convolutional Neural Networks(CNN). We will take a few thousand images, and pass them through the InceptionV3 convolutional network. We will extract the outputs from just before the dense layers of the network, and use that as our dimensions for visualizing.\n\nFortunately for us, there is an excellent tool that is part of Tensorboard. You can play with it at projector.tensorflow.org , and find the source code here.\n\nThis is meant to be used as Tensorboard , but in my opinion, tensorboard is too cumbersome, and unless you are already using Tensorflow, it is too much of a hassle to use.\n\nThat is why we will use just the standalone version. The authors didnt seem to have written any docs about the standalone version, but it is fairly straightforward and I will guide you through the steps in this article.\n\nI have written a few wrappers to export data from the model. The project details will be included at the end of the article.\n\nI wanted to see how I can use Deep learning to help with understanding photos of my University archives.\n\nThus I chose to use InceptionV3 as a feature extractor to extract 2048 length vectors , that are the representation of the image.\n\nThen I plotted those features using T-SNE."
    },
    {
        "url": "https://hackernoon.com/dynamically-expandable-neural-networks-ce75ff2b69cf?source=user_profile---------5----------------",
        "title": "Dynamically Expandable Neural Networks \u2013",
        "text": "Neural networks can learn complicated representations fairly easily. However, there are some tasks where new data (or categories of data) is constantly changing. For example, you may train a network to recognize pictures of 8 different types of cats. But in the future, you may want to change that to 12 breeds. If the network has to keep learning new data over time, it is called a continual learning problem. This article talks about a very recent technique that attempts to constantly adapt to new data at a fraction of the cost of retraining entire models.\n\nThis article follows the original paper. I add my own bits and explain the material to simply it.\n\nContinual Learning just means being able to learn continuously over time. The data arrives in sequences over time, and the algorithm has to learn to be able predict that new data. Usually, techniques like transfer learning are used, where the model is trained on previous data, and some features are used from that model to learn new data. This is usually done to reduce the time required to train models from scratch. It is also used when the new data is sparse.\n\nThe simplest way to perform such learning is by constantly fine-tuning the model based on newer data. However, if the new task is very different from the old tasks, the model will not be able to perform well on that new task, as features from the old task are not useful, e.g. if a model that is trained on a million images of animals, it will probably not work very well if it is fine-tuned on images of cars. The features learned from animals won\u2019t be very useful when trying to detect cars.\n\nAnother problem is that after fine-tuning, the model may begin to perform the original task poorly (in this example, predicting animals). For example, the stripes on a zebra has a vastly different meaning than a striped T-shirt or a fence. Fine-tuning such a model will degrade its performance recognizing zebras.\n\nAt a very high level, the idea of Expanding Networks is very logical. Train a model, and if it cannot predict very well, increase its capacity to learn. If a new task arrives that is vastly different from an existing task, extract whatever useful information you can from the old model and train a new model.The authors used these logical ideas and developed techniques to make such a construct possible.\n\nThe authors introduce 3 different techniques to make such a framework possible. Each method will be discussed in detail , but at a very high level, they are:\n\nIn the above, figure t denotes task number. Thus, t-1 denotes the previous task, and t denotes the current task.\n\nThe simplest way to train a new model would be to train the entire model every time a new task arrives. However, because deep neural networks can get very large, this method will become very expensive.\n\nTo prevent such an issue, the authors present a novel technique. At the first task, the model is trained with L1 regularization. This ensures sparsity in the network, i.e. only some neurons are connected to other neurons. We will see why this is useful in a moment.\n\nThe W^t denotes the weights of the model at time t. In this case t =1. D_t denotes training data at time t. The right half of the equation, starting from \u03bc, is simply the L1 regularization term, and \u03bc is the regularization strength. L denotes the layers of the network from the first layer to the last. This regulation tries to make the weights of the model close (or equal) to zero. You can read about l1 and l2 regularization here.\n\nWhen the next task needs to be learned, a sparse linear classifier is fit on the last layer of the model, then the network is trained using:\n\nThis means the weights of all the layers except the last layer. All these layers (from the first to the last) are fixed, while just the newly added layer is optimized with the same l1 regularization to promote sparse connections.\n\nBuilding this sparse connection helps identify those neurons that are affected in the rest of the model! The finding is done using Breadth First Search, which is a very popular search algorithm. Then, only those weights can be updated, saving a lot of computation time, and weights that aren\u2019t connected won\u2019t be touched. This also helps prevent negative learning, where the performance on old tasks degrades.\n\nSelective retraining works for tasks that are highly relevant from older tasks. But when newer tasks have fairly different distributions, it will begin to fail. The authors use another technique to ensure that newer data can be represented by increasing the capacity of the network. They do so by adding additional neurons. Their method will be discussed in detail here.\n\nSuppose that you wish to expand the Lth layer of a network by k neurons. The new weight matrix for that layer (and the previous layer ) will look have dimensions:\n\n\ud835\udca9 is the total number of neurons after adding the k neurons.\n\nUsually, we don\u2019t want to add k neurons. Instead, we would like the network to figure out the right number of neurons to add. Fortunately, there is already an existing technique that uses Lasso to regularize a network to have sparse weights (which can then be removed). This technique is described in detail in the paper Group Sparse Regularization for Deep Neural Networks.\n\nI won\u2019t go into detail here, but using that on a layer gives such results (Group Lasso is the technique that was used):\n\nThe authors used a layer basis (only on the newly added k neurons) instead of the entire network. The technique was used to nullify as many connections as possible, and keeping only the most relevant ones. Those neurons were then removed, making the model compact.\n\nThere is a common problem in transfer learning called semantic drift, or catastrophic forgetting, where the model slowly shifts its weights so much that it forgets about the original tasks.\n\nAlthough it is possible to add L2 regularization, which ensures that the weights don\u2019t shift dramatically, it won\u2019t help if the new tasks are very different (the model will just fail to learn after a certain point).\n\nInstead, it is better to duplicate the neurons if they shift beyond a certain range. If the value of a neuron changes beyond a certain value, a copy of the neuron is made, and a split occurs, and that duplicate unit is added as a copy to that same layer.\n\nSpecifically, for a hidden unit i, if the l2 distance between the new weight and the old weight ( \u03c1_i) is > \ud835\udf0e, then the split is made. \ud835\udf0e is a hyperparameter. After the split, the entire network will need to be trained again, but the convergence is fast because the initialization is not random, but has a reasonably optimal value.\n\nThree datasets were used. They are, namely:\n\nTo compare performance, a variety of models were used. They are:"
    },
    {
        "url": "https://buzzrobot.com/5-ways-to-get-started-with-reinforcement-learning-b96d1989c575?source=user_profile---------6----------------",
        "title": "5 Ways to Get Started with Reinforcement Learning \u2013",
        "text": "Machine learning algorithms, and neural networks in particular, are considered to be the cause of a new AI \u2018revolution\u2019. In this article I will introduce the concept of reinforcement learning but with limited technical details so that readers with a variety of backgrounds can understand the essence of the technique, its capabilities and limitations.\n\nAt the end of the article, I will provide links to a few resources for implementing RL.\n\nBroadly speaking, data-driven algorithms can be categorized into three types: Supervised, Unsupervised, and Reinforcement learning.\n\nThe first two are generally used to perform tasks such as image classification, detection, etc. While their accuracy is remarkable, these tasks differ from those that we would expect from an \u2018intelligent\u2019 being.\n\nThis is where reinforcement learning comes in. The concept itself is very simple, and much like our evolutionary process: the environment rewards the agent for things that it gets right and penalizes it for things that it gets wrong. The main challenge is developing the capacity to learn several million possible ways of doing things.\n\nQ learning is a widely used reinforcement learning algorithm. Without going into the detailed math, the given quality of an action is determined by what state the agent is in. The agent usually performs the action which gives it the maximum reward. The detailed math can be found here.\n\nIn this algorithm, the agent learns the quality(Q value) of each action (action is also called policy) based on how much reward the environment gave it. The value of each environment\u2019s state, along with the Q value is usually stored in a table. As the agent interacts with the environment, the Q values get updated from random values to values that actually help maximize reward.\n\nThe problem with using Q learning with tables is that it doesn\u2019t scale well. If the number of states is too high, the table will not fit in memory. This is where Deep Q learning could be applied. Deep learning is basically just a universal approximation machine which can understand and come up with abstract representations. Deep learning can be used to approximate Q values, and it can also easily learn optimal Q values by using gradient descent.\n\nIt is often the case that the agent memorizes one path and will never try to explore any other paths. In general, we would like an agent to not only exploit good paths, but also sometimes explore new paths that it can perform actions in. Therefore, a hyper-parameter, named \u03b5, is used to govern how much to explore new paths vs how much to exploit old paths.\n\nWhen training a neural network, data imbalance plays a very important role. If a model is trained as the agent interacts with the environment, there will be imbalances. The most recent play will obviously have more bearing than older plays.\n\nTherefore, all the states, along with related data, is stored in the memory, and the neural network can randomly pick a batch of some interactions and learn (this makes it very similar to supervised learning).\n\nThis is what the whole framework for deep Q learning looks like. Note the \ud835\udefe. This represents the discounted reward. It is a hyperparameter that controls how much weight the future reward will have. The symbol\u02ca denotes next. e.g. s\u02ca denotes next state."
    },
    {
        "url": "https://hackernoon.com/colorising-black-white-photos-using-deep-learning-4da22a05f531?source=user_profile---------7----------------",
        "title": "Colorising Black & White Photos using Deep Learning",
        "text": "Previous works have used deep learning. They used regression to predict the colour of each pixel. This , however, produces fairly bland and dull results.\n\nPrevious works used Mean Squared Error (MSE) as the loss function to train the model. The authors noted that MSE will try to \u2018average\u2019 out the colors in order to get the least average error, which will result in a bland look. The authors instead pose the task of colorising pictures as a classification problem.\n\nThe authors used the LAB colour space (the most common color space is RGB). In the LAB scheme, the L channel records the light intensity value, and the other two channels record the color opponents green\u2013red and blue\u2013yellow respectively. You can read about LAB in detail here.\n\nOne good reason to use LAB color space is that it keeps the light intensity values separate. B/W pictures can be considered to be just the L channel, and the model wont have to learn how to keep light intensities right when it makes predictions (it will have to do that if RGB is used). The model will only learn how to colour images, allowing it to focus at what matters.\n\nThe model outputs the AB values, which can then be applied to the B/W image to get the coloured version.\n\nThe model itself is a fairly standard convolutional neural network. The authors did not use any pooling layers, and instead chose to use upsampling/downsampling layers.\n\nAs briefly mentioned above, the authors used a classification model instead of a regression one. Therefore, the number of classes need to be fixed. The authors chose 313 AB pairs as the number of classes. Even though this may seem like a very low value, they used methods to ensure more color values are possible (which will be discussed later in this post).\n\nThe Loss function that the authors used was the standard Cross Entropy. Z is the actual class of a pixel, while Z hat is the output of the model.\n\nThe authors also noted that there would be class imbalances for the colour values. Cross entropy is a loss function that does not play very well with class imbalances, and usually classes that have fewer examples are given a higher weight. The authors noted that desaturated colors like gray and light blue are abundant compared to others, because of their appearance in backgrounds. Therefore they came up with their weighing scheme.\n\nThe authors calculate ~p , which is the distribution of classes, from the ImageNet database. Remember that Q is the number of classes (313). The authors used \u03bb value of .5 worked well. Note that the authors smoothened the distribution ~p , but I will skip the details here. If you are interested, you can read it in the original paper.\n\nSo after taking into account the weight, the final loss function looks like:\n\nThe new term v() is just the value of the weight for each of the classes. h and w are the height and width of the image , respectively.\n\nUsing a class number of 313 directly to color images would be too coarse. There are simply too few colours to realistically represent the real range of colors.\n\nThe authors used a post processing step in order to get a more diverse colour range from the model\u2019s predictions.\n\nH is a function , and Z is the output of the model. T is a hyper-parameter that the authors experimented with a few different values."
    },
    {
        "url": "https://hackernoon.com/using-ai-to-super-compress-images-5a948cf09489?source=user_profile---------8----------------",
        "title": "Using AI to Super Compress Images \u2013",
        "text": "Data driven algorithms like neural networks have taken the world by storm. Their recent surge is due to several factors, including cheap and powerful hardware, and vast amounts of data. Neural Networks are currently the state of the art when it comes to \u2018cognitive\u2019 tasks like image recognition, natural language understanding , etc. ,but they don\u2019t have to be limited to such tasks. In this post I will discuss a way to compress images using Neural Networks to achieve state of the art performance in image compression , at a considerably faster speed. This article is based on An End-to-End Compression Framework Based on Convolutional Neural Networks This article assumes some familiarity with neural networks , including convolutions and loss functions. Image compression is the process of converting an image so that it occupies less space. Simply storing the images would take up a lot of space, so there are codecs, such as JPEG and PNG that aim to reduce the size of the original image. There are two types of image compression :Lossless and Lossy. As their names suggest, in Lossless compression, it is possible to get back all the data of the original image, while in Lossy, some of the data is lost during the convsersion. E.g. JPG is a lossy algorithm, while PNG is a lossless algorithm\n\nNotice the image on the right has many blocky artifiacts. This is how the information is getting lost. Nearby pixels of similar colors are compressed as one area, saving space, but also losing the information about the actual pixels. Of course, the actual algorithms that codecs like JGEG , PNG etc use are much more sophisticated, but this is a good intuitive example of lossy compression. Losless is good, but it ends up taking a lot of space on disk. There are better ways to compress images without losing much information, but they are quite slow, and many use iterative approaches, which means they cannot be run in parallel over multiple CPU cores , or GPUs. This renders them quite impractical in everyday usage. If anything needs to be computed and it can be approximated , throw a neural network at it. The authors used a fairly standard Convolutional Neural Network to improve image compression. Their method not only performs at par with the \u2018better ways\u2019 (if not even better), it can also leverage parallel computing, resulting in a dramatic speed increase. The reasoning behind it is that convolution neural networks(CNN) are very good at extracting spatial information from images, which are then represented in a more compact form (e.g. only the \u2018important\u2019 bits of an image are stored). The authors wanted to leverage this capability of CNNs to be able to better represent images. The authors proposed a dual network. The first network , which will take the image and generate a compact representation(ComCNN). The output of this network will then be processed by a standard codec (e.g. JPEG). After going through the codec, the image will be passed to a 2nd network, which will \u2018fix\u2019 the image from the codec, trying to get back the original image. The authors called it Reconstructive CNN (RecCNN). Both networks are iteratively trained, similar to a GAN. Figure 2.0 : ComCNN The compact representation is passed on to a standard codec Figure 2.1 RecCNN. The output from ComCNN is upscaled and passed to RecCNN, which will attempt to learn a residual The output from the codec is upscaled , then passed to RecCNN. The RecCNN will try to output an image that looks as similar to original image as possible.\n\nWhat is the residual? The residual can be thought of as a post processing step to \u2018improve\u2019 the image that the codec decodes. The neural network, which has a lot of \u2018information\u2019 about the world, can make cognitive decisions about what to \u2018fix\u2019. This idea is based on residual learning , and you can read about it in depth here. Since there are two networks, two loss functions are used. The first one, for ComCNN, labelled as L1 is defined as: This equation may look complicated, but it is actually the standard (Mean Squared Error)MSE. The ||\u00b2s signify the \u2018norm\u2019 of the vector that they enclose. Cr denotes the output of the ComCNN. \u03b8 denotes the trainable parameters of ComCNN, and Xk denotes the input image Re() denotes the RecCNN. This equation just passes the value of Equation 1.1 to the RecCNN. \u03b8 hat denotes the trainable parameters of RecCNN (the hat denotes that the parameters are fixed) Equation 1.0 will make ComCNN modify its weights such that , after being recreated by RecCNN, the final image will look as close to the real input image as possible."
    },
    {
        "url": "https://hackernoon.com/facial-similarity-with-siamese-networks-in-pytorch-9642aa9db2f7?source=user_profile---------9----------------",
        "title": "Facial Similarity with Siamese Networks in PyTorch \u2013",
        "text": "In the last article discussed the class of problems that one shot learning aims to solve, and how siamese networks are a good candidate for such problems. We went over a special loss function that calculates similarity of two images in a pair. We will now implement all that we discussed previously in PyTorch.\n\nYou can find the full code as a Jupyter Notebook at the end of this article.\n\nWe will use a standard convolutional neural network architecture. We use batch normalisation after each convolution layer, followed by dropout.\n\nThere is nothing special about this network. It accepts an input of 100px*100px and has 3 full connected layers after the convolution layers.\n\nIn the previous post, I showed how a pair of networks process each image in a pair. But in this post, there is just one network. Because the weights are constrained to be identical for both networks, we use one model and feed it two images in succession. After that we calculate the loss value using both the images, and then back propagate. This saves a lot of memory at absolute no hit on other metrics(like accuracy).\n\nWe defined contrastive loss to be\n\nAnd we defined Dw(which is just the euclidean distance)as :\n\nGw is the output of our network for one image.\n\nThe contrastive loss in PyTorch looks like this:\n\nIn the previous post I wanted to use MNIST, but some readers suggested I instead use the facial similarity example I discussed in the same post. Therefore I switched from MNIST/OmniGlot to the AT&T faces dataset.\n\nThe dataset contains images of 40 subjects from various angles. I put aside the last 3 subjects from training to test our model."
    },
    {
        "url": "https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e?source=user_profile---------10----------------",
        "title": "One Shot Learning with Siamese Networks in PyTorch \u2013",
        "text": "Deep neural networks are the go to algorithm when it comes to image classification. This is partly because they can have arbitrarily large number of trainable parameters. However, this comes at a cost of requiring a large amount of data, which is sometimes not available. I will discuss One Shot Learning, which aims to mitigate such an issue, and how to implement a Neural Net capable of using it ,in PyTorch.\n\nThis article assumes some familiarity with neural networks.\n\nThis is a two part article. I will go through the theory in Part 1 , and the PyTorch implementation of the theory in Part 2.\n\nThis article takes cues from this paper.\n\nStandard classification is what nearly all classification models use. The input is fed into a series of layers, and in the end , the class probabilities are output. If you want to predict dogs from cats, you train the model on similar(but not same) dogs/cats pictures that you would expect during prediction time. Naturally, this requires that you have a dataset that is similar to what you would expect once you use the model for prediction.\n\nOne Shot Classification models, on the other hand, requires that you have just one training example of each class you want to predict on. The model is still trained on several instances, but they only have to be in the similar domain as your training example.\n\nA nice example would be facial recognition. You would train a One Shot classification model on a dataset that contains various angles , lighting , etc. of a few people. Then if you want to recognise if a person X is in an image, you take one single photo of that person, and then ask the model if that person is in the that image(note, the model was not trained using any pictures of person X).\n\nAs humans, we can recognize a person by his/her face by just meeting them once, and it is desirable by computers because many times data is at a minimum."
    },
    {
        "url": "https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643?source=user_profile---------11----------------",
        "title": "DeepMind\u2019s Relational Networks \u2014 Demystified \u2013",
        "text": "Every time DeepMind publishes a new paper, there is frenzied media coverage around it. Often you will read phrases that are often misleading. For example, its new paper on relational reasoning networks has futurism reporting it like\n\nThis is not only misleading, but it also makes the everyday non PhD person intimidated. In this post I will go through the paper in an attempt to explain this new architecture in simple terms.\n\nYou can find the original paper here.\n\nThis article assumes some basic knowledge about neural networks.\n\nI will follow the paper\u2019s structure as much as possible. I will add my own bits to simply the material.\n\nIn its simplest form, Relational Reasoning is learning to understand relations between different objects(ideas). This is considered an essential characteristic of intelligence. The authors have included a helpful infographic to explain what it is\n\nThe authors have presented a neural network that is made to inherently capture relations(e.g. Convolutional Neural networks are made to capture properties of images). They presented an architecture that is defined like so :\n\nThe Relational Network for O (O is the set of objects you want to learn relations of) is a function f\u0278.\n\ng\u03b8 is another function that takes two objects :oi , and oj. The output of g\u03b8 is the \u2018relation\u2019 that we are concerned about.\n\n\u03a3 i,j means , calculate g\u03b8 for all possible pairs of objects, and then sum them up.\n\nIt is easy to forget this when learning about neural networks, backprop ,etc. but a neural network is in fact a single mathematical function! Therefore, the function that I described in Equation 1.0 is a neural network!. More precisely , there are two neural networks:\n\nBoth g\u03b8 , and f\u0278 are multi layer perceptrons in the simplest case.\n\nThe authors present Relational Neural Network as a module. It can accept encoded objects and learn relations from them, but more importantly, they can be plugged into Convolutional Neural networks , and Long Short Term Memory Networks (LSTM).\n\nThe Convolutional network can be used to learn the objects using images. This makes it far more useful for applications because reasoning on an image is more useful than reasoning on an array of user defined objects.\n\nThe LSTMs along with word embeddings can be used to understand the meaning of the query that the model has been asked. This is again , more useful because the model can now accept an English sentence instead of encoded arrays.\n\nThe authors have presented a way to combine relational networks, convolutional networks , and LSTMs to construct an end to end neural network that can learn relations between objects."
    },
    {
        "url": "https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047?source=user_profile---------12----------------",
        "title": "Train your deep model faster and sharper \u2014 two novel techniques",
        "text": "The authors of this paper propose a method to increase training speed by freezing layers. They experiment with a few different ways of freezing the layers, and demonstrate the training speed up with little(or none) effect on accuracy.\n\nFreezing a layer prevents its weights from being modified. This technique is often used in transfer learning, where the base model(trained on some other dataset)is frozen.\n\nIf you dont want to modify the weights of a layer, the backward pass to that layer can be completely avoided, resulting in a significant speed boost. For e.g. if half your model is frozen, and you try to train the model, it will take about half the time compared to a fully trainable model.\n\nOn the other hand, you still need to train the model, so if you freeze it too early, it will give inaccurate predictions.\n\nThe authors demonstrated a way to freeze the layers one by one as soon as possible, resulting in fewer and fewer backward passes, which in turn lowers training time.\n\nAt first, the entire model is trainable (exactly like a regular model). After a few iterations the first layer is frozen, and the rest of the model is continued to train. After another few iterations , the next layer is frozen, and so on.\n\nThe authors used learning rate annealing to govern the learning rate of the model. The notably different technique they used was that they changed the learning rate layer by layer instead of the whole model. They used the following equation:\n\nThe sub i denotes the ith layer. So \u03b1 sub i denotes the learning rate for the ith layer. Similarly , t sub i denotes the number of iterations the ith layer has been trained on. t denotes the total number of iterations for the whole model.\n\nThis denotes the initial learning rate for the ith layer.\n\nThe authors experimented with different values for Equation 2.1\n\nThe authors tried scaling the initial learning rate so that each layer was trained for an equal amount of time.\n\nRemember that because the first layer of the model would be stopped first, it would be otherwise trained for the least amount of time. To remedy that, they scaled the the learning rate for each layer."
    },
    {
        "url": "https://hackernoon.com/can-creative-adversarial-network-explained-1e31aea1dfe8?source=user_profile---------13----------------",
        "title": "CAN (Creative Adversarial Network) \u2014 Explained \u2013",
        "text": "The authors propose a modified GAN to generate creative content. They propose sending an additional signal to the generator to prevent it from generating content that is too similar to existing content. How did they do it? They modified the oritinal GAN loss function from Equation 1.4.\n\nIn the original GAN, the generator modifies its weights based on the discriminator\u2019s output of wether or not what it generated was able to fool the discriminator. CAN extends this in two ways:\n\nThe original problem of GAN was they would not explore new work. Their objective is literally to just make their data look like it came from real dataset.\n\nBy having an additional metric which classifies the time period the data belongs to(along with the confidence), the generator is now getting feedback on how similar it\u2019s creation looks to some time period.\n\nNow, the generator not only has to make its data look similar to dataset, but also make sure it doesn\u2019t look too similar to a single category. This will allow it to prevent creating artwork that has very specific characteristics.\n\nThe new loss function is:\n\nThe first line is exactly the same as the original equation. Note that the subscript r means the discriminator\u2019s output of real/fake, and the subscript c is the output of the discriminator\u2019s classsification. \n\nThe 2nd line is the modification for promoting creativitity. I will explain it step by step.\n\nThis is the Discriminator getting the class of the input image correctly. The Discriminator will try to maximize this value. We want the discriminator to classify the images correctly.\n\nThis may look complicated , but this is just the Multi Label Cross Entropy Loss.Note that K here denotes the number of classes. You can find the detailed information about losses here. This is the same loss that classifiers use as a loss function. The generator will try to minimize this value in order to maximise Equation 2.0.\n\nThe way that Equation 2.2 works is , if the value of one of the classes score approaches 1 or 0 , the value of the whole equation approaches -infinity. The largest possible value(larger is what the generator wants) that Equation 2.2 can take is when the discriminator is completely unsure about what class the input belongs to, i.e. every term in the summation has the same value. This makes sense because its not possible to properly classify the input image into existing classes, so it must mean that it is its own new class.\n\nThis paper talks about a loss function that pushes a GAN into exploring new content based on what is given to it. This was done by modifying the loss function to allow for exploration.\n\nThis was my first technical post. Criticism and improvement tips are welcome and greatly appreciated.\n\nIf you learned something useful from my article, please share it with others by tapping on the \u2764. It lets me know I was of help."
    }
]