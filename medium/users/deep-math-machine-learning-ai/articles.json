[
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-11-chatbots-to-question-answer-systems-e06c648ac22a?source=---------0",
        "title": "Chapter 11: ChatBots to Question & Answer systems. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "I am really excited to write this story , so far I have talked about Machine learning,deep learning,Math and programming and I am sick of it.\n\nNow I wanna talk about simple things and also some research level stuff( AI Research in NLP) cause Natural language processing(NLP) is one of most complex problems in AI and it has long long long way to go.\n\nThere is a misconception by a lot of people , they think that Chatbots and Q&A systems are same or similar.\n\nbut in reality they are not ,they follow complete different approaches, methods, algorithms and models. except the input and output which is a text (I will definitely prove it to you by the EOS ).\n\nChatbots can have more functionalities( depends on the problem ) than Q&A systems and very easy to build now a days using tools like Diaglogflow.ai , wit.ai , LUIS , amazon lex and IBM watson and etc..\n\nbut they can\u2019t be true intelligent, they just get the work done for the problem you take.\n\nA Q&A system requires huge amount of data and expertise, still very hard to implement a system.\n\nEx : if you are building a chat bot using IBM watson, it might not have much intelligence(No offence IBM, infact all the frameworks are similar) but if you try to understand the technology ,methods the IBM watson used to win the game called jeopardy , you will be amazed.\n\nhere IBM watson is one of the best Q&A systems ( I am not talking about chatbot here cause for the chatbot, IBM watson is normal like any other tool).\n\nI hope it is not confusing , even if it is.. you will get the idea as you read through.\n\nA Chatbot known as an Conversational agent is a service either powered by rules or artificial intelligence(little) that you interact via a chat interface.\n\nEx: A weather bot ( you can ask all weather questions ), news bot(a bot for those who like to keep up with the news daily) , Cricketscore bot and etc\u2026\n\nBefore we dive into the topic, lemme just give an idea where you can build the bots and how.\n\nJust observe the below picture, the 2X2 grid says it all.\n\nAs the name says it retrieves the answers/responses from a set of predefined responses and some kind of heuristic to pick an appropriate response based on the input and context. The heuristic could be as simple as a rule-based expression match, or as complex as an ensemble of Machine Learning classifiers.\n\n1 . No grammtical or meaning less errors as we store the answers\n\n2. Works 100% well for the business problems and customer satisfaction and attention can be gained\n\n3. Super easy to build these models as we don\u2019t require huge data.\n\n2. A lot of hard coded rules have to be written so not much intelligent.\n\nthese models don\u2019t rely on pre-defined responses. They generate new responses from scratch. Generative models are typically based on Machine Translation techniques, but instead of translating from one language to another, we \u201ctranslate\u201d from an input to an output (response).\n\nit uses sequence to sequence models for generating the text ( we will implement these also in the next stories)\n\nopen domain is the place where the chat conversation can go anywhere, users can type/ask anything.There isn\u2019t necessarily have a well-defined goal or intention.\n\nthe chatbot mitsuku is the example for this.\n\nthe convo can go into all kinds of directions. The infinite number of topics and the fact that a certain amount of world knowledge is required to create reasonable responses makes this a hard problem.\n\nclosed domain is the place where you are solving a particular business problem ( The bussiness could be in any sector/industry )\n\nclosed domain bots focus on one particular sector or industry.\n\nso you can\u2019t ask questions like \u201chow is the weather now??\u201d, \u201cwhat is the score for IndVsPak match today?\u201d when you dealing with a banking bot or pizza bot.\n\nsimilarly you can\u2019t ask pizza bot a banking query. if you ask , you will get a decent answer \u201cI am sorry I don\u2019t understand\u201d.\n\nThe closed domain bots have the limited functionalities/ services based on the business problem.\n\nNote: In this story I only focus on the closed domains bots and I hope you get a picture about the chatbot architecture.\n\nNow lets get continued with the chatbots.\n\nA chat bot typically has 3 things in it\n\nEx: what\u2019s the weather in Seattle tomorrow??\n\nResponse \u2192 \u201cThe weather in {Location} {Date} is so and so\u201d\n\nThe chat bot has always canned responses depending upon the problem/service you provide.\n\nNote : NLP is hard at this moment. Computers started generating text with the help of deep learning recently so it can\u2019t produce a meaning full response so Chat bots always have canned responses ( For User/Customer services )\n\nif you are not a programmer, there are a lot of chatbots frameworks where you can build a bot very easily without coding.\n\nThere are dozes of frameworks out there you can use any to build a bot for your business.\n\nif you are a programmer,have a little experience with Machine learning and wanna build a chat bot for your services, you can use the following tools.\n\nThese tools backed by big companies like Google Dialogflow, Amazon Lex , Microsoft LUIS, Facebook Wit and IBM watson and perfectly works for small products/ services. (There are even more open source tools available on internet).\n\nThese services are mostly on cloud , through API\u2019s you can access the results and the entire piece of code is a blackbox to you (You don\u2019t have to know what machine learning algorithm is but you can pretty much easily use/train them according to your requirements)\n\nYou just need to need how API works , that\u2019s it you can use these tools.\n\nEvery tool has their own advantages and disadvantages but all of them are mostly similar, one uses these tools based on the requirements.\n\nas far as my experience is concerned, IBM watson is the safe and the best one. (actually wit.ai provides better NLU capabilities but it sometimes changes the entity values especially number values and date time values based on some training data patterns #rarecase but if you are dealing with bunch of numbers in your input data )\n\nif you have some idea on machine learning and NLP , I don\u2019t recommend you to use these tools as you can\u2019t customize / tune the model here plus your data is out of your hands.\n\nif you are an absolute beginner or not having much knowledge on machine learning, I suggest you to play with these tools and get some understanding, you can build bots easily then after.\n\nif you spend an hour on internet/ youtube , you can pretty much understand these tools and be able to use these tools. Trust me Very easy!\n\n1 . First of all we need to have the clear idea about what problem we are solving ( this is the most important part, 90 % people fail here as far as my experience is concerned for last 2 years.)\n\nso we have to prepare the conversational flows first.\n\nso what is a conversational flow????\n\nA conversational flow is a flow designed by us to drive the user to consume our services.\n\nlet\u2019s say we have a set of services say 10, so we need to have 10 different conversational flows (Remember we want customers/users to consume our services so we need to manipulate users to control the flow of the conversation)\n\nin other words, we need to fully take the control of the conversation.\n\nI will explain in detail as we go. #keepcalm.\n\n2 . Identify the intents , entities and responses of all.\n\n3 . Collect the training data to build the ml model.\n\n4. build the model (cloud platforms or customized tools ) and start coding based on the requirements.\n\nDon\u2019t worry! I will share the practical knowledge also.\n\nLet\u2019s take a simple problem, Let\u2019s say I open a Pizza shop (Mady\u2019s Pizza house) my ultimate goal is to sell pizza\u2019s and gain more customers so I want to build a bot for my customers for 3 services.\n\nThese flows should be developed in the way that you want to drive the users.\n\nThese are designed by me #justanexample.\n\nif you observe the above images closely , you find an interesting point.\n\nhere, we are actually controlling the user\u2019s actions and giving the responses according to the input ( that\u2019s the important part in building chatbots )\n\nI am not just giving an answer to the user , I am actually understanding the user and controlling the conversation so the user really feels like he/she is having a natural converstion.\n\nOkay Step1 is done #uffffff. remaining things are super easy\n\npizza type \u2192 Veg or Non veg or both\n\nNote: These are just for sake of understanding, I hope you see from that perceptive.\n\nStep 3: Prepare the training data for getting intents and entities."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235?source=---------1",
        "title": "Chapter 10.1: DeepNLP \u2014 LSTM (Long Short Term Memory) Networks with Math.",
        "text": "Note: I am writing this article with the assumption that you know the deep learning a bit. In case if you don\u2019t know much, Please read my earlier stories to understand the entire series on deep learning.\n\nIn the last story we talked about Recurrent neural networks, so we now know what RNN\u2019s are, How they work and what kind of problems it can solve and also we talked about a limitation in RNN\u2019s which is\n\nWe all know that a neural network uses an algorithm called BackPropagation to update the weights of the network. So what BP does is\n\nIt first calculates the gradients from the error using the chain rule in Calculus, then it updates the weights(Gradient descent).\n\nsince the BP starts from the output layer to all the way back to input layer , In a simple neural network we may not face problems with updating weights but in a deep neural network we might face some issues.\n\nAs we go back with the gradients , It is possible that the values get either smaller exponentially which causes Vanishing Gradient problem or larger exponentially which causes Exploding Gradient problem.\n\nDue to this we get the problems of training the network.\n\nIn RNN\u2019s, we have time steps and current time step value depends on the previous time step so we need to go all the way back to make an update.\n\nThere are couple of remedies there to avoid this problem.\n\nWe can use ReLu unit as an activation function, RMS Prop as an optimization algorithm and LSTM\u2019s or GRU\u2019s.\n\nLSTM ( Long Short Term Memory ) Networks are called fancy recurrent neural networks with some additional features.\n\nJust like RNN, we have time steps in LSTM but we have extra piece of information which is called \u201cMEMORY\u201d in LSTM cell for every time step.\n\nSo the LSTM cell contains the following components\n\nHere is the diagram for LSTM cell at the time step t\n\nDon\u2019t panic I will explain every single hecking detail of it. Just get the overall picture stored in your brain.\n\nLemme take only one time step (t) and explain it.\n\nInputs to the LSTM cell at any step are X (current input) , H ( previous hidden state ) and C ( previous memory state)\n\nOutputs from the LSTM cell are H ( current hidden state ) and C ( current memory state)\n\nHere is the diagram for a LSTM cell at T time step.\n\nIf you observe carefully,the above diagram explains it all.\n\nAnyway, lemme also try with words\n\nare single layered neural networks with the Sigmoid activation function except candidate layer( it takes Tanh as activation function)\n\nThese gates first take input vector.dot(U) and previous hidden state.dot(W) then concatenate them and apply activation function\n\nfinally these gate produce vectors ( between 0 and 1 for Sigmoid, -1 to 1 for Tanh) so we get four vectors f, C`, I, O for every time step.\n\nNow let me tell you an important piece called Memory state C\n\nThis is the state where the memory (context) of input is stored\n\nEx : Mady walks in to the room, Monica also walks in to the room. Mady Said \u201chi\u201d to ____??\n\nInorder to predict correctly Here it stores \u201cMonica\u201d into memory C.\n\nThis state can be modified. I mean LSTM cell can add /remove the information.\n\nEx : Mady and Monica walk in to the room together , later Richard walks in to the room. Mady Said \u201chi\u201d to ____??\n\nThe assumption I am making is memory might change from Monica to Richard.\n\nI hope you get the idea.\n\nso LSTM cell takes the previous memory state Ct-1 and does element wise multiplication with forget gate (f)\n\nif forget gate value is 0 then previous memory state is completely forgotten\n\nif forget gate value is 1 then previous memory state is completely passed to the cell ( Remember f gate gives values between 0 and 1 )\n\nNow with current memory state Ct we calculate new memory state from input state and C layer.\n\nCt = Current memory state at time step t. and it gets passed to next time step.\n\nHere is flow diagram for Ct\n\nFinally, we need to calculate what we\u2019re going to output. This output will be based on our cell state Ct but will be a filtered version. so we apply Tanh to Ct then we do element wise multiplication with the output gate O, That will be our current hidden state Ht\n\nWe pass these two Ct and Ht to the next time step and repeat the same process.\n\nHere is the full diagram for LSTM for different time steps."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-10-deepnlp-recurrent-neural-networks-with-math-c4a6846a50a2?source=---------2",
        "title": "Chapter 10: DeepNLP - Recurrent Neural Networks with Math.",
        "text": "we talked about normal neural networks quite a bit, Let\u2019s talk about fancy neural networks called recurrent neural networks.\n\nBefore we talk about what exactly RNN\u2019s are, let me first put this \u201cWhy RNN\u2019s ???\u201d (I am a big fan of Simon sinek, so I start with why.)\n\nA neural network usually takes an independent variable X (or a set of independent variables ) and a dependent variable y then it learns the mapping between X and y (we call this Training), Once training is done , we give a new independent variable to predict the dependent variable.\n\ninfact that\u2019s all most of machine learning(supervised).\n\nbut what if the order of data matters????? just imagine what if the order of all independent variables matter???\n\nLet me explain visually (I call this The RecurAnt Theory).\n\nJust assume every ant is an independent variable if one ant goes in a different direction , it does not matter for other ants right?\n\nBut what if the order of the ants matters ?\n\nif one ant misses or turns away from the group, it affects the following ants.\n\nSo a normal neural network does not follow the order, so when we tackle the real world problems where the order matters , we need Recurrent neural networks. Period.\n\nSo which data where the order matter in our ML space ????\n\nso how RNN\u2019s solve \u201cthe whole order matters thing\u201d data??????\n\nLet\u2019s say i am doing sentiment analysis on user reviews on a movie\n\nWe can classify these by using simple model \u201cBag of words\u201d and we can predict (Positive or Negative) but wait\u2026\n\nwhat if the review is \u201cThis movie is not good\u201d\n\nThe BOW model may say it\u2019s a positive sign but actually it\u2019s not.\n\nThe RNN understands it and predicts that it\u2019s negative.\n\nFirst let\u2019s admit that here the order of the text matters. cool? okay\n\nRNN has the following models\n\nRNN takes one input lets say an image and generates a sequence of words.\n\nRNN takes sequence of words as input and generates one output.\n\nRNN takes sequence of words as input and generates sequence of words as output. (lets say language translations).\n\nCurrently we are focusing on 2nd model \u201cMany to One\u201d.\n\nin RNN\u2019s Input is considered as time steps.\n\nTime stamp for \u201cthis\u201d is x(0), \u201cmovie\u201d is x(1), \u201cis\u201d is x(2) ,\u201cnot\u201d is x(3) and \u201cgood\u201d is x(4).\n\nFirst let\u2019s understand what RNN cell contains!\n\nI hope and assume you know Feed Forward NN or you can read my earlier story here NN. Summary of FFNN is\n\nIn Feed forward neural network we have X(input) and H(Hidden) and y(output)\n\nyou can have as many hidden layers as you want but weights (W)for every hidden layers are different.\n\nAbove Wh1 and Wh2 are different.\n\nThe RNN cell contains a set of feed forward neural networks cause we have time steps.\n\nThe RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers.\n\nUnlike FFNN , here we calculate hidden layer values not only from input values but also previous time step values and Weights ( W ) at hidden layers are same for time steps.\n\nHere is the complete picture for RNN and it\u2019s Math.\n\nIn the picture we are calculating the Hidden layer time step (t) values so\n\nHt-1 is the previous time step and as i said W\u2019s are same for all timesteps.\n\nThe activation function can be Tanh, Relu, Sigmoid, etc..\n\nAbove we calculated only for Ht similarly we can calculate for all other timesteps.\n\n1.U and V are weight vectors, different for every time step.\n\n2.We can even calculate hidden layer( all time steps ) first then calculate y values.\n\nOnce Feed forwarding is done then we need to calculate the error and backpropagate the error using back propagation.\n\nwe use Cross entropy as cost function ( assume you know so not going into details)\n\nif you know how Normal neural network works , the rest is pretty easy , if you don\u2019t know, here is my article that talks about Artificial Neural Networks.\n\nWe need to calculate the below terms\n\nSince W\u2019s are same for all time steps we need to go all the way back to make an update.\n\nRemember the BP for RNN is as same as neural networks BP\n\nbut here Current time step is calculated based on the previous time step so we have to traverse all the way back.\n\nif we apply chain rule which looks like this\n\nW\u2019s are same for all the time steps so the chain rule expands more and more\n\nA similar but a different way of working out the equations can be seen in Richard Sochers\u2019s Recurrent Neural Network lecture slide.\n\nSo here Et is same as our J( \u03b8)\n\nU, V and W should get updated using any optimization algorithms like gradient descent ( Take a look at my story here GD).\n\nNow if we go back and talk about our sentiment problem here is the RNN for that\n\nWe give word vectors or one hot encoding vectors for every word as input and we do feed forward and BPTT ,Once the training is done, we can give new text for prediction.\n\nIt learns something like whereever \u201cnot\u201d + positive word = negative.\n\nI hope you get that.\n\nSince W\u2019s are same for all timesteps, during back propagation as we go back adjusting the weights, The signal gets either too weak or too strong which cause either vanishing or exploding problem.\n\nTo avoid this we use either GRU or LSTM which I will cover in the next Stories\n\nSo That\u2019s it for this story , In the next story I will build the Recurrent neural network from scratch and using Tensorflow using the above steps and same Math.\n\nPhotos are designed using Paint in windows."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-9-2-nlp-code-for-word2vec-neural-network-tensorflow-544db99f5334?source=---------3",
        "title": "Chapter 9.2: NLP- Code for Word2Vec neural network(Tensorflow).",
        "text": "Last story we talked about word vectors , this story we write the code to build the word2vec model using Tensorflow Let\u2019s get started!!!\n\nLet\u2019s first take a data set ( Unstructured data ) , I take here is How i met your mother series subtitles data , you can take any data ( it does not matter).\n\nSo the sentence column is the actual raw data , we need to normalize the data ( removing symbols, spaces and etc\u2026)\n\nso now we have the clean data with us , let\u2019s create a dictionary out of these sentences ( here I take unigrams (1 word) ) you can take bi-grams also,\n\nhere we first identified the unique words by taking the count then we created the dictionary. ( word order depends based on the count).\n\nlet me take the first sentence in our data set to see how it looks\n\nWe just replaced the words with numbers , remember these numbers are not the vectors , these are just the indexes in our dictionary.\n\nTo build the model we can use skip gram ( as we discussed in the last story)\n\nIt predicts one word based on the surrounding words (it takes an entire context as an observation or takes a window sized context as an observation)\n\nEx: Text= \u201cMady goes crazy about machine leaning\u201d and window size is 3\n\nit takes 3 words at a time predicts the center word based on the surrounding words \u2192 [ [\u201cMady\u201d,\u201dcrazy\u201d ] , \u201cgoes\u201d] \u2192 \u201cgoes\u201d is the target word , and the other two are inputs.\n\nIt takes one word as input and try to predict the surrounding (neighboring) words,\n\n[\u201cMady\u201d, \u201cgoes\u201d],[\u201cgoes\u201d,\u201dcrazy\u201d] \u2192 \u201cgoes\u201d is the input word and \u201cMady\u201d and \u201cCrazy\u201d are the surrounding words (Output probabilities)\n\nnow we have the skip gram pairs as X and y values , lets create a functions that gives us the pairs batch wise\n\nNow we have the batch inputs to feed to Neural network so let\u2019s build the neural network using tensorflow\n\nAs we dicussed in the last story , word2vec model has a 3 layer neural network (input , hidden and output).\n\nHere Hidden layer is just the dot product of inputs and weights ( no activation function here)\n\nOutput layer has an activation function ( here it is Noise-contrastive estimation ) we can use softmax also here but NCE is good you can read about it in that paper(NCE internally uses the softmax).\n\nAnd we can define the size of hidden layer (here i took [voc_size, embedding_size(2)] but you can choose as your wish.\n\nokay let\u2019s train the model for 10000 iterations\n\nSo the error is decreased from 66 to 5 which is okay , now the model learned the weights so we got the final embeddings\n\nSo here are the first 100 words in vector space\n\nThe main theme of word2vec is we get the similar vectors for the words \u201cIndia\u201d, \u201cChina\u201d, \u201cAmerica\u201d, \u201cGermany\u201d, and etc\u2026 from a big corpus.\n\neven if we are not labeling or telling that those are country names.\n\nso if I give a text like \u201c I live in ____\u201d we can get the predictions like \u201cIndia\u201d, \u201cChina\u201d, \u201cAmerica\u201d, \u201cGermany\u201d, and etc\u2026\n\nif we try with different data we get this\n\nBased on the data , it understands the similar world (Idea is similar words appear in similar contexts)\n\nThe more data, the better results.\n\nWell, That\u2019s it for this story.\n\nIn the next story I will cover another interesting NLP/deep leaning concept until then See Ya!\n\nFull code is available on my Github."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1?source=---------4",
        "title": "Chapter 9.1 : NLP - Word vectors. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "Last story we talked about the basic fundamentals of natural language processing and data preprocessing, this story we talk about how documents will be converted to vectors of values.\n\nIn the last story only we talked about count vectorization and how the documents are converted into vectors with the help of count vectorizer.\n\nThis is how the documents are converted into vectors.\n\nJust taking count doesn\u2019t help in real data, because we can have words repeated many times in one document while not in other documents.\n\nThen we might get high count vectors for one document and low count vectors for others. This can be a problem, so to solve this problem we use a technique called TF-IDF(Term frequency-Inverse term frequency).\n\ntf-idf is a weighting factor which is used to get the important features from the documents(corpus).\n\nIt actually tells us how important a word is to a document in a corpus, the importance of a word increases proportionally to the number of times the word appears in the individual document, this is called Term Frequency(TF).\n\n\u201c Mady loves programming. He programs all day, he will be a world class programmer one day \u201d\n\nif we apply tokenization, steeming and stopwords (we discussed in the last story) to this document, we get features with high count like \u2192 program(3), day(2),love(1) and etc\u2026.\n\nTF = (no of times the word appear in the doc) / (total no of words in the doc)\n\nHere program is the highest frequent term in the document.\n\nso program is a good feature if we consider TF.\n\nHowever, if multiple documents contain the word \u201cprogram\u201d many times then we might say\u2026\n\nit\u2019s also a frequent word in all other documents in our corpus so it does not give much meaning so it probably may not be an important feature.\n\nTo adjust this we use IDF.\n\nThe inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents.\n\nIDF \u2014 Log(total no of documents / no of documents with the term t in it).\n\nYou can read more about it in wiki\n\nSo finally by using TF-IDF we get the most important features from the documents(corpus) with weights.\n\nHere is the comparison between count vectorizer and tfidf vectorizer based on the last story discussion and data.\n\nWe can use Scikit learn to this job\n\nif we take one word as a feature , that\u2019s called Unigram. if we take two words at a time as a feature , that\u2019s called Bigram. three words at a time as a feature , that\u2019s called Trigram\n\nif we take n words at a time as a feature , that\u2019s called n-gram\n\nThese values make more sense(adds much meaning) than just count values.\n\nThis table is called a sparse matrix ( a lot of zeros ) , we can now pass this table as X_Train to any machine learning model to train.\n\nThe count vectorizer and tfidf vectorizer focus only on the count, they just only take care of how many times a word appeared in the corpus , they just don\u2019t care about the order for example\n\nif we use count vectorizer or tfidf vectorizer, we get the same vectors for example , let\u2019s say we have good and movie are features and use (count vec)\n\nThese don\u2019t understand the meaning so we don\u2019t get useful representations.\n\nto get the meaningful numerical representations, we use one of the important model in natural language processing which is word2vec.\n\nlet\u2019s just forget about TFIDF for while and start with fresh mind, as we know if we want to feed words into machine learning models, we need to convert the words into some set of numeric vectors.\n\nA good way of doing this would be to use a \u201cone-hot\u201d method of converting the word into a sparse representation with only one element of the vector set to 1, the rest being zero.\n\nbut it would be extremely costly as the vocabulary gets increased and miss the meaning of the words.\n\nfor example if vocabulary size is 10000 then we have a 10000 sized vector for every word in the document for all documents in corpus.\n\nThe conversion of 10,000 columned matrix into a 200 columned matrix is called word embedding.\n\nthe word context / meaning can be created using 2 simple algorithms which are\n\nIt predicts one word based on the surrounding words (it takes an entire context as an observation or takes a window sized context as an observation)\n\nEx: Text= \u201cMady goes crazy about machine leaning\u201d and window size is 3\n\nit takes 3 words at a time predicts the center word based on the surrounding words \u2192 [ [\u201cMady\u201d,\u201dcrazy\u201d ] , \u201cgoes\u201d] \u2192 \u201cgoes\u201d is the target word , and the other two are inputs.\n\nIt takes one word as input and try to predict the surrounding (neighboring) words,\n\n[\u201cMady\u201d, \u201cgoes\u201d],[\u201cgoes\u201d,\u201dcrazy\u201d] \u2192 \u201cgoes\u201d is the input word and \u201cMady\u201d and \u201cCrazy\u201d are the surrounding words (Output probabilities)\n\nWe can use any of these models to get the word vectors , let\u2019s use skip gram model as it performs better, so here is how word2 vec works\n\nAssume we have 10000 unique words in dictionary so for one word \u201cgoes\u201d we get a 10000 sized vector as an input then we take 200 neurons and do the neural network training for all words using skip gram model ( I have already talked about neural network training here)\n\nOnce the training is complete , we get the final weights for hidden layer and output layer\n\n2. Ignore the last (output layer) and keep the input and hidden layer.\n\nso we get the 200 sized weights( scores ) for every word\n\nNow, input a word from within the vocabulary. The output given at the hidden layer is the \u2018word embedding\u2019 of the input word.\n\n\u2192 it\u2019s a neural network training for all the words in our dictionary to get the weights(vectors )\n\n\u2192 it has word embeddings for every word in the dictionary\n\nWell, we get the similar vectors for similar words for ex:\n\nwe get the similar vectors for the words \u201cIndia\u201d, \u201cChina\u201d, \u201cAmerica\u201d, \u201cGermany\u201d, and etc\u2026 from a big corpus.\n\neven if we are not labeling or telling that those are country names.\n\nso if I give a text like \u201c I live in ____\u201d we can get the predictions like \u201cIndia\u201d, \u201cChina\u201d, \u201cAmerica\u201d, \u201cGermany\u201d, and etc\u2026\n\nThat\u2019s it for this story. Hope you got an idea about how words can be represented numerically.\n\nIn the next story we will discuss how to write the code for word2 vec and the neural network training step by step with some more examples\n\nUntil then See Ya!!!"
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-9-natural-language-processing-14bbeb8edc79?source=---------5",
        "title": "Chapter 9 : Natural Language Processing. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "so far we have talked about machine learning and deep learning algorithms which can be used in any field. One of the main fields where ML/DL algorithms are used is Natural language processing(NLP) so from now onwards lets talk about the NLP.\n\nNLP is a big area, probably bigger than Machine learning cause the concept of language is really intense so we are not gonna focus on it completely but we focus on the small area where it meets machine learning and deep learning.\n\nlet\u2019s understand the natural language processing in our space.\n\nThe main goal here is , we wanna make the computer understand the language as we do and we wanna make the computer respond as we do.\n\nWe can break that into 2 sections\n\nThe system should be able to understand the language(parts of speech, context , syntax , semantics, interpretation and etc\u2026)\n\nThis can typically be done with the help of machine learning( Although problems are there).\n\nNot much difficult to do and gives good accuracy results.\n\nThe system should be able to respond / generate text (text planning, sentence planning, producing meaningful phrases and etc\u2026)\n\nThis can be done with the help of deep learning as deep understanding is required( Although problems are there).\n\nmuch difficult to do and the results may not be accurate.\n\nThese are the couple of applications where we focus\n\nI will try to explain and complete all the topics in next following stories , in this story we learn the basic fundamentals for text/document which is common for many applications.\n\nNote: Assume now that Text , data, document,sentence and paragraph all are same.\n\nEach word in the text has a meaning where the text may or may not have a meaning.\n\nin machine leaning we take features right? so here each word is a feature(unique).\n\nText : I love programming \u2192 I , love, programming are the features for this input.\n\nFirst apply Tokenization (a text is divided into token), we can use open source tools like NLTK to get tokens from the text.\n\nso here we have the programming repeated twice as tokens but we only take once so the features for this text are \u2192 I , love, programming, and, also, loves, me.\n\nbut wait the words love and loves mean same , these are called inflectional forms. we need to remove these\n\nso now the features for this text are \u2192 I , love, programming, and, also,me.\n\nwe can even think deep and say the word programming is similar to the word program\n\nso if we apply steeming then\n\nso now the features for this text are \u2192 I , love, program, and, also,me.\n\nthere are couple of words which occur very frequently in every language and don\u2019t have much meaning , these words are called Stop words.\n\nThe stop words in English are"
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-8-1-code-for-convolutional-neural-networks-tensorflow-and-keras-theano-33bef285dd93?source=---------6",
        "title": "Chapter 8 .1: Code for Convolutional neural networks(Tensorflow and Keras-Theano).",
        "text": "Last story we talked about convolutional neural networks, This story we will build the convoultional neural network using both Tensorflow and Keras (backed by Theano).\n\nFirst let\u2019s take a problem and the dataset, we will take image classification as problem and use CIFAR10 image dataset. you can download here.\n\nwe have 10 classes and a total of 50000 training and 10000 testing images.\n\nLet\u2019s take the dataset into the code. in Keras , we can get the data by calling this function.\n\nlet\u2019s understand the data and how it\u2019s represented\n\nso every image 3 X 32 X 32 (colors, width and height) and the values are in the image are pixel values (0\u2013255). and there are 50000 images.\n\nlet\u2019s convert the pixel values into between 0 and 1 to make the math easy.\n\nNow let\u2019s see how labels are represented\n\nlabels are class numbers (class 0, class 1 ,\u2026 class 9) so we need to convert into one hot vector (the vector length is 10 as we have 10 classes) so at the end we get the probabilities score for every class, we pick the maximum score.\n\nHere class 6 is the label so 1 is stored at 6th index, remaining all 0's.\n\nSo the basic data understanding is done. now let\u2019s pick any deep learning framework to implement the convolutional neural network.\n\nFirst let\u2019s pick the architecture , I am gonna choose this architecture (you can try any other architecture)\n\nplaceholders for the input and output of the network. Placeholders are variables which we need to fill in when we are ready to compute the graph.\n\nand weights for convolution 1 ,2,3 and Fully connected are initialized with the random numbers.\n\nLet\u2019s write the code to build the model\n\nWe defined the model according to the diagram , here we used dropout , The dropout is used to prevent overfitting of the data, in the network , some % of neurons(random) don\u2019t get computed.\n\nnext let\u2019s define the cost function ,optimizer and predicted function.\n\nwe used here RMSProp as optimizer, but we can try others also and learning rate is 0.001, momentum=0.9.\n\none more thing I forgot which is we need to reshape our data according to tensorflow,\n\nAll the variables done.\n\nNow we need to create a session in tensorflow to run the code.\n\nFor every 10 epochs let\u2019s try our test data to see the accuracy.\n\nSo after a few hours of laptop heating ( if CPU ) we would get the trained model with the accuracy.\n\nWe can increase the accuracy by just playing with network(trying out different numbers for neurons and filters), it took me 2 hours to train the model( as I have other models also running at the same time) so I am not gonna try other, if you wish you can try, to increase the accuracy.\n\nCIFAR 10 can get even more than 90% accuracy, if you wanna improve the accuracy , Please follow these papers (there are many ways to improve).\n\nLet\u2019s try out another model Using Keras(Backed by Theano).\n\nOkay let\u2019s try out a simple model of 2 convolution layer , 1 pooling layer and a fully connected layer.\n\nNote: I write only required code , Full code is on my Github.\n\nLet\u2019s call the model and check the network architecture.\n\nLet\u2019s do the training by calling the fit method.\n\nyou can still try to improve it Don\u2019t give up.\n\nThat\u2019s it for this story. Hope you enjoyed and learned something let me know if it helps .\n\nIn the next story I will cover another interesting machine leaning concept until then See Ya!\n\nFull code is available on my Github."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-8-0-convolutional-neural-networks-for-deep-learning-364971e34ab2?source=---------7",
        "title": "Chapter 8 .0: Convolutional neural networks for deep learning.",
        "text": "Last story we talked about ordinary neural networks which are basic building blocks for deep learning, This story I wanna talk about Convolutional neural networks or Convnets.\n\nThe convnets have been the major breakthroughs in the field of Deep learning and they perform really well for image recognition, we can also use CNN\u2019s for Natural language processing and speech analysis. In this story I focus on computer vision(Image recognition). Let\u2019s get started!!!!!!\n\nI follow Simon Sinek , he says always start with Why,how and finally What\n\nLet\u2019s say we are training a classifier to identify a cat using an ordinary neural net(where we have input, hidden and output layers)\n\nAn ordinary neural networks typically takes features as inputs, for this problem we take image array as inputs, so we have a vector, size of (image width*height) as an input.\n\nWe feed it to the model and train it (back propagation) for many images for many iterations.\n\nOnce the network is trained then we can give another cat picture to predict (to get the score) to see if it gives the result as cat(high probability score).\n\nwell, it works, but wait..\n\nwhat if I gave the test pictures like these for prediction.\n\nThe ordinary network may not predict well(or not get much score for the cat) and what if I gave b/w pictures as test images(assume the train set does not have b/w images)\n\nThe network might fail to give the highest probability score as this type of features(b/w) we did not train.\n\nSo what is happening here??\n\nWhat we feed is what we get.\n\nThe network understands the mapping between X and y but not the patterns in X.\n\nFor above 3 test images the CNN is gonna be able to predict well for cats.\n\nConvNets are used mainly to look for patterns in an image, we don\u2019t need to give features, the CNN understands the right features by itself as it goes deep.this is one of the reasons why we need CNN\u2019s. Period.\n\nAnd another reason is, ordinary neural networks don\u2019t scale well for full sized images , let\u2019s say that input images size =100(width) * 100 (height) * 3 (rgb).\n\nthen we need to have 30,000 neurons which is very expensive in the network.\n\nHence We need to learn CNN.\n\nFor every image , it creates many images by applying some filters ( just like photo editing tools )\n\nThese filters, we can call weights , kernels or features\n\nthey are initialized randomly first then during the training these weights will get updated (the network learns these weights)\n\nlet\u2019s take an example, suppose we have an image of 5X5 size like this and filters are like this,\n\nNote : For sake of understanding I assume that 5 X 5 array is full image and the values are pixel values, otherwise it would be a big table of matrix and the values can be anything 0 and 1 or continuous value (- to +).\n\nWe have 3 filters which we initialize randomly (we define the filter size).\n\nNote: Here I took 0 or 1 to make the math easy , usually these are continuous values.\n\nif we run each filter all over the image we will get the output images like these.\n\nHow did we get the output like these???\n\nHere is how"
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-7-1-neural-network-from-scratch-in-python-b880b0ff5f7b?source=---------8",
        "title": "Chapter 7.1 : Neural network from scratch in python",
        "text": "Last story we talked about neural networks and its Math , This story we will build the neural network from scratch in python. First let\u2019s take the simplest dataset which is XOR table.\n\nHere we have two inputs X1,X2 , 1 hidden layer of 3 neurons and 2 output neurons, if we initalize the network so the neural net will look like this. If we use numpy arrays , it\u2019s just a dot product of inputs and weights,Then we apply the sigmoid function , We do this for every neuron in every layer.\n\nYou can simply call the forward propagation method to predict. That\u2019s it for this story. Hope you enjoyed and learned something let me know if it helps . You can find the full code on my Github. In the next story I will talk about one of the important algorithms in deep learning which is Convolution neural networks."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b?source=---------9",
        "title": "Chapter 7 : Artificial neural networks with Math. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "I have been talking about the machine learning for a while, I wanna talk about Deep learning as I got bored of ML.\n\nso this article we will talk about Neural networks which are part of deep learning which is part of machine learning. Let\u2019s get started!!!!!!\n\nNote: Understanding of Math from previous article GD is required a bit.\n\nas usual before I dive into deep learning, I wanna point this.\n\nwell, there are plenty of reasons for why\u2019s from researchers and other scientists, as a machine learning scientist I believe in few which are below\n\netc\u2026 there are a lot actually\n\nNote: Just because DL is cool, does not mean that we don\u2019t need to use ML techniques,(Based data and problem we (\u2018I) choose models, algorithms , frame works and tools)\n\nOkay man got it let\u2019s go ahead and tell us about Neural networks.\n\nNeuron is a computational unit which takes the input(\u2018s) , does some calculations and produces the output. that\u2019s it no big deal.\n\nAbove, the 2nd picture is the one we use in neural networks, we have the input and we have some weights(parameters) we apply the dot product of these two vectors and produce the result (which would be a continuous value -infinity to + infinity).\n\nif we want to restrict the output values we use an Activation function.\n\nThe activation function squashes the output value and produce a value within a rage (which is based on the type of activation function).\n\nWe use often these three (Sigmoid range from 0 to 1, Tanh from -1 to 1 and Relu from 0 to +infinity).\n\nA neural network is a set of layers(a layer has set of neurons) stacked together sequentially.\n\nThe output of one layer would be the input of the next layer.\n\nHere we have three layers\n\nNote:We can have (n) no of hidden layers in between.(for sake of understanding let\u2019s take only one hidden layer).\n\n3. Output layer: it\u2019s same hidden layer except it gives the final result(outcome/class/value).\n\nso How do we define no of neurons in each layer and the whole network???\n\nwell, Input layer\u2019s neurons are based on no of features in the dataset.\n\nwe can define as many neurons/layers as we wish (it depends on the data and problem) but would be good to define more than features and all hidden layers have same no of neurons.\n\nOutput layer\u2019s neurons are based the type of problem and outcomes.\n\nif regression then 1 neuron ,for binary classification you can have 1 or 2 neurons. and for multi classification more than 2 neurons.\n\nNote: there is no bias here as it is the last layer in the network.\n\nWe got the basic understanding of neural network so let\u2019s get into deep.\n\nOnce you got the dataset and problem identified, you can follow the below Steps:\n\nThere are 2 algorithms in Neural networks\n\nLets take a toy dataset (XOR) and pick the architecture with\n\nThis is a simple process, we feed forward the inputs through each layer in the network , the outputs from the previous layer become the inputs to the next layer.(first we feed our data as the inputs)\n\nFirst we provide the inputs(example) from our dataset ,\n\nAssume random weights and Activation(A1,2\u2026) we get the errors for each neuron.\n\nOut cost function from Andrew Ng is\n\nNote: we take partial derivative w.r.t result (by using Chain rule in calculus)\n\nTrust me it\u2019s easy! or I will make it easy.\n\nThe main goal of backpropagation is to update each of the weights in the network so that they cause the predicted output to be closer the target output, thereby minimizing the error for each output neuron and the network as a whole.\n\nSo far we got the total error which is to be minimized.\n\nif you know how gradient descent works , the rest is pretty easy , if you don\u2019t know, here is my article that talks about Gradient descent.\n\nWe need to calculate the below terms\n\nWe repeat the process forwarding the weights(FP) and updating weights(BP) for no of epochs or we reach the minimum error.\n\nOnce the training process is done, we can do the prediction by feed forwarding the input to the trained network, that\u2019s it.\n\nHope its not confusing , and if you are not good at derivatives, you can let me know I can help, but I am sure that this will make sense as you go through again and again.\n\nI put lot of efforts in adding the Math stuff and diagrams I feel pictures are awesome than words so please let me know if it helps.\n\nSo That\u2019s it for this story , In the next story I will build the neural network from scratch using the above steps and same Math."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-5-k-nearest-neighbors-algorithm-with-code-from-scratch-7f93f653c860",
        "title": "Chapter 5 : K-nearest neighbors algorithm with code from scratch.",
        "text": "Last story we talked about the decision trees and the code is my Github, this story i wanna talk about the simplest algorithm in machine learning which is k-nearest neighbors. Let\u2019s get started!!\n\nk-nearest neighbor is used for both regression and classification problems and there is no training process for this algorithm, the entire data set is used for predicting/classifying new data.\n\nWhen a new data point is given, it calculates the distance from the new data point to all other points in our data-set.\n\nthen depending on the K value, it identifies the nearest neighbor(\u2018s) in our data set,\n\nif K=1 then then it takes the minimum distance of all points and classifies as the same class of the minimum distance data point.\n\nif k>1 then it takes a list of K minimum distances of all data points\n\nfor classification, it classifies the new data point based on the majority of votes in the list.\n\nfor regression, it takes the average of all values in the list.\n\nLet\u2019s understand how it works with data\n\nWe have a dataset(labeled) of two features like this , now we need to have new data points which are not labeled to identify the class for each data point.\n\nthen it calculates the distance from these new data points to all other points in our data-set\n\nWell, there are several distance metrics available, it uses one of the distance metrics\n\nLets say K=1 and we use Euclidean distance as a metric,\n\nNow we calculate the distance from the new data point(\u2018s) to all other points and then take the minimum value of all.\n\nif K>1 let\u2019s say 5 then we take a list of 5 minimum distances then classify the new data point based on the majority of votes in the list\n\nfor ex: observe below right picture.\n\nif k= 5 then we take 5 close points (4 purple, 1 yellow) we classify that point as purple(as the majority of purple is high).\n\nWell, the only parameter tuned in this algorithm is K, it can be adjust based on the data and accuracy of predicting.\n\none of the simple ways of adjusting K value is trying many K values , based on the accuracy you can decide the value of K.\n\nBest value for K is either 1 or 2\n\nNote : it would be better if we choose odd numbers for K values to avoid a tie at the time of prediction.\n\nYou can find the full code on my Github.\n\nThat\u2019s it for this story. Hope you enjoyed and learned something let me know if it helps .\n\nI think I covered most of machine learning,from the next story onwards I will talk about deep leaning algorithms."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1",
        "title": "Chapter 4: Decision Trees Algorithms \u2013 Deep Math Machine learning.ai \u2013",
        "text": "Decision tree is one of the most popular machine learning algorithms used all along, This story I wanna talk about it so let\u2019s get started!!!\n\nDecision trees are used for both classification and regression problems, this story we talk about classification.\n\nBefore we dive into it , let me ask you this\n\nWe have couple of other algorithms there, so why do we have to choose Decision trees??\n\nwell, there might be many reasons but I believe a few which are\n\nFor example : if we are classifying bank loan application for a customer, the decision tree may look like this\n\nHere we can see the logic how it is making the decision.\n\nA decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value).\n\nThe whole idea is to create a tree like this for the entire data and process a single outcome at every leaf(or minimize the error in every leaf).\n\nThere are couple of algorithms there to build a decision tree , we only talk about a few which are\n\nLets just first build decision tree for classification problem using above algorithms,\n\nLet\u2019s just take a famous dataset in the machine learning world which is whether dataset(playing game Y or N based on whether condition).\n\nWe have four X values (outlook,temp,humidity and windy) being categorical and one y value (play Y or N) also being categorical.\n\nso we need to learn the mapping (what machine learning always does) between X and y.\n\nThis is a binary classification problem, lets build the tree using the ID3 algorithm\n\nTo create a tree, we need to have a root node first and we know that nodes are features/attributes(outlook,temp,humidity and windy),\n\nAnswer: determine the attribute that best classifies the training data; use this attribute at the root of the tree. Repeat this process at for each branch.\n\nThis means we are performing top-down, greedy search through the space of possible decision trees.\n\nAnswer: use the attribute with the highest information gain in ID3\n\nIn order to define information gain precisely, we begin by defining a measure commonly used in information theory, called entropy that characterizes the (im)purity of an arbitrary collection of examples.\u201d\n\nOkay lets apply these metrics to our dataset to split the data(getting the root node)\n\nOkay I got it , if it does not make sense to you , let me make it sense to you.\n\nSimilarity we can calculate for other two attributes(Humidity and Temp).\n\nSo our root node is Outlook.\n\nFinally we get the tree something like his.\n\nIn CART we use Gini index as a metric,\n\nWe use the Gini Index as our cost function used to evaluate splits in the dataset.\n\nour target variable is Binary variable which means it take two values (Yes and No). There can be 4 combinations.\n\nA Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes.\n\nWe calculate it for every row and split the data accordingly in our binary tree. We repeat this process recursively.\n\nSimilarly if Target Variable is categorical variable with multiple levels, the Gini Index will be still similar. If Target variable takes k different values, the Gini Index will be\n\nMaximum value of Gini Index could be when all target values are equally distributed.\n\nSimilarly for Nominal variable with k level, the maximum value Gini Index is\n\nMinimum value of Gini Index will be 0 when all observations belong to one label.\n\nThe calculations are similar to ID3 ,except the formula changes.\n\nfor example :compute gini index for dataset\n\nsimilarly we can follow other steps to build the tree\n\nThat\u2019s it for this story. hope you enjoyed and learned something.\n\nLet me know your thoughts/suggestions/questions.\n\nwe just talked the first half of Decision trees , we can talk about the other half later (some statistical notations,theories and algorithms)\n\nIn the next story we will code this algorithm from scratch (without using any ML libraries)."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-3-1-svm-from-scratch-in-python-86f93f853dc",
        "title": "Chapter 3.1 : SVM from Scratch in Python. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "Last story we talked about the theory of SVM with math,this story I wanna talk about the coding SVM from scratch in python. Lets get our hands dirty!\n\nFull code is available on my Github.\n\nFirst things first, we take a toy data-set , we can generate random data X and y using sklearn make_blobs and plot it.\n\nWe have two features(Independent variables) and 1 dependent variable which is either 0 or 1, for SVM convenience we treat them either -1 or 1 separate our data-set like that and define required variables.\n\nwe have everything we need so lets talk about the training process\n\nWe are very sure from our last story, if the training points satisfy the below condition, then we have a separable hyperplane between those points.\n\nif it satisfies, we calculate the magnitude and save the weights\n\nelse we update the weights.\n\nThe below code does the same thing.\n\nNote : I only talk about required code, Full code is on my Github.\n\nThe main code has been taken from the awesome guy Sentdex so the credit goes to him.\n\nhere we are storing the magnitude of weights(w) in a dictionary, once its optimized so we get a dictionary(keys=|w|,values=[w,b]) ,\n\nfrom them we choose the minimum |w|\n\nAn example picture for above is this\n\nif we get these two pictures we pick the minimum magnitude one |w|\n\nokay we have optimized the weights, but wait , there might be a problem for optimizing the weights which is \u201cwhat if we overshot the minimum value??\u201d\n\nAfter overshooting we take a step back to minimum, but that step size(learning rate) is not same as earlier size, we need to decrease it,\n\nwe should give a list of step sizes as per our convince, which is why I gave a list variable in the beginning of code.\n\nfor every step size we run the whole optimization code(above block), then we finally reach the global minimum value.\n\nNow we have the whole optimization done, and we got final weights.\n\nif we visualize the data-set we get this.\n\nAs we talked in the last story ,\n\nThat\u2019s it for this story. Hope you enjoyed and learned something let me know if it helps .\n\nIn the next story I will cover another interesting machine leaning concept until then See Ya!\n\nFull code is available on my Github."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-3-support-vector-machine-with-math-47d6193c82be",
        "title": "Chapter 3: Support Vector machine with Math. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "Last story we talked about Logistic Regression for classification problems, This story I wanna talk about one of the main algorithms in machine learning which is support vector machine. Lets get started!\n\nSVM can be used to solve both classification and regression problems, in this story we talk about classification only(binary classification and linear data).\n\nBefore we dive into the story I wanna point this\n\nWe have couple of other classifiers there, so why do we have to choose SVM over any other ??\n\nWell! It does a pretty good job at classification than others. for example observe the below image.\n\nWe have a data-set which is linearly separable from +\u2019s and \u2014 \u2019s , we can separate the data using logistic regression (or other) we may get something like above (which is reasonable).\n\nThis is how SVM does\n\nThis is a high level view of what SVM does, The yellow dashed line is the line which separates the data (we call this line \u2018Decision Boundary\u2019 (Hyperplane) in SVM), The other two lines (also Hyperplanes) help us make the right decision boundary.\n\nThe answer is \u201ca line in more that 3 dimensions\u201d ( in 1-D it\u2019s called a point, in 2-D it\u2019s called a line, in 3-D it\u2019s called a plane, more than 3 - Hyperplane).\n\nMotivation: Maximize margin: we want to find the classifier whose decision boundary is furthest away from any data point.We can express the separating hyper-plane in terms of the data points that are closest to the boundary. And these points are called support vectors.\n\nWe would like to learn the weights that maximize the margin. So we have the hyperplane!\n\nMargin is the distance between the left hyperplane and right hyperplane.\n\nThese are couple of examples that I ran SVM (written from scratch) over different data sets.\n\nOkay, you get the idea what SVM does , now Lets talk about how it does???\n\nFirst I want you to be familiar with the above words so lets complete that\n\nThe magnitude or length of a vector u(O-A distance) is written \u2225 u\u2225 and is called its norm.\n\nThe direction of the vector u is defined by the angle \u03b8 with respect to the horizontal axis, and with the angle \u03b1 with respect to the vertical axis.\n\nIf we work on a bit we get this \u2016x\u2016\u2016y\u2016cos(\u03b8)=x1y1+x2y2\n\nWhy are we interested by the orthogonal projection ? Well , it allows us to compute the distance between x and the line which goes through y (x-z).\n\nThe line equation and hyperplane equation \u2014 same, its a different way to express the same thing,\n\nIt is easier to work in more than two dimensions with the hyperplane notation.\n\nso now we know how to draw a hyperplane with the given dataset,\n\nWe have a data-set , we want to draw a hyper plane something like above (which separates the data well).\n\nif we maximize the margin(distance) between two hyperplanes then divide by 2 we get the decision boundary.\n\nlets take only 2 dimensions, we get the equation for hyper line is\n\nw.x+b=0 which is same as w.x =0 (which has more dimensions)\n\nfor all positive(x) points satisfy this rule (w.x+b \u22651)\n\nfor all negative(x) points satisfy this rule (w.x+b\u2264-1)\n\nAnswer : pick one which has minimum Magnitude of w\n\nThis is the final picture designed by me\n\nso either we save the w and b values and keep going or we adjust the parameter (w and b) and keep going. Another optimization problem, SVM.\n\nAdjusting parameters? Sounds like Gradient descent right? Yes!!!\n\nIt is a convex optimization problem which surely gives us global minmum value.\n\nOnce its optimized we are done!\n\nNow we give a unknown point and we want to predict whether it belongs to positive class or negative class.\n\nThat\u2019s it for this story. hope you enjoyed and learned something.\n\nwe just talked about half of SVM only , the other half is about Kernals ( we will talk about it later)\n\nIn the next story we will code this algorithm from scratch (without using any ML libraries)."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-2-0-logistic-regression-with-math-e9cbb3ec6077",
        "title": "Chapter 2.0 : Logistic Regression with Math. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "In the previous story we talked about Linear Regression for solving regression problems in machine learning , This story we will talk about Logistic Regression for classification problems.\n\nYou may be wondering why the name says regression if it is a classification algorithm, well,It uses the regression inside to be the classification algorithm.\n\nClassification : Separates the data from one to another.\n\nThis story we talk about binary classification ( 0 or 1) Here target variable is either 0 or 1\n\nso we use regression for drawing the line , makes sense right?\n\nLets take a random dataset and see how it works,\n\nif we observe the right picture we have our independent variable (X) and dependent variable(y) so this is the graph we should consider for the classification problem\n\nGiven X or (Set of x values) we need to predict whether it\u2019s 0 or 1 (Yes/No).\n\nIf we apply Linear regression for above data we get something like this,\n\nGiven X value 6 we can say y is 0.7 (close to 1), that\u2019s cool but wait, What if I give negative X value or greater X value??? The output is this\n\nWe only accept the values between 0 and 1 (We don\u2019t accept other values) to make a decision (Yes/No)\n\nThere is an awesome function called Sigmoid or Logistic function , we use to get the values between 0 and 1\n\nThis function squashes the value (any value ) and gives the value between 0 and 1\n\nHow??? and what is \u2018e\u2019 ???\n\ne here is \u2018exponential function\u2019 the value is 2.71828\n\nthis is how the value is always between 0 and 1.\n\nSo far we know that we first apply the linear equation and apply Sigmoid function for the result so we get the value which is between 0 and 1.\n\nThe hypothesis for Linear regression is h(X) = \u03b80+\u03b81*X\n\nThe hypothesis for this algorithm is\n\n2. We apply the above Sigmoid function (Logistic function) to logit.\n\nhere it does not work as h(x) hypothesis gives non convex function for J(\u03b80,\u03b81) so we are not guaranteed that we reach best minimum.\n\nWe take log( hypothesis) to calculate the cost function\n\nIf it does not make sense , let me make it sense to you\n\nusually error is what?? (predicted \u2014 actual)**2 right??\n\njust take a look at this picture and observe something..\n\nIf actual y =1 and predicted =0 the cost goes to infinity and If actual y =1 and predicted =1 the cost goes to minimum.\n\nIf actual y =0 and predicted =1 the cost goes to infinity and If actual y =0 and predicted =0 the cost goes to minimum.\n\nif we apply log to hypothesis (predicted) we get some values (cost) which is useful to estimate the overall error.\n\nHere is the final picture.\n\nthat\u2019s it. based on the actual y values we calculate different functions.\n\n4. Next step is to apply Gradient descent to change the \u03b8 values in our hypothesis ( I already covered check this link).\n\nThat\u2019s it We are done!\n\nwe got the Logistic regression ready, we can now predict new data with the model we just built.\n\nPredicting new data, remember?? we give new X values we get the predicted y values how does it work ??\n\nwe get the probability score(s).\n\nSo That\u2019s it for this story , In the next story I will code this algorithm from scratch and also using Tensorflow and scikitlearn."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-1-3-7ba084ff7e6d",
        "title": "Chapter 1.3:Code \u2013 Deep Math Machine learning.ai \u2013",
        "text": "In last two parts we talked about Linear regression and Gradient descent.\n\nThis part I wanna code the algorithm from scratch and using other two libraries (Tensorflow and scikit learn). Lets get started!\n\nFull code is available on my Github.\n\nFirst things first, we should prepare the data-set , we can generate random data X and y using Numpy and plot it.\n\nNow we have the X values and y values (inputs and targets), next step is to generate random weights (\u03f4 values)\n\nWe got our \u03f4 values(\u03f40 and \u03f41) and X values , next step is to predict y values(remember these are different from actual y values from our data-set)\n\nthe above code does matrix multiplication same as y1=x0*\u03f40+x1*\u03f41 (x0=1, x1=our first X value in data-set). next step is to calculate the error and minimize the error (Gradient descent process)\n\nerror=(predicted-actual)**2 and , to minimize the error we need to find the derivatives(gradients) for weights(\u03f4 values) and update them as we reach the minimum value .\n\nAfter iterating it by no of epochs, we finally get the final weights (best fit line for our data) here is the graph for error with epochs\n\nAs we can see the error is reduced as no of epochs are increasing and final we reach the minimum value, and we got our updated weights and best fit line.\n\nNext step is to predict the new data we can simply do that by just calling the predict() function (Weights W are final weights) by giving the set of test data (X values)\n\nwe can also add many features to X (no of features can be more than one), create a data set with multiple features in X and run the same program\n\nplaceholders for the input and output of the network. Placeholders are variables which we need to fill in when we are ready to compute the graph.\n\nNext we predict the y values\n\nNow calculate the error for every X value and sum it up for the final error\n\nNow update the weights to reduce the error , how? Gradient descent\n\nTensorflow has already implemented GD so we don\u2019t need to write it.\n\nNote: we need to create a session in tensorflow to run the code\n\nNow We can do the same thing using scikit learn in just a few line of code , how awesome is that?\n\nOh! yeah! finally we made the same algorithm work in 3 different ways\n\nhere is the final picture comparing the best fit line for all 3 methods\n\nCan you guess which one is best??? let me know what you think\n\nThat\u2019s it for this story. Hope you changed and learned something let me know if it helps .\n\nIn the next story I will cover another interesting machine leaning concept until then See Ya!\n\nFull code is available on my Github"
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-1-2-gradient-descent-with-math-d4f2871af402",
        "title": "Chapter 1.2: Gradient Descent with Math. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "This story I wanna talk about a famous machine learning algorithm called Gradient Descent which is used for optimizing the machine leaning algorithms and how it works including the math.\n\nFrom chapter 1 we know that we need to update m and b values, we call them weights in machine learning. Lets alias b and m as \u2212 \u03b80 and \u03b81 (theta 0 and theta 1 ) respectively.\n\nFirst time we take random values for \u03b80 and \u03b81, and we calculate y\n\ny = \u03b80+\u03b81*X \n\nIn machine learning we say hypothesis so h(X) = \u03b80+\u03b81*X\n\nh(X)=y but this y is not actual value in our data-set, this is predicted y from our hypothesis.\n\nFor example lets say our data-set is something like below and we take random values which are 1 and 0.5 for \u03b80 and \u03b81 respectively.\n\nFrom this we calculate the error which is\n\nwe just calculated the error for one data point in our data-set , we need to repeat this for all data points in our data set and sum up the all errors to one error which is called Cost Function \u2018J(\u03b8)\u2019 in machine learning.\n\nOur goal is to minimize the cost function (error) we want our error close to zero Period.\n\nwe have the error 1 for first data-point so lets treat that as whole error and reduce to zero for sake of understanding.\n\nfor (h(x)-y)\u00b2 function we get always positive values and graph will look like this(Left) and lets plot the error graph.\n\nHere is the gradient descent work comes into the picture.\n\nBy taking the little steps down to reach the minimum value (bottom of the curve) and changing the \u03b8 values in the process.\n\nThe answer is in Math.\n\nFrom above picture we can define our \u03b80 and \u03b81.\n\nAnd alpha here is a learning rate usually we give 0.01 but it depends, it tells how big the step-size is towards reaching the minimum value.\n\nAgain we know our J(\u03b80,\u03b81) so if we apply this to above equations for \u03b80 and \u03b81, we get our new \u03b80 and \u03b81 values.\n\nFor example f(x) =x\u00b2 \u2192 df/dx=2x How ???\n\nits same as calculating derivatives but here we calculate the derivative with respective to that value , others are constants (so d/dx(constant)=0)\n\nThe same thing we can apply for calculating partial derivative with respective to \u03b80 and \u03b81.\n\nHow come that box drawn disappeared in the next step above? just wait and see.\n\nFor calculating partial derivative with respective to \u03b81 is also same as above except one little part is added\n\nHope its not confusing , and I know its little bit hard to grasp in the beginning but I am sure that this will make sense as you go through again and again.\n\nSo That\u2019s it for this story , In the next story I will cover another interesting topic in machine learning so See ya!"
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/chapter-1-complete-linear-regression-with-math-25b2639dde23",
        "title": "Chapter 1 :Complete Linear Regression with Math. \u2013 Deep Math Machine learning.ai \u2013",
        "text": "Linear Regression: it is a linear model that establishes the relationship between a dependent variable y(Target) and one or more independent variables denoted X(Inputs).\n\nOur Training Data consists of X and y values so we can plot them on the graph, that\u2019s damn easy. now what\u2019s next? how to find that blue line????\n\nFirst lets talk about how to draw a linear line in the graph,\n\nIn math we have an equation which is called linear equation\n\nso we can draw the line if we take any values for m and b\n\nLets take a simple data set (sine wave form -3 to 3) and First time we take random values of m and b values and we draw a line something like this.\n\nwe take the first X value(x1) from our data set and calculate y value(y1)\n\nThat line is not fitting well to the data so we need to change m and b values to get the best fit line.\n\nEither we can use an awesome algorithm called Gradient Descent (Which I will cover in next story with also the math used in there.)\n\nOr we can borrow direct formulas from statistics(they call this Least Square Method) I will also cover if possible in next story.\n\nRight now lets black box, we assume that we are getting the m and b values, Every time when the m and b values change we may get a different line and finally we get the best fit line\n\nSo What\u2019s next??? Predicting new data, remember?? so we give new X values we get the predicted y values how does it work ??\n\nsame as above y= m X +b , we now know the final m and b values.\n\nThis is called simple linear regression as we have only one independent X value. Lets say we wanna predict housing price based the size of house\n\nWhat if we have more independent values of X????\n\nLets say we wanna predict housing price not only by the size of house but also by no of bedrooms\n\nx1= Size (in sqft\u2019s), x2=N_rooms and y= Price (in dollar\u2019s)\n\nThe process same as above but the equation changes a bit\n\nNote: Lets alias b and m as \u2212 \u03b80 and \u03b81 (theta 0 and theta 1 ) respectively.\n\nThat\u2019s it for this story , Hoping it helps at least 1 person.\n\nIn the next story I will talk about Gradient Descent Algorithm.\n\nuntil then See ya!"
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/different-types-of-machine-learning-and-their-types-34760b9128a2",
        "title": "Different types of Machine learning and their types.",
        "text": "Supervised and unsupervised are mostly used by a lot machine learning engineers and data geeks\n\nReinforcement learning is really powerful and complex to apply for problems.\n\nas we know from last story machine learning takes data as input. lets call this data Training data\n\nThe training data includes both Inputs and Labels(Targets)\n\nwhat are Inputs and Labels(Targets)?? for example addition of two numbers a=5,b=6 result =11, Inputs are 5,6 and Target is 11\n\nWe first train the model with the lots of training data(inputs&targets)\n\nthen with new data and the logic we got before we predict the output\n\nThis process is called Supervised Learning which is really fast and accurate.\n\nRegression: This is a type of problem where we need to predict the continuous-response value (ex : above we predict number which can vary from -infinity to +infinity)\n\netc\u2026 there are tons of things we can predict if we wish.\n\nClassification: This is a type of problem where we predict the categorical response value where the data can be separated into specific \u201cclasses\u201d (ex: we predict one of the values in a set of values).\n\nHere is the final picture\n\nThe training data does not include Targets here so we don\u2019t tell the system where to go , the system has to understand itself from the data we give.\n\nHere training data is not structured (contains noisy data,unknown data and etc..)\n\nex: A random articles from different pages\n\nThere are also different types for unsupervised learning like Clustering and anomaly detection (clustering is pretty famous)\n\nClustering: This is a type of problem where we group similar things together.\n\nBit similar to multi class classification but here we don\u2019t provide the labels, the system understands from data itself and cluster the data.\n\nUnsupervised learning is bit difficult to implement and its not used as widely as supervised.\n\nI would like to cover reinforcement learning in a separate full article as it is intense. so\n\nThat\u2019s all for this story, Hope you get some idea.\n\nIn the next story I would like to talk about the first machine learning algorithm Linear Regression with Gradient descent."
    },
    {
        "url": "https://medium.com/deep-math-machine-learning-ai/introduction-of-machine-learning-why-how-what-84c881c70763",
        "title": "Introduction of Machine learning (Why,How,What) \u2013 Deep Math Machine learning.ai \u2013",
        "text": "Before we understand machine learning, lets talk about why machine learning is needed and why the heck we care?\n\nif I asked you to write a program of adding two numbers , you would probably write it but what if i asked you to do multiplication with the same program??\n\ndoes it work?? Absolutely \u201cNo\u201d , that program is written to do addition only\n\nAnother example is if I told you to write logic for the below block , how many programs would you write??\n\nYou need to write a separate program / logic for every block , wouldn\u2019t it be cool if I had only one program which could solve all 3 blocks? Yes it would.\n\nThat is where machine learning comes into picture , No need to write programs with hard coded rules and let the system understand the logic and produce the desire results.\n\nThe below picture explains clearly\n\nSo instead of giving the program / logic, we are giving the output\n\nFor example , Addition of two numbers , for traditional programming we give data and logic, a=2 , b=3 then we get answer 5\n\nFor machine learning we give a=2, b=3 and output = 5 so the system understands it ,how come 2 , 3 is 5 ?\n\nData for addition task is below\n\nAnd finally we ask something like what is the result for 5,6\n\nwe get the output as 11 ( actually it might be 10.989 or 11.01 etc.. depends on the data how much we provided).\n\nMachine learning is all about learning from data(examples) , building the logic and predicting the output for a given input\n\nA definition from Tom Mitchell( A famous computer scientist) is this\n\nso we can break that down into 2 parts\n\nThat\u2019s it for this Story , Hope you get some idea.\n\nIn the next story I would like to talk about different types of machine learning(what,how ,why)"
    }
]