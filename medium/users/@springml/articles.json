[
    {
        "url": "https://towardsdatascience.com/tensorflow-for-manufacturing-quality-control-bc1bc6740558?source=user_profile---------1----------------",
        "title": "Tensorflow for Manufacturing Quality Control \u2013",
        "text": "Artificial Intelligence is transforming every industry. In manufacturing, AI and Machine Learning are making a huge difference in automated quality control. Visual inspection and issue identification is a challenge that typically requires intensive human labor. For example correct packaging is an important step, used not only for branding but also for logistical purposes. Packages are imprinted with codes allowing manufacturers to keep track of the timing, origin and routing of packages.\n\nSpringML\u2019s label detection app uses machine learning to identify anomalies in package labeling. This app consumes live video or images of packages and extracts text in real time. If the text does not conform to standards then it raises alerts in real time.\n\nThis is a challenging problem for several reasons \u2014 the video footage may not be stable so the location of the serial number on the box changed across frames, the motion of the box blurred out the serial number and the cropped out text was low resolution which was a challenging input for a standard OCR package.\n\nWe solved the problem by using the Tensorflow Object Detection framework from Google.\n\nSee the result below. A green box forms on the predicted text when it has been read perfectly. Impressive right!!\n\nThere are two main steps to solving this problem:\n\nLets go through each in detail\n\nDetecting the location of serial number on the box\n\nIf the camera is fixed positioned and the belt moves at constant speed then you can use a logic based on space and time to identify and crop out the serial number as it moves across the frame. However this problem was more challenging due to hand held camera and so I used the Tensorflow Object Detection framework to train a model on identifying the serial number. To learn more about training custom models on this framework, please check out this article from me.\n\nThe object detection framework is very powerful and you can use transfer learning to train your own model on a very small set of images (50\u2013100 images in this case). You can then run the model across a clip and it will mark out the location of the serial number. We were able to crop out these images, invert them , increase their contrast and save them for the next steps in the process. See some sample images below:\n\nAs you can see from the images above, the cropped out images even after some pre-processing are very low quality due to blurring and the thin font. Even one of the best OCR packages \u2014 Google Vision API struggles with input of this type. Vision API provides a very nice web interface to try the API quickly on sample images. Please check out this link .\n\nSee below the output of Vision API OCR on the sample above:\n\nAfter some experimentation, the final approach used was to train a second object detection model that could identify each of the individual numbers and letters in the serial number like 1,7,3,N,H,4,2,C etc. This second object detection model would go over the image and mark out each letter or number it could identify and finally we concatenate the resulting characters to form the complete string.\n\nSo why did this approach work?\n\nThe reason is that the model is trained on this specific data. By showing it many blurry pics of \u201c7\u201d we could train it on what \u201c7\u201d looks like in this situation.\n\nSee below the output from this second object detection model:\n\nSuch automated techniques can be deployed at manufacturing facilities to improve accuracy of packaging and reducing logistical and shipment errors. TensorFlow object detection offers the flexibility to deploy such solutions within a manufacturing facility so that real time predictions can be made.\n\nSpringML is a premier Google Cloud Platform partner with specialization in Machine Learning and Big Data Analytics. We have implemented predictive and analytic solutions at several fortune 500 organizations. Please get in touch to know more: info@springml.com, www.springml.com"
    },
    {
        "url": "https://towardsdatascience.com/building-a-next-word-predictor-in-tensorflow-e7e681d4f03f?source=user_profile---------2----------------",
        "title": "Building a Next Word Predictor in Tensorflow \u2013",
        "text": "Next Word Prediction or what is also called Language Modeling is the task of predicting what word comes next. It is one of the fundamental tasks of NLP and has many applications. You might be using it daily when you write texts or emails without realizing it.\n\nI recently built a next word predictor on Tensorflow and in this blog I want to go through the steps I followed so you can replicate them and build your own word predictor.\n\nI used the text8 dataset which is en English Wikipedia dump from Mar 2006. The dataset is quite huge with a total of 16MM words. For the purpose of testing and building a word prediction model, I took a random subset of the data with a total of 0.5MM words of which 26k were unique words. As I will explain later as the no. of unique words increases the complexity of your model increases a lot.\n\nIn NLP, one the first tasks is to replace each word with its word vector as that enables a better representation of the meaning of the word. For more information on word vectors and how they capture the semantic meaning please look at the blog post here.\n\nFor this model, I initialised the model with Glove Vectors essentially replacing each word with a 100 dimensional word vector.\n\nFor this task we use a RNN since we would like to predict each word by looking at words that come before it and RNNs are able to maintain a hidden state that can transfer information from one time step to the next. See diagram below for how RNN works:\n\nA simple RNN has a weights matrix Wh and an Embedding to hidden matrix We that is the shared at each timestep. Each hidden state is calculated as\n\nAnd the output at any timestep depends on the hidden state as\n\nSo using this architecture the RNN is able to \u201ctheoretically\u201d use information from the past in predicting future. However plain vanilla RNNs suffer from vanishing and exploding gradients problem and so they are rarely practically used. For this problem, I used LSTM which uses gates to flow gradients back in time and reduce the vanishing gradient problem.\n\nI set up a multi layer LSTM in Tensorflow with 512 units per layer and 2 LSTM layers. The input to the LSTM is the last 5 words and the target for LSTM is the next word.\n\nThe final layer in the model is a softmax layer that predicts the likelihood of each word. As I mentioned previously my model had about 26k unique words so this layer is a classifier with 26k unique classes! This is the most computationally expensive part of the model and a fundamental challenge in Language Modelling of words.\n\nThe loss function I used was sequence_loss. The model was trained for 120 epochs. I looked at both train loss and the train perplexity to measure the progress of training. Perplexity is the typical metric used to measure the performance of a language model. Perplexity is the inverse probability of the test set normalized by number of words. Lower the perplexity, the better the model is. After training for 120 epochs, the model attained a perplexity of 35.\n\nI tested the model on some sample suggestions. The model outputs the top 3 highest probability words for the user to choose from. See screenshot below. The model works fairly well given that it has been trained on a limited vocabulary of only 26k words\n\nSpringML is a premier Google Cloud Platform partner with specialization in Machine Learning and Big Data Analytics. We have implemented predictive and analytic solutions at several fortune 500 organizations. Please get in touch to know more: info@springml.com, www.springml.com"
    }
]