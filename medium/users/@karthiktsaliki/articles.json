[
    {
        "url": "https://medium.com/@karthiktsaliki/sentence-wise-embeddings-dealing-with-context-wise-disambiguation-s-58dd862769ee?source=user_profile---------1----------------",
        "title": "Sentence wise Embedding\u2019s dealing with context wise Disambiguation\u2019s",
        "text": "Sentence wise embedding is becoming the heart of research in the field of Natural Language processing. Right embedding for a sentence is the one which deals the context in which it is stated about. There are many ways in which we can get an embedding for the sentence. Let\u2019s start comparing the various techniques and at the end i will drive you people into the solution which gives the right embedding for the sentence.\n\nAs the name indicates the term frequency, you might have been guessing word \u2018term\u2019 might be the word in the sentence. Yes you are right it is dealing with words in general we call words as tokens. Main use case of TF-IDF is to extract the relevant keywords in the sentence. Give a proper weightage for the word in the corpus and a right representation in vector form. By combining all the vectors finally we get the embedding for the sentence.\n\nIs that provide the right embedding for the sentence. Take a second and think about it.\n\nThe answer is no. Why? let\u2019s verify by taking two sentences.\n\nB: In Java. We can write code once and run anywhere.\n\nWhat score do you give?\n\nBoth are similar in context. we expect the score to be high, but it gives a score less than 10 out of 100. Why? it is only considering the syntax not the semantics.\n\nIs there any thing which gives the semantic representation for word. Answer to question has been given by the google scientists as yes. What did they do?\n\nWhat does this word2vec do, for giving semantic representation of the word. Let\u2019s dive into word2vec in brief.\n\nVector representation for the particular word is given by considering the neighboring words which the particular word is surrounded with.\n\nI recommend to go through this research paper for good understanding on word2vec https://arxiv.org/abs/1301.3781\n\nWe can train using skipgram and cbow, both the algorithms have their specific use case.\n\nWill the collection of word embedding using word2vec will solve our problem. Let\u2019s Verify\n\nA: JDK is java development kit and JRE is java runtime environment.\n\nB: JDK is java runtime environment and JRE is java development kit.\n\nThe disadvantage here is even change in order is sentence is giving the same embedding for the sentence.\n\nNow Let\u2019s dive into the state of the art for sentence embedding\n\nSlight change in the embedding for a sentence with the previous method stated is, here we get the principal component and normalize all the word vectors. Will that solve the problem.\n\nThe answer is no, why? because we are facing the same problem the order of words are not considered while giving an embedding for the sentence.\n\nreference for the state of the art paper: https://www.youtube.com/watch?v=BCsOrewkmH4\n\nFinally we are into the solution which deals this problem uniquely\n\nWhat is this concept-net? Answer is it holds the relationship between the words when the relationship is considered the order is considered by neighboring words relationship with the center word.\n\nA: JDK is java development kit and JRE is java runtime environment.\n\nB: JDK is java runtime environment and JRE is java development kit.\n\nJDK is not closely related to java runtime environment and similar for JRE, so the embedding for the sentence is considering the context.\n\nGeneral question which i get asked from most of the people:\n\nWill RNN or sequence model work for sentence embedding. My answer is no. Why? because it cannot deal the sarcasm and ambiguity in the words we counter this problem more often in Natural Language processing.\n\nGive me a \u2764\ufe0f if you liked this post:)\n\nPS: I live in India and I am machine learning engineer (NLP). If you like my post and can connect me to anyone, I will be grateful :). My email is 7karthikchowdary@gmail.com"
    },
    {
        "url": "https://towardsdatascience.com/recurrent-neural-networks-6b67535550ca?source=user_profile---------2----------------",
        "title": "Constructing your own Recurrent Neural Network \u2013",
        "text": "Neural networks are a class of machine learning algorithms modeled after the human brain and recurrent neural networks are a subclass that work well with sequences of data, like text.\n\nWith a vanilla neural network you take a set of input data, pass it through the network, and get a set of outputs\n\nIn order to train Deep Learning models you need to know what the model should ideally output, which is often called your labels or target variables. The neural network compares the data it outputs with the targets and updates the network learns to better mimic the targets.\n\nRecurrent Neural Networks (RNNs) are useful in modeling sequential data, which involves a temporal pattern like text, image captioning, ICU patient data etc. It is a simple feed forward neural network with feedback. At each time step, based on the current input and past output, it generates new output. A simple Recurrent Neural Network architecture looks like:\n\nRNNs are much more flexible than a simple feedforward neural network. We can pass variable sized inputs to RNNs and even get a variable sized output. For example, an RNN can be modeled to learn binary addition bit by bit. It learns the state machine diagram of a binary adder. After training, we can just pass two inputs of any arbitrary size to it and it can generate the correct output without performing any addition operation! RNN is able to implicitly learn these semantics on its own.\n\nFor RNNs to work, a good amount of data is needed to find a good model, I tried various architectures of RNNs with varying number of layers, number of hidden units in each layer, sequence length and batch size. All of these hyper parameters should be tuned intelligently according to the dataset otherwise overfitting or under fitting may happen the results generated will be heavily dependent on the data, and since the data has been crawled from various websites, it may be offensive and inappropriate at times.\n\nThis is first stage where you initialize weights, You can initialize in three different ways.\n\n3) Xavier initialization: \n\nRandom values in the interval from [-1/\u221an,1/\u221an],\n\nwhere n is the number of incoming connections\n\nThe function of forward propagation to traverse through the end.\n\nFunction f usually is a nonlinearity such as tanh or ReLU\n\nFinding the loss is the important stage in training the loss function determines the state at which your model is let\u2019s try\n\nIt is the most popular way of optimizing your model, SGD is the way in which you send the inputs in batches instead of whole input, By this way you can easily scale your model.\n\nIt is the technique in which you traverse again back to start and you will update your weights based on your optimizer results."
    },
    {
        "url": "https://becominghuman.ai/convolution-neural-network-in-modular-approach-e3a0969cf825?source=user_profile---------3----------------",
        "title": "Convolution Neural Network In Modular Approach \u2013",
        "text": "Convolutional Neural Networks are very similar to ordinary Neural Networks . They are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.\n\nAs per the above figure.\n\nINPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.\n\nCONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\n\nRELU layer will apply an element wise activation function, such as the max(0,x)max(0,x) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).\n\nPOOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].\n\nFC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.\n\nThe CONV layer\u2019s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume.\n\nSometimes when the images are too large, we would need to reduce the number of trainable parameters. It is then desired to periodically introduce pooling layers between subsequent convolution layers. Pooling is done for the sole purpose of reducing the spatial size of the image. Pooling is done independently on each depth dimension, therefore the depth of the image remains unchanged. The most common form of pooling layer generally applied is the max pooling.\n\nAfter multiple layers of convolution and padding, we would need the output in the form of a class. The convolution and pooling layers would only be able to extract features and reduce the number of parameters from the original images. However, to generate the final output we need to apply a fully connected layer to generate an output equal to the number of classes we need. It becomes tough to reach that number with just the convolution layers. Convolution layers generate 3D activation maps while we just need the output as whether or not an image belongs to a particular class. The output layer has a loss function like categorical cross-entropy, to compute the error in prediction. Once the forward pass is complete the back propagation begins to update the weight and biases for error and loss reduction."
    },
    {
        "url": "https://medium.com/@karthiktsaliki/liferay-7-your-best-choice-for-a-digital-experience-platform-faacc68ee42?source=user_profile---------4----------------",
        "title": "Liferay 7: Your best choice for a digital experience platform.",
        "text": "Liferay 7 is the good choice for developing commercial web-based applications. It is supporting OSGi modular based environment which is an initiative for modular based application development.\n\nOSGi will provide ability to create application in several components and all components together work as application and each component is independent and much decoupled from other components.\n\nLiferay 7 have used Bootstrap 3.0 so that look and feel for portal and its applications are really going to be good.\n\nLiferay 7 integrated Elastic Search as default search where indexing can be managed across the shards so that pulling up the records are much faster, By which the performance of the website increases.\n\nThe most exciting feature in Liferay for me is the ADT application display templates which are written in free-marker language by which common components are grouped up in one single template and this template is rendered across all the web pages by which the redundancy is controlled.\n\nContent management of the website provided by the Liferay is very user friendly by which any changes in the content of the website can be done on the fly.\n\nFinally, the support for unit and integration test cases, Liferay 7 have given flexibility to use integration testing for portlets and OSGi plugins using Arquillian integration framework."
    },
    {
        "url": "https://medium.com/@karthiktsaliki/blueprint-for-a-new-economy-bde82d214fb2?source=user_profile---------5----------------",
        "title": "Blueprint for a New Economy \u2013 Karthik Tsaliki \u2013",
        "text": "A blockchain is a Distributed Database that is used to maintain a continuously growing list of records, called blocks. Each block contain a timestamp and a link to previous block. A blockchain is typically managed by a peer-to-peer network. What is stored on the blockchain need not be just a monetary unit \u2014 it can be put to all manner of other interesting uses.\n\nDistributed Cloud Storage: Current cloud storage services are centralized \u2014 thus you the users must place trust in a single storage provider. \u201cThey\u201d control all of your online assets. Using blockchain anyone on the internet can store your data at a pre-agreed price. Hashing and having the data in multiple locations are the keys to securing it.\n\nDigital Identity: The data breach at Target was significantly broader than originally reported: The company said that 70 million customers had information such as their name, address, phone number and e-mail address hacked in the breach. Blockchain technology offers a solution to many digital identity issues, where identity can be uniquely authenticated in an irrefutable, immutable, and secure manner.\n\nDigital Voting: The greatest barrier to getting electoral processes online, according to its detractors, is security. Using the blockchain, a voter could check that her or his vote was successfully transmitted while remaining anonymous to the rest of the world.\n\nSmart Contracts: These are legally binding programmable digitized contracts entered on the blockchain. What developers do is to implement legal contracts as variables and statements that can release of funds using the bitcoin network as a \u20183rd party executor\u2019, rather than trusting a single central authority. For example, if two people want to exchange $100 at a specific time in future when a set of preconditions are met, the conditions, payout, and parties\u2019 details would be programmed into a smart contract. Once the defined conditions are met, funds would be released and sent to the appropriate party as per terms. Thus block chain paves a way for a new economy."
    }
]