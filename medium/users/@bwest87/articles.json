[
    {
        "url": "https://towardsdatascience.com/building-a-deep-neural-net-in-google-sheets-49cdaf466da0?source=user_profile---------1----------------",
        "title": "Building a Deep Neural Net In Google Sheets \u2013",
        "text": "I want to show you that Deep Convolutional Neural Nets are not nearly as intimidating as they sound. And I\u2019ll prove it by showing you an implementation of one that I made in Google Sheets. It\u2019s available here. Copy it (use the File \u2192 Make a copy option in top left) , and you can then play around with it to see how the different levers affect the model\u2019s prediction.\n\nThe rest of the article will be a short intro to understand the high level intuitions behind Convolutional Neural Nets (CNN), and then some recommended resources for further information.\n\nBefore continuing, I\u2019d like to make a shout out to FastAI. I recently finished their wonderful deep learning course, and all inspiration and credit really goes to them. The fantastic instructor Jeremy Howard, with his co-founder Rachel Thomas showed the class the idea of doing a CNN in Excel. But as far as I could tell, the spreadsheet was not available online, and also didn\u2019t seem to fully complete the network. I\u2019m doing a small extension to their work, and putting it on Google Sheets so it\u2019s easier for everyone to play around with.\n\nI trained a (very) simple CNN on the MNIST data set, which is a bunch of black and white images of handwritten digits. Each image is 28x28 pixels. Each pixel gets represented as a number between 0 (no ink) and 1 (maximum ink). It\u2019s a classic data set to use because it\u2019s small enough to be quick, but real enough to show the complexity of machine learning. The job of the model is to determine what number the image is. Each image will always be of exactly one number 0\u20139.\n\nI trained the model using a popular deep learning library called Keras (see that code here), and then put the trained weights from my model into Sheets. The trained weights are just numbers. To put it into Sheets, this meant literally copying and pasting a bunch of numbers from my model into Sheets. The last step was adding formulas to replicate what the model does, which is just regular old multiplication and addition. Let me reiterate that: the math to replicate a deep learning model\u2019s predictions stops at multiplication and addition [1].\n\nThere are weights (a.k.a. \u201cparameters\u201d) in each layer of the model. Weights are automatically learned by any machine learning model. This model has around 1000 weights. More complex models can easily have hundreds of millions. You can see all 1000 of this model\u2019s weights below:\n\nYou use CNN\u2019s to find patterns in sequential data where you\u2019re pretty positive patterns exist, yet you find it hard to put those patterns into words, or to extract them through simple rules. CNN\u2019s assume order matters.\n\nFor example, classifying pictures are a prime use case for CNN\u2019s, because the pixels are logically sequential, and it\u2019s clear to any human that there\u2019s loads of patterns. Yet, just try putting into words exactly what separates a cat from a chihuahua, and you\u2019ll see why CNN\u2019s are useful.\n\nOn the other hand, if you\u2019ve got current baseball stats between two teams, and you want to predict the winner, then CNN\u2019s would be a weird choice. The data you have (eg. number of wins, losses, or team batting average) is not inherently sequential. Order doesn\u2019t matter there, and we have already extracted the patterns we believe to be useful. So CNN\u2019s won\u2019t be helpful.\n\nTo understand these beasts, let\u2019s break apart the Deep Convolutional Neural Net into it\u2019s constituent parts of \u201cDeep\u201d, \u201cConvolutional\u201d and \u201cNeural Net\u201d.\n\nImagine for a minute that you\u2019re blind. But your job is to figure out what digit this handwritten image is of. You\u2019re allowed to talk to someone who sees the image, but they have no idea what numbers are. So all you can ask them is simple questions. What could you do?\n\nOne approach you could take is to ask things like, \u201cIs it mostly straight at the top?\u201d, \u201cdiagonal from right to left?\u201d, etc. With enough questions like that, you could actually take a pretty good guess that it\u2019s a 7, or a 2, or whatever.\n\nIntuitively, that\u2019s what the convolutions are doing. The computer is blind, so it does what it can and asks lots of little pattern questions.\n\nTo ask those questions, every pixel in the image gets run through a function (a.k.a. \u201cconvolution\u201d) that produces a corresponding pixel, which answers one of those little pattern questions. Convolutions use filters to find patterns. For example, notice how the filter above (no. 2 in the screenshot) is more red on the right side, and less red on the left. That filter will essentially look for left edges.\n\nIt may not be obvious why it will find left edges, but play with the spreadsheet and you\u2019ll see for yourself that\u2019s how the math works out. Filters find things that look like themselves. And CNN\u2019s will often use hundreds of filters, so you get lots of little \u201cscores\u201d for each pixel, kind of like a left-edge score, top-edge score, diagonals, corners, etc.\n\nOk, so asking about edges is cool, but what about more complex shapes?\u201d This is where the \u201cdeep\u201d multiple layers thing comes in. Because now that we have a \u201cleft-edge\u201d, \u201ctop-edge\u201d, and other simple \u201cfilters\u201d of the image, we can add another layer, and run convolutions on all the previous filters, and combine them! So combining 50/50 a left edge and top edge could give you a rounded left corner. Cool, huh?\n\nSerious CNN\u2019s will have many layers, which allow the model to build up increasingly abstract and complex shapes. Even after only 4 or 5 layers, your model could start finding faces, animals, and all kinds of meaningful shapes.\n\nNow you might be asking yourself, \u201cSo that\u2019s all great, but coming up with all the right filters sounds really tedious.\u201d \u201cAnd what about at the end? How do I combine all the answers from those filters into something useful?\u201d.\n\nFirst it\u2019s useful to realize that at a high level, our CNN really has two \u201csections\u201d to it. The first section, the convolutions, finds useful features in our image data for us. The second section, the \u201cdense\u201d layers (so named because there are so many weights tying to every neuron) towards the end of the spreadsheet, do the classifying for us. Once you have the features, the dense layers really aren\u2019t that dissimilar from running a bunch of linear regressions and combining them into a score for each possible number. The highest score is the model\u2019s guess.\n\nFiguring out all the right weights to use for the filters and the Dense layers at the end would be really annoying. Luckily figuring out those weights automatically is kind of the entire point of NN\u2019s, so we don\u2019t need to worry about that. But if you\u2019re curious, you should google \u201cback propagation\u201d.\n\nThere\u2019s roughly two parts to every CNN. The convolutions, which always go at the beginning to find useful features in the image, and the layers at the end, often called \u201cdense\u201d layers, which classify things based on those features.\n\nTo get a real feel for them, I would encourage you to play with the spreadsheet. Trace a pixel from beginning to end. Mess with the filters, and see what happens. I also explain more technical details in the comments of the spreadsheet.\n\nTo learn more, I recommend the following resources:\n\nInteractive Convolutions \u2014 A killer interactive tutorial on convolutions (ie. just the C part, not the NN part) by Victor Powell.\n\nPractical Deep Learning for Coders \u2014 The course from Fast.AI which I took, and learned a lot from. It\u2019s online and completely free.\n\nGreat video showing basics of CNN \u2014 This is from Jeremy Howard (FastAI founder), and is a 20 minute video going over CNN\u2019s. It\u2019s excellent. The video is embedded in that page. Start from minute 21 when you open the video.\n\n[1] \u2014 The math required to train the CNN includes calculus, so it can automatically adjust the weights. But once the model is trained, it does actually only require multiplication and addition to do predictions. And in practice, the calculus is handled by whatever deep learning library you\u2019re using."
    },
    {
        "url": "https://medium.com/@bwest87/the-accelerating-importance-of-willpower-648fc9af73c1?source=user_profile---------2----------------",
        "title": "The Accelerating Importance of Willpower \u2013 blake west \u2013",
        "text": "That\u2019s former NAVY SEAL, Adm. William H. McRaven expounding on the virtues of willpower. Few people understand willpower quite the way SEAL\u2019s do. And while everyone gets that it\u2019s very important, it may be less obvious that willpower is actually rapidly increasing in importance because of the double edged sword that is technology. In fact, it will continue to increase at the rate that technology becomes more pervasive in our lives, which is to say faster and faster.\n\nFirst, a primer; willpower is the thing that lets you make good long-term decisions every day. The ability to go to the gym, not buy dessert, or save money. You have a very limited amount of willpower that dwindles throughout the day (this is clinically verified). Every decision takes willpower. Even every online ad you see shaves a bit more off your daily count when you decide not to click on it.\n\nLet\u2019s call what you have at the end of the day your \u201cnet willpower\u201d. That\u2019s what you have left to improve yourself, work on side projects, eat right, etc.\n\nOne edge of the tech sword is attacking your net willpower every day. Gaming companies are hiring psychology PhD\u2019s to help make their games more addictive. Amazon and Google use thousands of data points on you to make the ads as attractive as possible. Have you noticed the New York Times has a \u201csuggested\u201d article fly in from the right side when you near the end of the article you\u2019re reading? It\u2019s non-stop questions that you have to respond \u201cno\u201d to in order to get on with your day.\n\nBut technology is just a tool, so while the one edge is on a non-stop assault of your willpower, the other is cutting non-stop opportunities. Online education is better and more open than ever. Any book can be downloaded instantly. There are more events to know about, and more news to empower you, all for free, no matter where you are.\n\nThese facts suggest two interesting ideas. One is that as more and more willpower gets taken away from us daily, then your net willpower becomes ever more important. Let\u2019s say you start with 10 \u201cwillpower points\u201d every day. 5 years ago, life reduced that by 5, so your net will power was 5. But today, under new stresses, it gets reduced to 4. That\u2019s a 20% reduction. One more would be a 33% reduction. The next, 50%. Since the initial amount is likely to stay the same (people aren\u2019t changing), every extra bit that modern life takes away is a higher percentage of the net willpower than the last. This makes whatever you do have left all the more valuable.\n\nThe second point brings it all together. If net willpower supply keeps dwindling, then the percentage difference among people will increase (smaller numbers, same absolute difference). This will create a growing divide among people who have more willpower and people who don\u2019t. And then here\u2019s the real kicker, since there are ever more productive ways to spend your willpower at all times, the opportunity cost of your time is going up and up. This will further exacerbate the divide between those who use their willpower well, and those who don\u2019t.\n\nThinking historically, this is just a continuation of the trend that technology empowers those with the discipline (and resources) to take advantage. Before the lightbulb, it was tougher to read at night. Even if you were smart and ambitious, you may not have had the option to read after dark. Before A/C, everyone in the south mostly just stopped working in the afternoon because it was too hot. After A/C, you could stop working, but that was a choice. Some people took the break, others kept working. I bet I know who found more long-term success.\n\nBut the good news is that willpower is indeed like a muscle. It can be trained; SEAL\u2019s prove that. And like the SEAL\u2019s making their bed, creating good habits can spill over to many other aspects of your life. The magic is that once you form good habits, they no longer require the use of willpower, and you\u2019re free to use it elsewhere. Our best defense against the modern assault on willpower \u2014 and our best use of modern technology \u2014 is to form habits for the things we know are truly important.\n\nInterestingly, one of the god father\u2019s of our modern technology, Steve Jobs, once spoke to the power of habits when he referenced an old hindu proverb in an interview from the 80's. \u201cFor the first 30 years of your life, you make your habits. For the last 30 years\u201d, he said, \u201cyour habits make you.\u201d"
    },
    {
        "url": "https://byrslf.co/how-to-slow-down-time-38cdcd2a8034?source=user_profile---------3----------------",
        "title": "How To Slow Down Time \u2013",
        "text": "A french chronobiologist (he studies time and living things) named Michael Siffre once conducted a hell of a self experiment. He spent two months in a cave alone. No clock, calendar, or sun light. He did have artificial lights, food, and a journal though. His experiment is covered in the excellent book \u2018Moonwalking With Einstein\u2019. It makes some pertinent points more eloquently than I could, so I will quote it at length right now.\n\nI remember the first time I read that passage. I sat on my couch and soaked in the implications. I had visions for how it might change my daily life. Over the months, however, the ideas from that passage became \u2014 as ideas are wont to do \u2014 rather intellectual. But recently I had an experience that brought them to life again. I understood them personally and viscerally.\n\nFor 13 weeks, I averaged 12 hours each day focused on one goal. Specifically, it was spent learning software, but it could have been anything. Painting, accounting, writing; any deep pool of knowledge would do. What I learned is that the passage of time isn\u2019t constant. In fact, it\u2019s under your control. You just have to pull the right levers.\n\nThere\u2019s a lay theory that as you get older, time seems to speed up because each passing year is a smaller and smaller percentage of your life. That the year from 9 to 10 represents 10% of your life, but the one from 33 to 34 is only about 3% of your life. It\u2019s intuitively appealing because it explains the universal feeling in a way that puts no fault on ourselves. It\u2019s just math, nothing I could have done to stop that inevitable speed-up of time. But I don\u2019t buy it anymore.\n\nYour sense of time is actually answered by a simple question: how much are you learning? Each thing you learn is another \u2018hook\u2019 you give to your memory, and each hook is another time marker. When you have a lot of markers, time feels slow. When you have few, life starts to feel a lot like driving down the open, straight roads of the midwest. Hundreds of miles will fly by because you don\u2019t have to do anything except keep your foot on the gas. The sad part is you still feel like you\u2019re going somewhere.\n\nWhile I was at Hack Reactor (an intense programming school), I was learning a metric ton each day. I felt so disconnected from other responsibilities that it felt like a different world. We had a 3-week personal project period where I was given (basically) free reign to build anything I wanted. I built a tool that visualizes, in 3D, a two-handed piano performance of any song, given a MIDI file. It required some complex algorithms, and new technologies I hadn\u2019t used before. I was working on it so much that my brain had no choice but to show it in my dreams each night for 3 weeks. I also learned more in those three weeks than about any other time in my life.\n\nAnd here\u2019s the paradox: while in these periods of \u2018flow\u2019, time moves fast. The hours cruise by, and your 14 hour day of work feels like a pittance. And yet, I look back for even a second, and it feels very slow. I have so many memories and associations for those 3 weeks that it feels more like 3 months. And that was the pattern at Hack Reactor. In fact, after a few weeks, we were so deep and disconnected that fellow Hack Reactants and I would joke that we had no lives before HR; and \u2018this is all that has ever been.\u2019\n\nWhich is funny, because there\u2019s the old adage about cherishing time, \u201cThe days are long, but the years are short\u201d, but, actually, it seems when you do things right, it\u2019s the opposite: \u201cthe days are short, but the years are long\u201d.\n\nThis theory also explains the child/adult time gap phenomenon. As kids, we all learned a lot each day. New words, new skills, new social rules. As you become an adult and leave school, that slows down. Certainly we learn, but less so than before, so time flows faster.\n\nSo how do you slow down time? A clich\u00e9 would be dangerously easy here. \u2018Live life to its fullest\u2019 or some garbage like that. But dig in a bit, and it goes deeper than that. Your perception of life is built exclusively from your memories, which are in turn built from surprising experiences. I purposely didn\u2019t say \u2018novel\u2019, because hell, your commute to work each day is technically novel, since I guarantee you don\u2019t see exactly the same people on the way, or hear exactly the same broadcast, etc. But surprising captures the reality better. You must have experiences where you aren\u2019t totally sure what the results of your actions might be. Where maybe, you\u2019re even a bit uncomfortable. Such experiences often happen when learning a new skill, or traveling, but they can happen everyday in the strangest of times if you go after them.\n\nFor instance, there\u2019s a bakery just around the corner from my house. I\u2019ve always heard that bakeries tend to just give stuff away at the end of the day, because they\u2019re gonna throw it out anyway. I didn\u2019t know for sure though, partly because I never go to bakeries, but also because I\u2019m shy about asking for things that shouldn\u2019t obviously be mine. But I was walking home late at night the other day, and I saw the bakery owners closing up shop. The lights were on, but the door was locked. I thought, hell, why not try this out? I knocked and pointed at the doughnuts. They were hesitant, but opened up, and asked in broken English, \u201cWhat you want?\u201d I asked for 2 chocolate doughnuts. They put in 3 and threw in 2 bear claws too for free. It was the most exhilarating doughnut purchase of my life. I finally learned the truth of this \u2018free baked goods myth\u2019. It was true, and I didn\u2019t even have to ask. I did have to knock though.\n\nYa know, fruits and veggies are great, and I eat them every day so that I\u2019ll stick around. But that night, I think the doughnuts lengthened my life more."
    }
]