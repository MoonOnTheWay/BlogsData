[
    {
        "url": "https://medium.com/@surmenok/self-driving-cars-are-new-rocket-science-451dcb14d825?source=user_profile---------1----------------",
        "title": "Self-driving Cars are New Rocket Science \u2013 Pavel Surmenok \u2013",
        "text": "Huge complexity is in algorithms for computer vision, path planning, control. Many of these problems require state-of-the-art deep learning algorithms and large amounts of high-quality training data. But safety is a key. If one sensor fails, there should be other sensors to get the information needed. If a computer fails, it still should do some work to ensure safety. If any hardware or software fault happens, the car should continue safe operation. Hardware and software should conform to ASIL D level of the automotive safety and integrity standard. ASIL D \u201crefers to the highest classification of initial hazard (injury risk) defined within ISO 26262 and to that standard\u2019s most stringent level of safety measures to apply for avoiding an unreasonable residual risk.\u201d\n\nHardware for computation is an important part. The computation requirements are enormous. The car should handle streams of data from multiple sensors: high-resolution cameras, LIDARs, radars, sonars, GPS. It needs to localize the vehicle within centimeter accuracy, detect cars, pedestrians, bicyclists and other objects on the road, estimate their velocities, plan a safe path for the car and control the car hardware."
    },
    {
        "url": "https://towardsdatascience.com/resnet-for-traffic-sign-classification-with-pytorch-5883a97bbaa3?source=user_profile---------2----------------",
        "title": "ResNet for Traffic Sign Classification With PyTorch",
        "text": "German Traffic Sign Recognition Benchmark dataset is probably the most popular image classification related to self-driving cars. Autonomous vehicles need to detect and classify traffic signs to understand traffic rules applied to a segment of the road. Perhaps, this dataset is too small and incomplete to use it for real applications. Nevertheless, it is a good benchmark of computer vision algorithms.\n\nThe dataset consists of two parts: a training set and a test set. The training set contains 39209 images of traffic signs classified into 43 classes, such as stop sign, bicycles crossing, and speed limit 30 km/h.\n\nThe dataset is very imbalanced. For example, there are 1800 instances of \u201cspeed limit (50 km/h)\u201d sign, but just 168 instances of \u201cdangerous curve to the left\u201d sign.\n\nThe test set has 12630 labeled images. These images were used for evaluation in competition at IJCNN 2011.\n\nYou can download the dataset from the official website.\n\nI tried transfer learning using a ResNet34 convolutional neural network pretrained on the ImageNet dataset.\n\nThe general approach to solving computer vision problems that I learned in the latest version of \u201cDeep Learning for Coders\u201d course at fast.ai. I attended the offline version of that course at the University of San Francisco last year. The course uses fastai, a deep learning library built on top of PyTorch. It provides easy to use building blocks for training deep learning models.\n\nI spent most of the time optimizing hyperparameters and tuning image augmentation.\n\nI published my code on GitHub. You can download a Jupyter notebook that contains all steps from downloading the dataset to creating a submission file based on the unlabeled test set. The code for training the CNN model is mostly based on CNN lessons of the fast.ai course.\n\nLet\u2019s go through the steps to train and evaluate the model.\n\nPrepare the environment. I had to install the fastai library and all its dependencies.\n\nDownload the dataset, unpack it. Split the training set (39209 images) into training and validation sets and move files to correct folders. I used 80% of samples for training, 20% of samples for validation.\n\nBe careful with splitting. The dataset contains series of 30 photos for each physical traffic sign. Distinguishing the series from each other based on filenames is easy. If you just split the dataset randomly, then there will be leakage of information from the validation set to the training set.\n\nI made this mistake in the beginning. I split the dataset randomly and got amazingly good validation accuracy over 99.6%. I was surprised when the test accuracy was just 87% :) Large difference between test accuracy and validation accuracy is a sign of a poorly designed validation set or overfitting to the validation set.\n\nThe right way is to find the series of images and put each series entirely into the training or validation sets, make sure it\u2019s not split between two. To learn more about creating a good validation set, read this article by Rachel Thomas.\n\nExplore the dataset. Check distribution of classes, look at a few examples of images from each class.\n\nImages have different sizes. Look at the histogram of sizes. It will give you an insight on what the input dimensions for the CNN should be.\n\nLoad a ResNet34 model pretrained on the ImageNet dataset. Remove the last layer and add a new softmax layer on top.\n\nMy general approach to training: start from a small model input (I started from 32x32 image size) and a short training procedure (7 epochs total) to optimize for training speed. You need to iterate fast. Ideally, an experiment shouldn\u2019t take more than few minutes.\n\nAlso, optimize batch size. Try to make batch size as large as GPU memory allows. Larger batch sizes help to reduce training time. But, experimentally, I found that too large batch sizes (e.g., 1024 samples and more) lead to lower validation accuracy. I guess the model starts overfitting early. I ended up with a batch size 256.\n\nOnly after you find a decent set of hyperparameters, switch to larger images and longer fine-grained training. I ended up using 96x96 images and training for 19 epochs.\n\nSet up image augmentation. It is a technique that helps the model to generalize better. You add a lot of artificial examples to the training set. These examples are based on already existing ones, but you change them a little bit: rotate a few degrees, change lighting, zoom in, etc.\n\nI used a combination of the following transformations: rotation up to 20 degrees, lighting change up to 80%, and zoom up to 20%.\n\nLighting augmentation is quite important. In early phases of the project, I\u2019ve seen that very dark images have the most incorrect predictions. Aggressive in lighting augmentation improves validation accuracy by more than 3%. Lighting change is done by changing values of R, G and B channels directly. See RandomLighting class for details.\n\nOther things I\u2019ve tried and rejected: histogram equalization to improve contrast, random blurring, padding.\n\nSearch for a good starting learning rate for training using a simple algorithm described here.\n\nFreeze all the layers except the last one. Train the model for one epoch with that learning rate. In my case, the learning rate is 0.01. The goal here is to get reasonable weights for the last layer. If we don\u2019t do that, training an unfrozen model later will lead to messing up lower layers because gradients will be larger. I tried both options and training the last layer for one epoch gives about 1% improvement in validation accuracy. I also used weight decay that makes a small improvement.\n\nUnfreeze all layers. Train for three epochs.\n\nThen train using stochastic gradient descent with warm restarts (SGDR) for a few epochs.\n\nI tried to use discriminative fine-tuning that sets different learning rates to different parts of the model. In this case, we want to train the first layers of the model less than the last layers. The first layers are more generic than others. When trained on ImageNet dataset, these layers learned patterns that are very useful for our task, and we don\u2019t want to lose this knowledge. On the other hand, the last layers are very task-specific, and we want to retrain them on our task. Unfortunately, it didn\u2019t help to improve metrics. The model trains much better if you apply a large learning rate to all layers. I guess it is because traffic signs are very different from dogs, cats and airplanes, hence information in the lower layers is not as useful as in other computer vision applications.\n\nThe accuracy of the best model on the validation set is 99.0302%.\n\nBesides usual tools like a confusion matrix, you can analyze errors by examining a few extreme cases: most incorrect predictions, most correct predictions, most uncertain predictions.\n\nTo find the most incorrect predictions for each class, you must run inference on the validation set and select examples where the predicted probability of the correct class is the smallest.\n\nThese images look too blurry and too bright.\n\nSimilarly, you can find examples where the highest probability assigned to the correct class (\u201cmost correct\u201d) and examples where the probability of the correct class is close to 1/num_classes (\u201cmost uncertain\u201d).\n\nResults of this analysis help you to adjust the image augmentation parameters and, probably, some hyperparameters of the model.\n\nIn all the steps before, we used 80% of the training set for training and 20% for validation. Now, as we found good hyperparameters, we don\u2019t need the validation set anymore and can add these 20% images to the training set to improve the model a bit more.\n\nHere I just rerun all the training steps with the same parameters but using all 32909 training images for training.\n\nThe test set (12630 images) is meant to test the performance of the final model. We didn\u2019t look at the test set in previous steps to avoid overfitting to the test set. Now, we can evaluate the model on the test set. I got 99.2953% accuracy on the test set. Pretty good! Can we improve it further?\n\nTest-time augmentation (TTA) often helps to boost accuracy a bit more. The trick is to create a few augmented versions of the input image, run prediction on each of them, and then average results. The intuition behind this is that the model might be wrong in classifying some images but changing the image a bit can help the model to classify it better. It is like if a human wants to classify an object and they look at it from different angles, change lighting a bit, moving it closer to the eyes, until they can find the best viewpoint that helps to recognize the object with the greatest confidence.\n\nIndeed, TTA helped me to increase the accuracy from 99.2953% to 99.6120%. It reduced the error by 45% (from 0.7047% to 0.388%).\n\nAccuracy on the test set is 99.6120%. Let\u2019s compare with a few benchmarks.\n\nState of the art is Inception-based CNN by Mrinal Haloi. 99.81%. The error rate is twice better than mine.\n\nTop places on the leaderboard for the competition at IJCNN 2011:\n\nIf my model were in that competition, it would take the 2nd place. Overall, not bad for a few days of work."
    },
    {
        "url": "https://medium.com/@surmenok/remote-operators-for-self-driving-cars-f9960ab201cd?source=user_profile---------3----------------",
        "title": "Remote Operators for Self-Driving Cars \u2013 Pavel Surmenok \u2013",
        "text": "It seems to me, the largest obstacle to get this working will be qualities of communication channels.\n\nDo we have enough mobile network coverage to have 100% connectivity for major use cases like urban driving and highway driving?\n\nWhat bandwidth will we need to give the human operator enough information to make decisions? At least, the operator will need a high definition video stream from a few cameras. Data from radars, sonars, lidars, and telemetry from other vehicle sensors won\u2019t hurt either. Perhaps, some of this can be pre-processed an compressed on the vehicle, but even after compression, it will, probably, be over 1 megabyte per second.\n\nLatency is important as well. Currently, U.S. mobile networks have average latencies (measured as time to ping google.com) in the range about 80\u2013170 milliseconds. What would the impact be if we add this time to the average driver reaction time?"
    },
    {
        "url": "https://medium.com/@surmenok/my-learning-and-conferences-for-the-next-few-months-e3a60f6b49d7?source=user_profile---------4----------------",
        "title": "My Learning and Conferences for the Next Few Months",
        "text": "Then I\u2019ll visit NVIDIA GPU Technology Conference in San Jose. I bought an exhibit pass, so I\u2019ll be there only at night on March 27\u201328, including the party on March 28.\n\nThen I got a pass to TensorFlow Dev Summit that will be held in Mountain View on March 30.\n\nThen the SuperBot conference in San Francisco on April 3."
    },
    {
        "url": "https://medium.com/@surmenok/getting-started-with-nvidia-jetson-tx2-5952a2d8d7ae?source=user_profile---------5----------------",
        "title": "Getting Started with NVIDIA Jetson TX2 \u2013 Pavel Surmenok \u2013",
        "text": "I\u2019ve been playing with Jetson for a couple of days. My first objective is to flash it with all required software to be able to run Python programs that access camera input using OpenCV. Something simple: just get a video stream from a USB camera and output it to the screen.\n\nWhy I spent two days on this? First, it\u2019s Linux, spending few days compiling libraries, installing dependencies and figuring out compatibility of different libraries is a typical experience when setting up a dev environment on Linux. I\u2019m comparing it with Windows, where developer life is easier, perhaps, because Microsoft provides most developer tools, and you know that Microsoft builds great developer tools. Second, it\u2019s ARM. Many libraries don\u2019t have packages compiled for ARM architecture, so you would need to find where to get them or compile from source. Third, it\u2019s cutting-edge technology where hardware, libraries, and frameworks are updates very often. With that breakneck speed, developers just don\u2019t have time to rapidly update their software to be compatible with the latest version of everything else.\n\nBelow, I describe what I\u2019ve tried to do, what issues I encountered and how I resolved them."
    },
    {
        "url": "https://towardsdatascience.com/2017-review-4caaa19e308d?source=user_profile---------6----------------",
        "title": "2017 Review \u2013",
        "text": "I completed the new version of fast.ai Deep Learning course. I attended it in person at the University of San Francisco; it was a great learning experience. This course is very helpful for the understanding of how to use deep learning in practice. There are tons of little tricks, so-called \u201cblack magic of deep learning,\u201d that is not covered by textbooks or typical ML courses in universities or on MOOC platforms. This course makes emphasis on uncovering the magic of deep learning and \u201cmaking neural networks uncool again.\u201d\n\nI went to a few ML-related conferences. The largest was NVIDIA GPU Technology Conferenc (Silicon Valley). AI By The Bay was interesting. Smaller ones were good too: TensorBeat, AI Tech Forum. And one Python dev conference: PyBay.\n\nAlso, went to a countless number of meetups. The best meetups were \u201cEngineering Leadership talks by Facebook Executives\u201d and a series of ML engineering meetups at Twitter HQ."
    },
    {
        "url": "https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0?source=user_profile---------7----------------",
        "title": "Estimating an Optimal Learning Rate For a Deep Neural Network",
        "text": "The learning rate is one of the most important hyper-parameters to tune for training deep neural networks.\n\nIn this post, I\u2019m describing a simple and powerful way to find a reasonable learning rate that I learned from fast.ai Deep Learning course. I\u2019m taking the new version of the course in person at University of San Francisco. It\u2019s not available to the general public yet, but will be at the end of the year at course.fast.ai (which currently has the last year\u2019s version).\n\nDeep learning models are typically trained by a stochastic gradient descent optimizer. There are many variations of stochastic gradient descent: Adam, RMSProp, Adagrad, etc. All of them let you set the learning rate. This parameter tells the optimizer how far to move the weights in the direction of the gradient for a mini-batch.\n\nIf the learning rate is low, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny.\n\nIf the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse.\n\nThe training should start from a relatively large learning rate because, in the beginning, random weights are far from optimal, and then the learning rate can decrease during training to allow more fine-grained weight updates.\n\nThere are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc. When we start training with a large learning rate, the loss doesn\u2019t improve and probably even grows while we run the first few iterations of training. When training with a smaller learning rate, at some point the value of the loss function starts decreasing in the first few iterations. This learning rate is the maximum we can use, any higher value doesn\u2019t let the training converge. Even this value is too high: it won\u2019t be good enough to train for multiple epochs because over time the network will require more fine-grained weight updates. Therefore, a reasonable learning rate to start training from will be probably 1\u20132 orders of magnitude lower.\n\nLeslie N. Smith describes a powerful technique to select a range of learning rates for a neural network in section 3.3 of the 2015 paper \u201cCyclical Learning Rates for Training Neural Networks\u201d .\n\nThe trick is to train a network starting from a low learning rate and increase the learning rate exponentially for every batch.\n\nRecord the learning rate and training loss for every batch. Then, plot the loss and the learning rate. Typically, it looks like this:\n\nFirst, with low learning rates, the loss improves slowly, then training accelerates until the learning rate becomes too large and loss goes up: the training process diverges.\n\nWe need to select a point on the graph with the fastest decrease in the loss. In this example, the loss function decreases fast when the learning rate is between 0.001 and 0.01.\n\nAnother way to look at these numbers is calculating the rate of change of the loss (a derivative of the loss function with respect to iteration number), then plot the change rate on the y-axis and the learning rate on the x-axis.\n\nIt looks too noisy, let\u2019s smooth it out using simple moving average.\n\nThis looks better. On this graph, we need to find the minimum. It is close to lr=0.01.\n\nJeremy Howard and his team at USF Data Institute developed fast.ai, a deep learning library that is a high-level abstraction on top of PyTorch. It\u2019s an easy to use and yet powerful toolset for training state of the art deep learning models. Jeremy uses the library in the latest version of the Deep Learning course (fast.ai).\n\nThe library provides an implementation of the learning rate finder. You need just two lines of code to plot the loss over learning rates for your model:\n\nThe library doesn\u2019t have the code to plot the rate of change of the loss function, but it\u2019s trivial to calculate:\n\nNote that selecting a learning rate once, before training, is not enough. The optimal learning rate decreases while training. You can rerun the same learning rate search procedure periodically to find the learning rate at a later point in the training process.\n\nI haven\u2019t seen ready to use implementations of this learning rate search method for other libraries like Keras, but it should be trivial to write. Just run the training multiple times, one mini-batch at a time. Increase the learning rate after each mini-batch by multiplying it by a small constant. Stop the procedure when the loss gets a lot higher than the previously observed best value (e.g., when current loss > best loss * 4).\n\nSelecting a starting value for the learning rate is just one part of the problem. Another thing to optimize is the learning schedule: how to change the learning rate during training. The conventional wisdom is that the learning rate should decrease over time, and there are multiple ways to set this up: step-wise learning rate annealing when the loss stops improving, exponential learning rate decay, cosine annealing, etc.\n\nThe paper that I referenced above describes a novel way to change the learning rate cyclically. This method improves performance of convolutional neural networks on a variety of image classification tasks.\n\nPlease send me a message if you know other interesting tips and tricks for training deep neural networks."
    },
    {
        "url": "https://hackernoon.com/fast-ai-what-i-learned-from-lessons-1-3-b10f9958e3ff?source=user_profile---------8----------------",
        "title": "Fast.ai: What I Learned from Lessons 1\u20133 \u2013",
        "text": "Fast.ai is a great deep learning course for those who prefer to learn by doing. Unlike other courses, here you will build a deep neural network that achieves good results in an image recognition problem in an hour. You start from working code and then dig deeper into the theory and the ways to improve your algorithm. Authors of the course are Jeremy Howard and Rachel Thomas.\n\nI had prior knowledge of machine learning in general and neural networks in particular. I completed Andrew Ng\u2019s Machine Learning course on Coursera, read a lot of articles, papers, and books, including parts of the Deep Learning book, and I\u2019ve been building machine learning applications at work for more than a year. Nevertheless, I learned a lot from fast.ai lessons. When you read books or papers, you learn a lot about the mathematics of deep learning, and little about how to use them in practice. Fast.ai is very helpful to learn practical aspects: how to split the dataset, how to choose good values of hyperparameters, how to prevent overfitting."
    },
    {
        "url": "https://hackernoon.com/gamers-paid-for-deep-learning-and-self-driving-cars-research-b9a7834e531e?source=user_profile---------9----------------",
        "title": "Gamers Paid for Deep Learning and Self-Driving Cars Research",
        "text": "Simulation is another example of leveraging software developed by the gaming industry to fuel artificial intelligence progress. For example, when you develop self-driving car software, you need to test it on a large variety of situations. Most of the real driving is uneventful. Interesting events, like a road construction, a car going against traffic, or a mountain lion running into the road, are rare. If you collect data and test your software only in the real world, it will take very long time and cost a lot of money, because self-driving cars and time of test engineers are expensive. If some interesting case appears only once every 1000 hours of driving, you will have to drive a lot to collect enough data about it.\n\nOne way to make it more efficient is to simulate interesting situations in the real world. Companies like Waymo, Uber, Tesla set up test sites with roads, road signs, traffic lights, and have humans driving around to create traffic that looks real. Then they can set up situations like the car stopping suddenly ahead of you, pedestrians running into the road, etc. That also takes time and resources, but at least you can test exactly what you want to test and can repeat the test case as many times as you want.\n\nWhat if you can make a software simulation of the world? Then you can control everything and run tests faster, as fast as your computation hardware allows it. You can scale computation by adding more GPUs or more servers. You can test any exotic situations with great precision. In the time that it takes a real car fleet to drive a thousand miles, your simulation can drive millions of virtual miles. Software Development Times published a piece on using game engines for self-driving car simulation.\n\nThere are challenges here, of course. You need a good physics engine. Maps should look real. Ideally, you would need to recreate good copies of real cities with same traffic signs, lane markings, buildings. You need to simulate signals for all kinds of sensors: cameras, lidars, radars, etc. A lot of interesting challenges, but in the end, you get much faster experimentation. Also, it gives you an ability to do regression testing of your software at scale. When you release a new version of the software, you can test it on millions of interesting test cases, including rare edge cases.\n\nThe good thing is that you probably don\u2019t have to build everything from scratch. Gaming industry already built a lot of low-level functionality into game engines. For example, Udacity built a car simulator on top of Unity engine for its self-driving car nanodegree program (see details in an article by Naoki Shibuya). Others are using Unreal Engine 4 (e.g., UE4Sim simulator). Even Grand Theft Auto can be used to test simple algorithms. DeepDrive project used GTA V for simulation, but, unfortunately, it was shut down for legal reasons."
    },
    {
        "url": "https://medium.com/@surmenok/udacity-flying-car-nanodegree-program-d949dc16459b?source=user_profile---------10----------------",
        "title": "Udacity Flying Car Nanodegree Program \u2013 Pavel Surmenok \u2013",
        "text": "The program will start in early 2018. Price wasn\u2019t announced yet. The curriculum will take two terms.\n\nNicholas Roy is building the curriculum in collaboration with Sebastian Thrun (CEO of the Kitty Hawk Corporation, co-founder of Udacity), Raff D\u2019Andrea (co-founder of Kiva Systems), and Angela Schoellig (professor at the University of Toronto Institute for Aerospace Studies).\n\nMore about the projects."
    },
    {
        "url": "https://medium.com/@surmenok/new-gps-chip-with-30cm-accuracy-c04e2ffe1c33?source=user_profile---------11----------------",
        "title": "New GPS Chip With 30cm Accuracy \u2013 Pavel Surmenok \u2013",
        "text": "Broadcom announced a new GPS chip that will give 30-centimeter accuracy instead of today\u2019s 5 meters.\n\nThey are able to achieve better accuracy because of the new GPS L5 signal. It is a civilian \u201csafety of life\u201d signal, designed to provide means of secure and robust navigation enough for life-critical applications, like aircraft precision approach guidance. GPS, QZSS (Japanese satellite system) and Galileo (European satellite system) started deploying satellites with L5 support in 2011, and now there are enough satellites in space to start using this functionality.\n\nIt should have an impact on self-driving cars. Autonomous vehicles must know precisely where they are, relative to a high-definition map. Using L5 together with together with older L1 and L2 signals gives an order of magnitude improvement in navigation accuracy."
    },
    {
        "url": "https://becominghuman.ai/jeff-deans-talk-on-large-scale-deep-learning-171fb8c8ac57?source=user_profile---------12----------------",
        "title": "Jeff Dean\u2019s Talk on Large-Scale Deep Learning \u2013",
        "text": "A large area of research is learning to learn.\n\nDeep learning talent is scarce and expensive. Computation gets cheaper every year. That makes sense to try to automate this work. Google is trying 2 approaches: learning model architectures and learning optimizers.\n\nThe idea for architecture search is to use reinforcement learning. Generate ten models, train them for a few hours, use the loss of generated models as reinforcement learning signal. This work appeared at ICLR 2017. A preprint is on Arxiv. A model learned using this algorithm did well on CIFAR-10 image recognition task and Penn Tree Bank language modeling task. So far, this approach is too computationally expensive and is feasible only for small models, but over time it will be able to handle larger models.\n\nGoogle researchers used reinforcement learning to train optimizers. A recurrent neural network constructs a weight update function from a few primitives, trains a child model using this optimizer, and uses accuracy as reinforcement learning signal. A few optimizers learning with this approach achieved better results than popular handcrafted optimizers like SGD, Momentum, ADAM and RMSProp. See a preprint on Arxiv."
    },
    {
        "url": "https://towardsdatascience.com/contextual-bandits-and-reinforcement-learning-6bdfeaece72a?source=user_profile---------13----------------",
        "title": "Contextual Bandits and Reinforcement Learning \u2013",
        "text": "\u201cBandit\u201d in \u201cmulti-armed bandits\u201d comes from \u201cone-armed bandit\u201d machines used in a casino. Imagine that you are in a casino with many one-armed bandit machines. Each machine has a different probability of a win. Your goal is to maximize total payout. You can pull a limited number of arms, and you don\u2019t know which bandit to use to get the best payout. The problem involves exploration/exploitation tradeoff: you should balance between trying different bandits to learn more about an expected payout of every machine, but you also want to exploit the best bandit you know about more. This problem has many real-world applications, including website optimization, clinical trials, adaptive routing and financial portfolio design. You can think about it as smarter A/B testing.\n\nThe multi-armed bandit algorithm outputs an action but doesn\u2019t use any information about the state of the environment (context). For example, if you use a multi-armed bandit to choose whether to display cat images or dog images to the user of your website, you\u2019ll make the same random decision even if you know something about preferences of the user. The contextual bandit extends the model by making the decision conditional on the state of the environment.\n\nWith such model, you not only optimize decision based on previous observations, but you also personalize decisions for every situation. You will show an image of a cat to a cat person, and an image of a dog to a dog person, you may show different images at different times of the day and days of the week.\n\nThe algorithm observes a context, makes a decision, choosing one action from a number of alternative actions, and observes an outcome of that decision. An outcome defines a reward. The goal is to maximize average reward.\n\nFor example, you can use a contextual bandit to select which news article to show first on the main page of your website to optimize click through rate. The context is information about the user: where they come from, previously visited pages of the site, device information, geolocation, etc. An action is a choice of what news article to display. An outcome is whether the user clicked on a link or not. A reward is binary: 0 if there is no click, 1 if there is a click.\n\nIf we would have reward values for every possible action for every example, we could just use any classification algorithm, using context data as features and action with the best reward as a label. The challenge is that we don\u2019t know which action is the best, we have only partial information: reward value for the action which was used in the example. You still can use machine learning models for this, but you\u2019ll have to change the cost function. A na\u00efve implementation is to try to predict the reward."
    },
    {
        "url": "https://medium.com/@surmenok/open-ai-deep-learning-for-dota-2-160ed762100?source=user_profile---------14----------------",
        "title": "Open AI Deep Learning for DotA 2 \u2013 Pavel Surmenok \u2013",
        "text": "OpenAI\u2019s bot is the first ever to defeat world\u2019s best players in DotA 2 at The International 2017. It is a major step for AI in eSports. The bot was trained through self-play, but some tactics were hardcoded.\n\nThe bot doesn\u2019t play DotA in regular 5v5 setup. It can only beat humans in 1v1 play. Team work will be harder to learn. Also, the bot plays only one character and has a few unfair advantages comparing to human players, e.g. it\u2019s likely to have access to exact information such as distance to other players on the map and health."
    },
    {
        "url": "https://medium.com/@surmenok/best-sources-of-deep-learning-news-fbc98815bad3?source=user_profile---------15----------------",
        "title": "Best Sources of Deep Learning News \u2013 Pavel Surmenok \u2013",
        "text": "The field of deep learning is very active, arguably there are one or two breakthroughs every week. Research papers, industry news, startups, and investments. How to keep up with the news?"
    },
    {
        "url": "https://medium.com/@surmenok/large-text-classification-datasets-765cc40fece7?source=user_profile---------16----------------",
        "title": "Large Text Classification Datasets \u2013 Pavel Surmenok \u2013",
        "text": "Data is the most important component for building a machine learning model. Recently researchers from Google trained a CNN model for image classification on 300 million images and they demonstrated that even on a scale of hundreds of millions of examples adding more data helps to improve the model performance. Apparently, more data is better. But where can you get large datasets if you are doing research on text classification?\n\nI found nice references to a few large text classification datasets in \u201cText Understanding from Scratch\u201d paper by Xiang Zhang and Yann LeCun. The paper describes a character-level CNN model for text classification. Authors provide benchmarks of different CNN architectures and a few simple models on a few datasets. More recent version of this paper: \u201cCharacter-level Convolutional Networks for Text Classification\u201d contains more experimental results but it misses some details on dataset usage: which fields to use, how to truncate long texts, etc. If you are looking for information about datasets, read the older paper. If you want to learn more about the character level CNN models, read the latest paper.\n\nSomebody uploaded the datasets to Google Drive, so you can download them here.\n\nIf you have other large text classification datasets, please share in comments to this post."
    },
    {
        "url": "https://towardsdatascience.com/kaggle-competition-intel-mobileodt-cervical-cancer-screening-8a594a54d5ca?source=user_profile---------17----------------",
        "title": "Kaggle Competition: Intel & MobileODT Cervical Cancer Screening",
        "text": "I tried to approach the problem in a na\u00efve way: just get a pre-trained Inception V3 image classification model and fine-tune it on this dataset.\n\nPhilipp Schmidt published Cervix EDA notebook: researching the basic properties of the dataset.\n\nI loaded all labeled images and resized them to 224x224 shape, which is used in Inception V3. Shuffled and split into train and dev sets in 80/20 proportion.\n\nInception V3 model and weights, pre-trained on ImageNet dataset, were loaded using Keras. Top classification layer was removed, a new dense layer with dropout and a softmax layer were added on top. I froze all Inception layers and trained new dense layers first. Then last two convolutional blocks of Inception were unfrozen and I fine-tuned them as well. The model was trained on 80% of labeled data and validated on 20%.\n\nNot great. Validation loss doesn\u2019t go lower than 0.95. The model overfits quickly. I got 54.5% accuracy on the validation set.\n\nMy code is available here."
    },
    {
        "url": "https://medium.com/@surmenok/improving-deepspell-code-bdaab1c5fb7e?source=user_profile---------18----------------",
        "title": "Improving DeepSpell Code \u2013 Pavel Surmenok \u2013",
        "text": "Building a spell checker using deep learning is a great idea. After reading Tal Weiss\u2019s article about the character-based model for spell checking I wanted to run his code, see how well it works for real applications, and work on improvements. This became my fun one-month side project for January 2017.\n\nThe goal of the project is to have an algorithm which can correct spelling errors in English texts. Then, if it works with decent accuracy, I will use it in an NLP pipeline for chatbots. I hope that spelling correction can improve the accuracy of downstream tasks in the NLP pipeline.\n\nOriginal DeepSpell code by Tal Weiss is on GitHub. The code reads 286 MB news from a text file that is a part of a billion word dataset. Preprocessing pipeline cleans the text, parses into lines, creates artificial spelling errors and splits every line into chunks of no longer than 40 characters. A result of this pipeline is a set of question/answer pairs where a question is a string with spelling errors, and an answer is a string in original, syntactically correct, form. The dataset is split into a training set and a dev (validation) set in 90/10 proportion. Then an LSTM sequence-to-sequence model is trained to predict original text based on a text with typos.\n\nThe first challenge is to install all the required software on my machine. As you know, installing a bunch of dependencies to run a deep learning algorithm is often a painful and error-prone process. I simplified it a bit using Docker. When you package an application in a Docker container, all dependencies are nicely isolated, and you don\u2019t need to worry that the code requires a different version of Python/Theano/Keras/CUDA than already installed on your machine.\n\nNVIDIA Docker can be used to run Docker containers with GPU access.\n\nTal wrote that he chose Theano as a back-end because TensorFlow is painful to install, and it is slow. I decided to run it with TensorFlow because Docker makes it easy to do. Google provides an official TensorFlow Docker image. There is a separate Docker image for running computation on GPU, so both CPU and GPU are supported.\n\nA billion word dataset is a 10.5 GB archive. Downloading this file from a Docker container would be inefficient because every time you instantiate a container, it would download 10 gigabytes again. I wrote a build script which downloads the file, extracts one file which is used for training and places it in a directory available for Docker build.\n\nThe next challenge is to make the code work on my machine. The first time I run it on my 8GB MacBook Pro I got an issue with insufficient amount of memory available. Python process consumed 6.9 GB memory and crashed with \u201cKilled\u201d message. The problem was that it tried to load the entire dataset into memory and do all pre-processing steps on the entire dataset.\n\nI rewrote pre-processing code to clean each line separately. Vectorization should process each batch separately. Then model training code should work with a batch generator instead of full X and y matrices.\n\nKeras allows applying callbacks in certain points of model training. You can use callbacks, for example, for saving intermediary training results to CSV file and writing a log for TensorBoard. Also, Keras lets you save model checkpoints in the middle of the training process.\n\nSpelling errors in the dataset are artificial, so the model learns to fix artificial typos. Will it work as well for fixing real typos? I haven\u2019t checked it yet.\n\nThe dataset is based on news articles. Language in the news is different from language on Reddit or Twitter. Will the model work well on text from other sources?\n\nThe model trains on strings up to 40 characters, and often it is only part of a sentence. Is it enough context? Will it work better if trained on whole sentences?\n\nCurrently, DeepSpell code runs on Keras 1.1.2 and TensorFlow 0.12-rc1. I tried to upgrade it to Keras 1.2.1 and TensorFlow 1.0.0-rc1, but model compilation fails on this configuration. It needs investigation. Maybe something changed in Keras API or TensorFlow API.\n\nSee my changes on GitHub.\n\nIf you are doing any research in this area, please let me know!"
    },
    {
        "url": "https://chatbotslife.com/power-of-bot-discovery-in-google-assistant-cb172704348a?source=user_profile---------19----------------",
        "title": "Power of Bot Discovery in Google Assistant \u2013",
        "text": "How will users discover bots when there are thousands of them? App stores just don\u2019t scale. Google can be the first to find a new model of bot discovery. Brad Abrams, group product management of Google Assistant, touched the topic of bot discovery in an episode of O\u2019Reilly Bots Podcast.\n\nGoogle Assistant supports two models of connecting users to bots. The first model is \u201ctalk to\u201d model. You call the name of the bot explicitly. This approach works fine if you know the brand. It is similar to an app store model.\n\nThe problem is that you may not know a brand. Then you can just tell the assistant what you want to do and it will choose the bot for you. An example Brad was talking about: \u201cOK Google, I want to meditate,\u201d and Google Assistant connects you to Headspace bot. Developers of Google Assistant Actions can register invocation triggers, which work as discovery phrases. Over the long term, Brad expects that most bot invocations will be done using discovery phrases.\n\nMultiple developers can register the same discovery phrase. Ultimately Google will be able to do ranking on discovery phrases. When a user requests something, Google Assistant will use ranking signals to figure out which bot to recommend."
    },
    {
        "url": "https://medium.com/@surmenok/nips-2016-reviews-5fb27c88b0f2?source=user_profile---------20----------------",
        "title": "NIPS 2016 Reviews \u2013 Pavel Surmenok \u2013",
        "text": "NIPS (the Conference and Workshop on Neural Information Processing Systems) is a machine learning and computational neuroscience conference held every December. It was first proposed in 1986, and for a long time, it was a small conference. Interest to NIPS significantly increased when deep learning started demonstrating great results in image recognition, speech recognition, and multiple other areas. Last year NIPS had 2500+ papers submitted and 5000+ people in attendance."
    },
    {
        "url": "https://medium.com/@surmenok/2017-q1-plans-6374ebba4e2d?source=user_profile---------21----------------",
        "title": "2017 Q1 Plans \u2013 Pavel Surmenok \u2013",
        "text": "Ok, a bit of methodology first. I want to be close to artificial intelligence in general and machine learning in particular. What layer of the intelligence stack?\n\nLower levels (hardware, languages) don\u2019t interest me much. Extending ML frameworks like TensorFlow could be interesting, but I\u2019ll skip it for now. Neural network architecture layer is very interesting; it\u2019s where all the fun is going on, but fundamentally requires a lot of research typically done in academia or labs in Google/Microsoft/etc. Not sure if I can push the frontier of research without having time and resources for fundamental research.\n\nWhat\u2019s left is cognitive architecture and application layers: looking for existing research papers, figuring out how different kinds of models work, what problems they are good for, designing implementation for any problem in particular.\n\nMachine learning field if huge, I should narrow it down. It should be something related to text data and conversational interfaces so that I can apply learning to my work projects right away."
    },
    {
        "url": "https://medium.com/@surmenok/2016-review-9ca9a6f16c23?source=user_profile---------22----------------",
        "title": "2016 Review \u2013 Pavel Surmenok \u2013",
        "text": "Reading is important. I read or started reading a few books in 2016.\n\nTensorFlow For Machine Intelligence. A comprehensive (271 pages) introduction to using TensorFlow for Machine Learning.\n\nMachine Learning Yearning by Andrew Ng. The book is not finished yet, but Andrew published drafts of a few chapters for those who signed up on the website. This is very valuable and unique content that you won\u2019t find anywhere else. Andrew focuses on tips and tricks for applying machine learning algorithms to real problems: how to split the dataset, how to decide what to do to improve the model: get more training data, get more validation data, use a larger model, etc., how to analyze errors.\n\nThe Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World by Pedro Domingos. An overview of 5 major schools of machine learning: symbolist, connectionist, evolutionist, bayesian, analogizer. There is no math there, and no deep dive into any algorithm in particular. It is good just as a reminder that there is something else besides neural networks, and the Master Algorithm may need to use a combination of ideas from a few different schools of machine learning.\n\nSuperintelligence: Paths, Dangers, Strategies by Nick Bostrom. I heard a lot of criticism about this book from machine learning experts, so I avoided reading this book for a while. But after I\u2019ve read this Yoshua Bengio\u2019s post, I decided that I shall read the book to understand the context of the discussion :) Neil Lawrence commentary should be as interesting as the book itself.\n\nGood Strategy Bad Strategy: The Difference and Why It Matters. This book was interesting to read, and it was useful for me to think about strategy when I was building a Machine Learning roadmap at JustAnswer.\n\nUnder New Management: How Leading Organizations Are Upending Business as Usual by David Burkus. David describes a few innovative management ideas: put customers second (and employees first), make salaries transparent, close open offices, ditch performance appraisals, use unlimited vacation policy, etc. I can agree with some of them, and some are controversial. It\u2019s definitely worth to think about pros and cons of these policies.\n\nWhy Greatness Cannot Be Planned: The Myth of the Objective. Interesting read, challenging the myth of objectives. Often it is not efficient to have an objective if you don\u2019t have a concrete path to reach it. The innovation is not driven by focused effort. It is better to embrace serendipitous discovery.\n\nSnow Crash by Neal Stephenson. I read it a few years ago in Russian translation, this time I decided to re-read the original text. After all, it is one of the most influential science fiction books.\n\nOceanic by Greg Egan. Greg Egan writes the hardest science fiction I have ever seen. Mathematical and quantum ontology themes.\n\nGlasshouse by Charles Stross. I would classify it as post-singularity science fiction, picturing the world after the technological singularity.\n\nThe Atrocity Archives by Charles Stross. This is the first book in The Laundry Files series. It is quite entertaining, but I feel that it is more of a spy thriller than science fiction.\n\nTranscend: Nine Steps to Living Well Forever by Ray Kurzweil. Ray Kurzweil summarized the state of research on longevity and provided recommendations spanning from diet and exercise to drugs and supplements. As immortality is one of my long term goals, this book was quite interesting for me.\n\nPrisoners of Geography: Ten Maps That Explain Everything About the World. The author explains many events happening in the world by geography."
    },
    {
        "url": "https://medium.com/@surmenok/machine-learning-and-artificial-intelligence-in-2016-3e49093f6297?source=user_profile---------23----------------",
        "title": "Machine Learning and Artificial Intelligence in 2016",
        "text": "2016 was an interesting year. AI winter is over, but this time AI is almost a synonym for deep learning. Major technology companies (Google, Microsoft, Facebook, Amazon and Apple) announced new products and services built using machine learning. DeepMind AlphaGo beat the world champion in Go. Salesforce bought MetaMind to build a deep learning lab. Apple promised to open up its deep learning research.\n\nNIPS 2016 (Neural Information Processing Systems) conference had 2500+ papers submitted and 5000+ people in attendance. A lot of exciting research was published last year. Overview of NIPS 2016: day 0&1, day 2, day 3.\n\nBay Area Deep Learning School was a good way to learn about state of the art in different areas of deep learning research: computer vision, NLP, unsupervised learning, tips and tricks for applying deep learning. It was a two-day event with great speakers, including Andrew Ng, Yoshua Bengio, Andrej Karpathy, Richard Socher, Russ Salakhutdinov and others. The video is available on YouTube: day 1, day 2.\n\nChatbots are on the rise. Facebook rolled out bot API for Messenger. Microsoft made it possible to build bots for Skype. Google bought api.ai. Facebook bought wit.ai. Microsoft built Bot Framework and luis.ai. Microsoft launched and shut down Tay.ai. Later they quietly rolled out the next version of the chatbot, called Zo.\n\nBot builder community is large, Bots group in Facebook has more than 20,000 members, but there are very few really good use cases. Bot builders are actively exploring the space trying to figure out what use cases are good for bots, how to design dialogs, how to make bots smarter. Smartness is still an unsolved problem, interfaces of the majority of the bots are based on buttons rather than natural language understanding.\n\nSelf-driving cars are a hot topic. 2016 news: Tesla announced the self-driving car plan, GM, BMW and Ford are building self-driving cars, Apple pulls back, and Uber launches and then cancels self-driving cars in San Francisco due to troubles with regulation. Self-driving car technology is rapidly improving, and regulation will be the main obstacle for a wider roll-out of the technology.\n\nAI will replace many jobs, probably starting from truck drivers, but also knowledge jobs. It is already happening, and even hedge fund managers can be replaced. Some people are scared of this transition, some are excited, and some try to think how to guide the transition to get to the better future. AI is coming no matter what we think about it, so let\u2019s learn how to make it useful.\n\nAm I missing any interesting areas of research? Please share in comments!"
    },
    {
        "url": "https://medium.com/@surmenok/deep-learning-for-spell-checking-2ffdbad65554?source=user_profile---------24----------------",
        "title": "Deep Learning for Spell Checking \u2013 Pavel Surmenok \u2013",
        "text": "I use spell checking every day, it is built into word processors, browsers, smartphone keyboards. It helps to create better documents and make communication clear. More sophisticated spell checker can find grammatical and stylistic errors.\n\nHow to add spell checking to your application? A very simple implementation by Peter Norvig is just 22 lines of Python code.\n\nIn \u201cDeep Spelling\u201d article, Tal Weiss wrote that he tried to use this code and found that it is slow. The code is slow because it is brute forcing all possible combinations of edits on the original text.\n\nAn interesting approach is to use deep learning. Just create artificial dataset by adding spelling errors into correct English texts. And you better have lots of text! The author has been using one billion words dataset released by Google. Then train character-level sequence-to-sequence model with LSTM layers to convert a text with spelling errors to a correct text. Tal got very interesting results. Read the article for details.\n\nGood quality spell checkers can be very useful for chatbots. Most of the chatbots rely on simple NLP techniques, and typical NLP pipeline includes syntax analysis and part of speech tagging, which can be easily broken if the input message is not grammatically correct or has spelling errors. Perhaps fixing spelling errors earlier in the NLP pipeline can improve the accuracy of natural language understanding.\n\nIt can be good to try train such spell checker model on another dataset, more conversational.\n\nHave you tried to use any models like this in your apps?\n\nThe article was originally published on http://pavel.surmenok.com/2016/11/08/deep-learning-for-spell-checking/"
    },
    {
        "url": "https://medium.com/@surmenok/intelligence-platform-stack-8c623f71f990?source=user_profile---------25----------------",
        "title": "Intelligence Platform Stack \u2013 Pavel Surmenok \u2013",
        "text": "Machine intelligence field grows with breakneck speed since 2012. That year Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton achieved the best result in image classification on LSVRC-2010 ImageNet dataset using convolutional neural networks. It\u2019s amazing that end-to-end training of a deep neural network worked better than sophisticated computer vision systems with handcrafted feature engineering pipelines being refined by researchers for decades.\n\nSince then deep learning field got the attention of machine learning researchers, software engineers, entrepreneurs, venture investors, even artists and musicians. Deep learning algorithms surpassed the human level of image classification and conversational speech recognition, won Go match versus 18-time world champion. Every day new applications of deep learning emerge, and tons of research papers are published. It\u2019s hard to keep up. We live in a very interesting time, future is already here.\n\nDeep learning is rapidly moving from research labs to the industry. Google had 2700+ projects using deep learning in 2015.\n\nA neural network is only one layer of an intelligent application. If you look at a typical deep learning system, it includes hardware (CPU, GPU, FPGA), drivers, programming languages, libraries, models, etc. Almost no company can develop the entire stack alone, the work should be divided between multiple organizations and multiple teams inside of an organization. It is natural to structure computer systems in layers, from bare metal in the bottom to more abstract layers on the top. If interfaces between layers are stable and well defined, it is easy for engineers to focus on one of the layers and reuse lower layers developed by someone else.\n\nThere is no stable intelligence platform stack yet, it is open for interpretation and changes. Most common model of the stack includes 8 layers, from hardware in the bottom to an application layer on the top. Layers from the bottom up:\n\nTypically it is multi-core CPU or GPU. GPUs are much more powerful than CPUs for deep learning applications. Thanks to gamers who paid for the progress on GPU engineering. Now there are companies working on hardware which is more power efficient than GPUs: FPGA and ASIC chips. For example, Google is using special chips named TPU (tensor processing unit) in their data centers.\n\nCompanies working on this layer: NVIDIA, Intel, Altera (acquired by Intel), Google (tensor processing unit).\n\n2. Hardware abstraction. NVIDIA CUDA and CuDNN provide functionality for training and inference of neural networks on NVIDIA GPU.\n\n3. Programming language. Developers need to use a programming language to do data preprocessing, model training, inference. Python is the most popular language for deep learning, at least for training, because there are very good scientific computing libraries for Python. C++, Lua, R are also being used, though less than Python.\n\n4. Deep learning libraries/frameworks. Perhaps the most important layer. It contains means for expressing deep learning models and often contains algorithms for auto-differentiation and optimization. Theano, Torch, and Caffe have been used by researchers and engineers a lot. In 2015 Google released TensorFlow, and it quickly became the most popular framework for deep learning. A few more recently released libraries: CNTK (Microsoft), Warp-CTC (Baidu).\n\nAll these frameworks have different levels of abstraction, e.g. Theano has auto-diff but doesn\u2019t have built-in code for popular types of neural networks (LSTM, CNN), so you will need to program neural network code yourself or use higher level frameworks which work on top of Theano, e.g. Keras.\n\nThe libraries have different characteristics for use in a production environment: TensorFlow provides very efficient C++ based TensorFlow Serving component which can do inference, Theano doesn\u2019t have anything like that.\n\n5. Neural network architecture. There are lots of different ways how you can configure a neural network: which types of neurons you use, how many layers and neurons, regularization techniques. See a catalog of architectures in Neural Network Zoo.\n\nNeural networks tend to get larger and more sophisticated. AlexNet had 8 learned layers, more recent ResNet networks from Microsoft researchers use 200\u20131000 layers.\n\nRecent neural network models also are more complex and modular than earlier architectures. A Dynamic Memory Network model, presented by Richard Socher at Bay Area Deep Learning School 2016 contains 5 modules:\n\nThis is an area of active research: Google, Facebook, Baidu, Microsoft, Stanford, University of Toronto and others.\n\n6. Cognitive architecture. A few neural networks and other kinds of AI modules can be combined to support smarter reasoning. Deep learning models which extract meaning from the text can work together with symbolic computation, use predefines rules and knowledge bases to get to deeper levels of text understanding. Supervised learning models can be combined with unsupervised learning models to use unlabeled data better. Large neural networks can be split into smaller neural networks trained and configured separately. This area is quite interesting, and there are more problems than solutions here yet.\n\n7. Machine learning as a service. Cloud APIs for machine learning, e.g. Google Vision API, Google Cloud ML. The chatbot ML services wit.ai, api.ai, luis.ai are in this category too.\n\n8. Application. An application which is accessible by end users and provides business value."
    },
    {
        "url": "https://medium.com/@surmenok/natural-language-pipeline-for-chatbots-897bda41482?source=user_profile---------26----------------",
        "title": "Natural Language Pipeline for Chatbots \u2013 Pavel Surmenok \u2013",
        "text": "Your chatbot needs a preprocessing NLP pipeline to handle typical errors. It may include these steps:\n\nGet the raw input and fix spelling errors. You can do something very simple or build a spell checker using deep learning.\n\nIt is very helpful to analyze every sentence separately. Splitting the text into sentences is easy, you can use one of NLP libraries, e.g. NLTK, StanfordNLP, SpaCy.\n\nThis is also very important because hardcoded rules typically operate with words. Same NLP libraries can do it.\n\nSome words have multiple meanings, for an example \u201ccharge\u201d as a noun and \u201ccharge\u201d as a verb. Knowing a part of speech can help to disambiguate the meaning. You can use same NLP libraries, or Google SyntaxNet, that is a little bit more accurate and supports multiple languages.\n\nOne word can have many forms: \u201cpay\u201d, \u201cpaying\u201d, \u201cpaid\u201d. In many cases, an exact form of the word is not important for writing a hardcoded rule. If preprocessing code can identify a lemma, a canonical form of the word, it helps to simplify the rule. Lemmatization, identifying lemmas, is based on dictionaries which list all forms of every word. The most popular dictionary for English is WordNet. NLTK and some other libraries allow using it for lemmatization.\n\nDates and numbers can be expressed in different formats: \u201c3/1/2016\u201d, \u201c1st of March\u201d, \u201cnext Wednesday\u201d, \u201c2016\u201303\u201301\u201d, \u201c123\u201d, \u201cone hundred\u201d, etc. It may be helpful to convert them to unified format before doing pattern matching. Other entities which require special treatment: locations (countries, regions, cities, street addresses, places), people, phone numbers.\n\nIf you want to search for a breed of a dog, you don\u2019t want to list all the dog breeds in the rule, because there are hundreds of them. It is nice if preprocessing code identified a dog breed in the message and marked the word with a special tag. Then you can just look for that tag when applying the rule.\n\nWordNet can be used to identify common concepts. You may need to add domain specific concept libraries, e.g. a list of drug names if you are building a healthcare bot."
    },
    {
        "url": "https://hackernoon.com/how-to-run-text-summarization-with-tensorflow-d4472587602d?source=user_profile---------27----------------",
        "title": "How to Run Text Summarization with TensorFlow \u2013",
        "text": "Text summarization problem has many useful applications. If you run a website, you can create titles and short summaries for user generated content. If you want to read a lot of articles and don\u2019t have time to do that, your virtual assistant can summarize main points from these articles for you.\n\nIt is not an easy problem to solve. There are multiple approaches, including various supervised and unsupervised algorithms. Some algorithms rank the importance of sentences within the text and then construct a summary out of important sentences, others are end-to-end generative models.\n\nEnd-to-end machine learning algorithms are interesting to try. After all, end-to-end algorithms demonstrate good results in other areas, like image recognition, speech recognition, language translation, and even question-answering.\n\nIn August 2016, Peter Liu and Xin Pan, software engineers on Google Brain Team, published a blog post \u201cText summarization with TensorFlow\u201d. Their algorithm is extracting interesting parts of the text and create a summary by using these parts of the text and allow for rephrasings to make summary more grammatically correct. This approach is called abstractive summarization.\n\nPeter and Xin trained a text summarization model to produce headlines for news articles, using Annotated English Gigaword, a dataset often used in summarization research. The dataset contains about 10 million documents. The model was trained end-to-end with a deep learning technique called sequence-to-sequence learning.\n\nCode for training and testing the model is included into TensorFlow Models GitHub repository. The core model is a sequence-to-sequence model with attention. When training, the model is using the first two sentences from the article as an input and generates a headline.\n\nWhen decoding, the algorithm is using beam search to find the best headline from candidate headlines generated by the model.\n\nGitHub repository doesn\u2019t include a trained model. The dataset is not publicly available, a license costs $6000 for organizations which are not members of Linguistic Data Consortium. But they include a toy dataset which is enough to run the code.\n\nYou will need TensorFlow and Bazel as prerequisites for training the model.\n\nThe toy dataset included into the repository, contains two files in \u201cdata\u201d directory: \u201cdata\u201d and \u201cvocab\u201d. The first one contains a sequence of serialized tensorflow.core.example.example_pb2.Example objects. An example of code to create a file with this format:\n\n\u201cvocab\u201d file is a text file with the frequency of words in a vocabulary. Each line contains a word, space character and number of occurrences of that word in the dataset. The list is being used to vectorize texts.\n\nRunning the code on toy dataset is really simple. Readme on GitHub repo lists a sequence of commands to run training and testing code.\n\nYou can run TensorBoard to monitor training process:"
    },
    {
        "url": "https://chatbotslife.com/google-assistant-bot-platform-318b26ee1d80?source=user_profile---------28----------------",
        "title": "Google Assistant Bot Platform \u2013",
        "text": "Google announced Google Assistant bot in May 2016 on Google I/O conference. The bot is integrated into a new messaging application Google Allo that was released on September 21, 2016.\n\nGoogle Assistant can show weather, news, travel ideas, restaurants, put events on your calendar, and make restaurant reservations. I guess it can do most of the things that Google Now could handle.\n\nGoogle Assistant is going to be an \u201cuber-bot\u201d: a bot that serves as an entry point for any user requests. The bot can recognize what you are asking and route the request to an appropriate specialized bot. On October 3, 2016, Google announced \u201cActions on Google\u201d program, which will allow developers to build \u201cactions\u201d for Google Assistant.\n\nA few dozen companies plan to integrate their services with Google Assistant. The platform will launch in December 2016.\n\nActions can be direct or conversational. Direct actions are similar to Siri or Alexa interactions: ask something, get a response. Conversational actions are back and forth conversation. Google is going to help developers with building conversational UX using services like API.AI, which was acquired by Google in September.\n\nGoogle also promises to launch an Embedded Google Assistant SDK to allow hardware manufacturers to include Google Assistant in their products.\n\nGoogle Assistant can\u2019t process purchases yet and doesn\u2019t provide Payment API to bot makers. It will be logical for Google to add payment functionality, considering that Facebook Messenger allows payments.\n\nThere is no Actions development documentation publicly available. It will be interesting to know how much freedom developers will have in design and integration of their bots.\n\nThe article was originally published on http://pavel.surmenok.com/2016/10/12/google-assistant-bot-platform/"
    },
    {
        "url": "https://medium.com/@surmenok/chatbot-architecture-496f5bf820ed?source=user_profile---------29----------------",
        "title": "Chatbot Architecture \u2013 Pavel Surmenok \u2013",
        "text": "Chatbots are on the rise. Startups are building chatbots, platforms, APIs, tools, analytics. Microsoft, Google, Facebook introduce tools and frameworks, and build smart assistants on top of these frameworks. Multiple blogs, magazines, podcasts report on news in this industry, and chatbot developers gather on meetups and conferences.\n\nI have been working on chatbot software for a while, and I have been looking on what is going on in the industry. See my previous posts:\n\nIn this article, I will dive into architecture of chatbots.\n\nThere are two major types of chatbots: chatbots for entertainment and chatbots for business.\n\nEngineers have been developing chatbots for entertainment for decades since famous chatbot-psychotherapist ELIZA was introduced in 1966. Creators of these chatbots usually try to make a bot which can look like a human, pass the Turing test. Perhaps all of the bots which participate in Loebner\u2019s prize and similar competitions are in this group. Microsoft\u2019s bots Xiaoice and Tay have similar behavior. The most recent example is \u201cSpock\u201d bot in Skype: \u201cChat with Spock, second in command of the USS Enterprise, to learn the ways of Vulcan logic!\u201d\n\nChatbot responses to user messages should be smart enough for user to continue the conversation. The chatbot doesn\u2019t need to understand what user is saying and doesn\u2019t have to remember all the details of the dialogue.\n\nOne way to assess an entertainment bot is to compare the bot with a human (Turing test). Other, quantitative, metrics are an average length of conversation between the bot and end users or average time spent by a user per week. If conversations are short then the bot is not entertaining enough.\n\nChatbots for business are often transactional, and they have a specific purpose. Conversation is typically focused on user\u2019s needs. Travel chatbot is providing an information about flights, hotels, and tours and helps to find the best package according to user\u2019s criteria. Google Assistant readily provides information requested by the user. Uber bot takes a ride request.\n\nConversations are typically short, less than 15 minutes. Each conversation has a goal, and quality of the bot can be assessed by how many users get to the goal. Has the user found information she was looking for? Has the user successfully booked a flight and a hotel? Has the user bought products which help to solve the problem at hand? Usually, these metrics are easy to track.\n\nPerhaps some bots don\u2019t fit into this classification, but it should be good enough to work for the majority of bots which are live now.\n\nAnother useful classification is based on a type of conversation: one-to-one or one-to-many, if the chatbot is added into a group chat. Dynamics of conversation, use cases, complexity of chatbot software is very different for these cases.\n\nAs Denny Britz wrote in \u201cDeep learning for chatbots\u201d, the chatbot can either generate responses from scratch based on machine learning models or use some heuristic to select a response from a library of predefined responses.\n\nGenerative models are harder to build and train. Typically it requires millions of examples to train a deep learning model to get decent quality of conversation, and still you can\u2019t be totally sure what responses the model will generate.\n\nGenerative models are the future of chatbots, they make bots smarter. This approach is not widely used by chatbot developers, it is mostly in the labs now.\n\nRetrieval-based models are much easier to build. They also provide more predictable results. You probably won\u2019t get 100% accuracy of responses, but at least you know all possible responses and can make sure that there are no inappropriate or grammatically incorrect responses.\n\nRetrieval-based models are more practical at the moment, many algorithms and APIs are readily available for developers.\n\nThe chatbot uses the message and context of conversation for selecting the best response from a predefined list of bot messages. The context can include current position in the dialog tree, all previous messages in the conversation, previously saved variables (e.g. username).\n\nIf the bot doesn\u2019t use context then it is stateless. It will only respond to the latest user message, disregarding all the history of the conversation.\n\nHeuristics for selecting a response can be engineered in many different ways, from if-else conditional logic to machine learning classifiers. The simplest technology is using a set of rules with patterns as conditions for the rules. This type of models is very popular for entertainment bots. AIML is a widely used language for writing patterns and response templates. Bot developers write code in AIML language, code can include multiple units like this:\n\nWhen the chatbot receives a message, it goes through all the patterns until finds a pattern which matches user message. If the match is found, the chatbot uses the corresponding template to generate a response.\n\nChatScript is a modern implementation of this idea. It is an open source chatbot engine which allows defining a chatbot in a rule-based language. Each rule contains a pattern and an output:\n\nChatScript engine has a powerful natural language processing pipeline and a rich pattern language. Using ChatScript you can do much more than with AIML. It will parse user message, tag parts of speech, find synonyms and concepts, and find which rule matches the input. In addition to NLP abilities, ChatScript will keep track of dialog, so that you can design long scripts which cover different topics. It won\u2019t do anything fancy, though. It won\u2019t run machine learning algorithms and won\u2019t access external knowledge bases or 3rd party APIs unless you do all the necessary programming.\n\nThe inherent problem of pattern-based heuristics is that patterns should be programmed manually, and it is not an easy task, especially if the chatbot has to correctly distinguish hundreds of intents. Imagine that you are building a customer service bot and the bot should respond to a refund request. Users can express it in hundreds of different ways: \u201cI want a refund\u201d, \u201cRefund my money\u201d, \u201cI need my money back\u201d. At the same time, the bot should respond differently if the same words are used in another context: \u201cCan I request a refund if I don\u2019t like the service?\u201d, \u201cWhat is your refund policy?\u201d. Humans are not good at writing patterns and rules for natural language understanding, computers are much better at this task.\n\nMachine learning lets us train an intent classification algorithm. You just need a training set of a few hundred or thousands of examples, and it will pick up patterns in the data.\n\nSuch algorithms can be built using any popular machine learning library like scikit-learn. Another option is to use one of cloud API: wit.ai, api.ai, Microsoft LUIS. Wit.ai was probably the first machine learning API for chatbots, it was bought by Facebook this year, and it is free.\n\nPatterns or machine learning classification algorithms help to understand what user message means. When the chatbot gets the intent of the message, it shall generate a response. How can the bot do it? The simplest way is just to respond with a static response, one for each intent. Or, perhaps, get a template based on intent and put in some variables. It is what ChatScript based bots and most of other contemporary bots are doing.\n\nHow can bots do better? There is no single answer. Response generation mechanism must depend on the task at hand. A medical chatbot will probably use a statistical model of symptoms and conditions to decide which questions to ask to clarify a diagnosis. A question-answering bot will dig into a knowledge graph, generate potential answers and then use other algorithms to score these answers, see how IBM Watson is doing it. A weather bot will just access an API to get a weather forecast for a given location.\n\nThe chatbot can express the same message using different words. A weather bot can say \u201cIt\u2019s rainy\u201d, or \u201cProbability of rain is 80%\u201d or \u201cPlease carry an umbrella today\u201d. Which one will work the best for the user? Different users prefer different styles of response. The bot can analyze previous chats and associated metrics (length of the conversation, probability of sale, rating of customer satisfaction, etc.) to tailor responses for the user.\n\nThe chatbot can have separate response generation and response selection modules, as shown in the diagram below.\n\nMessage processing begins from understanding what the user is talking about. Intent classification module identifies the intent of user message. Typically it is selection of one out of a number of predefined intents, though more sophisticated bots can identify multiple intents from one message. Intent classification can use context information, such as intents of previous messages, user profile, and preferences. Entity recognition module extracts structured bits of information from the message. The weather bot can extract location and date.\n\nThe candidate response generator is doing all the domain-specific calculations to process the user request. It can use different algorithms, call a few external APIs, or even ask a human to help with response generation. The result of these calculations is a list of response candidates. All these responses should be correct according to domain-specific logic, it can\u2019t be just tons of random responses. The response generator must use the context of the conversation as well as intent and entities extracted from the last user message, otherwise, it can\u2019t support multi-message conversations.\n\nThe response selector just scores all the response candidate and selects a response which should work better for the user.\n\nHow are you designing software for your bots, and what algorithms, libraries, and APIs are you using?"
    },
    {
        "url": "https://medium.com/@surmenok/character-level-convolutional-networks-for-text-classification-d582c0c36ace?source=user_profile---------30----------------",
        "title": "Character-level Convolutional Networks for Text Classification",
        "text": "One of the common natural language understanding problems is text classification. Over last few decades, machine learning researchers have been moving from the simplest \u201cbag of words\u201d model to more sophisticated models for text classification.\n\nBag of words model uses only information about which words are used in the text. Adding TFIDF to the bag of words helps to track relevancy of each word to the document. Bag of n-grams enables using partial information about structure of the text. Recurrent neural networks, like LSTM, can capture dependencies between words even if they are far from each other. LSTM learns structure of sentences from the raw data, but we still have to provide a list of words. Word2vec algorithm adds knowledge about word similarity, which helps a lot. Convolutional neural networks can also help to process word-based datasets.\n\nA trend is to learn using raw data, and provide machine learning models with an access to more information about text structure. A logical next step would be to feed a stream of characters to the model and let it learn all about the words. What can be cruder than a stream of characters? An additional benefit is that the model can learn misspellings and emoticons. Also, the same model can be used for different languages, even those where segmentation into words is not possible.\n\nThe article \u201cCharacter-level Convolutional Networks for Text Classification\u201d (Xiang Zhang, Junbo Zhao, Yann LeCun) explores usage of character-level ConvNet networks for text classification. They compare performance of a few different models on several large-scale datasets.\n\nDatasets contain from 120,000 to 3,600,000 samples in the training set, from 2 to 14 classes. The smallest dataset is AG\u2019s News: news articles divided into 4 classes, 30,000 articles for each class in the training set. The largest is Amazon Review Polarity: 2 polarity segments with 1,800,000 reviews for each of them.\n\nCharacter-level ConvNet was compared with state-of-the-art models: Bag-of-words and its TFIDF, Bag-of-ngrams and its TFIDF, Bag-of-means on word embedding, word-based ConvNet, word-based LSTM.\n\nResults are quite interesting. N-gram and N-gram TFIDF models have are the best for smaller datasets, up to several hundreds of thousands of samples. But when dataset size grows to several million we can observe that character-level ConvNet performs better.\n\nConvNet tends to work better for texts which are less curated. For example, ConvNet works better on Amazon reviews dataset. Amazon Reviews are raw user inputs, whereas users could be more careful in writings on Yahoo! Answers.\n\nChoice of alphabet matters. ConvNet works better is not distinguishing between upper and lower case characters.\n\nAnother overview of this paper\n\nA Set of Character-Based Models for Sentiment Analysis, Ad Blocking and other tasks"
    },
    {
        "url": "https://medium.com/@surmenok/2016-is-the-year-of-chat-bots-892ed85cf81c?source=user_profile---------31----------------",
        "title": "2016 is the Year of Chat Bots \u2013 Pavel Surmenok \u2013",
        "text": "2016 is the Year of Chat Bots\n\nWhen Apple introduced App Store in 2008, developers\u2019 attention moved from web-based to native mobile apps.\n\nA few years later the app market stabilized. Facebook, Amazon, and Google apps dominate in their verticals. Consumers don\u2019t want to install new apps anymore. According to comScore\u2019s mobile app report, most US smartphone owners download zero apps in a typical month, and a \u201cstaggering 42% of all app time spent on smartphones occurs on the individual\u2019s single most used app\u201d.\n\nMore than half of the time we spend on our phones is talking, texting or in email, according to Experian\u2019s report.\n\nDevelopment of native mobile apps is expensive. Developers need to support multiple platforms and different hardware, and it slows down release cycle. Moving important parts of the software to back-end and making UI relatively simple allows developers to move fast. This shift also makes possible to use extensive computation on the back-end, which is important for machine intelligence applications.\n\nIn the old paradigm, users had to understand their objective, then learn UI of the application, and then work towards the objective. When using conversational agents, you just tell the agent what you want to be done, and the agent works on that. Users can interact with the agent using natural language. No checkboxes, no buttons.\n\nAnother important reason is that universal text-based interface makes easier adding machine intelligence to the application. People generate terabytes of text every day, and application developers can leverage already existing text datasets to train machine learning models. For example, as mentioned in New York Times article about Microsoft\u2019s chat bot Xiaoice, \u201cMicrosoft has been able to give Xiaoice a more compelling personality and sense of \u201cintelligence\u201d by systematically mining the Chinese Internet for human conversations\u201d.\n\n50 years ago people used Command Line Interfaces (CLI). You type a command and then read the result.\n\nOn December 9, 1968, Douglas Engelbart did a live presentation of Graphical User Interface (GUI) and a bunch of other exciting things. The demo was later called \u201cThe Mother of All Demos\u201d.\n\nConversational user interfaces (CUIs) are a spoken or written way of interacting with a device. In some sense, we are going back to simpler text-based interface of CLI era. The difference is that computers with conversational interfaces can handle natural language, and keep the conversation.\n\nIt\u2019s important to make a distinction between conversational interface and a chat bot. Conversational interfaces can be used to talk to a human, group of humans, or, as in Facebook M, a group of humans and bots.\n\n\u201cBots and Chat Interfaces are not the same things, but they\u2019re related. For the purpose of this post, lets define them as follows: Bots are chat-based conversations between a user and an automated response system. No humans. Chat interfaces are a superset of this \u2014 any app where the user interface looks more like messaging than point-and-click. So, all bots have chat interfaces, but not all chat interfaces are bots. Square is a rectangle type situation.\u201d\n\n\u201cThe Hidden Homescreen\u201d article lists some of the services which currently use chat interfaces, but not necessarily bots.\n\nLack of graphical elements doesn\u2019t mean that we don\u2019t need designers anymore. \u201cNo UI\u201d buzzword is misleading. We still need designers, but their responsibilities and skill set will change.\n\nThe first, and probably the most important, design question is whether to humanize the bot. Should the bot pretend to be a human? Does personality matter? Is there a way to keep the personality, but don\u2019t pretend to be a human?\n\nHow polite the bot should be? Does it always respond to questions? Does it say \u201cI don\u2019t understand\u201d if it doesn\u2019t catch the meaning of user\u2019s message, or tries to quibble? Will your bot pass the Beer Test?\n\nChatbot technology ranges from simple rule-based systems, like those based on AIML language, to deep learning algorithms. Most of modern real world applications contain many different components, including deep learning models for tasks like customer intent classification, question answering, and search.\n\nA few research labs actively work on training deep learning models which are capable of chatting with people without adding any custom logic, but it is far from being perfect yet. Recent \u201cA Neural Conversational Model\u201d paper from Google describes building a neural network model which learns from large datasets of movie transcripts and tech support chats.\n\nPeople use many different devices and applications to chat nowadays. E-mails, text messages, messengers (Facebook, Skype, WhatsUp, Kik, Slack, HipChat, Viber, etc.), web-based chats and forums. And it makes perfect sense for chat bots to use all these mediums too, right? If we want to make the interface as easy to use as possible, we have to integrate with devices, applications, and protocols which are already familiar to the users.\n\nx.ai, a personal assistant which schedules meetings for you, is working over email. There are many chatbots which integrate with Slack. Facebook is developing a Chat SDK which will allow building bots integrated with Facebook Messenger.\n\nAre you building an incredible chat bot yet? What technology do you use, and what are design challenges? Let me know in the comments.\n\nThis article was originally published on http://pavel.surmenok.com/2016/03/05/2016-is-the-year-of-chatbots/"
    },
    {
        "url": "https://medium.com/@surmenok/re-work-virtual-assistant-summit-notes-e9a30600213d?source=user_profile---------32----------------",
        "title": "RE-WORK Virtual Assistant Summit Notes \u2013 Pavel Surmenok \u2013",
        "text": "Two decades ago if you wanted to use a software, you needed to install it on your desktop and use it only from your desktop. Then Salesforce came along and told us that it\u2019s not necessary and you can use software from everywhere. Then Apple came along and told us that there is an app for that. Applications became mobile. The next paradigm is the one where intelligent agents arrive.\n\nIn the old paradigm user had to understand the objective, learn the interface and then work towards the objective. In the new paradigm you just tell the agent what you want to be done, and the agent works on that. There is nothing user can touch there. We need some new setting here where we see the syntax disappear and people move from little tasks to jobs.\n\nThere are two choices. Do you create it as a traditional piece of software where user descries the objective, or you humanize it? When humanizing the agent, you democratize the software. This will be black and white, you can\u2019t be anything in between. Either you humanize it or you don\u2019t.\n\nWho do you hire to humanize your agents, who will design that persona? Engineering bot rules is not humanizing, it\u2019s just moving from pixels to text.\n\nx.ai doesn\u2019t try to fool people that their bot Amy is a human. They tell up-front that it is a machine, and then see if they can create a relationship.\n\nMany users treat the bot like a human. They thank Amy, ask if she will be joining the meeting. Users even send flowers and whiskey to Amy, although they were advertised up-front that it is a machine. When they want to reschedule the meeting, instead of sending just \u201ccancel the meeting\u201d, they apologize and tell why they can\u2019t make it. In 11% of the meetings which they do, at least one email has one intent \u2014 to thank Amy.\n\nWe will be degrading as humans if using systems where there is no penalty for being an asshole.\n\nWe can extract much richer dataset if we humanize it. We can tease more details from the user."
    }
]